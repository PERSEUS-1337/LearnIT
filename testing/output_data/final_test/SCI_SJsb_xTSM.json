{
    "title": "SJsb_xTSM",
    "content": "We present a novel multi-task training approach for learning multilingual distributed representations of text. Our system learns word and sentence embeddings jointly using a multilingual skip-gram model and a cross-lingual sentence similarity model. The architecture can utilize both monolingual and bilingual corpora to cover a larger vocabulary. The model demonstrates competitive performance in cross-lingual document classification and is effective in low-resource scenarios. Multilingual text representations, a widely researched subject in natural language processing, involve learning shared embeddings across languages to transfer knowledge and build complex NLP systems. This approach allows for leveraging semantic and linguistic information in distributed representations, particularly beneficial for languages with limited supervised resources. The most popular approach to learning multilingual embeddings involves training a multilingual word embedding model to derive representations for sentences and documents. This model is trained on word or sentence aligned corpora, using simple predefined functions or parametric composition models. In this work, word and sentence embeddings are learned jointly through a multilingual skip-gram model and a cross-lingual sentence similarity model. The skip-gram model consumes pairs from monolingual and sentence aligned bilingual corpora, while a parametric composition model constructs sentence embeddings from word embeddings using a Bi-directional LSTM and averaging the outputs. The approach involves training a multilingual word embedding model with a Bi-directional LSTM to derive context-dependent word embeddings. The model also includes a cross-lingual sentence similarity component to promote compositionality of learned word embeddings. The model's scalability improves with more languages added, and it performs well in low-resource scenarios, producing comparable representations to state-of-the-art multilingual sentence embeddings. Our sentence embedding model is trained on a large vocabulary and can be used for multi-task modeling, such as sentiment classification. It utilizes both parallel and monolingual corpora to create cross-lingual sentence embeddings. Cross-lingual Sentence Embeddings: Constructing sentence embeddings for different languages is challenging due to the complex nature of sentence semantics. BID29 addresses this by extending the paragraph vector model to a bilingual context, predicting n-grams from both sides of parallel sentence pairs. The model is trained to predict n-grams of a given sentence using a randomly initialized sentence vector. This approach is similar to BID14, which constructs sentence embeddings by averaging word or bi-gram embeddings. Approach taken in BID14 involves constructing sentence embeddings by averaging word or bi-gram embeddings and using a noise-contrastive loss based on euclidean distance. Multi-task learning has been effective in various NLP applications, with shared parameters among tasks. BID23 demonstrates the effectiveness of multi-task learning in sentiment classification tasks by sharing an RNN layer across tasks. BID37 shows benefits of learning a common semantic space for multiple tasks. The multi-task architecture treats training multilingual word embeddings as a separate task with a separate objective. Our model is trained to optimize multilingual skip-gram BID24 and cross-lingual sentence similarity. The architecture computes sentence representations for input word sequences. Multilingual skip-gram model BID24 predicts words from both monolingual and cross-lingual contexts. Word embeddings are learned by optimizing for a pair of languages. The word embeddings for languages L1 and L2 are learned by optimizing skip-gram with pairs sampled from monolingual and cross-lingual neighbors. A parametric composition model constructs sentence embeddings from word embeddings using a bi-directional LSTM. The LSTM contextualizes input word embeddings by encoding the history of each word into its representation. The sentence encoder model utilizes bi-directional LSTM to process word embeddings and generate a sentence representation. The loss function minimizes the difference between embeddings of similar sentences and enforces a margin between non-aligned sentence representations. The sentence encoder model uses bi-directional LSTM to generate sentence representations. The loss function minimizes the difference between similar sentences and enforces a margin between non-aligned representations. For each parallel sentence pair, negative sentences are randomly sampled. The model is trained using the Europarl corpus for initial development and testing, with 90% of sentences used for training and 10% for development. Additional monolingual sentences are also used, which do not overlap with the parallel data. In the joint multi-task setting, word frequencies are counted using combined monolingual and parallel corpora. Vocabulary sizes vary depending on the data used, with 39K for German and 21K for English when using just parallel data, and 120K for German and 68K for English when using both parallel and monolingual data. The model is evaluated on the RCV1/RCV2 cross-lingual document classification task using 1K documents for training and 5K for testing. The Multilingual Skip-gram approach uses stochastic gradient descent with specific parameters for training. In the multi-task scenario, the skip-gram model is modified to converge with the sentence similarity objective. Equal numbers of monolingual and cross-lingual word pairs are sampled for mini-batches. The model uses LSTM with varying hidden dimensions and dropout at the embedding layer. Training is done with Adam optimizer, learning rate of 0.001, and exponential decay after 10k steps. The system is optimized by alternating between mini-batches of two tasks, training models that project words from all input languages to a shared vector space. Four types of models are trained: Sent-Avg, Sent-LSTM, JMT-Sent-Avg, and JMT-Sent-LSTM. These models use different techniques to generate sentence embeddings and are jointly trained in a multilingual setting. The system is optimized by alternating between mini-batches of two tasks, training models that project words from all input languages to a shared vector space. The models include Sent-Avg, Sent-LSTM, JMT-Sent-Avg, and JMT-Sent-LSTM, which generate sentence embeddings and are jointly trained in a multilingual setting. The document embeddings are constructed by averaging sentence embeddings, and a document classifier is trained using these representations for cross-lingual document classification. Results show outperformance of most systems when the sentence embedding dimension is 128, and close performance when increased to 512. Our models with an LSTM layer (Sent-LSTM and JMT-Sent-LSTM) show significant improvement compared to models without one, especially when using document embeddings from sentence encoders trained in a multi-task setting. Multilingual models trained on data from multiple languages perform better, particularly when English is the source language. The multilingual CLDC system shows promising results for es-de pair with no direct parallel data available. Validation loss for JMT-sent-add model is more stable and achieves a lower value than Sent-add model in low-resource scenarios. The main motivation is to create high-quality multilingual embeddings for languages with limited parallel data. Joint multi-task models are evaluated on the RCV1/RCV2 document classification task using 100k parallel sentences and 1 million monolingual sentences for training. The study utilized 90% of parallel data for training and 10% for development. Results indicate that the JMT-Sent-add model outperformed the Sent-add model in terms of loss curves. The use of parametric composition models for deriving sentence embeddings and joint multi-task learning of multilingual word and sentence embeddings show promise. Future work will focus on improving sentence embedding models through modifications to the architecture, such as using stacked LSTMs. Additionally, the exploration of a self-attention layer for sentence encoding was considered but later deemed unnecessary. The study focused on multilingual word and sentence embeddings, highlighting the conflict between multilingual skip-gram and cross-lingual sentence similarity models. The use of hyperparameters can help alleviate this conflict, suggesting that better hyperparameters can improve results in multi-task learning scenarios. Further exploration of hyperparameters is needed to optimize current models. Exploring hyperparameters can enhance results in multi-task learning scenarios, offering potential gains. Further optimization of current models is necessary."
}