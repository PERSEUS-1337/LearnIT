{
    "title": "ry6-G_66b",
    "content": "Localization is the problem of estimating the location of an autonomous agent from an observation and a map of the environment. Traditional methods of localization are sub-optimal in the number of steps required. The \"Active Neural Localizer\" is a neural network that learns to localize efficiently by combining traditional filtering-based methods with a policy model to minimize the number of steps needed. It is trained end-to-end with reinforcement learning and tested in various simulation environments. The study demonstrates the effectiveness of a learned policy in 2D and 3D environments, showing the model's ability to generalize from random textures in Doom to a photo-realistic office space in Unreal engine. Localization, essential for autonomous agents, involves estimating the agent's position in the environment. In this paper, the authors address the global localization problem where the initial position of the agent is unknown. They highlight the limitations of current passive localization methods and propose an active approach that allows the agent to decide its actions for faster and more accurate localization. The authors propose the \"Active Neural Localizer,\" a neural network model for active localization using raw pixel-based observations and a map of the environment. The model incorporates Bayesian filtering for localization, with components for estimating observation likelihood, belief representation, belief propagation, and policy modeling to localize accurately while minimizing steps. The model is fully differentiable and trained using reinforcement learning, demonstrating effectiveness in various simulation environments. The Active Neural Localizer is capable of generalizing to unseen maps in the same domain and across domains. Localization in mobile autonomous agents involves local and global localization problems. Various methods, including classical, geometry-based, and learning-based visual odometry, address local localization, often with restrictive assumptions about the agent's location. Global localization is a more challenging problem than local localization, as it requires determining the initial position of the agent when it is unknown. This is essential for truly autonomous agents. The proposed method is similar to Markov Localization, which computes a probability distribution over all possible locations. Unlike local approaches, global localization methods can handle localization failures without constant human intervention. Active Markov Localization is a variant of Markov Localization where the agent chooses actions to maximize the reduction in belief entropy. This active approach aims to improve localization accuracy and efficiency, unlike passive methods. The proposed method is faster, more accurate, and capable of learning from data for localization. Recent work has shown success in end-to-end localization using deep learning models, such as a stacked LSTM with auxiliary objectives for navigation tasks in complex 3D environments. The agent successfully navigated complex 3D mazes and accurately predicted its position using hidden states learned by the models. PoseNet BID22 used a deep convolutional network to map monocular images to 3D poses, but required training on each scene. VidLoc BID7 improved pose estimates by using temporal information with a Bidirectional LSTM. Our method utilizes LSTM for smoothing pose estimates over monocular images in a sequence, demonstrating generalization to new maps. Bayesian filtering is used to estimate a dynamic system's state based on observations and actions, aiming to sequentially estimate the belief. Bayesian filtering estimates belief sequentially for localization, using likelihood of observation and state transition function. Belief at time 0 is based on prior knowledge. In active localization, the goal is to estimate the belief of the agent's state using past observations, actions, and map information. The belief is represented by the probability distribution of the agent's state conditioned on the data. In active localization, the goal is to estimate the belief of the agent's state using past observations, actions, and map information. The belief is represented by the probability distribution of the agent's state conditioned on the data. The map is represented by a tensor grid for localization methods, offering advantages over topological representations. The likelihood of an observation in a certain state is also represented by a tensor grid. The proposed model, Active Neural Localizer (ANL), consists of a perceptual model and a policy model. The Belief Map and Likelihood Map are used to estimate the agent's state based on observations. The Belief Map is updated by taking the element-wise dot product with the Likelihood Map at each timestep. The Belief Map is then normalized to calculate the belief after observing the current state. The Belief Map is updated using the transition function after the agent takes an action sampled from the policy model. The transition function updates the belief at each location based on the action taken, similar to the egomotion model. The perceptual model computes feature representation from the agent's observation and states in the map. The feature representation is computed from the agent's observation and map information states. Cosine similarity is used to calculate the likelihood of each state in the map. In 2D environments, the observation is used to create a one-hot vector representing depth. For 3D environments, a deep convolutional network is used to obtain the feature representation of each observation. The feature representation of each observation is obtained using a deep convolutional network. The policy model, trained with reinforcement learning, predicts the next action based on the agent's belief. It utilizes convolutional layers and a fully-connected layer to compute the policy and value function. Losses are calculated using rewards observed by the agent. The agent's observation in different domains includes pixels in front of the agent until the first obstacle in 2D domains and a first-person view image in 3D domains. The agent's state is represented by a tuple of x-coordinate, y-coordinate, and orientation. The map size varies across domains, with a fixed number of possible orientations. Possible actions include 'move forward', 'turn left', and 'turn right' with fixed turn angles. The agent's orientation is always one of the 4 directions: North, South, East, or West. The agent receives an intermediate reward based on the maximum probability of its state. This encourages the agent to reduce the Belief Map entropy for faster localization. An additional reward is given if the prediction matches the true state. Training details and hyper-parameters can be found in Appendix B. Evaluation is based on accuracy (Acc). In the Maze2D environment, maps are represented by a binary matrix with 0 for obstacles and 1 for free space. The agent's observation is a series of pixels in front of it, with the view obscured by obstacles. The agent's orientation is always North, South, East, or West. The observation at any state can be computed using the map design. The observation in 2D environments can be computed using the map design. Bayesian filtering works well in 2D environments due to lack of noise. Experiments evaluate policy learning effectiveness in ideal conditions. Environment size can be varied for scalability testing. In 3D environments, observation is an RGB image of the agent's first-person view. Agent's x-y coordinates are continuous in 3D, unlike discrete grid-based coordinates in 2D. Agent's belief matrix is discretized. In 3D environments, the agent's x-y coordinates are continuous, unlike the discrete grid-based coordinates in 2D. The agent's belief matrix is discretized, and memory images are used for global localization. Maze3D environments are built using the ViZDoom API for interaction with the game engine. The ViZDoom API is used to interact with the game engine for map designs generated using Kruskal's algorithm. The maps have thicker paths than walls, with control over textures for walls, floor, and ceiling to create different 'landmarks'. Unreal3D, a photo-realistic simulation environment, is built using the Unreal Game Engine and AirSim API for interaction. The environment features a modern office space for testing the proposed model's ability to learn perceptual and policy models in 3D environments. The agent learns perceptual and policy models in challenging environments using the ViZDoom and Unreal environments. Markov Localization is used for passive probabilistic localization, with a grid-based state space representation for advantages over topological representations. Active Markov Localization is the active variant of Markov Localization. Active Markov Localization (AML) maximizes the utility of actions chosen by the agent to reduce uncertainty in the agent's state. The algorithm allows for maximizing utility over a sequence of actions rather than just a single action. The AML algorithm maximizes utility by choosing action sequences to reduce uncertainty in the agent's state. The agent does not need to take all actions in the optimal sequence at once, as new observations may affect the utility of remaining actions. The algorithm's greediness is determined by the number of actions taken, and it performs random actions until belief is concentrated on a certain number of states. The computational complexity of the algorithm is high, but with sufficient power, the optimal action sequence can be calculated. The AML algorithm maximizes utility by choosing action sequences to reduce uncertainty in the agent's state. It runs with different values of n l, n g, and n m to optimize performance and runtime. Two versions are defined: Active Markov Localization (Fast) and Active Markov Localization (Slow) based on runtime constraints. The perceptual model for Markov Localization and Active Markov Localization is specified separately for 2D and 3D environments. In the Maze2D environment, models are tested on mazes of different sizes and episode lengths. The proposed Active Neural Localization model outperforms all baselines in accuracy and speed, being 100 times faster than AML (Slow). The proposed Active Neural Localization (ANL) model outperforms all baselines in accuracy and speed, being 100 times faster than AML (Slow). ANL is more efficient and accurate, requiring fewer actions to localize, especially in 3D environments with varying difficulty levels. Landmarks are defined as walls with unique textures in the Maze3D environment. The maze grid has a single wall with a unique texture, while other walls have a common texture, making the map ambiguous. Landmarks are used to aid localization by reducing entropy in the Belief Map. Experiments with different numbers of landmarks show the effectiveness of domain randomization in generalizing to unknown maps. The agent is trained on 40 mazes and tested on unseen mazes with seen or unseen textures. In the Unreal3D environment, the model is tested on adapting to dynamic lighting changes in the Office map with lights on and off. Memory images provided to the agent are taken with lights on, different from experiments on unseen mazes in Maze3D environment. The proposed model outperforms baseline models in Maze3D environments, with a focus on runtime and accuracy trade-offs. A decrease in the look-ahead parameter and an increase in the greediness hyper-parameter were made to ensure runtimes under 24hrs. The ANL model performs better in the Unreal environment due to unique landmarks present. In the Unreal environment, the model struggles with dynamic lighting changes, unlike in the Maze3D environment where it generalizes well to unseen map designs and textures. RGB image-based localization approaches face limitations compared to depth-based approaches due to lighting sensitivity. Belief maps display location probabilities, and true orientation is highlighted for the agent. The belief maps show the agent's orientation and true orientation highlighted by colors. The agent's actions are crucial for localization, as seen when the agent reduces uncertainty by moving forward and turning left. The model's ability to adapt between different simulation environments is tested, showing promising results without fine-tuning. The model is able to generalize well to Unreal environment from the Doom Environment. The policy model generalizes well based on belief and map design, while the perceptual model measures similarity with memory images. A fully-differentiable model for active global localization uses structured components for belief propagation and reinforcement learning for training policy and observation models jointly. The proposed model, trained using reinforcement learning, outperforms baseline models in 2D and 3D environments. It shows potential for transfer to real-world environments but struggles with dynamic lighting adaptation. Possible extensions include combining the model with Neural Map BID32 for an end-to-end SLAM-type system. The architecture can be combined with Neural Map BID32 for end-to-end planning and training in a SLAM-type system. Reinforcement learning involves an agent receiving observations, performing actions, and receiving rewards to learn a policy that maximizes expected return. Policy-based methods directly optimize the policy function, while value-based methods use temporal difference learning for low variance estimates. Actor-Critic methods combine the benefits of both approaches. Actor-Critic methods combine value-based and policy-based approaches by estimating both the value function and policy function. The Advantage Actor-Critic algorithm uses the value function as a baseline to scale the policy gradient. The Asynchronous Advantage Actor-Critic algorithm utilizes deep neural networks to parametrize the policy and value functions. The A3C algorithm utilizes deep neural networks to parametrize policy and value functions, with entropy regularization for exploration. The Generalized Advantage Estimator is used to reduce policy gradient update variance. The perceptual model for 3D Environments processes RGB images with 2 Convolutional Layers and a fully-connected layer, as shown in FIG5. The perceptual model in 3D environments, adapted from previous work, consists of two convolutional layers with specific filter sizes and strides. The policy model in 3D environments includes action history input and the current time step to prevent the agent from getting stuck in repetitive actions. The policy model in 3D environments includes action history input and the current time step to prevent the agent from getting stuck in repetitive actions. Actions and time step are passed through an Embedding Layer to get an embedding of size 8, then concatenated with the output of a fully-connected layer. The resulting vector is passed through branches to get the policy and value function. Models are trained with A3C using Stochastic Gradient Descent, with specific thread numbers and update steps. Weight for entropy regularization, discount factor, and gradient clipping are also specified for reinforcement learning training. The transition function transforms the belief according to the action taken by the agent in the training environment. Beliefs maps are swapped for turn actions and move forward actions move probability values except for collisions. Sample outputs of the transition function are shown in FIG7. The transition function in the training environment transforms beliefs based on agent actions. Techniques like Pre-computation and hashing of expected entropies are used for efficiency in implementing generalized AML. Runtime restrictions determine parameters for AML in 2D and 3D environments. Expected entropies computation requires future state observations, challenging in 3D with RGB images. In the context of training environments, techniques like pre-computation and hashing are used for efficiency in implementing generalized AML. AML assumes a perfect model of the environment and provides future observations by rolling out action sequences in a simulation environment."
}