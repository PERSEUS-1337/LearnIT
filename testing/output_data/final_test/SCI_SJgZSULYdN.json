{
    "title": "SJgZSULYdN",
    "content": "Generative models often rely on human evaluations to assess progress, but current evaluation methods lack standardization and reliability. To address this, the Human-eYe Perceptual Evaluation (HYPE) metric is introduced, grounded in psychophysics research and offering reliable and efficient evaluation of model performance. HYPE includes two methods: HYPE-Time measures visual perception under time constraints to determine the minimum time needed for distinguishing real from fake outputs, while HYPE-Infinity assesses human error rates on fake outputs. The HYPE metric introduces two methods: HYPE-Time measures visual perception under time constraints, while HYPE-Infinity assesses human error rates on fake outputs. Testing HYPE on various generative adversarial networks using different datasets and sampling techniques consistently ranks StyleGAN with truncation trick sampling as superior. Generative models like BID14 BID34 have made likelihood estimation challenging for complex tasks like image and text generation BID46. Evaluating model outputs BID41 BID43 BID11 BID21 BID7 BID37 is now more common, using automatic algorithms or human-derived methods BID41 BID11. Popular face generation tasks use metrics like Inception Score (IS) BID43 and Fr\u00e9chet Inception Distance (FID) BID17, but they are criticized for non-ImageNet datasets BID2 BID40 BID6 BID38 and sensitivity to visual corruptions. Generative modeling challenges often rely on human assessments due to the limitations of automatic metrics. These human measures lack reliability, show high variance, and hinder clear model separability. Theoretically, with large sample sizes, variance could be smoothed out, but this poses challenges for convergence. HYPE (HUMAN EYE PERCEPTUAL EVALUATION) addresses the challenges of generative modeling by measuring perceptual fidelity through a reliable method inspired by psychophysics. It ensures cost and time efficiency through crowdsourcing techniques and offers two evaluation methods, including HYPE time for determining perceptual thresholds. The HYPE time score measures the minimum time needed to distinguish real from fake model outputs. HYPE \u221e simplifies this method by measuring human deception without time constraints. It evaluates GANs for generating human faces and tracks progress over the years on CelebA dataset. StyleGAN, ProGAN, BEGAN, and WGAN-GP are ranked based on perception and error rate on the CelebA dataset. HYPE time measures the time needed to distinguish real from fake outputs, while HYPE \u221e evaluates human deception without time constraints. StyleGAN is tested on the FFHQ dataset with and without the truncation trick to improve generated image quality. The study compared outputs generated with and without the truncation trick in GANs, finding that the trick significantly improved image quality. HYPE scores indicate clear perceptual differences, with room for improvement even in the best-performing model. Results were reproducible with 95% confidence using human evaluators. The study did not focus on diversity, overfitting, entanglement, or training stability. The study did not focus on diversity, overfitting, entanglement, or training stability. Instead, the focus was on constructing a gold standard for human perceptual fidelity using HYPE as a rapid solution for measuring generative models. HYPE can be accessed at https://hype.stanford.edu for a fee of $60, providing reliable scores in just 10 minutes. Future work aims to extend HYPE to other generative tasks like text generation. Model creators can choose between two evaluations: the HYPE time score and the HYPE \u221e score. The HYPE \u221e score assesses people's error rate under no time constraint by displaying images for evaluation. Half are real images from the model's training set, and the other half are from the model's output. The HYPE time method measures time-limited perceptual thresholds using psychophysics literature. The psychometric function method involves flashing images for evaluators to judge as real or fake, adjusting exposure time based on accuracy to determine perceptual thresholds. The HYPE time evaluation method involves displaying images for evaluators to judge, adjusting exposure time based on performance to determine perceptual thresholds. Images are shown for varying durations between 100ms and 1000ms, with noise masks displayed after each image to prevent visual afterimages. Feedback on accuracy is provided after each submission. The evaluation method involves displaying images for evaluators to judge, adjusting exposure time based on performance to determine perceptual thresholds. Blocks start at 500ms and last for 150 images (50% generated, 50% real), using a 3-up/1-down adaptive staircase approach. Multiple blocks are completed by each evaluator, with modal exposure times averaged across blocks for a final value. Higher scores indicate a better model that takes longer exposures to discern from real. The HYPE \u221e method is introduced as a simpler, faster, and cheaper alternative. HYPE \u221e is a faster and cheaper method that measures human deception rate on images, capturing errors on both fake and real images. It requires fewer images than HYPE time to find a stable value, with a 6x reduction in time and cost. Higher scores indicate hyperrealistic images where fake images are indistinguishable from real ones. HYPE \u221e measures human deception rate on images, with scores above 50% suggesting hyperrealistic images. Evaluators are shown 100 images (50 real, 50 fake) to judge incorrectly. To ensure reliability, sufficient model outputs are sampled, suitable real images are selected, and a large number of evaluators are hired. To ensure the reliability of HYPE, a large number of evaluators are needed, treated as a hyperparameter. Evaluators must pass a qualification task by correctly classifying 65% of real and fake images. This pre-task filtering approach is known to outperform post-task strategies. The threshold of 65% can be adjusted based on the GANs used and desired discernment ability of evaluators. Evaluators must pass a qualification task by correctly classifying 65% of real and fake images. They are paid a base rate of $1 for the qualification task and receive a bonus of $0.02 per correctly labeled image thereafter. This payment structure aims to keep evaluators engaged throughout the task. The pay rate for evaluators is $1 for the qualification task and $0.02 per correctly labeled image. Evaluators must sample K=5000 generated and real images for evaluation on two datasets of human faces. The text chunk discusses the training of models on different datasets of human faces, including CelebA-64 and FFHQ-1024, using architectures like StyleGAN, ProGAN, BEGAN, and WGAN-GP. Noise vectors are sampled from a Gaussian noise prior for training. During training and testing, noise vectors are sampled from a Gaussian noise prior. Evaluations were conducted with 360 human evaluators recruited from Amazon Mechanical Turk for tasks involving CelebA-64 and FFHQ-1024 datasets using HYPE time and HYPE \u221e methods. A total of 99,000 responses were recorded for the evaluations. The HYPE evaluation gathered a total of 99,000 responses for CelebA-64 and FFHQ-1024 models. The evaluation included modal perceptual threshold for HYPE time and error rate for HYPE \u221e on real and fake images. Reliability was assessed using bootstrap BID12 simulation to ensure consistent results. The evaluation gathered 99,000 responses for CelebA-64 and FFHQ-1024 models, assessing modal perceptual threshold for HYPE time and error rate for HYPE \u221e on real and fake images. Bootstrapped confidence intervals were calculated by resampling 30 evaluators with replacement across 10,000 iterations to measure variation in outcomes. These CIs do not imply substantial uncertainty, as the reported modal exposure or detection rate remains the best point estimate. The evaluation gathered 99,000 responses for CelebA-64 and FFHQ-1024 models, assessing modal perceptual threshold for HYPE time and error rate for HYPE \u221e on real and fake images. Results show that StyleGAN trunc had the highest HYPE time score at 439.3ms, followed by ProGAN at 363.7ms. BEGAN and WGAN-GP were easily identifiable as fake with a minimum exposure time of 100ms. The evaluation results showed that StyleGAN trunc had a higher exposure time of 363.2ms compared to StyleGAN no-trunc at 240.7ms. Pairwise analysis confirmed separability between all models except BEGAN and WGAN-GP. The evaluation results confirm significant differences between StyleGAN trunc and StyleGAN no-trunc, with higher exposure time for the former. Evaluators were most deceived by StyleGAN trunc images, followed by ProGAN, BEGAN, and WGAN-GP. Real errors increase with fake image errors, making it harder to distinguish between the two distributions. HYPE \u221e ranks StyleGAN trunc higher than StyleGAN no-trunc, with clear separability confirmed by statistical tests. One of HYPE's goals is to be cost and time efficient, balancing accuracy with the number of evaluators in a crowdsourcing task. An experiment with HYPE \u221e on StyleGAN trunc involved 60 evaluators, showing that 30 evaluators are recommended for reliable results. FID is crucial for image generation evaluation. The evaluation methods for unconditional image generation compare HYPE against FID on the same models. Spearman correlation coefficients show FID is not correlated with human judgment measures or HYPE time. HYPE time and HYPE \u221e exhibit a strong correlation. FID scores do not fully correlate with human evaluation scores of HYPE \u221e on CelebA-64. The FID scores do not fully correlate with human evaluation scores of HYPE \u221e on CelebA-64 and FFHQ-1024 tasks. Stimulus timing is used to gauge the perceptual realism of generated images based on cognitive psychology research. Perceptual masks are utilized to eliminate post-processing of stimuli after the desired exposure time. The texture-synthesis algorithm BID35 is used to establish lower bounds on exposure time, time between images, and noise mask usage. Generative modeling tasks like image generation, machine translation, image captioning, and abstract summarization rely on metrics like Inception Score (IS), Fr\u00e9chet Inception Distance (FID), BLEU, CIDEr, and METEOR scores for evaluation. Automatic metrics also measure diversity, overfitting, entanglement, training stability, and efficiency. The evaluation metric may capture output diversity detectable by human evaluators. The evaluation metric FID BID17 and IS BID43 rely on the Inception v3 Network BID45 to calculate statistics on generated output and distributions. However, the validity of these metrics on different datasets has been questioned due to imperceptible perturbations affecting their values. FID also computes differences between real and generated distributions, leading to inherent variance in the metric. The cost of computation correlates with accuracy in improving FID scores. Human evaluations have been used to assess GAN outputs, with generated MNIST samples reaching human performance saturation. The study found a 21.3% error rate on CIFAR-10 using model BID43, indicating varying complexity levels across datasets. Comparing models using a tournament of discriminators, the design could impact human thresholds, requiring more humans for evaluation. The community should focus on challenging categories, especially in text generation, where adjustments to perceptual time thresholds may be necessary. Future work includes extending HYPE to different imaging datasets and tasks, such as conditional image generation, text translation, and video captioning. There is a plan to explore budget-optimal estimation of HYPE scores, adaptive evaluation of evaluator quality, and identifying images that require more evaluators. Additionally, efforts will be made to implement faster time exposures under 100ms, down to 13ms, for tasks requiring finer granularity. This will involve careful engineering solutions to address limitations imposed by JavaScript paint and rendering times on modern browsers. The ecological validity of these methods will also be investigated. HYPE provides researchers with two human evaluation methods for GANs that are grounded in psychophysics literature. The reliability of HYPE may be impacted by the time of year it is run, but anecdotally, it has been found to be reliable regardless of the time of day. HYPE offers two human evaluation methods for GANs based on psychophysics, providing consistent results and distinguishing between model performances. Two metrics, HYPE time and HYPE \u221e, measure perceptual fidelity and error rates under different time constraints. The approach is demonstrated on unconditional image generation with various GANs and datasets. The evaluation system HYPE offers two human evaluation methods for GANs, measuring perceptual fidelity and error rates. It includes output sampling on StyleGAN with and without the truncation trick, and provides confidence intervals. The system can be accessed at https://hype.stanford.edu for easy model evaluation."
}