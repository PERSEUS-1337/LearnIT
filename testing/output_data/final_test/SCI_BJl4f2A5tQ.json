{
    "title": "BJl4f2A5tQ",
    "content": "In this paper, the authors propose a new algorithm called generative adversarial tree search (GATS) that combines Monte-Carlo Tree Search (MCTS) with Pix2Pix GANs to simulate rollouts in deep reinforcement learning. GATS uses a GAN-based dynamics model and a reward predictor to simulate rollouts up to a specified depth, and then employs MCTS for planning and DQN to estimate the Q-function at the leaf states. Theoretical analysis of GATS shows favorable properties in bias-variance trade-off. Empirical results demonstrate quick convergence to accurate solutions on 5 Atari games. However, GATS underperforms compared to DQNs in 4 out of 5 games. Short rollouts in MCTS may explain this failure despite perfect modeling. Deep reinforcement learning (DRL) combined with MCTS has been successful in Atari games and Go. Alpha Go employs depth-limited MCTS with an estimated Q-function to speed up planning in long episodes like Go. In real-world applications like robotics and dialogue systems, collecting samples is time-consuming. Generative adversarial networks (GANs) are used for synthesizing realistic data. Generative adversarial networks (GANs) have become popular for synthesizing realistic data, especially for high-dimensional domains like images and audio. Unlike previous methods, GANs produce crisp images and have been extended for conditional generation tasks. A new DRL algorithm called generative adversarial tree search (GATS) is proposed in this work, which learns a Q-function approximator, a near-term reward predictor, and a GAN-based model of the environment's dynamics using samples from the environment. Generative adversarial tree search (GATS) introduces a learned simulator using a GAN-based model of the environment's dynamics and a reward predictor for MCTS. GATS leverages PIX2PIX GANs to efficiently learn the environment's dynamics, producing visually crisp images. The GDM converges quickly and is robust to distribution shifts, bridging model-based and model-free reinforcement learning. The GATS algorithm combines MCTS planning with DQN to estimate Q-function values for leaf states. It is flexible with modular building blocks for value learning, planning, reward prediction, and dynamics modeling. GATS serves as a general framework for exploring the balance between model-based and model-free reinforcement learning. GATS combines MCTS planning with DQN to estimate Q-function values for leaf states, analyzing error components and bias-variance trade-offs. It reduces biases in Q estimation, leading to a sample complexity reduction by a factor of 2 on Pong. A heuristic optimism-based strategy using GDM is developed, with a focus on bias-variance analysis for different planning and exploration strategies. In this work, a new OpenAI gym BID11-like interface was developed for the Atari Learning Environment (ALE), supporting different modes and difficulties for Atari games. The study focuses on the sample complexity required by GDM and RP to adapt and transfer between different game domains. Despite efficient convergence and high accuracy of learning environment models, the return of the learned policy could not be improved using short rollouts on games other than Pong. The negative result persisted across different games. Despite extensive study and various strategies, the GATS model failed to perform well with short rollouts on games other than Pong. The hypothesis suggests that the Q-learner may not learn from its mistakes due to the training regime. Testing this hypothesis would require longer rollouts, which could be computationally expensive. Previous successes with MCTS have involved tree depths in the hundreds, indicating the challenge of achieving good performance with short rollouts. The study highlights the challenges of using MCTS with deep tree depths in reinforcement learning. Despite not advancing the leaderboard, it provides insights for future research on combining model-based and model-free RL methods. The study discusses an infinite horizon \u03b3-discounted MDP and the agent's objective. The agent's objective in reinforcement learning is to maximize the overall expected discounted reward by finding a policy that maximizes the cumulative discounted reward. Value-based RL aims to learn the Q-function to derive the optimal policy. To minimize the Bellman residual and avoid double sampling costs, a common approach is to use DQN with a separate function approximator for computing the target value. In reinforcement learning, addressing bias in computing the target value is crucial. GATS can help mitigate biases caused by network capacity, optimization algorithms, and model mismatch. A generative dynamic model (GDM) with a generator G and discriminator D is proposed, trained adversarially using the extended conditional Wasserstein metric. The objective is to minimize the distance between probability measures P and P G conditioned on a third measure P. In GDM, D minimizes the interior sup distance while G aims to learn the distribution over pairs of (x, a) in GATS. The bias in the DQN objective function is discussed, showing biases in practice. The bias vanishes in deterministic domains, and the expected return using GDM, RP, and estimated Q is calculated. The GATS method efficiently estimates the expected return without interacting with the real environment by using the estimated Q-function and given GDM and RP. The deviation in estimating the expected return is bounded when using the DQN procedure with the estimated model of the environment. The error in Q estimation decreases exponentially, but variances in model estimation increase. The agent can choose the depth of roll-out, H, to balance these factors. Generative Adversarial Tree Search (GATS) Alg. 1 is based on DQN/DDQN and learns a reward model RP, model dynamics GDM, and Q-function from replay buffer experiences. GATS uses bounded-depth MCTS on the learned model (GDM and RP) for planning, estimating maximum expected return at leaf nodes using the learned Q-function. A novel deep neural network, GDM, parameterized by \u03b8 GDM, is proposed for learning model dynamics, showing sample-efficient and strong predictive performance. The Generative Adversarial Tree Search (GATS) algorithm is based on DQN/DDQN and involves learning a reward model RP, model dynamics GDM, and Q-function from replay buffer experiences. GATS utilizes bounded-depth MCTS for planning and introduces a new exploration method based on Wasserstein distance. The Wasserstein distance helps distinguish between familiar and unfamiliar experiences, guiding the generator's learning process. The algorithm GATS is based on DQN/DDQN and involves learning a reward model, model dynamics, and Q-function from replay buffer experiences. It utilizes bounded-depth MCTS for planning and introduces an exploration method based on Wasserstein distance. The pseudo-count is used to compute an optimistic Q-function, encouraging the agent to explore. The GATS algorithm utilizes a reward model, model dynamics, and Q-function from replay buffer experiences. It incorporates bounded-depth MCTS for planning and introduces exploration based on Wasserstein distance. The approach encourages exploration in less visited parts of the state space for better performance on Atari games. The GDM is trained using prioritized weighted mini-batches and updates every 16 decision steps of GATS. It adapts quickly to changes in policy or domain mode, generalizes well, and outperforms DQN with fewer samples. Various model architectures and loss functions were experimented with to develop the GDM. The PatchGAN discriminator and ACVP architecture were experimented with for learning game dynamics in Pong. PatchGAN took 10\u00d7 more training iterations than the current GDM, while ACVP required an order of magnitude more iterations. This is likely due to the need for the entire frame in learning game dynamics, which PatchGAN lacks. The original GAN-loss BID15 based on Jensen-Shannon distance was unstable for non-stationary domains like Pong. Wasserstein metric with W-GAN was deployed for GDM, but still lacked fast and stable convergence required for RL tasks. Gradient clipping was used to make the discriminator a bounded Lipschitz function. After trying improved-W-GAN BID16 with gradient penalty for stability, spectral normalization was implemented for GDM. This technique provided high-quality frames, fast convergence, and stability in the loss function, allowing GDM to handle state distribution changes in RL while preserving frame quality. Further detailed study is in the Appendix. FIG1 demonstrates GDM's efficiency in generating accurate next frames based on previous frames and trajectory. The GDM can generate next frames based on previous frames and actions. It is trained on a replay buffer of 100,000 frames with a 3-step loss and evaluated on 8-step roll-outs. The learned Q function shows small deviation, allowing for re-use of generated data to train the Q-function. Strategies for sampling fresher experiences from the replay buffer are crucial due to the quick adaptation of GDM and RP modes. Ways to incorporate generated samples in training the Q-function are explored, similar to Dyna-Q. In a study on model dynamics in gaming, the DDQN agent unexpectedly performed poorly when the game mode was changed to make it easier. It took 3M time steps to master the game again, compared to 5M from scratch. GDM and RP adapted quickly in 3k samples. A Gym-style wrapper was created for ALE to support different difficulty levels. The exploration-exploitation trade-off in RL literature was extensively studied. The exploration-exploitation trade-off in RL literature is extensively studied. Regret analysis of MDPs is investigated, applying the Optimism in the Face of Uncertainty principle to guarantee a high probability regret upper bound. For Partially Observable MDPs, OFU provides a high probability regret upper bound. GDM generates sequences of future frames almost identical to real frames in Atari games. Theoretical RL addresses trade-offs in exploration-exploitation, which is still prominent in practical settings. Recent research in reinforcement learning has focused on exploring efficient exploration strategies to combat sample complexity issues. Methods such as optimism and Thompson Sampling have been proposed to address the exploration-exploitation trade-off. Value-based deep reinforcement learning methods aim to minimize the Bellman residual, although this approach may lead to biased estimations of the value function. DQN proposes updating the target value less frequently to reduce bias in the value estimator, but this increases sample complexity. Monte Carlo sampling strategies are efficient for planning but have high sample complexity. Despite GANs' ability to generate realistic images, they are difficult to train and unstable for non-stationary tasks like RL. Progress has been made in developing stable learning procedures like Wasserstein GAN. The improved W-GAN penalizes the discriminator's gradient to ensure boundedness, while spectral normalization of discriminators has shown to converge reliably. These advances are leveraged for stable learning in the GDM. Conditional video prediction is a growing research area, with previous work using large models with L2 loss to predict future frames given actions. However, these models struggle with high frequency details and meaningful frame generation. The implementation of a new model compared to GDM shows faster convergence and lower error rates when using a Q function on generated frames. Weber et al. (2017) utilize learned environment models for conditional video prediction, demonstrating their effectiveness on Sokoban and miniPacman tasks without explicit planning strategies like GATS. Weber et al. (2017) does not use explicit planning strategies like GATS. They employ transition models for rollouts in the encoded state representation, showing gains on Atari games. GATS offers a flexible framework for model-based and model-free reinforcement learning with value learning, planning, reward prediction, and dynamics model components. This framework allows for various adaptations and exploration in different domains and problems. The text discusses various methods for value learning, planning, reward prediction, and dynamics model components in reinforcement learning. It mentions using Count-based methods, UCT, policy gradient methods, regression models, and image generating models. The GATS framework is highlighted for its flexibility but noted for high computation costs, suggesting potential solutions like parallelization or distilled policy methods. In this section, hypotheses for GATS under-performance compared to DQN are discussed, along with attempts to improve GATS. Techniques like storing samples from tree search to train the Q-model and using different optimizers were explored but did not enhance GATS performance. In the Dyna-Q setting with generated frames, various learning rates and minibatch sizes were tested to tune the Q-learner. Different methods were considered to utilize samples from tree search for learning in Dyna-Q, including using generated experience at leaf nodes in the replay buffer, sampling additional experience from the tree, and training Q on its own decisions. In the Dyna-Q setting, different methods were tested to utilize samples from tree search for learning, including storing experience from greedy and \u03b5-greedy policies in the later part of the tree. Optimism-based exploration strategy with the GDM was used to encourage exploration by adding W-loss and its exponent as extrinsic rewards. Despite an extensive study, the benefits of GATS were not demonstrated. Despite an extensive study on GATS, it was not able to show significant benefits besides a limited improvement in training speed on Pong. A hypothesis is proposed for the negative results, suggesting that the GATS algorithm may not boost performance even with perfect modeling, due to factors like local bias. The example scenario illustrates how the GATS algorithm with a 2-step look-ahead may not effectively guide the agent to avoid negative rewards, even with a true model simulator. The MCTS roll-out with GATS action locally avoids bad states, but struggles to globally propagate this information. With \u03b5-greedy exploration, the agent eventually receives a negative reward for choosing action down, much later than a DQN agent. This delay in feedback slows down learning that action down is sub-optimal, even with future event prediction for further learning. GATS with depth of 10 (GATS-10) yields the highest return in a 10x10 grid world. GATS can locally help avoid catastrophic events but may slow down overall learning. Using MCTS experience in replay buffer with Dyna-Q can address this issue. However, in certain scenarios, Dyna-Q does not accelerate learning and requires more computation. In certain scenarios, Dyna-Q does not speed up learning and requires more computation. Deploying MCTS with Q-learning can have complex interactions, leading to potential performance issues. Empirical testing in a controlled environment using the Goldfish and gold bucket experiment showed that using a worse estimated Q-function in leaf nodes with MCTS can result in decreased performance. In testing different algorithms like GATS and GATS + Dyna-Q with varying depths, GATS with a depth of 10 showed the highest return. GATS with nonzero depth helped prevent the agent from hitting sharks initially, but shorter roll-outs degraded performance in the long run. Additionally, Dyna-Q approach also had potential performance issues. In testing different algorithms like GATS and GATS + Dyna-Q with varying depths, GATS with a depth of 10 showed the highest return. However, GATS + Dyna-Q did not provide much benefit over GATS and could cause overfitting without sophisticated sampling algorithms. For complex applications like hard Atari games, GATS may require longer roll-out depths with Dyna-Q to perform well, which was computationally infeasible for this study. The insights in designing near-perfect modeling algorithms and the extensive study of GATS highlight key considerations for combining model-based and model-free reinforcement learning algorithms. The text discusses reinforcement learning and the estimation of returns using a learned model. It introduces Lemma 1 regarding the deviation in the Q-function and provides a proof for it. The text then explains the application of addition and subtraction techniques to upper bound errors in the estimation process. The text discusses reinforcement learning and the estimation of returns using a learned model. It introduces Lemma 1 regarding the deviation in the Q-function and provides a proof for it. The text then explains the application of addition and subtraction techniques to upper bound errors in the estimation process. In Eq. 10, the error term is derived from the reward term, and the remaining terms are expanded for subsequent time steps to derive the full bound. The sequence of decision states and corresponding learned Q-function by DQN are shown, where the agent loses a point due to suboptimal actions. In the context of reinforcement learning and Q-function estimation, the text explores the deviation in Q-values for different actions. It discusses scenarios where suboptimal actions lead to negative consequences, emphasizing the importance of accurate Q-value estimation for decision-making. The text discusses the importance of accurate Q-value estimation in decision-making, particularly in earlier stages of Q learning. It compares the performance of GATS with different lookahead steps to DQN, showing improvement in RP prediction accuracy. The RP model quickly adapts to shifts in positive rewards, reducing classification errors. DRL methods are data hungry, but re-using data can help efficiently learn model dynamics. The GDM accurately generates future frames based on the first frame and action trajectory. In a study comparing GATS with different lookahead steps to DQN, the RP model quickly adapts to shifts in positive rewards, reducing errors. DDQN showed vulnerability and broke when the game mode changed, requiring fine-tuning to master it again. GDM and RP adapted to new model dynamics in 3k samples, showing detailed design benefits. In experiments with GATS +DDQN, one-step roll-outs were found to be more beneficial for exploration. The Wasserstein-optimism approach improved exploration compared to \u03b5-greedy approaches. Prioritizing fresher training samples for the GDM led to better initial performance but lagged behind DDQN due to state distribution shifts. In experiments with GATS +DDQN, one-step roll-outs were found to be more beneficial for exploration. The optimism approach for GATS improves sample complexity and learns a better policy faster, but extending it to more games was hindered by high computation costs and hyperparameter tuning challenges. GATS improves sample complexity and learns a better policy faster. Results for GATS-1 compared to DDQN show challenges in learning strong global policies with short roll-outs. Hyperparameter tuning specific to each game is crucial for performance improvement. The GDM model consists of seven convolution and deconvolution layers with Batch Normalization and activation functions. The encoder part uses channel dimensions and kernel sizes, while the decoder follows a similar pattern. The GDM model consists of convolution and deconvolution layers with Batch Normalization and activation functions. The generator is trained using Adam optimizer with weight decay and learning rate, while the discriminator uses SGD optimizer with a smaller learning rate and momentum. The training involves using Wasserstein metric for GDM training and includes generator and discriminator gradient updates. The discriminator in the GDM model uses SN-convolution to ensure a Lipschitz constant below 1. It consists of four SN-convolution layers with Batch Normalization and leaky RELU activation. The output is a single number without non-linearity, and the action sequence is represented using one hot encoding. The generator and discriminator in the GDM model are trained using different optimizers and parameters. The generator uses Adam optimizer with specific settings, while the discriminator uses SGD optimizer with different settings. The gradient updates for both generator and discriminator are based on the Wasserstein metric. Additionally, to enhance the quality of generated frames, multiple losses are added to capture various frequency aspects. In order to improve the training process, 10 * L1 + 90 * L2 loss is added to the GAN loss. The losses are defined on frames with pixel values in [\u22121, 1] to speed up learning. The generator is also trained using self-generated samples to preserve GDM quality and allow for longer roll-outs. This involves training the generator and discriminator on generated samples for a depth of three, enabling a horizon of more than 10 while maintaining accuracy. The text discusses training a Q-network to assign values to generated frames in a GAN model. To compensate for errors, another Q-network is trained to provide similar Q-values. The training process involves minimizing the L2 norm between the Q-values for real and generated frames using Adam optimizer. Weight decay and L1 loss optimizations were found to degrade network performance. The GDM's ability for domain adaptation was evaluated using the Arcade Learning Environment. Training was done on Pong with Difficulty 0 and Mode 0, then transferred to Pong with Difficulty 1 and Mode 1. L1 and L2 loss decreased over 3,000 training iterations. The GDM quickly learns new model dynamics and generalizes well with fewer samples than the Q-learner. L1 and L2 loss are not good measures of capturing game dynamics. GDM can generate different future trajectories from an initial state efficiently. The GDM efficiently generates different future trajectories from an initial state with 8 different 5-action length sequences."
}