{
    "title": "B1J_rgWRW",
    "content": "In this paper, the authors investigate functions representable by deep neural networks (DNN) with rectified linear units (ReLU). They provide an algorithm to train a ReLU DNN with one hidden layer to global optimality with polynomial runtime in data size but exponential in input dimension. The authors also improve lower bounds on approximating a ReLU deep net function by a shallower ReLU net. Their gap theorems apply to smoothly parametrized families of \"hard\" functions, showing that for every natural number k, there exists a function representable by a ReLU DNN with k^2 hidden layers and total size k^3, requiring at least 1/2k^(k+1)-1 total nodes for any ReLU DNN with at most k hidden layers. Additionally, they demonstrate new results for the family of DNNs with ReLU activations. Deep neural networks (DNNs) with ReLU activations can represent any continuous function on a compact subset of R^n well. A new lowerbound on the number of affine pieces for this family of DNNs is shown, utilizing zonotopes from polyhedral theory. The universal approximation result for neural networks was first given by Cybenko in 1989 for sigmoidal activation functions and later generalized by Hornik. Additionally, neural networks have finite VC dimension. Neural networks with ReLU activations can represent any continuous function well. DNNs have finite VC dimension and are PAC learnable. However, they were computationally hard to learn in the late 90s. To address this, alternate deep architectures like CNNs, deep belief networks, and deep Boltzmann machines were introduced. New non-saturating activation functions like ReLU have made deep architectures more trainable. In this paper, the focus is on studying deep neural networks with rectified linear units (ReLU). The investigation is inspired by recent attempts to understand the success of deep learning and the non-convex nature of training DNNs. The function space represented by ReLU DNNs is also explored, drawing inspiration from classical circuit complexity theory. Gap results are inspired by previous studies showing a strict separation of complexity. The paper focuses on studying deep neural networks with ReLU activation, extending the function space to vectors in R^n. A ReLU DNN is defined by specifying hidden layer widths, affine transformations, and linear transformations. The investigation is inspired by understanding the success of deep learning and non-convex training nature, with results showing a strict separation of complexity classes. A ReLU DNN with k hidden layers computes a function from R^n1 to R^n2 using function composition. The depth of the network is k + 1, width is max{w1, ..., wk}, and size is w1 + w2 + ... + wk. Piecewise linear functions are defined as affine linear over finite polyhedra in R^n, ensuring continuity. In this section, the text discusses the representation of functions by ReLU DNNs, emphasizing their ability to represent a wide range of functions with few parameters. It also explores how the depth and width of ReLU DNNs impact their expressive power, highlighting that any function represented by a ReLU DNN is a continuous piecewise linear function. The text further establishes a one-to-one correspondence between ReLU DNNs and piecewise linear functions. Theorem 2.1 establishes a correspondence between ReLU DNNs and piecewise linear functions. Every ReLU DNN represents a piecewise linear function, and every piecewise linear function can be represented by a ReLU DNN with at most log 2 (n + 1) + 1 depth. This is proven by showing that any PWL function can be represented as a linear combination of piecewise linear convex functions. Theorem 2.1 establishes a correspondence between ReLU DNNs and piecewise linear functions, showing that any PWL function can be represented by a ReLU DNN with at most log 2 (n + 1) + 1 depth. Theorem 2.2 provides tight bounds on the size of 2-layer DNNs needed to represent a given piecewise linear function, with a minimum size of p - 1 nodes. Theorem 2.1 establishes a connection between ReLU DNNs and piecewise linear functions, showing that any PWL function can be represented by a ReLU DNN with at most log 2 (n + 1) hidden layers. The family of compactly supported continuous functions is dense in the space of Lebesgue integrable functions, which can be well-approximated by a ReLU DNN. For n = 1, any L q function can be approximated by a 2-layer DNN with tight bounds on its size. The authors of BID9 and BID11 have established upper bounds for ReLU net representation of positive PWL functions. The depth of networks in deep learning is crucial for extracting hierarchical features from data, unlike traditional machine learning frameworks. In this section, the importance of depth in ReLU DNNs is explored. A parametrized family of \"hard\" functions representable by ReLU DNNs is provided, showing the need for exponentially larger size in shallower networks. Additionally, a continuum of \"hard\" functions representable by ReLU DNNs is constructed, with the first explicit construction of functions whose affine pieces grow exponentially with input dimension. The focus is on R \u2192 R ReLU DNNs, with the depth-size trade-off highlighted in this setting. Theorem 3.1 demonstrates the depth-size trade-off in ReLU DNNs, showing that for every pair of natural numbers k \u2265 1, w \u2265 2, there exists a family of hard functions representable by a (k + 1)-layer ReLU DNN of width w. The intricate structure of this family is further described in Theorem 3.2, stating that every member has w^k pieces and can be parametrized uniquely. Corollary 3.3 follows from Theorem 3.1, indicating the existence of a family of functions defined on the real line with specific properties. The family of functions on the real line can be represented by a (k + 1)-layer DNN with size k^2, and a k + 1-layer DNN representing f must have size at least DISPLAYFORM2. A special case is when k is a natural number, where a family of functions can be represented by a k^2 + 1-layer DNN with k^3 nodes. Theorem 3.5 shows that for every k \u2265 1, w \u2265 2, there exists a function f_k,w representable by a (k + 1)-layer ReLU DNN with w nodes in each layer. Theorems 3.1 and 3.5 extend Telgarsky's results on the depth-size trade-off in ReLU DNNs. The size lower bound for approximation in the 1-norm scales exponentially with depth, surpassing Telgarsky's bounds. Additionally, Telgarsky's hard function family is parameterized by a single number k, while we show results for every pair of functions. In contrast to Telgarsky's results on depth-size trade-off in ReLU DNNs, we demonstrate that a \"hard\" function requires a size of at least wkkk to be represented by a depth k network. Our findings show gaps in representation ability with super-exponential size lower bounds in depth. Additionally, we introduce a smoothly parameterized family of \"hard\" functions, unlike the countable family of functions typically seen in Boolean circuit complexity. Telgarsky's results in (Telgarsky, 2016) are more general than our results, but with weaker gap guarantees. Eldan-Shamir BID36 BID7 show that there exists an R n \u2192 R function that can be represented by a 3-layer DNN, that takes exponential in n number of nodes to be approximated to within some constant by a 2-layer DNN. Their results are not immediately comparable with Telgarsky's or our results, but it is an interesting open question to extend their results to a constant depth hierarchy statement. Telgarsky's results in (Telgarsky, 2016) are more general than the recent result of Rossman et al BID28. Efforts have been made to show size lowerbounds on ReLU DNNs approximating functions not exactly representable by them. The complexity of a family of \"hard\" functions represented by ReLU DNNs is measured by the number of pieces as a function of dimension, depth, and size of the networks. Telgarsky's results in (Telgarsky, 2016) are more general than recent findings. The complexity of \"hard\" functions represented by ReLU DNNs is measured by the number of pieces as a function of dimension, depth, and network size. The measure comp H (n, k, w) determines the maximum number of pieces of a function from H that can be represented by a ReLU DNN with depth at most k + 1 and maximum width at most w. Various constructions achieve this, with improvements discussed in subsequent sections. Telgarsky's results in (Telgarsky, 2016) show the complexity of functions represented by ReLU DNNs. The function can be seen as a composition of H a 1 ,a 2 with 1-norm. The set of vertices of zonotopes is well-known in the theory. The extremal zonotope set S(n, m) is a subset of R nm with zero Lebesgue measure. Lemma 3.7 states that for any b 1 , . . . , b m \u2208 R n , there exists a 2-layer ReLU DNN representing the function \u03b3 Z(b 1 ,...,b m ) (r). Definition 8 defines a function h a : R \u2192 R which is piecewise linear. Telgarsky's results in (Telgarsky, 2016) demonstrate the complexity of functions represented by ReLU DNNs. The extremal zonotope set S(n, m) is a subset of R nm with zero Lebesgue measure. Proposition 3.8 shows that any tuple in S(n, m) can be represented by a k + 2 layer ReLU DNN. The main result, Theorem 3.9, introduces the family of functions ZONOTOPE n k,w,m with specific properties regarding their representation by ReLU DNNs. Comparing to results in BID24, ZONOTOPE n k,w,m does not require hidden layers to have width at least as big as the input dimensionality n. Our construction of networks with bottleneck architectures allows for independent network size regardless of input dimensionality. Our complexity measure shows advantages in certain regimes, such as when n \u2264 w < 2n and k \u2208 \u2126( n log(n) ). Unlike the construction in BID24, we have a smoothly parameterized family of functions corresponding to a higher-dimensional torus. The empirical risk minimization problem involves finding a function represented by 2-layer ReLU DNNs that minimizes a convex loss function. A new algorithm is proposed to solve this problem to global optimality by searching over the space of functions representable by 2-layer DNNs. The algorithm proposed solves the empirical risk minimization problem using 2-layer DNNs, guaranteeing global optimality by breaking it into a combinatorial search and a convex problem. The network output is represented by a matrix and vectors, allowing for efficient optimization. The algorithm implements empirical risk minimization for training ReLU DNN with one hidden layer, ensuring global optimality. It runs in polynomial time and addresses the complexity gap in deep net training. The algorithm addresses the complexity gap in deep net training by returning a ReLU DNN from the class being learned. The running time is exponential in input dimension and number of hidden nodes, but there is potential for a polynomial time algorithm for global optimality. In the context of addressing the complexity gap in deep net training, resolving the dependence on network size is a key step towards understanding the theoretical complexity of training ReLU DNNs. Future research could focus on optimal training algorithms for DNNs with multiple hidden layers, which presents a more challenging task. Additionally, achieving gap results between different depths would be a significant breakthrough. Special thanks are given to Christian Tjandraatmadja for pointing out errors in previous versions of the paper, and to others for discussions on circuit complexity. The paper discusses the influence of discussions on Boolean and arithmetic circuit complexity. It acknowledges support from NSF grants and presents a proof related to piecewise linear functions. The functions can be implemented using ReLU DNNs with specific sizes. The paper explores the implementation of piecewise linear functions using ReLU DNNs with specific sizes. It discusses decomposing functions into a sum of m functions and arranging breakpoints accordingly. Adding a constant to a function does not affect the complexity of the ReLU DNN expressing it. The paper discusses decomposing functions into a sum of m functions with specific slopes and breakpoints. The existence of a solution to a set of simultaneous linear equations is explored to find values for the slopes. The paper explores decomposing functions into a sum of m functions with specific slopes and breakpoints. The lower bound on the size of a 2-layer ReLU DNN expressing a p piece function is discussed. When the rightmost or leftmost piece of a piecewise linear function has a 0 slope, a 2-layer DNN of size p-1 can compute the function. This is useful for constructing hard functions. The paper discusses decomposing functions into piecewise linear functions with specific slopes and breakpoints. It shows that composing piecewise linear functions results in a function with at most (p + 1) k + 2 pieces. Each piece in the range [0, M] is affine with minimum value 0 and maximum value M. The functions can be represented by 2-layer ReLU DNNs with specific sizes. The paper discusses decomposing functions into piecewise linear functions with specific slopes and breakpoints, showing that composing these functions results in a function with at most (p + 1) k + 2 pieces. Each piece is affine within the range [0, M]. The functions can be represented by 2-layer ReLU DNNs with specific sizes. Using Lemma D.1, a k + 1 layer DNN with size w can represent H a 1 ,...,a k, where each hidden layer has w nodes. In any triangle of s q, an affine function will incur an error of at least 1. The problem requires finding affine and linear transformations to minimize empirical loss. The paper discusses decomposing functions into piecewise linear functions with specific slopes and breakpoints, showing that composing these functions results in a function with at most (p + 1) k + 2 pieces. Each piece is affine within the range [0, M]. The functions can be represented by 2-layer ReLU DNNs with specific sizes. The family of functions being searched is of the form where i \u2208 R n, b i \u2208 R, and s i \u2208 {\u22121, +1}. For a given data point (x j, y j), if \u00e3 i \u00b7 x j + b i \u2264 0, then the i-th term does not contribute to the loss function. Sets S j can be defined for each data point, inducing a partition of the data set into two parts. The paper discusses decomposing functions into piecewise linear functions with specific slopes and breakpoints, showing that composing these functions results in a function with at most (p + 1) k + 2 pieces. Each piece is affine within the range [0, M]. The functions can be represented by 2-layer ReLU DNNs with specific sizes. The family of functions being searched is of the form where i \u2208 R n, b i \u2208 R, and s i \u2208 {\u22121, +1}. For a given data point (x j, y j), if \u00e3 i \u00b7 x j + b i \u2264 0, then the i-th term does not contribute to the loss function. Sets S j can be defined for each data point, inducing a partition of the data set into two parts. Our strategy involves guessing partitions P i + and P i \u2212 for a fixed selection, and solving a convex optimization problem with decision variables \u00e3 i \u2208 R n, b i \u2208 R for i = 1, . . . , w. The objective is to minimize a convex function subject to linear inequality constraints. The paper discusses decomposing functions into piecewise linear functions with specific slopes and breakpoints, showing that composing these functions results in a function with at most (p + 1) k + 2 pieces. Each piece is affine within the range [0, M]. The optimization problem involves finding a piecewise linear function that minimizes total loss, which is equivalent to linear regression with multiple affine linear pieces. The algorithm guesses the optimal partition of data points to minimize loss. The paper discusses decomposing functions into piecewise linear functions with specific slopes and breakpoints to minimize total loss through linear regression with multiple affine linear pieces. The algorithm optimizes the partition of data points to achieve this. The paper discusses decomposing functions into piecewise linear functions with specific slopes and breakpoints to minimize total loss through linear regression with multiple affine linear pieces. The algorithm optimizes the partition of data points to achieve this by imposing linear inequalities on the parameters of the piecewise linear functions. The paper discusses decomposing functions into piecewise linear functions to minimize total loss through linear regression. The algorithm optimizes data point partition by imposing linear constraints on the parameters. The operations preserving representability by a ReLU DNN are discussed, along with proofs by induction on the depth and widths of hidden layers. The paper discusses decomposing functions into piecewise linear functions to minimize total loss through linear regression. The algorithm optimizes data point partition by imposing linear constraints on the parameters. The operations preserving representability by a ReLU DNN are discussed, along with proofs by induction on the depth and widths of hidden layers. In the context of ReLU DNN with depth k + 2 and widths w 1 , . . . , w k+1 of the k + 1 hidden layers, it is shown that the output of a node in the last layer can have at most 2 k \u00b7(w 1 +1)\u00b7w 2 \u00b7. . .\u00b7w k \u00b7w k+1 pieces after applying the activation function. The paper discusses decomposing functions into piecewise linear functions to minimize total loss through linear regression. Lemma D.6 states that a piecewise linear function with p pieces represented by a ReLU DNN of depth k + 1 must have a size of at least 1/2kp^(1/k) - 1. Conversely, a piecewise linear function represented by a ReLU DNN of depth k + 1 and size s can have at most (2s/k)^k pieces."
}