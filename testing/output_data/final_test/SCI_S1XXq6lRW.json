{
    "title": "S1XXq6lRW",
    "content": "In this paper, a method for sharing label information across languages using a language independent text encoder is proposed. The encoder allows for transferring label information from one language to another, enabling the training of a classifier that works for multiple languages. The encoder is trained independently of specific classification tasks, making it versatile for various classification tasks. Good performance can be achieved even without a specific labeled dataset in the target language. In this paper, a method is proposed for sharing label information across languages using a language-independent text encoder. The encoder allows for transferring label information from one language to another, enabling the training of a classifier that works for multiple languages. Automatic systems for document classification are valuable for practical applications, such as sentiment analysis of opinion posts like tweets mentioning products and services. Organizations can use sentiment analysis to improve products based on feedback. Training a classification model for a language without a suitable dataset involves creating a new labeled dataset or using a comparable corpus of texts. The method presented in this article involves using a universal encoder for cross-language text classification. The encoder is trained to provide similar representations for texts on the same topic in different languages. A classification module then uses these language-independent representations to predict document categories, offering advantages over previous methods. The method presented in this article involves a universal encoder for cross-language text classification, similar to Google's zero-shot machine translation model. This model requires a large corpus of aligned sentences for training and is more complex than necessary. The article is organized with the CLTC model in section 2 and experiments in section 3. The CLTC model, similar to Google's zero-shot machine translation model, consists of a universal encoder and a classifier module. The universal encoder provides a language-independent encoding of text, allowing for cross-lingual classification. The encoder transforms input text using a function F u, which could include word embeddings or recurrent layers. This approach aims to enable classifiers to be usable across all languages. The CLTC model includes a universal encoder that provides a language-independent encoding of text for cross-lingual classification. The encoder is trained to predict if two texts are comparable based on their representations, allowing for a universal classifier that can classify text in any language. The universal classifier, part of the CLTC model, can classify text in any language. Comparisons are made with a monolingual classifier and a model based on machine translation for predicting the category of Italian Wikipedia articles. The performance of the native classifier is evaluated based on the number of available native samples. The universal classifier in the CLTC model can classify text in any language. It is compared to a model based on machine translation, specifically Google's zero-shot translation model. The evaluation is done using Wikipedia article abstracts in Italian, German, French, and English, with topics assigned to categories based on DBPedia mapping. No pre-processing is done besides removing punctuation characters, and the dataset is split evenly into an encoder dataset and a. The dataset is split evenly into an encoder dataset and a classification dataset. The encoder dataset is used to train the universal encoder, while the classification dataset is used to train different classification models. Training the encoder involves sampling uniformly between pairs of articles with the same topic and pairs with different topics. The number of training pairs for the encoder and samples in the classifier training and test sets for different language pairs are shown in table 2. Topics with only one article and those not belonging to the 200 target classes are removed. The classification training dataset for the model does not include any Italian samples. Only the first 200 words of each article are used, and a random snippet of 3-200 tokens is drawn for input. This snippet sampling serves as a form of regularization for the models. The architectures of the models are kept as similar as possible for consistency. The architectures of the three models are kept similar for a fair comparison. Adam updates BID7 are used with default parameters, except for a learning rate of 10^-4. The universal encoder's architecture is shown in fig. 2a, with a shared embedding layer for all input languages. A word-based hash embedding with 25k buckets and two hash functions is used for experiments, requiring fewer parameters for large vocabulary or embedding sizes. The experiments use a hash embedding with 25k buckets and two hash functions. The embedding vector size ranges from 250 to 1500. A sum layer computes the coordinate-wise sum of word vectors, followed by a dense layer with twice the embedding vector size. The classification module consists of three dense layers with 2000 units and a dense layer with 200 units using softmax activation. Dropout with a 50% probability is used for regularization. The architecture of the native classifier mirrors that of the universal encoder. The architecture of the native classifier is similar to the universal encoder. The model is trained and tested on Italian samples, with training on various sample sizes and testing on 8k Italian samples. The model is first trained on English samples and then tested on Italian samples translated to English. The t-SNE plots show universal representations of articles about persons and multilingual articles of different categories. The universal encoder provides representations for articles, ensuring separation between different topics and similarity within the same topic, even across different languages. t-SNE plots demonstrate good separation between articles on different people and categories, showing the effectiveness of the encoder despite not being explicitly trained for multilingual tasks. The zero-shot classifier achieves high accuracy, especially in bilingual settings with an embedding size of 1500 reaching 78.5%. Performance is better than machine translation (72.1%) and close to the upper bound of 80.4% (native classifier). Accuracy increases with embedding size but decreases with the number of languages, reasons for this decline are unclear. The performance of the zero-shot classifier is affected by the sampling method used, which may favor certain languages. The model performs well in scenarios with a small amount of labeled data and a large corpus available. In comparison, the native monolingual classifier requires about 100k samples to outperform the zero-shot model. The different strategies for multilingual text classification can be grouped into three categories based on the type of corpus they require: aligned sentences, comparable corpus, or just a dictionary. Aligned corpora are limited, with Europarl dataset being an example available in 21 European languages. Word embeddings, mapping words to dense vectors, have proven useful in various natural language processing tasks by capturing similar meanings of words in similar contexts through unsupervised training. Multilingual word embeddings can be trained using a multitask learning algorithm or autoencoding method. These embeddings allow for cross-lingual text classification without specific classification tasks in mind. They can also be trained using only a dictionary. Multilingual word embeddings can be trained using a multitask learning algorithm or autoencoding method, allowing for cross-lingual text classification without specific tasks in mind. Methods based on machine translation introduce noise, but various approaches aim to reduce this penalty. Model translation based on the EM algorithm and domain adaption methods have been proposed to address this issue. Obtaining a comparable corpus is easier than a parallel corpus as it only requires a dictionary. Latent Dirichlet Allocation (LDA) is a Bayesian Network model used for topic modeling in documents. It assumes each document has a latent topic distribution with corresponding word distributions. This model can be extended to a multilingual setting by allowing documents with the same content in different languages to share the same topic. Multilingual LDA allows documents in different languages with the same content to share topic distribution, enabling estimation of document similarity. Cross-lingual classifiers can be trained using this property. Various approaches, including multi-view learning, optimize monolingual classifiers with similar representations of different language versions of a document. Machine translation is often used to construct different views, with low data alignment requirements. In structural correspondence learning (SCL), discriminative words called pivots are identified in a source language and translated to a target language. Each pivot induces a bisection of the texts in both languages, and a bilingual classifier is created using the information from the classifiers trained on the pivots. SCL can be trained with labeled data in a source language, pivot translations, and an unlabeled corpus in the target language, showing performance equal to other models. Multilingual word embeddings can be trained using a dictionary and unlabeled text by switching words with the same meaning in different languages. However, challenges arise as the method relies on the distributional hypothesis of word meaning. Future work could explore different directions in this area. Future work in multilingual word embeddings could involve experimenting with more complex versions of F u, such as recurrent nets, and optimizing hyper-parameters for better results. It may also be possible to use different encoder and classifier corpora to improve performance, as shown in the experiments where performance decreased with the number of languages used. The article discusses creating a language-independent representation using a corpus of comparable texts for zero-shot classification. It suggests testing the hypothesis of sampling method impact by super sampling smaller languages and exploring the use of hinge loss as a more natural loss function for the encoder. The unsupervised classifier's performance is comparable to a native language supervised classifier trained on about a hundred thousand samples. The study shows that zero-shot classification performance can be improved by using a large comparable corpus and a limited number of native samples. Results indicate that using only two languages yields the best performance, and a large embedding size is necessary for optimal results."
}