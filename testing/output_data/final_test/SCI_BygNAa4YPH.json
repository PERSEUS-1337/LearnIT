{
    "title": "BygNAa4YPH",
    "content": "In this work, tasks for out-of-distribution detection in few-shot classification are proposed, along with benchmark datasets based on popular few-shot classification datasets. Two new methods for this task are introduced and their performance is investigated. In this study, new benchmark datasets are used to establish baseline out-of-distribution detection results. Proposed methods show improved performance in few-shot learning, where a model learns new concepts from only a few examples. This paradigm is attractive as it mimics real-world intelligent behavior, where access to millions of labeled examples is not feasible. In the few-shot setting, out-of-distribution detection is crucial for safety in real-world applications. Current deep neural networks need to defer actions in unforeseen situations to ensure system reliability. In the few-shot setting, deep neural networks can be overly confident with unrecognizable inputs and their predictions can be manipulated with subtle changes. The behavior of deep nets is uncertain when test queries are out-of-distribution. Evaluating detection performance with mixed datasets is a common practice. There are two types of out-of-distribution inputs: out-of-dataset (OOS) and inputs from classes not represented in the support set. In the few-shot setting, deep neural networks can struggle with out-of-distribution examples, including out-of-dataset (OOS) and out-of-episode (OOE) inputs. Detecting these examples is crucial for various applications like semi-supervised learning and continual learning. In semi-supervised learning, even a small percentage of OOE examples in the unlabeled set can negatively impact performance. In continual learning, detecting when examples do not belong to previously learned classes is a fundamental challenge. In this work, the focus is on out-of-distribution detection in the few-shot setting. Benchmark datasets for OOE and OOS are developed based on standard datasets like Omniglot, CIFAR100, miniImageNet, and tieredImageNet. Baseline results for OOS and OOE tasks are established for Prototypical Networks and MAML classifiers. A distance metric-based approach improves performance, and a learned scoring function further enhances tasks on new benchmark datasets. Few-shot classification involves classifying unlabeled queries into classes from a test set with limited labeled examples per class. The model has access to a training set with examples from different classes. The challenge is to leverage this training data effectively to improve performance in classification tasks. Recent approaches in few-shot classification focus on exploiting seemingly-irrelevant data at training time to enable models to learn new episodes at test time using a small support set. This involves creating episodes from the training set of classes, where the training loss is based on performing well on query examples after learning on the support set. Episodic training involves randomly sampling classes and examples to form support and query sets, aiming to mimic test-time scenarios. Episodic training in few-shot learning involves different methods like Prototypical Networks and MAML. Prototypical Networks use a parameterized embedding function to compute class prototypes for classification. The model is updated based on the Prototypical Network loss during training episodes. The Prototypical Network loss updates parameters during training episodes. MAML is another episodic model that adapts weights via gradient descent to minimize cross entropy loss for classification. Out-of-distribution detection involves learning a global initialization of weights for adaptation on new episodes. Different benchmark datasets are used as a source of out-of-distribution examples, such as Omniglot and black-and-white CIFAR10. The evaluation setup includes treating data from the same dataset but different classes as out-of-distribution. This problem is a binary detection task where the model produces a score at test-time. The model is required to produce a score at test-time, with the goal of having higher scores for in-distribution examples compared to out-of-distribution examples. Different approaches to out-of-distribution detection include predictive probability, fitting a density model to inputs directly, and fitting a density model to representations of a pretrained model. The detection of in-distribution data is achieved using classifier parameters. Commonly used scores include softmax prediction probability and negative predictive entropy. Another approach is fitting a density model on the data, but this is less effective for high-dimensional images. Deep generative models have shown promise in this area. Deep generative models, such as flow-based or auto-regressive models, can assign higher densities to out-of-distribution examples. Fitting simple density models on learned classifier representations has proven useful for out-of-distribution detection. In this study, two types of out-of-distribution detection problems are focused on. In this study, out-of-distribution examples are categorized into in-distribution and out-of-episode examples. Out-of-episode examples come from classes not in the current episode, and a modified episodic training algorithm is proposed for handling them. The study focuses on out-of-distribution examples, specifically on detecting gestures that are not registered in the system. The goal is for the system to recognize when a gesture is out-of-distribution and not take inappropriate actions. The examples used for out-of-distribution testing come from a different dataset than the in-distribution set. The scoring function s(\u00b7) is used to determine the model's confidence in identifying in-distribution examples. The study focuses on detecting out-of-distribution examples, aiming for the system to recognize when a gesture is out-of-distribution. Two novel methods are proposed: 1) a parameter-free method measuring distance in the learned embedding of a few-shot classifier, and 2) a learned scoring function on top of the classifier's embedding. The Minimum Distance Confidence Score (-MinDist) is introduced to address the limitations of standard softmax prediction probability in the few-shot setting. The study introduces two novel methods for detecting out-of-distribution examples: a parameter-free distance measurement in the learned embedding of a few-shot classifier and a learned scoring function on top of the classifier's embedding. The proposed alternative confidence score, Episodic Optimization with OOE Inputs, aims to accurately detect OOE examples during training. The Learnable Class Boundary (LCBO) Network introduces a class-conditional confidence score to determine if a query belongs to a specific class. The LCBO takes support and query embeddings to output a confidence score for class membership. Aggregating class-conditional scores, the maximum confidence is used to determine in-distribution vs OOS for each query. The method is more powerful than MinDist as it considers a new set of weights. The LCBO uses a learned confidence score to determine class membership based on support and query embeddings. Training is done episodically with a binary cross-entropy objective for out-of-distribution queries. In this section, the OOE and OOS detection performance of standard few-shot methods and a novel variant are evaluated using AUROC. The experiments use a standard 4-layer ConvNet architecture without sacrificing in-distribution classification accuracy. The AUROC score for separating s(x in) from s(x out) is 100%. Results for AUPR and FPR are also reported. Three few-shot classifiers show similar performance in detecting out-of-distribution examples. In this section, the study compares the performance of different few-shot classifiers on out-of-distribution tasks. Results for 5-shot 5-way settings are presented, with detailed results for other settings in the appendix. The SPP, -MinDist, and LCBO methods were evaluated on four datasets, showing that either -MinDist or LCBO outperformed the baseline method consistently. This suggests that -MinDist may not be the most suitable confidence score for all embedding spaces. On large datasets like ImageNet, LCBO outperformed -MinDist on out-of-distribution tasks, despite -MinDist being parameter-free. The difference in performance was attributed to the increase in embedding dimensions with larger image sizes, making a learnable score like LCBO critical. LCBO's impact on out-of-distribution tasks is crucial due to the effect of different backbones on detection. Ren et al. (2018) proposed few-shot semi-supervised learning with 'distractors' for a more realistic evaluation. LCBO does not claim to improve semi-supervised learning methods but shows promise, especially when distractor inputs are out-of-dcope instead of only out-of-embedding examples. When distractor inputs are OOS instead of only OOE examples, baseline semi-supervised methods degrade classification accuracy significantly. Recent attempts to study uncertainty in few-shot settings and previous approaches for out-of-distribution detection in the supervised setting are discussed. ODIN (Liang et al., 2017) uses temperature scaling and virtual adversarial perturbations for out-of-distribution detection. Lee et al. (2018b) found this approach complementary to fitting a Gaussian density to network activations. In the few-shot setting, ODIN and outlier exposure methods did not show significant impact. Previous approaches using predictive probability for out-of-distribution detection have been criticized for incorrect usage of learned density models. Ren et al. (2019) proposed a separate \"background\" model with likelihood ratio scoring. Generative/density models have not been extensively studied in few-shot settings due to lack of task evaluation. The study investigates OOS and OOE tasks in few-shot classification, proposing new confidence scores - MinDist and LCBO, which outperformed existing methods on four datasets. This work aims to encourage future research on quantitative evaluation in this area. The study introduces new confidence scores for OOS and OOE tasks in few-shot classification, outperforming existing methods on multiple datasets. The work aims to promote future research on quantitative evaluation in this field, including episodic training and evaluation algorithms. The Omniglot dataset contains 28x28 greyscale images of handwritten characters, CIFAR100 has 32x32 color images with 100 classes, and miniImageNet consists of 84x84 colored images with 100 classes. These datasets are commonly used for few-shot classification tasks. The tieredImageNet dataset, proposed by Ren et al. (2018), has 608 classes instead of 100 like miniImageNet. Out-of-Dataset datasets were adopted from previous studies by Hendrycks et al. (2019) and Liang et al. (2017), with noise variations used for scaling OOS inputs. Other datasets mentioned include notMNIST, CIFAR10bw, and LSUN. The LSUN dataset is a large-scale scene understanding dataset, while iSUN is a subset of SUN with 8925 images. Texture dataset contains real-world patterns, and Places is another scene understanding dataset. SVHN refers to the Google Street View House Numbers dataset, and TinyImagenet consists of 64x64 color images from 200 ImageNet classes. Results show that MinDist improved OOE and OOS detection under all metrics, especially on the OOS task due to erratic behavior of baseline scoring functions. When the embedding network is trained on CIFAR100, points far from the in-distribution have significantly larger L2-norms. Using -MinDist assigns low confidence to such points, improving OOE detection. This effect is more pronounced with CIFAR100 compared to easier datasets like Omniglot. The softmax function is invariant to a constant additive bias in the logits, making points outside the convex hull formed by prototypes equivalent to being on the boundary. In higher dimensions, moving away from the range of prototypes causes the softmax output to decrease in entropy, contrary to the desired increase in entropy. In higher dimensions, confidence functions in predicted probability space are not suitable for out-of-distribution data. Lee et al. (2018b) use Mahalanobis distance with full covariance Gaussian, facing difficulty in few-shot setting due to singular covariance matrix. The ABML algorithm aims to learn a posterior distribution of global initialization \u03c8 that maximizes a variational lower bound of data likelihood. It uses a prior on \u03c8 and episode-specific model weights, performing Bayes by Backprop on the support set in each episode. The initial variational parameter for model weights is set to \u03c8 to reduce parameters without negative impact on performance. The ABML algorithm learns a posterior distribution of global initialization \u03c8 to maximize data likelihood. Ravi & Beatson simplified the inference of \u03c8 to a point estimate using gradient descent. ABML was implemented based on MAML, showing similar calibration results and classification accuracy. The ABML algorithm with 10 posterior samples shows better calibration than ABML with 1 posterior sample and MAML. Semi-supervised few-shot learning has gained recent interest, involving unlabeled sets with 'distractor' classes. Out-of-distribution examples in the unlabelled dataset can degrade classifier accuracy. In semi-supervised learning, methods like soft k-Means can be ineffective without sophisticated techniques to mask out distractors. A new approach using Prototypical Networks based on LCBO is proposed, where classification is done based on updated prototypes. Outlier exposure (OE) is also investigated for its impact on the model. The LCBO network is trained with outlier exposure (OE) to improve classification using updated prototypes. LCBO+OE differs from previous studies by incorporating OE inputs as out-of-distribution examples during training. Two auxiliary dataset settings are explored for LCBO+OE, including TinyImages and a combination of TinyImages with other datasets. LCBO+OE incorporates outlier exposure inputs during training, utilizing a combination of TinyImages and three OOS noise distributions."
}