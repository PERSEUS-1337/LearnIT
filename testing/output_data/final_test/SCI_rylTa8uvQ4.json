{
    "title": "rylTa8uvQ4",
    "content": "We present a method to construct task-optimized embeddings from existing word embeddings without the need for additional labeling or complex model architectures. This method can be used to estimate if specific end-tasks can be learned from an unlabeled dataset. Evaluations show that unsupervisedly pretrained word embeddings can significantly improve performance on supervised end-tasks through transfer learning. Recent works suggest that universally best embeddings are not yet achievable, and instead, embeddings should be tailored to specific end-tasks using inductive bias. This approach allows embeddings to be fine-tuned for Single-task (ST) or Multi-task (MT) semantic purposes without the need for extensive labeled datasets. Unsupervised fine-tuning of word embeddings for specific end-tasks or a set of tasks can be explored as an alternative to complex supervised fine-tuning, especially when labeled datasets are limited or of low quality. The method MORTY aims to optimize widely used word embeddings like Fasttext and GloVe for specific end-tasks or multiple supervised tasks. It involves unsupervised postprocessing to improve performance on probing tasks. The approach can benefit embeddings trained on smaller corpora, making it useful for low-labeling-resource domains. The method MORTY optimizes word embeddings for specific end-tasks or multiple supervised tasks by unsupervised postprocessing to improve performance on probing tasks. It benefits embeddings trained on smaller corpora, making it useful for low-labeling-resource domains like biology or medicine. MORTY uses multiple Autoencoders to create specialized inputs for supervised end-tasks, allowing further pretraining exploitation with minimum extra effort, time, and compute resources. MORTY uses multiple Autoencoders to optimize word embeddings for specific end-tasks or multiple supervised tasks. It creates new representations that can improve performance on probing tasks, benefiting embeddings trained on smaller corpora. The method can be efficiently trained as a hyperparameter to end or proxy tasks, using 100 dimensional embeddings from Fasttext and GloVe on wikitext-2 and wikitext-103. Public Fasttext and GloVe embeddings can also be utilized for reconstruction. MORTY optimizes word embeddings for specific tasks using multiple Autoencoders. It evaluates embedding performance scores for Fasttext and GloVe before and after postprocessing with MORTY. Results are shown for Single-task and Multi-task application settings. Performance impact is measured in percentual change for different tasks. In evaluating word embedding performance, MORTY optimizes embeddings for specific tasks using Autoencoders. Results show improvements in classification, similarity, and analogy tasks with larger training corpus sizes. Fasttext and GloVe models were run multiple times on different corpus sizes, with slight variations in results. Using MORTY improves overall performance for Fasttext and GloVe embeddings, especially for smaller corpus sizes. Fasttext outperforms GloVe for analogy tasks on smaller corpora, with GloVe (840B) showing the best analogy performance. MORTY further enhances analogy scores for both public embeddings (600B/840B). Similarity sees improvements for smaller corpora, while classification results are more mixed. Overall, MORTY improves the performance of Fasttext and GloVe embeddings, particularly on smaller datasets. Fasttext outperforms GloVe in overall performance on smaller datasets, while GloVe excels in analogy tasks. MORTY enhances analogy scores for both embeddings. Classification results show mixed changes, with both performance increases and drops for individual tasks, especially on smaller datasets and for Fasttext. An overall best MORTY specializes the original Fasttext embedding to better fit specific tasks, beating base embeddings in overall score. MORTY improves Fasttext and GloVe embeddings on smaller datasets, showing consistent improvements. Fasttext benefits more on smaller datasets, while GloVe excels on larger datasets. MORTY is beneficial for low-resource settings and training parameters are similar to the original embedding model. In low-resource settings, MORTY improves Fasttext and GloVe embeddings by training with a learning rate of 0.01 for 1 epoch and a representation size equal to or twice as large as the original embedding. Larger vocabularies can be used with sublinear performance loss when compressing from the original embedding size. Additional parameter exploration did not yield significant gains. Transfer Learning literature has heavily focused on information transfer between supervised tasks, while transfer between unsupervised tasks has received less attention. Word2Vec, Fastext, and GloVe provide pretrained embeddings that can improve performance on supervised tasks. Transfer can also be used to specialize embeddings for specific supervised signals or enforce relevant semantics using multi-task supervision. Automated methods like Bayesian optimization can tune embeddings for specific tasks. Knowledge transfer can also occur between supervised tasks. Another method for knowledge transfer is through supervised tasks, which can be leveraged successively, jointly, and in joint-succession to enhance performance. MORTY offers unsupervised fine-tuning of embeddings without the need for external embeddings or target domain texts, making it a low-effort solution that can be optimized for specific supervised semantics. This approach eliminates the need for complex multitask learning setups or creating task-related supervised datasets. MORTY can be optimized as a data-input parameter for specific supervised tasks, offering significant performance improvements without the need for complex model architectures or additional labeling. It provides a low-effort method to construct task-optimized word embeddings from existing ones, even in low-resource settings, resulting in enhanced performance on various end-tasks."
}