{
    "title": "r1GaAjRcF7",
    "content": "In this paper, a subset selection algorithm is proposed for claim verification in the FEVER task. The algorithm is trainable with gradient-based methods and achieves near-optimal performance through submodular optimization. The differentiable greedy network (DGN) outperforms discrete optimization algorithms and baseline methods in terms of precision and recall. In this paper, a differentiable and discrete subset selection algorithm is developed for natural language processing tasks like fact extraction and question answering. The algorithm can model complex dependencies between elements and address the shortcomings of conventional evidence retrieval methods by using a diversity promoting submodular objective function. Submodularity allows near-optimal solutions to be found in polynomial time for NP-hard problems. Submodular functions, with their diminishing returns property, are well-suited for tasks like claim verification. They help discount redundancy in favor of diverse but relevant evidence, which is crucial for complex induction from multiple sentences in claim verification. The main contribution of this paper is a new optimization scheme called the Differentiable Greedy Network (DGN), which integrates continuous gradient-based and discrete submodular frameworks for claim verification. By unfolding a greedy algorithm into a computational graph, the model can efficiently handle dependencies between sentences and features, providing a near-optimal solution using a simple forward greedy algorithm. The Differentiable Greedy Network (DGN) integrates continuous gradient-based and discrete submodular frameworks for claim verification. It improves recall@k and precision@k for sentence selection tasks on the FEVER dataset. The DGN can be extended to other information retrieval tasks and relies on a simpler model compared to more sophisticated neural architectures. In Section 2, related work in information retrieval, submodularity, and deep unfolding is discussed. Section 3 introduces submodularity and the Differentiable Greedy Network (DGN). Section 4 presents experiments and results for baseline models and DGN applied to sentence selection on the FEVER dataset. Section 5 concludes the study, with an additional example in Appendix 6 showcasing the utility of promoting diversity in evidence retrieval for claim verification and question answering. The curr_chunk discusses the use of deep unfolding, a technique that combines generative and deep learning models to improve the modeling of dependencies between candidate sentences. This approach fills the gap left by models that only focus on similarities between claims and evidence sentences. The curr_chunk discusses how deep unfolding has been used to derive interpretable network architectures from inference algorithms, particularly for sparse models. The proposed method allows for gradient-based learning by interpreting the output as a distribution over sets, although it differs from other methods by relying on stochastic submodular optimization algorithms. The curr_chunk discusses learning through discrete functions using stochastic submodular optimization algorithms, focusing on sentence selection complexities. The proposed network learns parameters through greedy optimization with a relaxed argmax function at training time, solving the submodular function maximization problem at test time. The innovation of Differentiable Greedy Networks lies in making greedy optimization of a submodular set function differentiable. Submodularity allows for fast computation of near-optimal solutions by describing diminishing returns as the context grows. The incremental gain of adding an element to a set is defined by a submodular function, which can be maximized or minimized. Submodular functions exhibit concave behavior and can be optimized using algorithms like the forward greedy algorithm. Submodular function optimization is useful for subset selection problems like retrieval and summarization. It allows for dependence between pieces of evidence in tasks like evidence extraction. While selection models often assume independence between sentences for simplicity, this can lead to redundancy. The submodular function optimized is an SCMM. The submodular function optimized in SCMM for sentence selection tasks aims to find the optimal subset A* that maximizes the objective function f\u03b1(\u00b7). Despite the NP-hard nature of the problem, a near-optimal solution can be found efficiently using the forward greedy algorithm. The Differentiable Greedy Network (DGN) is introduced as an unfolded discrete optimization algorithm to train parameters discriminatively while maintaining the guarantees of the forward greedy algorithm. The key change is making the network differentiable by approximating the argmax function with softmax during training. The temperature parameter \u03c4 is crucial for training the Differentiable Greedy Network (DGN) as it affects gradient flow. Setting it too low leads to unstable training, while setting it too high results in a more uniform output. The DGN is depicted as a computational graph with input features X going through a linear encoding layer followed by ReLU activation for stability during training. The SCMM function requires non-negativity for stability during training. Non-negative features H are processed through greedy layers with a state vector encoding sentence selections. The network evaluates potential next states to find the sentence that maximizes f \u03b1 (\u00b7) and adds it to the current state. The connections between H and the greedy layers resemble a recurrent network, with shared weights. The greedy layers in the network deviate from conventional architectures and function similarly to an attention mechanism. They measure sentence relevance and model dependencies like redundancy in a straightforward manner. The DGN models sentence dependencies based on the submodularity of the objective function. For the evidence extraction task, input features are similarity values between the claim and sentences. The Differentiable Greedy Network (DGN) is applied to the Fact Extraction and Verification (FEVER) dataset. The network structure involves linear encoding of input features followed by greedy layers for sentence selection based on similarity to the claim. The DGN models sentence dependencies using submodularity of the objective function. The Differentiable Greedy Network (DGN) is used for sentence selection in the Fact Extraction and Verification (FEVER) dataset. The network evaluates potential next states to find the sentence that maximizes a specific function. The FEVER dataset contains verifiable claims from Wikipedia and involves evidence retrieval and recognizing textual entailment (RTE). The FEVER dataset contains verifiable claims from Wikipedia, with evidence retrieval and RTE. The baseline system selects sentences with high TF-IDF cosine similarity to claims. The FEVER scoring function computes precision, recall, and F1 score for retrieved sentences. The best RTE baseline system drops in accuracy from oracle to baseline evidence retrieval. Appendix 6 shows an example promoting diversity in sentence retrieval. In experiments, the max number of greedy iterations for sentence retrieval is set to k = 7, covering 93% of claims. A more complex model is needed to handle interactions between sentences for claim verification. Introducing redundancy with a diversity-promoting value function, SCMM, is the next step. FastText word embeddings are used for claims and potential evidence sentences, with element-wise multiplication for similarity scores. The curr_chunk discusses using FastText word embeddings to encode semantic structures for similarity scoring between claims and sentences. The SCMM algorithm is used to select the best evidence sentences, with a forward greedy algorithm improving recall. The DGN model quickly fits to the full set with decreasing learning rate resulting in smoother loss but lower recall. The SCMM algorithm utilizes FastText word embeddings to score similarity between claims and sentences, promoting semantic diversity. The DGN encoder projects input features to a matrix through linear layers. Random hyperparameter searches are conducted for learning rate, encoder output dimension, and softmax temperature. The training loss is accumulated layer-wise cross entropy, computed at each layer to improve decision-making. The DGN encoder assigns scores to sentences at layer k using normalized scores. Cross Entropy loss is calculated with indicator functions and greedy layers. The DGN is compared to baselines like feedforward greedy algorithms and shallow/deep feedforward networks. Models consist of dense linear layers with ReLU activation and softmax output, trained with binary cross entropy loss. Adam optimizer is used with default parameters except for learning rate. Models are implemented in PyTorch. The DGN model outperforms untrained baselines in terms of recall and precision when selecting different numbers of sentences as evidence. It improves recall@7 by 10% and shows a 14-18% improvement in recall when selecting fewer sentences. The model has 172K trainable parameters, while the DeepEncoder has 915K parameters. The DGN model shows improved recall and precision when selecting fewer sentences as evidence, with gains in precision of 5%, 10%, and 27% for k=5, 3, 1 respectively. It is more robust to class imbalances compared to Encoder and DeepEncoder models, as it does not require weighting of positive samples in cross entropy loss and is insensitive to class imbalance due to the greedy algorithm. The DGN explicitly models dependencies between functions during submodular function optimization. The DGN model explicitly models dependencies between functions using submodular functions. It verifies multiple facets of a claim by selecting relevant sentences as evidence, showing improved recall and precision. The DGN model uses submodular functions to rank sentences for evidence selection, considering redundancy between sentences. It can have failure cases when promoting diversity in selected sentences, potentially selecting the wrong sentence if actual evidence sentences are deemed redundant. Phoenix is the largest city in Arizona, with a thriving farming community due to its canal system. The DGN model may select the wrong sentence for evidence if promoting diversity leads to redundancy issues. The DGN model may select the wrong sentence for evidence if promoting diversity leads to redundancy issues. In this case, the algorithm should select more redundant sentences to refute the claim connecting Arizona and the Atlantic Ocean. Lowering the learning rate improves mean layer-wise cross entropy and recall, while raising it achieves a lower cross entropy initially but with a higher and monotonically increasing recall. This behavior is attributed to the greedy layers restriction of selecting a single sentence each. The DGN model's behavior is influenced by the greedy layers restriction of selecting a single sentence each. The summed loss approach helps maximize recall and avoids vanishing gradients in recurrent networks. Shuffling sentence order slightly reduces model performance in terms of recall, precision, and F1. Training the DGN model was initially challenging due to the choice of softmax temperature as a crucial hyperparameter setting. Two approaches were experimented with - setting the temperature in a range or applying temperature annealing. Both approaches yielded similar results, with the first approach chosen for simplicity. The experiments showed that temperatures in the (3,6) range produce the best results for the proposed differentiable greedy network (DGN), outperforming conventional discrete optimization algorithms in terms of recall and precision. Unfolding a greedy algorithm into a computational graph allows for interpretability and unsupervised initialization while benefiting from supervised learning techniques. The SCMM function in equation 2 is proven to be monotone nondecreasing submodular, showing the usefulness of submodularity for diverse evidence extraction. The experiments demonstrated that temperatures in the (3,6) range are optimal for the differentiable greedy network, outperforming traditional discrete optimization algorithms. The claim that \"Stripes only featured women\" in the movie \"Stripes\" is refuted. The DeepEncoder selects true evidence about actor Bill Murray's career, including his roles in various films and awards received. Initially, the DGN rates these sentences highly. The DGN initially rates two sentences highly, selecting the first evidence sentence as the DeepEncoder. In the second layer, the value of b drops significantly due to redundancy with a. Instead, the DGN selects sentence c, providing information refuting the claim with a list of male actor names."
}