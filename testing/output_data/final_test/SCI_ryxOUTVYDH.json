{
    "title": "ryxOUTVYDH",
    "content": "Deep neural networks can memorize noisy examples due to being over-parameterized. A novel training method called Learning with Ensemble Consensus (LEC) addresses this issue by eliminating noisy examples using an ensemble of perturbed networks. LTEC, a type of LEC, outperforms current methods on noisy MNIST, CIFAR-10, and CIFAR-100 datasets efficiently. In practice, obtaining high-quality labeled datasets is challenging. Deep neural networks struggle to generalize training data with noisy examples, leading to a demand for robust training methods. Recent studies focus on identifying clean examples by considering small-loss examples that do not overfit noisy data. However, choosing safe examples from noisy datasets based on small-loss criteria may be impractical. To address this, a method is proposed to screen out noisy examples among small-loss examples by recognizing that noisy examples are learned through memorization rather than generalization. In this work, a method is introduced to filter noisy examples from small-loss examples by perturbing network parameters. This method, called learning with ensemble consensus (LEC), embeds the filtering process into training to improve robustness in deep neural networks optimized with SGD. Learning with ensemble consensus (LEC) involves training the network on the entire training set and then on small-loss examples from perturbed networks. Three LECs with different perturbations are evaluated on benchmark datasets with random label noise, open-set noise, and semantic noise. The proposed LEC outperforms existing robust training methods by efficiently removing noisy examples from training batches. DNNs are over-parameterized but have strong generalization ability, with gradient-based optimization playing a key role in regularization. Training DNNs with Noisy datasets involves addressing label noise issues by reducing the negative impact of noisy examples. This can be done by training with a modified loss function based on the noise distribution or by training with modified labels using the current model prediction. Recent work also suggests exploiting small-loss examples to improve training outcomes. This study proposes a method to address the problem of small-loss criteria in training deep neural networks with noisy datasets. It suggests identifying noisy examples within the small-loss set to improve generalization ability, as noisy examples are likely memorized but not well generalized by the network. Deep neural networks (DNNs) struggle to generalize neighborhoods of memorized features, leading to increased losses for noisy examples under perturbations. Clean examples, however, show consistent small losses even with perturbations. By identifying clean examples through ensemble consensus filtering, a new robust training method called learning with ensemble consensus (LEC) is proposed. The proposed robust training method, learning with ensemble consensus (LEC), involves filtering clean examples through ensemble consensus to improve generalization in deep neural networks. Two algorithms, described in Algorithms 1 and 2, consist of warming up and filtering processes. Algorithm 1 trains on the intersection of small-loss examples from multiple networks within a mini batch, while Algorithm 2 samples a subset of examples from the intersection within a full batch for more stable training. The proposed robust training method, learning with ensemble consensus (LEC), involves filtering clean examples through ensemble consensus to improve generalization in deep neural networks. The process includes warming up and filtering processes, with the goal of finding a perturbation \u03b4 to distinguish between generalized and memorized features. Three LECs with different perturbations are presented, including Network-Ensemble Consensus (LNEC) which utilizes an ensemble of networks to differentiate between generalization and memorization. The perturbation \u03b4 in the filtering process comes from the difference between M networks or M stochastic predictions of a single network. Self-Ensemble Consensus (LSEC) focuses on the relationship between network predictions for memorized and generalized features. Temporal-Ensemble Consensus (LTEC) is inspired by the observation that atypical features are more easily distinguished during training. During training, LTEC uses perturbation \u03b4 from differences between networks at current and preceding epochs to filter out noisy examples from small-loss examples. It focuses on the effectiveness of perturbations and compares with other methods under various annotation noises like random label noise, open-set noise, and semantic noise using datasets like MNIST and CIFAR-10/100. To assess robustness, datasets like MNIST and CIFAR-10/100 are used with various annotation noises such as random label noise and open-set noise. Different levels of noise are simulated, including sym-20%, asym-20%, sym-60%, and asym-40%. The details can be found in Section A.1.1. In open-set noise experiments, a percentage of examples are replaced with images from another dataset while keeping labels intact. Semantic noise involves flipping labels of uncertain images based on disagreement between predictions of networks trained with clean data. Two types of noise levels are studied: 20% and 40%. The study involves training with clean data except for ground-truth labels, exploring 20% and 40% semantic noise. A 9-convolutional layer architecture is used for 200 epochs with Adam optimizer. The LEC method includes hyperparameters like warming-up duration, noise ratio, and number of networks. Evaluation metrics include test accuracy and label precision. Test accuracy and label precision are key metrics for evaluating methods with multiple networks. Peak and final accuracy are reported due to limited validation data availability. Each method is run four times with different noise types and initial network parameters are randomized. Semantic noise is generated deterministically, while random label noise and open-set noise are randomly generated. The uncertainty of image x is defined by f (x; \u03b8n)) where f (; \u03b8) denotes softmax output of network parameterized by \u03b8. N is set to 5 as in Lakshminarayanan et al. (2017). Label precision (%) of small-loss examples of the current network and the intersection of small-loss examples of the current and preceding networks during running LTEC on CIFAR-10 with random label noise is compared. Self-training method is discussed, showing low label precision especially under high noise levels. The label precision of Selftraining is low under high noise levels. Three LECs trained on higher precision data achieve higher test accuracy, with LTEC performing the best. Noisy examples are removed through ensemble consensus filtering in LTEC, resulting in higher label precision. Competing methods include a regular training method: Standard. Competing methods include Standard, D2L, Forward, and Co-teaching. Results on MNIST/CIFAR with random label noise show D2L's test accuracy increases at low noise levels but not at high levels due to large weights on given labels. Forward performs well only in limited scenarios like MNIST. Our methods LTEC and LTEC-full outperform Co-teaching on CIFAR-100 with asym-40% noise by 6% and 5% respectively. Results on CIFAR with open-set noise show all methods, including LTEC and LTEC-full, perform well. The low correlation between open-set noisy examples may contribute to this performance. Additionally, out-of-distribution examples are challenging to handle. Results on CIFAR with semantic noise show that LTEC and LTEC-full perform comparably or best among other methods, especially on 40% semantic noise due to LTEC-full's training stability. Ensemble consensus filtering is used to remove out-of-distribution examples during the warming-up process. Clean examples with large losses may be excluded from training batches to improve generalization. During the filtering process of LEC, clean examples with large losses may be excluded from training batches. The recall of clean examples used for training gradually increases as training proceeds, preventing overfitting. Pre-training before filtering can help retain clean examples. The number of examples used for training depends on the number of networks used for filtering. The number of examples used for training depends on the number of networks used for filtering. Training with a larger M does not always lead to better performance as too many examples may be removed from batches. Poorly estimated noise ratios can affect performance, as shown in the results of LTEC and Co-teaching on CIFAR-10 with random label noise. The key idea of LTEC is rooted in the difference between generalization and memorization, focusing on learning clean examples and noisy examples in early SGD optimization. LTEC shows robustness against noise estimation errors due to ensemble consensus filtering, making it applicable to different architectures like ResNet-20. The method of generating and using ensembles for robust training is presented, with three perturbation methods explored. The ensemble consensus on small-loss examples helps identify noisy examples. Annotation noises such as random label noise and open-set noise are discussed. The study explores perturbation methods for robust training, including open-set noise and semantic noise. Clean examples have simple images, while noisy examples are more complex. A 9-convolutional layer architecture is used, optimized with Adam for 200 epochs. The study uses a 9-convolutional layer architecture optimized with Adam for 200 epochs. Parameters such as batch size, learning rate, and momentum are specified. Regularizations like data augmentation are not applied. The results on clean MNIST, CIFAR-10, and CIFAR-100 can be found in Table A2. The study compares different training methods including Standard, D2L, Forward, and Co-teaching. Each method is tuned individually with specific approaches. The Fully Connected layer in the architecture measures LID estimates with a window size of 12. The network is trained using original labels until a turning point is found, then trained using bootstrapping target with adaptively tunable mixing coefficient. Prior to training, a corruption matrix is estimated based on class probabilities. Co-teaching involves two networks selecting small-loss examples within a minibatch and providing them to each other, with the ratio of selected examples linearly annealed over the first 10 epochs. The computational complexity of different methods depends on hyperparameter values such as the warming-up process duration and noise ratio. LTEC is the most efficient method as it can be implemented with a single network and only updates a subset of the training set after the warming-up process. Ensemble consensus filtering with a large M value removes clean examples from training batches early on. Ensemble consensus filtering with a large M value removes clean examples from training batches early on, but a larger M does not always lead to better performance. Pre-training prior to the filtering process may help reduce the number of clean examples removed regardless of M."
}