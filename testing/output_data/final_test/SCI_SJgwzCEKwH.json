{
    "title": "SJgwzCEKwH",
    "content": "Mode connectivity is utilized to analyze loss landscapes and improve adversarial robustness in deep neural networks. Experiments show that limited bonafide data can effectively mitigate adversarial effects while maintaining accuracy on clean data, allowing for the repair of backdoored or error-injected models. Additionally, mode connectivity is used to study loss landscapes of regular and robust models against evasion attacks. Experiments reveal a barrier in adversarial robustness loss between regular and adversarially-trained models, with a high correlation between robustness loss and the largest eigenvalue of the input Hessian matrix. Mode connectivity provides a tool for evaluating and enhancing adversarial robustness by connecting independently trained DNN models on their loss landscape. In this study, the researchers propose to use mode connectivity to improve the adversarial robustness of DNNs against various threats, such as trojan attacks during the training phase. This approach is motivated by recent research on geometric properties related to adversarial robustness in DNNs. In this research, mode connectivity in loss landscapes is used to repair backdoored or error-injected DNNs and analyze the robustness loss barrier between regular and adversarially-trained models. This approach aims to mitigate training-phase adversarial threats by leveraging pre-trained models. Users often use publicly available pre-trained models for model fine-tuning or transfer learning with limited data. However, these models may be vulnerable to tampering, such as backdoor attacks, which are difficult to detect. To address this issue, a method using mode connectivity can repair backdoored or error-injected DNNs, effectively countering adversarial effects. Using mode connectivity, the path trained with limited bonafide data can repair attacked models from backdoor and error-injection attacks, resulting in high-accuracy and low-risk models. This method outperforms baselines like fine-tuning, training from scratch, pruning, and random weight perturbations. Additionally, mode connectivity reveals insights into standard and adversarial-robustness loss landscapes, showing differences in barriers between regular and adversarially-trained models. The path reveals a barrier, providing a geometric interpretation of the \"no free lunch\" hypothesis in adversarial robustness. Experimental results on different DNN architectures and datasets support the effectiveness of using mode connectivity in loss landscapes to enhance adversarial robustness. This approach is resilient to adaptive attacks and is the first to propose using mode connectivity for this purpose. The text discusses finding a high-accuracy path between two neural network model weights by minimizing a user-specified loss function. It proposes using a parametric curve with parameters \u03b8 to achieve this, and suggests using a uniform distribution on the curve to find the optimal parameters. This method aims to make the training of high-accuracy path connections more computationally tractable. The text discusses using parametric curves to connect neural network model weights for high-accuracy paths. Bezier curves are used for smoothness, while research on mode connectivity focuses on generalization analysis and adversarial robustness. The text explores backdoor attacks on DNNs, where a trigger pattern is implanted in training data to manipulate model output. Research focuses on detecting and filtering anomalous data to mitigate these attacks. Evasion attacks manipulate model predictions by adding imperceptible noises to natural images, resulting in misclassification. These attacks can be executed without access to training data and even when the model details are unknown to the adversary. Error-injection attacks inject errors into model weights. Error-injection attacks modify model weights to cause misclassification, using methods like laser beams and row hammer at the hardware level. Experimental results on adversarial robustness were conducted on various network architectures and datasets, with details in Appendix A. The experiment setups for connecting models using cross entropy loss and quadratic Bezier curve are detailed in Appendix A. The results of high-accuracy pathways between untampered models are summarized, followed by detailed discussions. Error rate and accuracy on clean/adversarial samples are used interchangeably. Results connecting VGG models trained on CIFAR-10 are shown in Figure 1, with additional results on SVHN and ResNet in Appendix B. The inference results on test set using SVHN and ResNet are evaluated with 5000 samples. The problem setup involves backdoor and error-injection attacks, where a user has two potentially tampered models and limited bonafide data. The tampered models behave normally on non-triggered inputs, leading to the need to exploit model power while mitigating adversarial effects. In the case of one tampered model, bonafide data is used to train a fine-tuned model, which is then connected to the given model. Additionally, for evasion attacks, the scenario involves a user with access to the entire training dataset studying model behavior on the path. The study aims to analyze model behavior on the path connecting two independently trained models using different portions of the original test data. Despite prior high-accuracy results using the entire training data, this path connection is trained with limited bonafide data, showing that even a small amount of clean data can lead to models with good performance. Data shows that models with good performance can be found using limited bonafide data. Path connection with only 1000/2500 CIFAR-10 samples reduces VGG16 model accuracy by at most 10%/5% compared to well-trained models. The worst-performing model is typically around t = 0.5 on the path. Backdoor attacks are implemented following specific procedures, resulting in two backdoored models. The error rate against backdoor attacks on the connection path for CIFAR-10 (VGG) is shown in Figure 2. The study investigates backdoor attacks on models trained with poisoned data. Two types of attacks are considered: single-target attack and all-targets attack. The backdoored models perform similarly to untampered models on clean data but misclassify the majority of triggered samples. All-targets attack is more challenging than single-target attack. The study explores backdoor attacks on models trained with poisoned data, including single-target and all-targets attacks. By connecting backdoored models with limited bonafide data, the models show low error rates on clean data but vulnerability to backdoor attacks. However, this connection can effectively mitigate backdoor attacks and reduce the attack success rate from close to 100% to nearly 0%. Models along the path exhibit high resilience to backdoor attacks. Most models on the path show high resilience to backdoor attacks, indicating that mode connectivity with limited bonafide data can effectively counteract these attacks. The amount of bonafide data used for training the path significantly impacts the performance on clean data. Using fewer data samples for path connection results in models with higher error rates on clean data. The benefits of leveraging mode connectivity remain consistent across different network architectures and datasets. Comparisons with baseline methods show the effectiveness of mode connectivity in mitigating backdoor attacks. The results of different methods for countering backdoor attacks are summarized in Table 2, with the path connection method consistently outperforming baseline methods. Even with as few as 50 bonafide samples, the path connection method maintains 63% clean accuracy while limiting backdoor accuracy to 2.5%. Fine-tuning is identified as the best baseline method. The best baseline method for countering backdoor attacks is fine-tuning, which has similar backdoor accuracy as path connection but lower clean accuracy. Fine-tuning can achieve comparable clean accuracy to path connection on SVHN (ResNet), but with significantly higher backdoor accuracy. Training from scratch with limited data does not yield competitive results. Noisy models perturbed by Gaussian noises are not effective against backdoor attacks. Pruning gives high clean accuracy but little effect on mitigating backdoor accuracy. Our proposal of using mode connectivity to repair backdoor models can be extended. Our proposal to repair backdoor models using mode connectivity can be extended to cases with only one tampered model. By fine-tuning the model with bonafide data and connecting it with the given model, path connection can maintain good accuracy on clean data while being resilient to backdoor attacks. Similar conclusions are drawn when two backdoored models are trained with different poisoned datasets. Technical explanations for the effectiveness of the path connection method involve model weight space exploration and data similarity comparison. The study highlights the challenges of finding a model with high clean accuracy and low attack success rate by chance. It emphasizes the importance of using path connection for attack mitigation and model repairing. The similarity analysis of input gradients shows that the path connection method can neutralize the backdoor effect. This method outperforms fine-tuning, showcasing the significance of leveraging knowledge in model repair. Our path connection method, superior to fine-tuning, utilizes knowledge of mode connectivity for model repairing and defense against adaptive attacks. In an advanced attack scenario, where the attacker is aware of path connection but cannot compromise private data, our approach remains resilient. The attacker attempts to break the path connection by training compromised models, but our method proves effective against this adaptive attack. Our approach remains resilient to adaptive attacks, including the fault sneaking attack for injecting errors to model weights. The attack accuracy is nearly 100% on targeted samples, but using path connection and limited bonafide data, the injected errors can be removed almost completely. Using path connection and varying the size of training data can sanitize error-injected models, achieving nearly 100% fault tolerance. Models near t = 0 or t = 1 show strong fault tolerance to errors and perform well on clean data. Comparison with baselines shows path connection and training-from-scratch are most effective in sanitizing models, with 0% attack accuracy. Clean accuracy of path connection is significantly better than other methods. The clean accuracy of path connection is substantially better than baselines, indicating novel applications for finding accurate and robust models. The model weight space demonstrates significant differences between connection path models and random noisy models. Error-injected images show lower similarity than clean images, and path connection is resilient to advanced error-injection attacks. Investigating mode connectivity against evasion attacks reveals insights on standard and adversarial-robustness loss landscapes. The path connecting two independently trained models is trained using the entire dataset to minimize equation 2 with cross entropy loss. Robustness loss, measured by the cross entropy of class predictions on adversarial examples, indicates vulnerability to evasion attacks. The largest eigenvalue of the input Hessian matrix can provide insights into the robustness loss landscape. The eigenvalue of the input Hessian can provide insights into the robustness loss landscape, similar to the model-weight Hessian for generalization performance. Evasion attacks are conducted on 9 models using the PGD method, with evaluation based on non-targeted adversarial examples. Adversarial training with the PGD method is used to obtain robust models at the expense of reduced accuracy on clean data. The study focuses on analyzing standard and robustness loss landscapes by examining models on the connecting path. Loss landscapes were analyzed by scrutinizing models on paths connecting different pairs of models, including independently trained regular models, regular to adversarially-trained models, and independently adversarially-trained models. Results showed no standard loss barrier in all cases, with similar standard loss metrics for all models on the paths. However, a distinct robustness loss barrier was observed, indicating differences in robustness loss along the connection path. The robustness loss barrier between regular and adversarially-trained models suggests that adversarially robust models cannot be obtained without additional costs. This barrier is more pronounced in certain cases, indicating a lack of better adversarially robust models on the path connected by regular and adversarially-trained models. The largest eigenvalue of input Hessian shows a high correlation with robustness loss, indicating the relationship between local loss curvature and adversarial robustness. This correlation is verified by a high empirical Pearson correlation coefficient. The largest eigenvalue of input Hessian is highly correlated with robustness loss, showing the relationship between local loss curvature and adversarial robustness. This correlation is supported by a high empirical Pearson correlation coefficient. The oracle robustness loss on the path is defined as max \u03b4 \u2264 l(w(t), x + \u03b4), where \u03b4 is a perturbation to x confined by an -ball induced by a vector norm. The standard loss on the path is assumed to be constant for all, with the input gradient \u2207 x l(w(t), x) and input Hessian H t (x) at x. The normalized inner product in absolute value for the largest eigenvector v of H t (x) and \u2207 x l(w(t), x) is denoted as c, where T v| \u2207xl(w(t),x) = c. As c approaches 1, max \u03b4 \u2264 l(w(t), x + \u03b4) \u223c \u03bb max (t). Assumption (a) is based on the existence of a high-accuracy path in the standard loss landscape from mode connectivity analysis. Assumption (b) suggests that the local landscape of input x can be well approximated by its second-order curvature. The value of c, empirically verified to be large, plays a significant role in both regular and adversarially-trained models. Additional experiments show the possibility of finding a robust path connecting different model pairs using adversarial training, known as the \"robust connection\" method. Model ensembling through either regular or robust connections offers little defense against evasion attacks due to the transferability of adversarial examples between similar models. This paper explores adversarial robustness of deep neural networks by analyzing mode connectivity in loss landscapes. It demonstrates that path connection trained with limited clean data can repair backdoored models effectively. The study also reveals a robustness loss barrier against evasion attacks and explains the correlation between robustness loss and input Hessian's largest eigenvalue. The findings are validated across different network models. The study explores adversarial robustness of deep neural networks by analyzing mode connectivity in loss landscapes. Path connection trained with limited clean data can effectively repair backdoored models. A robustness loss barrier against evasion attacks is identified, correlated with the input Hessian's largest eigenvalue. Backdoor attacks involve poisoning the training dataset with triggers added to images. The study investigates adversarial attacks on deep neural networks by injecting errors into the model parameters to change the classification of targeted images while maintaining accuracy on other images. Two models are trained with high accuracy on CIFAR-10 dataset before injecting errors with 100% success rate. The model weights are successfully injected with 4 errors, achieving 100% accuracy for target labels. Accuracy for clean images is 78% and 75%. Prediction error rate is high at path ends due to tampered models. Path connection method finds models with good classification accuracy on triggered data. The experiment involves adding Gaussian noise perturbations to two given models at t = 0 and t = 1, testing their accuracy for clean and triggered images. The Gaussian noise has zero mean with a standard deviation based on the difference between the two models. The model performance is evaluated after training for 100 epochs using different numbers of images. Adding Gaussian noise does not necessarily change the model's robustness. The experiment involves adding noise perturbations to models at t = 0 and t = 1, maintaining robustness. Filter pruning is done using the method by Li et al. (2017) to reduce computation costs. Parameters are pruned for VGG and ResNet, followed by retraining with different image numbers. Clean and backdoor accuracies are reported. The study evaluates the error rates of clean and backdoored samples using CIFAR-10 and SVHN on the connection path against single-target attacks. Performance comparison of path connection and other methods is shown in Table A2 for CIFAR-10 (ResNet) and SVHN (VGG). Implementation details for the path connection method are provided in Table 3. The study evaluates model performance after training for 100 epochs with different methods, including fine-tuning, training-from-scratch, and random Gaussian perturbation. Two models at t=0 and t=1 are perturbed with Gaussian noise for testing accuracy on clean and triggered images. The experiment is repeated 50 times. The experiment evaluates model performance after training with different methods for 100 epochs. Perturbing models with Gaussian noise does not change robustness status. Training-from-scratch method shows lowest accuracy, especially with minimal images. Path connection achieves robustness against backdoor attacks. Performance against error-injection attacks is shown in Figure A6 for CIFAR-10 and SVHN. The experiment evaluates model performance after training with different methods for 100 epochs. Path connection achieves robustness against backdoor attacks and error-injection attacks. The resilient effect of path connection holds even when the two given models are tampered in different ways. The fine-tuning process uses 2000 images with 100 epochs to eliminate the influence of triggers on misclassification. Path connection achieves robustness against backdoor attacks and error-injection attacks, even when models are tampered in different ways. The path connection method quickly eliminates the effects of injected errors and backdoor attacks on tampered models. It shows high accuracy and robustness against different types of attacks, as demonstrated in Figures A8 and A9. The path connection method effectively counters injected errors and backdoor attacks on tampered models, showcasing high accuracy and robustness. The performance is demonstrated in Figures A8 and A9, where different settings and attack types are considered. The study explores the impact of backdoor attacks on model accuracy. Gaussian noise is added to backdoored model weights to generate noisy models, showing low clean accuracy and high attack success rate. Finding a path robust to backdoor attacks is challenging. The study investigates the challenge of finding a path robust to backdoor attacks between two backdoored models, highlighting the high failure rate of adding noise. The approach involves comparing input gradients between models on the connection path and end models to mitigate backdoor and injection attacks. The study explores the robustness of a path against backdoor attacks between two models by comparing input gradients to mitigate attacks. The average cosine similarity distance of input gradients between models on the path and end models is analyzed for backdoor and injection attacks. The study analyzes the similarity of input gradients between models on a path and end models to mitigate backdoor attacks. Clean data shows higher similarity than tampered data, indicating that sanitized models on the path can maintain accuracy while reducing adversarial effects. The attacker attempts a backdoor attack by training two poisoned models separately and then connecting them with a poisoned dataset to compromise the path. The start and end models are fine-tuned by the poisoned dataset. The attacker releases the compromised models, and the defender trains a path from them with clean data. The path is successfully compromised, resulting in less than 5% attack error rate on CIFAR-10 (VGG) dataset. The attacker's poisoned path training data led to less than 5% attack error rate on 10000 test samples. Path connection remains resilient to this advanced attack, with most models on the path repairable. Path connection method outperforms other approaches in generalization and defense performance. Leveraging path connection with one tampered model can mitigate adversarial effects in scenarios where models are close in parameter space. The \"Extensions\" paragraph and Appendix G discuss performance against path-aware single-target backdoor attack on CIFAR-10 (VGG). Different methods like path connection, fine-tune, train from scratch, noisy models, prune, and error-injection attack are compared based on clean accuracy and backdoor accuracy. Path connection method shows resilience to advanced attacks, outperforming other approaches in generalization and defense performance. The attacker can compromise the entire path by injecting errors, but path connection with bonafide data can effectively eliminate these errors. This method shows resilience to advanced attacks and outperforms other approaches in defense performance. The injected errors on the path can be countered by training the connection with the training set and using adversarial examples for testing. Adversarial training involves generating adversarial examples with the PGD method and updating model weights based on the induced training losses. Adversarial examples relate to the Hessian matrix of the loss function. The Hessian matrix describes the local curvature of a function with multiple variables. A large Hessian spectrum indicates a sharp minima, making the model more vulnerable to small input distortions. On the other hand, a flat minima with a small Hessian spectrum requires more effort for the input to leave. The largest eigenvalue of the Hessian is computed for models on the path to understand the evolution of the input Hessian. The power iteration method is used to handle high dimension difficulties in Hessian calculation. The power iteration method is used to compute the largest eigenvalue of the input Hessian, with a relative error of 1E-4. The evolution of the input Hessian is compared to the loss of adversarial examples, showing a lack of correlation with error rates of clean images. The largest eigenvalue of the input Hessian is highly correlated with the loss of adversarial examples, inspiring further exploration of their relationship. The analysis considers two models on a path and proves a lemma regarding vector norms. The path connecting different model pairs on CIFAR-10 using robust training method shows a robustness loss. The largest eigenvalue of the input Hessian is correlated with the loss of adversarial examples. To address this, perturbations are found using a projected gradient descent method within a feasible perturbation space. The text discusses finding perturbations to maximize loss and minimize expectation in training models. It shows a robust connection between different model pairs, indicating a loss barrier between non-robust and robust models. The results suggest a consistent loss barrier between these models, regardless of the loss functions used. Model ensembling is tested against evasion attacks using two independently trained CIFAR-10 models. Adversarial examples are generated based on the start or end model, and the attack success rate can decrease when using model ensembling with models on the connection path. The defense improvement of the naive model ensembling strategy against adversarial examples is not significant. The proposed path connection method shows promise in removing adversarial effects of backdooring or error-injection attacks on CIFAR-100. The performance of adversarial training on CIFAR-100 is not as significant as that on CIFAR-10. The connection of two regular models and regular and adversarially-trained models is investigated. Fine-tuning with various hyper-parameter configurations is demonstrated for CIFAR-10 (VGG). Clean accuracy and attack accuracy are analyzed through the path connection method. The performance of fine-tuning with different hyper-parameter configurations for CIFAR-10 (VGG) is analyzed. Larger learning rates can decrease attack accuracy rapidly but may degrade clean accuracy. Smaller learning rates achieve high clean accuracy but slower decrease in attack accuracy. Fine-tuning is sensitive to hyper-parameters, making it challenging to choose appropriate settings. In Figure A17, the path connection method achieves the highest clean accuracy, while lr = 0.01 maintains high attack accuracy. There is a correlation between robustness loss and the largest eigenvalue of input Hessian. The attack accuracy for lr = 0.01 remains high at about 40%, much larger than that of path connection. Multiple runs for the Gaussian noise experiment were conducted in Appendix E. In Appendix E, multiple runs were conducted for the Gaussian noise experiment, with only the average accuracy reported. The stability of the path connection method was investigated by performing one representative experiment setup with multiple runs, showing mean and standard deviation. Error bars for path connection against backdoor attack on CIFAR-10 dataset using ResNet model architecture were displayed in Figure A18. The stability of the proposed method was demonstrated through different hyper-parameter settings and training with various learning rates. Despite starting from different initializations, the performance on the path showed consistency with a small variance. The process of choosing the parameter \"t\" for the repaired model was discussed, highlighting the need for selecting an appropriate value based on general principles. Based on the stability of the proposed method and the need to choose the parameter \"t\" for the repaired model, it is suggested that users select a model with a test accuracy a \u2212 \u2206a, where \u2206a is set to 6% to eliminate the effects of attacks without sacrificing clean accuracy significantly. If unable to access the accuracy of the whole clean test set, users should use k-fold cross-validation to assess test accuracy based on bonafide data. The k-fold cross-validation method is used to split data into groups for testing and training. Additional experiments were conducted using 5-fold cross-validation for CIFAR-10 and SVHN datasets. The test error rate on a smaller validation set is less stable but generally follows trends of the clean test set. The threshold method can then be used to choose parameters for the repaired model. When using the k-fold cross-validation method, it is recommended to set a more conservative threshold (\u2206a of 10%) for model selection on a limited validation set. It is also advised to use a larger bonafide data size with k-fold cross validation, especially for data settings with 50 images showing significant performance deviation."
}