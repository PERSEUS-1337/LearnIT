{
    "title": "SkfNU2e0Z",
    "content": "Building deep neural networks for autonomous agents like robots or vehicles requires integrating time into the network's architecture. This work explores how the temporal nature of reality should be reflected in deep neural networks. Most networks process units synchronously and in parallel within layers, but layers themselves are processed sequentially. This paper introduces a class of networks that are executed in a streaming or synchronous layerwise-parallel manner, bridging the gap between biological neural networks and traditional deep neural networks. Layerwise-parallel deep networks offer a new approach to deep neural network design, allowing for parallel processing of network layers. These networks show different temporal behavior and information flow compared to traditional sequential deep networks, making them better suited for complex architectures and tasks. Challenges and tools for designing and interacting with layerwise-parallel networks are discussed. The recent advancements in deep learning have led to significant improvements in various fields like vision, speech, and reinforcement learning. Integrating time into neural network design is essential for real-time applications such as robots, autonomous vehicles, and chat-bots. Methods have been developed to enable networks to learn and represent temporal features, including short-term dynamic features and storing information over longer time periods. In this work, the focus is on the implementation aspect of neural networks and how it influences the network's temporal behavior and information integration over time. The role of time in neural networks is crucial for representing temporal features, such as optic-flow, through mechanisms like recurrent connections. The way network components are implemented, either sequentially or in parallel, also plays a significant role in temporal processing. In this paper, the focus is on the implementation aspect of neural networks and how it influences temporal behavior and information integration over time. Modern deep neural networks are constructed using collections of neurons, sometimes called layers, modules, or nodes, with neurons in the same layer computed in parallel and layers computed sequentially. The text discusses the execution of different types of neural networks, showing the difference between sequential and layerwise-parallel network execution. It illustrates the inference on four succeeding time frames and how information is processed in each type of network. The focus is on the implementation aspect of neural networks and how it influences temporal behavior and information integration over time. In this work, the concept of layerwise-parallel deep neural networks is introduced, which lies between the traditional layerwise-sequential networks. These networks have layers that remember their previous state and compute the next state based on the previous states of all preceding layers. The network architecture includes neuron-pools for current states, synapse-pools for transformations between neuron-pools, and plasticities for updating network parameters. In this work, network inference is illustrated for layerwise-sequential and layerwise-parallel networks over succeeding time frames. For layerwise-sequential networks, local frames depend on network architecture, while layerwise-parallel networks have two local frames. Information is encoded in stimuli and neuron-pool states. Introducing skip or recurrent connections in layerwise-sequential and layerwise-parallel networks can lead to different temporal behavior. Network responses vary between the two types of networks, especially when considering larger and more complex architectures. Gating mechanisms are suggested to guide content and time integration in these networks. Layerwise-parallel networks utilize gating mechanisms for content and time integration, emphasizing synchronization of parallel updated layers. This approach avoids the need for additional time input and enhances interpretability of temporal features. The network design involves constrained shallow elements parallelized across all elements, independent of architecture. Layerwise-parallel networks differ from other model-parallel approaches by allowing parallel computation of network elements like neuron-pools and plasticities. This architecture-independent parallelization avoids computation locks present in sequential deep networks, improving efficiency. Various methods have been proposed to address these locks and provide auxiliary error signals for intermediate layers. The integration of temporal information in deep networks is often achieved using recurrent neural networks (RNN), which are converted into feed forward networks for inference and training. This conversion involves network rollout and weight sharing over time to train feed forward networks effectively. The idea of network rollout tackles a symptom from layerwise-sequential networks, creating challenges like network initialization and scalability. Skip connections, like in ResNets, act as local filtering, similar to recurrent self connections. Skip connections are primarily used to mitigate backpropagation issues rather than forming early, shallow representations in abstract layers. Layerwise-parallel networks utilize skip connections to introduce temporal shortcuts, resulting in shorter response times for simple stimuli and longer response times for complex stimuli. This concept is related to ideas like BranchyNet, which enable stimulus complexity-dependent allocation of network resources. Layerwise-parallel networks introduce delays in response time based on network architecture, requiring adaptations in performance measures and training methods. This includes adjusting existing measures like accuracies and confusion matrices to consider time, and utilizing new measures like reaction times. Training for layerwise-parallel networks involves formulating errors for network outputs and back-propagating signals to update parameters. Layerwise-parallel networks introduce delays in response time based on network architecture, requiring adaptations in performance measures and training methods. Existing training mechanisms for layerwise-sequential networks can be applied to layerwise-parallel networks by using local losses and biologically inspired training rules. The underrepresentation of layerwise-parallel networks in deep learning literature is due to a lack of tools to explore this class of networks. One of the main contributions of this work is to provide an open source toolbox for evaluation and training of layerwise-parallel networks. The work introduces layerwise-parallel networks and provides an open source toolbox for designing, training, and evaluating these networks. It defines the network as a graph with vertices and edges, and describes temporal propagation for each vertex. The provided toolbox introduces restrictions on update functions for layerwise-parallel networks, including common elements of deep neural networks like convolutional layers and gated connections. The network architecture is designed with flexible elements to allow for various architectures.\u2126 t = (\u03b8 t , \u03d1 t ) denotes the network state. The toolbox introduces restrictions on update functions for layerwise-parallel networks, allowing for various architectures. Plasticities are mappings that produce parameter updates based on current and previous states. Plasticities can be computed independently and in parallel to network inference. Challenges in layerwise-parallel networks arise from information distribution across different time frames. The distribution pattern of information in layerwise-parallel networks is determined by the network's architecture. Gating neuron-pools can control the flow of information based on input stimuli. A small example network for MNIST classification uses two paths of different depths. The network consists of stacked convolutions and fully connected classification layers. Visualizations of the network architecture and information flow are provided in FIG1 and FIG2. The network architecture for MNIST classification includes two paths of different depths, with convolutions and fully connected layers. Intermediate classification results are aggregated through summation for the final prediction. Plasticities operate in parallel to neuron-pools, updating parameters based on the current time step. Plasticities compute updates locally, considering only a subset of neuron-pools at each time step. The toolbox separates plasticities into an update part and an optimizer part for training neural networks. It offers popular optimizers like stochastic gradient descent and ADAM, with the ability to add new ones. Three types of update estimators are provided for specifying parameter updates. The toolbox separates plasticities into an update part and an optimizer part for training neural networks, offering popular optimizers like stochastic gradient descent and ADAM, with the ability to add new ones. Three types of update estimators are provided to specify plasticities, including loss-based update for training layerwise-sequential deep networks. Loss-based plasticities are costly as they require the network to perform inference twice, once in neuron-pools and potentially more due to rollout in plasticity. To mitigate this, local plasticities operating on a small set of neuron-pools without deep rollout are preferred. This ensures temporal properties are maintained without introducing unintended behavior. To achieve network transparency and trainability, functional modularisation of the network is suggested. Hebbian learning rule BID7 and its variants can be used as an update estimator for synapse connections. Plasticities based on this estimator provide local plasticities compared to loss-based plasticities. Parameter update estimators can also be based on parameters, such as L1 or L2 regularization. Performance measures for layerwise-parallel networks can be derived from layerwise-sequential deep networks. Reaction times can be defined based on temporal offsets between the network's responses and previous stimuli. Time-dependent accuracy evaluation is illustrated for a 2-path network. The 2-path network from FIG1 shows time-dependent accuracy evaluation, with performance starting at chance level for the first three time steps. As stimuli are presented for 12 frames, the network reaches its highest accuracy when both short and longer paths become active. An open source toolbox in Python using Theano enables design, training, and evaluation of layerwise-parallel deep networks. The toolbox in Python using Theano allows for parallelization of operations across multiple processes and GPUs on one machine. Current deep learning literature suggests that network architectures will become more complex and heterogeneous, with functional modularized architectures increasing network understanding and trainability. Understanding internal dynamics of networks is crucial for debugging, optimizing architectures, and guiding future designs. The implementation of layerwise-parallel networks focuses on specific network architectures, prioritizing the slowest network element for overall frame rate. The toolbox aims to explore layerwise-parallel deep networks independently of architecture, which can benefit future trends in deep network design. Layerwise-parallel networks can be distributed across processes or GPUs without explicit specification. An open source toolbox is available for exploring these networks, aiming towards native model-parallel deep networks. Challenges for the future include formulating more general neuron and synapse-pools, designing new local plasticities, and creating more suitable tasks. This section describes computations in neuron and synapse pools, including features, activation, and target neuron-pools. It defines dimensions and parameters for batch samples, neuron-pools, synapse-pools, and plasticities. The section describes computations in neuron and synapse pools, including features, activation, and target neuron-pools. It defines dimensions and parameters for batch samples, neuron-pools, synapse-pools, and plasticities. The pseudocode outlines the read operations for all neuron-pools, with a focus on synchronization and alternating read and write phases. The execution phase at frame t + 1 involves computing post-synaptics for input sps, normalization, and gain adjustments. The pseudocode outlines computations for neuron and synapse pools, including normalization, gain adjustments, noise, activation, and bias. Updates for parameters from plasticities are also applied. The specification file for the layerwise-parallel example is shown in FIG1."
}