{
    "title": "HkxKH2AcFm",
    "content": "Evaluation metrics commonly used for unconditional image generation often favor models that simply memorize the training set, rather than state-of-the-art models, which is seen as problematic. To address this issue, a necessary condition for an evaluation metric is proposed: it should require a large sample from the model. Neural network divergences (NNDs) are suggested as a potential solution, as they cannot be manipulated by training set memorization, are perceptually correlated, and computable only from samples. Past work on using NNDs for evaluation is reviewed, an example black-box metric is implemented based on these concepts, and experimental validation shows it can measure generalization. In machine learning, defining a standardized benchmark task is crucial to measure progress towards complex goals. The benchmark task should have an objective evaluation procedure and require insights for improved performance, reducing the risk of inadequate baselines or evaluation mistakes. The benchmark task in machine learning should be clearly defined and reflect the difficulty of the final task. It should have an evaluation metric that measures what is important for the final goal, such as unconditional generation of natural images. This paper aims to improve evaluation practices in models like Generative Adversarial Networks (GANs) for generative modeling tasks. Unconditional image generation in modeling has proven useful for tasks like domain adaptation, disentangled representation learning, and imitation learning. Generalization is crucial for the difficulty and interest in unconditional generation, as simply memorizing training data would make the task meaningless. Some recent GAN research focuses on improving convergence properties, but generalization remains important for GANs themselves. Generalization is crucial for unconditional image generation tasks like GANs, as memorizing training data would make the task meaningless. Recent research focuses on convergence properties, but benchmarks based on perceptual quality and diversity may lead to trivial tasks that hinder progress towards useful methods. This paper discusses evaluation metrics for generative models that create nontrivial benchmarks aligned with the goal of generating diverse and realistic data. The focus is on defining benchmarks to prevent artificially high scores from undesirable solutions like memorization of training data. The paper establishes a framework for sample-based evaluation to measure generalization in generative models. It explores using neural network divergences as evaluation metrics, focusing on the \"CNN divergence\" experimentally. The goal is to detect memorization and measure diversity in generated data. Generative modeling aims to minimize divergence between distributions, with the choice of divergence reflecting the final task. Given a data distribution and a set of distributions, the goal is to find the closest distribution according to a defined closeness measure. Statistical divergence is used to characterize closeness, with estimates computed using finite samples. Generative models aim to minimize divergence between distributions by finding the closest distribution using statistical divergence measures. Evaluation criteria like Inception Score (IS) and Fr\u00e9chet Inception Distance (FID) are commonly used to assess generative models of natural images based on samples. Test set log-likelihood is another popular metric, but not directly computable for models where only samples can be drawn. In this section, a framework for sample-based evaluation is established to allow for a meaningful notion of generalization in generative models. The focus is on estimating divergences from finite samples, proposing a baseline for performance, and emphasizing the importance of generalization in minimizing divergences. The limitations of log-likelihood and the need to consider a large model sample for achieving better performance are highlighted. The text discusses the limitations of estimating divergences from finite samples in generative models. It emphasizes the importance of generalization and how a divergence estimated from a sample can be influenced by memorization. The concept of \"insensitivity to diversity\" is introduced, where differences between a distribution and its empirical counterpart can affect the estimation of divergences. The text discusses the challenges of estimating divergences from finite samples in generative models. It highlights the impact of memorization on divergence estimation and introduces the concept of \"insensitivity to diversity.\" The sample size needed to estimate a divergence is related to its sensitivity to diversity, with trivial memorization of the training set likely minimizing the divergence. However, this approach may not be beneficial for the final task at hand. The text proposes that a model should outperform trivial memorization of the training set to be considered useful. The goal is to find a model that minimizes the divergence between the true distribution and the training set, but this depends on the choice of sensitivity to diversity. If the sensitivity is not enough, very few models may outperform the training set. The text discusses the challenge of finding a model that minimizes divergence between the true distribution and the training set. It emphasizes the difficulty of achieving this with a small sample size and suggests using divergences that require a large sample size. The example of the CIFAR-10 training set containing 50 thousand images is given, but sampling 50 million images from a GAN generator is feasible. The text also mentions the assumption of an exponentially large underlying distribution, making purely sample-based evaluation insufficient to determine if the model covers the full distribution. The text discusses the challenge of finding a model that minimizes divergence between the true distribution and the training set. It emphasizes the difficulty of achieving this with a small sample size and suggests using divergences that require a large sample size. The example of the CIFAR-10 training set containing 50 thousand images is given, but sampling 50 million images from a GAN generator is feasible. The text also mentions the assumption of an exponentially large underlying distribution, making purely sample-based evaluation insufficient to determine if the model covers the full distribution. The work suggests what generalization should mean in sample-based applications and how to evaluate it, cautioning against poor estimates that can lead to unreliable results. In machine learning, the challenge lies in minimizing divergence between the true distribution and the training set. When the divergence is insensitive to diversity, the usual methods to bound generalization error may not apply. For instance, in evaluation metrics like FID, if the training set yields a near-optimal score, the purpose of generalization becomes unclear. This behavior is illustrated in Figure 1. Neural network divergences (NNDs) like BID28 and BID22 are used to compare distributions with finite samples. These divergences are defined based on the loss of a neural network trained to distinguish between samples from the two distributions. By training a critic network to differentiate between real and generated samples, NNDs can be used to evaluate generative models. After training, the critic's loss reflects the discriminability of real and generated data. NNDs are used as an evaluation metric by drawing samples from a generative model and a held-out test set. Recent work has shown NNDs can detect overfitting, with further research comparing them to existing metrics. Model evaluation through two-sample testing is also proposed, with GAN evaluation based on the MMD test. Sutherland et al. evaluate GANs using the MMD test and find that a generic kernel isn't very discriminative. BID38 show that MMD test power decreases polynomially in dimension with generic kernels. BID27 and BID5 propose methods combining MMD with a learned discriminator for evaluation. BID6 connects these methods to each other and to NNDs, proving bias in estimators. BID29 explore evaluation using a binary classifier test, considering neural nets as classifiers. Neural Network Divergences (NNDs) are used to measure diversity in neural networks by assessing their ability to overfit to data. This allows for meaningful generalization and outperforming training set memorization. The capacity of the neural network plays a crucial role in measuring diversity through NNDs. Neural Network Divergences (NNDs) measure diversity in neural networks by assessing overfitting. Capacity of the network affects divergence. NNDs incorporate task-specific knowledge. CNN-based NNDs correlate with human perception. IS and FID are also perceptually correlated. The curr_chunk discusses the use of Neural Network Divergences (NNDs) for evaluating generative models, highlighting their applicability to various datasets and tasks. It also addresses concerns about using NNDs for GAN evaluation and the potential issues of optimizing a loss too similar to benchmark metrics. The study by Im et al. investigated if the neural network divergence metric favors samples from models trained with the same metric. Despite potential bias, the metrics largely agreed, suggesting similar minimizers for objectives. NNDs may prefer generative models trained with similar objectives like GANs. Training directly against metrics like IS can produce noise-like samples with high scores. The study highlights concerns about the Inception Score (IS) being biased towards samples near the classifier decision boundary, potentially leading to overfitting. This raises doubts about using IS as a benchmark. In contrast, there is no known similar issue with Neural Network Divergence metrics (NNDs), but further research on their robustness is needed. Consistent implementation of evaluation metrics is crucial for reliable benchmarking, as varying implementation details can significantly impact the scores and hinder fair comparisons between different methods. The importance of consistent implementation of evaluation metrics, especially for Neural Network Divergence metrics (NNDs), is emphasized to avoid issues in benchmarking machine translation and music information retrieval. Using the same implementation across studies is crucial to prevent biased estimates from small test sets. The text discusses the potential bias in Neural Network Divergence (NND) metrics when evaluating models. It highlights the importance of reliable estimates and proposes a new architecture called CNN divergence (D CNN) for empirical investigation. The architecture is based on a convolutional network similar to DCGAN discriminator, aimed at addressing bias issues in model evaluation. The CNN divergence is based on a network similar to DCGAN discriminator and uses the critic objective from WGAN-GP to provide useful signal. It includes a carefully-tuned learning rate schedule and an exponential moving average of model parameters for evaluation. The goal is to develop a testbed for experimenting with adversarial divergences, not to propose a new benchmark. An example implementation of the CNN divergence is available for future work on NNDs. Experiments evaluate its ability to assess generalization. The CNN divergence, based on a network similar to DCGAN discriminator, evaluates generalization ability. Evaluation metrics like IS and FID favor memorization, while CNN divergence detects overfitting. Results show CNN divergence assigns higher score to training set memorization than GAN. The IS and FID prioritize memorizing a small sample, while the CNN divergence favors a model that imperfectly fits the distribution but covers more of its support. Various evaluation metrics are tested to measure diversity, including IS, Fr\u00e9chet Inception Distance, and the CNN divergence. A WGAN-GP BID17 model is trained on the ImageNet dataset to generate diverse images for evaluation. The evaluation metrics D(p, q) are estimated using a test set and a sample from q, along with Ep n [D(p,p n )]. The smallest n where memorizing training set images scores better than a GAN is reported in TAB1. Higher n indicates more importance assigned to diversity. CNN divergences require memorizing more images to match a GAN score, with smaller critic networks being less sensitive to diversity. A large test set is needed for accurate evaluation, as a biased estimate results from using a normal-sized test set. The study evaluates the bias in estimating evaluation metrics using different test set sizes. Results show that small test set estimates rank models similarly to large test set estimates. Training GAN models on a subset of ImageNet and evaluating CNN divergences suggest that using a small test set may be safe in this setting. The study evaluates CNN divergences on a 32x32 ImageNet dataset. Results show a decrease in divergences over training, with a significant gap between training and test set divergences. This suggests the need to pay attention to model performance on both sets. The study evaluates CNN divergences on a 32x32 ImageNet dataset, showing a decrease in divergences over training. The convergence and generalization results are related to previous works, with a key difference being the use of a black-box evaluation metric. This rules out artifacts in the critic training process and avoids overfitting issues seen in previous studies. The study evaluates CNN divergences on generative models, including PixelCNN++, ResNet VAE with IAF, and DCGAN trained with WGAN-GP objective. Results show that the GAN outperforms other models and even memorizing the training set, indicating a closer relation to the CNN divergence. The study shows that the GAN achieves a nontrivial benchmark score through generalization, indicating a promising direction for evaluating generative models. NNDs are not easily solved by memorizing the training set, emphasizing the importance of measuring generalization ability with diverse metrics. It is suggested that models should be evaluated based on their intended final task, with memorization being a valid solution for some tasks. Evaluation metrics for generative modeling should reflect the model's ability to generalize rather than just memorize the training set. Asymptotically consistent metrics like data log-likelihood and MMD are examples of such evaluation metrics. The choice of metric may not significantly impact the results, so using a generic metric like MMD could suffice for optimization. When the model is misspecified, choosing between different properties of the data distribution becomes crucial. BID47's study on generative model evaluation shows that optimizing log-likelihood, MMD, and Jensen-Shannon divergence can lead to varying results, especially in image generation tasks. In image generation tasks, optimizing log-likelihood, MMD, and Jensen-Shannon divergence can lead to varying results, cautioning against the use of \"generic\" metrics. Different generative models can score better in one metric but worse in another, highlighting the importance of evaluating models with metrics aligned with the intended downstream task. The Inception Score IS(q) is a metric used to evaluate generative models in image generation tasks. It considers the output of an Inception network trained on ImageNet, producing a distribution over labels given an image. By using CNNs as discriminators, metrics can assign greater importance to perceptually relevant properties of natural image distributions. The Inception Score is a metric used to evaluate generative models in image generation tasks. It is computed on 10 samples of size 5,000, with scores averaged for a final score. To improve on this, the Fr\u00e9chet Inception Distance (FID) was proposed, which considers the mean and covariance of feature maps from an Inception network for real and generated data. FID is more sensitive to image quality and correlations compared to the Inception Score. The Fr\u00e9chet Inception Distance (FID) is a metric that evaluates image quality by comparing statistics from real and generated images. It uses a convolutional network with specific architecture and training parameters, including the WGAN-GP objective. The CNN divergence is verified to be reliable across runs by training a single GAN on CIFAR-10 and estimating the divergence 50 times. The mean divergence value is 5.51 with a standard deviation of 0.03, indicating consistency. Different training runs of the same model may yield varying scores, attributed to the model's training process rather than the evaluation metric. For IS and FID, sample sizes of 5K and 10K are used, following past work. For VAE, PixelCNN++, and GAN models, multiple models are trained and evaluated for divergence using different sample sizes and hyperparameters. The best score overall is reported for each model."
}