{
    "title": "HylxE1HKwS",
    "content": "We propose a novel approach for efficient deep learning model deployment by decoupling model training from architecture search. Our once-for-all network (OFA) can support diverse architectural settings and quickly generate specialized sub-networks for different deployment scenarios without additional training. Additionally, we introduce a progressive shrinking algorithm to prevent interference between sub-networks during training, enabling the training of a large number of sub-networks. The once-for-all network (OFA) is a novel approach for efficient deep learning model deployment, decoupling model training from architecture search. OFA can train a large number of sub-networks simultaneously and consistently outperforms state-of-the-art NAS methods. It achieves a new state-of-the-art 80.0% ImageNet top1 accuracy under the mobile setting with reduced GPU hours and CO2 emissions. Pre-trained models and code are available at https://github.com/mit-han-lab/once-for-all. Researchers design compact neural network models specialized for mobile devices to address hardware efficiency constraints such as latency and energy consumption. Different hardware resources and deployment scenarios require varying optimal neural network architectures. While designing specialized models for each scenario is costly, compressing existing models for efficient deployment is a viable alternative. This paper introduces a new solution to tackle the challenge of designing a once-for-all network that can be directly deployed under diverse architectural configurations, amortizing the training cost. The paper introduces a solution for designing a once-for-all network that can be deployed under various architectural configurations, reducing deployment costs from O(N) to O(1). The approach allows for model selection to achieve accuracy-latency trade-offs by training only once. Inference is performed by selecting parts of the network without retraining, supporting different depths, widths, kernel sizes, and resolutions. In the model specialization stage, a subset of sub-networks is sampled to train predictors for accuracy and latency. A predictor-guided architecture search is then conducted to obtain a specialized sub-network, reducing the total cost of specialized neural network design. However, training the once-for-all network is challenging due to the need for joint optimization of weights for a large number of sub-networks. To address the challenge of training the once-for-all network efficiently, a progressive shrinking algorithm is proposed. This involves initially training the largest neural network and then fine-tuning it to support smaller sub-networks. This approach improves training efficiency by selecting important weights from larger sub-networks and distilling them into smaller ones. The effectiveness of this method was evaluated on ImageNet across various hardware platforms and efficiency constraints, showing promising results. OFA consistently improves ImageNet accuracy compared to other hardware-aware NAS methods while saving GPU hours, dollars, and CO2 emissions. It achieves a new SOTA 80.0% top1 accuracy on ImageNet mobile setting with 595M FLOPs, the first time reaching 80% accuracy. Various efficient neural network architectures like SqueezeNet, MobileNets, and ShuffleNets are proposed for hardware efficiency. Model compression, including network pruning, is another effective technique for efficient deep learning. Neural architecture search (NAS) automates architecture design, with recent methods incorporating hardware feedback for improved efficiency. However, new hardware platforms may require repeating the search process. Recent methods in neural architecture search (NAS) face challenges with new inference hardware platforms, requiring repetitive architecture searches and model retraining. This process consumes excessive GPU hours, dollars, and CO2 emissions, making it non-scalable for various deployment scenarios. To address efficiency issues, some studies have explored methods like skipping model parts based on input images, introducing early-exit branches, adaptively pruning channels, and stochastic downsampling. Slimmable Nets propose training models to support multiple width multipliers. In contrast to existing methods like Slimmable Nets, this work enables a more diverse architecture space with a larger number of architectural settings, allowing for the derivation of specialized neural networks for various deployment scenarios. The progressive shrinking algorithm is designed to train a network with flexibility in deployment scenarios. It optimizes a once-for-all network to support sub-networks of different sizes while maintaining accuracy. This approach covers four key dimensions of convolutional neural networks. The algorithm allows for flexibility in CNN architectures by adjusting depth, width, kernel size, and resolution. Units in the model have gradually reduced feature map size and increased channel numbers, with the first layer having stride 2 if the feature map size decreases. Each unit can have varying numbers of layers, channels, and kernel sizes, as well as arbitrary input image sizes. The algorithm enables flexibility in CNN architectures by adjusting depth, width, kernel size, and resolution. With various unit configurations, there are 19 different neural network architectures and 25 input resolutions. Sharing weights reduces the parameter size to 7.7M. Training the once-for-all network involves optimizing multiple objectives for each sub-network. The algorithm allows flexibility in CNN architectures by adjusting depth, width, kernel size, and resolution, resulting in 19 different neural network architectures and 25 input resolutions. Sharing weights reduces parameter size to 7.7M. Training the once-for-all network involves optimizing multiple objectives for each sub-network. A naive training approach involves sampling a few sub-networks in each update step, but suffers from accuracy drop due to interference among a large number of sub-networks. To address this, a progressive shrinking training order is introduced to the training process. The progressive shrinking (PS) training approach involves training a once-for-all network with sub-networks of different sizes in a specific order from large to small. This method gradually fine-tunes the network to support smaller sub-networks by adding them into the sampling space. The progressive shrinking (PS) training approach involves training a once-for-all network with sub-networks of different sizes in a specific order from large to small. This method supports elastic depth and width, with resolution being elastic throughout the training process. Knowledge distillation is used after training the largest neural network, combining soft labels and real labels. Channel sorting is performed to initialize smaller channel settings with important channels' weights shared. PS prevents small sub-networks from interfering with large sub-networks. The progressive shrinking (PS) training approach involves training a once-for-all network with sub-networks of different sizes in a specific order from large to small. PS allows initializing small sub-networks with the most important weights of well-trained large sub-networks, expediting the training process. The elastic kernel size concept is introduced, making the kernel size elastic by sharing centering sub-kernels. The elastic kernel size concept introduces kernel transformation matrices to share centering sub-kernels, allowing them to play multiple roles. This approach reduces the number of parameters needed and improves the performance of sub-networks. Additionally, the elastic depth method retains the first D layers of a unit with N layers, skipping the last N - D layers, to create a more efficient sub-network. The elastic width concept allows for flexibility in choosing different channel expansion ratios for each layer, supporting partial widths by reorganizing channels based on importance. This enables smaller sub-networks to be initialized with the most important channels. The elastic width concept enables flexibility in channel expansion ratios for each layer, supporting partial widths by reorganizing channels based on importance. Smaller sub-networks are initialized with the most important channels from a well-trained once-for-all network. The goal is to derive specialized sub-networks for specific deployment scenarios, optimizing efficiency constraints without the need for additional training costs. To optimize efficiency without additional training costs, a predictor-guided architecture search process is used to find specialized sub-networks. By sampling different architectures and input image sizes, an accuracy predictor and latency lookup table are trained to predict model accuracy and latency. An evolutionary search based on these predictors is conducted to obtain a specialized sub-network for specific deployment scenarios. The cost of searching with predictors is minimal, requiring only 40 GPU hours to collect data pairs. In this section, the once-for-all network is trained using the progressive shrinking algorithm on ImageNet. The effectiveness of the network is demonstrated on various hardware platforms with different latency constraints. Training details include using the same architecture space as MobileNetV3, standard SGD optimizer with Nesterov momentum, and weight decay. The full network is trained for 180 epochs with batch size 2048 on 32 GPUs. Further fine-tuning is done following a specific schedule. The training process for the once-for-all network on ImageNet takes around 1,200 GPU hours on V100 GPUs. Results show that the progressive shrinking algorithm improves the accuracy of sub-networks significantly compared to random order, achieving 74.8% top1 accuracy using 226M FLOPs under specific architectural settings. The once-for-all network achieves 74.8% top1 accuracy using 226M FLOPs under specific architectural settings, comparable to MobileNetV3-Large. Specialized sub-networks are obtained for different hardware platforms. Latency measurements are conducted on GPU platforms, CPUs, mobile devices, and FPGAs. OFA is more efficient than NAS on the mobile platform (Pixel1) due to its constant cost compared to linear costs of NAS methods. The once-for-all network (OFA) achieves high accuracy on ImageNet with faster training times compared to ProxylessNAS, FBNet, and MnasNet. OFA maintains similar mobile latency to MobileNetV3-Large and can be fine-tuned for even higher accuracy. Additionally, OFA with Progressive Shrinking (PS) achieves better accuracy, showing the effectiveness of PS. OFA also outperforms EfficientNet-B0 in terms of accuracy with similar FLOPs. OFA achieves a new state-of-the-art 80.0% ImageNet top1 accuracy with 595M FLOPs, outperforming EfficientNet-B2 with 1.68\u00d7 fewer FLOPs. It runs faster than EfficientNets on hardware, achieving 80.1% accuracy with 143ms Pixel1 latency, 0.3% more accurate and 2.6\u00d7 faster than EfficientNet-B2. OFA can produce trade-off curves over a wide range of latency constraints, impossible for previous NAS methods. It consistently improves trade-offs on various hardware platforms. OFA improves accuracy and latency trade-off significantly, especially on GPUs with more parallelism. \"OFA #25\" boosts ImageNet top1 accuracy from MobileNetV2's 60.3% to 72.6% on 1080Ti GPU. On Xilinx FPGAs, quantized OFA models outperform MobileNetV2 under all latency constraints. OFA designs specialized models for better hardware fit, improving arithmetic intensity and GOPS/s compared to MobileNetV2/MnasNet family. OFA introduces Once for All (OFA), a methodology for efficient deep learning deployment across various scenarios. It designs a single network supporting different configurations, reducing training costs significantly. Specialized models by OFA achieve higher ImageNet accuracy with similar latency on CPU, GPU, mGPU, and FPGA accelerators. Additionally, customizing for new hardware platforms using OFA does not increase training costs. The proposed progressive shrinking algorithm allows multiple sub-networks to achieve the same accuracy level without interference. Experiments on various hardware platforms validated the effectiveness of the approach using a three-layer feedforward neural network for accuracy prediction. The neural network architecture and input image size are encoded into vectors and fed into the network for accuracy prediction. In experiments, a three-layer feedforward neural network accurately predicts accuracy with low RMSE. Specialized models are analyzed on FPGA using DNNDK, with profiling results and roofline models shown. ZU9EG and ZU3EG share DRAM module with processing system, impacting memory bandwidth. Our specialized models aim to reduce memory accesses by increasing arithmetic intensity, resulting in higher performance. We implement two stages of fine-tuning to incorporate elastic width, with different epochs and learning rates for each stage."
}