{
    "title": "rkgINf1G1m",
    "content": "The isotropic loss is proposed as a new criterion, called isomax, to improve deep neural network training for multi-class classification. It penalizes intra-class features while maintaining inter-class distances, without requiring modifications to the network or training process. Extensive experiments show its effectiveness in classification and clustering tasks. Recent advancements in image classification tasks have seen significant progress with convolutional neural networks (CNN). The softmax criterion is commonly used for supervised learning in classification problems, but it has limitations. The isomax loss is proposed as a superior alternative, demonstrating robustness in classification and clustering tasks. When applying feature vectors in tasks like image retrieval or clustering, results may be suboptimal. The softmax classifier aims to separate classes but doesn't guarantee smaller distances within the same class. To improve feature extraction, approaches like adding new loss terms such as center loss have been proposed. This loss penalizes distances to class centers, enhancing class compactness, but heavily relies on center initialization and batch size. The contrastive-center loss combines center loss and contrastive loss to penalize distances within the same class and increase distances between classes. It has shown good performance on tasks like CIFAR-10 classification and LFW verification. Another approach, the triplet loss, directly minimizes distances between anchors and positive samples while maximizing distances to negative samples, instead of using softmax loss. The triplet loss aims to minimize distances between anchors and positive samples while maximizing distances to negative samples. Selecting proper triplets is crucial for the algorithm's convergence. The scalar margin and learning strategy also impact the model's performance. Isotropic normalization is proposed as a label-free approach to reshape data distribution for easy classification. In this section, a new loss called the isotropic softmax (isomax) loss is proposed to improve classification tasks using feature distances. The isomax minimizes intra-class distances and ensures inter-class separability, enhancing the effectiveness, simplicity, and portability of the method. Extensive experiments with different networks and datasets validate the approach. The softmax loss is analyzed for its limitations in tasks using feature distances. An approach to improve the feature distribution is presented, involving N training samples of n classes with image feature vectors x i and labels y i. The softmax loss is formulated using a fully connected layer output f, weights W, and bias b. The inner product of W j and x i is calculated as W j x i cos(\u03b8 j ). This formulation enhances the method's effectiveness, simplicity, and portability. The softmax classifier enforces similar \u03b8 within a class and different \u03b8 across classes, resulting in thin elliptical feature distributions. MNIST dataset, with 60,000 training samples and 10,000 testing samples of hand-written digits, is used to demonstrate this. Reducing feature vectors to 2 dimensions still maintains 98% accuracy with the softmax classifier. The network architecture is trained with 60,000 samples without data augmentation. Testing images are randomly sampled from a set of 10,000. The softmax loss leads to separability of classes but can result in small inter-class distances for samples near the center. A new approach is needed to reshape the feature distribution for better inter-class distances. The softmax loss ensures class separability but may lead to small inter-class distances. A new method reshapes the feature distribution to improve inter-class distances by optimizing the shape of each class based on the normal distribution of features. The isotropic loss reshapes the feature distribution to improve inter-class distances by penalizing features to make each class more compact. This loss function acts on data points to minimize the unbiased variance estimate of distance. It differs from feature normalization as it is applied during training, deforming the shape of each class. The isotropic loss reshapes feature distribution to improve inter-class distances by penalizing features for compactness during training. The isotropic softmax loss is formulated with softmax supervision and isotropic regularization, allowing for efficient computation within a batch. The loss function is formulated with softmax and isotropic regularization, controlled by \u03b1. Different values of \u03b1 influence the distribution of features. The loss term is differentiable to network parameters and can be minimized using various optimization methods. The isotropic loss requires no conditions on training batches or class labels. The loss function combines softmax and isotropic regularization, controlled by \u03b1. The feature distribution is influenced by \u03b1, with a threshold of 0.1 for acceptable results. A small \u03b1 quickly reduces isotropic loss, while a large \u03b1 hinders class shaping. The value of \u03b1 is typically set to 0.05. Normalization of features to a hypersphere surface, specifically a circle for 2-D features, may be an alternative approach without additional operations or loss terms during training. However, in high-dimensional space, normalizing features to a hypersphere after training may cause features from different classes to overlap on the hyperspherical surface. Isotropic loss, a part of isomax loss, aims to gather feature points around the sphere surface slowly, preventing direct stacking of features from different classes. Our method differs from feature normalization after training as softmax loss helps push different feature points away. Related works include triplet loss and center loss. Center loss helps reduce intra-class distances by gathering features of the same class together. Equation FORMULA6 defines the formulation where c yi is the center of the i-th sample class in a batch. The authors introduce the formulation DISPLAYFORM0 for learning centers in each batch, aiming to avoid perturbations but facing challenges with limited class samples. They compare the center loss and isomax loss on MNIST models, showing that unstable centers lead to unfavorable feature distribution for classification. The isomax loss proves more stable and robust, with shorter training time and an emphasis on unsupervised learning. In their algorithm, the isotropic loss is an unsupervised loss that learns distribution directly from the data without labels. The triplet loss is used to supervise the learning of an Euclidean embedding per image, minimizing intra-class distances and maximizing inter-class distances. Learning rate and method are crucial for performance. The model trained with triplet loss is sensitive to learning rate and method, with slower convergence compared to softmax loss. Results are not optimal due to the complexity of triplet combinations. Batch Normalization reduces internal covariate shift in data by transforming features with \u03b3 and \u03b2 parameters. Both batch normalization and triplet loss aim at feature distribution transformation but differ in operation. Our method normalizes Euclidean distances of feature vectors with respect to the global center of data distribution, aiming for a final feature map with small intra-class distance and large inter-class distance. In contrast, batch normalization transforms features inside a hypersphere to reduce variance of distances to the distribution center. Batch normalization is applied for the last hidden layer to replace x with the output y. It is a local normalization on a feature map to improve network training. The method focuses on global geometric supervision for better representations. Experiments are conducted on image classification, feature clustering, and face verification tasks using datasets like MNIST, CIFAR-10, ILSVRC2012 subset, and CASIA-WebFace. For image classification, feature clustering, and face verification tasks, datasets like MNIST, CIFAR-10, FRGC BID16, and LFW are used. Three models are trained under different supervision for each dataset, along with a model using triplet loss for verification task. Experiments are implemented with TensorFlow, using specific network architectures for each dataset. In designing the VGG-net BID20 architecture, the network consists of 3 cascaded convolution blocks with 4 cascaded convolution layers and a max-pooling layer in each block. Filter numbers in blocks are 64, 96, and 128. Data augmentation is not used, but a dropout of 0.8 is applied. The initial learning rate is 0.1 and decays exponentially every 1000 steps. Adagrad optimization method is used with a batch size of 128. The Inception-ResNetv1 network BID24 is utilized with an input size of 160x160 for simplicity. Random adjustments of hue, brightness, and contrast are made during training. In training, images are resized to 182x182 and randomly cropped to 160x160. Adagrad with a decaying learning rate is used, starting at 0.05 and decaying by 0.94 every two epochs. Batch size is 128. CASIA-WebFace dataset has 0.49M labeled face images from 10,575 individuals. MTCNN is used for face alignment. Two-thirds of images are randomly selected for training. RMSProp with a decay of 0.9 and = 1.0 is used for training with a batch size of 512. The model uses a batch size of 512 for center loss due to the large number of classes in the CASIA dataset. The initial learning rate is 0.1, decreased by 10 after 50 and 65 epochs. The algorithm does not affect softmax classifier performance but outperforms in k-NN classification tasks. Our proposed algorithm outperforms in k-NN classification tasks, especially for datasets with many classes. By reshaping the global distribution, our algorithm effectively reduces intra-class distance, as shown in the evaluation with center loss. The Fast Gradient Sign Method is used to test different losses on neural networks attacked by adversarial samples. The isomax loss consistently outperforms other losses, maintaining separability of classes with a compact isotropic distribution. The isomax loss shows better performance in maintaining separability of classes with a compact isotropic distribution compared to softmax and center losses. Center loss performs poorly on CIFAR-10 under adversarial perturbation. In clustering tasks, compact intra-class features are beneficial, as shown in evaluations using K-means and agglomerative clustering on MNIST, CIFAR-10, ILSVRC-sub, and face images dataset. In the study, 12,776 face images of 466 identities were used from the FRGC Phillips et al. (2005) dataset. Evaluation was done using 12 pre-trained models with different losses on 4 datasets, with NMI BID21 as the clustering metric. Results showed the algorithm's superiority. Performance was also evaluated on the LFW BID7 verification benchmark with models trained on CASIA-WebFace data. MTCNN was used to align face images on LFW dataset. Our study evaluated the verification performance of face pairs using different loss functions. Our proposed method outperformed softmax, center, and triplet loss under the same conditions. The model with center loss slightly surpassed softmax, while ours had a significant improvement. Triplet loss was also compared, but its long training process was a drawback. The study compared different loss functions for face pair verification. The proposed isomax loss outperformed softmax, center, and triplet loss. Triplet loss plateaued at 96.88% accuracy due to training data limitations. The isomax loss, combined with softmax, improved feature distribution and reduced class distance. Extensive experiments showed the effectiveness of the approach, highlighting its advantage over other methods in tasks using feature distance."
}