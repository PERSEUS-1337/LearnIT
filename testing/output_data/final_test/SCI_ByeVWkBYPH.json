{
    "title": "ByeVWkBYPH",
    "content": "In this paper, a new loss function is proposed for principal component analysis (PCA) using linear autoencoders (LAEs). The new loss function addresses the issue of the decoder matrix failing to identify the exact eigenvectors, unlike the standard L2 loss. It is proven that the new loss function ensures the decoder converges to the exact ordered unnormalized eigenvectors of the sample covariance matrix. The new loss function also guarantees that all local minima are global optima and has the same computational complexity as the classical loss. Numerical results on synthetic simulations and a real-data PCA experiment on MNIST demonstrate the practical applicability and improvements over previous LAEs. Principal Component Analysis (PCA) is a widely-used statistical tool that represents data in a new orthogonal coordinate system with uncorrelated data variance. Eigendecomposition of the covariance matrix provides this transformation, but for large datasets, other numerical methods like least squares approximation are used. Linear autoencoders (LAEs) are employed for dimensionality reduction, focusing on the largest principal components as the best data representatives. Linear autoencoders (LAEs) are a scheme for dimensionality reduction applicable to large datasets. They use a single fully-connected linear hidden layer and Mean Squared Error (MSE) loss function to discover the linear subspace spanned by principal components. However, LAEs fail to identify the exact principal directions due to a symmetry in the loss function under the action of invertible matrices. Previous research has shown the lack of identifiability of principal components in LAEs. Several methods for neural networks compute the exact eigenvectors, but they depend on specific network structures or optimization methods. Regularization causes the left singular vectors of the decoder to become the exact eigenvectors, but an extra decomposition step is still required. This work aims to recover eigenvectors from a linear autoencoder in an optimization-independent way. Analysis of the loss surface for different neural network architectures is an active area of research. In contrast to previous works extending results for shallow LAEs, this study proposes a new loss function for PCA using LAEs. The loss function ensures the decoder converges to the exact ordered unnormalized eigenvectors of the sample covariance matrix by summing squared error losses for each principal direction. This approach breaks symmetry and improves loss surface properties. The proposed loss function for PCA using LAEs ensures convergence to the exact ordered unnormalized eigenvectors of the sample covariance matrix by summing squared error losses for each principal direction. This approach breaks symmetry and improves loss surface properties, with theoretical and practical implications. The loss function L can be used for PCA/SVD on large datasets with computational efficiency similar to MSE loss. It is suitable for optimization methods like Stochastic Gradient Descent and offers parallelizability and distributability without prescribing specific algorithms. The loss function L allows for PCA/SVD computation on large datasets efficiently, fitting seamlessly into optimization pipelines like Stochastic Gradient Descent. It enables parallelizability and distributability without specifying algorithms, benefiting from the competition between optimization methods. The matrix M is defined with specific diagonal elements, denoted by A and B, representing weights in an LAE. Standard assumptions include centered input and output data with positive definite covariance matrices. The decoder matrix A is required to have no zero columns. The main result of this work is that all local minima of L(A, B) are achieved when A and B are specific forms related to eigenvectors of \u03a3. The loss function at these global minima is determined by the largest eigenvalues of \u03a3. The evaluation of L(A, B) can be done with a constant number of matrix products, independent of the value p. The underlying field in this paper is always R. The paper focuses on the analysis of positive semidefinite matrices in the context of loss functions. It introduces constant matrices Tp and Sp, with \u015cp defined as Tp^-1. The analytical gradients of the loss are derived, comparing them with the original MSE loss. The loss surface is analyzed to determine critical points based on the rank of the decoder matrix A. Various properties of the critical points are discussed, highlighting their significance in the optimization process. The paper analyzes positive semidefinite matrices in loss functions, introducing constant matrices Tp and Sp. It derives analytical gradients of the loss and compares them with the original MSE loss. Critical points are determined based on the rank of decoder matrix A, with properties discussed. The exact equations for local minima are derived through second-order analysis of the loss. Gradients with respect to A and B are calculated to find implicit expressions for critical points. The function L(A, B) is shown to be convex in the coefficients of B and attains its minimum for any B satisfying a specific equation. If A has no zero column, L(A, B) is strictly convex in B and has a unique minimum. The function L(A, B) is convex in B coefficients and has a unique minimum when B satisfies a specific equation. If A has no zero column, L(A, B) is strictly convex in B. The added computational cost of evaluating gradients for the loss function is negligible compared to MSE loss. The structure of (A, B) satisfying certain equations needs to be determined to find solutions accounting for local minima. Different cases for A being of full rank or rank less than p are considered. The main theorem is needed before stating critical points of the losses L and L. The main theorem introduces definitions of rectangular permutation matrices and eigenvalue decomposition of a matrix \u03a3. The matrix \u03a3 has eigenvectors organized into a rectangular matrix U. The matrix \u03a3 is decomposed into a rectangular matrix U with orthonormal eigenvectors. Critical points of L(A, B) are defined by matrices A and B satisfying specific conditions. If A is full rank, a squared permutation matrix \u03a0 is required for the conditions to be met. The explicit structure of the loss surface critical points is analyzed in terms of the rank of the decoder matrix A and the eigenvectors of \u03a3. The proof sketch for the theorem clarifies the claims by showing that the matrix \u03a3 can be decomposed into U \u039bU. The loss L(A, B) results in an identity where \u2206 commutes with the diagonal matrix of eigenvalues \u039b, implying that \u2206 must also be diagonal. The matrix A can be decomposed as A = U Ir CD, where U is an orthogonal matrix, Ir is a diagonal matrix with nonzero entries, C is a full rank matrix, and D is a diagonal matrix containing column norms. This decomposition is different from Baldi & Hornik's approach as C is constrained to be in R r\u00d7p with rank r \u2264 p. In our case, the matrix C corresponding to rank r \u2264 p matrix A has to satisfy specific equations. Solving for C leads to its specific structure as detailed in the theorem. When A is of rank r < p with no zero columns, the invariant matrix C is not necessarily a rectangular permutation matrix but a permutation matrix with certain properties. The global map is always valid for all r \u2264 p. The global map is always valid for all r \u2264 p, and further structure for the invariant matrix C is found when r < p. Rank deficient matrix As are identified as saddle points for the loss and should be avoided during gradient descent. The matrix C can start with a r \u00d7 k rectangular permutation matrix of rank r with r \u2264 k \u2264 p, with the remaining columns being arbitrary as long as none are zero. Corollary 1 states that for any critical pair (A, B) with rankA = r \u2264 p, the global map G := AB holds true. Theorem 2 states that for matrices A* and B* to define a local minimum of the loss function, they must be of a specific form where A* contains ordered unnormalized eigenvectors of \u03a3. The number of matrix operations needed to compute the loss is constant and independent of the value of p. The output data can have a different dimension than the input, with Y \u2208 R n\u00d7m and X \u2208 R n \u00d7m, where n = n. The covariance matrix \u03a3 is still n \u00d7 n, and the full rank assumption of \u03a3 holds for under-constrained systems with n < n. For the overdetermined case where n > n, the full rank assumption of \u03a3 can be relaxed. If p > min(n, n), the encoder A will have rank r < p. The first r columns of A converge to ordered eigenvectors of \u03a3, while the remaining columns span the kernel space of \u03a3. \u03a3 may have identical eigenvectors, resulting in non-unique eigenvectors in A. In an overdetermined case, the encoder A's rank is less than p if p is greater than min(n, n). The first r columns of A converge to ordered eigenvectors of \u03a3, while the remaining columns span the kernel space of \u03a3. The eigenvectors in A are not unique but span the respective eigenspace. The weights of networks are initialized with random numbers and Adam optimizer with scheduled learning rate is used. Training stops when one model finds all principal directions. Classical PCA approach is used to obtain the ground truth principal direction matrix. The text discusses the absolute cosine similarity (ACS) matrix to measure the distance between decoder weight matrices in a trained LAE. The ACS matrix calculates pair-wise similarity between ground truth principal directions and decoder columns. Performance metrics are evaluated using a manual tolerance threshold. In the study, Ratio TP and Ratio FP are introduced to assess correct principal directions in the decoder. Ratio Total measures the overall ratio of correctly obtained principal directions by the LAE. Synthetic and real data, including the MNIST dataset, are used for experiments. Synthetic data consists of 2000 zero-centered samples from a 1000-dimension distribution. MNIST dataset comprises 60,000 grayscale images of handwritten digits. In experiments, the dimension is reduced from 1000 to 100 for 100 desired principal components. During training, both losses converge to optimal values, but L requires more iterations to find all principal directions. Using loss L results in finding correct principal directions, with Ratio TP continuously rising to 100%. Performance of both losses in finding principal directions is shown in Figure 2. During optimization, the decoder columns align with principal directions, leading to Ratio TP reaching 100%. Some principal directions are initially misplaced, causing Ratio FP to rise temporarily. Loss L fails to identify any principal directions, keeping Ratio TP and Ratio FP at 0. The optimizer quickly finds most principal directions but requires more iterations for final ones due to close eigenvalues in the covariance matrix. In this paper, a loss function for principal component analysis and linear regression using linear autoencoders is introduced. The optimization with this loss results in the decoder matrix converging to the exact ordered unnormalized eigenvectors of the sample covariance matrix. The reconstruction performance of the proposed method is consistently better than traditional methods, as demonstrated on synthetic and MNIST datasets. The current work focuses on improving performance in scenarios where eigenvalues of principal directions are very close and generalizing the loss for tensor decomposition. Lemma 2 introduces key notations and relations necessary for the proofs, involving constant matrices Tp and Sp. Properties of Hadamard product and matrices Tp and Sp are utilized, along with the invertibility and positive definiteness of Sp. The current work discusses the properties of the symmetric tridiagonal matrix Sp, which is invertible and positive definite. It is shown that the inverse of Sp is also positive definite, leading to the conclusion that Sp is positive definite. The eigenvalues of Sp are proven to be real, nonzero, and non-negative, making Sp positive definite. The matrix Tp is obtained by setting off-diagonal elements of Sp to zero. The property of simultaneous diagonalization by congruence is discussed, where M1 is positive definite, M2 is positive semidefinite, and D, E are positive definite diagonal matrices. The rank of M2 is r plus the number of nonzero diagonal elements of E. Theorem states that for symmetric matrices M1, M2, if M1 is positive definite, there exists an invertible matrix S such that SM1S = Ip and SM2S is a diagonal matrix with the same inertia as M2. Lemma 4 establishes a critical point of L with matrices A, B, V, and W. The proof involves matrices V and W, with a focus on the second order Taylor expansion for the loss function L(A, B). By utilizing derivatives and quadratic forms, the critical points of L(A, B) for a fixed A are examined, showing convexity in the coefficients of B. The matrix \u03a3 xx is positive-definite and plays a key role in the analysis. The cost function L(A, B) is convex in matrix B for a fixed A, with critical points satisfying a specific equation. The cost becomes strictly convex if A has no zero column, leading to a unique global minimum at B = B(A). The second derivative of L(A, B) with respect to A is a positive-definite matrix, showing convexity in coefficients of A for a fixed B. The critical points of L(A, B) for a fixed B are matrices A that satisfy a certain equation. The matrix \u03a3 has an eigenvalue decomposition \u03a3 = U \u039bU, where U is a matrix of eigenvectors and \u039b is a diagonal matrix of eigenvalues. A matrix A of rank r \u2264 p with no zero column satisfies a specific equation, along with matrix B, to meet certain conditions. Based on Proposition 1 and Proposition 2, for a matrix A (with no zero column) and matrix B to define a critical point of L(A, B), B must be B(A) given by eq. (6), and A must satisfy eq. (7). The matrix \u03a0 C is a rectangular permutation matrix, and B\u03a3 xx B is diagonal. The matrix A and B together define a critical point of L(A, B), where \u2206 := U AT p (S p \u2022 (A A)) \u22121 T p A U is symmetric and positive semidefinite. The matrix \u2206 commutes with the diagonal matrix of eigenvalues \u039b, implying \u2206 is diagonal. The matrix A can be expressed as A = U IrC, where C is a full rank matrix with no zero columns. By normalizing the columns of C, A can be written as A = U IrCD. The matrix C in A = U IrCD must satisfy certain equations, including eq. (38). By setting A = U IrCD, we can evaluate B(A) in eq. (6) as xx. The matrix C must also satisfy eq. (39), while A and B must satisfy eq. (37). Solving for C in this form seems intractable due to C being a rectangular matrix. Solving for matrix C in the given equations is challenging due to its rectangular shape. To address this, a trick is used to temporarily extend C into an invertible square matrix. By utilizing diagonal matrices and specific manipulations, the equations can be simplified and solved effectively. The matrix manipulation involves collecting diagonal matrices around S to obtain D and E, both positive semidefinite. The rank of D is r with r positive diagonal elements. The submatrix \u03a0C[Jr, Nr] is nonsingular, and the columns of C[Nr, Jr] are orthogonal. The columns of C[Nr, Jr] are orthogonal, forming an orthogonal matrix. Using this, the original equations can be solved by shrinking them. The matrix T satisfies one of the conditions. \u03a0C is a rectangular permutation matrix with r nonzero columns. C[Nr, Jr] is a positive definite diagonal matrix, making C a permutation matrix with C\u03a0C = Ir. The matrix C should satisfy C\u03a0C = Ir, where C is a permutation matrix. Further analysis is not necessary except for the case where C is a square invertible matrix. In this case, the matrix \u03a0C is of full rank p, verifying certain equations for A and B. The critical points of L(A, B) are defined by equations (8) and (9) with conditions on the invariance C. The second equation is satisfied by B\u03a3xxB being diagonal. Any critical point of L(A, B) is also a critical point of L(A, B). The matrices A* and B* are critical points of L(A, B) and are the only local minima, with other critical points being saddle points. The proof involves a fourth-order tensor Hessian. The Hessian is a fourth-order tensor, and the second-order Taylor approximation of the loss is used. Critical points are defined by equations with conditions on invariance. A* and B* are the only local minima, with other critical points being saddle points. The curr_chunk discusses different cases where the critical points (A, B) are saddle points due to not having the right eigenvectors or ordering. It introduces notation and investigates each case separately using encoder direction V and decoder xx. The direction infinitesimally reduces the loss, confirming that the critical points are saddle points. The curr_chunk discusses cases where critical points (A, B) are saddle points due to specific conditions on matrices A and B. It explores scenarios with diagonal matrices and permutations, showing how certain arrangements lead to saddle points. The curr_chunk discusses the decomposition of permutation matrices and the selection of diagonal elements in the context of saddle points. It shows how specific arrangements of diagonal matrices and permutations lead to saddle points. The curr_chunk discusses the simplification of equations involving involutory matrices and diagonal elements to identify saddle points. It shows that specific arrangements of diagonal matrices and permutations lead to negative results, indicating saddle points. The curr_chunk explains that critical points not in the form of (A*, B*) are saddle points, while points in the form of (A*, B*) are local and global minima. It delves into the decomposition of permutation matrices and the ordering of diagonal elements based on permutations. The curr_chunk discusses the cancellation of terms in equation (31) by examining diagonal elements of permutation matrices. It shows that diagonal elements of \u03a0S p\u03a0 have the same value due to specific permutations, leading to the cancellation of terms. The text discusses the cancellation of terms in equation (31) by examining diagonal elements of permutation matrices. It shows that diagonal elements of \u03a0S p\u03a0 have the same value, leading to the cancellation of terms in the sum of equations. The critical points of the cost function are analyzed using first and second order Fr\u00e9chet derivatives, determining the type of critical point based on the sign of the sum of second order partial Fr\u00e9chet derivatives. The smoothness of the loss function ensures that both the Fr\u00e9chet derivative and directional derivative exist and are equivalent. Positive signs in all directions indicate minima, while saddle points have positive signs in some directions and negative in at least one direction."
}