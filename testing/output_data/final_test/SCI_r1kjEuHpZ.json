{
    "title": "r1kjEuHpZ",
    "content": "In representation learning, a new regularization approach is studied to make learned representations more interpretable and less overfitted. The approach encourages weight vectors in models to have small overlap, promoting near-orthogonality and sparsity. The regularizer is applied to neural networks and sparse coding, with experiments showing improved interpretability and generalization performance. In a clinical setting, explaining representations from DL/ML models to physicians for transparent decision-making is crucial. Addressing overfitting and improving generalization performance on unseen data are key challenges. A unified approach is proposed, focusing on parameterized weight vectors in DL/ML models for representation learning. Neural networks use multiple layers of hidden units parameterized by weight vectors. Sparse coding uses a dictionary of basis vectors for data reconstruction. Weight vectors in representation learning models are interpreted by examining top observed features. Basis vectors in sparse coding correspond to words in vocabulary for document reconstruction. Various constraints are imposed on weight vectors for better interpretability. In order to achieve better interpretability, constraints are imposed on weight vectors. These constraints include sparsity, diversity, and non-negativeness. A new perspective of interpretability called less-overlapness is proposed, which encourages weight vectors to have small overlap in supports. The less-overlapness perspective encourages weight vectors to have small overlap in supports, facilitating interpretation and reducing overfitting. A regularizer is proposed to promote orthogonality and sparsity in weight vectors, improving generalization performance on unseen data. The work proposes a new regularization approach to encourage less overlap in vectors' supports, aiming to enhance interpretability and reduce overfitting. This regularizer is applied to neural networks and sparse coding models, with an efficient ADMM-based algorithm developed for the regularized SC problem. Experimental results demonstrate the effectiveness of the nonoverlapness-promoting regularizer. The proposed regularization approach aims to reduce overlap in vectors' supports by encouraging sparsity and orthogonality. This method enhances interpretability and reduces overfitting in neural networks and sparse coding models. An efficient ADMM-based algorithm is developed for this regularized SC problem, demonstrating its effectiveness in promoting nonoverlapness. The method proposed aims to promote orthogonality among vectors by encouraging the Gram matrix to be close to an identity matrix. The log-determinant divergence is used to measure the \"closeness\" between two matrices. The method aims to promote orthogonality among vectors by encouraging the Gram matrix to be close to an identity matrix. This is achieved by combining an orthogonality-promoting LDD regularizer with a sparsity-promoting L1 regularizer, resulting in an effective promotion of non-overlapness. The relationship between the two regularizers is left for future study, as applying either L1 or LDD alone is not sufficient to reduce overlap. In a neural network with hidden layers, the LDD-L1 regularizer is applied to encourage small overlap among hidden units. The LDD-L1 regularized NN problem is defined with an objective function to promote non-overlapping supports. Sparse Coding aims to reconstruct data samples using a dictionary of basis vectors by taking a sparse linear combination of the basis vectors. L1 regularization is used to achieve sparsity among the codes, while L2 regularization is applied to the basis vectors to avoid degeneration. The LDD-L1 regularizer is utilized to encourage small overlap among the supports of basis vectors in the LDD-L1 regularized Sparse Coding problem. The LDD-L1-NNs algorithm uses subgradient descent to learn weight parameters, while LDD-L1-SC alternates between updating A and W until convergence. The problem over A is decomposed into Lasso problems, solvable by algorithms like proximal gradient descent. The problem over W is solved using an ADMM-based algorithm by alternating among W, U, and W. The LDD-L1-NNs algorithm uses subgradient descent to learn weight parameters, while LDD-L1-SC alternates between updating A and W until convergence. The problem over A is decomposed into Lasso problems, solvable by algorithms like proximal gradient descent. The problem over W is minimized by alternating among W, U, and W using PGD BID75 and coordinate descent algorithm. The update equation for U is simple, and the optimal solution for W is obtained by taking the derivative of the objective function and setting it to zero. The full gradient of the objective function in Eq. FORMULA0 is obtained and simplified to a quadratic function. The solution for \u03b3 is derived and used to find the solution for w1. Experimental results are presented for sparse coding (SC), long short-term memory (LSTM) network, and document modeling. For language modeling, LSTM network and CNN BID63 for image classification were used on 20-News and RCV1 datasets. The datasets were preprocessed by removing stopwords, converting words to lowercase, and selecting the top 1000 words with the highest document frequency. Tf-idf was used to represent documents. The LSTM experiments were conducted on the Penn Treebank dataset with 923K training, 73K validation, and 82K test words. The CNN experiments were performed on the CIFAR-10 dataset with 32x32 color images belonging to 10 categories. The dataset was augmented by zero-padding and randomly cropping the images. The study verified the LDD-L1 regularizer's ability to promote non-overlapness on the SC model. The study on the SC model and the 20-News dataset tested the LDD-L1 regularizer's ability to encourage non-overlapness. With 5 choices of regularization parameters, the overlap score consistently decreased as the LDD-L1 regularization parameter increased. Comparatively, LDD-only did not reduce overlap as the vectors remained dense and completely overlapped. The LDD-L1 regularizer promotes non-overlapness by encouraging vectors to be mutually orthogonal, resulting in lower overlap scores compared to L1. This is achieved with a regularization parameter of \u03b3=1, imposing the same sparsity level for both LDD-L1 and L1-only. In this section, the weight vectors learned under LDD-L1 regularization are examined for interpretability using SC as a case study. Representative words corresponding to the supports of the learned basis vectors are analyzed, showing distinct semantics for each vector without overlap. The visualized vectors have minimal overlap in their supports, making them easy to interpret. In this section, the weight vectors learned under LDD-L1 regularization are examined for interpretability using SC as a case study. The studies were performed on SC, LSTM, and CNN with hyperparameters tuned on the validation set. LDD-L1 was compared with LDD-only and L1-only, showing classification accuracy on test sets of 20-News and RCV1. Without regularization, SC achieves a test accuracy. The study examines the impact of LDD-L1 regularization on test accuracy and overfitting in SC, LSTM, and CNN models. LDD-L1 improves test accuracy and reduces the train/test gap compared to LDD-only and L1-only regularization. Results on 20-News and RCV1 datasets show the effectiveness of LDD-L1 in reducing overfitting. The LSTM network architecture follows a word language model in Pytorch. The LSTM network architecture in Pytorch uses a word language model with 2 hidden layers, 1500 embedding size and hidden state, tied word embedding and softmax weights, 40 training epochs, 0.65 dropout, 20 initial learning rate, 0.25 gradient clipping threshold, and 20 mini-batch size. Perplexity is used for evaluation, initialized weight parameters uniformly between [-0.1, 0.1], bias parameters as 0. Regularizers include L1 and various orthogonality-promoting methods. PytorchLM achieves a perplexity of 72.3 without regularization. The PytorchLM achieves a perplexity of 72.3 on the PTB test set. With LDD-L1 regularization, the perplexity significantly decreases to 71.1, demonstrating improved generalization performance. LDD-L1 promotes non-overlapness by simultaneously encouraging sparsity and orthogonality, resulting in lower perplexity compared to other regularizers. The regularizer can be applied to other deep learning models to potentially enhance their performance. The BID88 network has a depth and width of 28 and 10, trained using SGD with specific parameters such as epoch number, learning rate schedule, minibatch size, and dropout probability. The minibatch size is 128 with Nesterov momentum at 0.9, dropout probability at 0.3, and L2 weight decay at 0.0005. Model performance is evaluated using error rate. The proposed LDD-L1 regularizer reduces error rate on CIFAR-10 test set to 3.60% compared to 3.89% for unregularized WideResNet. LDD-L1 outperforms other regularizers by promoting non-overlapness. Other state-of-the-art methods' error rates are also compared. In interpretability research, various models have been proposed to detect influential variables and interpret predictions. A new regularization approach is introduced to encourage weight vectors to have specific properties, aiming to improve interpretability in machine learning models. In this paper, a new regularization approach called LDD-L1 is proposed to encourage weight vectors to be sparse and close to orthogonal, reducing overlap. This approach is applied to neural networks and sparse coding models, with an efficient ADMM-based algorithm developed for solving the regularized sparse coding problem. Experimental results show that this regularization technique helps alleviate overfitting and improves interpretability in machine learning models."
}