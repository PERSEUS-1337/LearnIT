{
    "title": "SyoDInJ0-",
    "content": "The paper introduces the problem of online algorithm selection in Reinforcement Learning (RL) and presents a novel meta-algorithm called ESBAS. ESBAS freezes policy updates at each epoch and uses a stochastic bandit for algorithm selection, showing near-optimality in theoretical analysis. Empirical evaluation on a dialogue task demonstrates ESBAS outperforming individual algorithms. ESBAS is further adapted to an online setting as SSBAS for policy updates after each transition. SSBAS is an online algorithm for policy updates in Reinforcement Learning, showing efficient adaptation of stepsize parameters and improved performance in tasks like fruit collection and Atari games. RL requires careful selection of models, representations, and optimization techniques due to costly trajectory collection and the need for sample efficiency. Gagliolo develops an online learning version to address the complexity of choice and the limitations of the trial and error approach. The online learning version of Algorithm Selection (AS) addresses practical problems for online RL-based systems by selecting the best algorithm at a given time, improving robustness, and uniting convergence guarantees with empirical efficiency. The online learning version of Algorithm Selection (AS) addresses practical problems for online RL-based systems by selecting the best algorithm at a given time, improving robustness, and uniting convergence guarantees with empirical efficiency. It enables curriculum learning, fair algorithm selection, and defines an objective function that is not an RL return. The reinforcement algorithms in the portfolio are assumed to be off-policy and trained on every trajectory. Section 2 provides a unifying view of RL algorithms, allowing information sharing between algorithms, and formalizes the problem of online selection of off-policy RL algorithms. Section 3 introduces the Epochal Stochastic Bandit AS (ESBAS), a novel approach. ESBAS is a meta-algorithm for online off-policy RL AS, dividing time into epochs with constant policies. It outperforms individual algorithms on a dialogue task and adapts to true online settings as SSBAS. It shows promise in a fruit collection task. SSBAS is evaluated on a fruit collection task and Q*bert, showing efficient adaptation of stepsize parameter and improved performance by running multiple DQN in parallel. The paper concludes with prospective ideas for improvement in reinforcement learning. The agent in the RL framework performs actions, receives observations, and rewards at each time step. The interaction process is non-Markovian, allowing the agent to have internal memory. The RL problem is episodic, with meta-time for algorithm selection and RL-time for observation-action-reward triplets within a trajectory. Trajectories are formalized as collections in the space E. The trajectory set at meta-time T is denoted by D T = {\u03b5 \u03c4 } \u03c4 \u2208 1,T \u2208 E T. The goal of each RL algorithm is to find a policy \u03c0 * : E \u2192 A which yields optimal expected returns. The algorithm takes a trajectory set D as input to generate trajectories with high discounted cumulative reward. An algorithm \u03b1 in reinforcement learning takes a trajectory set D as input and outputs a policy \u03c0 \u03b1 D. Trajectories can be shared between algorithms, allowing for different decision processes and state representations. Off-policy RL optimization techniques are compatible with this approach, offering flexibility in state set treatment and decision processes. The online learning approach in reinforcement learning allows for different algorithms to be evaluated during data collection, with a focus on exploration/exploitation trade-offs. Multi-armed bandit and evolutionary algorithm meta-learning are used for combinatorial search. The novel online off-policy RL problem is defined with a current trajectory set and a portfolio of off-policy RL algorithms. The online off-policy RL problem involves a trajectory set and a portfolio of algorithms. A meta-algorithm selects an algorithm at each meta-time to control the generation of trajectories in a stochastic environment. The goal is to optimize the cumulative expected return. The focus of this paper is on the meta-algorithm level in online off-policy RL. It aims to improve sample efficiency by selecting algorithms that compute policies for better task performance. Regret on task failure is prioritized over time expenditure, and policy selection in non-episodic RL is challenging. In non-episodic RL, policy selection is challenging with regrets of O( \u221a T log(T )). Regret on decision steps is hard to assess due to discounted rewards. Bandit rewards may vary with episode length. Meta-algorithms are evaluated using optimal expected return and canonical metaalgorithm definitions. Absolute pseudo-regret measures loss for not controlling trajectory optimally. In non-episodic RL, policy selection is challenging with regrets of O( \u221a T log(T )). Meta-algorithms are evaluated using optimal expected return and canonical definitions. Absolute pseudo-regret measures loss for not controlling trajectory optimally. In non-episodic RL, policy selection is challenging with regrets of O( \u221a T log(T )). Meta-algorithms are evaluated using optimal expected return and canonical definitions. Absolute pseudo-regret measures loss for not controlling trajectory optimally. In Cauwet et al., the RL AS problem is solved with a portfolio of online RL algorithms. Meta-learning for the eligibility trace parameter has also been studied. Batch AS is related to selecting the best RL algorithm for faster learning on a new task. Algorithms are considered as arms in a multi-armed bandit setting to solve the AS problem. The bandit meta-algorithm selects the algorithm controlling the next trajectory, with the aim of minimizing regret against the best expert. The exponential weight algorithms are designed for prediction with expert advice in non-stationary environments. They have been extended to handle rewards generated by stochastic processes. A recent study used the Exp3.S algorithm for curriculum learning. The approach discussed focuses on designing a stochastic algorithm for curriculum learning based on the doubling trick, as opposed to conservative algorithms for adversarial bandits. It extends stochastic bandit algorithms like UCB to handle switching bandits using a discount factor or a window to forget the past. Restless bandits assume a Markov chain governs arm rewards independently of learner interaction. This approach reduces the algorithm selection problem into several stochastic bandit problems with a doubling time horizon. The Epochal Stochastic Bandit AS (ESBAS) meta-algorithm addresses the off-policy RL AS problem by dividing the meta-time scale into epochs where algorithms' policies remain constant. This doubling trick allows for the problem to be cast into an independent stochastic K-armed bandit. The ESBAS meta-algorithm, based on UCB1, operates in epochs of exponential size where algorithms update policies at the start. Within each epoch, a new instance of the bandit problem is run, selecting algorithms for each step to minimize regret. ESBAS aims to minimize regret for not choosing the algorithm with the highest return at a given meta-time. The short-sighted pseudo-regret is defined based on the expected return differences between algorithms. Analysis is based on assumptions that more data improves algorithms, dataset ordering remains consistent, and additional samples do not change dataset rankings. The fairness of budget distribution in online algorithm selection is formalized to ensure equal resources for all algorithms. To address the issue of biased data distribution, requiring all algorithms to learn off-policy allows for information sharing between them. The fairness of budget distribution in online algorithm selection is formalized to ensure equal resources for all algorithms. Algorithms share trajectories to learn equally, preventing direct unfairness in performance. However, imbalance may still exist in exploration strategies. Three theorems show that ESBAS absolute pseudo-regret can be expressed in terms of the best canonical algorithm's regret and ESBAS shortsighted pseudo-regret, with upper bounds provided for the latter. The ESBAS short-sighted pseudo-regret is inversely proportional to the gap between the best arm and others. When the gap tends to 0, algorithm selection becomes difficult. Theoretical analysis is detailed in the supplementary material, with numerical bounds provided for a two-fold portfolio. The short-sighted upper bounds for ESBAS algorithm convergence show regret in O(log2(T))/\u2206\u2020\u221e, with the impossibility to determine the better algorithm interpreted as a budget issue. Meta-time needed to distinguish arms \u2206\u2020\u03b2 apart is \u0398(1/\u2206\u20202\u03b2) steps. If budget is insufficient, bounds are obtained by summing gaps. The ESBAS algorithm's upper bounds for convergence have regret in O(log2(T))/\u2206\u2020\u221e, with the inability to distinguish between algorithms seen as a budget constraint. The structural limit of distinguishability is the length of epoch \u03b2 at meta-time T = 2 \u03b2. ESBAS is designed for RL tasks where policy updates are costly, like in dialogue systems, favoring a growing batch setting. The ESBAS algorithm is efficient in selecting the best algorithm for each run in dialogue systems, with the ability to reach the best algorithm in each point. Algorithm selection ratios are not strongly in favor of one over the other, showing variance in trajectory set collection. ESBAS is efficient at selecting the best algorithm for each run, achieving a negative relative pseudo-regret of -90. It saves 5361 return value by not selecting the constant algorithm, but overall yields regret for not using only the best algorithm. ESBAS performs well on larger portfolios of 8 learners with negative relative pseudo-regrets: -10. It offers curriculum learning and avoids early bad policies. ESBAS avoids using constant policies to prevent unnecessary exploration of underperforming algorithms. By not resetting arms for constant algorithms, ESBAS can follow the learning algorithm's curve and achieve strong performance. The constant algorithm may hinder the natural exploration of the learning algorithm in the early stages. ESBAS adapts to a true online setting by updating policies after each transition. The algorithm, called SSBAS, trains on a sliding window of the last \u03c4 /2 selections. Despite lacking theoretical convergence bounds, SSBAS outperforms all algorithms in hyper-parameter optimization tasks. SSBAS efficiently optimizes hyper-parameters on a 5x5 gridworld problem where the objective is to collect fruits quickly. The agent has 18 positions and 15 non-terminal fruit configurations. The reward function is corrupted with Gaussian white noise, and the portfolio includes 4 Q-learning algorithms with varying learning rates. SSBAS efficiently optimizes hyper-parameters on a 5x5 gridworld problem with a reward function corrupted by Gaussian white noise. The portfolio includes 4 Q-learning algorithms with different learning rates: 0.001, 0.01, 0.1, 0.5. SSBAS selects algorithms based on their learning rates, saving transitions compared to fixed learning rates. It outperforms a linearly annealing learning rate algorithm in terms of efficiency. SSBAS outperforms Exp3.S in algorithm selection history, showing better efficiency and quality in training. It adapts the best performing learning rate over time, leading to superior performance compared to Exp3.S. In the game Q*bert, the goal is to step on each block once, with later levels requiring multiple steps on each block. Three different settings of DQN instances were used: small, large, and huge. SSBAS runs 6 algorithms with 2 random initialisations of each DQN setting. Each curve represents a single run, and improvements may be random due to the long DQN training process. SSBAS experiences a slight delay in performance during initial learning but outperforms single algorithms and previous DQN articles. The large setting is surprisingly the worst on the Q*bert task, showing the challenge of predicting the most efficient model. ESBAS, a meta-algorithm, divides the meta-time scale into epochs to select the best algorithm online from a fixed portfolio for off-policy RL learning. ESBAS is a meta-algorithm that divides the meta-time scale into epochs for off-policy RL learning. Policies are updated only at the start of each epoch, making it similar to a stochastic multi-armed bandit problem. The algorithm is designed for the growing batch RL setting and has shown efficiency in selecting the best algorithm in various experiments, outperforming the best algorithm in the portfolio through curriculum learning and variance reduction techniques. Additionally, ESBAS can be adapted to a full online setting where algorithms can update after each transition. SSBAS, a meta-algorithm, is validated on a fruit collection task and the Q*bert Atari game, showing substantial improvement over single algorithm counterparts. Its success is attributed to curriculum learning and diversified policies, selecting the most fitting algorithm based on data size and generating less redundant trajectories. The ESBAS/SSBAS meta-algorithm focuses on efficiency, robustness, and run adaptation by selecting the best algorithm for each run based on trajectory collection variance. It differs from BID23 by choosing the best performing algorithm instead of using a voting system. Portfolio design is crucial and depends on various factors. For efficient RL portfolio design, it is recommended to limit the use of highly demanding algorithms and include diverse models. Adding similar algorithms may cause inertia and hinder distinguishability by ESBAS/SSBAS. Detailed recommendations for portfolio building are left for future work. Theoretical analysis of algorithm selection for reinforcement learning, specifically Epochal Stochastic Bandit Algorithm Selection, is detailed in this section. Absolute pseudo-regret definition is formalized, comparing expected returns. AS influences return and trajectory set distributions, impacting policies. The analysis focuses on the impact of return and trajectory set distributions on policies learned by algorithms for future trajectories. Assumption 1 states that algorithms train better policies with a larger trajectory set without performance degradation on average. This assumption is crucial for policy comparison based on the relation of trajectory sets derived from algorithms. Theorem 1 provides an upper bound on the performance relation between policies trained on different trajectory sets, showing that the absolute pseudo-regret is bounded by the worst algorithm's absolute pseudo-regret in order of magnitude. This means that a meta-algorithm may be worse than the worst algorithm, but not in order of magnitude. The proof for this result is complex due to the need to control all possible outcomes. ESBAS aims to minimize regret for not choosing the best algorithm at a given meta-time \u03c4, but it is short-sighted and does not optimize algorithm learning. The short-sighted pseudo-regret is the difference between the immediate best expected return algorithm and the one selected. Theorem 2 provides an upper bound for the short-sighted pseudo-regret of ESBAS in terms of regret order. The gaps are defined as the difference in expected return between the best algorithm and algorithm \u03b1 during epoch \u03b2. The smallest non-null gap at epoch \u03b2 is noted between the best algorithm and algorithm \u03b1. Upper bounds on \u03c1 ss (T) can be deduced from Theorem 2, depending on \u2206 \u2020 \u03b2. Short-sighted pseudo-regret optimality depends on the meta-algorithm. Selecting a poor deterministic algorithm may yield no new information. Theorem 2 is a necessary step towards absolute pseudo-regret analysis. The absolute pseudo-regret can be decomposed into regret for not selecting the best algorithm, not learning as fast, and short-sighted regret. Theorem 3 provides an upper bound on pseudo-regret based on the best canonical meta-algorithm and short-sighted regret. Fairness of budget distribution ensures equal resources for all algorithms. Theorem 3 assumes fairness in learning. The ESBAS algorithm guarantees the selection of the best arm in a stochastic multi-armed bandit problem. It uses a meta-algorithm to select the best algorithm based on absolute regret. Examples of suitable algorithms include Successive and Median Elimination, and Upper Confidence Bound. ESBAS practical efficiency is demonstrated in a negotiation dialogue game involving two players. In a negotiation dialogue game, two players, the system ps and a user pu, aim to reach an agreement on 4 options with private costs. If an agreement is reached, the system's reward is based on the costs of both players. Players take turns proposing options and can face penalties for misunderstanding or failing to agree. The negotiation dialogue game involves two players, the system ps and a user pu, aiming to reach an agreement on 4 options with private costs. The system has four possible actions: REFPROP(\u03b7), ASKREPEAT, ACCEPT(\u03b7), and ENDDIAL. Speech recognition is assumed to be noisy with a sentence error rate of probability SER u s = 0.3. The simulated user always understands what the system says. The system's portfolio algorithms are limited to five non-parametric actions: REFINSIST, REFNEWPROP, ASKREPEAT, ACCEPT, and ENDDIAL. Learning algorithms use Fitted-Q Iteration with linear parametrization and \u03b2-greedy exploration. Six algorithms with different state space representations are considered. The system's portfolio algorithms are limited to five non-parametric actions: REFINSIST, REFNEWPROP, ASKREPEAT, ACCEPT, and ENDDIAL. Learning algorithms use Fitted-Q Iteration with linear parametrization and \u03b2-greedy exploration. Six algorithms with different state space representations are considered. The algorithms include fast, simple-2, fast-2, n-\u03b6-simple/fast/simple-2/fast-2, and constant-\u00b5. ESBAS is run with a UCB parameter of 1/4 and 12 epochs, with \u03b3 set to 0.9. The algorithms and ESBAS interact with a stationary user simulator built through Imitation Learning. Algorithms and ESBAS interact with a stationary user simulator using Imitation Learning. Results are averaged over 1000 runs, showing performance curves of algorithms against ESBAS portfolio control. Average algorithm selection proportions of ESBAS are plotted at each epoch. Relative pseudo-regrets are calculated, with a 95% confidence interval of \u00b11.5 \u00d7 10 \u22124 per trajectory. In practice, assumptions about trajectory sets are often violated, but this does not significantly impact experimental results. Off-policy reinforcement learning algorithms may be used, but state space representations can distort their properties. Experimental results do not show any bias related to the trajectory sets used by the algorithms. The three DQN networks (small, large, and huge) are constructed similarly with relu activations and RMSprop optimizer. Small has 2 convolution layers and a dense layer of size 128, large has 2 convolution layers and a dense layer of size 256, and huge has 3 convolution layers and a dense layer of size 512. The curr_chunk describes various combinations of layers and parameters used in a neural network model, including different types of layers and their corresponding performance metrics. The curr_chunk discusses ESBAS pseudo-regret after 12 epochs compared with different algorithms in a portfolio, showcasing performance metrics based on algorithm combinations. The curr_chunk discusses ESBAS pseudo-regret after 12 epochs compared with different algorithms in a portfolio, showcasing performance metrics based on algorithm combinations. The colour of the number indicates ESBAS performance compared to the best algorithm, with red being worse and green being better. The bluer the cell, the weaker the worst algorithm in the portfolio. Positive regrets are triggered by weak worst algorithms, but ESBAS efficiently dismisses them. Theorem 1 states that ESBAS is not worse than the worst algorithm in terms of absolute pseudo-regret. The curr_chunk provides a mathematical proof regarding ESBAS pseudo-regret compared to the worst algorithm in a portfolio. It uses mathematical induction to show that ESBAS is not worse than the worst algorithm in terms of absolute pseudo-regret. The proof concludes with a series of inequalities leading to the final result. The proof demonstrates that ESBAS pseudo-regret is not worse than the worst algorithm in a portfolio, using mathematical induction and inequalities to reach the final result. The ESBAS absolute pseudo-regret upper bound theorem states that if the best arm is selected at least T/K times in the first T episodes with high probability, then a specific meta-algorithm selects the optimal algorithm. This algorithm may not be constant and could potentially yield a smaller pseudo-regret. The pseudo-regret is evaluated based on optimal algorithm selection and algorithm return. Fairness assumption states all algorithms learn at the same rate. Probability \u03b4 \u03c4 determines if inequality 34 holds true. E * (\u03b1, N ) is the set of all sets D with |sub \u03b1 (D)|= N and last trajectory generated by \u03b1. ESBAS guarantees that all algorithms will eventually be selected an infinity of times. By recursively applying Assumption 2, it is shown that all algorithms will be selected. Additionally, by applying Assumption 1, it is demonstrated that ESBAS will try the best algorithm at least \u03c4/3K times with high probability. The theorem is proven by displaying FORM5 with \u03ba = \u03ba3 E\u00b5* \u221e \u2212 R min 1 \u2212 \u03b3, where probability 1 \u2212 \u03b4 \u03c4 and \u03b4 \u03c4 \u2208 O(\u03c4 \u22121)."
}