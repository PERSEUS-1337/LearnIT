{
    "title": "HJx0Yn4FPB",
    "content": "Recent studies have shown that deep neural networks, despite their state-of-the-art performance in visual classification, are vulnerable to adversarial attacks. To address this issue, a novel regularized adversarial training framework called ATLPA (Adversarial Tolerant Logit Pairing with Attention) has been proposed. ATLPA uses Tolerant Logit, which captures inter-class similarities at the image level, to improve accuracy on adversarial examples. ATLPA improves accuracy on adversarial examples compared to state-of-the-art algorithms. It outperforms baselines under challenging PGD attacks with high perturbation levels. Deep neural networks have been widely used in computer vision tasks, but recent studies have shown their vulnerability to adversarial attacks. Adversarial examples can deceive powerful deep neural networks with small perturbations. Adversarial training is a key defense method, training networks on adversarial images generated during training. Research is needed to improve defense methods by considering strictness vs. tolerance in handling adversarial examples. Adversarial examples can deceive deep neural networks. Research aims to improve defense methods by considering strictness vs. tolerance. Kannan et al. (2018) fit confidence distribution on clean examples' logits, called Logits Pair. Yang et al. (2018) suggest fitting Tolerant Logit, focusing on top-k classes for inter-class similarities. Limited attention to top-k classes may be more beneficial than fitting all classes' confidence distribution. In a study on adversarial images, it was found that small perturbations can cause significant noise in the attention map of a neural network. Current adversarial training methods focus on aligning clean examples with adversarial counterparts, but more needs to be done to address the differences between them. The study highlights the need to align attention maps of middle layers in neural networks, not just at the output layer. The proposed ATLPA framework encourages similarity in attention maps for clean and adversarial examples, improving accuracy on adversarial images. The ATLPA framework improves model robustness by capturing inter-class similarities and diversity among learned features. It achieves state-of-the-art defense against strong PGD attacks on various datasets. The paper discusses the ATLPA framework for improving model robustness against adversarial attacks. It evaluates the robustness of nine defenses against adversarial examples and highlights the limitations of obfuscated gradients in providing security. The paper introduces the ATLPA framework to enhance model robustness against adversarial attacks. It emphasizes the effectiveness of adversarial training in defending against adversarial perturbations. Adversarial Logit Pairing (ALP) and ATLPA aim to make logits or attention maps for pairs of examples similar, respectively. Various studies provide differing perspectives on the robustness of ALP. The ATLPA framework improves accuracy on adversarial examples by constraining attention maps for pairs of examples. It incorporates Tolerant Logit, which captures inter-class similarities at the image level. The methodology is related to deep transfer learning and knowledge distillation, with relevant works by Zagoruyko & Komodakis and Li et al. focusing on constraining the L2-norm of differences between behaviors. In this paper, the ATLPA framework focuses on defending against gray-box attacks, where the attacker knows the original network and defense algorithm but not the defense model parameters. It also considers black-box attacks, where the attacker has no information about the model's architecture or parameters. The ATLPA framework focuses on defending against gray-box and black-box attacks by adversarial training a baseline model to have similar output labels, Tolerant Logits, and spatial attention maps as the original images and adversarial images using Projected Gradient Descent. The ATLPA framework aims to defend against gray-box and black-box attacks by training a baseline model to have similar output labels, Tolerant Logits, and spatial attention maps as the original and adversarial images. The loss function includes Tolerant Logit Loss and Attention Map Loss, with hyper-parameters \u03b1 and \u03b2 balancing the two. The model focuses on a few classes with high confidence scores to capture inter-class similarities at the image level. The experiments use a non-negative weight w_k to adjust the influence of the k-th largest element of the model's logit. In experiments using K=5, Attention Map Loss is utilized to encourage similarity between attention maps of clean examples and their adversarial counterparts. The total loss is defined based on the activation layer pairs of interest, with F summing absolute values of attention maps raised to the power of p. Image-classification experiments on Flower Category and BMW-10 Databases were conducted to evaluate the defense strategy against PGD adversarial attacks. In this paper, untargeted attacks are evaluated under gray and black-box settings, and used in adversarial training. Adversarial perturbation is considered under L \u221e norm with maximum perturbation values. PGD attacker with varying attack iterations and step size is used. ResNet-101/152 are the baselines with different convolutional structures. Image-classification experiments were conducted on datasets with larger image sizes compared to MNIST and CIFAR-10. The data sets used in the study have images resized to 256 * 256 and normalized. The 17 Flower Category Database contains images of flowers in 17 categories, while the BMW-10 dataset has images of 10 BMW sedans split into training and testing sets. ResNet-101/152 models were used for image classification on these data sets. For image classification, ResNet-101/152 models trained on specific datasets are used. Two attack settings are considered: gray-box (using ResNet-101) and black-box (using ResNet-152) with different defense methods like IGR, PAT, RAT, Randomization, ALP, and FD. All defense methods are trained under the same parameters with ensemble learning. Ensemble learning among different algorithms and models is a good idea, but this study focuses on one algorithm and one model with default hyper-parameter settings. Results are presented using ATLPA on the 17 Flower Category Database, evaluated under a highly challenging PGD attack with perturbations of 0.25 and 0.5. This attack has not been explored extensively before. Our ATLPA demonstrates superior performance in adversarial robustness against gray-box and black-box PGD attacks on various datasets, outperforming prior art significantly. For instance, on the BMW-10 Database, our method achieves 61% and 62% accuracy compared to 35% and 36% accuracy of previous methods under strong 200-iteration PGD attacks. The maximum perturbation considered is \u2208 {0.25, 0.5}. Our ATLPA, including the version without Attention, shows impressive results in defending against PGD attacks. Our ATLPA model, pre-trained on ImageNet and fine-tuned on the 17 Flower Category Database, showed higher activation levels on key regions compared to other defense methods. This was demonstrated through normalized activations on discriminative parts of flowers, with ATLPA achieving the highest average activations on these regions. The ATLPA model, compared to other defense methods, showed higher activation levels on key regions for flower recognition. The main factor contributing to this is the attention mechanism. Previous work has highlighted the importance of promoting diversity among learned features to improve adversarial robustness. The training procedure of ATLPA conceals normal examples on low-dimensional manifolds in the final-layer hidden space. The ATLPA model promotes diversity among learned features of different classes, as shown by the highest silhouette score. This indicates that ATLPA enhances adversarial robustness by concealing normal examples on low-dimensional manifolds in the final-layer hidden space. The ATLPA model enhances adversarial robustness by promoting diversity among learned features of different classes. Loss plots are generated by varying input to the models, showing that ATLPA has better robustness. The proposed ATLPA framework uses Tolerant Logit to capture inter-class similarities and encourage attention maps for pairs of examples. Our ATLPA model improves model robustness by promoting diversity among learned features of different classes and encouraging attention maps for pairs of examples. It achieves state-of-the-art defense against strong attacks and enhances robustness through average activations on discriminate parts, feature diversity, and loss landscape trends. Visualization and calculations confirm the effectiveness of our method in improving model robustness."
}