{
    "title": "H1eSS3CcKX",
    "content": "NeuralSort proposes a continuous relaxation of the sorting operator output, allowing for gradient-based optimization in machine learning pipelines. This enables optimization over permutations and facilitates learning semantic orderings of high-dimensional objects. NeuralSort proposes a continuous relaxation of the sorting operator output for gradient-based optimization in machine learning. This allows for learning semantic orderings of high-dimensional objects, essential for various applications like multi-class classification, document ranking, and target tracking in computer vision. Deep neural networks can be used to learn informative representations of complex data before sorting and downstream processing. NeuralSort introduces a differentiable continuous relaxation to the sorting operator, allowing for optimization in machine learning tasks. It returns a unimodal row-stochastic matrix instead of a permutation matrix, with a temperature knob to control the level of approximation. This approach enables learning semantic orderings for various applications like classification and document ranking. NeuralSort offers a continuous relaxation for sorting, allowing for optimization in machine learning tasks. It can efficiently project any unimodal matrix to a desired permutation matrix, making it suitable for straight-through gradient optimization. Additionally, NeuralSort can be used for stochastic optimization over permutations, which is beneficial for models where permutations may be latent but directly influence observed behavior. The challenge with stochastic optimization over discrete distributions lies in gradient estimation with respect to the distribution parameters. The Plackett-Luce (PL) family of distributions over permutations is considered for ranking models, parameterized by n scores. A reparameterizable sampler based on Gumbel perturbations to the scores is derived for stochastic optimization with respect to this distribution. However, sorting these perturbed scores in the reparameterized sampler makes gradients of a downstream learning objective with respect to the scores undefined. NeuralSort approximates the objective for well-defined reparameterized gradient estimates in stochastic optimization. Tasks include sorting images of handwritten digits by unobserved labels, quantile regression for estimating the median of handwritten numbers, and learning a basis representation for the k-nearest neighbors classifier. NeuralSort improves kNN classifier performance by using a differentiable surrogate for sorting the k nearest neighbors. Permutations are represented by permutation matrices, and the sort operator maps real-valued inputs to descending order permutations. The Plackett-Luce distributions over permutations are described through a generative process where items are assigned indices based on their scores. Luce's choice axiom defines the probability of choosing an item, leading to a discrete distribution over all possible permutations. The Plackett-Luce distribution assigns probabilities to permutations based on scores. Stochastic computation graphs specify forward and backward computations in computational circuits using input, deterministic, and stochastic nodes. In this work, the authors propose extending stochastic computation graphs with nodes representing a relaxation of the deterministic sort operator. This extension allows for the inclusion of stochastic nodes corresponding to distributions over permutations in computation graphs. The goal is to optimize training objectives involving a sort operator with gradient-based methods. The authors propose extending stochastic computation graphs with nodes representing a relaxation of the deterministic sort operator. This allows for optimization of training objectives involving a sort operator with gradient-based methods. The objective involves real-valued scores, a permutation that sorts the scores, and an arbitrary differentiable function of interest. The gradient w.r.t. the scores is not defined due to the non-differentiability of the sort operator, so a relaxation is derived to create a surrogate objective with well-defined gradients. The authors propose extending stochastic computation graphs with nodes representing a relaxation of the deterministic sort operator to optimize training objectives involving a sort operator with gradient-based methods. The relaxation aims to create a surrogate objective with well-defined gradients by replacing the permutation matrix with an approximation that is differentiable with respect to the scores. The key properties sought in the relaxation are continuity, differentiability, and computational efficiency for projection back to the original discrete domain. The second requirement involves evaluating metrics and losses that need a discrete output, such as in straight-through gradient estimation. The non-relaxed operator is used in the forward pass, while the relaxed operator is used in the backward pass for gradient estimation. The example given is the 0/1 loss for binary classification, which is discontinuous, but surrogates like logistic and hinge losses are continuous and differentiable. The logistic and hinge losses are continuous and differentiable, allowing for hard binary predictions via thresholding. The sort operator maps input vectors to permutation matrices, motivated by the geometric structure of permutation matrices. This relaxation to sort is based on the subset of doubly-stochastic matrices, leading to a larger set of row stochastic matrices. In this work, a relaxation to sort is proposed that maps inputs to a subset of row stochastic matrices called unimodal row stochastic matrices. These matrices satisfy specific conditions, including an argmax permutation property, which is useful for optimization involving sorting-based losses. This approach provides a straightforward way to extract a permutation from a unimodal row stochastic matrix. The relaxation to the sort operator proposed in this work involves mapping inputs to row stochastic matrices with an argmax permutation property. This allows for extracting a permutation from the matrix, facilitating optimization with sorting-based losses. The identity in Lemma 2 helps compute the sum of the top-k elements in a vector, aiding in recovering the k-th largest element. The permutation matrix corresponding to the sorted vector is defined using absolute pairwise differences. The proposed relaxation to the sort operator involves mapping inputs to row stochastic matrices with an argmax permutation property, allowing for optimization with sorting-based losses. The continuous relaxation uses soft max instead of arg max, making it differentiable almost everywhere with respect to the elements of s. This approach is highly parallelizable and can be efficiently implemented on GPU hardware. Theorem 4 states that the continuous relaxation of the permutation matrix for an input vector is unimodal and converges almost surely. Unimodality allows for efficient projection to a hard matrix using arg max. Independent draws ensure distinct elements of the vector, and temperature controls the smoothness of the approximation. The approximation tightens as temperature decreases, impacting variance of estimates. Uncertainty in inferring permutations is expressed in latent variable models. Permutation values are represented by stochastic nodes in a computation graph for parameter optimization. The objective involves sets of parameters, permutation matrices, parameterized distributions, and differentiable functions. The stochastic computation graph is illustrated in a figure. The SCG in FIG0 deals with a distribution over permutations, obtained via Monte Carlo estimates of gradients. REINFORCE gradient estimators are used to address high variance in gradient estimates. Reparameterized samplers offer an alternative gradient estimator for reducing high variance in gradient estimates. By expressing samples as a deterministic function of parameters and fixed randomness, Monte Carlo gradient estimates can be derived efficiently. In this section, a reparameterized sampler and gradient estimator for the Plackett-Luce family of distributions are derived, involving drawing scores from underlying distributions and generating permutations based on these scores. Proposition 5 provides a method for sampling from Plackett-Luce (PL) distributions by adding Gumbel perturbations to log-scores and applying the sort operator. This reparameterization trick allows for efficient gradient estimation in Monte Carlo methods. The reparameterization trick for sampling from PL distributions involves expressing a sample as a deterministic function of scores and Gumbel perturbations. This method allows for efficient gradient estimation in Monte Carlo methods by approximating the objective using NeuralSort relaxation. Ranking algorithms based on relevance have been extensively studied in information retrieval. Listwise approaches focus on mapping objects to scores, with the RankNet algorithm maximizing the PL likelihood of pairwise comparisons. The ListMLE ranking algorithm extends this by maximizing the PL likelihood of ground-truth permutations. Differentiable pairwise approaches, like Rigutini et al., approximate comparators between object pairs. Our work explores a generalized setting where sorting operators can be inserted in computation graphs to enhance traditional pipelines. Prior works have relaxed permutation matrices to the Birkhoff polytope, a convex hull of permutation matrices also known as doubly-stochastic matrices. The BID1 proposed using the Sinkhorn operator to map square matrices to the Birkhoff polytope, interpreting the resulting doubly-stochastic matrix as marginals of a distribution over permutations. Mena et al. (2018) suggest a method where the square matrix defines a latent distribution over doubly-stochastic matrices, which can be sampled by adding Gumbel perturbations. Linderman et al. FORMULA1 propose a rounding procedure using the Sinkhorn operator to sample matrices near the Birkhoff polytope. The BID1 proposed using the Sinkhorn operator to map matrices to the Birkhoff polytope, resulting in a tractable density distribution. NeuralSort maps permutation matrices to row-stochastic matrices efficiently. The PL distribution allows for efficient sampling and tractable density estimation, making it suitable for variational inference over permutations. The Gumbel distribution is used for defining continuous relaxations to discrete distributions. The Gumbel-Softmax method was proposed for categorical variables, but it does not scale well to a large number of categories. This led to the development of Deterministic NeuralSort and Stochastic NeuralSort approaches. The large-MNIST dataset was created by concatenating 4 randomly selected MNIST images. For more details, refer to Appendix D. The dataset consists of sequences of large-MNIST images with associated labels. The goal is to predict the permutation that sorts the labels of the images. This task is an extension of sorting scalars and involves learning the semantics of high-dimensional objects. Figure 4 illustrates the sorting and quantile regression task. The model is trained to sort sequences of n = 5 large-MNIST images and regress the median value. The ground-truth permutation sorts the input sequence from largest to smallest. The model needs to learn to dissect individual digits in an image, rank them, and compose rankings based on digit positions within an image. Baselines use a CNN to map each large-MNIST image to a feature space. The study compares different methods for mapping large-MNIST images to a feature space and evaluating the accuracy of predicted permutations. The proposed sorting relaxation approach outperforms baseline methods for all considered scenarios. The study compares different methods for mapping large-MNIST images to a feature space and evaluating the accuracy of predicted permutations. Sorting relaxation significantly outperforms baseline approaches for all scenarios considered, with deterministic and stochastic variants showing comparable performance. The poor performance of Sinkhorn baselines may be due to their design for matchings, which do not necessarily satisfy Luce's choice axiom or imply a total ordering. In an experiment extending the sorting task to regression, each sequence contains n large-MNIST images, with the regression target being the 50th quantile of the labels. The study explores different methods for sorting large-MNIST images and predicting permutations. Baselines include Sinkhorn, Gumbel-Sinkhorn, Constant, and vanilla neural net (NN). The task involves learning a representation for sorting high-dimensional inputs and approximating the label through regression. Baselines like Constant return the median of possible outputs, while NN maps input images to a real-valued prediction for the median. The study evaluates different sorting methods for large-MNIST images and predicting permutations. Baselines include Sinkhorn, Gumbel-Sinkhorn, Constant, and vanilla neural net (NN). Results show that the proposed NeuralSort approaches outperform other methods on mean squared error (MSE) and R 2 metrics. The experiment involves designing a fully differentiable, end-to-end k-nearest neighbors (kNN) classifier to learn a representation of data points before evaluating the k-nearest neighbors. The differentiable kNN algorithm involves hyperparameters such as the number of training neighbors, top candidates, and sorting temperature. Query points and candidate nearest neighbors are randomly sampled from the training set. The loss function optimizes for a representation space to find the top-k candidate points with the minimum Euclidean distance. The differentiable kNN algorithm involves hyperparameters like the number of training neighbors, top candidates, and sorting temperature. It optimizes for a representation space to find the top-k candidate points with the minimum Euclidean distance. The training objectives for Deterministic and Stochastic NeuralSort are given as minimizing a certain function. NeuralSort involves Stochastic optimization on benchmark datasets like MNIST, Fashion-MNIST, and CIFAR-10. Baselines include kNN with different representation spaces and distance metrics. Results show classification accuracies on standard test sets. NeuralSort introduces a continuous relaxation of the sorting operator for efficient gradient estimation in computation graphs. It outperforms prior work in end-to-end learning of semantic orderings on tasks like fully differentiable k-nearest neighbors. Future work will explore alternate relaxations to sorting and new applications. In the future, we aim to explore alternate relaxations to sorting and extend widely-used algorithms like beam search. Both deterministic and stochastic NeuralSort are easy to implement, with reference implementations in Tensorflow BID0 Proof. For any value of \u03bb, certain inequalities hold, concluding the proof by considering values of \u03bb that minimize the sum in Lemma 2. This is proven for the case where all values of s are distinct. The theorem's properties are proved independently. The softmax function is defined with positive entries summing to 1. It satisfies the argmax permutation property. The function can be equivalently defined as soft max(z/\u03c4) = arg max x. The proof involves distributional assumptions and a generalization of the memoryless property. The induction process is completed by showing the property of argmin of exponential distributions. Applying a strictly decreasing function to the identity leads to implications from the Gumbel distribution. The arg max operator is defined for vectors with ties in the context of the proposed relaxation. The arg max operator is efficiently computable with additional bookkeeping. For an input vector s with the sort permutation matrix P sort(s), s j1 = s j2 if and only if there exists a row i such that P[i, :]. This is proven by showing that if j1 \u2208 arg max set(P[i, :]), then j2 is also an element, and vice versa. The proof shows that each row in the matrix P sort(s) has a distinct arg max element, ensuring uniqueness in the arg max of each row. This is achieved by considering the tie-breaking protocol and the pigeon-hole principle. The experiments were conducted using Tensorflow BID0 and PyTorch. Code snippets for implementation were provided in both frameworks. Standard splits of MNIST and CIFAR-10 datasets were used for sorting and quantile regression experiments. Baselines included REINFORCE based estimators. The arg max of each row in the matrix P sort(s) is distinct, ensuring uniqueness. The experiments used Tensorflow BID0 and PyTorch, with standard MNIST and CIFAR-10 datasets. REINFORCE estimators were found to be worse than other baselines. The convolutional network architecture was consistent across sorting methods, with differences in how scores were combined to output a row-stochastic prediction matrix. NeuralSort methods used a fully-connected layer for mapping image representations to scalar scores, while Sinkhorn methods utilized a different approach. The experiments used Tensorflow BID0 and PyTorch with standard MNIST and CIFAR-10 datasets. Different methods were employed to map image representations to prediction matrices, including Sinkhorn-based methods and Vanilla RS baseline. The final output of all methods was row-stochastic n \u00d7 n matrices. The loss function used was row-wise cross-entropy loss. In the experiment, row-stochastic n \u00d7 n matrices were used as the final output. The loss function employed was row-wise cross-entropy loss. Hyperparameters included an Adam optimizer with a learning rate of 10 \u22124 and a batch size of 20. The temperature \u03c4 was tuned on the set {1, 2, 4, 8, 16} for the Sinkhorn-based and NeuralSort-based approaches. The log-variance in gradient estimates was reported in FIG8 as a function of temperature \u03c4. Higher temperatures led to lower variance. Higher temperatures result in lower variance in gradient estimates. The quantile regression experiment used a neural network architecture with three fully-connected layers for 4-digit numbers instead of 5-digit numbers. The network mapped CNN representations to a single-unit estimate of the median. In experiments, a neural network architecture mapped CNN representations to estimate medians and ranks using methods like Gumbel-Sinkhorn or NeuralSort. Point predictions were obtained by multiplying matrices and vectors, minimizing loss between predictions and true medians. Adam optimizer with a learning rate of 10^-4 was used, and temperature \u03c4 was tuned. Scatter plots in FIG9 show true vs. predicted medians on test points from the large-MNIST dataset. Increasing n led to averaged predictions across samples for stochastic NeuralSort. For stochastic NeuralSort, predictions are averaged across 5 samples. Increasing n concentrates the distribution of true medians, making prediction easier but more challenging as the model learns semantic sorting across a larger set of elements. The R2 values show a slight dip as n increases. Different architectures were used for baseline kNN implementations and autoencoder baselines. ResNet18 architecture was used for Fashion-MNIST and CIFAR experiments with NeuralSort. For the experiment, hyperparameters were set for an SGD optimizer with momentum of 0.9, batch size of 100, and varying values for temperature, learning rate, and number of nearest neighbors. The best model was evaluated on the test set, suggesting potential accuracy improvements with a more extensive hyperparameter search and refined learning rate schedule. Performance of Deterministic and Stochastic NeuralSort for different k values in the differentiable k-nearest neighbors algorithm was analyzed. The goal was to learn a predictor for y given x from a finite dataset. The training objective optimized by Deterministic and Stochastic NeuralSort involves learning a predictor for y given x in sorting and quantile regression experiments using a dataset of sequences of large-MNIST images and their corresponding permutations. Each datapoint consists of an input sequence x with n images and the desired output label y representing the sorting permutation. The goal is to minimize the average multiclass cross entropy error between the true permutation matrix and the predicted permutation. The cross entropy error is calculated between the true permutation matrix and a predicted permutation based on a CNN model. The dataset consists of sequences of large-MNIST images with only the median element values given. The goal is to minimize the mean-squared error between the true median and the predicted value. For NeuralSort approaches, the objective is to optimize the mean-squared error between the true median and the predicted value using a CNN model. The predicted permutation matrix is used to extract the median image, which is then regressed to a scalar prediction for the median using a neural network."
}