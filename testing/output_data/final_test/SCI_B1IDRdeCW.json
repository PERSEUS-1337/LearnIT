{
    "title": "B1IDRdeCW",
    "content": "Recent research has shown that neural networks with binary weights and activations can effectively capture features in data by utilizing high-dimensional geometry of binary vectors. This method works because ideal continuous vectors in intermediate representations of these networks are well-approximated by binary vectors, preserving dot products. This explains why binary neural networks work in terms of high-dimensional geometry, unlike previous research that focused on classification performance. Our work explains the effectiveness of Binary Neural Networks (BNNs) in terms of high-dimensional (HD) geometry. The results and analysis on BNNs can be applied to neural networks with ternary weights and activations, serving as a foundation for understanding various methods to compress traditional neural networks. This understanding can also be extended to other neural network architectures like recurrent neural networks. Researchers are exploring the application of deep learning in resource-limited hardware, such as neuromorphic chips and smart phones, due to the decreasing cost of computation. The effectiveness of Binary Neural Networks (BNNs) is explained in terms of high-dimensional geometry, allowing for efficient training with binary weights and activations. BNNs execute 7 times faster using a dedicated GPU kernel at test time and require significantly fewer memory accesses, resulting in energy savings. Key ideas include replacing costly floating point multiplications with cheap binary operations. The effectiveness of Binary Neural Networks (BNNs) is explained in terms of high-dimensional geometry, allowing for efficient training with binary weights and activations. BNNs execute 7 times faster using a dedicated GPU kernel at test time and require significantly fewer memory accesses, resulting in energy savings. Key ideas in their papers include continuous weight associated with binary weight and replacing non-differentiable binarize function during backpropagation. This allows training of neural networks with binary weights and activations using stochastic gradient descent. The binary weight transformers in neural networks have forward and backward functions, with continuous weights associated with binary weights. The backward function uses a smoothed version of the forward function due to non-differentiability. This approach allows for training with binary weights and activations using stochastic gradient descent. The angle between a random vector and its binarized version converges to arccos 2/\u03c0 \u2248 37\u2022 as the dimension goes to infinity. This property is present in weight vectors of a network trained using the method of Dot Product Proportionality. Weight-activation dot products are approximately proportional under binarization, indicating continuous weights in the method are not just a learning artifact. The continuous weights obtained from the BNN training algorithm are an approximation of weights learned with continuous training. The computations in the first layer of the network differ from the rest due to correlations in the data, leading to high variance principal components. It is recommended to use a continuous convolution for the first layer to embed the image in a high-dimensional binary space. A GBT can be useful for embedding low dimensional data in a high-dimensional binary space. The analysis also applies to ternary neural networks. The angle between a random vector from a standard normal distribution and its ternarized version predicts the distribution of angles in a CIFAR10-trained network. The dot product proportionality property holds for ternary neural networks. Researchers aim to reduce computational costs for neural networks used in tasks like IMAGENET object recognition. Approaches include compressing pre-trained networks using methods like Tucker decomposition and pruning low magnitude connections. Researchers have explored various methods to reduce computational costs for neural networks, including pruning low magnitude connections and training networks with quantized weights and activations. Some approaches involve using low precision floating point or fixed point numbers for cheaper multiplications, while others focus on techniques like weight sharing quantization and Huffman coding. Additionally, researchers have experimented with training networks with binary weights and activations, as well as using scalar times binary matrix approximations for weight activation dot products. Researchers have explored methods to reduce computational costs for neural networks, including training with quantized weights and activations. Some approaches involve using low precision numbers for cheaper multiplications, while others focus on techniques like weight sharing quantization. BID studies focus on networks with binary or ternary weights, and quantizing backpropagation. Analysis of internal representations helps understand compression methods. BID0 found that feature magnitudes in higher layers do not significantly impact classification performance. BID7 observed benefits of binarizing features in intermediate layers of a CNN. In their study, BID7 found that binarizing features in intermediate layers of a CNN with backpropagation leads to minimal image distortion compared to dropping out features. They further investigate the internal representations of neural networks with binary weights and activations, conducting experiments on CIFAR-10 and MNIST datasets with similar results. The CIFAR-10 convolutional neural network used in the experiments has six layers of convolutions with specific spatial kernels and feature maps. In their study, BID7 found that binarizing features in intermediate layers of a CNN with backpropagation leads to minimal image distortion compared to dropping out features. The network architecture includes six layers of convolutions with specific spatial kernels and feature maps, followed by max pooling and fully connected layers with batch norm. Experiments using ternary neural networks also follow the same architecture, analyzing angle distributions of high-dimensional binary vectors crucial for understanding binary neural networks. In high-dimensional spaces, binarization of vectors does not significantly alter their direction. The distribution of binary vectors relative to continuous vectors is studied, showing that binarizing a continuous vector results in the closest binary vector in terms of angle. Binarizing continuous vectors in high dimensions changes their direction by approximately 37 degrees, providing insight into the success of binary neural networks. This change is significant compared to low-dimensional intuition, where randomly chosen vectors are approximately orthogonal in high dimensions. In high dimensions, vectors are nearly orthogonal, making a 37-degree angle rare. A multilayer binary CNN trained on CIFAR10 shows a close match between experimental results and theory on angles between binary and continuous weights. The distribution of these angles peaks near arccos 2/\u03c0, with a slight deviation towards larger angles in higher network layers. Ternary neural networks are also considered. In high dimensions, vectors are nearly orthogonal. A multilayer binary CNN trained on CIFAR10 shows a close match between experimental results and theory on angles between binary and continuous weights. Ternary neural networks are also considered. The Dot Product Proportionality (DPP) property states that the dot products of activations with prebinarization and post-binarization weights are highly correlated. The proportionality constant in binary neural networks depends on weight magnitudes and cosine angles. The modified gradient in BNN training acts like traditional backpropagation for continuous weights. Optimization is simplified due to dot product structure. Dot products of activations with pre-binarized weights are highly correlated. The analysis focuses on transformers in the network where forward and backward propagation functions are not typically related in gradient descent. It involves neural network tensors u and v, derivatives \u03b4u and \u03b4v, loss function L(x), and forward propagation functions f and g. The computations involve derivatives of the cost function and modifications in training under normal conditions. In a modified backpropagation scheme, computations are done to ensure \u03b4u remains the same. This observation is applied to the binarize transformer in the network. The sufficient condition for this is derived as dot products followed by batch normalization, which is the DPP property. The DPP property is crucial in learning dynamics, where the second derivative can bound the error between gradients. BNN learning dynamics approximate continuous weight training, implying the DPP property is ideal. Stochastic binarization also upholds the DPP property. The DPP property holds in deterministic binarization, showcasing the fundamental nature of neural network representations. The binarize block also follows the DPP property, empirically verified. The impact on classification is discussed. The binarization of weights in convolutional layers has a minimal impact on classification performance, with only a 3 percent drop when removed. Removing weight binarization in the first layer accounts for the entire drop, while other layers show no degradation in performance. Dot product histograms can be used to study the effects of binarization on weight-activation interactions. The impact of removing binarization of activations on classification performance is significant, as it removes the main non-linearity of the network. Some distributions are more affected by binarization than others, requiring the network to adapt its internal representations to mitigate degradation. The first layer of the network faces challenges in training due to the orientation of principal components relative to binarization. The Generalized Binarization Transformation (GBT) is defined as a rotation matrix R applied to a column vector x with a pointwise binarization function \u03b8. The rotation changes the distribution being binarized, with strategic choices minimizing changes in input vector directions. The angle between a vector and its binarized version is dependent on the dot product x \u00b7 \u03b8 R (x), impacting classification performance significantly. The Generalized Binarization Transformation (GBT) involves applying a rotation matrix R to a vector x with a binarization function \u03b8. The dot product x \u00b7 \u03b8(Rx) affects classification performance, with changes in correlation observed when replacing binary weights with continuous weights. In the context of GBT, the correlation is weaker in the first layer, and as the dimension approaches infinity, the angle between vectors changes. Binarization of vectors from a non-isotropic Gaussian distribution is destructive, but a random rotation can compensate for errors. In a binary neural network, the first layer weights must approximate important directions in activations, such as Gabor filters in natural images. The input features in the network are not randomly oriented, with the first layer capturing spatially uniform colors. Large images like those in IMAGENET face similar issues with translation and scale invariance affecting the principal components. Trained networks show lower dot product correlation in the first layer compared to other layers. Randomly permuting activations reveals this issue. The study involves randomly permuting activations to generate a distribution with independent joint statistics. This transformation affects the correlation between weight vectors, showing that correlations observed are not solely due to binary and continuous weight vectors. The study demonstrates that correlations between weight vectors are not solely due to binary and continuous weight vectors, but also to high variance directions in the data. The shuffling operation in the first layer shows that binary weight vectors are not well-aligned with continuous weight vectors. Previous research suggests that compressing the first layer weights has a significant impact on performance. Neural networks with binary weights and activations show similar performance to continuous counterparts but with reduced execution time and power usage. Binarization of high-dimensional vectors preserves direction and weight-activation dot products, making it a viable network compression technique. When using network compression techniques, analyzing weight activation dot product histograms can help identify layers affecting performance. The low effective dimensionality of data on the first layer can be addressed by using continuous weights or a Generalized Binarization Transformation. Ternary weights and activations in neural networks can also be understood using this approach. Network compression techniques involve transforming weights and activations to reduce execution cost without performance degradation. Random n-dimensional vectors are drawn from a rotationally invariant distribution, and comparisons are made between angles of vectors. A rotationally invariant distribution can be factorized into a pdf for vector magnitude and a distribution on angles. Lemmas are used to calculate expectations, with computations done using a Gaussian distribution. The integral factorizes and terms are independent of \u03c1 0 except the integral over \u03c1. Taking the derivative with respect to \u03c1 0 gives g(\u03c1) \u223c (1 \u2212 \u03c1 2 ) (n\u22123)/2. The normalization constant is a beta function that evaluates to the desired result. The distribution of angles between two random vectors is discussed, with one vector assumed to be (1, 0, 0, . . . 0) for simplicity. The exact distribution of \u03c1 is given by Lemma 1, noting that -E(\u03c1) = 0 due to distribution symmetry. The variance of \u03c1 is calculated using Lemma 2, leading to the desired result. The calculation involves changing variables and integrating over a volume element, resulting in the variance being \u03c0n. In this subsection, the learning dynamics for the BNN training algorithm in a simple case are examined. Regression is considered with a binary linear predictor, using a squared error loss. The angle between a vector and a binarized version of that vector converges to a very small angle in high dimensions. The result is 2\u03c0*n. In a simple case, the learning dynamics for the BNN training algorithm are examined using binary linear regression with a squared error loss. The equation for back propagating through the binarize function is derived, and the stable point of the equations is when w = \u03b1 in the special case of binary weight linear regression. In binary weight linear regression, the learning dynamics involve annealing the learning rate to freeze the state of fluctuating vector components. Steps are taken based on the sign of w, with a maximum magnitude constraint. The equation C yx \u2248 wC xx is easier to satisfy in high dimensions. In binary weight linear regression, the learning dynamics involve balancing steps based on the sign of the weight vector to achieve equilibrium. When the weight vector is in equilibrium, the expected value of the function is determined by the fraction of positive and negative weights. The importance of normalization techniques like batch normalization is highlighted when the dynamics diverge. The correlation between weight vectors A and B can be determined by the cosine angle between them when activations are randomly permuted. The cosine angle between weight vectors A and B is analyzed in a binary CNN trained on CIFAR-10, showing a deviation towards larger angles compared to theoretical expectations. Ternarization, quantizing activations to three values, is suggested as a more natural method than binarization for neural network weights. Ternary neural networks use a quantization function with a ternarization threshold to classify images in CIFAR-10. The network adapts the standard deviation of activations to the threshold value. Approximately 10 percent of weights and 2 percent of activations were zero, indicating the network did not ignore zero weights during the learning process. Further research is needed to optimize the use of zero weights. The empirical distribution of angles between continuous vectors and their ternarized counterparts is highly peaked at the predicted value. Random vectors are chosen from a standard normal distribution and quantized using different values of a. The peak angle varies substantially as a function of a, with a \u2248 0.11 for higher layers. The theoretical prediction for the peak angle closely matches the empirical value. Theoretical prediction closely matches empirical results for peak angle in ternary neural networks. Ternarization threshold chosen to match trained network, causing small angle change in high dimensions. Ternarization preserves dot products between weights and activations in high dimensions, showing a large variation in angle over different thresholds."
}