{
    "title": "BylctiCctX",
    "content": "In the domain of intuitive physics, the study focuses on visually predicting stability of block towers in deep learning. The framework of neural stethoscopes is introduced to quantify the importance of specific factors in deep networks and actively influence information. The analysis of a deep neural network for stability prediction reveals susceptibility to incorrect visual cues, leading to performance breakdown. The study explores how neural networks can be misled by incorrect visual cues, affecting performance in stability prediction tasks. Using stethoscopes to enhance feature extraction significantly improves prediction accuracy. Training on datasets with different correlations between visual cues and stability levels highlights the importance of debiasing techniques. Intuitive physics in deep learning emphasizes the acquisition of physical understanding through data-driven approaches. In contrast to rule-based approaches, neural networks may develop a form of physical understanding based on the scenarios and tasks they are exposed to. This paper focuses on assessing whether networks use visual cues as shortcuts that may not align with the underlying laws of physics they are meant to learn, specifically in stability prediction tasks for block towers. The text discusses the use of neural networks in predicting the stability of block towers. It compares end-to-end learning and simulation-based approaches, highlighting the need to understand if neural networks consider physical principles like center-of-mass or rely solely on visual cues. A new dataset is introduced to analyze the impact of visual cues on the learning process. Neural stethoscopes, consisting of an encoder and decoder, predict global stability of block towers. A two-layered perceptron called the stethoscope predicts local stability using a learned feature Z from the main network. The stethoscope loss is back-propagated with a weighting factor \u03bb, determining its operation mode. This framework allows for the interrogation and perturbation of task-specific information at any layer. Neural stethoscopes predict stability of block towers using an encoder and decoder. An auxiliary or adversarial training mechanism can promote or suppress specific information without changing the main network. This approach improves network performance and can mitigate biases. Additionally, an extension to the ShapeStacks dataset is introduced for intuitive stability prediction. A framework for interpreting, suppressing, or promoting extraction of features specific to a secondary task unifying existing approaches from interpretability, auxiliary, and adversarial learning in the context of intuitive physics. This work touches on intuitive physics and targeted model adaptation/interpretation in deep learning, specifically in vision-based stability prediction of block towers. Pipelines using computer vision for abstract state representation in robotic stone stacking show impressive results. End-to-end learning and pipeline methods using a physics simulator share the state-of-the-art, with interpretability being a key advantage for model trust and improvement. Occlusion-based attention analyses reveal focus on stability violations in algorithms. The study highlights limitations in the network's understanding of physical principles despite detecting stability violations in 80% of cases. Attention analyses can be misleading, as shown by Grad-CAM visualizations remaining unchanged even with adversarial examples. The concept of neural stethoscopes involves influencing feature representations in hidden layers, connecting interpretability, auxiliary losses, multitask learning, and adversarial training. Existing deep learning sub-domains already have the necessary tools for neural stethoscopes. Both deep supervision and linear classifier probes reinforce the original loss signal at various levels in a network stack. Regularisation techniques are used to encourage representations invariant to input signal components. Methods for domain adaptation and preventing model fitting to specific nuisance factors are also proposed. BID13 and BID24 address nuisance factors by minimizing the Maximum Mean Discrepancy between conditional distributions with different values of a binary nuisance variable. BID24 uses adversarial training to optimize an encoding to confuse an additional discriminator. BID14 applies the discriminator to the output of the main model. This approach assumes the nuisance variable is known and is an input during training and deployment. The framework can be applied to any set of tasks in supervised deep learning. In supervised deep learning, a framework is used to map input to target output through intermediate representations. The framework can be applied to classification tasks with the introduction of supplementary tasks to improve network performance. Neural stethoscopes are proposed to analyze the impact of supplementary tasks on primary tasks in deep learning frameworks. The stethoscopes help improve network performance by introducing auxiliary and adversarial losses to extract features related to the supplementary task. Loss functions are defined to measure prediction accuracy and performance on the supplementary task, with stethoscope weights updated accordingly. Neural stethoscopes introduce auxiliary and adversarial losses to improve network performance by updating stethoscope weights based on different values of \u03bb. Analytic Stethoscope (\u03bb=0) uses stethoscope as a passive observer to interrogate feature representations. Auxiliary Stethoscope (\u03bb>0) enforces correlation between main network and supplemental task. Adversarial Stethoscope (\u03bb<0) maximizes stethoscope loss to train the encoder. The stethoscope is trained with \u03bb=0 to maximize stethoscope loss, promoting independence between main network and supplemental tasks. The stethoscope architecture must have constant capacity and access each input neuron separately, achieved by fully connecting input to the first layer with a sparse matrix. In auxiliary and adversarial mode, the stethoscope is attached to the main network's last layer before the logits in a fully connected manner. The stethoscope is implemented as a two-layer perceptron with ReLU activation and trained with sigmoid or softmax cross-entropy loss on its task S. The loss of the encoder in the adversarial setting is rewritten for numerical stability, with the objective similar to confusion loss formulation in GANs to prevent vanishing gradients. Previous work has shown neural networks are capable of learning physical tasks like stability prediction. Neural networks can learn physical tasks such as stability prediction, but it is challenging to assess their reasoning and understanding of physical principles. Occlusion-based attention analysis provides limited insights. A study on visual stability prediction of block towers explores the network's learning behavior by manipulating feature extraction. The study explores how promoting the extraction of certain features affects the performance of neural networks in predicting the stability of block towers. Global stability is defined as the center of mass of the entire tower being supported by the contact area, while local stability is defined as the center of mass of each block being supported by the contact area. Binary prediction tasks are associated with global and local stability, where stability is labeled as 0 and instability as 1. The study focuses on global and local stability in predicting block tower stability. A simulated dataset with 4,000 block tower scenarios is created, divided into easy and hard subsets based on the correlation between local and global stability. The Inception-v4 network is chosen for stability prediction, trained using image examples with global and local stability labels. Classification losses use sigmoid cross entropy. The study examines the influence of local stability on network predictions using the Inception-v4 network. Results show a significant impact of local stability on prediction performance, with error rates varying significantly between easy and hard scenarios. The study demonstrates that local stability significantly affects prediction performance in the Inception-v4 network, with error rates varying between easy and hard scenarios. Neural stethoscopes are used to quantify and mitigate this influence, showing that negatively correlated local and global stability poses a challenge for the network. The study uses neural stethoscopes to analyze the correlation between global stability prediction and local instability detection in the Inception-v4 network. The network gradually develops features for both global and local stability prediction, with local stability peaking at layer 7a. This suggests that information about local stability can be inferred from the network's activation tensor. The study uses neural stethoscopes to analyze the correlation between global stability prediction and local instability detection in the Inception-v4 network. The information about local stability is considered a nuisance factor, while information about the global site of stability violation can complement the main task. The main network is trained on binary global stability labels, while the stethoscope is trained on predicting the origin of global instability. The network was trained on hard scenarios only but evaluated on all, showing an improvement in performance. The study focuses on improving network performance by addressing the bias towards local stability cues. Training the network on global stability labels for easy scenarios results in significant performance decrease when tested on hard scenarios with inversely correlated stability labels. The study aims to reduce bias towards local stability cues by training the network on global stability labels for hard scenarios. Using active stethoscopes, the results show that adversarial training partially removes the bias and improves network performance on difficult scenarios while maintaining accuracy on easy ones. The study focuses on reducing bias towards local stability cues by training the network on global stability labels for hard scenarios. Increasing the weighting factor \u03bb removes bias but can jeopardize performance on the main task as the encoder focuses more on confusing the stethoscope. The performance of the stethoscope decreases as \u03bb increases, indicating the need for more diverse training data. The study aims to reduce bias towards local stability cues by training the network on global stability labels for hard scenarios. Increasing the weighting factor \u03bb can remove bias but may impact performance on the main task as the encoder focuses more on confusing the stethoscope. The performance of the stethoscope decreases with increasing \u03bb, highlighting the need for more diverse training data. The study introduces a new dataset and neural stethoscopes framework for stability prediction of block towers. The stethoscopes measure relationships between prediction tasks, providing a more detailed physical understanding of the scene while being susceptible to misleading visual cues. The auxiliary and adversarial modes of the stethoscopes help suppress harmful information without changing the network architecture, resulting in substantial performance gains. The study introduces a new dataset and neural stethoscopes framework for stability prediction of block towers. The stethoscopes measure relationships between prediction tasks, providing a more detailed physical understanding of the scene. The main predictor's network architecture is enhanced in unfavourable training conditions. Neural stethoscopes can be used as a general tool to analyze task relationships and extract specific features. Connecting the stethoscope's first layer with any layer of the main network ensures equal capacity for comparable results. The sparse matrix M reduces the full vector x to a smaller vector x1 while adding biases. The sparse matrix M reduces the full vector x to a smaller vector x1 while adding biases. It is generated to ensure each input is connected at least once and each output is connected an equal amount of times. Adversarial training improves performance with biased datasets, potentially leading the network to discard information about local stability when trying to confuse the adversarial stethoscope. Adversarial training improves performance by potentially reducing the accessibility of information about local stability to the adversarial stethoscope. This behavior also makes such information less accessible to the main network's decoder, leading to a decrease in its dependence on features related to local stability. The section introduces two toy experiments on the MNIST dataset to demonstrate the issue of biased training datasets and the use of an adversarial stethoscope to mitigate overfitting. The experiments involve providing additional hints as network input, either as a one-hot vector or directly incorporated in the pixel space of the images. In two toy experiments on the MNIST dataset, additional hints are provided as network input, either as a one-hot vector or incorporated in the pixel space of the images. The network is trained on defined loss and gradients are used to update weights. High correlation between hint and true label can lead to the network learning an identity transformation and ignoring the image. At test time, the hint is independent of the true label, resembling challenges with biased datasets. During training, hints with varying quality levels (q) are introduced to investigate biases. The hints are correlated to the main objective labels but not at test time. The setup involves separate encoders for image and hint inputs, with the stethoscope accessing only the hint encoding to demonstrate its effect without affecting the image encoding. The hint encoder multiplies the hint vector with a weight to hide information from the main network, making the task of the stethoscope more difficult. The image encoder consists of convolutional and fully connected layers, while the decoder acts as a digit classifier. Adversarial training on classifier performance is shown in a toy experiment variant. The network is evaluated against ground truth and hint labels during test time to determine overfitting. Adversarial training forces the encoder to suppress hint information, leading to classification based solely on image data. Adversarial training helps the encoder learn to classify digits from images by hiding the hint, improving classifier independence from input images. In the second MNIST experiment, adversarial training is used to discourage the encoder from relying on hints provided as high intensity pixels in input images. Increasing hint quality requires higher adversarial weights to maintain classifier performance. High adversarial weights can degrade performance by suppressing input information. In the second MNIST experiment, adversarial training is used to discourage the encoder from relying on hints provided as high intensity pixels in input images. The main network is a two-layer multi-layer perceptron (MLP) with a simpler architecture chosen to lower performance on vanilla MNIST classification for clearer effects. 100 different hints are introduced, each related to a single digit in a many-to-one mapping, allowing for theoretical suppression of hint information without degrading the main task. The experiment involves using adversarial training to discourage the encoder from relying on hints provided in input images. The main network's accuracy is affected by applying the stethoscope with weights in positive and negative directions. Positive stethoscope loss degrades test accuracy by focusing on artificial hints, while negative stethoscope loss improves accuracy by removing misleading hint information. However, increasing adversarial strength leads to the encoder suppressing relevant information for the main task."
}