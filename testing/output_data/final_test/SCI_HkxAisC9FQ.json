{
    "title": "HkxAisC9FQ",
    "content": "We improve adversarial robustness by 11% on CIFAR-10 using worst case adversarial training (WCAT) combined with adversarial training (AT). WCAT is interpreted as Lipschitz regularization, providing verifiable robustness guarantees based on gradient norms. Our method combines worst case adversarial training (WCAT) with adversarial training (AT) to improve adversarial robustness by 11% over the current state-of-the-art. WCAT uses only one gradient evaluation compared to seven in previous work, achieving results comparable to the best in the \u221e norm. During training, WCAT computes the gradient of the loss for each perturbed image, recording the largest gradient norm and adding a penalty to the loss. Models trained with AT and WCAT show improved test/validation error, demonstrating the robustness of the model to adversarial examples. Our method combines worst case adversarial training (WCAT) with adversarial training (AT) to improve adversarial robustness. WCAT uses one gradient evaluation, achieving results comparable to the best in the \u221e norm. Training with AT and WCAT leads to proportional improvements in adversarial robustness. Adversarial training can be interpreted as Total Variation (TV) Regularization, while WCAT corresponds to Lipschitz regularization. Training with Adversarial Training (AT) and Worst Case Adversarial Training (WCAT) is equivalent to minimizing a certain formula. Adversarial attacks aim to find the minimum norm vector that causes misclassification by the model, but finding the optimal attack is difficult. Ensemble Adversarial Training has been successful in defending against adversarial attacks. Adversarial training improves robustness by considering attack vectors that increase loss, using the Lipschitz constant to establish worst-case bounds on adversarial robustness. Training models with small Lipschitz constants should enhance adversarial robustness. In theory, extending data with a function matching its Lipschitz constant can bring models closer to the ground truth. Lipschitz regularization helps align a model's Lipschitz constant with the data it was trained on, improving adversarial robustness. For example, on CIFAR-10, the dataset's Lipschitz constant is 0.36, while a regularized model had an estimated Lipschitz constant of 1.32 compared to 13.70 for an undefended model. Adversarial robustness bounds are obtained based on the model's loss gradient, estimating average and worst-case robustness. The result provides estimates for average and worst-case robustness of a model's loss. It also discusses the application of robustness guarantees, including estimating adversarial robustness on unseen data. Adversarial training using different attack vectors is interpreted as Total Variation regularization. The \u03b5-scaled one step attack vector is equivalent to augmenting the loss with Total Variation regularization. Rademacher's Theorem is the basis for this result, providing an underestimate of Lip(g) by sampling the norm of the gradient. Regularization of the loss corresponds to partially regularizing the model at a lower cost, with Lipschitz constant estimation used during training and on the full test/validation dataset. Regularizing the model f in one direction is equivalent to the Lipschitz constant of the loss. Data independent upper bounds on the Lipschitz constant are based on the norm of weight matrices and neglect the activation function effects. Recent works include estimates using distance to orthonormal matrix, 2-norm, and more. Recent works have proposed different methods for estimating the Lipschitz constant of a model, including using distance to orthonormal matrix, 2-norm, 1-norm, and \u221e-norm. Only one implementation accounts for batch normalization, which is crucial for accurate results. Another method presented in a different paper uses statistical techniques to estimate the Lipschitz constant accurately but is not practical for training due to the high computational cost. Interpreting FORMULA8 as Lipschitz regularization provides a more accurate and efficient method for estimating the Lipschitz constant in deep neural networks compared to other recent methods. Empirical results show values less than 10, suggesting FORMULA8 is an accurate estimate. Tested on CIFAR-10 and CIFAR-100 datasets, the method shows meaningful results for image classification. In \u00a75.1, error curves are defined as a metric for comparing model robustness and attack strength across different architectures. In \u00a75.2, common attack methods are ranked based on these curves, showing a clear ranking across models. The most effective attack varies based on the norm used, with Iterative FGSM being most effective in the \u221e norm and 2 PGD in the 2 norm. This ranking simplifies assessing defence robustness. In \u00a75.3, different adversarial defences are compared, providing insights on their effectiveness. The paper discusses different adversarial defences, combining adversarial training and Lipschitz regularization. Error curves measure model robustness against attacks. Adversarial statistics can be derived from the error curve, showing performance at adversarial distances. The study evaluates model robustness against attacks using various untargeted attack methods, including gradient-based and iterative methods. Adversarial statistics are derived from error curves, providing insights into model performance at adversarial distances. The study evaluates model robustness against untargeted attacks using different methods and calculates adversarial distances. Attack error curves are compared to determine the most effective attack methods, with Iterative FGSM and projected gradient descent ranking highest. The results were consistent across all models and defenses tested. The study tested various defence methods using projected gradient descent with different adversarial training types. Results showed the 2 PGD attack was most effective in the 2 norm, and adding a final sigmoid layer with tanh activation was considered. The choice of tanh activation in the final sigmoid layer is inspired by tanh-estimators used in classical statistics for robustness. Model robustness is evaluated using adversarial distances in 2 and \u221e, with misclassification rates reported at different epsilon values. The ResNeXt-34 model achieved 4% test error on CIFAR-10 without adversarial perturbations. However, the undefended model had a 54% test error at an adversarial distance of 0.1. Adversarial training reduced the test error to 24.6% with FGSM and 13.5% with 2 adversarial training. Combining all defenses further reduced the test error to 12.1%. The models were ranked based on their adversarial distances, with the defended model having six times the median distance compared to the undefended model. The ranking of defenses based on adversarial distances remains consistent across all perturbation distances on CIFAR-100. Adversarial training significantly reduces test error, with a combination of defenses further improving performance. Median adversarial distance increases with the addition of defenses, resulting in better test/validation error compared to the baseline undefended model. On CIFAR-10 and CIFAR-100, adversarial training and additional defenses improve test/validation error compared to the baseline undefended model. The model's Lipschitz constant is also measured, showing a decrease in the norm of the model Jacobian with all defenses, enhancing model robustness. The combination of 2-Lipschitz regularization and adversarial training significantly reduces the Lipschitz constant of a network, improving robustness to attacks. Adversarial training alone improves model robustness the most, with 2 adversarial training being more effective than FGSM. Adding Lipschitz regularization to adversarial training further improves model robustness by decreasing the Jacobian norm of the model on test data. Both methods increase training time by a factor of no more than four. Additionally, adding a final tanh layer to normalize logits is a cost-effective way to enhance model robustness. Without adversarial training, Lipschitz regularization alone achieves a significant improvement in error rate on the CIFAR-10 dataset. The implementation achieves 78.42% error at \u03b5 = 1.5, an improvement of over 11%. Results are comparable to BID27 with 78.59% error at \u03b5 = 1.5. A direct comparison is difficult due to different attack methods used. The best model achieves 54.2% error at \u03b5 = 8 255. Our best model, trained with J 2\u2212Lip regularization, achieves 62.4% error at \u03b5 = 8 255. Standard data augmentation techniques were used for the CIFAR dataset, including horizontal flips, random crops, and pixel value scaling. Batch normalization was applied after every convolution layer, with SGD optimization, initial learning rate of 0.1, momentum of 0.9, and batch size of 128. Training details include 200 epochs for CIFAR-10 with learning rate adjustments, and 300 epochs for CIFAR-100 with weight decay. For CIFAR-10 and CIFAR-100 datasets, different hyperparameters were used such as weight decay and Lipschitz regularization. The ResNeXt architecture employed for both datasets had specific configurations like depth, cardinality, and width. Adversarially trained models were perturbed with a distance of \u03b5 = 0.01. Prior to the final softmax layer, a sigmoid activation function was added to improve model robustness. The defense strategies discussed include input validation, preprocessing, and architecture modifications to enhance robustness against adversarial samples. Lipschitz continuity was used to analyze attack methods, showing that the Signed Gradient BID11 attack and normalized gradient attack are nearly optimal. The Signed Gradient BID11 attack and normalized gradient attack are nearly optimal in the sense that the attack vector directions are nearly optimal in the \u221e and 2 norm. The optimal attack solves a specific problem and can be approximated to order \u03b5 2. The Lipschitz constant of the training data for common datasets is small, with most below 1 in the 2, \u221e norm. The Lipschitz extension theorem states that given function values, an extension can be made that perfectly fits the data with the same Lipschitz constant. Different norms can be used for X and Y spaces, affecting the Lipschitz constant. Results for all regularization types on various models and datasets are presented, along with adversarial distances generated using different methods. Model defense method % Err at median % Err at \u03b5 = 0 distance \u03b5 = 0.1."
}