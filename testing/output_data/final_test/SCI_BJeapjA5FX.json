{
    "title": "BJeapjA5FX",
    "content": "We introduce a novel geometric perspective and unsupervised model augmentation framework for transforming deep neural networks into robust classifiers. Class-conditional probability densities based on Bayesian nonparametric mixtures of factor analyzers are used to design soft decision labels for feature to label isometry. This framework, called geometrically robust networks (GRN), is applied to CIFAR-10, CIFAR-100, and Radio-ML datasets, demonstrating robustness to adversarial attacks. DeepConvNets are widely used in various fields but are vulnerable to attacks. Adversarial images can be manipulated to resist translation, scale, and rotation. Attacks have targeted deep reinforcement learning, speech recognition, and automatic modulation recognition using deep convolutional networks. Previous methods for creating robust classifiers include robust optimization, adversarial training, ensemble adversarial training, defensive distillation, and detector-reformer networks. Defensive distillation has shown to be an inadequate defense. In response to the inadequacy of defensive distillation as a defense against attacks, a statistical geometric model augmentation approach is proposed in this paper to design robust neural networks. This approach involves creating auxiliary signal space structural information using a statistical union of subspaces learned through a mixture of factor analyzers. Additionally, unsupervised soft probabilistic decision labels are generated based on the geometry of the input space, and accurate class-conditional information is derived from the feature space after soft-decision supervised training. The paper introduces a new framework called geometrically robust networks (GRN) that utilizes unsupervised geometric augmentation to improve MAP classifiers. It includes a soft decision label coding framework and a classification framework based on class-conditional feature vector density estimation. The paper also analyzes neural networks from a geometric perspective and presents experimental results on two datasets and three attacks. The paper introduces geometrically robust networks (GRN) using unsupervised geometric augmentation to enhance MAP classifiers. It discusses neural networks from a geometric viewpoint and presents experimental results on two datasets and three attacks. The deep neural network is a nested nonlinear function approximator with parameters for classification and feature extraction stages. The investigation focuses on understanding why current architectures are vulnerable to adversarial noise. The cross entropy loss objective function is typically used for training the function approximator. The paper discusses the limitations of using one-hot encoding for hard decision labels in neural networks, which can lead to overconfidence and brittleness in the final classification layer. This constraint arises from the stochastic gradient descent training algorithm, which aims to output zeros for elements not corresponding to the correct class. Additionally, there is a geometric argument against one-hot encoding due to the mismatch between the number of unique features and classes, leading to a surjective mapping from feature space to label space. The limitations of using one-hot encoding for hard decision labels in neural networks are discussed. The mismatch between the number of unique features and classes prevents an injective map from feature space to label space. Enlarging the set of labels to be infinite allows for a bijection to be learned, leading to distance preserving maps that are crucial for adversarially robust networks. The curr_chunk discusses reducing distortion of adversarial input by projecting data into a distortion-reducing latent space. A density estimate based on this structure can minimize label space deviation. The approach involves modeling data as small error displacements from a low dimensional linear subspace, learned statistically using a mixture of factor analyzers and a Bayesian nonparametric model. The curr_chunk introduces the union of subspaces (UoS) model for capturing the latent structure of data in both input and feature spaces. It involves using a Bayesian nonparametric model to learn the manifold or union of subspaces topology, assisting supervised learning. The model represents data points as small error displacements from linear subspaces, with basis vectors and modeling errors taken into account. The curr_chunk discusses the framework for geometrically robust networks (GRN), which integrates a statistical-geometric viewpoint into a robust design approach. It involves unsupervised learning for label encoding and density estimation in a MAP classifier. The goal is to target the classification layers in g \u0398 for improved performance. The study focuses on improving the classification layers in g \u0398 by using soft decision labels and Bayesian nonparametric mixture of factor analyzers (BNP-MFA). The tunable hyperparameters influence the number of mixtures/subspaces and the dimensionality of each subspace. The classification layers generally implement a softmax multinomial logistic regression with a Bayesian maximum a posteriori (MAP) classifier. The study focuses on improving classification layers using Bayesian nonparametric mixture of factor analyzers (BNP-MFA). The BNP-MFA model is used for union of subspace learning and involves label encoding and MAP classification steps. The model offers advantages in accuracy, speed, and scalability with data. The BNP-MFA framework uses Bayesian nonparametrics to infer subspaces and subspace rank from data. A Dirichlet process mixture model is used for clustering, while a Beta process estimates local subspaces. The model is hyperparameter insensitive with only two parameters to set. The Dirichlet process is used in the BNP-MFA model for clustering, with fixed constants a-h and \u03c4 0. After training, parameters are obtained for estimating class conditional probability density function for MAP classification and soft-decision label encoding. Soft decision labels were used in defensive distillation, where the distilled network g d (\u00b7) converges towards the original network. The distilled network g d (\u00b7) converges towards the original network g(\u00b7) under a cross entropy loss with enough training data, retaining some of the original network's vulnerabilities. A different approach from Papernot's defensive distillation is used here, utilizing class conditional density estimates on class-partitioned input data to create labels. The BNP-MFA model is learned with parameters \u03a8 from the original class-partitioned signals/images as input, using demarginalization of a Gaussian mixture model to form density estimates. The BNP-MFA model is learned with parameters \u03a8 from the original class-partitioned signals/images as input, estimating class-conditional pdf over the input space. Labels are assigned based on posterior probabilities, with a correction factor \u03b2 ki to scale correct class labels higher. Soft decision label encoding is independent of deep architecture. Neural network g \u0398 is trained using backpropagation with SGD on cross entropy loss. Features are extracted after model convergence. After model convergence, features z i = h \u0398 f (x i ) are extracted and used to learn a new BNP-MFA with model parameters \u03a6. The class-conditional pdfs in the feature space are learned by swapping x with z and \u03a8 with \u03a6. The approximate posterior pdf is calculated as p \u03a6 (k|z) \u221d p \u03a6 (z|k)p(k). In real-world scenarios, class priors are not uniform, and MAP classification provides a significant improvement over multinomial logistic regression and ML classification. The training and testing procedures of GRN are summarized in Algorithm 1. Plots of the negative squared Mahalanobis distance for each cluster of the class conditional input space pdf are shown in Figure 3 for a single image confuser sample. After model convergence, features z i = h \u0398 f (x i ) are extracted and used to learn a new BNP-MFA with model parameters \u03a6. The class-conditional pdfs in the feature space are learned by swapping x with z and \u03a8 with \u03a6. The approximate posterior pdf is calculated as p \u03a6 (k|z) \u221d p \u03a6 (z|k)p(k). In real-world scenarios, class priors are not uniform, and MAP classification provides a significant improvement over multinomial logistic regression and ML classification. The training and testing procedures of GRN are summarized in Algorithm 1. Plots of the negative squared Mahalanobis distance for each cluster of the class conditional input space pdf are shown in Figure 3 for a single image confuser sample. For each cluster, the adversarial attack has almost no impact. The squared Mahalanobis distance for each cluster of the class conditional input space pdf in a single image confuser sample from a Carlini-Wagner attack has almost no influence, demonstrating little deviation in the latent space. The base neural network used for CIFAR-10 and CIFAR-100 is Springenberger's \"All convolutional network\" BID22. The Radio-ML dataset is a new time series dataset for radio modulation recognition tasks. The Radio-ML dataset is a new time series dataset for benchmarking radio modulation recognition tasks. It includes 11 modulation schemes undergoing various distortions, and the data is scaled between zero and one for adversarial parameter settings. Adversarial samples are crafted using different methods on CIFAR-10 and CIFAR-100 datasets. The Radio-ML dataset includes 11 modulation schemes scaled between zero and one for adversarial parameter settings. Adversarial samples are crafted using different methods on CIFAR-10 and CIFAR-100 datasets, with various attack parameter settings tested to confuse the base classifier while maintaining structural similarity. The proposed GRN model for CIFAR-10 performed well against attacks, with only a 10-20 percent decrease in accuracy compared to the base model. The GRN model showed resilience against adversarial attacks, with only a 10-20 percent decrease in accuracy compared to the base model. Results on CIFAR-100 did not match previous reports due to less extensive data augmentation. Experimentation with hyperparameters showed noticeable changes in classification based on parameter settings. Geometrical statistically augmented neural network models demonstrated robustness on CIFAR-10 against adversarial attacks. Further research is needed on using unsupervised learning methods to enhance deep learning model resilience to various types of noise. Investigating soft decision labels and training algorithms for incorporating structural data models is essential. Bayesian nonparametrics offer potential for model complexity growth with more data. The current training algorithm for the BNP-MFA model is Gibbs sampling, which is not scalable to massive datasets. Stochastic variational inference has been introduced as a way to handle large or streaming datasets. Work is ongoing to apply stochastic variational framework to the BNP-MFA model for scalability."
}