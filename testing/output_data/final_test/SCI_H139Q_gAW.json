{
    "title": "H139Q_gAW",
    "content": "In this paper, the authors propose a novel Depthwise Separable Graph Convolution method that unifies traditional 2D-convolution and graph convolution. Experimental results demonstrate that this approach outperforms other graph convolution methods on benchmark datasets. Neural Network (CNN) BID14 is efficient in extracting patterns from grid-structured data, advancing machine learning tasks like image classification. Attention is shifting to data with graph/non-Euclidean structures, posing a challenge to replicate CNN success. Various graph convolution methods, including spectral convolution methods BID3 BID5 BID11, have been proposed, with limitations due to their reliance on graph Fourier analysis BID20. The limitations of graph Fourier analysis-based models include the inability to transfer knowledge between graphs. Geometric convolution methods aim to leverage spatial information about nodes but struggle with capturing local information and overfitting. Both spectral and geometric convolution methods have not matched the performance of CNNs. Graph convolution methods are commonly used in pure graph structure data like citation networks and social networks. Our paper focuses on data with spatial information, providing a unified view of graph convolution and traditional 2D-convolution methods. We introduce a novel Depthwise Separable Graph Convolution (DSGC) that aligns closely with depthwise separable convolution networks. The DSGC is more expressive and has a small parameter size, outperforming previous graph and geometric methods. The proposed Depthwise Separable Graph Convolution (DSGC) aligns with depthwise separable convolution networks and outperforms previous graph and geometric methods in various machine learning tasks. The DSGC demonstrates efficiency through experiments and can leverage advanced techniques from standard convolution networks to enhance model performance. The Depthwise Separable Graph Convolution (DSGC) is shown to improve model performance by aggregating local information over graphs or data manifolds. It connects graph convolution with depthwise separable convolution on a 2D-grid graph, proposing a new formulation that encompasses both methods. The operations discussed transform input feature maps to output feature maps, denoted by X and Y respectively. The Depthwise Separable Graph Convolution (DSGC) improves model performance by aggregating local information over graphs. It connects graph convolution with depthwise separable convolution on a 2D-grid graph. The operations transform input feature maps X to output feature maps Y, where N is the number of nodes, P and Q are the number of input and output features associated with each node. Label propagation (LP) and Graph convolution (GC) are classic approaches to aggregate local information over a graph. LP uses a normalized adjacency matrix W to update node values based on neighbors, while GC extends LP with a symmetrically normalized adjacency matrix W and a linear transformation matrix U. The original form of graph convolution, derived from graph signal processing, is a linear transformation. Graph convolution captures dependencies among features, improving performance by encoding channel and spatial correlation simultaneously. Depthwise separable convolution decouples channel and spatial correlation for improved performance. Depthwise separable convolution (DSC) decouples channel and spatial correlation, showing strong empirical performance with few parameters. DSC is formulated in a graph-based manner, treating each pixel as a node with relative positions and lookup tables. This approach is closely related to graph convolution and improves image classification architectures. The formulation of GC and DSC is similar, with differences in spatial filters and constraints. DSC learns channel-specific spatial filters from data, while GC uses a global spatial filter. DSC is not suitable for spatial data on manifolds with infinite space differences, while GC restricts all channels to share the same spatial filter, limiting model capacity, especially in deeper networks. Depthwise Separable Graph Convolution (DSGC) is proposed to address limitations in current graph convolution methods. It generalizes both GC and DSC by enabling channel-specific convolution and implementing stationarity requirements in a \"soft\" manner. The formulation involves using multiple spatial filters to capture diverse diffusion patterns over the graph or data manifold. Depthwise Separable Graph Convolution (DSGC) enables channel-specific convolution by learning multiple spatial filters. To address overfitting, channels can be grouped into C groups where D = Q/C channels share the same filter. Normalizing the adjacency matrix is crucial for DSGC to adapt nodes to their contexts. Normalization through a softmax function improves performance and speeds up convergence in Depthwise Separable Graph Convolution (DSGC). The method is evaluated in image classification, time series forecasting, and document categorization tasks using PyTorch implementation. Data and code are publicly available for reproducibility. The PyTorch implementation of graph convolution methods for image classification on CIFAR10 and CIFAR100 datasets includes a two-layer MLP with 256 hidden dimensions and tanh activation function. Ablation tests show that the two-layer MLP performs well with the shortest running time. More details can be found in Appendix A. The CIFAR10 and CIFAR100 datasets are popular in image classification, containing 60000 images with 32 \u00d7 32 pixels. CIFAR10 has 10 category labels while CIFAR100 has 100. Modified versions of these datasets were created by subsampling 25% of the pixels. Various convolution networks were used for comparison, including traditional 2d convolution, Xception network, DCNN, ChebyNet, and GCN. The experiment results using different methods for graph transformation on irregular graphs are summarized in Table 1. Xception and CNN showed the best results, as they use grid-based convolution which is well-suited for image recognition. The VGG13 architecture was used as the basic platform for all methods, with convolution layers replaced according to the method. Pooling was done using kmean clustering, where the centroid of each cluster became a new node. Input signals were normalized to [0,1] without any other preprocessing or data augmentation. The DSGC model outperforms other graph-based convolution methods and is close to grid-based convolution in performance. It achieves competitive results without increasing parameters like GCN. The model's variance shows significant and stable improvement. The focus is on utilizing locality information in sensor networks for time series forecasting, such as incorporating sensor locations for predicting solar energy output. The task involves predicting the output of solar energy farms in the United States using spatiotemporal modeling. Three benchmark datasets are chosen, including the U.S Historical Climatology Network dataset with daily climatological data from meteorology sensors. The focus is on utilizing locality information in sensor networks for time series forecasting. The solar power production data from 2006 includes records from 1,082 solar power stations in the western U.S. The datasets are split into training, validation, and test sets. Various graph convolution methods and traditional time series forecasting models are used for comparison. The evaluation results of various methods for solar power production forecasting are summarized in TAB3. The first chunk includes methods that do not leverage spatial information, while the second chunk consists of neural network models that do. Graph convolution methods in the second chunk outperform those in the first chunk, highlighting the importance of modeling locational dependencies. Our proposed method (DSGC) outperforms other methods by modeling spatial correlation within sensor networks. It demonstrates superior performance on all datasets by capturing informative local propagation patterns. For text categorization, we use the 20NEWS dataset with 18,845 text documents and 20 topic labels. Nodes in the graph for convolution are individual words in the document vocabulary, each with its word embedding vector learned using Word2Vec algorithm. Top 1000 most frequent words are selected as nodes for the experiment. The proposed method DSGC shows improved performance in image classification by incorporating popular techniques like Inception module, DenseNet framework, and Squeeze-and-Excitation block. The results indicate that DSGC can benefit from these techniques to enhance its performance further. The DSGC model demonstrates improved performance in image classification by incorporating traditional 2d-convolution network development. It computes convolution weights for each edge of the graph, leading to better performance over previous methods with efficient training. The training time per epoch for DSGC is higher than GCN but still fairly efficient. In recent years, graph convolution methods like full convolution BID14 and Spectral Network BID3 have been proposed, showcasing differences from traditional 2D-convolution. The Spectral Network utilizes graph Fourier transformation for convolution, matching full convolution but with different filter subspace. Researchers have proposed ChebyNet BID5 as an improvement over previous graph convolution methods like GCN. ChebyNet uses Chebyshev polynomials to approximate non-parameter filters, allowing for depthwise separable convolution components in a layer. However, it still faces limitations such as using one graph filter over all channels and a constant graph filter given the input, limiting its model capacity compared to depthwise separable convolution. The ChebyNet model capacity cannot compare with depthwise separable convolution due to limitations in using one graph filter over all channels. MoNet framework is an advanced geometric convolution method related to our paper, extending ChebyNet by allowing graph filters to learn from data. The proposed method (DSGC) improves upon ChebyNet by using non-parametric filters and neural network functions, outperforming MoNet with fewer parameters in various tasks on CIFAR10 and CIFAR100 datasets. The architecture settings for the DSGC and baseline models are illustrated in TAB5. In DSGC-VGG13 and DSGC-DenseNet models, k-conv refers to spatial convolution with k-nearest neighbors. The hidden dimensions of VGG13 and DSGC-VGG13 are {256, 512, 512, 512} and {256, 512, 512, 1024} respectively. DSGC-DenseNet has a growth rate of 32. The baseline models use the same architecture as DSGC-VGG13. For the subsampled CIFAR experiment, the spatial convolution is changed from 9-conv to {16-conv, 12-conv, 8-conv, 4-conv}. DSGC-SE adds the SE block to DSGC-VGG13, and DSGC-DenseNet uses a dropout scheme. The architecture for DSGC models includes dropout layers and different convolution filter sizes. Inception model design is imitated for DSGC-Inception. Input signals are formatted in matrix shape with invalid points set as 0. A mask matrix is added for fair comparison with standard CNN in subsampled situations. Softmax trick is applied to MoNet for training acceleration. ChebyNet has a polynomial order of K=3. The polynomial order is set as K=3 for DSGC and MoNet models, using a 5-dimensional feature vector. A learning schedule with SGD training for 400 epochs is applied, with an initial learning rate of 0.1. The spatiotemporal regression problem is defined as a multivariate time series forecasting task using sensor locations as input to predict future signals. The task involves predicting future signals in a rolling forecasting fashion based on available historical data. An autoregressive model is used with a defined window size and node-level hidden features influencing propagation patterns. Each node learns an embedding vector to interact with its signals. In this task of predicting future signals, each node has limited freedom to interface with its propagation patterns. The embedding size is set as 10 for USHCN-PRCP and Solar datasets. An additional feature channel is added to handle missing data in the USHCN dataset. The historical window size is tuned for time series models, while for other models, the window size is set to p = 18 for Solar dataset and p = 6 for USHCN datasets. The network architecture consists of 7 convolution layers followed by a regression layer. The Adam optimizer BID10 is used for training each model for 200 epochs with a learning rate of 0.001. Data preprocessing follows the experiment details in BID5, and the network architecture for all models includes 5 convolution layers followed by two MLP. The DSGC model is evaluated in three tasks with 5 convolution layers and two MLP layers. A dropout rate of 0.5 is applied after each convolution layer. The optimizer used is the same as in the CIFAR experiment. The variance of the DSGC method in all tasks is reported as CIFAR 7.39 \u00b1 0.136, USHCN-TMAX 5.211 \u00b1 0.0498, 20news 71.70 \u00b1 0.285. The variance is smaller than the performance gap between the DSGC model and the best baseline results."
}