{
    "title": "Sklia3EFPH",
    "content": "Anatomical studies show how the brain processes information for computations, but the encoding of complex spatio-temporal patterns by neural circuits is still unclear. Neural dynamics are influenced by the alignment between input and chaotic activity, with aligned input stabilizing chaotic trajectories into attractors, enhancing computational capability. Input alignment affects attractor stability and noise suppression, impacting the network's inference ability. The brain organizes sensory data to fit behaviorally relevant dimensions for optimal performance. The brain processes sensory data to fit behaviorally relevant dimensions for optimal performance, enabling recognition despite variations. Chaos-guided input alignment in a recurrent network helps untangle stimuli in the input space and improve neural dynamics entrainment. Using a randomly connected reservoir, the alignment of input phase with chaotic neuronal response improves inference capability by creating stable attractor states. Varying phase association between input and network activity strongly influences attractor formation, as demonstrated in a motor pattern generation task. The effect of chaos guided input alignment on a firing-rate based reservoir model of interconnected neurons is described. The network consists of neurons with firing rates characterized by a nonlinear response function. The recurrent weight matrix is sparse and chosen randomly from a Gaussian distribution. The output unit reads out the network activity through a connectivity matrix. The output unit z reads network activity through connectivity matrix W Out with initial values from Gaussian distribution. Readout weights trained using Recursive Least Square algorithm. Input weight matrix W Input from Gaussian distribution. External input I is an oscillatory sinusoidal signal with amplitude I 0, frequency f, and phase factor \u03c7 randomly chosen. Input alignment analysis finds optimal phases to project inputs in preferred direction of network's activity. The input alignment analysis determines the optimal phases to project inputs in the preferred direction of the network's activity. Using Principal Component Analysis (PCA), it was observed that the input-driven trajectory becomes more circular with increasing input amplitude. Principal angles are utilized to visualize the relationship between chaotic and input-driven subspaces. The alignment of inputs in directions that maximize variance in chaotic activity suppresses intrinsic noise, allowing stable network trajectories even at low input amplitudes. This is visualized using Principal Component Analysis, showing a uniform circular orbit in the network activity. The network activity in a uniform circular orbit in the PC subspace indicates reduction in intrinsic noise and input sensitization. Stable and synchronized trajectories are maintained even after input is turned off, showing the effectiveness of subspace alignment for noise suppression. Low input-amplitude regimes offer higher network dimensionality, improving overall discriminative ability. The input phase influences neuronal activity in a recurrent network, aligning driven activity with chaotic activity. Input frequency and phase determine the orientation of the input-driven circular orbit in relation to intrinsic chaotic activity. Input amplitude affects the alignment, with lower amplitudes increasing network dimensionality for better discriminative ability. The input frequency and phase influence the orientation of the input-driven circular orbit in a recurrent network, aligning driven activity with chaotic activity. Subspace alignment is sensitive to input phase, showing non-linear behavior due to recurrent connectivity. This behavior is utilized for neuro-biological computational experiments, with input phase aligning the input in the preferred direction. The input phase aligns input in the preferred direction, influencing network learning ability. A recurrent network is trained to generate timed responses using sinusoidal inputs, with trajectories mapped to output units. The network's reliability is tested by consistent responses across repeated presentations during testing. The experiment involves neural dynamics in a recurrent network encoding timing for processing complex patterns. Inputs are briefly turned on in a timing window, with similar dynamics creating a circular orbit in the network activity. To differentiate output responses, inputs need to be aligned in different directions using principal angles. The study focuses on aligning inputs in a recurrent network to differentiate output responses. It involves defining chaotic activity and analyzing discriminative performance using Euclidean distances. Input phase rotation is used for theoretical analysis, with inter-and intra-input trajectory distances measured to assess network performance. The study analyzes the alignment of inputs in a recurrent network to differentiate output responses. Input phase rotation is used to calculate Euclidean distances for inter-and intra-input trajectory distances, aiming for larger inter-trajectory and smaller intra-trajectory distances to enhance network performance. Aligning inputs parallel and perpendicular to dominant projections increases inter-trajectory distance. Subspace alignment reduces intrinsic fluctuations within a network, enhancing discrimination capability. Chaotic trajectories become locally stable channels with input alignment along dominant projections, leading to potential wells as attractor states. Chaotic trajectories in a network become stable attractor states with input alignment, influenced by orientation for noise suppression and stability. Rotation of inputs affects attractor stability, with different orientations leading to varying levels of noise suppression. In Fig. 2 (c, Right), I 1 is more stable than I 2 at 180 \u2022 phase difference. The phase difference between I 1 and I 2 in the chaotic subspace influences attractor states. Visualizing network activity in 3-D PC space shows the impact of input orientation on attractor formation. The alignment of I 1 and I 2 in the subspace affects the 2D projection onto PC1 and PC2, while PC3 marks another dimension. The alignment of input scenarios in 3-D PC space affects attractor formation, with PC3 marking a key difference between input projections. Input scenarios with different phases show distinct network activity cycles, indicating changes in attractor state stability. Increasing input amplitude can lead to more coherent activity and noise suppression. Mean-field methods are used to evaluate random network models in the limit N \u2192 \u221e. The average autocorrelation function characterizes network interactions, with the network interaction term replaced by Gaussian noise in Mean Field Theory. The temporal correlation of the noise is calculated self-consistently, with the moments of the noise matching the moments of the network interaction term. The recurrent synaptic matrix < W ij >= 0 is used to calculate the second moment. The equation resembles Newtonian motion with a force depending on input subspace alignment. Analyzing the potential energy function visualizes different attractor states in response to stimuli. An expression for the correlation function is formulated using Taylor series expansion. The correlation function is derived using Taylor series expansion with constraints to determine the network dynamics under input alignment conditions. The non-linear firing rate function can be expanded for small values of g, satisfying the criterion for chaotic regime operation. The firing-rate function is expressed as a gain factor instead of synaptic strength, leading to the expression of C(\u03c4) and Eqn. 4 simplifies to include parameters like G and n defined in terms of m and \u03b4. Appendix B provides a detailed derivation of Eqn. 5. The potential of the network is driven by a force, F, and can be expressed as Eqn. 5. Solving Eqn. 5 with initial conditions k(0) = 1, k(0) = 0, we monitor the change in force, F, and potential, V, for different values of G. When there is no input stimulus (G = 0), the network dynamics are chaotic, leading to the formation of attractor states. Fig. 3 illustrates the evolution of potential energy as k varies for different G values. When G = 0, network dynamics are chaotic, leading to the formation of stable potential wells. The network converges to attractor states based on initial conditions. For nonzero G, the force equation depends on \u03b6. Numerical solutions show that intrinsic fluctuations are suppressed with input presence. Different values of \u03b6 affect the stability of attractors in the potential well. The stability and convergence of a recurrent network are influenced by input subspace alignment with the initial chaotic state. Changing \u03b6 affects the stability of attractor states, confirming that input orientation alters attractor state stability. Solving Eqn. 6 with different conditions results in varying \u03b6 values but similar evolution of potential well and attractor state stability. MFT calculations use \u03b6 to represent a parameter. The MFT calculations use \u03b6 to show the relationship between subspace alignment and input phase, impacting attractor state stability. Future studies will focus on real-time evaluation of \u03b6. The constraint g = 1 + \u03b4 alters attractor state, valid for large g. Input alignment effectiveness was demonstrated in a motor pattern generation task. In a study on chaotic spontaneous activity, input alignment in the chaotic subspace was found to create stable patterns of activity. The alignment of inputs corresponding to \"chaos\" and \"neuron\" resulted in robust handwritten patterns generated by the network. The chaotic trajectories acted as dynamic attractor states, but external perturbations could disrupt the stability of the patterns. The injection of noise onto a trained model with input alignment can disrupt stable patterns of activity. Increasing noise levels lead to a degradation in the prediction capability of the network, as shown by the mean squared error in Fig. 4 (b). The network exhibits high robustness with negligible degradation in prediction capability for moderate noise levels. \"Neuron\" is more stable than \"chaos\" with increased reproducibility for 90\u00b0 phase difference, while \"chaos\" is less sensitive to noise for 180\u00b0 phase. The network is sensitive to slight perturbations for 45\u00b0 phase alignment, indicating unstable attractor states based on input alignment. In a test trial, neural trajectories for \"neuron\" are stable and coherent, converging to a stable state even with external perturbation. In contrast, trajectories for \"chaos\" become divergent and incoherent beyond 1000ms. This stability phenomenon is reversed for a 180\u00b0 phase difference. Input alignment alone produces stable and repeatable trajectories in cortical networks, even in the presence of variable internal neuronal dynamics. Combining input alignment with recurrent synaptic plasticity enables learning of stable correlated network activity resistant to external perturbations. Operating networks at low amplitude while maintaining stability allows for higher dimensionality and a larger number of advantages. Operating networks at low amplitude while maintaining stability allows for higher dimensionality and a larger number of advantages. A network of higher dimensionality offers disassociated chaotic projections for input alignment, improving inference ability in classification tasks. Input alignment in the chaotic subspace impacts network dynamics and stability of attractor states. Further investigation is needed to determine specific orientations that enhance discrimination capability and the effects of alignment on readout dynamics. The stability of an attractor state can be controlled by regulating input orientation in a neural network. Input stimulus alignment, along with synaptic strength variance, plays a critical role in modulating neural circuit dynamics. Principal Component Analysis (PCA) is used to visualize and compare neural trajectories in response to varying inputs, describing network activity in an N-dimensional space. PCA is used to outline the subspace of neural trajectories in an N-dimensional space by diagonalizing the equal-time cross-correlation matrix of firing rates. Eigenvalues of the matrix indicate the contribution of Principal Components (PCs) to fluctuations in network activity. Varying input amplitude impacts chaotic activity, with leading PCs explaining most of the variance. The leading 10-15% of Principal Components (PCs) account for 95% of a network's chaotic activity, visualized in a 3D space. Trajectories of recurrent units show diverging and incoherent activity, with smaller variance components fluctuating rapidly. Leading PCs define a network's spontaneous chaotic activity, which can be sensitized by a sinusoidal input of high amplitude. A high-amplitude sinusoidal input sensitizes a recurrent network, suppressing chaotic fluctuations and leading to stable trajectories. The input amplitude determines encoding trajectories and inference ability, with larger inputs completely suppressing chaotic activity. However, excessive input dominance reduces the network's discriminative ability. The effective dimensionality of a reservoir is calculated as -1. The effective dimensionality of a reservoir is calculated as -1, providing a measure of the effective number of PCs describing a network's activity for a given input stimulus condition. Input drive must be strong enough to influence network activity without overriding intrinsic chaotic dynamics, enabling operation at the edge of chaos. Higher input values yield larger dimensionality due to richer chaotic activity. Network trajectories are chaotic until the input is turned on, returning to stable and non-chaotic fluctuations when the input is turned off. After the input is turned off, network trajectories become stable and non-chaotic, aligning with previous studies. The input-driven trajectory converges to a circular shape in a two-dimensional subspace, indicating a circle orbit in the neuronal activity hyperspace. All simulations in the Appendix use similar parameters as mentioned in the main text. The text discusses the correlation function C(\u03c4) in Eqn. 3 of the main text, focusing on the statistical properties of neuronal variables driven by Gaussian noise. It presents equations for x1(t) and x1(t + \u03c4), and computes C by integrating over Gaussian random variables. The context suggests stable network trajectories aligning with previous studies, with simulations using parameters from the main text. The correlation function C(\u03c4) in Eqn. 3 of the main text is discussed, focusing on neuronal variables driven by Gaussian noise. Equations for x1(t) and x1(t + \u03c4) are presented, with a focus on stable network trajectories aligning with previous studies. The text then delves into solving equations using Taylor series approximation."
}