{
    "title": "BkgL7kBtDH",
    "content": "The neural framework presented learns associations between interrelated groups of words in Subject-Verb-Object (SVO) structures. It induces a joint function-specific word vector space, retaining information about word group membership. The model shows robustness and versatility by achieving state-of-the-art results in estimating selectional preference and event similarity. It outperforms task-specific architectures while reducing parameters by up to 95%. This framework has the potential to support learning function-specific representations beyond SVO. The neural framework presented in the previous paragraph learns associations between groups of words in SVO structures. It aims to support learning function-specific representations beyond SVO by addressing the deficiency of standard approaches in NLP applications. In this work, a novel approach to word representation learning is proposed, moving beyond the single-space assumption. The method involves training a joint function-specific word vector space grouped by different roles/functions a word can take in text, such as subject, verb, and object. The space topology is governed by associations between the groups, allowing for vectors of plausible combinations to lie close together. This approach enables the generation of multiple vectors for words that can occur in different roles, enhancing the understanding of word associations in text. The novel approach proposed involves training a joint function-specific word vector space grouped by different roles/functions a word can take in text, such as subject and object. This is achieved through a multidirectional neural representation learning approach, factorizing groups of words and training sub-models jointly with an objective similar to skip-gram negative sampling. The SVO structure is chosen for its well-defined tasks, with potential for future work to explore combining vectors from different function-specific spaces. The multidirectional model focuses on combining vectors from different function-specific spaces to model linguistic phenomena, specifically the SVO structure. It aims to optimize vectors for plausible SVO compositions and validate its effectiveness in language applications. The model proposed by Gell-Mann & Ruhlen (2011) focuses on SVO structures in language, aligning with how humans process sentences. It combines sub-networks with shared parameters to create joint spaces for subjects, verbs, and objects, generating event representations through composition functions. The study by Milajevs et al. (2014) focuses on event representations for the full SVO structure, showing consistent improvements over standard single vector spaces and tensor-based architectures in the compositional event similarity task. The method is not limited to a specific group condition and includes experiments with indirect objects and selectional preference evaluations. Representation Learning. Standard word representation models like skip-gram negative sampling (SGNS), Glove, or FastText induce a single word embedding space capturing semantic relatedness. SGNS utilizes two vector spaces, A w and A c, to represent co-occurrence strengths between words and their context words. Typically, only A w is used while A c is discarded or the two vector spaces are averaged to produce the final space. Levy & Goldberg (2014a) showed that SGNS corresponds to factorizing a matrix M = A w A T c. Neuroscience research suggests that single-space word representation models may not accurately reflect semantic organization in the human brain. Recent studies support the idea that there is no single semantic system in the brain. Recent fMRI studies suggest that semantic memory is a widely distributed neural network with sub-networks activating selectively for specific functions. Single-space word models partially correlate with brain activity patterns but do not fully explain fine-grained word associations. Compositional Distributional Semantics is inspired by the ability of humans to make fine-grained word associations. Prior work often uses tensor-based methods to compose separate tensor spaces, distinguishing between atomic types (e.g., nouns) and compositional types (e.g., verbs). The goal is to create a semantic representation independent of grammatical structure by finding appropriate composition functions. The compositional approach in prior work focuses on finding suitable composition functions to apply on word representations, using tensor-based methods. State-of-the-art models combine tensor-based approaches with neural training for task-specific solutions, but they struggle with transferring knowledge to other tasks and may not be robust. Modeling SVO-s is crucial for tasks like compositional event similarity and thematic fit modeling. The text discusses compositional event similarity and thematic fit modeling using various methods such as neural networks and tensor-based approaches. It proposes inducing function-specific vector spaces to improve event representations by encoding relevant associations between concepts. The text discusses improving event representations by encoding relevant information into word vectors during training. Multidirectional training achieves good performance by using function-specific vector spaces across multiple tasks. The directionality of prediction in neural models is crucial, and the multidirectional approach resolves this by training on shared representations in all directions. The model aims to learn associations between different groups of words by creating embedding matrices for each group. It focuses on creating a function-specific vector space for multiple groups, such as A, B, and C, to score the plausibility of combining vectors from these groups. The model is not limited to three groups and can handle any number of interrelated phenomena. The model aims to create distinct vocabularies for different groups (A, B, C) and considers the influence of prediction directionality in representation learning. Prediction directionality significantly impacts the quality of induced representations. The model aims to create distinct vocabularies for different groups (A, B, C) and considers the influence of prediction directionality in representation learning. Prediction directionality significantly impacts the quality of induced representations, as illustrated in an example of n:1 assignment case. The customised word vector space shows three clearly separated clusters, but this trend is not observed in the opposite direction (1:n assignment). This issue also affects representations for more complex phenomena like verbs with varying arguments. The model focuses on creating distinct vocabularies for different groups and considers the impact of prediction directionality on representation learning. By training representations in a multidirectional way with shared parameters, the model efficiently learns sensibly clustered representations without explicit directionality assumptions. The model factorizes groups into sub-networks to represent all prediction directions. It calculates the dot-product between word vectors to quantify their association. Each sub-network computes its prediction using a sigmoid function and cross-entropy loss. The sub-networks are integrated into a joint model with shared parameters for efficient representation learning. The embedding matrices for groups A, B, and C are shared across all sub-networks, leading to a reduction in model size. The sub-networks are trained with a single joint loss and backward pass, synchronizing the optimization process. This synchronous approach combines the cross-entropy losses of all sub-networks to compute the overall joint loss. The sub-networks share embedding matrices for groups A, B, and C, reducing model size. They are trained with a single joint loss and synchronized optimization process. The evaluation includes a pseudo-disambiguation task to score true triplets above corrupted ones, serving as a preliminary sanity check. The pseudo-disambiguation task is a preliminary check influenced by factors like vocabulary size and corrupted examples. Models are also evaluated on established datasets, including event similarity to measure SVO structures. Our modeling approach achieves a large reduction in the number of parameters, >95%, by training with larger sets of parameters. Event similarity is evaluated on benchmarking datasets GS199 and KS108 to measure robust and flexible event representations for language understanding. The curr_chunk discusses how the model avoids relying solely on lexical overlap for similarity computation by using function-specific representations for event pairs. Different composition methods are used to create event vectors, and thematic-fit evaluation is conducted to assess the plausibility of subject-verb and verb-object pairs separately. The results are summarized in Table 3. The evaluation quantifies how well nouns fulfill the selectional preference of verbs based on their roles. Two function-specific spaces are evaluated on two benchmarks: MST1444 and PADO414, which contain word pairs with human thematic fit ratings. Plausibility is computed by cosine similarity between verb and noun vectors. Training data is parsed from the ukWaC corpus. The training data for the evaluation includes parsing the ukWaC corpus and the British National Corpus using the Stanford Parser with Universal Dependencies. Co-occurring subjects, verbs, and objects are extracted, and preprocessing steps are taken to filter out certain tuples. The final training corpus consists of 22M SVO triplets. The study reports training details such as batch size, optimizer, and hyperparameters used. Different setups with varying vector dimensions are trained, with high accuracy scores observed in the pseudo-disambiguation task. The model shows promise in capturing associations between interrelated groups for modeling SVO structures. The study captures associations between interrelated groups for modeling SVO structures and event representations. The model based on function-specific word vectors outperforms previous state-of-the-art scores on event similarity tasks. The verb vectors alone show reasonable correlation scores, indicating their importance in selectional processes. The verb vectors encode selectional preference information successfully. Correlation scores on thematic-fit evaluation data sets show that vectors from the model trained in the joint 3-group SVO setup perform well. The function-specific approach leads to peak performance on both data sets. Our function-specific approach leads to peak performance on data sets with 25-dim SVO vectors, proving superiority over standard single-space word vectors. The model learns a joint space where co-occurring words from different groups are close to each other, achieving state-of-the-art scores in cosine similarity calculations. The study analyzes a joint space where words from different groups are close to each other, reflecting the SVO structure. Nearest neighbors in the function-specific space show relations needed for modeling. Model variants are compared through an ablation study, varying training regimes and parameter sharing. The study compares model variants through an ablation study, highlighting the importance of shared parameters and synchronous training for improved performance. The sync+shared variant achieves peak scores on all evaluation sets, indicating that synchronous training guides the model towards beneficial updates for all sub-networks. The novel multidirectional neural framework presented focuses on learning function-specific word representations for reasoning over event similarity and thematic fit. The study introduces a novel neural framework for learning function-specific word representations to reason over event similarity and thematic fit. The resulting function-specific vectors outperform task-specific methods on established benchmarks. Future work will explore more sophisticated neural networks within the framework and apply function-specific training to other linguistic phenomena and languages. The pre-trained word vectors used in the study are available online. In an asynchronous setup, shared parameters are updated per sub-network based on their own loss. Separate parameters are used to merge vectors from \"duplicate\" vector spaces through non-weighted averaging."
}