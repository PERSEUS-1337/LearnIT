{
    "title": "BygSXCNFDB",
    "content": "The work presents an exploration and imitation-learning-based agent achieving state-of-the-art performance in playing text-based computer games. These games use natural language to describe the world and require text interactions from the player. They serve as a testbed for language understanding and problem-solving by artificial agents, offering a learning environment for skill acquisition through interactions with the environment. The large action space in these games poses a significant challenge for learning agents. In this work, the exploration approach of Go-Explore is proposed for solving text-based games by extracting high-reward trajectories and training a policy to imitate them. Our experiments demonstrate that this approach surpasses existing solutions in solving text-based games, being more sample efficient with fewer interactions. The learned policy can generalize well to unseen games without restricting the action space. Text-based games, popularized in the mid-80s with Zork, require players to interact using natural-language commands. There is a growing interest in developing agents that can automatically solve these games by understanding natural language and common sense knowledge. The text discusses the challenges of developing agents to interact with environments using natural language commands in text-based games. Existing solutions focus on simpler games with small vocabularies or predetermined sets of actions to avoid the large action space problem. The text proposes using the GoExplore algorithm to train an agent that can operate in the full action space of natural language in text-based games. The methodology involves extracting high reward trajectories and training a policy using a Seq2Seq model. The effectiveness of this approach is demonstrated on the CoinCollector game and the \"First TextWorld Problems\" dataset. The Go-Explore algorithm is used to train an agent for text-based games, showing faster winning trajectories in the CoinCollector game. Training a Seq2Seq model on these trajectories results in stronger generalization, outperforming existing competitive baselines. The first approach involves selecting a fixed number of words in advance without enforcing temporal dependency, while the second approach learns a Q-function to score all possible actions. The number of possible actions can grow exponentially if admissible actions are not provided, but a solution using a hierarchical pointer-generator has been proposed. In text-based games, models struggle with sparse rewards due to the large action space. Different approaches have been proposed to handle this issue, such as reducing the set of admissible actions or compressing them in a latent space. However, a Q-scorer model does not generalize well in certain settings, even when the true set of admissible actions is provided. Sparse reward environments pose a challenge for reinforcement learning, requiring long-term planning. Various exploration methods like novelty search, intrinsic motivation, and curiosity-based rewards have been proposed to address this issue. In text-based games, the episodic discovery bonus has shown effectiveness in environments with sparse rewards, but is limited to games with small action and state spaces. Go-Explore (Ecoffet et al., 2019) is an exploration algorithm that tracks under-explored areas of the state space and utilizes simulator determinism for efficient exploration in sparse-reward environments. It consists of two phases, with the first phase focusing on exploring the state space by maintaining an archive of visited states. This algorithm is suitable for experiments that prioritize final policy performance over training methods. In the second phase of Go-Explore, the algorithm trains a policy using trajectories from the first phase. Text-based games are defined as discrete-time POMDPs, with states, actions, observations, and rewards. In the second phase of Go-Explore, the algorithm trains a policy using trajectories from the first phase. Text-based games are discrete-time POMDPs with states, actions, observations, and rewards. The observations in the game are sequences of tokens, controlled by conditional observation probability, and the reward function is defined as R(s, a). Admissible actions are grammatically correct sequences of tokens that are related to the observation. Go-Explore builds an archive of cells in phase 1, where each cell contains a set of observations. In phase 1 of Go-Explore, an archive of cells is built where each cell represents a set of observations. Cells are associated with meta-data including trajectory, length, and cumulative reward. New cells are added or updated based on their performance. The algorithm selects a cell from the archive and explores randomly from it. Phase 1 involves embedding observations into cell representations, cell selection, and random action selection during exploration. In our variant of the Go-Explore algorithm, cell representations are constructed by summing word embeddings of observations and concatenating them with cumulative rewards. This new approach has proven to be highly effective in discovering high reward trajectories quickly. The action space is restricted in phase 1 to admissible actions. Phase 1 of the Go-Explore algorithm restricts the action space to admissible actions provided by the game, crucial for random search to find high reward trajectories faster. The trajectory found in phase 1 is denoted for each game. The ultimate goal is to generalize to test environments without admissible actions. Phase 2 uses the trajectories from phase 1 to train a policy for generalization. In phase 2 of the Go-Explore algorithm, a policy is trained based on trajectories to achieve generalization across different games. The authors used Seq2Seq imitation learning instead of PPO to train this policy, aiming to learn a general policy that can be applied across various games. In phase 2 of the Go-Explore algorithm, a Seq2Seq model is trained to minimize negative log-likelihood for actions given observations. An embedding matrix maps input tokens to vectors, with an encoder LSTM and decoder LSTM used to generate actions token by token. The decoder hidden state, concatenated with a context vector, is mapped to a vocabulary size vector using a matrix W during training. During training, the model parameters are trained by minimizing the negative log likelihood of each token in a sequence. At test time, the model generates sequences in an auto-regressive way using greedy search. The paper discusses more challenging environments where multiple words are needed at each step to solve games with sparse rewards. The environments selected for the challenge include CoinCollector, a text-based game where the agent must find and collect a coin, and CookingWorld, where the goal is to cook and eat food from a given recipe by collecting ingredients and objects. The parser of each game accepts 18 verbs and 51 entities with a predefined grammar, but the overall size of the vocabulary of the observations is 20,000. In our experiments, we benchmark the exploration power of phase 1 of Go-Explore in comparison to existing approaches in text-based games by generating 10 CoinCollector games with hard-level 30. This game requires many actions to find a reward, making it suitable for testing exploration capabilities. Additionally, we aim to verify the generalization ability of our model in creating complex strategies using natural language. Our model is designed to benchmark models on dealing with sparse rewards using CoinCollector, while using CookingWorld games to evaluate generalization capabilities. Three different settings are used for CookingWorld: Single, Joint, and Zero-Shot, each testing the robustness and generalization of models across multiple games. The Zero-Shot setting is the most challenging as it requires generalization to unseen games. In CoinCollector and CookingWorld games, observations include room description, inventory, quest, previous action, and feedback. Go-Explore was compared with episodic discovery bonus for CoinCollector games and implemented three treatments for CookingWorld games. The approach involves converting tokens into embeddings, using an LSTM to extract hidden states, applying a mean-pool layer to produce a single vector, and generating Q values with a linear transformation. Q-functions are jointly trained using the DQN algorithm with -greedy exploration. At evaluation time, the argmax of each Q-function is concatenated. A special token <s> is used in V type to denote the absence of a word. The model in V type uses a special token <s> to denote the absence of a word for actions with different lengths. DRRN learns to score admissible actions using an LSTM and represents actions as the sum of word embeddings. The Q value is the dot product of the observation and action embeddings. The Q-function is defined using an embedding matrix. At testing time, the action with the highest Q value is chosen. The cell representation in the Go-Explore archive is computed by summing embeddings of room description tokens with the cumulative reward. Pre-trained GloVe vectors are used for the embeddings. Different hyper-parameters are used for different baselines. The LSTM hidden state is set to 300 for all models. The comparison is made between the number of actions played and the score achieved by the agent. In Go-Explore, the efficiency of exploration is compared to algorithms using count-based rewards in text-based games. DQN and DRQN struggle in hard games without counting rewards. DRQN++ outperforms DQN++ in finding optimal trajectories. Go-Explore phase 1 finds optimal trajectories with fewer interactions with the environment. In CookingWorld, Go-Explore finds optimal trajectories with fewer interactions compared to DQN++ and DRQN++. The trajectory length is always optimal at 30 steps. Results show Go-Explore's effectiveness in single games with a total score close to the maximum possible points. In the evaluation phase, the simple imitation learning policy trained using extracted trajectories in phase 1 of Go-Explore is tested in different settings. The LSTM-DQN approach without admissible actions performs poorly, possibly due to the challenge of exploring language and game strategy simultaneously. However, incorporating admissible actions in the -greedy step improves the score achieved by the LSTM-DQN. Our approach using a Seq2Seq model trained on a single trajectory from phase 1 of Go-Explore achieves the highest score among all methods, even without using admissible actions. However, the model cannot perfectly replicate the trajectory, resulting in a 9.4% lower total score compared to phase 1 of Go-Explore. The gap between our model and others widens as games become more challenging. In a study testing a single model on multiple games, performance was lower compared to single-game settings. Learning multiple games simultaneously led to confusion for the agent due to similar observations but different required actions. The order of game presentation significantly impacted LSTM-DQN and DRRN performance, with shuffling games resulting in better outcomes. In Figure 2, the score breakdown shows that all baselines quickly fail, even for easier games. Zero-Shot setting splits 4,440 games into training, validation, and test games with equal ratios of difficulty levels. Table 3 displays poor zero-shot performance of RL baselines, with DRRN performing lower than Go-Explore Seq2Seq model. Go-Explore Seq2Seq shows promising results in solving games. The Go-Explore Seq2Seq model demonstrates promising results in solving a significant portion of unseen games, particularly in the hardest set where long sequences of actions are required. This highlights the effectiveness of training a Seq2Seq model on Go-Explore trajectories, but also underscores the need for further development in reinforcement learning algorithms to generalize to new games. The experimental results support the viability of the Go-Explore exploration strategy for extracting high-performing trajectories in text-based games, enabling supervised models to outperform existing ones in the studied settings. However, there are still challenges and limitations that need to be addressed by both current and previous methodologies. The state representation is a main limitation of the proposed imitation learning model, with a large overlap in game descriptions leading to similar observations but different actions. Using a simple Seq2Seq model, the goal is to demonstrate the effectiveness of exploration methods, but a more complex model or better encoder representation could improve performance. In Go-Explore, using general language models to generate actions could address the issue of exploding action space in complex games like Zork I. Alternatively, learning a policy to sample actions iteratively while exploring with Go-Explore is another viable strategy. A hand-tailored solution for CookingWorld games has also been proposed in a competition. The solution achieved 91.9% score in 514 test games but relies on entity extraction and template filling, limiting generalization. It is complementary to a novel methodology presented in the paper, combining Go-Explore and Seq2Seq model for improved results in text-based games. The approach shows promise in three settings with enhanced generalization and sample efficiency, while also highlighting limitations and areas for future research. In text-based games, new research challenges arise due to improvements in methodology. Two games, CoinCollector and CookingWorld, present different levels of complexity based on the number and types of skills required for success. CookingWorld's hardest game involves a recipe with 3 ingredients across 12 locations, requiring cutting and cooking, and navigating through multiple doors. In CookingWorld's hardest game, players navigate 12 locations, cut and cook ingredients, and open doors to progress. The limited inventory capacity adds difficulty, requiring players to drop and pick up objects as needed. The game uses specific commands for actions like cooking, slicing, chopping, and preparing meals with different tools."
}