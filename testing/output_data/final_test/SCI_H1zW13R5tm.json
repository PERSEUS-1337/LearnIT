{
    "title": "H1zW13R5tm",
    "content": "Deep neural networks (DNNs) are widely used in cognitive applications for their high accuracy, but their robustness is challenged by adversarial attacks. State-of-the-art defending algorithms like adversarial training or robust optimization improve resilience but come with high computational costs and are limited to known attacking techniques. Bamboo is a data augmentation method aimed at enhancing the general robustness of DNN models under adversarial attacks without relying on specific attacking algorithms. Bamboo enhances DNN robustness by augmenting training data with samples on a fixed radius ball, increasing distance between data points and decision boundary. It improves general robustness against various attacks and noises, outperforming previous methods with the same data amount. DNN models are vulnerable to adversarial attacks, where small perturbations on input data can lead to misclassification. Various attacking algorithms like FGSM, DeepFool, CW, and PGD pose a serious threat. Defenses like adversarial training, using known adversarial examples in training, have been proposed to mitigate these attacks. Various approaches like adversarial training and BID11 use PGD noise as adversaries to improve model robustness against specific attacks. However, these methods may not guarantee performance against all types of attacks. Optimization-based methods, such as those used by BID19 and BID21, treat training as a min-max problem to minimize loss from adversarial examples. While this approach can increase the margin between data points and decision boundaries, it requires high computational load, especially for large models like VGG and ResNet. There is a gap between existing defense algorithms and the goal of efficiently enhancing overall DNN model robustness without assumptions about the attacking algorithm. Data augmentation methods can improve DNN model robustness against adversarial attacks by augmenting the training set with shifted or rotated versions of the original data. Augmenting with data sampled from a Gaussian distribution centered at the original data can also enhance model robustness against natural noise. The Mixup method augments the training set with linear combinations of the original data. Bamboo is a ball shape data augmentation technique that enhances the general robustness of DNN models against adversarial attacks from all directions. It augments the training set with data sampled uniformly on a fixed radius ball around each training data point. The method does not require prior knowledge of the attacking algorithm and effectively improves model robustness against adversarial noise. Bamboo is a data augmentation method designed to improve the general robustness of DNN models against all directions of adversarial attacks and noise. It offers enhanced model robustness without high computational complexity and can achieve further improvement using the same amount of augmented data. The method works against all kinds of adversarial and natural noise, making no prior assumptions on the distribution of adversarial examples. In Section 3, Bamboo's design principle and theoretical analysis are discussed. Section 4 examines parameter selection and performance, comparing it with related works. The paper concludes in Section 5, discussing future work. A metric for measuring DNN robustness is proposed using the fast gradient sign method (FGSM) noise. FGSM generates adversarial examples efficiently, with classification accuracy under the attack as a metric for model robustness. The FGSM attack leverages local gradients to perturb input, while gradient masking can improve accuracy under this attack but may not increase overall DNN robustness. PGD, proposed by BID11, uses multi-step FGSM in a projected space to enhance model robustness. Madry et al. found that adversarial training with PGD leads to a universally robust model compared to FGSM. In this work, the focus is on untargeted optimization-based attacks, specifically on finding adversarial examples that result in misclassification with minimum distance to the original data point. The objective includes the DNN classification function as a constraint, which is challenging due to the nonlinearity and nonconvexity of the DNN classifier. The CW attack BID0 proposes an objective function f to efficiently generate strong optimization-based attacks. By modifying the optimization problem, it becomes easier to find the optimal perturbation strength \u03b4, leading to successful attacks. Previous works with high performance under FGSM attack may not be robust under CW attack, as it aims to minimize perturbation strength for successful attacks. The resulted \u03b4 estimates the distance to the nearest decision boundary around x. The average strength of untargeted CW noise is used as a metric of model robustness in testing. Comparisons with FGSM and PGD attacks are made based on average CW strength and testing accuracy. Analyzing the decision boundary shape and DNN model parameters can improve robustness. The decision boundary shape and DNN model parameters can enhance robustness. However, visualizing the boundary's curvature near training data points is not practical for guiding DNN training. Deriving bounds of DNN robustness from network weights is often too loose or complex. Adversarial training, like generating adversarial examples and incorporating their loss into the training process, is a more practical approach. The generation of adversarial examples relies on existing attacking techniques like FGSM, DeepFool, or PGD. However, these methods may not ensure robustness against all attacking methods. Alternatively, defenders can generate worst-case adversarial examples during training to minimize loss through minmax optimization. The distributional robustness method trains DNN models to minimize loss of adversarial examples near original data points, improving robustness but facing challenges. The proposed approach focuses on minimizing total risk on training set data using empirical risk minimization in supervised machine learning algorithms. It aims to improve robustness by training DNN models to generate worst-case adversarial examples during training. The proposed approach aims to improve model robustness against adversarial attacks by following the principle of vicinity risk minimization (VRM) during training. VRM targets to minimize the vicinity risk on virtual data pairs sampled from a vicinity distribution generated from the original training set distribution. This approach differs from traditional empirical risk minimization (ERM) and focuses on optimizing DNN models for better performance on adversarial examples. The optimization objective of VRM-based training involves using a Gaussian distribution centered at original training data to improve model robustness. Adversarial attacks can be defended against by considering adversarial examples as points within a radius ball around the original data. This can be achieved by optimizing the objective following the VRM principle. By utilizing geometry analysis of DNN models, the data efficiency of optimizing the objective in Equation FORMULA8 with vicinity distribution can be improved. The curvature of the decision boundary near training data points suggests that minimizing the loss of data points sampled on the edge of the ball approximates minimizing the loss of data points sampled within the ball. This modification can be achieved by adjusting the vicinity distribution in Equation (9) to P (x,\u0177|x, y) = Uniform(||x \u2212 x|| 2 = r) \u00b7 \u03b4(\u0177, y). Bamboo is a ball-shape data augmentation scheme that improves the robustness of DNN against adversarial attacks by sampling virtual data points from a r-radius ball around each original training data point. Perturbations are sampled from a Gaussian distribution, normalized to r, and added to the training set for higher data efficiency. Bamboo data augmentation method samples perturbations from a Gaussian distribution, normalizes them to r, and adds them to the training set to increase model robustness. The augmented data points push the decision boundary further away from the original training data points, enhancing the DNN model's ability to fit and generalize. The Bamboo data augmentation method increases model robustness by sampling perturbations from a Gaussian distribution and adding them to the training set. This pushes the decision boundary further away from the original training data points, improving the DNN model's ability to fit and generalize. In a simple classification problem, applying data augmentation results in a smoother decision boundary that is further away from the original training points. The Bamboo data augmentation method improves model robustness by moving the decision boundary further from original training points. Testing with Cleverhans library shows increased robustness against adversarial attacks. Performance is evaluated using CW attack strength, FGSM attack accuracy, and PGD attack accuracy. The DNN model is evaluated for robustness against attacks using Gaussian noise and parameter tuning on MNIST and CIFAR-10 datasets. Different network structures are used for each dataset, demonstrating scalability of defending methods. The models achieve an accuracy of 98.18% on CIFAR-10 without additional training tricks. The DNN models achieve 98.18% accuracy on the original MNIST testing set after 10 epochs and 83.95% accuracy on the CIFAR-10 testing set after 100 epochs. A comparison is made between Bamboo and Mixup methods using ImageNet data. Linear search method is used to analyze the effect of defending methods on decision boundaries of DNNs. In BID7's work, 784 random orthogonal directions are used for testing MNIST and 1000 for testing CIFAR-10. The top 20 directions with the smallest decision boundary distance for each training method are found for each testing data point. The average of the top 20 smallest distances across all testing data points is computed to assess the effectiveness of different defending methods. Comparison is made with FGSM adversarial training, distributional robust optimization, PGD attack, and Mixup data augmentation methods. The original implementation of these algorithms is adopted for implementation. The implementation of adversarial training in Cleverhans and Tensorflow replicates the functionality of distributional robust optimization and Mixup methods. Hyper-parameters are carefully selected for best performance. Bamboo augmentation has parameters for ball radius and augmented data ratio. Testing accuracy and model robustness are analyzed by tuning these parameters. Testing accuracy increases with more augmented points, while adjusting the radius has little impact. The relationship between the number of augmented points and CW robustness is also examined. The relationship between the number of augmented points and CW robustness is explored under different ball radius settings. Increasing data augmentation ratio N improves robustness, but the impact decreases as N grows. Similarly, increasing radius r enhances robustness, but the effect saturates as r increases. Manual tuning of r and N is done for the best tradeoff between robustness and training cost. Visualization of decision boundaries for MNIST and CIFAR-10 testing points shows the impact of different training methods. Adversarial training methods like FGSM and Madry can improve model performance. Our Bamboo data augmentation method improves robustness for vulnerable directions without introducing new vulnerabilities, outperforming previous adversarial training, optimization, and data augmentation methods. Results show overall model robustness enhancement compared to previous techniques. Training methods can enhance robustness against specific attacking methods but may not guarantee protection against other types of attacks. The performance of these methods on CIFAR-10 dataset is not as strong as on MNIST, possibly due to scalability issues. Mixup shows promise in robustness and accuracy against adversarial attacks with low strength, but overall robustness is not as high as Bamboo. ImageNet experiment results confirm this trend. Bamboo demonstrates high robustness under CW attack on MNIST and CIFAR-10, and minimal accuracy drop against Gaussian noise. It outperforms adversarial training methods like FGSM and PGD, showing robustness against a wide range of attacks. Bamboo is less sensitive to changes in attack strength compared to Distributional robust optimization. It is more data-efficient than Mixup in improving DNN robustness. Our proposed Bamboo method effectively improves DNN robustness against various attacks by augmenting the training set with data points sampled on a radius ball around original data. It outperforms previous adversarial training methods and achieves stable performance. Bamboo method enhances DNN robustness against adversarial attacks by augmenting data with points on a radius ball. It outperforms previous methods and shows improved performance with augmented data. Future work will explore theoretical relationships and propose new training tricks for better performance on new DNN models."
}