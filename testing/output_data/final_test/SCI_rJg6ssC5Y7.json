{
    "title": "rJg6ssC5Y7",
    "content": "There is significant research on the choice and tuning of optimizers for deep learning, but there is no agreed-upon protocol for evaluating them. DeepOBS is a Python package that provides benchmarks for stochastic optimization in deep learning, addressing key challenges and automating benchmarking steps. It includes a variety of realistic optimization problems like training Residual Networks for image classification on ImageNet. DeepOBS is a Python package that provides benchmarks for stochastic optimization in deep learning, including image classification on ImageNet and character-level language prediction models. It offers baseline results for popular optimizers, supports TensorFlow, and produces LaTeX code for academic publications. Research on deep learning architectures and hardware has grown, while optimization routines like SGD, momentum variants, and ADAM remain standard. The low practical relevance of advanced optimization methods is due to their complexity and lack of comparison to simpler alternatives in research papers. New optimization routines are often not provided in popular frameworks like TensorFlow and PyTorch, making it difficult for practitioners to use them effectively. Designing an empirical protocol for deep learning optimizers can be challenging due to the idiosyncrasies of the domain, such as the need to consider generalization and surrogate loss functions. The choice of which score to present in a comparison of optimizers, whether it be train loss, test loss, or test accuracy, is crucial for evaluating their performance. The need to consider generalization and surrogate loss functions makes designing an empirical protocol for deep learning optimizers challenging. Stochasticity introduced by sub-sampling the data-set can lead to misleading performance of optimizers due to random fluctuations. Popular standards like MNIST and CIFAR-10 may not be realistic placeholders for large-scale data sets and architectures. Researchers have difficulty meeting reviewers' demands for new data sets and architectures, which can lead to bias in evaluating optimization methods for deep learning. To address this issue, an extensible, open-source benchmark for optimization methods on deep learning architectures is proposed. The text discusses a protocol for benchmarking stochastic optimizers in deep learning architectures, emphasizing the importance of evaluating final performance, speed, and tunability. It introduces DEEPOBS, a benchmark suite written in TENSORFLOW that automates the evaluation process and provides over twenty optimization methods for comparison. The package automates test problem setup, model configuration, and L A T E X code generation for academic publications. It includes over twenty test problems in image classification and natural language processing, ranging from simple functions to deep neural networks. The package is easy to install in python and helps researchers create reproducible and interpretable results. The core set of benchmarks includes two sets of simple and demanding problems for algorithm design. Researchers can iterate on the simple set and then test on the more challenging set to save time and reduce over-fitting risks. The package also provides baseline results for popular optimizers on these test problems, with performance reported for SGD, MOMENTUM, and ADAM on both sets. The best results are available as a performance metric for new optimizers without the need to recompute baselines. The authors invite other algorithm authors to add their methods to a benchmark for optimization algorithms in the deep learning setting. Currently, there is no widely accepted benchmark for optimization algorithms in deep learning, as evidenced by recent research papers. Existing benchmarks for deep learning focus on frameworks or hardware, rather than the optimizer itself. Popular benchmarks for deep learning include DAWNBench, MLPerf, DLBS, DeepBench, and Fathom, each focusing on different aspects such as training speed, hardware performance, and low-level operations. Fathom is a hardware-centric benchmark that assesses computational resource usage. It focuses on deep learning frameworks but is not ideal for optimization research. Unit tests for stochastic optimization are defined by BID42, which mainly concentrate on small-scale problems. These tests do not provide a realistic view of algorithm performance in practice. The discussion expands on design desiderata for a good benchmark protocol and suggests ways to create an informative, fair, and reproducible benchmark despite the noisy nature of optimizer performance. DEEPOBS provides a solution for running optimizers with different random seeds to assess statistical significance and variability in performance. It offers the ability to run multiple runs with different seeds and stored baselines of popular optimizers, saving computational resources for collecting statistics. In deep learning optimization, different optimizers can have varying generalization performance, not just training loss. It is important to consider training loss, test loss, training accuracy, and test accuracy to fully evaluate an optimizer. Test accuracy or test loss is recommended for hyperparameter tuning and performance assessment. In deep learning optimization, it is crucial to consider training loss, test loss, training accuracy, and test accuracy for evaluating an optimizer. To get a more accurate estimate of training-set performance, evaluating on a larger chunk of training data, known as a train eval set, is recommended. Additionally, the time required to reach a solution is also important in practice. A fast optimizer that finds a decent solution using fewer resources can be highly relevant. Learning curves lack a parametric form, making it challenging to determine the correct approach. In DEEPOBS, the time to convergence is measured based on reaching an \"acceptable\" performance defined individually for each test problem from baselines SGD, MOMENTUM, and ADAM. Wall-clock time is considered the most relevant measure of speed, but it has drawbacks like hardware dependency. Authors often report performance against gradient evaluations, which can hide per-iteration overhead. It is recommended to measure the wall-clock time of the new competitor and SGD on small test problems for a few iterations to compute their ratio. In DEEPOBS, the time to convergence is measured based on reaching an \"acceptable\" performance defined individually for each test problem from baselines SGD, MOMENTUM, and ADAM. Wall-clock time is considered the most relevant measure of speed, but it has drawbacks like hardware dependency. Authors often report performance against gradient evaluations, which can hide per-iteration overhead. It is recommended to measure the wall-clock time of the new competitor and SGD on small test problems for a few iterations, and compute their ratio. The choice of hyperparameters can significantly influence the runtime of more evolved optimization methods that dynamically estimate the Hessian. Tunable hyperparameters in deep learning optimizers, such as step sizes or averaging constants, play a crucial role in the ease of tuning and comparing optimizers. Authors are advised to find and report the best-performing hyperparameters for each test problem in DEEPOBS. This approach provides insight into the required tuning without the need for extensive computations. Additionally, reporting the relative performance of hyperparameter settings used during the tuning process can characterize tunability effectively. DEEPOBS supports authors in adhering to good scientific practice by removing moral hazards and ensuring fair competition. Different hyperparameter tuning methods can be used, but consistency in computational budget allocation is crucial when comparing optimization methods. DEEPOBS provides a comprehensive framework for benchmarking deep learning optimizers, ensuring fair competition and removing moral hazards. It includes tools for loading and preprocessing data, defining test problems with various models, and running training processes. The benchmark spreads over multiple problem sets to prevent overfitting to established datasets like MNIST. Performance results of popular optimizers are available, along with learning curves for new optimizers and baselines. The library DEEPOBS includes tools for training deep learning models, providing baselines for popular optimizers like SGD and ADAM. It also offers visualization scripts for L A T E X output. Future releases will follow a version numbering system for benchmark sets. The DEEPOBS library includes tools for training deep learning models, with data sets requiring less than one GB of disk space. The data loading module processes the data sets for deep learning models, including image classification. It can also be used to create new models not included in DEEPOBS. The library offers a variety of data sets and models for tasks like image classification, natural language processing, and generative models. The DEEPOBS library includes tools for training deep learning models with various datasets and models for tasks like image classification, natural language processing, and generative models. It also provides simple tests for illustrative toy problems and handles training and logging of optimizer performance. The library plans to expand its list of problems over time to stay current with research progress. DEEPOBS, a subclass of the TensorFlow optimizer API, offers baselines for popular optimizers like SGD, MOMENTUM, and ADAM. These baselines allow comparison without bias and computational overhead. The library also estimates the runtime overhead of new optimization methods compared to SGD. The DEEPOBS visualization module reduces preparation overhead and standardizes presentation for fair evaluation of new optimizers. It generates .tex files with learning curves for proposed optimizers and relevant baselines. Baseline results in DEEPOBS evaluate SGD, MOMENTUM, and ADAM on test problems P1 to P8. The learning rate was tuned individually for each optimizer and test problem, using a logarithmic grid search. Results from the small and large benchmark sets showed no optimal optimizer for all test problems, with ADAM performing well on most but not all problems. ADAM performs well on most test problems but MOMENTUM outperforms on CIFAR-100. The relationship between learning metrics is complex, with optimizers ranking differently on train vs. test loss. Generalization performance varies by problem, and tuning ADAM is easier. Optimal learning rates vary significantly across test problems, with no significant difference in learning rate sensitivity between optimizers. Deep learning presents challenges for optimization algorithms due to stochasticity and generalization. The DEEPOBS package offers best practices for experimental protocols, aiming to simplify empirical evaluation and promote reproducibility. By providing a common ground for comparison, DEEPOBS aims to accelerate the development of deep-learning optimizers and assist practitioners in algorithm selection."
}