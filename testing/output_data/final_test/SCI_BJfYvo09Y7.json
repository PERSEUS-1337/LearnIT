{
    "title": "BJfYvo09Y7",
    "content": "In this work, a complex humanoid agent architecture is developed for flexible motor control using low-level proprioception and high-level vision coordination. The system can control a high-DoF humanoid body for tasks requiring visual perception during locomotion. The architecture combines pre-training of low-level motor controllers with a task-focused controller switching among sub-policies. The system's capabilities are demonstrated in a physically-simulated environment. In this study, advancements in training high-dimensional vision-based policies from scratch for various behaviors are discussed. The focus is on full-body visuomotor control through modular agents combining high-level vision with low-level motor control. The goal is to simplify engineering and reduce task-directed exploration complexity. Our approach builds flexible low-level motor controllers for high-DoF humanoids, coordinated by a high-level system trained to maximize task reward. Inspired by neuroscience and robotics literature, our method leverages prestructured motor skills and movement primitives for efficient visuomotor control. Research in robotics has focused on developing primitives for controlling movements, sequencing them to solve tasks, and increasing the complexity of control inputs. Reinforcement learning has been used to couple primitives with perceptual inputs. In animation, physically simulated characters capable of distinct movements have been created, with recent work using reinforcement learning to schedule specialized movement segments. Control fragments have yet to be fully developed in this area. Our work aims to contribute to the robotics literature by demonstrating how low-level control fragments can be coupled to visual input and controlled by a high-level controller. We show scalability to a greater number of control fragments and progress towards integrated agents with vision, memory, and motor control. Our system can solve tasks by switching among low-level motor controllers for the humanoid body. The scheme involves a separation of control with low-level controllers handling motor coordination and high-level controllers selecting behavior based on task context. Low-level controllers use proprioceptive observations, while high-level controllers use proprioception and vision. The SAMCON algorithm converts motion capture data into physical trajectories for simulated character control. The planning algorithm BID26 infers action sequences corresponding to pose sequences. RL has been used to produce time-indexed policies for tracking controllers. Mocap trajectories are used to derive single-purpose policies for tracking short reference motions. A 56-DoF humanoid body is actuated in the DeepMind control suite. In the DeepMind control suite BID39, joints are actuated with position-control within an actuation range of [-1, 1]. Policies are trained for single-clip tracking to maximize rewards based on a custom scoring function. The state may include normalized time for cyclical behaviors like locomotion. Training iteratively refines a policy to track the reference trajectory and ensure physical feasibility. The proposed method involves tracking a reference trajectory using linear interpolation to create a repeating walk. Episodes are initialized along the motion capture trajectory and can be terminated if certain conditions are met, such as body parts other than hands or feet making contact with the ground. An energy function is defined based on joint angles, velocities, body orientation, end-effector positions, translational velocities, and rotational velocities. The reward function is derived from the energy function with specific coefficients for different parameters. The reward is normalized between 0 and 1, with perfect tracking receiving a reward of 1. Reference data features are acquired by setting the body to specified joint angles and computing end-effector vectors. The policy received egocentric observations for executing behaviors, including joint angles, velocities, end-effector vectors, rotational velocity, and orientation relative to the z-axis. The reward function was not limited to egocentric features and was derived from the energy function with specific coefficients. The low-level controller reinforcement learning details involve pre-training the policy to produce target poses through supervised learning, followed by off-policy training using a distributed actor-critic implementation. This approach significantly shortened training time and improved resulting policies. The policy was learned off-policy using importance-weighted Retrace BID30 and SVG(0). Policy updates needed to be conservative with a term restricting BID35. Learning the variance of policy actions led to premature convergence, so a stochastic policy with fixed noise was used. The text discusses the design of low-level motor controllers derived from motion capture trajectories. Two categories of controllers are mentioned: structured and cold-switching controllers. Structured controllers have a hand-designed relationship between skill-selection variables and behavior, while cold-switching controllers combine behaviors automatically for a wider range of results. The text describes specific choices for structured and cold-switching controllers in motor control design. Structured controllers include a steerable controller for controllable turning radius and a switching controller that can switch between behaviors learned from multiple mocap clips. Cold switching does not train transitions between behaviors. The steerable controller allows for parametric turning by distorting the reference trajectory and training the policy to track it with an additional input for turning radius. The text discusses a policy that can turn with a specified rate of turning by rotating the reference clip heading at a constant rate. An alternative switching controller can switch among a discrete set of behaviors based on input. Training involves transitioning among clips according to permitted transitions in a Markov process. Clips are manually cut to begin and end at similar points in a gait cycle. The text discusses transitioning between compatible clips using control fragments for better locomotion flexibility and smoother transitions. Control fragments are short micro-behaviors that allow for easy scaling to many clips without manual intervention. Time-indexing variables are used to select and transition between these fragments. The text discusses using control fragments for smoother transitions between compatible clips in locomotion. Control fragments are short micro-behaviors that can be easily scaled without manual intervention. Time-indexing variables are used to select and transition between these fragments, allowing for cold-switching among behaviors. The text discusses integrating low-level controllers into an agent architecture with vision and LSTM memory for tasks like directed movements, obstacle courses, foraging, and memory tasks. The high-level controller interfaces with the low-level controllers based on their type, and high-level policies are trained off-policy using data from a replay buffer. The high-level controller senses inputs from proprioceptive data and visual tasks. The high-level controller integrates proprioceptive data and visual inputs from an egocentric camera for decision-making. Proprioceptive inputs are encoded linearly, while visual inputs are processed by a ResNet. These inputs are then combined and passed to an LSTM for integrated decision-making with a stochastic policy and value function head. The high-level controller uses vision to select and modulate low-level skills, enabling visuomotor computations. The high-level controller uses vision to select and modulate low-level skills for visuomotor computations. The policy for the steerable controller is a parameterized Gaussian distribution, while the switching controller uses a multinomial policy over a discrete set of behaviors. The controllers are trained using TD-learning and V-Trace methods. The high-level controller uses vision to select and modulate low-level skills for visuomotor computations. The V-function is updated using V-Trace and the policy is updated according to the method in BID4. Different tasks include Go-to-target, Walls, Gaps, Forage, and Heterogeneous Forage. The policy gradient loss for the high-level is non-zero only when a new action is sampled. Alternative ideas for control fragments involve producing a Gaussian policy search query to be compared against a feature-key. The method involved selecting control fragments based on a Gaussian policy search query compared to a feature-key. The Q-function was evaluated for k nearest neighbors to the query-action, with control fragments selected using Boltzmann exploration. However, this approach under-performed compared to discrete action selection in various tasks tested in MuJoCo. The agent performed various tasks such as Go-to-target, wall navigation, running on gapped platforms, foraging for colored ball rewards, and a foraging task requiring the agent to remember the reward value of different colored balls. Rewards varied based on the task, with the agent receiving sparse rewards in some tasks and rewards proportional to forward velocity in others. The tasks involved exploring object locations and rewards in different environments, with the agent receiving visual input and proprioceptive information. The study compared agents on various tasks using control fragments with discrete selection, achieving the best results. The rolling ball's performance was isolated from humanoid control complexity. Switching controllers selected from a set of four policies for different tasks. The study compared agents on various tasks using control fragments with discrete selection, achieving the best results. The switching controllers selected between a base set of four policies for different tasks, with the end-to-end approach succeeding only at Go-to-target. The steering controller had limitations in making direct approaches, resulting in longer travel times. The control fragments approach allowed for sharper turns and quicker adjustments, leading to higher velocity in the Walls task. In the Forage task, the switching controller with transitions was able to traverse the maze but struggled to adjust to the layout for sharper turns. The control fragments approach, on the other hand, could construct rotations and abrupt turns to collect objects efficiently. In the Gaps task, the control fragments approach with 12 single-clip policies outperformed pretraining transitions for each, showcasing its flexibility and effectiveness. In the final Heterogeneous Forage task, the agent with an LSTM in the high-level controller showed memory-dependent control behavior. Control fragment comparisons were done using fragments of 3 time steps. Further analysis was conducted on the performance of the control fragment approach, including the effect of fragment length and number of fragments. Example agent-view frames and visuomotor salience visualizations were also presented. The study explored the use of fragments for control in a high-level controller, finding that adding more fragments was beneficial when they were functionally similar to the standard set. However, query-based approaches did not outperform discrete control fragment selection. The proposal distribution over queries generated by the high-level policy was high variance, leading to imprecise indexing of fragments. The study found that discrete selection of control fragments led to structured transitions between movements, with the high-level controller focusing on specific behaviors. Visual analysis showed sensitivity to visual features like ball borders and walls, indicating sensorimotor affordances. The study aimed to learn how to reuse motor skills for whole body humanoid tasks. In this study, the focus was on learning to reuse motor skills for whole body humanoid tasks using egocentric camera observations. Various approaches were compared, including those from motion capture data. The technical contribution was moving towards using front-facing camera observations for a more realistic setting. Hierarchical motor skill reuse was found to be more effective than a flat policy for solving tasks like walls and go-to-target. Learning from scratch was slower and less robust in comparison. Learning from scratch was slower and less robust for tasks, including forage tasks. Heterogeneous forage integrates memory and perception. Low-level tracking policies were learned from motion capture data using a similarity measure. Previous work focused on time-indexed policies, while less research has been done on imitation policies without pre-specified scoring functions or time-indexing. Low-level controllers were built without a sampling-based planner. The low-level controllers were built without a sampling-based planner and parameterized as neural networks. The graph-transition and structured low-level control approaches require manual curation and design, involving labor-intensive segmentation and manipulation of motion capture clips. This process treats humanoid control as a computer-aided animation problem, while aiming to approach humanoid motor control as a data-driven machine learning problem. The controllers may be less graceful compared to previous work in graphics and animation. Methods to automatically reduce movement artifacts in large movement repertoires without human labor are of interest. Training structured low-level controllers involves human-intensive components, making objective algorithm comparison challenging. The focus is on building movement behaviors at scale without extensive curation, presenting two curation-free methods for re-using low-level skills. The study presented two methods for scaling movement behaviors without human curation. Future work aims to refine motor skills for general environment interactions and complex tasks. Training details involved a distributed actor-learner architecture with asynchronous actors generating trajectories stored in a replay buffer. The learner sampled trajectories of length N at random and performed updates using a single Pascal 100 or Volta 100 GPU. Plots show steps processed by the learner on the x-axis, equivalent to the number of gradient updates x batch size x rollout length. Optimization was done with Adam and hyperparameter sweeps were used to select learning rates and batch sizes. Four policies trained from motion capture were used for the switching controller and control fragments approach. In the heterogeneous forage task, the humanoid must collect positive balls in a room with 6 balls, 3 red and 3 green. The high-level controller's architecture includes an encoder passed to an LSTM for training the policy with SVG(0) update. A state-action value function was implemented as an MLP and trained with a Retrace target. Target networks were updated every 100 training steps. The policy in the heterogeneous forage task was trained with a Retrace target and updated every 100 training iterations. A switching controller policy head, consisting of an LSTM with a state size of 128, was used to produce logits of a multinomial distribution. The policy was updated using a policy gradient with N-step empirical returns and bootstrapping to compute an advantage. Additionally, an entropy cost was added to the policy update with a weight of .01. The policy in the heterogeneous forage task was trained with a Retrace target and updated every 100 training iterations using a switching controller policy head. The selector is parameterized by a diagonal multivariate Gaussian action model, with query actions corresponding to features in control fragment feature-key vectors. The Q function was trained with 1 step returns, and the total loss is summed across minibatches sampled from replay for query-based control fragment selection. The policy in the heterogeneous forage task was trained with a Retrace target and updated every 100 training iterations using a switching controller policy head. The selector is parameterized by a diagonal multivariate Gaussian action model, with query actions corresponding to features in control fragment feature-key vectors. The high-level policy emits query actions, which are rectified to reference keys by a nearest lookup. Targets and loss terms are computed for both query-actions and selected actions for each state from replay, with a single Q function representing the value of both types of actions. The policy update used the SVG(0)-style update. The policy update in the heterogeneous forage task used the SVG(0)-style update. Visualization showed impreciseness in query actions, with only 45/105 control fragments selected. Limited success was attributed to the lack of selectivity in the query actions. Additional analysis of a trained discrete-selection agent was conducted after the unsuccessful attempts. The analysis of a trained discrete-selection agent found that the second most preferred control fragment was not always the most similar reference key, questioning the effectiveness of the query-based approach. Despite this, there is optimism that the approach may still prove useful. End-to-end training on each task was conducted similarly to training the low-level controllers, using the same architecture and training methods. SVG(0) update and Q was trained with a Retrace target, comparing performance based on the number and length of control fragments. Shorter fragments tend to perform best for precise control, allowing greater responsiveness. Control fragments of different lengths were compared for performance, with shorter fragments performing best for precise control and greater responsiveness. Longer fragments showed smoother visual appearance but traded off behavioral coherence. Hypothetically, longer fragments may shape action-space and exploration distribution favorably. Transition density between control fragments and behavior of a trained agent on Forage were depicted in figures. The agent frequently transitioned to a particular stand fragment it relied on. The agent frequently transitions to a specific stand fragment it relies on, depicted with green vertical lines showing reward acquisition."
}