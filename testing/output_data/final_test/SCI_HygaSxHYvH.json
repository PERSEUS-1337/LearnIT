{
    "title": "HygaSxHYvH",
    "content": "The masked translation model (MTM) combines encoding and decoding within the same model component, supporting autoregressive and non-autoregressive decoding strategies. In experiments on the WMT 2016 Romanian-English task, the MTM outperformed related approaches in translation performance. Neural machine translation (NMT) operates under the encoder-decoder framework with an attention mechanism. The encoder learns contextualized representations of source tokens, utilized by the decoder for predicting target tokens. The masked translation model (MTM) combines encoding and decoding in a single component, simplifying machine translation models by eliminating the conventional decoder. This unified model outperformed related approaches in translation performance on the WMT 2016 Romanian-English task. The MTM combines encoding and decoding in a single component, eliminating the conventional decoder and encoder-decoder attention mechanism. Trained with a masked language modeling objective function, it predicts masked target words using self-attention layers considering both the source and masked target sentence. This non-autoregressive model predicts every position in parallel, improving translation performance. The MTM improves translation performance by predicting every position in parallel, using various decoding strategies such as left-to-right or non-autoregressive. It outperforms comparable methods on translation tasks while maintaining a simple architecture. The MTM model allows for decoding speed-up by adjusting the number of iterations with a small cost to translation performance. Various approaches have been taken to combine encoder and decoder components for simplified translation modeling, all focusing on self-attention coupling. The MTM model enables faster decoding by refining source representations based on target hypotheses in the decoder states. Non-autoregressive NMT predicts all target words in parallel, utilizing full bidirectional context in decoding. Different methods like reusing source words as inputs and adding attention modules have been proposed for parallel decoding. In this work, the authors propose a non-autoregressive NMT model that collapses the boundary between encoding and decoding sequences. Their model utilizes self-attention layers that attend to all available source and target words for flexible information flow. This approach aims to improve the integrity of hypotheses and simplify the model architecture. The authors propose a non-autoregressive NMT model that utilizes self-attention layers for flexible information flow. They address the issue of predefined output length by training a separate length model on bilingual data, based on the MLM objective. This approach improves cross-lingual natural language understanding tasks. The masked translation model (MTM) focuses on improving cross-lingual natural language inference by selectively masking out target tokens for source\u2192target translation. It relaxes architectural constraints, learns to perform both encoder and decoder tasks, and compares different decoding strategies. The masked translation model (MTM) improves cross-lingual natural language inference by selectively masking target tokens for translation. It relaxes architectural constraints, performs encoder and decoder tasks, and utilizes various decoding strategies. The MTM independently models the true target sentence for each position, given both the true source and a noisy target context. The masked translation model (MTM) enhances cross-lingual natural language inference by masking target tokens for translation. It consists of N transformer encoder layers with self-attention and linear layers. Positional encoding and language embeddings are added to differentiate between source and target vocabularies. The attention in the encoder layers has no direction constraints, allowing bidirectional context utilization beyond language boundaries. The masked translation model (MTM) utilizes bidirectional context beyond language boundaries by allowing hidden representations of source words to attend to partially hypothesized target tokens. Different levels of target side input are used during training to simulate various stages of the translation process. A separate model for length prediction is trained, details of which can be found in Appendix A. The masked translation model (MTM) utilizes bidirectional context beyond language boundaries by allowing hidden representations of source words to attend to partially hypothesized target tokens. In training, a separate model for length prediction is used, with details provided in Appendix A. The MTM aims to reconstruct the original target sentence by simulating a partial hypothesis through probabilistic models for corruption. The masked translation model (MTM) reconstructs the original target sentence by simulating a partial hypothesis through probabilistic models for corruption. The selection process for corruption is done through a uniform distribution until a certain number of samples are drawn, with customization options for training by selecting appropriate parameters. The MTM training involves customizing parameters p s and p c. The corruption model p c assigns probabilities to operations like replacing with a mask token or a random word. Decoding in MTM involves dealing with training losses for corrupted positions. During decoding, the MTM iteratively refines hypotheses based on source and target inputs to maximize translation probability. The process involves non-autoregressive generation of initial hypotheses and iterative refinements inspired by previous work. The goal is to find a hypothesis that maximizes translation probability through a two-stage maximization process. The MTM uses a two-stage maximization process to refine hypotheses for translation probability. It involves step-wise optimization on the whole sequence, introducing an intermediate representation for fine-grained control of the decoding process. The probability is modeled by a neural network with parameters \u03b8, and a masked sequence is defined by a specific search strategy. The MTM network focuses on refining translation probability through a two-stage maximization process. The score is determined solely by the network, and a greedy optimization approach is used to select the best hypothesis in each iteration. While there is no theoretical proof for the quality of the final output, empirical evidence suggests that the maximum score is achieved in the last iteration. Following the recursive definition in Equation (10) allows for finding the best hypothesis efficiently. The MTM decoding process, as described in Algorithm 1, involves generating a hypothesis, masking positions, and feeding the masked hypothesis back iteratively. The decoding is non-autoregressive in the first iteration, but subsequent iterations prevent the multimodality problem by making predictions conditioned on the previous sequence. Each iteration results from a forward pass. The MTM decoding process involves generating a hypothesis, masking positions, and iteratively feeding back the masked hypothesis. Each iteration is conditioned on the previous sequence, resulting in a complexity of O(T) decoding steps. This approach can potentially be faster than traditional autoregressive NMT models. The masking probability introduced in the decoding algorithm resembles the corruption during training, where only specific positions are masked. The MTM decoding process involves generating a hypothesis, masking positions, and iteratively feeding back the masked hypothesis. The Mask operation is activated in decoding to shift the model's focus towards selected words. Different decoding strategies determine the masked hypothesis based on position selection. The simplest solution is to feed back the completely unmasked sequence in each iteration, but this may hurt the model's performance initially. The MTM decoding process involves generating a hypothesis, masking positions, and iteratively feeding back the masked hypothesis. Instead of unmasking all positions from the beginning, one can unmask the sequence randomly one position at a time. This method is nondeterministic and takes at least I iterations before the output is conditioned on the completely unmasked sequence. Another alternative is to unveil the sequence step-wise, starting from the left-most position in the target sequence. This allows for updating predictions for any position at any time. The MTM decoding process involves masking positions and iteratively updating predictions without re-training the model. Different strategies like R2L and middleout can be used. L2R decoding ignores the bidirectional model's advantage of predicting sequence elements in any order. Masking a decreasing number of positions in each iteration is key. The MTM was implemented in the RETURNN framework and evaluated on the WMT 2016 Romanian\u2192English translation task. Data preprocessing involved using the MOSES tokenizer and frequent-casing. A joint source/target byte pair encoding (BPE) was learned with 20k merge operations. Results were reported on the newstest2016 test set with case sensitivity and tokenization using SacreBLEU software. The MTMs in the experiments followed a base configuration with 12 layers and 16 attention heads, trained using Adam with an initial learning rate. During training of the MTM model, Adam optimizer is used with a learning rate of 0.0001 and a batch size of 7,200 tokens. Dropout is applied to hidden layers and word embeddings with a probability of 0.1. Checkpoints are set every 400k sentence pairs, with a learning rate reduction if perplexity does not improve. A percentage of random target tokens are corrupted during training. Hyperparameters for corruption are set to \u03c1 mask = 0.6, \u03c1 rand = 0.3, and \u03c1 keep = 0.1. The final models are selected based on development set perplexity after 200 checkpoints. Our MTM model outperforms constant-time NMT methods and falls slightly behind autoregressive baseline by -2.4% BLEU. Using gold length instead of predicted length improves results by 1.5% BLEU. The model can improve decoding speed by reducing iterations. Other methods in Table 1 are based on encoder-decoder architecture. The MTM model outperforms constant-time NMT methods and slightly lags behind autoregressive baseline by -2.4% BLEU. It can enhance decoding speed by reducing iterations and allows for easy implementation of different decoding strategies within a single model. The performance of other methods in Table 1 relies heavily on knowledge distillation from a well-trained autoregressive model, involving additional steps and computation time in training. Multiple translations in parallel for likely length hypotheses could potentially yield even better results. The MTM model outperforms constant-time NMT methods and slightly lags behind autoregressive baseline by -2.4% BLEU. It enhances decoding speed by reducing iterations and allows for easy implementation of different decoding strategies within a single model. Pure non-autoregressive decoding yields poor translation performance, but monitoring the average model score shows steady improvement in each iteration. Various decoding strategies are compared in Figure 4 for different number of decoding iterations. The MTM model outperforms constant-time NMT methods and slightly lags behind autoregressive baseline by -2.4% BLEU. Various decoding strategies are compared for different number of decoding iterations, with confidence-based one-by-one decoding reaching the strongest final performance of 31.9% BLEU. Position-wise unmasking stagnates at 22% BLEU, while decoding with a fixed number of T only needs ten iterations to reach close to optimal performance. \"Middle-out\" decoding shows faster improvement but worse final performance compared to L2R or R2L decoding. Random selection of decoding positions also shows fast improvements. The study compares various decoding strategies for machine translation models. R2L decoding shows fast improvements initially but saturates below other strategies. Confidence-based decoding with a fixed number of iterations yields the best results. The choice of decoding strategy significantly impacts performance, with the model score playing a crucial role in selecting target positions to unmask. The Transformer architecture is simplified by combining encoder and decoder elements into a single component for training. The masked translation model is trained by concatenating source and target sentences and applying BERT-style masking. Different decoding strategies are compared, with unmasking the sequence one-by-one showing the best performance. There is potential for a 1.5% BLEU improvement with more elaborate length models. The study plans to extend decoding strategies to work with beam search and further verify observations on different languages. In this section, ablation studies and a deeper investigation into decoding strategies for the masked translation model are presented. Autoregressive models determine output length by predicting a special token marking the end of the sentence, which is not compatible with the masked translation model. Different approaches such as assuming a given output length and using a count-based table or Poisson distribution are explored for decoding. The translation performance heavily relies on the length model used, with the Poisson distribution showing a +1.1% improvement in BLEU score compared to a count-based model. RNN models take into account both source length and target length for prediction. The bidirectional RNN model slightly outperforms the Poisson distribution in predicting target length, but translation performance only marginally improves. Using the Reference length further enhances performance by +1.6% BLEU. Thorough hyperparameter search was crucial for optimal model performance in the MTM system. The baseline MTM configuration includes 16 attention heads, dropout of 0.1, and a specific learning rate reduction scheme. The ablation study in table 3 emphasizes the importance of attention heads and dropout in the MTM system. Increasing model depth to match encoder-decoder layers is crucial. A detailed derivation of the decoding process justifies the iterative optimization procedure. The goal is to find an optimal target sequence given source sentence F. Decoding is performed in T iterations with latent variables E(1), ..., E(T-1). The decoding process in the MTM system involves optimizing latent variables E(1), ..., E(T-1) in T iterations to find the optimal target sequence given the source sentence F. By introducing a score function Q and focusing on step-wise maximization, the iterative optimization procedure aims to find the best hypothesis for each iteration \u03c4 based on the previous hypothesis \u00ca(\u03c4-1). The iterative optimization procedure aims to find the best hypothesis for each iteration based on the previous hypothesis, with the goal of improving the score in each iteration. Empirically, it is observed that the maximum score in the last iteration can be approximated, leading to the strongest result for each decoding strategy."
}