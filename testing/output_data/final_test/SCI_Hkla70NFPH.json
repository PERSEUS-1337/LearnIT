{
    "title": "Hkla70NFPH",
    "content": "Recent work in cross-lingual word embeddings challenges the Anglocentric approach by showing that the choice of hub language significantly impacts lexicon induction performance. The evaluation dictionaries are expanded to include all language pairs, shedding light on the suitability of established methods and presenting new challenges for the field. General guidelines for strong cross-lingual embeddings baselines are identified based on diverse experiments beyond just English-centric ones. Continuous distributional vectors for representing words have become common in modern neural NLP. Cross-lingual representations allow words from different languages to be represented in a shared space, useful for tasks like Bilingual Lexicon Induction (BLI). BLI is a crucial step for various downstream tasks such as Part-Of-Speech tagging, parsing, document classification, and machine translation. Shared representations are typically learned through a two-step process, whether in bilingual or multilingual settings. Cross-lingual word embeddings are widely used for various NLP tasks. A mapping between languages is learned through supervised, minimal supervision, or unsupervised methods. One language embedding space serves as the target hub for alignment. However, previous work in cross-lingual embeddings has been predominantly English-centric. The prevailing approach in cross-lingual word embeddings is English-centric, with English often chosen as the hub language during training. However, research suggests that this choice may not be optimal, especially for aligning distant languages. Evaluation methods also tend to be Anglocentric, with lexica primarily focused on English and European languages. The prevailing approach in cross-lingual word embeddings is English-centric, with English often chosen as the hub language during training. However, research suggests that this choice may not be optimal, especially for aligning distant languages. Evaluation methods also tend to be Anglocentric, with lexica primarily focused on English and European languages. Baroni (2014), Artetxe et al. (2017), and Zhang et al. (2017) have reported results on various language pairs, highlighting the need for evaluation benchmarks beyond English. English's disproportionately large available data and relatively poor inflectional morphology make it an overly easy evaluation setting, not generalizable to other languages. Therefore, equal focus should be given to evaluating more diverse language pairs, including morphologically rich and low-resource languages. This work aims to address these shortcomings. The study addresses the limitations in cross-lingual word embeddings, emphasizing the importance of choosing an optimal hub language for diverse language pairs. It provides guidelines for selecting a hub language, recommends multilingual systems for distant languages, and offers resources for training and evaluating non-Anglocentric language pairs. In extending MUSE dictionaries to 2352 lexicons covering 49 languages, new evaluation lexica for under-resourced languages like Azerbaijani, Belarusian, and Galician were created. Recipes for creating such dictionaries for any language pair with available parallel data were also provided. The supervised bilingual setting involves learning a transformation matrix M for two languages L = {l1, l2} with pre-trained row-aligned embeddings X1, X2. The orthogonality assumption in this formulation has been found to be competitive with other alternatives. In the minimally supervised or unsupervised setting, popular methods follow an iterative refinement approach. Starting with a seed dictionary, an initial mapping is learned and used to expand the dictionary with high confidence word translation pairs. This process continues through iterations to learn a better mapping. In the minimally supervised or unsupervised setting, popular methods follow an iterative refinement approach. Starting with a seed dictionary, an initial mapping is learned and used to expand the dictionary with high confidence word translation pairs. The new dictionary is then used to learn a better mapping through iterations until convergence. In a multilingual setting, one could start with N languages and their respective pre-trained embeddings, then learn N-1 bilingual mappings between a pre-selected target language and all others. The MAT+MPSR methods generalize the adversarial approach and approximate the joint mapping with gradient-based methods. The MAT+MPSR method generalizes the adversarial approach to multiple languages by choosing a hub language and learning mappings for the other languages. Another unsupervised multilingual approach by Heyman et al. (2019) suggests aligning languages incrementally, but it requires learning many mappings for small improvements. Comparing to this method was avoided due to the complexity it adds to the experimental space. Lexicon Induction is a common evaluation task for crosslingual word mappings, involving retrieving appropriate word translations. The task involves retrieving word translations using mapped embedding spaces, with specialized evaluation dictionaries like MUSE dictionaries. Cross-Lingual Similarity Scaling (CSLS) is commonly used for this purpose, improving alignment accuracy by decreasing scores of dense word pairs. The retrieved translations are evaluated using precision at k (P@k) against a gold standard. In this section, the method for creating new dictionaries for low resource languages is outlined. A simple triangulation process is described to create dictionaries among all 49 MUSE languages. The approach is inspired by phrase table extraction techniques from phrase-based MT, avoiding manual inspection. The method involves collecting parallel data between English and low-resource languages like Azerbaijani, Belarusian, and Galician from various datasets. The data is aligned and symmetrized using specific techniques to extract word pairs, ensuring they are not highly domain-specific. The process involves extracting word pairs from parallel data of low-resource languages like Azerbaijani, Belarusian, and Galician. Word pairs are filtered based on frequency and alignment probability, resulting in dictionaries with thousands of word pairs for each language pair. This method can be applied to other language pairs with sufficient parallel data, leveraging datasets like JW300 and WikiMatrix to create dictionaries for over 300 languages. The process involves creating new dictionaries by triangulating word translations between languages using parallel data. By applying this method, 2352 new dictionaries were generated among 49 languages in the MUSE dictionaries. The approach ensures consistency in train and test splits across all dictionaries. When creating new dictionaries in the MUSE project, it is important to avoid triangulating through English due to its morphological limitations and lack of gender information. This can lead to multiple inflected forms in rich languages mapping to the same English form, causing translation inaccuracies. For example, gendered nouns or adjectives in languages like Greek may be translated to English forms that lack gender specificity. When translating between morphologically rich languages like Greek and Italian, gender and number must be taken into account. A filtering method is devised to remove mistakes when triangulating languages, using automatic morphological tagging from the StanfordNLP toolkit. The technique filters out translations with different morphological analysis, ensuring accuracy in translation. In the case of feature mismatch or partial tag match, translations with disagreeing tags are filtered out. This technique removes about 17% of entries in bridged dictionaries, limited to languages with a morphological analyzer. Lexicon Induction performance is measured over 10 European languages, showing where a bilingual MUSE system outperforms MAT+MSPR. The shaded cells indicate where a bilingual MUSE system outperforms MAT+MSPR. MAT+MPSR systems are trained for main MWE experiments aligning language subsets with varying hub languages. LI performance differences highlight the importance of hub language choice. New language pairs are evaluated using triangulated dictionaries. Comparing multilingual alignment quality with different hub languages is the focus, even with slightly noisy dictionaries. The study initially focuses on 10 European languages. In this experiment, the focus is on 10 European languages of varying complexity and data availability. The languages include Azerbaijani, Belarusian, Czech, English, Galician, Portuguese, Russian, Slovak, Spanish, and Turkish. The best performing systems for this experiment are summarized in Table 2. Additionally, a set of 7 more distant languages is used in the second setting, with systems presented in Table 3. The best performing systems for the experiment on European languages are presented in Table 3. The focus is on minimally supervised scenario using character strings for supervision. MAT+MPSR method is used for learning MWE, and MUSE systems are compared for BWE experiments. Statistical significance is determined using paired bootstrap resampling. The hub is crucial for distant languages when using MUSE. The hub is important for distant languages in MUSE. MAT+MPSR method shows better performance than MUSE for aligning distant languages. Hub choice significantly impacts alignment results. In multilingual settings, the choice of hub language significantly impacts alignment results. The higher-resourced language tends to be a better hub, especially when resource levels differ significantly. English is not always the best hub language in multilingual experiments. The choice of hub language significantly impacts alignment results in multilingual settings. English is not always the best hub language, as shown in experiments with European and distant languages. Using English as the hub led to a drop in performance compared to optimal selections in both experiments. In multilingual settings, the choice of hub language greatly affects alignment results. Not all languages are equally suitable as hub languages for different language pairs. The expected gain of using a specific language as the hub can be quantified based on accuracy. The choice of hub language significantly impacts alignment results in multilingual settings. The expected gain when using a language as the hub can be quantified based on accuracy. Results show that Az is the worst hub language choice among 10 European languages, with an expected loss of -0.4 due to its distance from other languages. The choice of hub language significantly impacts alignment results in multilingual settings. Az is the worst hub language choice among 10 European languages, with a lower quality of pre-trained embeddings due to its smaller dataset. Hi and Sv also show expected loss in the experiments. English exhibits a positive expected gain, but languages like Es, Gl, and Ru have larger expected gains in different experiments. The language subset composition could impact these numbers, but the trends remain consistent. The best hub language for multilingual alignment significantly impacts results. Even after removing certain pairs, Es remains the language with the largest expected gain. Choosing a hub language that is neither the source nor the target of the evaluation set yields the best accuracy in over 93% of cases. Typological similarity to the source or target languages does not necessarily lead to better performance. In multilingual alignment, using a hub language different from the source and target languages leads to better accuracy in over 93% of cases. Learning mappings for both language spaces allows for more flexible alignment and better downstream performance. This contradicts the mathematical intuition that a model learning a single mapping is as expressive as a model learning two mappings for each language. Downstream performance is also influenced by the distance between languages. The downstream performance in multilingual alignment is influenced by the distance between languages. Genealogical distance and Gromov-Hausdroff distance are used to approximate language distances, with a positive correlation found between performance and genealogical distances. In multilingual alignment, downstream performance is influenced by language distance. Positive correlation is found between performance and genealogical distances. Pearson's correlation coefficient between P@1 and d gen is 0.49 for distant languages and 0.38 for European languages. High correlation between performance and GH distances. Bilingual, trilingual, and multilingual systems are compared, showing the importance of using multiple languages for typologically distant languages. In multilingual alignment, downstream performance is influenced by language distance. Positive correlation is found between performance and genealogical distances. Pearson's correlation coefficient between P@1 and d gen is 0.49 for distant languages and 0.38 for European languages. High correlation between performance and GH distances. Bilingual, trilingual, and multilingual systems are compared, showing the importance of using multiple languages for typologically distant languages. One should use as many languages as possible to achieve higher performance with multilingual systems containing related languages and one hub language. The choice of the hub language affects lexicon induction performance in both bilingual and multilingual settings. New dictionaries and baseline results on several language pairs are provided to encourage community evaluation. Our analysis provides insights and directions for stronger baselines for non-Anglocentric cross-lingual word embeddings. Evaluation directionality matters, especially for distant languages with different morphological complexity. For example, in European languages, word translation accuracy is significantly higher when evaluating in specific directions. In European languages, word translation accuracy is significantly higher when evaluating in specific directions, with a notable difference in P@1 between language pairs. This observation holds true even for closely related languages, indicating the impact of pre-trained embeddings quality. Directionality differences are not observed in distant language pairs with presumably high-quality embeddings. Evaluation results for multilingual experiments are provided in tables, showcasing P@1, P@5, and P@10 for European languages and distant languages. Tables 12, 13, and 14 display experiment results, while Table 15 shows the P@1 of bilingual experiments using MUSE. The accuracy varies between language pairs, with differences observed even in closely related languages."
}