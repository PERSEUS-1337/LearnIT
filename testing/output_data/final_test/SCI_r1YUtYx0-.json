{
    "title": "r1YUtYx0-",
    "content": "In this work, the focus is on the robustness approach in deep learning algorithms. The hypothesis error should not change much with perturbations to training examples for good generalization. A new approach called ensemble robustness is introduced, which looks at the robustness of a population of hypotheses. It is found that stochastic learning algorithms can generalize well if their sensitivity to adversarial perturbations is limited. The text discusses the importance of deep learning algorithms being robust to adversarial perturbations for good generalization. Extensive simulations show a strong correlation between different algorithms and network architectures in this aspect. The text discusses the connection between the robustness of deep learning algorithms and their generalization performance. Deep Neural Networks (DNNs) have shown remarkable performance but often have more parameters than training samples, leading to a need for new complexity measures for generalization. Researchers are exploring the relationship between algorithm robustness and generalization in DNNs. Xu & Mannor have shown that algorithm robustness guarantees generalization performance. In the context of DNNs, there is contradicting evidence between generalization and fragility to adversarial perturbations. Methods like adversarial training and Parseval regularization improve robustness and generalization in deep neural networks. The ensemble robustness approach aims to explain the generalization performance of deep learning algorithms by considering the connection between algorithm robustness and generalization. It focuses on the distribution of hypotheses generated by randomized algorithms like SGD, Dropout, and Bayes-by-backprop, rather than deterministic ones. This approach provides evidence, both in theory and simulation, that ensemble robustness plays a key role in deep learning's ability to perform well despite being vulnerable to adversarial examples. Ensemble robustness considers the robustness of the population of hypotheses generated by randomized algorithms in deep learning. It suggests that deep learning methods are typically robust despite being fragile to adversarial examples. The approach shows that randomized learning algorithms can generalize well if the output hypothesis is not highly sensitive to perturbations on average. Ensemble robustness in deep learning methods can be a reliable indicator of performance if hypotheses consistently perform well in terms of robustness. Empirical estimates of ensemble robustness show its role in explaining the generalization performance of deep neural networks. This measure, based solely on training data, allows for selecting the best model based on its ensemble robustness. This work focuses on model robustness for estimating generalization performance for deterministic algorithms like SVM and Lasso. It suggests using robust optimization to minimize empirical loss with adversarial perturbed training examples. Stochasticity in deep learning algorithms has been successful, with Dropout introduced to control over-fitting. The study extends results to randomized algorithms for analysis. This work extends results from deterministic algorithms to randomized algorithms, focusing on ensemble robustness and stability in learning algorithms. Robustness involves global modifications of training data, while stability is more local. The study emphasizes the differences between the two properties and their impact on generalization performance. In this work, the focus is on investigating the generalization property of stochastic learning algorithms in deep neural networks by establishing their PAC bounds. The study aims to develop the approach of ensemble robustness by providing preliminary facts and introducing the problem setup. The text discusses the problem setup for deep neural networks, focusing on the inherent randomness of deep learning algorithms and the relationship between robustness and generalization performance. It introduces the sample set Z, hypothesis set H, and the training sample set s, with the goal of minimizing expected classification error. The learning algorithm A is a mapping from Zn to H, with a fixed size training set s. The learning algorithm A maps Zn to H with a fixed size training set s. The loss function is nonnegative and bounded by M. Deep learning algorithms are often randomized, producing a distribution of hypotheses instead of a single one. This randomness is formalized before analyzing their performance. Definition 1 (Randomized Algorithms) defines a randomized learning algorithm A as a function from Zn to a set of distributions of hypotheses \u2206(H), outputting a hypothesis h s \u223c \u2206(H) with probability \u03c0 s (h). The goal is to minimize the expected empirical loss for a specific output hypothesis h s. Internal randomness in deep learning algorithms includes dropout rate, random shuffle in SGD, and weight initialization. BID23 established the link between algorithmic robustness and generalization. The relation between algorithmic robustness and generalization is explored for the first time. Robust algorithms ensure that close samples have similar associated losses. Xu et al. proved that robust algorithms also generalize well. Shaham et al. introduced an adversarial training algorithm to minimize empirical loss over synthesized adversarial examples. However, these results do not fully characterize the performance of modern deep learning models. Understanding the internal randomness of deep learning algorithms is crucial for explaining their proper performance. Deep learning algorithms output hypotheses sampled from a distribution, leading to good generalization. Ensemble robustness is defined over the distribution of output hypotheses, ensuring good generalization for all samples. Theorem 1 states that a randomized algorithm with ensemble robustness can provide good generalization performance. There is a trade-off between the ensemble robustness measure and the partition size. The tradeoff between ensemble robustness and partition size is highlighted in Theorem 2. Theorem 1 leads to Corollary 1, which provides a way to minimize expected loss directly for deep learning algorithms. Corollary 1 suggests minimizing expected error in deep learning by minimizing empirical error over adversarially perturbed training samples. Theorem 2 states that ensemble robustness is a weaker requirement compared to Robustness. When \u03b1 = 0, the algorithm becomes deterministic. To achieve the upper bound, the output hypothesis must satisfy a certain condition. Deep neural networks often have a large deterministic model robustness measure. Allowing variance in hypotheses can lead to multiple possible outputs. By tolerating non-robustness in some hypotheses, a randomized algorithm can still perform well as long as ensemble robustness is low. Generalization of deep learning models is shown to be more correlated in simulations. In simulations, generalization of deep learning models is found to be more correlated with ensemble robustness than individual model robustness. The experiments were conducted on two datasets, MNIST and NotMNIST, using multi-layer perceptrons without cross-validation data. In simulations, multi-layer perceptrons with varying network architectures were used to evaluate ensemble robustness and generalization performance of deep learning algorithms. Different ensemble training methods were compared, including explicit ensembles trained with stochastic algorithms. A mini-batch size of 128 training examples was used, and experiments were conducted on MNIST and NotMNIST datasets without cross-validation. The algorithm was implemented using SGD to minimize cross-entropy loss. Implicit ensembles were created by learning a probability distribution on neural network weights. Adversarial training was added to ensemble methods with perturbations sampled uniformly over {0.1, 0.3, 0.5}. Specific configurations refer to unique sets of parameters. The ensemble robustness of a DNN is highly correlated with its generalization performance, validated empirically using adversarial examples. K partitions are defined around training examples to approximate loss changes, showing correlation with generalization. This partition method does not violate the i.i.d assumption for stochastic algorithms. The i.i.d assumption is not violated for general stochastic algorithms, but it is for adversarial training. Despite this, stronger correlation is observed for these algorithms. Ensemble robustness in deep learning is computationally intractable to measure exactly, so empirical averages are used. Different hypotheses of the same learning algorithm are considered for ensemble robustness. SGD variants collect output hypotheses by repeating training with different random seeds, while Bayes-by-backprop methods output a distribution over output. In the case of Bayes-by-backprop methods, the algorithm outputs a distribution over output hypotheses, and we sample networks from the learned weight distribution. We aim to show that a deep learning algorithm with stronger ensemble robustness has better generalization performance. Ensemble robustness is defined as finding the most adversarial perturbation for a specific training sample within a partition set. We use an approximate search strategy to find adversarial examples by optimizing a first-order Taylor expansion of the loss function with a pre-defined magnitude constraint. In simulations, the magnitude constraint r is varied to calculate empirical ensemble robustness at different perturbation levels. The ensemble robustness is determined by averaging the difference between algorithm loss on training samples and adversarial samples. The generalization performance of learning algorithms and networks is compared based on empirical ensemble robustness on MNIST. A high correlation is observed between ensemble robustness and test error. In simulations, the correlation between ensemble robustness and generalization for all learning algorithms is high. Results on the notMNIST dataset show a positive correlation, with Bayes-by-backprop algorithm having a lower correlation. Comparing ensemble robustness with robustness on MNIST, ensemble robustness shows a higher correlation with generalization performance. In this paper, the investigation focused on the generalization ability of stochastic deep learning algorithms based on ensemble robustness. The theory and experiments suggest that DNNs can be robust and generalize well, despite being vulnerable to specific adversarial examples. Measuring ensemble robustness may be computationally challenging, but learning the probability distribution of neural network weights can help address this issue. Ensemble robustness can be used to measure generalization error without testing examples. Future research will explore using ensemble robustness for model selection and studying deep learning methods' resilience to adversarial attacks. Explicit ensemble methods show promise in defending against adversarial attacks compared to implicit methods. In this section, ensemble robustness is highlighted as a defense strategy against adversarial attacks in deep learning. Using dropout as an example, it is shown how random perturbations can optimize neural network models. Theorem 2 suggests that randomized algorithms can tolerate non-robust hypotheses, shedding light on the generalization gap in datasets. This insight is left for future exploration. The text discusses the use of dropout in deep neural networks, where random units are masked out during parameter updating. The randomness of the algorithm is parametrized by r = (r1, ..., rL) for L layers. The generalization performance of dropout training is established, showing how ensemble robustness can defend against adversarial attacks. Theorem 3 highlights the output hypothesis distribution and the probability of success with respect to random draws. The text discusses the importance of controlling the variance \u03b2 in neural network models during dropout training. Increasing the layer number L can improve performance when \u03b2 converges at the rate of L \u22123/4. Voting from multiple models is used to reduce variance and decrease generalization error. Smaller variance in model performance is preferred when dropout training is applied to more layers, which can be achieved by increasing training example size or ensemble of multiple models. The text discusses the importance of controlling variance in neural network models during dropout training. Voting from multiple models is used to reduce variance and decrease generalization error. Lemma 1 and Lemma 2 provide conditions for a randomized learning algorithm with uniform ensemble robustness and loss function constraints. The text discusses controlling variance in neural network models during dropout training. Lemma 3 introduces the Bounded Difference Inequality for analyzing generalization performance. Theorem 1 is proven using Chebyshev's inequality and Lemma 2. The proof of Theorem 1 involves using Chebyshev's inequality and Lemma 2 to derive bounds on generalization error. By applying Lemma 3, it is shown that with high probability, the difference between observed and expected risk is bounded by a function of \u03b4. This result holds simultaneously with another inequality, leading to a final bound on the risk. The final bound on the risk is R(s, r) \u2264 \u03b2 2L log(1/\u03b4) +\u00af (n) + 2K ln 2 + 2 ln(2/\u03b4) n."
}