{
    "title": "BJxlmeBKwS",
    "content": "People with high-frequency hearing loss use hearing aids with frequency lowering algorithms to shift sounds from high to lower frequency bands for better perception. Timely and accurate detection of fricative phonemes is crucial for quality hearing aids. A deep learning based fricative phoneme detection algorithm with zero delay is presented in this paper, achieving state-of-the-art accuracy on the TIMIT Speech Corpus. The results are reproducible and include easy-to-use code for future research. High-frequency hearing loss affects word recognition in conversations due to difficulty discriminating high-frequency sounds, especially fricative phonemes. Modern hearing aids may not fully address this issue, prompting researchers to suggest frequency lowering approaches. Robinson et al. (2009) demonstrated improved detection and recognition of fricative sounds using a frequency transposition technique. In high frequency hearing impaired individuals, detecting fricative phonemes with zero delay is crucial for real-time sound alterations in hearing aids. This paper explores using deep learning, specifically a fully convolutional neural network, to achieve this. The network analyzes raw speech segments to predict the likelihood of the segment ending with a fricative phoneme. The network analyzes raw speech segments to predict the likelihood of the segment ending with a fricative phoneme, with zero statistical delay. The green segment in Figure 1 spans four phonemes, including the fricative phoneme /z/, which the network must accurately detect. The TIMIT Speech Corpus contains raw speech sentences and phonetic labels for training a network to make accurate inferences about future phonemes based on currently observed phonemes. It includes 6300 sentences spoken by 630 speakers from 8 dialect regions in the US, with a training set of 462 speakers and a test set of 24 speakers. A validation set was created following a specific methodology. After excluding SA sentences from the TIMIT dataset to prevent bias, 8 sentences per speaker were left. To maintain balance, 16 speech segments were extracted per sentence with a 50/50 fricative-to-non-fricative ratio. The training set consisted of 59136 speech segments with 8 fricative and 8 non-fricative segments per sentence. The segments were randomly placed and regenerated for each epoch. The validation set was formed in a similar manner but remained fixed for different training runs and epochs. The model used standard deviation normalization for speech segments and had a layer structure with 5 stages, each consisting of convolutional layers with specific kernel sizes, stride values, and number of filters. The model utilized residual connections in stages 2-5, applying zero padding when filter dimensions did not match. A 1D global average pooling layer was used after the convolutional stages, with a total of 1.1 million parameters in the model. The model utilized residual connections in stages 2-5, with a total of 1.1 million parameters. The network architecture was extensively experimented with, and the network in Table 2 showed the best performance. The input size of 3072 samples corresponds to 2-3 phonemes, providing abundant information. The network receives raw speech signal as input, enabling the learning of problem-specific low-level features. The text discusses the use of Mel-Frequency Cepstrum Coefficients (MFCC) for feature extraction in a classifier, with a window size of 25 ms. The classifier must learn to extrapolate detections into the future due to time-frequency uncertainty. Additionally, a high-quality time-frequency filterbank and recurrent neural network were used for fricative detection, approaching but not surpassing the performance of the model in Table 2. The text discusses using time-frequency transformation followed by a recurrent neural network for detection accuracy. They used binary crossentropy loss, Adam optimizer, and applied various techniques for efficient training. The study trained a network multiple times and selected the best performing one for validation. They evaluated the detection algorithm on a test set from the TIMIT dataset, explaining the process of using a sliding window with the trained network for inferences. The study trained a network multiple times and selected the best performing one for validation. They evaluated the detection algorithm on a test set from the TIMIT dataset using a sliding window approach with posterior probabilities for fricative phoneme detection. The network accurately detected the beginning and end of fricative phoneme regions, and a threshold was applied based on the posterior probabilities to maximize the Unweighted Average Recall (UAR) of the network. The study evaluated a detection algorithm on a test set using a sliding window approach with posterior probabilities for fricative phoneme detection. They used Unweighted Average Recall (UAR) as the metric due to the unbalanced test set. Precision rates and F1-scores were also reported for fricative and non-fricative phonemes. Previous research used S-Transform and Linear Prediction Coefficients for fricative phoneme detection. The authors evaluated a fricative phoneme detection algorithm using majority voting and tested it on 40 sentences from the TIMIT dataset. The reported recall rate for fricative phonemes was 83.81%, but the recall rate for non-fricative phonemes was not provided. Ruinskiy & Lavner (2014) also investigated fricative phoneme detection using time and frequency domain features. The authors achieved a fricative phoneme detection rate of 94.08% UAR on the TIMIT dataset using domain features and postprocessing techniques. The evaluation included unvoiced fricative phonemes and highlighted potential biases in the reported detection rate. The non-causal post-processing method used may be suitable for certain applications like speech-to-text conversion. The generic phoneme identification task is more challenging than fricative phoneme detection, with a higher number of classes to identify individually. Success in phoneme identification relies on processing input context forward and backward in time from the speech frame. This approach introduces a delay in real-time applications like hearing aids. The algorithm used for phoneme identification introduces a delay due to processing input context. Different methods report varying recall rates for fricative phonemes, with significant detection delays. The algorithm for phoneme identification introduces delays due to processing input context. Various studies have addressed phoneme detection on the TIMIT dataset using neural networks, with majority voting post-processing leading to delays in phoneme-length detection. Comparison of results with other approaches on fricative phoneme detection in the TIMIT dataset is provided in Table 4. Table 4 compares results from different approaches on fricative phoneme detection. Our study closely relates to Ruinskiy & Lavner (2014), where both used majority voting for unvoiced fricative phonemes. We also conducted a fair comparison by testing our detection with the same phonemes and applying majority voting. Further comparison with ROC curves is shown in Figure A.3 in the Appendix. Our basic approach, evaluated with all 8 fricative phonemes, showed promising results without introducing delays. The rates for the basic approach were evaluated with unvoiced fricative phonemes, followed by majority voting. Rates reported were obtained using the same evaluation strategy as Ruinskiy & Lavner (2014), except for median filtering. Voiced fricative phonemes /v/ and /dh/ can be easily confused with vowels, reducing recall. The majority voting post-processing method improved the performance of the network to 96.41% UAR rate, a 39% decrease in error rate compared to the previous study. Even without post-processing, the network achieved a 94.76% UAR rate, better than the previous study's 94.08% UAR rate. In this paper, the method introduced has zero delay as detection is based only on past signals. However, computational aspects like neural network inference time introduce a delay depending on the computer's power, network size, and software optimization. Future hearing aids could potentially use low-power chips for neural network inference, similar to those found in modern phones. Future hearing aids could potentially use low-power chips for neural network inference, similar to those found in modern phones. A smaller model is more efficient, with Net25h having fewer parameters, faster processing speed, and slightly lower detection rates compared to Net25. The network aims to extrapolate fricative phoneme detections into the future by retraining with relabeled data. The goal is to calculate the probability of a sample in the future belonging to a fricative phoneme. The training process involves retraining the network with relabeled data to extrapolate fricative phoneme detections into the future. Performance gracefully decays with increasing extrapolation gap, with our method outperforming previous ones even in harder detection tasks. The research directions include restructuring the computational graph of the evaluation pipeline to utilize the data structure efficiently and reducing network size for low-power devices. Various deep learning compression techniques can be used for this purpose. Additionally, a table provides recall percentages for fricative phoneme detection tasks in related approaches. The curr_chunk discusses the detection of phoneme boundaries, specifically focusing on fricative phonemes. It introduces the R-value metric for measuring boundary identification quality. The curr_chunk discusses the performance of Net25 in identifying phoneme boundaries for fricative phonemes. The results are not state-of-the-art, and future work is needed to improve error sources and understand the importance of precise boundary detection for hearing aid devices."
}