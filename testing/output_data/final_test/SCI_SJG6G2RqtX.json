{
    "title": "SJG6G2RqtX",
    "content": "Value Propagation (VProp) is a parameter-efficient differentiable planning module based on Value Iteration, trained using reinforcement learning to solve new tasks and generalize to larger map sizes. It can navigate in dynamic environments and learn to plan in the presence of stochastic elements, offering a cost-efficient system for interactive navigation problems. Evaluation on MazeBase grid-worlds and a StarCraft scenario shows its effectiveness in planning for artificial agents across various domains. Learning to plan in dynamic environments is a challenging task due to the increasing complexity of states and environment dynamics. To address this, research has focused on developing methods that can learn planners without requiring traces from an optimal planner. These methods should also be able to generalize and function on unseen instances and planning horizons. In Reinforcement Learning, learning to plan involves finding a policy that maximizes the expected return from the environment. Value Iteration (VI) is a commonly used algorithm in Reinforcement Learning to estimate state values by propagating observed rewards. By using deep convolutional neural networks (CNN) on occupancy maps, Value Iteration Networks (VIN) can be trained to learn the value function through end-to-end learning. In this work, Value Iteration Networks (VIN) are extended to better represent grid-world-like scenarios, allowing for their use in reinforcement learning beyond the initial scope. This approach removes limitations and underlying assumptions, enabling models to learn to plan and navigate without relying on good target value estimates. Our models, VProp and MVProp, can plan and navigate in dynamic environments with a hierarchical structure that allows for generalization to larger maps. They learn pathfinding tasks via reinforcement learning with minimal parametrization and can generalize to unseen maps. The modules can handle environments with complex dynamics beyond a static grid world. Results have been achieved in various tasks and environments using Deep Reinforcement Learning. Navigation tasks in 3D and 2D have been addressed within the RL framework, with some methods utilizing 2D occupancy maps for localization and feature grounding. Other approaches have explored VIN-like architectures for navigation problems, such as generalizing VIN to learn modules on generic graph structures and multi-agent planning in a cooperative setting. Additionally, strategies have been proposed to handle partially observable settings. In mobile robotics, hierarchical planners using VI modules are proposed to handle partially observable settings. The goal is to learn to plan through reinforcement learning in a \"grid world\" environment where entities interact based on attributes. Learning a policy trained on various environment configurations is essential for generalizing to larger environments and more entities. Reinforcement learning involves learning a policy to guide an agent through an environment to reach a goal efficiently. The process includes observing the environment, extracting entities using an embedding function, and formulating optimal policies for a Markov Decision Problem (MDP). An MDP consists of states, actions, a state-transition matrix, a reward function, and a discount factor. The goal is to find an optimal policy that maximizes the expected sum of discounted rewards. Reinforcement learning involves learning optimal policies for a Markov Decision Problem (MDP) by maximizing the expected sum of discounted rewards. Different algorithms like Value Iteration, policy gradient methods, and actor-critic algorithms are used for this purpose. The policy gradient methods compute the policy directly, using low-variance performance estimation from value-based RL as feedback. Value Iteration can be represented as a graph convolutional network for navigation problems, with nodes as agent positions and edges as possible transitions. In 2D grids, the graph structure corresponds to the neighborhood, and the convolution structure resembles a convolutional network taking the entire environment as input. The Value Iteration module is defined by an embedding function of the state, a transition function, and control policy computation for agent positions in a 2D grid. The transition function is represented as a convolutional layer, enabling efficient computation of action values. The Value Iteration module uses a convolutional layer to compute action values in a 2D grid. It involves a specific parameterization of rewards and transition probabilities, corresponding to the original value iteration algorithm for grid worlds. The Value Iteration module implements reward parameterization and transition probabilities using convolutional networks. This approach allows for efficient computation and clear motivation for weight sharing between layers, guiding the network depth based on the propagation of reward signals. The shared weights reduce sample complexity and enable interesting generalization possibilities. The Value Iteration module uses convolutional networks for reward parameterization and transition probabilities, enabling efficient computation and weight sharing. However, learning VI modules with reinforcement learning remains challenging due to sample complexity issues. Two alternative approaches are proposed to improve sample complexity and generalization while maintaining a deep network with shared weights. In the Value Iteration module, the design choices involve making transition probabilities state-dependent and parametrizing reward functions more constrained for increased sample efficiency in grid worlds. The dynamics in a grid world are deterministic with actions limited to moving into adjacent cells, considering blocking cells and goal states in the world model and reward function. Value Propagation (VProp) is an architecture proposed to account for terminal states in episodic tasks when implementing value iteration. It uses deep convolutional networks with shared weights and an embedding function \u03a6 to model reward functions, including absorbing states. Value Propagation (VProp) utilizes deep convolutional networks with shared weights and an embedding function \u03a6 to model reward functions, including absorbing states. Absorbing states are represented by reward values for different positions, with a propagation parameter p i,j indicating the state-dependent discount factor. The algorithm uses a sigmoid activation function to keep all values within [0, 1]. The architecture of Value Propagation (VProp) utilizes deep convolutional networks with shared weights and an embedding function \u03a6 to model reward functions, including absorbing states. It is designed for environments that behave like 2D grid structures, such as robotic navigation. The mapping from propagated values to actual actions can be done using DISPLAYFORM2 for unknown mappings. The VI module and VProp face difficulties when generalizing to larger environments due to obstacles being represented in different ways. The Max-Propagation module (MVProp) proposes a new approach where only positive rewards are propagated to represent blocking paths, overcoming the limitations of negative rewards in larger environments. The MVProp module focuses on propagating only positive rewards in value iteration, overcoming the limitations of negative rewards. It employs an actor-critic architecture with experience replay for training in Reinforcement Learning. The actor-critic architecture with experience replay is used for training in Reinforcement Learning. Transition traces are collected with observations, actions, probabilities, and rewards. The architecture includes a policy \u03c0 \u03b8 and a value function V w. Gradient ascent is performed over importance-weighted rewards during training. The capped importance weights control variance in off-policy policy gradient. The update term acts as a regularizer for model predictions. Learning rates control weighting of objectives. Comparison between VIN dataset and custom grid-worlds with increasing blocks but fixed percentage. Agent and goal shown as circles in grid-world setting. In a 2D grid-world setting, entities are sampled from a fixed distribution. The agent can move in 8 directions until reaching a terminal state by reaching the goal or hitting a wall. MazeBase BID21 is used to generate world configurations. Trained agents are evaluated on maps from a 16x16 dataset for comparison with previous work. Other datasets (8x8 and 28x28) show no significant changes in performance. A curriculum is employed to bound the optimal path length from the starting position. The optimal path length from the starting agent position is bounded, gradually increasing after a few training episodes. VProp and MVProp outperformed other models in training rewards. Performance was tested on VIN dataset and 64x64 maps. Training rewards were averaged across 5 runs. Original VIN architecture was mostly tested in a fully supervised setting. The authors claim that VIN can perform well in a RL setting, achieving an 82.5% success rate on a 16x16 map. Results for larger maps are not provided, but overall performance is consistent with the best results obtained. VProp and MVProp demonstrate strength in static-world experiments, quickly outperforming the baseline and learning transition functions effectively. The study demonstrates the effectiveness of models in learning non-static environments by testing them with dynamic adversarial entities controlled by various fixed policies. These policies include noop, direction, and adversarial strategies, enhancing standard path-planning experiments. In dynamic experiments, agents navigate environments with different entities and policies, including fixed walls and adversarial entities. The goal is to reach the target quickly while avoiding obstacles. The scenarios vary in difficulty, requiring agents to learn from sparse rewards and complex transitions. The study shows the effectiveness of models in adapting to changing environments. In dynamic experiments, agents navigate environments with various entities and policies, aiming to reach the target while avoiding obstacles. Training on 8x8 maps allows for successful generalization to larger 32x32 maps, outperforming the baseline. The study also evaluates VProp on a navigation task in StarCraft: Brood War with low-level physical dynamics. In StarCraft: Brood War, planning trajectories around enemy units is crucial due to their auto-attack behaviors. Using TorchCraft, the environment is set up to simulate scenarios with randomly spawned units. The state space is larger, and positive rewards are sparse, so a mixed curriculum is employed to sample units and positions for the models. The mixed curriculum approach is used to sample units and positions for models in StarCraft: Brood War simulations. MVProp enables planning around move action noise, allowing agents to navigate around enemies. The generated trajectory shows efficient learning of the state-action transition function, leading to accurate planning modules in a non-trivial environment. The architecture with added convolutional layers and max-pooling struggles to condition correctly on complex transition functions in navigation tasks. Planners learned from data need to be sample efficient to adapt quickly to environment dynamics for flexible planning horizons. Reinforcement Learning can successfully train such planners when task dynamics are considered. Reinforcement Learning can successfully train planners to generalize in 2D path-planning tasks, even in dynamic and noisy environments. Computational cost remains a challenge, but architectures using VI modules show promise in tackling complex tasks. VProp and MVProp can be applied to mobile robotics and visual tracking tasks, learning arbitrary propagation functions for graph structures. They belong to the class of graph convolutional neural networks and can be used in various graph-like structures for pathfinding tasks. Our work focuses on parametrization relevant in navigation and pathfinding scenarios, using graph-convolutional neural networks. The method differs from the original VIN by the reward parametrization and focus on deterministic models for navigation problems. All models and agent code were implemented in PyTorch. The models and agent code were implemented in PyTorch BID15, with shared learning hyperparameters for fair comparison. An n-step memory replay buffer was used, keeping only the last 50000 transitions. RMSProp was used instead of plain SGD, with a learning rate of 0.001 and mini-batch size of 128. Learning updates were set at a frequency of 32 steps. The agents navigated the maze quickly to minimize total cost, with discrete collision boundaries and constraints on entity presence in cells. Illegal actions terminated episodes, with rewards for valid movements based on cost. In experiments, the agent receives a negative reward for moving based on cost, with a positive reward for reaching the goal. The environment consists of obstacles and narrow paths, with different ratios of blocks and entities in dynamic environments. The study tested various models on static grid-worlds with adversarial agents and mixed environments. Results showed varying performance levels across different map sizes, with VIN VProp MVProp models achieving high success rates. The study tested VIN, VProp, and MVProp models on grid-world environments with adversarial agents. The models used 8 input filters and unpadded convolutions. TorchCraft was set to skip 15 frames to observe changes effectively. All entities were spawned similarly to grid-world experiments. At training time, a fixed max-distance curriculum was used to increase the distance between spawned goal and agent gradually. Enemies were prevented from spawning in the first 500 episodes to allow the stochastic policy to condition on the goal. A fixed 8x downsampling operator was used to reduce the size of raw observations from 480x360 pixels to 60x45. The downsampled observation was either fully featurised in a grid-world fashion or transformed into greyscale. The model's capacity was increased by adding 2 additional convolutional layers with 32 filters and max-pooling layers before the recurrent step."
}