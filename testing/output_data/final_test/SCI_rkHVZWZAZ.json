{
    "title": "rkHVZWZAZ",
    "content": "In this work, a new agent architecture called Reactor is introduced, combining various algorithmic contributions for higher sample-efficiency than Prioritized Dueling DQN and Categorical DQN, and better run-time performance than A3C. The contributions include a new policy evaluation algorithm called Distributional Retrace, the \u03b2-leaveone-out policy gradient algorithm, and a prioritized replay algorithm for sequences. The new prioritized replay algorithm for sequences in Reactor improves sample efficiency and agent performance in Atari 2600 benchmarks. Reactor achieves state-of-the-art performance after 200 million frames and less than a day of training. Value-function agents benefit from improved sample complexity but suffer from long runtimes. Actor-critic agents, like A3C, train on transitions collected by multiple actors in parallel. Deep actor-critic agents have worse sample complexity but train faster in terms of wall-clock time. Various hybrid approaches exist for training algorithms in Markov decision processes. The Deep Q-Network (DQN) framework, introduced by Mnih et al. (2015), revolutionized deep reinforcement learning by achieving human-level performance in Atari 2600 games. The goal in reinforcement learning is to find an optimal policy that maximizes action-values. DQN popularized research in this area and set a new benchmark in performance. The DQN framework uses a deep convolutional neural network to approximate action-values in Atari 2600 games. It trains the network using the Q-Learning algorithm with mini-batches of transitions from an experience replay buffer. The target network is periodically updated to match the current network, and the algorithms use the temporal difference error for each transition. Numerous extensions and improvements have been made since the seminal work by Mnih et al. (2015). Recently, extensions and improvements to the DQN framework have been introduced, such as Double DQN, dueling architecture, and Rainbow. Rainbow combines various improvements into a single agent, outperforming previous methods. However, it inherits the time-efficiency issues of the DQN framework. Further recent improvements to DQN are discussed in the section, including the introduction of experience replay. Prioritized experience replay, inspired by prioritized sweeping, replaces uniform sampling with sampling proportional to TD error. This method significantly improves sample-efficiency and final performance over DQN on Atari 2600 benchmarks. Retrace(\u03bb) is an off-policy multi-step algorithm that extends the DQN agent and aims to evaluate the value of a target policy \u03c0 by updating the current estimate Q in the direction of the temporal difference. The algorithm guarantees convergence to Q \u03c0 in finite state and action spaces, or to Q* for increasingly greedy policies. Distributional reinforcement learning involves algorithms that estimate the distribution over returns, with C51 being a notable example that showed high performance in Atari 2600 benchmarks. Actor-critic frameworks in reinforcement learning have seen recent advances, such as the asynchronous advantage actor-critic (A3C) algorithm, which maintains a parameterized policy and value function. Actor-Critic Experience Replay (ACER) extends the A3C framework with an experience replay buffer, Retrace. ACER collects trajectories independently in the environment and uses asynchronous updates to a shared set of parameters. It is closely related to Proximal Policy Optimization (PPO), which performs synchronous updates with a different advantage calculation. The Reactor is a novel addition to deep RL algorithms, combining improvements in value-based RL and policy-gradient methods. It aims for sample and time efficiency by integrating a policy and action-value function. The policy gradient algorithm trains the actor using the current Q-value estimate. Various methods are considered to estimate the gradient for policy optimization. The Reactor integrates policy and action-value functions for sample and time efficiency in deep RL algorithms. Various methods are considered to estimate the gradient for policy optimization, including the use of likelihood ratio and importance sampling ratios to build unbiased estimates of the quantity G. The leave-one-out policy-gradient estimate is suggested as a way to reduce variance in estimating G directly. The leave-one-out (LOO) policy-gradient estimate aims to reduce variance in estimating G directly, but may be biased if Q values differ from Q \u03c0. A better bias-variance tradeoff is achieved with the \u03b2-LOO policy-gradient estimate, where \u03b2 is a function of both policies, \u03c0 and \u00b5, and the selected action \u00e2. This estimate is unbiased and cancels bias by using a state-and-action-dependent baseline and a correction term \u2207\u03c0(a)Q(a). The Q-estimates are close to the true Q \u03c0 values and unbiased with \u03b2(a) = 1/\u00b5(a). To improve bias-variance tradeoff, use the \u03b2-LOO estimate with \u03b2 defined as \u03b2(\u00e2) = min c, 1 \u00b5(\u00e2). This approach reduces variance and shares similarities with the truncated IS gradient estimate. In off-policy learning, producing an unbiased sample R(\u00e2) of Q \u03c0 (\u00e2) when following another policy \u00b5 is challenging. In Reactor, we extend the C51 algorithm to include multi-step Bellman backups and handle off-policy correction using the Retrace algorithm. The Retrace algorithm provides a biased estimate of Q \u03c0 (\u00e2) that converges asymptotically. We predict return distribution functions from state-action pairs and consider extensions to handle multi-step updates. Reactor extends the C51 algorithm by incorporating multi-step Bellman backups and off-policy correction with the Retrace algorithm. The n-step backed-up return-distribution is built from observed sequences, and a projection step minimizes the KL-loss between the target and current estimate. The Distributional Retrace algorithm involves off-policy correction for Q \u03c0 (\u00e2) estimation. The Retrace algorithm involves off-policy correction by rewriting it as a linear combination of n-step Bellman backups weighted by coefficients \u03b1 n,a. These coefficients depend on the degree of off-policy-ness along the trajectory and in expectation, the Retrace update can be seen as a convex combination of n-step Bellman updates. The Retrace algorithm involves off-policy correction by combining n-step Bellman updates using coefficients \u03b1 n,a. The Retrace target distribution is defined as a mixture of n-step distributions, updated using a gradient step on the KL-loss. The update rule provides an unbiased estimate of the gradient between the expected Retrace target distribution and the current predicted distribution. This method can also be applied to other algorithms like TB(\u03bb) and importance sampling. Prioritized experience replay boosts efficiency and performance of deep RL agents. However, it does not handle sequences of transitions and treats all unsampled transitions equally. Lazy initialization is proposed as an alternative strategy to better encode temporal difference errors. A computationally efficient prioritized sequence sampling algorithm is briefly described, with detailed information in the appendix. Temporal correlation of TD errors is a key factor in the justification for using experience replay. Our proposed algorithm for prioritized experience replay improves efficiency by adding experience to the buffer without priority, assigning priority only after sampling for training. Sequences of length n are assigned priorities, with sampling based on these priorities. The proposed algorithm for prioritized experience replay improves efficiency by assigning priorities to sequences of length n based on local neighborhood averages. Lazy initialization extrapolates priorities for unassigned sequences using a weighted average scheme. The proposed algorithm, called Contextual Priority Tree (CPT), assigns priorities to sequences based on cell sizes as estimates of inverse local density. The algorithm ensures unbiasedness by allowing freedom in choosing partitions and cell collections. CPT, based on AVL trees, can perform sampling, insertion, deletion, and density evaluation in O(ln(n)) time. Prioritization is viewed as a variance reduction technique. The architecture of the proposed algorithm, Contextual Priority Tree (CPT), involves decoupling acting from learning to improve CPU utilization. An acting thread interacts with the environment and stores transitions, while a learning thread trains on sequences of experiences. Acting steps are executed 4-6 times per learning step, with sequences of length 33 sampled in batches of 4. A moving network operates over frames 1-32, while the target network operates over frames 2-33. In the proposed algorithm, the network is unrolled over frames 1-32, while the target network is unrolled over frames 2-33. The agent can be distributed over multiple machines, each with its own local replay memory. Training involves downloading a shared network, evaluating local gradients, and applying them to the shared network. Results are presented with 10 or 20 actor-learner workers and one parameter server. The architecture allows decisions based on a short history of past observations in domains like Atari. The Reactor architecture uses a recurrent neural network to process observations and produce action-value distributions and policy probabilities. It is inspired by the duelling network architecture and utilizes an LSTM network for state-value and advantage logits. This approach allows for efficient evaluation of action-values over sequences of trajectories. The Reactor architecture utilizes an LSTM network for state-value and advantage logits, connected to a shared linear layer and convolutional neural network. Gradients from the policy LSTM are blocked for stability, allowing only gradients from the Q-network LSTM to back-propagate. The Reactor architecture uses an LSTM network for state-value and advantage logits, connected to a shared linear layer and convolutional neural network. Gradients from the policy LSTM are blocked for stability, allowing only gradients from the Q-network LSTM to back-propagate. The policy head aims for increased stability by avoiding positive feedback loops between \u03c0 and q i caused by shared representations. The Adam optimiser with a learning rate of 5 \u00d7 10 \u22125 and zero momentum was used. Reactor was trained and evaluated on 57 Atari games, showing that algorithmic improvements like Distributional retrace, beta-LOO, and prioritized replay all contributed to the final results. Prioritization was considered the most important component, with Beta-LOO outperforming the TISLR algorithm. Reactor was evaluated on 57 Atari games using different versions of the algorithm. The distributional version showed better generalization with random human starts. Mean and median human normalized scores were calculated, along with rankings of all algorithms. Reactor was compared against DQN and Double DQN across all games. Reactor outperforms various algorithms across all metrics in Atari games, including DQN and Rainbow, with under two days of training. Its performance continues to improve significantly with 500 million frames and four days of training. Reactor does not use Noisy Networks like Rainbow, which contributed to performance gains. Reactor, a new off-policy agent based on Retrace actor-critic architecture, achieves similar performance as the current state-of-the-art in Atari games. It outperforms various algorithms, including Rainbow, with significant real-time performance gains. The algorithm does not use Noisy Networks and demonstrates benefits from suggested algorithmic improvements. The benefits of algorithmic improvements like Distributional Retrace, beta-LOO policy gradient, and contextual priority tree are demonstrated. Little effort was spent on parameter optimization, exploring different values for learning rates and ADAM momentum. Commonly used mean and median human normalized scores have drawbacks, with a mean score favoring games where computers excel over humans. The comparison of algorithms on Atari games shows that a set of ten specific games can explain most of the variance in scores. Median human normalized scores have drawbacks as they do not reflect incremental improvements on hard games. To address these issues, improvements like Distributional Retrace and beta-LOO policy gradient are demonstrated. The evaluation of algorithms on Atari games focused on mean rank and Elo metrics to compare performance across different games. Separate scores were calculated for each algorithm using test evaluations with random starts. Elo scores were determined based on wins across games, with a ranking difference of 400 indicating significant odds. The evaluation of algorithms on Atari games focused on mean rank and Elo metrics to compare performance. A ranking difference of 400 corresponds to significant odds of winning. Contextual priority tree is a lazy prioritization implementation using a balanced binary search tree. AVL tree was chosen for its ease of implementation and balance. Sampling is done stochastically proportional to priorities. The contextual priority tree is a lazy prioritization implementation using a balanced binary search tree. Nodes in the tree have different priority estimates based on their relationships with other nodes. Summary statistics are tracked at each node and updated after tree rotations. The contextual priority tree utilizes mean-priority estimates for nodes, updating them based on specific rules. Sampling is done by traversing the tree from the root node, with probabilities calculated at each branching point. Insertion, deletion, and sampling operations are supported. The suggested algorithm for the contextual priority tree maintains probabilities at branching points, allowing for efficient insertion, deletion, sampling, and probability queries. Unknown priorities are estimated using nearby known priorities. New sequence keys are initially set as unknown priorities and only assigned after sampling. Priorities are updated by removing and reinserting key nodes to improve accuracy. The minimum probability of choosing a random action is set at 0.01. The minimum probability of choosing a random action is 0.01 and is hard-coded into the policy network. Reactor is compared to the Rainbow agent in terms of performance, highlighting their similarities and differences in architecture and algorithms. Both use a categorical action-value distribution critic, prioritized replay, and n-step Bellman updates, but they are fundamentally different algorithms based on different research lines. Rainbow and Reactor are different algorithms based on Q-Learning and A3C, respectively. They have distinct network structures and approaches to handling partial observability. Rainbow uses noisy linear layers and frame stacking, while Reactor uses standard linear layers and LSTMs after convolutional layers. Comparing the two directly is challenging due to these differences. Rainbow and Reactor have different network structures and approaches to handling partial observability. Rainbow uses noisy linear layers and frame stacking, while Reactor uses standard linear layers and LSTMs after convolutional layers. Rainbow's n-step update does not use off-policy correction, limiting it to small values of n. In contrast, Reactor uses distributional Retrace algorithm for off-policy correction, allowing larger values of n without performance loss. Both agents use prioritized replay buffers but store different information and prioritize differently. Rainbow and Reactor use different prioritized replay buffer structures. Rainbow stores tuples prioritized by TD error, while Reactor stores sequences prioritized based on neighboring sequences' priorities. Rainbow does not use \u03b5-greedy exploration like DQN. Rainbow and Reactor have different exploration approaches. Rainbow uses noisy linear layers for adaptive exploration, while Reactor uses entropy cost method to penalize deterministic policies. Rainbow may become greedy early on but still explores less frequent states. This may lead to deterministic trajectories and overfitting. Rainbow and Reactor have different exploration approaches. Rainbow's performance differs between evaluation under no-op and random human starts, possibly due to overfitting to trajectories. ATARI RESULTS show scores for each game evaluated with random human starts, with Reactor's scores averaged over 200 episodes."
}