{
    "title": "r1xywsC9tQ",
    "content": "The paper investigates mapping the hyponymy relation of WordNet to feature vectors to model lexical knowledge for use in machine-learning models. Two models are proposed: one leveraging fasttext for classification and the other fully supervised using WordNet as ground truth. The first model approaches state-of-the-art performance, while the second model achieves near-perfect accuracy. Distributional encoding of word meanings from large corpora has been useful for NLP tasks, based on a probabilistic language model of word sequences represented as feature vectors. These approaches aim to identify distributional patterns and fine-grained relations between words. Simple manipulations of distributional word embedding vectors may be inadequate for detecting other types of word relations. In this paper, the focus is on mapping distributional word embeddings to ontological relations, specifically the hyponymy relation. Various resources like WORDNET, FRAMENET, and JEUXDEMOTS provide ground truth for hyponymy but have symbolic representations not suitable for neural NLP models. Different approaches to encoding hyponymy relations in feature vectors have been proposed, but there is no consensus on the properties of such encodings. The paper aims to characterize the properties such an embedding should have and proposes two solutions. The paper focuses on mapping distributional word embeddings to the hyponymy relation, aiming to characterize the properties such an embedding should have. Two baseline models are proposed to approach these properties, with the goal of modeling the hyponymy relation given by WORDNET. The model should ideally be sound and complete with respect to the ground truth, but machine-learned models typically only approximate these properties to a certain level. The paper discusses mapping word embeddings to the hyponymy relation, aiming to define properties for the embeddings. Two baseline models are introduced to model the hyponymy relation from WORDNET. Machine-learned models approximate these properties to a certain level. The properties of partial soundness and partial completeness are defined, not restricting how the inclusion relation is generated. A classifier function P maps words to subspaces in a vector space, indicating if a word applies to a given feature vector. The inclusion model is fully characterized by a classifier function P, denoted as ((\u2286 P )). The relation (\u2286 P ) is reflexive and transitive, not needing to be learned. A measure of the subspace of situations satisfying a predicate p is defined. Predicate p is included in predicate q iff. A relaxed definition is then introduced. The inclusion model is characterized by a classifier function P. Predicate p is included in predicate q iff a relaxed inclusion relation is defined with a proportion of \u03c1 of p included in q. Cosine similarity is unable to model HYPONYMY due to its asymmetric nature. The first model of HYPONYMY leverages fastText word embeddings for generating word vectors. The inclusion model aims to correctly model the relationship between words in fastText embeddings based on classes from WORDNET. By finding a classifier function P that satisfies certain properties, the model can effectively distinguish between words that belong to a class and those that do not. This approach is necessary due to the limited number of representative elements in many classes relative to the dimensions of the embeddings. The model uses logistic regression to learn bias and vector values for WORDNET words, with positive and negative examples for training and testing. Results show 89.4% accuracy on positive examples and 89.7% on negative examples. The model achieved 89.7% accuracy on negative examples, indicating low recall and precision due to the imbalance between true negative and true positive cases. Testing on random negative entities resulted in over 97% accuracy, but accuracy dropped to 75% when testing on non-bird animals. Principal Component Analysis showed a mixing of bird and non-bird classes. The presence of uncommon words in the database leads to a mixing of bird and non-bird classes. While it may limit examples, the focus is on properties 1 and 2. Checking if subspaces intersect has challenges due to hyperplanes always intersecting. Density of fastText embeddings must be considered for accuracy. The density of fastText embeddings is a challenging problem due to the high dimensionality of the space. Modeling the density is difficult, and the low density of word vectors makes it hard to determine inclusion relations of euclidean subspaces. An experimental approach is taken to test the suitability of learned P(w) by checking if elements of subclasses are contained in the superclass. The proportion of elements found in w is defined as a quantity to determine the closeness of the inclusion relation. The distribution of Q(w, w) for all pairs w \u2286 w is plotted, with most density concentrated at the extrema. The choice of \u03c1 has little influence on accuracy. The recall is 88.8% for \u03c1 = 0.5, with a high ratio of false positives to total negative test cases. Despite a large number of negative cases, the precision is only 0.07%. Results are comparable with state-of-the-art models. A baseline fully supervised model for HYPONYMY is proposed in this section. The proposed baseline fully supervised model for HYPONYMY focuses on fitting the relation into a tree structure. By removing direct edges, a tree can be obtained with a precision of 100% and a recall of about 90%. The tree is then mapped to one-dimensional intervals based on node positions, creating a DAG with corresponding intervals assigned to nodes. The proposed model for HYPONYMY fits the relation into a tree structure with precision of 100% and recall of about 90%. Nodes are assigned intervals in a DAG, characterized by n-dimensional co-products to improve recall. Increasing n can lead to a near perfect model, as shown in Table 4b. Table 4b presents recall results for different n values, highlighting that Property 3 is not satisfied as interval co-products do not form subspaces. Authors have explored hyponymy modeling, with some evaluating their models on recovering withheld edges from the transitive closure of HYPONYMY. Results from authors like BID0, BID13, and BID15 using Feature-vector embeddings of WORDNET are discussed, showing the average recovery of withheld edges and correct classification of random negative edges. The results for the task in Table 4a show issues with mainly focusing on recall and not precision for WORDNET. It is debated whether any edge should be withheld beyond those in the transitive closure. The authors argue that the gold standard should be the transitive closure due to WORDNET's sparse nature. They did not withhold any edge when training their second model for this task. The second model trained for the task did not withhold any edge, unlike the first model. The task involves modeling hyponymy and associating subspaces to nouns. Using lower-dimensional hyperbolic vector spaces, like the Poincar\u00e9 ball model, is proposed to address issues with sparse and high-dimensional feature spaces. The Poincar\u00e9 ball model is useful for hosting vectorial embeddings of complex networks, while Lorenz's hyperbolic space is another proposed model. A lattice model with non-negative coordinates is also discussed for modeling the HYPONYMY relation. Each word is associated with a vector, and properties can be generalized to interpret the ratio of subspace measures as a probability. The approach by BID0 uses Gaussian densities to detect HYPONYMY relations between words, outperforming previous methods by BID13 and BID16. They advocate supervised training for building word representations. The probabilistic nature of the model is used to model uncertainty in meaning, not to classify the vector space. BID0 uses divergence measures to model inclusion for HYPONYMY detection. BID4 proposes a probabilistic setting of entailment, embedding phrases in a vector space inspired by BID13's lattice model. Denotational probability is defined for phrases represented by vectors, with joint probability calculated as the maximum value of their vectors. BID15 proposes using box embeddings within a unit hypercube to represent words, allowing for any correlation between them. The distribution of the vector space is assumed to be uniform, with the probability of a word defined as the volume of the corresponding box. Property 6 introduces the use of probabilistic reasoning over WORDNET concepts, assigning probabilities to nodes based on their descendants. This approach allows for the computation of joint probabilities between nodes, which can be used to augment training data with \"soft edges\" according to certain conditions. BID15 uses data from WORDNET to detect HYPONYMY by pruning graphs based on probability values. They found that representing HYPONYMY in a feature vector is challenging due to data sparseness and generalization issues. Despite WORDNET classes not being linearly separated in fastText, they still provide useful recall for approximate inclusion properties. However, the high rate of false negatives remains a challenge. The second model proposes constructing intervals directly from the HY-PONYMY relation for simplicity and high accuracy, even with a single dimension. However, the multi-dimensional version may require disjunctions not available in models using the HYPONYMY relation. Future work aims to address the issue of matching interval size to word probability. The study also critiques WORDNET as a representative of HY-PONYMY due to its tree-like structure. The study critiques WORDNET as a representative of HY-PONYMY due to its tree-like structure, suggesting that other resources like JEUXDE-MOTS may have similar flaws. Further analysis is needed to confirm this."
}