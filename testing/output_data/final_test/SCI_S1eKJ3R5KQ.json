{
    "title": "S1eKJ3R5KQ",
    "content": "We propose a generative adversarial training approach for clarification question generation, aiming to elicit new information to enhance context completeness. Our model, a Generative Adversarial Network (GAN), consists of a sequence-to-sequence generator and a utility function discriminator. Evaluation on two datasets demonstrates superior performance compared to retrieval-based models, with higher scores in usefulness, specificity, and relevance based on both automatic metrics and human judgments. A goal of natural language processing is to develop techniques for machines to understand language. This work focuses on automatically generating clarification questions to fill gaps in linguistic context. The model uses a sequence-to-sequence approach but aims to generate specific and useful questions, unlike generic ones commonly produced. The proposed Generative Adversarial Network (GAN) model outperforms retrieval-based models in terms of usefulness, specificity, and relevance. The text discusses a question generation model inspired by the task of question reranking. It involves generating specific and useful questions to improve context completeness. The model uses a utility calculator trained with generative adversarial networks and the MIXER algorithm. The text introduces a question generation model trained with the MIXER algorithm BID29. It evaluates the model on Stack Exchange and Amazon product description datasets, showing that the adversarially trained model generates a more diverse set of context-specific questions. The goal is to generate appropriate clarification questions given a context, such as a product description on Amazon. The question generator model is trained with the MIXER algorithm BID29 and evaluated on Stack Exchange and Amazon datasets. It uses a sequence-to-sequence model to generate questions and an answer generator to provide hypothetical answers. The UTILITY calculator estimates the usefulness of the question/answer pair in a given context, which is used as a reward to update the question generator. The question generator is updated using the MIXER BID29 algorithm. The answer-generator-plus-utility-calculator component is reinterpreted as a discriminator for true and synthetic triples, optimized with MIXER. The question generator model is based on a standard attention-based sequence-to-sequence model BID21. It uses an encoder-decoder architecture with attention, where the encoder is an RNN and the decoder generates the target sequence. The RNN hidden state at time t, along with parameters W s and W c, are used in the model to predict the token q t with the highest probability. Training sequence-to-sequence models for question generation aims to maximize log-likelihood by minimizing loss. However, this approach often leads to generic questions. The utility of a question can be better measured by the context it is used in. The utility of a question can be better measured by the context it is used in. A UTILITY based reward function is defined to optimize question generation. The UTILITY reward predicts the likelihood of a question adding useful information to the context. The Mixed Incremental Cross-Entropy Reinforce BID29 algorithm (MIXER) deals with non-differentiability in optimizing text outputs. MIXER optimizes the expected reward by shifting from maximum likelihood to external reward. The model minimizes the UTILITY-based loss to improve question generation, using a baseline reward to reduce variance. The baseline is estimated using a linear regressor trained on the current hidden states of the model. The model uses a utility function to calculate the value of updating a context with an answer to a question. It treats this as a binary classification problem and uses an LSTM to embed words in the context. The model utilizes an LSTM to embed words in the context and generate neural representations for the context, question, and answer. A feed forward neural network predicts the usefulness of the question. The utility function is trained on true data and used as a weak reward signal, which is then optimized in an adversarial learning setting to strengthen the reward signal. The model is optimized to distinguish between true question/answer pairs and model-generated ones, turning it into a generative adversarial network (GAN). A GAN involves a game between a generator and a discriminator, where the generator produces outputs (questions) to fool the discriminator, which aims to classify between real and generated data. The goal is for the generator to produce data close to the real data distribution. The GAN objective involves sampling from true data distribution and input noise variables. Training GANs for text generation is challenging due to discrete outputs. A sequence GAN model for text generation was proposed to address this issue, treating the generator as an agent and using the discriminator as a reward function. Our GAN-based approach is inspired by this model with modifications in the generator and discriminator. In our model, the answer is a latent variable used to train the discriminator. The discriminator is trained using (context, true question, generated answer) triples as positive instances and (context, generated question, generated answer) triples as negative instances. The objective function is defined with the UTILITY discriminator, MIXER generator, and answer generator. The question generator is pretrained using a sequence-to-sequence model, maximizing the log-likelihood of ([context+question], answer) pairs in the training data. The answer generator is pre-trained using a sequence-to-sequence model to maximize the log-likelihood of (context, question) pairs in the training data. Parameters are updated during adversarial training, while the discriminator is pre-trained using (context, question, answer) triples. Positive instances consist of true question-answer pairs, while negative instances involve randomly sampled questions paired with the context. The experimental design is based on research questions regarding the performance of generation models compared to retrieval baselines, optimizing UTILITY reward, using adversarial training, and evaluating model performance on StackExchange and Amazon datasets. The StackExchange dataset includes posts, questions, and answers from askubuntu, unix, and superuser subdomains, while the Amazon dataset has not been used for question generation before. The dataset includes 1 to 5 valid questions identified by human annotators from a pool of candidates. It consists of 61,681 training, 7710 validation, and 7709 test examples from Amazon, with each instance containing a question about a product on amazon.com. The dataset also includes product metadata information from the amazon reviews dataset. The Home and Kitchen category was chosen for evaluation due to its high number of questions. This dataset has 19,119 training, 2435 validation, and 2305 test examples, with each product description having 3 to 10 questions on average. The product description contains 3 to 10 questions on average. Three variants of the proposed approach are compared: GAN-Utility, Max-Utility, and MLE. Lucene 3 is a TF-IDF based document ranking system used to retrieve similar contexts. A question is randomly chosen from the top 10 retrieved questions. The Lucene baseline BID45 is constructed by randomly choosing a question from 10 questions paired with contexts. Experimental details of all models are described in Appendix B. Evaluation is done using automated metrics like Diversity, BLEU, and METEOR, with results shown in Table 1. The evaluation process involves using BLEU and METEOR scores with multiple references for the Amazon and StackExchange datasets. Human judgments are obtained by presenting contexts and generated questions to crowdworkers for evaluation based on relevance, grammar, specificity, new information, and usefulness to potential buyers. In the evaluation process, BLEU and METEOR scores are used with multiple references for the Amazon and StackExchange datasets. GAN-Utility outperforms all ablations on DIVERSITY in the Amazon dataset, while Lucene has the highest DIVERSITY due to human-generated questions. However, Lucene has lower match with the reference in terms of BLEU and METEOR scores. In the StackExchange dataset, GAN-Utility outperforms all ablations on both BLEU and METEOR. Unlike in the Amazon dataset, MLE does not outperform GAN-Utility in BLEU due to the technical nature of contexts. GAN-Utility excels in DIVERSITY in both datasets, with Max-Utility achieving higher DIVERSITY but with less grammatical outputs. Table 2 displays human evaluation results on model-generated questions from the Amazon dataset. The GAN-Utility model excels in generating specific questions and seeking new information, but falls short compared to Lucene in terms of new information retrieval. All models produce relevant and grammatical questions. The GAN-Utility model performs significantly better in generating specific and useful questions compared to Lucene and Reference. It excels in specificity to the product and has a higher DIVERSITY score. Previous work on question generation has focused on reading comprehension style questions, but the GAN-Utility model stands out in its ability to generate questions that seek new information. In text BID11, BID33, BID15, BID19, BID24, BID25, BID3, and BID6 introduce various methods for question generation, including crowdsourcing, templated questions, visual question answering, and active question answering models. These approaches aim to generate natural and engaging questions from images or textual context. BID6 specifically focuses on extracting question-answer pairs from forums to train a model for question generation. Neural network models have been successful in text generation tasks like machine translation, summarization, dialog, textual style transfer, and question answering. Adversarial training is used to generate natural-looking outputs, similar to dialog tasks. SeqGAN approach is introduced to optimize over discrete output spaces like text. In a novel approach to clarification question generation, a sequence-to-sequence model is used to generate a question given a context. This approach is based on the observation that the usefulness of a clarification question can be measured by updating the context with an answer to the question. In a novel approach, a sequence-to-sequence model is used to generate questions based on context. The utility of the (context, predicted question, predicted answer) triple is calculated as a reward to retrain the question generator using reinforcement learning. An adversarial training approach is employed to improve the utility function and train the model in a minimax fashion, resulting in more diverse questions. Future work includes combining text and image inputs to generate more relevant questions. The attention-based sequence-to-sequence model described in the main paper is used to generate questions based on context. Automatic evaluation is a significant research challenge in free text generation, with some correlation between human judgments and automatic metrics. Integrating the question generation model into platforms like StackExchange or Amazon is crucial to understand its real-world utility and unearth further research questions. The experimental setup involves preprocessing inputs using tokenization and lowercasing, setting max lengths for context, question, and answer, using word embeddings pretrained on in-domain data, and utilizing a sequence-to-sequence model with Glove embeddings of size 200. During training, Glove BID28 embeddings of size 200 are used with a vocabulary cutoff frequency of 10. Teacher forcing is employed during training, while beam search decoding with a beam size of 5 is used during testing. The encoder and decoder recurrent neural network models have a hidden layer size of two and hidden unit size of 100. A dropout of 0.5 and learning rate of 0.0001 are utilized. The MIXER model starts with \u2206 = T and decreases by 2 for every epoch. Human-based evaluation methodology is detailed in this section."
}