{
    "title": "H1lUp1BYDH",
    "content": "The focus has shifted towards easing network congestion to enable more efficient flow in systems like traffic, supply chains, and electrical grids. A framework is proposed to induce cooperation and coordination amongst agents connected via an underlying network using emergent communication in a Multi-Agent Reinforcement Learning setup. In a networked MARL setting, emergent communication protocols are studied through a case study on traffic systems. Intelligent agents interact in non-trivial ways, such as in a scenario where two agents cooperate to shoot a target based on each other's policies. In a multi-agent setup, agent A's policy depends on agent B's policy, leading to non-stationarity in the environment dynamics. This complexity challenges traditional approaches to learning optimal behavior independently. To address this, models tailored for training multiple agents simultaneously are crucial. In a specific setup with agents connected via a fixed network topology, the problem is framed in the multi-agent reinforcement learning framework. Agents aim to cooperatively maximize rewards, with the ability to communicate. Communication is crucial in a multi-agent setup to achieve cooperation and maximize rewards. Agents must learn a protocol to share their intents and complement local observations. This ability allows agents to affect each other in non-trivial ways and achieve long-term cooperation. In this paper, the focus is on intelligently managing traffic using a MARL-based system where traffic lights act as agents that communicate and cooperate to maximize rewards and ensure smooth traffic flow. The system allows for inter-agent communication and cooperative rewards to achieve coordination between traffic signals. The MARL-based system manages traffic by enabling traffic lights to communicate and cooperate for maximizing rewards and ensuring smooth traffic flow. Agents communicate via broadcasting messages to coordinate and maximize rewards, leading to the emergence of a communication protocol. The model utilizes a query-based soft attention mechanism for complex cooperative behavior. The text discusses a query-based soft attention mechanism used by agents to develop complex cooperative strategies in a Markov game environment with multiple intelligent agents. Experimental evaluation shows that the method outperforms baseline approaches, highlighting the importance of communication and coordination in achieving cooperative rewards. The text discusses a Markov game environment where multiple agents aim to maximize long-term rewards by finding optimal policies through communication over an underlying network. In a cooperative setting, agents in a Markov game environment maximize long-term rewards by considering rewards received from neighbors.\u03b2 assigns importance to own rewards and neighbors' rewards, with agents acting cooperatively to achieve this goal. This setup can model scenarios like intelligent electrical distribution networks. In a cooperative setting, agents in a Markov game environment maximize long-term rewards by considering rewards received from neighbors. They must learn to communicate effectively to share resources and meet changing demands, such as in intelligent electrical distribution networks or interconnected warehouses in a supply chain. In this paper, the focus is on traffic management as a concrete example of agents working together to meet changing demands. Each traffic junction corresponds to an agent that can control traffic flow by adjusting traffic lights based on local observations. Agents are rewarded based on attributes like queue length and average waiting time of vehicles at the junction. Agents in the traffic management system communicate with each other using binary messages with a vocabulary of 256 words. Communication is limited to immediate neighbors, with agents broadcasting 8-bit messages at each time-step to modulate traffic signals. Discrete communication is preferred for interpretability in real-world applications. The traffic management system uses discrete communication with binary messages to modulate traffic signals. Different control methods like Fixed-time Control, Max-Pressure Controller, and Self-Organizing Traffic Light Control are employed for efficient traffic flow. The Light Control method switches traffic lights based on vehicle thresholds for global synchronization. Optimization methods like TUC aim to minimize travel time but may not account for dynamic traffic conditions. Recent approaches use Reinforcement Learning to adapt traffic control dynamically, treating each junction as an agent. Recent approaches in traffic management utilize Deep-Q Learning (DQN) to map observation space to action space for long term rewards. However, issues arise with independent agents, leading to non-stationary environments. Solutions include agent coordination with neighboring information and centralized training, but scalability and real-time policy learning remain challenges. Scalable multi-agent approaches propose training a smaller problem with 2 agents and transferring it to a larger problem using transfer-learning. Prior work extends MARL setup to networks with fully decentralized training and sharing local parameters through communication. Inducing coordination among agents with partial observability and cooperative reward structure allows agents to pass on relevant information to neighbors. Sukhbaatar et al. (2016) proposed a communication-based approach for traffic networks, different from ours in agent representation and communication method. Our 10-agent traffic network uses discrete communication and image representation for observation space, allowing agents to extract queue length information. The agent in the traffic network uses space to extract necessary information from images, such as queue length and current phase at junctions. Adding more information to the state space can lead to better results, but systems with redundant information are impractical. A CNN processes input images to extract features for optimizing traffic, and agents use this information to take actions. Agents are constrained to take actions once every t timesteps, with t set to 5 seconds in the setup. The Accumulator, an RNN, keeps track of the information. The Accumulator, an RNN, uses encoded image information to update its memory. Agents in the communication setup generate queries to prioritize messages from neighbors using a soft-attention mechanism. The weights assigned to received messages help understand contributions from neighbors in the state information. The Accumulator RNN uses image information to update memory. Agents prioritize messages from neighbors using attention parameters. The output message is broadcasted to neighbors using Gumbel-Softmax for differentiability. Action Selector handles various combinations of actions at junctions. The action space at junctions is fixed to 4 actions for a 4-way junction and 3 actions for a 3-way junction to ensure smooth traffic flow. Action values are determined based on Accumulator-RNN and Communicator-RNN hidden states. Rewards at junctions are based on factors like queue length and waiting time of vehicles. Communication among agents is essential for meaningful information exchange, especially when there is uncertainty in local observations. Cooperation is key to effective communication, as selfish agents may not prioritize messages from others. In a scenario with two interconnected agents A and B modeling traffic, agent A's arrival rate is Poisson with rate \u03bb. When agent B's actions change the environment, \u03bb becomes (\u03bb - \u03b4\u03bb), causing agent A to ignore these variables. Agent B must communicate the difference in \u03bb to agent A, assumed to be Poisson distributed with rate \u03b4\u03bb. Coordination is crucial for effective communication among groups of agents in a network. Agents in a network must synchronize to achieve common goals by developing a universal protocol and common language. Comparison with baselines includes action switching and self-organizing traffic light control. In our implementation, we set a threshold of 5 for queue length at adjoining lanes. We use Deep-Q Learning (DQN) with the same observation and action space as ours, and replicate IntelliLight's refined state representation. Our method outperforms all baselines in terms of final reward at convergence. The study compares the performance of DQN with their setup, noting a difference of \u2248 55 in final rewards at convergence. Experiments show that communication is essential for agents to adapt, as broadcasting random symbols led to slower convergence and lower rewards. The study found that communication is crucial for agents to adapt, even when one agent is visually impaired. Removing local observations did not hinder convergence, as the blind-agent received necessary information from its neighbors through communication. However, when two neighboring blind-agents were tested, convergence worsened, indicating the importance of communication. The study conducted experiments on agents 4 & 5 to verify grounding using Pointwise Mutual Information (PMI). A matrix was constructed for each pair of agents, and Singular Value Decomposition (SVD) was computed. The t-SNE plot of the V matrix showed distinct clusters for different actions, indicating that neighboring agents use specific words to indicate actions. Words that are haphazardly placed may not have a specific meaning. The study conducted experiments on agents 4 & 5 to verify grounding using Pointwise Mutual Information (PMI). A matrix was constructed for each pair of agents, and Singular Value Decomposition (SVD) was computed. The t-SNE plot of the V matrix showed distinct clusters for different actions, indicating that neighboring agents use specific words to indicate actions. Words that are haphazardly placed may not have a specific meaning. In addition, the rows of the U matrix (with k = 2) were plotted, showing overlap between neighbors' action embeddings, suggesting consistency in the use of messages. See Appendix A.3 for more plots on grounding. The study conducted experiments on agents 4 & 5 to verify grounding using Pointwise Mutual Information (PMI) and Singular Value Decomposition (SVD). The t-SNE plot of the V matrix showed distinct clusters for different actions, indicating specific words used by neighboring agents. In larger networks, well-defined communities emerge based on vocabulary usage. Connecting two 14-agent networks resulted in two distinct clusters, as shown in the t-SNE embeddings of the tf-idf matrix. The study demonstrated evidence of cooperation and coordination among agents in different networks, with distinct clusters identified in the tf-idf matrix. Removing the cooperative reward structure resulted in slower convergence, highlighting the importance of communication incentives. The approach proposed in the paper aimed to alleviate network congestion using traffic networks and emergent communication, showing benefits in optimizing traffic flow compared to existing MARL methods. Qualitative studies on emergent language revealed grounding in actions, emphasizing the discrete nature of human communication represented by categorical variables. The Gumbel Softmax provides a differentiable sample from a discrete distribution, allowing transparency in real-life problems like traffic management. The Gumbel distribution has parameters \u00b5 and \u03b2, with a probability density function. In a model outputting a Multinomial distribution of message bits, samples can be drawn using the Gumbel-max trick. The Gumbel-max trick is used to sample from a discrete distribution, with noise generated from a standard Gumbel distribution. The temperature parameter \u03b2 is set to 0.5 in experiments, allowing for well-defined gradients. The Gumbel-Softmax estimator can produce a 0.9999-hot vector based on \u03b2. The Straight-Through version of the Gumbel-Softmax estimator is employed for discrete communication. Binary 8-bit messages are used, with k = 1 fixed during training to ensure discrete communication. The detach function prevents gradients from flowing through nodes, maintaining discreteness in the forward pass. The communication in the model is discrete in the forward pass and smooth during the backward pass. The final output message of an agent is represented as a vector. Action embeddings from an orthonormal matrix U of agent 0 correspond to messages from all agents. The action embeddings of each agent in response to messages from neighbors are overlapped. Different agents are represented by different colors in the plots."
}