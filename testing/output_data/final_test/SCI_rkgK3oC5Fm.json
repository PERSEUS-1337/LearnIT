{
    "title": "rkgK3oC5Fm",
    "content": "In real-world scenarios, anticipating future scene states is crucial for autonomous agents. Dropout based Bayesian inference offers a way to learn diverse models to deal with uncertain futures, but struggles to capture complex scenes accurately due to a lack of diversity. A novel Bayesian formulation leveraging synthetic likelihoods is proposed to accurately capture the multi-modal nature of future scenes. Our approach achieves state-of-the-art predictions and calibrated probabilities for scene anticipation on Cityscapes dataset. It generalizes across tasks like digit generation and precipitation forecasting. Anticipating future scene states is crucial for autonomous agents to operate successfully in the real world, especially for predicting movements of pedestrians and vehicles. The future states of street scenes are uncertain and often multi-modal, particularly for important classes like pedestrians. Previous works on anticipating street scenes did not systematically consider uncertainty, but Bayesian inference provides a solution. Bayesian inference offers a way to capture model and observation uncertainty, but it comes with computational overhead. A new approach using dropout has made Bayesian inference with deep neural networks more efficient, allowing for minor changes to existing architectures. However, this approach struggles with recovering true model uncertainty in multimodal data distributions without explicit latent variables, as it tends to conflate risk and uncertainty. The text discusses the limitations of Bayesian inference in capturing model uncertainty in multimodal data distributions without explicit latent variables. It introduces a new approach using synthetic likelihoods to address this issue and improve model diversity. The approach aims to anticipate the multi-modal future of street scenes with state-of-the-art accuracy on the Cityscapes dataset. Additionally, a novel optimization scheme for dropout based Bayesian inference is proposed. The text introduces a novel optimization scheme for dropout based Bayesian inference using synthetic likelihoods to encourage diversity and accurately capture model uncertainty. It shows that this approach is applicable to various tasks beyond street scenes, such as digit generation and precipitation forecasting. The proposed method addresses model uncertainty in segmentation and depth regression tasks, outperforming previous approaches. In contrast, a Bayesian GAN framework for image generation had limited success. To tackle model uncertainty issues, a dropout based Bayesian scheme with synthetic likelihoods is used. SFNNs and CVAEs have also shown success in modeling multimodal conditional distributions, but SFNNs struggle with optimization on large datasets. SFNNs struggle with optimization on large datasets due to binary stochastic variables, while CVAEs, which assume Gaussian stochastic variables, are easier to optimize using the re-parameterization trick. CVAEs have been successfully applied in various tasks like conditional image generation, next frame synthesis, video generation, and trajectory prediction. However, careful architecture selection is necessary for CVAEs, and there is a lack of experimental evidence for uncertainty calibration, which is crucial for applications like autonomous driving. In BID13, a Bayesian approach is adopted for predicting future scene segmentations using a fully convolutional model with predictions at multiple scales. BID9 improves upon this by jointly predicting future scene segmentation and optical flow. BID14 extends the model to future instance segmentation prediction, showing the competence of fully convolutional models. BID23 introduces a Convolutional LSTM based model for further improving short-term results. Incorporating a Bayesian framework, a ResNet based fully-convolutional architecture is utilized to capture model uncertainty in predicting outcomes. The predictive distribution of outcomes is obtained by marginalizing over the posterior distribution, with an intractable integral approximated in two steps. Incorporating a Bayesian framework, a ResNet based fully-convolutional architecture is utilized to capture model uncertainty in predicting outcomes. The predictive distribution of outcomes is obtained by marginalizing over the posterior distribution, with an intractable integral approximated in two steps. The approach involves constraining models to a finite set of variables \u03c9 and using an approximating variational distribution q(\u03c9) for efficient sampling. This results in a smaller number of possible models compared to traditional methods, making optimization more manageable. Incorporating a Bayesian framework, a ResNet based fully-convolutional architecture is utilized to capture model uncertainty in predicting outcomes. The approach involves defining a novel approximating Bernoulli variational distribution q(\u03c9) over convolutional kernels and biases. Observation uncertainty is captured by assuming an appropriate distribution of observation noise. The approach involves predicting the sufficient statistics of the distribution BID10 using a Gaussian distribution with diagonal covariance matrix at each pixel. Sampling from the predictive distribution is done by sampling weight matrices and then sampling from a Gaussian distribution. The final class probabilities are obtained through a linear transformation ensuring differentiability. The approach involves predicting class probabilities by pushing the class-confidence vector through a softmax. To ensure a good variational approximation, the KL divergence between the variational distribution and true posterior is minimized. However, in the case of multi-modal data, the log-likelihood term may push models to converge to the mean, discouraging diversity in learned modes. In multi-modal data, recovering likely models is challenging, hindering the capture of model uncertainty. To address this, an approximate objective using synthetic likelihoods obtained from a classifier is proposed. This approach allows diverse models to deal with multi-modality by estimating likelihood based on data samples likely under the true distribution. The KL divergence estimate is reformulated to a likelihood ratio form, enabling the use of a classifier to estimate synthetic likelihoods. The approach proposes using synthetic likelihoods obtained from a classifier to estimate likelihoods in multi-modal data, enabling diverse models to handle multi-modality. The KL divergence estimate is reformulated into a likelihood ratio form, allowing the use of a classifier to estimate synthetic likelihoods. The approach involves using a discriminator to distinguish between true data samples and generated samples by the model, allowing diverse models to capture uncertainty and handle multi-modality. The synthetic likelihood is independent of specific data pairs and only needs to generate likely samples under the true data distribution. The architecture details of the generative models \u03c9 and the discriminator D(x,\u0177) are described. The ResNet based generative models in the model distribution q(\u03c9) take input of past segmentation class-confidences and vehicle odometry to produce class-confidences at the next time-step. The approach involves using a discriminator to distinguish between true data samples and generated samples, allowing diverse models to capture uncertainty and handle multi-modality. The generative model architecture consists of a fully convolutional encoder-decoder pair with key differences from prior work. It uses recursion to predict future scene segmentations based on vehicle odometry. The discriminator classifies whether the generated sequence is from the model or true data distribution. The model has one level with five convolutional blocks, including three residual blocks in the encoder and one in the decoder. The model architecture includes fifteen convolutional layers with max-pooling to preserve resolution and capture spatio-temporal dependencies. Residual connections aid in optimizing the deep model, with the possibility of adding more levels. In contrast, a model with significantly more layers introduces excessive pooling, leading to a loss of resolution and spatial information, degrading performance. The discriminator model comprises six convolutional layers with max-pooling layers in-between. The curr_chunk discusses a model architecture with six convolutional layers and max-pooling layers, followed by two fully connected layers. It evaluates the approach on tasks like MNIST digit generation and street scene anticipation. The model uses a fully connected generator with hidden units and dropout probability. The curr_chunk introduces a synthetic likelihood based approach (Bayes-SL) compared to other models like a non-Bayesian mean model and a Conditional Variational Autoencoder (CVAE). Evaluation is done using a standard Alex-Net based classifier on the MNIST test-set, showing Bayes-SL outperforming the CVAE model. In TAB1, models sampled from our learned distribution produce clear digits compared to the blurry digits from the Bayes-S model. Evaluation on the Cityscapes dataset shows multi-modality in street scenes. Evaluation metrics include mean Intersection-over-Union (mIoU) and per-pixel conditional log-likelihood (CLL) using PSPNet BID30. Baselines are used for comparison to our Resnet based architecture. Our Resnet based Bayesian model (Bayes-WD-SL) is compared to baselines including copying the last seen input, a non-Bayesian version (ResG-Mean), a Bayesian version with standard patch dropout (Bayes-S), and a Bayesian version with weight dropout (Bayes-WD). Grid search sets dropout rates for Bayes-S and Bayes-WD models. Training details include using Adam for 50 epochs with batch size 8. Bayesian models are evaluated using mIoU metric and compared to state-of-the-art methods. Our Bayes-WD-SL model outperforms baselines and improves on prior work by 2.8 mIoU at +0.06sec and 4.8 mIoU/3.4 mIoU at +0.18sec/+0.54sec respectively. It also obtains higher relative gains in comparison to BID13 with respect to the Last Input Baseline, validating our model architecture choice and novel approach. The results validate the model architecture choice, showing that Bayes-WD-SL outperforms the state-of-the-art without sacrificing mean performance. Evaluating the models on their ability to capture uncertainty and multi-modal futures up to t + 10 frames. The Bayesian models accurately capture uncertainty and contain likely models corresponding to the ground truth. The distribution q(\u03c9) contains likely models corresponding to the ground truth. Bayesian models improve predictions by capturing uncertainty and dealing with multi-modal futures. Bayes-WD-SL model performs best, exceeding the performance of the ResG-Mean model and demonstrating the effectiveness of synthetic likelihoods during training. Examples comparing predictions of Bayes-WD-SL and ResG-Mean at t + 9 show differences in prediction accuracy. Our Bayes-WD-SL model outperforms ResG-Mean in predicting classes like cars and pedestrians, showing better performance in capturing uncertainty and variation in data. The Bayesian models significantly outperform ResG-Mean, with Bayes-WD-SL demonstrating the best results. Comparisons to a CVAE baseline are made due to the lack of a CVAE BID24 model for future segmentation prediction. Our Bayesian approach (Bayes-WD-SL) outperforms existing CVAE based models in capturing data variation, with noise in the last layers proving to be most effective. This highlights the superiority of our model in predicting future segmentation. The Bayesian approach (Bayes-WD-SL) outperforms CVAE models in capturing data variation and uncertainty calibration. Results show Bayesian approaches are more effective in predicting future semantic segmentations. The Bayesian approach introduces a novel optimization scheme using synthetic likelihoods for diverse multi-modal futures in street scenes. It demonstrates state-of-the-art performance and captures uncertainty inherent in the task, extending to enhance deep learning architectures with principled Bayesian inference formulations. The text discusses results on simple multi-modal 2D data using a two hidden layer neural network. The Bayes-SL approach learns models covering both modes, while Bayes-S fits to the mean. The Bayes-WD-SL training details are provided in TAB3. The Bayes-WD-SL model achieves higher gains compared to BID13 in generating training segmentations using a PSPNet. The model shows a 29.5% gain over BID13's Last Input Baseline, translating to a 10.9 mIoU increase. The Bayes-WD-SL model outperforms BID13 in short-term and long-term predictions, showing a 33.6% gain over the Last Input Baseline. Additionally, the model demonstrates improved results using the same Dialation 10 approach for training segmentations. The study involves a model with 20 frames, using 5 for input and 15 for prediction at 6-minute intervals. It compares to deterministic and Bayesian models, reporting top scores for various metrics. The Bayes-WD-SL model outperforms state-of-the-art methods, showcasing its versatility and ability to surpass strong baselines like ResG-Mean. Our Bayes-WD-SL model outperforms the ResG-Mean baseline, showcasing its ability to capture uncertainty. Calibration comparison with ResG-Mean model shows our approach leads to more accurate predictions without compromising on calibration. In TAB6, layer-wise details of generative and discriminative models are provided. The recognition network of the CVAE baseline in TAB2 is also discussed. TAB0 compares the number of possible models using weight dropout versus patch dropout, showing significantly lower parameters with weight dropout. The generative model, discriminator model, and recognition model details are shown in TAB6, TAB8, and TAB0 respectively. A comparison in the number of possible models using weight dropout versus patch dropout is presented in Table 11, indicating significantly lower parameters with weight dropout."
}