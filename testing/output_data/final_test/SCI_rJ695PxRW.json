{
    "title": "rJ695PxRW",
    "content": "The Generative Markov Network (GMN) is introduced to automatically extract the order of data instances by assuming they are sampled from a Markov chain. Neural networks are used as a soft lookup table to approximate the transition matrix, allowing for space complexity amortization and generalizability to unseen instances. The goal is to learn the transitional operator and generation order by maximizing generation probability under all possible data permutations. The Generative Markov Network (GMN) introduces a greedy batch-wise permutation scheme for fast training, ensuring ergodicity. GMNs can discover data instance orders and perform well on recognition tasks. Deep neural networks like CNNs offer automated learning of image features, outperforming hand-crafted methods like SIFT and SURF. Learning to learn for automatic parameter estimation updates model parameters without predefined rules. In our paper, we explore the idea of automatically ordering an unordered dataset based on implicit order. This order can exist even in data that is considered i.i.d., such as joint locations on a person's body taken on different days. This approach has shown improved performance and faster convergence in tasks like object recognition and image style transformation. The data instances can be arranged to follow articulated motion, making poses predictable. This arrangement leads to a robust model explaining correlations among joints. Reordering frames of a video clip can create an i.i.d. model, while reconstructing the order allows for easier fitting of transitions between frames. Sparse sampling of a ballerina's dancing can be seen as a reshuffled sequence needing reordering for a temporal model to generate it. To address the ordering problem in datasets, a distance-metric-free model is proposed to automatically discover the proper distance metric. By treating the data as generated from a Markov chain, the model aims to learn the transitional operator simultaneously. The proposed model, Generative Markov Networks (GMNs), uses neural networks to approximate transition matrices, reducing space complexity. The model's differentiable property allows for generalization on unseen data. A greedy batch-wise permutation scheme ensures ergodicity in the learned Markov chain. GMNs are effective for one-shot recognition tasks with limited labeled data. The proposed Generative Markov Networks (GMNs) model uses neural networks to approximate transition matrices, reducing space complexity. It is effective for one-shot recognition tasks with limited labeled data and can discover implicit orders among data instances. The literature on deep generative models and stochastic sampling is discussed, focusing on relevant work in the target domain. Our approach, categorized as an iterative sampling-based model, differs significantly from previous works. The proposed model, GMN, differs from previous works in three key aspects. Firstly, it does not assume training instances are i.i.d. sampled from a stationary Markov chain, allowing for better observation of data relationships. Secondly, while prior approaches focused on denoising models for image generation, GMN aims to discover orders in datasets. Lastly, GMN is an explicit model, unlike existing implicit models, allowing for more efficient sampling schemes. The proposed GMN model is explicit and maintains a tractable likelihood function for efficient computation. Deep one-shot learning approaches can be categorized into distance-metric-learning and categories-separation-metric-learning methods. The former focuses on similarity measurement or specific metric loss, while the latter uses a meta-learning framework to train parametric classifiers. The proposed GMN model updates base learners' parameters in the first stage and meta learner parameters in the second stage. Unlike previous methods, which view one-shot recognition as a discriminative task, we take a generative perspective by considering a Markov chain data generation assumption. This approach simplifies training as we can directly determine labels for query instances based on the observed Markov chain generated from support instances. Our goal is to recover the unknown Markov chain and the order of the generation process. The proposed GMN model takes a generative perspective by assuming a Markov chain data generation process. To address the challenge of finding the optimal order, a greedy algorithm is proposed to estimate the transitional operator. This involves considering all possible permutations and solving a joint log-likelihood estimation problem. The proposed GMN model uses a generative approach with a Markov chain data generation process. To find the optimal order, a greedy algorithm is used to estimate the transitional operator by considering all possible permutations and solving a joint log-likelihood estimation problem. The initial distribution P(1) diminishes with data size increase, leading to an optimization problem where direct optimization is computationally intractable. An efficient greedy algorithm is presented to approximate the expensive function of all possible permutations. Maintaining the tabular transition matrix directly is often infeasible for huge state spaces, so optimization should first focus on finding the optimal order. Neural networks are used to approximate the discrete tabular transition matrix in the proposed GMN model, reducing space complexity and allowing better generalization of transition probabilities across states. This approach is advantageous as it amortizes space required by each state into a unified model and eliminates the need to store transition vectors for each state explicitly. The differentiable approximation to a discrete structure in the GMN model allows for smooth transition probabilities between states using neural networks as soft lookup tables. The transition function takes two states as inputs and returns the corresponding probability, making direct evaluation computationally intractable. Coordinate ascent style training is used to optimize the parameters of the neural networks. The coordinate ascent style training algorithm efficiently optimizes the transition function in the GMN model. A greedy algorithm is proposed to approximate the optimal order, reducing the time complexity to O(n 2 log n). The algorithm efficiently optimizes the transition function in the GMN model by finding the next state through local search. The approximate order is defined as the maximum of all orders, with a time complexity of O(n 2 log n) achieved by pre-computing and sorting transition probabilities. The model parameter is optimized using gradient-based optimization in a coordinate ascent algorithm, ensuring convergence. The algorithm optimizes the transition function in the GMN model efficiently by partitioning the training set into batches and performing greedy approximate order on each batch. This reduces the time complexity to O(nb log b), which is linear in n. However, the learned transitional operator may not have nonzero transition probabilities between different states. The algorithm optimizes the transition function in the GMN model efficiently by partitioning the training set into batches and performing greedy approximate order on each batch. To address the issue of isolated states and ensure nonzero transition probabilities, a strategy is proposed to enforce overlapping samples between consecutive batches. The transitional operator implementation involves introducing stochastic latent variables and utilizing Variational Bayes Inference. The transitional operator in the GMN model optimizes the ELBO of the log likelihood by replacing T(s0|s; \u2713) with a distribution P(s0|s, z; ) parametrized by , allowing dependence of s on z. The encoder function Q(z|s; ) encodes latent code z given state s, with a fixed prior P(z) as a Gaussian distribution. Reparameterization is used to draw Q(z|s; ) from a Gaussian. Two distribution families for P(s0|s, z; \u2713) are considered: Bernoulli and Gaussian, with specific formulations for binary and real-valued features. The GMN model optimizes the ELBO of the log likelihood by using a distribution P(s0|s, z; ) parametrized by , with experiments conducted on ordering data in datasets like MNIST, Horse, and MSR SenseCam BID11. Moving MNIST is also included as a dataset with explicit orders. The experiments show partial ordering results, with the full version available in the Supplementary. MNIST is a well-studied dataset with 60,000 training examples. The MNIST dataset contains 60,000 digit images rescaled to [0, 1]. The Horse dataset has 328 horse images with object-background segmentation. MSR SenseCam dataset has 45 classes with 150 images each, resized to 224x224 for feature extraction. In the dataset, only the office category with 362 images is considered. The Generative Markov Networks are trained using Alg. 2, and the images are plotted following permutation suggested by GMNs. Results show high autocorrelation in data ordered by GMNs, indicating the discovery of implicit orders. Comparison with Nearest Neighbor sorting shows difficulty in determining superiority. GMN is a distance-metric-free model that suggests an implicit order based on a generative modeling viewpoint. It aims to discover datasets' orders, unlike other iterative sampling models. Nearest Neighbor search is used for comparison. The proposed GMN model allows for efficient consecutive sampling compared to Nearest Neighbor search. A one-shot recognition task is performed on the miniImageNet dataset, which contains 100 classes with 600 images each downsized to 84x84. The dataset is split into training, validation, and testing sets, following a 5 way 1 shot problem setup. The GMN model is used for one-shot recognition on the miniImageNet dataset, split into training, validation, and testing sets. The 5 way 1 shot problem is considered, with 5 classes sampled for testing. GMNs are trained on training classes and applied to testing classes for generative recognition. For each testing episode, GMNs generate chains from support instances and fit query examples by computing average loglikelihood. Predicted class for query sample is determined by generative viewpoint. Architecture with 4 blocks is used for feature extraction, pretrained with softmax regression on image-label pairs. The Generative Markov Networks (GMN) are trained using ReLU activation and 2x2 Max-Pooling layer. The network architecture for parameterizing T is specified in the Supplementary. A variant of GMN with fine-tuning is also provided, where GMN is fine-tuned using specific parameters on support and query instances. The performance of GMN is comparable to other methods, as shown in Table 1. The Generative Markov Networks (GMN) show comparable performance to other methods, with Meta-SGD BID22 achieving the best result. GMN requires less computational budget due to its smaller network size compared to parametric approaches. In contrast, Prototypical Networks BID28 achieves the best performance in distance-metric learning approaches. GMN offers more flexibility without the need for defining distance metrics, unlike other models. Our proposed Generative Markov Network (GMN) shows a significant improvement from fine-tuning over support and query instances, simulating the Markov chain data generation process. The GMN considers implicit orders in data instances, offering a novel approach to understanding datasets through a Markov chain data generation scheme. The proposed Generative Markov Network (GMN) demonstrates improved performance by identifying implicit orders in unordered datasets. The model performs well on one-shot recognition tasks and is validated through experiments. The Moving MNIST dataset consists of sequences of two digits moving in a frame, with pixel values rescaled to [0, 1]. Training episodes involve applying Alg. 2 to train GMN on randomly chosen sequences, with parameters set accordingly. Evaluation involves sampling a disjoint sequence to observe the optimal permutation. The proposed Generative Markov Network (GMN) shows improved performance by identifying implicit orders in unordered datasets. GMN outperforms Nearest Neighbor search in sampling results for Moving MNIST dataset. GMN does not require defining a distance metric and utilizes a generative model for transition operator design. In the design of the transition operator in FIG0, U acts as a gating mechanism between input Xt and the learned updateX, with the output represented as DISPLAYFORM0. Each function f is specified in Tbl. 1, 2, 3, 4, and 5, omitting the bias term for simplicity. ADAM with a learning rate of 0.001 and a dropout rate of 0.2 is used to train T."
}