{
    "title": "HJxeWnCcF7",
    "content": "The text discusses the use of embeddings in different geometric spaces to better represent structured data. It introduces a method of learning embeddings in a product manifold combining spherical, hyperbolic, and Euclidean spaces to accommodate various data structures. The approach involves estimating sectional curvature of graph data and optimizing the embedding in the product space via Riemannian optimization. In this study, product space embeddings are utilized to improve representation of structured data. By combining spherical, hyperbolic, and Euclidean spaces, the embeddings outperform previous works on various datasets, reducing distortion by 32.55% on a Facebook social network dataset. Additionally, word embeddings in a product of hyperbolic spaces consistently outperform baseline Euclidean and hyperbolic embeddings on similarity and analogy tasks. Euclidean space, with four decades of use, is the traditional embedding space. Recently, non-Euclidean spaces like hyperbolic and spherical have gained attention for providing better representations for structured data. These spaces offer higher mean average precision and lower distortion compared to Euclidean spaces due to their constant curvature model. The key role of curvature improves representation fidelity by matching the data structure with the geometry of non-Euclidean space. The search for spaces with heterogeneous curvature aims to enhance representations for various data types. Mixed spaces with varying structure, including tree-like and cyclical regions, aim to match the geometry of data for higher quality representations. Challenges include manifold optimization to learn curvature and embedding, as well as operating on embedded points. Embedding into product spaces with constant curvature is proposed for improved representation fidelity. The text discusses embedding into product spaces with constant curvature to capture a wider range of curvatures than traditional embeddings. A Riemannian product manifold is formed combining hyperbolic, spherical, and Euclidean components with a decomposable Riemannian metric. The challenge lies in selecting appropriate curvatures for the embedding space, which can be addressed by directly learning the curvature for each component space along with the embedding through Riemannian optimization. This approach allows for the recovery of non-uniform curvatures and improves representation fidelity. The text discusses the challenge of selecting the appropriate curvatures for embedding into product spaces with constant curvature. A theory-guided heuristic estimator for the signature is introduced to address this challenge by matching empirical discrete curvature with the theoretical distribution of sectional curvature. This approach allows for the recovery of non-uniform curvatures and improves performance on reconstruction metrics. In product space embeddings, a formulation of mean for embedded points is developed, utilizing the decomposability of distance. Gradient descent can recover the mean with error \u03b5 in time O(nr log \u03b5 \u22121 ). Product space embeddings offer advantages over single spaces, especially for structures unsuitable for single space embeddings. Product space embeddings offer significant improvements over single spaces, particularly for structures not suitable for single space embeddings. Reconstruction quality is measured for synthetic and real datasets, showing a 32.55% improvement in distortion on a Facebook social network graph. Product spaces applied to skip-gram word embeddings show improved performance on benchmark evaluations, indicating that words form multiple smaller hierarchies. Mixed product spaces show an improvement of 3.4 points on the Google word analogy benchmark and 2.6 points in Spearman rank correlation on a word similarity task using the WS-353 corpus. The curr_chunk discusses embeddings in metric spaces, measuring quality with fidelity measures like average distortion and mean average precision. It explains distortion as a global metric and introduces the concept of R a,bi in the embedding f. The curr_chunk delves into Riemannian geometry on smooth manifolds, defining Riemannian metrics, tangent spaces, and geodesics for computing distance functions. It highlights the importance of the metric tensor in inducing geometric properties like length and angle on the manifold. The curr_chunk discusses product manifolds formed by a sequence of smooth manifolds and how the metric tensor on each manifold contributes to the Riemannian metric on the product manifold. It also touches on geodesics and distances in optimization on manifolds. The curr_chunk explains the decomposition of the exponential map in a product manifold, highlighting the analogy to Euclidean products. It introduces the hyperboloid model of hyperbolic space and defines the manifold on a subset in R^(d+1). The challenges of mixed spaces are addressed by introducing a product manifold embedding space composed of multiple copies of simple model spaces. The curr_chunk discusses a product manifold embedding space composed of multiple copies of simple model spaces, allowing for heterogeneous curvature. It explains how to learn an embedding and curvature for each component simultaneously through optimization, choose the signature heuristically, and introduce a Karcher-style mean for efficient recovery. The product manifold has m + n + 1 component spaces with a total dimension of i s i + j h j + e, with each component referred to as factors. The distances on the product manifold are discussed, emphasizing its definition as a Riemannian manifold. The product manifold P is a Riemannian manifold defined by the structure of its components. Distances on P are represented by d Mi for p, q \u2208 P, decomposing via (1) into the squared distance. Different distances can be used on P, leading to various embedding spaces. The 1 distance is considered for simple applications that only interact through distances, allowing for combinatorial constructions without optimization. The text discusses constructions for embeddings without optimization, focusing on Riemannian distance for direct optimization on the manifold. An example is given with a graph structure, showing how removing an edge can lead to better embedding in hyperbolic space. The full details and another construction for minimum distance are provided in the Appendix. The text discusses optimizing embeddings through an auxiliary loss function using Riemannian optimization methods. Algorithm 1 R-SGD is specialized for product spaces, optimizing the placement of points to capture average distortion. The loss function is dependent on hyperbolic distance and can be optimized through standard Riemannian optimization methods. The text explains the process of optimizing embeddings in product spaces using Riemannian optimization methods. It involves computing the Euclidean gradient and converting it to the Riemannian gradient, applying corrections specific to the hyperboloid model. The gradient step is performed independently in each component, with additional steps for projection and rescaling in the hyperboloid model. The text discusses optimizing embeddings in product spaces using Riemannian optimization methods, specifically focusing on the hyperboloid model. It explains the need for rescaling by the inverse of the metric in the hyperboloid model, unlike the spherical model. Different models exist for various curvatures, and the curvature K is optimized along with the embeddings. The radius R can be treated as a parameter to optimize K and represent points on the manifold of curvature K. The text discusses optimizing embeddings in product spaces using Riemannian optimization methods, specifically focusing on the hyperboloid model and the need for rescaling by the inverse of the metric. It explains optimizing curvature K and representing points on the manifold of curvature K. The sectional curvature is used to choose an appropriate space P corresponding to given data. The text discusses the Gaussian curvature of the surface and how it relates to volume growth. It explains how the sectional curvature of P interpolates between the sectional curvatures of the factors, enabling a wider range of structures in embeddings. The estimation technique employs a triangle comparison theorem to characterize sectional curvature through the behavior of small triangles in the manifold. The curvature estimation in undirected graphs defines an analog for every node and two neighbors, recovering the curvature for various graph structures. The curvature is zero for lines, positive for cycles, and negative for trees, providing a basis for embedding in a potential product manifold. The empirical sectional curvature of a generic graph G is estimated using Algorithm 3, based on the homogeneity of product manifolds. The distributions of sectional curvature are moment-matched through uniformly random 2-planes in the graph and manifold. Taking the mean on manifolds is crucial for various applications, such as analogy tasks with word embeddings and clustering. Defining a mean even in simple settings like the circle S1 is nontrivial, as the classic approach of taking the Euclidean mean fails in certain cases. The mean on manifolds is important for applications like analogy tasks with word embeddings and clustering. Defining a mean on the circle S1 is challenging due to the varying curvature of P. A Karcher-style weighted mean is introduced to address this issue, allowing for optimization on the mean with positive weights. The optimization on the mean with gradient descent via the exponential map offers linear rate convergence on product manifolds of model spaces. Riemannian gradient descent can recover the mean within a distance in time O(nr log \u22121), even when some weights are negative, useful for analogy operations. The curr_chunk discusses the tractable optimization of components' curvature through reparametrization and matching discrete notions of curvature on graphs with sectional curvature on manifolds. It also evaluates the representation quality of synthetic graphs and real datasets in different embedding spaces, emphasizing the importance of mixed product spaces for nonhomogeneous data. Additionally, it examines the curvatures learned through optimization and theoretical allocation, as well as the intrinsic performance of product space embeddings in a skip-gram word embedding model. The curr_chunk discusses optimal embeddings of synthetic and real-world datasets in spaces of positive and negative curvature, as well as a mixture of spaces for dense graphs. It compares different datasets and hypothesizes on the embedding spaces. The dense graph benefits from a mixture of spaces for optimal embeddings. The curvatures are initialized and learned for different signature values. Quality is measured by average distortion and mAP metric. Matching geometries yield the best distortion for synthetic graphs. In TAB2, the quality of embedding different graphs is reported across various space allocations, with the structure of each graph influencing the best allocation. The cities graph embeds well into spaces with a spherical component, while the treelike Ph.D.s graph embeds well into hyperbolic products. Product construction achieves high-quality reconstruction, outperforming the traditional Euclidean approach. Non-uniform curvatures are found even for products of identical spaces. Table 3 reports the signature estimations. In Table 3, signature estimations of Algorithms 2 and 3 for unweighted graphs are reported. The estimated curvature signs agree with distortion results from Table 2. Word embeddings were learned and evaluated on benchmark datasets for word similarity and analogy, showing that hyperbolic embeddings perform well in low dimensions but less so in higher dimensions. It is hypothesized that a product of multiple smaller-dimension hyperbolic spaces will improve performance in high dimensions. The study extends the skip-gram model to a generic objective suitable for arbitrary manifolds. Training follows a setup similar to previous work, with word similarity measured using Spearman rank correlation. The study explores heuristic allocation for embedding unweighted graphs into two factors, finding hyperbolic word embeddings to be competitive with Euclidean embeddings. Analogies in manifolds can be defined via intrinsic product manifold operations, despite the lack of \"word arithmetic\" in conventional word embeddings. The loss function depends on the embeddings. The loss function in product manifold operations depends solely on embeddings through pairwise distances. Analogies are defined by geodesically reflecting a through the mean of b, c. Evaluation on the Google word analogy benchmark shows a 22% accuracy improvement over single-space hyperbolic embeddings. Product spaces improve representations by matching the geometry of the embedding space. The curr_chunk introduces a tractable Riemannian product manifold class combining Euclidean, spherical, and hyperbolic spaces to improve embeddings. It discusses learning embeddings, estimating product signature, and mean formulation. The focus is on encouraging research on non-Euclidean embedding spaces, with a mention of improved performance in hyperbolic spaces. The Appendix includes a glossary, related work discussion, Lemma 2 proof, and curvature estimation algorithm treatment. The curr_chunk provides additional details on the curvature estimation algorithm and introduces combinatorial constructions for embedding techniques. It also evaluates the interpretability of embeddings through visualizations and includes a glossary of commonly-used terms. The curr_chunk discusses the mathematical concepts related to manifolds, Riemannian metrics, curvature tensors, and metric distances in various spaces such as Euclidean, spherical, hyperbolic, and graph spaces. Hyperbolic space is proposed as an alternative to Euclidean space for learning embeddings with a hierarchical structure. Various types of data exhibit a non-Euclidean latent structure, such as social networks and DNA sequences. Initial works on hyperbolic embeddings include neural graph embeddings for classifying complex networks and link prediction in a lexical database. Follow-up work performs optimizations in the hyperboloid model. BID28, BID34, BID15, and BID33 proposed various advancements in hyperbolic space embeddings, including optimizations in the hyperboloid model, a neural ranking based question answering system, hyperbolic embeddings of entailment relations using directed acyclic graphs, and efficient embedding of trees and tree-like graphs. These works showed improvements in representational capacity and generalization compared to traditional models. In complex geometries, standard machine learning tools like convolutional neural networks and LSTMs lack exact correspondences. Recent approaches aim to adapt machine learning methods to hyperbolic space, showing improvements in generalization and compact neural representations. Recent approaches in machine learning aim to adapt tools to hyperbolic space for improved generalization and compact neural representations. BID16 introduces basic machine learning tools in hyperbolic space, showing empirical improvements in tasks like textual entailment. BID7 presents a hyperbolic formulation for support vector machine classifiers, demonstrating performance enhancements in multi-class prediction tasks on real-world networks. Zipf's law defines a hierarchy based on word-frequency distributions, with semantically general words closer to the root and rarer words further down. In order to capture the latent hierarchy in natural language, there have been proposals for training word embeddings in hyperbolic space. BID9 trains word embeddings using an algorithm from BID27, showing better performance on inferring lexical entailment relations than Euclidean embeddings. The popularity of hyperbolic embeddings has led to interest in descent methods suitable for hyperbolic space optimization. Our work explores convergence rate analysis for various algorithms on Hadamard manifolds. BID11 introduces an update rule in hyperbolic space with guaranteed convergence, while BID44 presents accelerated Riemannian gradient methods. Previous work on MDS and PCA-like algorithms in hyperbolic, spherical, and general manifolds is also discussed. Specific algorithms for visualization in hyperbolic space are developed in BID38 and BID20, while embeddings with a PCA-like loss function are explored in BID42. Other forms of PCA, such as Geodesic PCA in BID19 and PGA in BID14, are considered. A comprehensive study of PCA-like algorithms can be found in BID32, along with proofs and discussions on manifold notions like curvature. Lemma 2 states that Riemannian gradient descent can recover the mean within a certain distance in time O(nr log \u22121). The Hessian of the squared distance of a sphere is discussed, showing that it is bounded and positive definite under certain conditions. The Hessian for hyperbolic space is bounded and positive definite. The Hessian for Euclidean space is also positive definite. The Hessian of the weighted mean is expressed, and the gradient descent can be initialized. The Hessian for hyperbolic space is bounded and positive definite, allowing for gradient descent initialization. The angle \u03b8 in spherical distances is in [0, \u03c0/2], ensuring PD Hessians. The weights satisfy w i \u2265 0, with at least one positive weight, leading to a PD H p,P. Theorem 4.2 in BID37 shows linear rate convergence. Various definitions of curvature are discussed, with a focus on product manifold curvature. Gauss introduced Gaussian curvature as the product of principal curvatures, representing the smallest and largest curvatures. Scalar curvature relates to the area of geodesic balls at a point, with negative curvature indicating faster volume growth and positive curvature indicating slower growth. Sectional curvature varies over all \"sheets\" passing through a point, providing a more detailed view of curvature on surfaces. The sectional curvature fully captures the most general notion of curvature on two-dimensional surfaces. It measures how geodesics diverge from a point, with positive curvature leading to slower divergence. The Ricci curvature averages sectional curvature over all planes containing a tangent vector, indicating volume differences compared to Euclidean space. Positive curvature results in smaller volumes, while negative curvature leads to larger volumes. The scalar curvature is defined as an average over the Ricci curvature. The scalar curvature is an average over the Ricci curvature, which is defined as an average over the sectional curvature. Discrete analogs of curvature, such as \u03be, aim to provide a discrete version of curvature for graphs without manifold structure. Different discrete curvatures like Forman-Ricci and OllivierRicci have been proposed. The input for discrete curvature estimation is similar to other discrete curvature analogs. The coarse Ricci curvature is defined for a node and neighbor, while the sectional curvature is defined for a point and two tangent vectors. In the proposed product space, the curvature of a Riemannian manifold assigns a function to each pair of vector fields. The sectional curvature is then defined for a two-dimensional subspace. The sectional curvature is defined for a two-dimensional subspace in a Riemannian manifold. It involves evaluating the curvature tensor for a product manifold, decomposing the curvature tensor, and calculating the sectional curvature for the product manifold. The sectional curvature is calculated for a two-dimensional subspace in a Riemannian manifold by evaluating the curvature tensor for a product manifold and decomposing it. When x1, y1 are linearly independent, the product sectional curvature relates to K1, K2 as a convex combination of the factor sectional curvatures. This relationship holds for non-negative (Euclidean and spherical spaces) and non-positive (Euclidean and hyperbolic spaces) cases, as well as for one negative and one positive space. The range of curvatures from Lemma 1 can be extended for a refined distributional analysis. Sampling a point p and a random plane V \u2286 T p M forms the Grassmannian manifold Gr(2, T p M). The uniform measure on this can be recovered from the Haar measure on the orthogonal group O(d). Embedding the subgraph induced by B into P with worst-case distortion 1 + \u03b4, and connecting nodes in T to a single node, embedding it into H r using combinatorial construction. The subgraph induced by B is embedded into P with worst-case distortion 1 + \u03b4, and nodes in T are connected to a single node, embedding it into H r using combinatorial construction. The overall distortion for the ring of trees is 1 + \u03b5, with linear complexity in the number of nodes. The construction of an embedding for any graph G on r nodes with low distortion is achieved by forming minimum distance trees rooted at each node and embedding them into copies of H2. This results in an overall distortion of 1 + \u03b5, with linear complexity in the number of nodes. The construction of an embedding for any graph G on r nodes with low distortion is achieved by forming minimum distance trees rooted at each node and embedding them into copies of H2. This results in an overall distortion of 1 + \u03b5. The combinatorial construction using the 1 distance can embed the hanging tree graph well, providing interpretability to the embedding. Empirical evidence shows this phenomenon, even with optimization over the 2 distance. Experimental setups involved implementing the optimization framework in PyTorch, optimizing the loss function with SGD using minibatches of 65536 edges for real-world datasets, and running for 2000 epochs. Learning rates were chosen from specific ranges for different datasets. The input corpus for word embeddings is a preprocessed 2013 Wikipedia dump, with hyperparameters chosen similar to LW. Curvatures are initialized to -1 for hyperbolic components and 1 for spherical components, and are learned using a specific method. The final curvatures of the best signature are reported in the \"Best model\" row. The datasets and embeddings used in the current setup are the same as in the previous setup, including their numbers for Euclidean embeddings from fastText."
}