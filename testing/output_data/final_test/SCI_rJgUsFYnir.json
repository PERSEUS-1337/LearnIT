{
    "title": "rJgUsFYnir",
    "content": "The Disentanglement-PyTorch library facilitates research on unsupervised learning of disentangled representations by providing modular neural architectures, latent space dimensionality, and training algorithms. It handles training scheduling, logging, visualizations, and evaluation of encodings using various disentanglement metrics. The library includes implementations of VAE, Beta-VAE, Factor-VAE, DIP-I-VAE, DIP-II-VAE, Info-VAE, Beta-TCVAE, CVAE, and IFCVAE, and is compatible with the Disentanglement Challenge of NeurIPS 2019. The Disentanglement-PyTorch library supports research on unsupervised learning of disentangled representations by providing neural architectures, training algorithms, and evaluation metrics. It includes various VAE implementations and is compatible with the NeurIPS 2019 Disentanglement Challenge. The Disentanglement-PyTorch library offers unsupervised variational algorithms like VAE, \u03b2-VAE, \u03b2-TCVAE, Factor-VAE, Info-VAE, DIP-I-VAE, and DIP-II-VAE. It is released under the GNU General Public License and was used in the NeurIPS 2019 Disentanglement Challenge. The Disentanglement-PyTorch library offers various unsupervised variational algorithms specified by loss terms flag. Researchers can mix and match loss terms to optimize towards correlated goals. The library supports conditional approaches like CVAE and IFCVAE, which enforce certain latent factors to encode known attributes using discriminators in a supervised fashion. Unsupervised loss terms in neural architectures can encourage disentanglement across attribute-invariant latents. The dimensionality of data and latent spaces can be configured independently from the training algorithm, allowing for investigation of new encoder and decoder network architectures. Google's disentanglement metrics are used to evaluate representations, with support for metrics like BetaVAE, FactorVAE, Mutual Information Gap, Interventional Robustness Score, Disentanglement Completeness and Informativeness, and Separated Attribute Predictability. Gradually increasing capacity during training improves disentanglement without sacrificing reconstruction accuracy. Dynamic learning rate scheduling is supported to decrease the rate when the objective function stops decreasing. Researchers are encouraged to use logging for monitoring training progress. The library uses Weights & Biases for logging and visualization of training process. The \u03b2-TCVAE algorithm achieved best disentanglement results on mpi3d real dataset. Model trained with Adam optimizer for 90k iterations on batches of size 64. Learning rate reduced on plateau of objective function. The model used a \u03b2-TCVAE algorithm with specific network configurations for encoding and decoding. The encoder had 5 convolutional layers and the decoder had 6 deconvolutional layers. ReLU activations were used, and the model's performance on mpi3d datasets is shown in Table 1. The experiments had the same configurations. The model consistently performed better on the mpi3d real dataset compared to the toy dataset it was initialized on. Disentanglement performance on the mpi3d realistic dataset is visualized in Appendix A, showing incomplete disentanglement with some features encoded in the same latent factor. A latent space of size 20 was used, with changes in 13 latent factors having no effect on reconstruction."
}