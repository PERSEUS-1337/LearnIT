{
    "title": "ryey1TNKvH",
    "content": "Node classes in colored graphs can be associated with topological features, improving Graph machine learning, including Graph Convolutional Networks (GCN). Good accuracy in predicting node class can be achieved using topological features or neighbors' class as input to GCN, even without external node information. This accuracy is slightly lower than content-based GCN. Adding an additional adjacency matrix with edges between distant nodes with similar topology significantly improves GCN accuracy, outperforming state-of-the-art methods. Node classification tasks rely on the assumption that neighboring nodes have similar classes, a concept extensively used in graph neural networks. Graphs in machine learning can be utilized in various ways, such as regularizing output based on neighboring nodes, propagating labels for optimal learning, projecting nodes into real-valued vectors for supervised or unsupervised learning, and using Graph Convolutional Networks (GCN) for convolutions. Methods like graph partitioning based on Laplacian eigenvalues are also employed for regularization. The Laplacian of a graph is calculated as L = D - A, where D is a diagonal matrix and A is the adjacency matrix. This Laplacian is often weighted for normalization by multiplying it with D on both sides. Various approaches have been proposed for utilizing graphs in machine learning, including methods like DeepWalk and Planetoid for node projection and random walks combined with negative sampling. Other works have focused on diffusion models and information propagation through real-valued vectors. The DNGR model uses random walk to compute mutual information between points and then projects into space using SVD decomposition. Another approach involves graph projection using Laplacian eigenvectors for classification tasks. Various methods have been used to project points in graphs, including convolution with eigenvectors, Multi-Dimensional-Scaling (MDS) projection, and word embedding techniques like word2vec. These approaches define context for node embedding construction and can be used as features for prediction tasks. Local features can also be used to predict classes, and recent work has proposed simplifications of spectral-based convolutions. Recent work has proposed simplifications of spectral-based convolutions, using a two-layer approach with a normalized adjacency matrix. Testing on multiple graphs with labeled nodes, including CiteSeer, Cora, Pubmed, and Nell, convolution approaches can be applied as filters on the input. Random filters have been introduced in recent methods, such as predetermined convolutions with powers of the adjacency matrix by Atwood & Towsley (2016) and multi-level graph convolution with pooling by Bruna et al. (2013). Agglomerative clustering methods are used with a pooling method to represent image resolutions. Various convolutional kernels, including spectral and diffusion-based, are applied for image classification. Polynomial convolution in the spectral domain is utilized by Vandergheynst and colleagues. Graph convolutional networks (GCNs) have been extended and applied in various ways, including combinations with recurrent neural networks, GANs, and active learning methods. GCNs capture dependencies of nodes' features. GCNs capture dependencies of nodes' features by considering local neighborhoods. To capture long-range dependencies, stacking multiple layers of GCN has been proposed, although in practice, 2-3 layers work best. NGCN trains multiple instances of GCNs over different distance regions, but it is inefficient and does not scale well. Recently, a correlation has been shown between the topological attributes of nodes and their class. Inspired by the improvement of non-local operations in computer vision tasks, a novel non-local operation for GCN based on graph topology is proposed to capture long-range dependencies. This operation allows information propagation to distant nodes by computing interactions between any two nodes, regardless of their positional distance. The proposed non-local operation for GCN based on graph topology allows efficient information propagation between distant nodes, achieving good results with few layers. It can be easily combined with other graph convolution techniques. Node topology can be used to predict node class using a feed-forward network, with attributes like degree and centrality. This information can be translated to GCN input, showing the importance of adding additional adjacency in the context of GCN. In the context of GCN, it is more effective to include an additional adjacency matrix representing node topology rather than adding the nodes' topology as input. GCN and GAT with this matrix outperform state-of-the-art methods on various datasets. These datasets include PubMed, CiteSeer, CORA, CORA-Full, Coauthor CS, and Coauthor Physics. The standard GCN model by Kipf & Welling or GAT by Veli\u010dkovi\u0107 was used for the experiments. The GAT model involves multiple heads in each layer, with attention coefficients calculated for connected nodes. Proposed extensions include T-GCN for information propagation through distant neighbors with similar topological features. The T-GCN and T-GAT models involve constructing topological edges based on node similarity, using GCN or GAT layers for information processing. The T-GCN includes two GCN layers operating on input features, while the T-GAT replaces GCN layers with GAT layers. These models aim to enhance information propagation through nodes with similar topological attributes. The T-GCN and T-GAT models use GCN or GAT layers for information processing based on node similarity. The A-GCN and C-GCN are alternative methods tested, with A-GCN incorporating the direction of directed networks and C-GCN including two input types: a topology features matrix and an external features matrix. Multiple inputs were tested in these configurations. The data matrix is processed through a GCN layer to obtain a 2n \u00d7 L 1 output. The inputs are concatenated and rearranged before passing through A-GCN layers. Different hyper-parameters were used for Cora and Pubmed datasets. For Cora, T-GCN used 1 hidden layer of size 32 for each graph, while T-GAT had 16 internal nodes for regular and 8 internal nodes for dual graphs. Multiple inputs were tested in these configurations. For the T-GAT, 16 internal nodes were chosen for the regular graph and 8 internal nodes for the dual graph. 8 heads were used for both operations at the first layer, and 1 head for the last layer. Optimal parameters from PubMed were used for other datasets. SoftMax was performed on the last layer of all models. External features were normalized and GAT heads were concatenated on the first layer, and averaged on the last layer. The T-GAT model used 16 internal nodes for the regular graph and 8 for the dual graph. SoftMax was applied on the last layer, and GAT heads were concatenated on the first layer and averaged on the last layer. Tests were conducted to analyze the correlation between neighbor class, node self-topology, and node class in networks like Cora or Citeseer. The study analyzed the correlation between node class and topological features in networks like Cora or Citeseer. Over sixty small scale motifs were associated with node class. Topological features and neighbor information were used to classify node classes accurately using a Feed Forward-Network. The study analyzed the correlation between node class and topological features in networks like Cora or Citeseer. All values were normalized to 1, with an equal distribution resulting in equally divided columns. The main topological factors correlated with the class are small scale motifs, and an alternative method was proposed to test their contribution to classification without explicit computation of subgraph frequencies. The study proposed an indirect computation of network topology features using products of the adjacency matrix, avoiding the explicit calculation of subgraph frequencies. By combining these products as inputs to a feedforward neural network, the method can effectively analyze the correlation between node class and topological features without the need for computationally expensive calculations. The study proposed a method to analyze the correlation between node class and topological features using products of the adjacency matrix as inputs to a neural network. This approach outperformed explicit topological measures and information propagation in predicting node class accuracy. Different topological inputs were tested, including the number of neighbors belonging to each class and topological features of each node. The study compared different models using topological features as input, showing that neighbors feature performed best. The standard GCN outperformed the A-GCN, while the T-GCN was equal to or better than the standard GCN. Models with BOW input outperformed those without it. In the study, models with Bag of Words (BOW) outperformed those without it. Combining BOW with topology as input reduced accuracy. However, using only neighbors features without BOW was better in some cases. Testing topology-based convolution (T-GCN) showed promising results compared to state-of-the-art models on various real-world networks. In Cora, CiteSeer, and PubMed networks, data was split for training, validation, and testing. Experiments were repeated 100 times, and average accuracy was reported. GCN and GAT results were evaluated using a pytorch implementation. GAT was trained for 500 epochs to ensure fair evaluation. The study evaluated GAT using 500 epochs for training and compared its performance with other models in Cora, Pubmed, and Physics. Results showed that T-GCN and T-GAT outperformed all other models. The comparison was based on average accuracy over 100 trials using standard splits for different datasets. The T-GCN consistently outperforms the GCN in random splits, with up to a 3.3% higher accuracy in the CiteSeer dataset. Graphs' complex topology can be leveraged with convolutional networks to improve classification accuracy. The complex topology of graphs can be captured using unsupervised or supervised methods, creating sub-graph motifs associated with node attributes. Topology around each node is linked to manuscript class in a novel GCN proposed for information propagation. The novel GCN proposed combines information propagation with topology-based classification to improve performance in datasets. Using the fraction of second neighbors belonging to each class as input, the method achieves high accuracy but less than using a BOW input. Combining the topology with BOW reduces accuracy, but adding new edges between nodes with similar topological features enhances performance. This approach correlates distant nodes' classes using topology, presenting a novel content-independent method for classification. The GCN based classifier offers a new content-independent method for node classification in citation networks. BOW vectors and co-authorship graphs are utilized to represent publication data and author relationships. Parameters are optimized for models like T-GCN and T-GAT, specifically for PubMed. The parameters for models T-GCN and T-GAT were optimized for PubMed data, with slight variations for Cora data. The activation functions and hidden sizes are detailed in Table 3. The goal is to classify node colors based solely on the graph structure, ignoring external content. The attributes used to convert nodes into network attribute vectors (NAV) include degree, betweenness centrality, closeness centrality, distance distribution, and flow. These features are utilized to classify node colors based solely on the graph structure. The curr_chunk discusses various network analysis techniques such as flow measurement, attraction basin hierarchy, network motifs, K-cores, and the Louvain community detection algorithm. These methods are used to analyze network structures and relationships between nodes. The Louvain community detection algorithm optimizes modularity to detect communities in a network. A feature based on the number of neighbors belonging to each class in the training set is used to create a vector of sums for each node. In directed graphs, features are calculated for both In and Out neighbors. The results are generated using a feed-forward network with internal layers and an output layer. The feed-forward network used for community detection had internal layers of sizes 300 and 100 nodes, with a linear output layer. It employed Relus for nonlinearities, L2 regularization of 0.2, and a 10% dropout rate. The loss function was categorical cross-entropy. The network classified nodes based on their neighbors' class distribution or adjacency matrix products. Sub-graph frequency was determined through adjacency matrix products."
}