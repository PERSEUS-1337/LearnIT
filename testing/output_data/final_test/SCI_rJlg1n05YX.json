{
    "title": "rJlg1n05YX",
    "content": "In response to the increasing demand for deploying CNNs on mobile platforms, the sparse kernel approach was introduced to save parameters while maintaining accuracy. This paper focuses on crafting an effective sparse kernel design by reducing the design space and eliminating repeated layers and designs with significant accuracy degradation. The paper focuses on crafting an effective sparse kernel design by reducing the design space and eliminating repeated layers and designs with significant accuracy degradation. Additionally, detailed efficiency analysis is provided for the final 4 designs in the scheme, showing improved parameter efficiency and computation while maintaining or improving accuracy. Model compression techniques like pruning, quantization, and low rank approximation have been proposed to reduce model size for deployment on memory-constrained platforms. However, these approaches suffer from drawbacks such as irregular network structure, increased training complexity, and heuristic compression ratios. A recent solution involves training networks using sparse convolutional kernels with fixed compression ratios to address these issues. Sparse convolutional kernels with fixed compression ratios are being explored in the context of CNNs to save parameters and computation. MobileNets BID12 achieved a 7x parameter savings with only 1% accuracy loss by combining two sparse kernels, depthwise convolution BID26 and pointwise convolution BID20. However, crafting an effective sparse kernel design with such potential remains a challenge in the field. Prior works have only adopted simple combinations of existing sparse kernels. Sparse convolutional kernels with fixed compression ratios are being explored in the context of CNNs to save parameters and computation. Prior works have only adopted simple combinations of existing sparse kernels like group convolution, depthwise convolution, pointwise convolution, and pointwise group convolution. However, it remains a challenge to craft an effective sparse kernel design that is more efficient than existing ones while maintaining similar accuracy. The number of possible combinations grows exponentially with the number of kernels, making it infeasible to train each one. In this paper, the effective sparse kernel design is crafted by efficiently eliminating poor candidates from the large design space. The design space is reduced by ignoring combinations with repeated patterns and designs leading to large accuracy degradation. An easily measurable quantity called information field is used to identify designs closely related to model accuracy. Designs with a smaller information field compared to the standard are discarded. In this study, a sparse kernel scheme is presented, incorporating four different designs manually reduced from the original design space. Efficiency factors for each design are addressed, focusing on parameter and computation efficiency. Researchers aim to select the most efficient sparse kernel designs based on their needs, driving the demand for studying efficiency in different designs. The study introduces a sparse kernel scheme with four designs, focusing on efficiency factors and model accuracy. The information field is highlighted as crucial for sparse kernel designs, with a positive correlation to model accuracy. The scheme outperforms state-of-the-art methods, showcasing the effectiveness of the approach. The study introduces a sparse kernel scheme with four designs, focusing on efficiency factors and model accuracy. It incorporates final 4 types of designs with a rigorous mathematical foundation on efficiency. Group convolution is first used in AlexNet for distributing the model over two GPUs. Group convolution is utilized in AlexNet to distribute the model over two GPUs by splitting input and output channels into disjoint groups. This approach reduces the number of parameters and computation needed. Depthwise convolution is similar to group convolution, where kernels are sparsified in the channel extent. It can be seen as an extreme case of group convolution when the number of groups equals the number of input channels. Typically, the number of channels remains unchanged after applying depthwise convolution. Pointwise convolution is a 1 \u00d7 1 standard convolution that achieves sparsity over the spatial extent using kernels with 1 \u00d7 1 spatial size. Pointwise group convolution combines group convolution with pointwise convolution to sparsify kernels in both the channel and spatial extents. Each output channel in pointwise group convolution depends on a portion of input channels. The total number of combinations grows exponentially with the number of kernels. In this paper, the authors focus on crafting an effective sparse kernel design by efficiently exploring the design space. They set the maximum length of sparse kernels to 6, considering the need to find efficient designs while minimizing parameter consumption. The authors set the maximum length of sparse kernels to 6 to explore efficient designs. They focus on reducing the design space in three aspects: composition, performance, and efficiency in CNNs. The authors aim to reduce the design space in CNNs by eliminating repeated patterns in sparse kernels using regular expression matching, which allows for efficient detection and removal of such patterns. The authors efficiently eliminate design space in CNNs by using regular expressions to detect and remove repeated patterns in sparse kernels, leading to significant accuracy improvements. They introduce the concept of an \"information field\" as a measurable property that directly indicates final accuracy, helping to streamline the design process. The information field, with a spatial kernel size of 3x3, connects output and input channels, influencing model accuracy. Sparse kernel designs can be optimized by considering the information field size, leading to improved accuracy without training models. The size of the information field for each design is calculated by updating a vector based on sparse kernel properties. Designs are compared to standard convolution, with those not matching in size being discarded for efficiency. The efficiency of designs is crucial in comparison to standard convolution. Redundant designs that do not decrease the information field size or match the standard convolution size should be eliminated for better parameter efficiency. In comparison to standard convolution, designs should eliminate non-contributed kernels to improve parameter efficiency. An early-stop mechanism is introduced to detect designs with redundant kernels by checking the size of the information field. The text discusses reducing design space by adding conditional checks based on information field size and introducing sparse kernel designs. It also mentions combining techniques like bottleneck structure to improve parameter efficiency. The combination of depthwise convolution (DW) and pointwise convolution (PW) splits spatial and channel information for separate processing. The output activation is calculated using kernels W1 and W2. Group convolution (GC) and pointwise group convolution (PWG) extend this design, applying group convolution on the pointwise convolution. The combination of depthwise convolution (DW) and pointwise convolution (PW) splits spatial and channel information for separate processing. Group convolution (GC) and pointwise group convolution (PWG) extend this design, applying group convolution on the pointwise convolution to recover the information field. The output activation is calculated using kernels W1 and W2, with N denoting the number of groups for group convolution and pointwise group convolution. The design includes a channel permutation between the layers and utilizes a bottleneck structure to improve efficiency. The combination of two pointwise group convolutions and one depthwise convolution can help improve efficiency by reducing the number of parameters. The output activation is calculated using kernels W1, W2, and W3, with K representing the number of bottleneck channels. The information field of this design remains the same as standard convolution. The efficiency of different designs in the scheme varies, with the combination of two pointwise group convolutions and one depthwise convolution ensuring the same information field size. Channel permutation and bottleneck structure are also utilized for better efficiency. The output activation is calculated using specific kernels, and the efficiency of each design is studied to help researchers find the most efficient option based on their needs. In studying the efficiency of different designs, two real situations are considered for sparse kernel designs. The parameter efficiency is analyzed based on input and output channels, as well as the total number of parameters. The best efficiency conditions are determined for each situation, providing insights for researchers to choose the most efficient design. The total number of parameters determines the greatest width for best efficiency in design. Efficiency remains constant when the number of parameters is fixed. Parameter efficiency is shown through the ratio of parameters in the design. The best parameter efficiency in design is achieved when the product of the two group numbers equals the channel number of the intermediate layer. This means that M \u00b7 N should be equal to the number of output channels from the first layer, i.e., M \u00b7 N = C. Efficiency is maximized when M \u00b7 N = C, with the ratio being Efficiency given the total parameters. The width of the network is influenced by both M and N. The greatest width is achieved when C = M \u00b7 N. The number of parameters P can be expressed as DISPLAYFORM2 with an upper bound when 9N = \u03b1M for best efficiency. The total parameters are reduced to DISPLAYFORM0 after design implementation. The ratio for standard convolution is 36C when K = F/4, resulting in fixed efficiency. Efficiency is optimized when the number of bottleneck channels K = M \u00b7 N. The best parameter efficiency is achieved when K = F/4. The total number of parameters P is fixed, with the greatest width G also fixed. The ratio of parameters can be represented as DISPLAYFORM1. The best parameter efficiency is reached by setting DISPLAYFORM2. The network layout includes identity mapping over each block, batch normalization after each layer, and ReLU activation after block output summation. Evaluation is done on the ImageNet 2012 dataset with data augmentation. The study includes training models on ImageNet dataset with data augmentation and specific training parameters. Sparse kernel designs generated by their scheme have the same information field size with varying parameter/computation savings compared to standard convolution. The study explores sparse kernel designs to increase accuracy by saving computation/parameters through increasing channels and information field size. Results show that changing the number of groups in bottleneck-like design does not affect information field size, with significant parameter savings and minimal accuracy decrease. The study examines the impact of increasing network width on model accuracy, showing that widening the network can lead to improved performance. Results suggest that the size of the information field plays a crucial role in accuracy improvement, with potential gains from increasing network width rather than depth. Additionally, sparse kernel design (PW+DW+PW) is further explained in TAB1. In Section 3.2, the study explains the sparse kernel design (PW+DW+PW) in TAB1, highlighting the parameter-efficient depthwise convolution. Comparisons of different sparse kernel designs are discussed in Section 3, with results shown in TAB2. The analysis reveals that models with different widths can be constructed by choosing different sparse kernel designs or group numbers, with accuracy positively correlated to the model width. The results do not indicate one sparse kernel design is always superior in terms of parameter efficiency. The design GConv(M)+PWGConv(N) is more efficient than DW+PW in terms of parameter efficiency, as shown in Section 3. Our sparse kernel scheme allows for the creation of more efficient designs compared to state-of-the-art ones, as demonstrated in TAB3. Fair comparisons were made using the same network layout, with model size around 11.0M selected for evaluation. Results in TAB3 show that sparse kernel designs in the scheme can achieve better accuracy with a smaller model size. The group numbers chosen in the designs aim to accommodate both model size and network layout efficiently. Traditional model compression techniques like pruning, quantization, and low-rank approximation are used to reduce redundant weights, network connections, or channels. The focus of this paper is on the sparse kernel approach, which directly trains networks using structural sparse convolutional kernels to mitigate issues faced by traditional model compression techniques like pruning, quantization, and low-rank approximation. This approach aims to craft effective sparse kernel designs for better accuracy and smaller model size. The paper presents a scheme for crafting effective sparse kernel designs by reducing the design space from three aspects: composition, performance, and efficiency. An unified property named information field is identified behind various designs, directly indicating final accuracy. Four designs are showcased with efficiency analysis, and experimental results validate the scheme. Theorem 1 is proven by contradiction, demonstrating the minimum value of P can be achieved only if M \u00b7 N = C. The theorem is proven by contradiction, showing that the minimum value of P can only be achieved when M \u00b7 N = C."
}