{
    "title": "HyeL9yh4KH",
    "content": "Reverse-mode automatic differentiation for stochastic differential equations (SDEs) enables efficient computation of pathwise gradients. A backward SDE is constructed to provide the gradient solution with conditions for numerical convergence. This approach is combined with stochastic variational inference for continuous-time SDE models, allowing distribution learning over functions using stochastic gradient descent. The latent SDE model shows competitive performance in time series modeling. The stochastic adjoint sensitivity method is a memory-efficient approach for computing gradients through stochastic differential equations (SDEs). It builds on theoretical advances to reconstruct trajectories and evaluate gradients by solving a backward SDE. The SDE formulation detailed in Section 3 allows for retracing the original trajectory during the backward pass by reusing noise samples. An algorithm in Section 4 enables precise querying of a Brownian motion realization with only a single random seed, resulting in a constant-memory algorithm approximating gradients well. The method is compared to previous approaches in Table 2 in terms of time and memory complexity. SDEs are incorporated into a stochastic variational inference framework for efficient computation of likelihood ratios and backpropagation through the evidence lower bound using the adjoint approach, generalizing existing model families like latent ODEs. The adjoint approach generalizes model families like latent ODEs and deep Kalman filters. It efficiently solves optimization problems by considering the dual form. Recent works have explored SDEs with drift and diffusion functions defined by neural networks. Consider a filtered probability space with an m-dimensional adapted Wiener process defined by an It\u00f4 SDE. When coefficients are globally Lipschitz, a unique strong solution exists. Neural networks can define coefficients with smooth activation functions, resulting in a neural SDE model. A backward Stratonovich SDE is derived for the stochastic adjoint process, leading to a gradient computation algorithm. The text discusses a gradient computation algorithm that works by solving dynamics in reverse time using vector-Jacobian products. It involves the solution of a backward SDE system and the unique strong solution of an augmented state. The algorithm is based on neural networks defining coefficients with smooth activation functions, resulting in a neural SDE model. The algorithm presented involves solving dynamics in reverse time to compute gradients using vector-Jacobian products. It utilizes a black-box solver for SDEs and incorporates parameters into the state. The method allows for precise querying of Wiener process sample paths. The data structure enables precise querying of Wiener process sample paths based on a global random seed. It facilitates the adjoint method to ensure consistency in noise samples between forward and backward solves. The variational free energy can be computed by augmenting the forward equation with an extra variable. In the context of the data structure enabling precise querying of Wiener process sample paths, the forward equation is augmented with an extra variable for variational free energy computation. The theory is verified by comparing gradients obtained using a stochastic adjoint framework against analytically derived gradients. Latent SDE models are fitted on synthetic and real datasets, promoting learning a generative model of time series. Results on numerical studies and synthetic data are detailed in the appendices, with only the motion capture dataset results presented here. The dataset used is from the CMU motion capture library, specifically sequences of subject number 35 partitioned into training, validation, and test sequences. The test MSE is reported following previous studies. Recent work on neural SDEs has not provided an efficient training framework, with different approaches considered for computing gradients. The approach presented requires evaluating vector-Jacobian products a constant number of times with respect to the number of parameters and states. It involves stochastic processes running forward and backward in time, utilizing the Stratonovich stochastic integral for backward dynamics. Results can be applied to It\u00f4 SDEs using a conversion result. The forward and backward Stratonovich integrals are introduced following Kunita's treatment. The Stratonovich stochastic integral for a continuous semimartingale adapted to the forward filtration is defined using the size of the largest segment of the partition. The It\u00f4 integral differs by using the left endpoint instead of the average. The backward Stratonovich integral is defined using the backward Wiener process and a partition. The Stratonovich SDE defines a stochastic flow of diffeomorphisms, where \u03a6 s,t is a smooth diffeomorphism from R d to itself. The collection S = {\u03a6 s,t } satisfies the flow property and is generated by the SDE. The stochastic flow of diffeomorphisms, denoted as S, is generated by the SDE. The backward flow \u03a8 s,t satisfies a backward SDE, with coefficients differing only by a negative sign. By using the Stratonovich integral, negating the drift and diffusion functions for the converted Stratonovich SDE allows for correct path simulation in reverse time. The main contribution is the stochastic adjoint process, enabling a gradient computation algorithm based on solving dynamics in reverse time. The main theoretical result is Theorem 3.1, which derives a backward SDE for the process {\u2202Z T /\u2202Z t } t\u2208T, extending to cases where adaptiveness is lost and formulating results using the It\u00f4 map. The state of Z is extended to include parameters, obtaining gradients with respect to them. The backward SDE for the Jacobian matrix of the backward flow is derived, satisfying the backward SDE for all s \u2264 t and x \u2208 R d a.s. The proof in Appendix I relies on It\u00f4's lemma in the Stratonovich form. The system is a backward Stratonovich SDE with a unique strong solution. The solution to the backward SDE is computed by algorithm F, while G is for the forward flow. Theorems are provided for convergence conditions of numerical solvers Fh and Gh. The Euler-Maruyama and Milstein methods converge pathwise with explicit rates for any fixed starting point. The SDEs considered have smooth coefficients, leading to nice regularity properties in the starting position, suggesting numerical schemes will behave well with mesh size and starting position variations. This property is crucial for the proof of Theorem C.2 and is not typically addressed in literature on SDE numerical methods. The text discusses deriving gradients of the loss with respect to the initial state and parameters of drift and diffusion functions. It introduces Algorithm 1 2 for this purpose, utilizing a numerical solver SDESolve. The Euler-Maruyama scheme is highlighted for its costly terms in computing gradients, emphasizing the need for high-order solvers to obtain a strong numerical solution. The Wiener process involves difficult-to-simulate random variables, with a focus on using adaptive solvers and strong order numerical schemes for diagonal noise dynamics. This allows for the adoption of specific numerical schemes without the need for approximating multiple integrals or the L\u00e9vy area during simulation. We have implemented several SDE solvers in PyTorch, including Euler-Maruyama, Milstein, and stochastic Runge-Kutta schemes with adaptive time-stepping. Additionally, a user-friendly subclass of torch.autograd.Function has been created to facilitate gradient computation using a stochastic adjoint framework. The adjoint formulation allows for numerical integration by evaluating dynamics defined by vector-Jacobian products. To address the challenge of querying Wiener process values at arbitrary times during the backward pass, a data structure combining Brownian trees with splittable PRNGs has been developed. This structure allows for efficient querying with logarithmic time cost and enables adaptive time-stepping numerical integrators. The Brownian tree is constructed by recursively applying a formula to evaluate the process at the midpoint of known timestamps. A splittable PRNG is used to generate keys for sampling from the Brownian tree. The algorithm terminates when the Wiener process value at a specific time is obtained. The algorithm for constructing the Brownian tree terminates when the Wiener process value at a specific time is obtained. Algorithm D.2 outlines the procedure with constant memory cost, scaling complexity per step as log L. The variational free energy can be efficiently estimated using Monte Carlo methods, simplifying the equation through Novikov's condition. The It\u00f4 integral t 0 V s dW s is a Martingale (\u00d8ksendal, 2003). To Monte Carlo simulate the quantity in the forward pass, extend the original augmented state with an extra variable L t. The backward SDEs of the adjoint processes become simplified. Three test problems with closed-form solutions are considered. Comparison of gradients computed from simulating the stochastic adjoint process using the Milstein scheme is done. The error between the adjoint gradient and analytical gradient decreases as the step size decreases. The adaptive solver can control the error, with smaller mean-square errors as the absolute tolerance is reduced. The Number of Function Evaluations tends to be larger due to the roughness of Brownian motion paths. Gradient computation is crucial for sensitivity analysis in SDEs. Gradients with respect to parameters of vector fields in SDEs have been extensively studied in stochastic control literature. Different approaches like dynamic programming and finite differences are used for low dimensional problems, but they scale poorly with parameter vector dimensionality. Yang and Kushner introduced a method similar to REINFORCE for a random variable H, which depends on the density of ZT with respect to the Lebesgue measure. Gobet and Munos extended this approach by weakening a non-degeneracy condition using Mallianvin calculus. The pathwise method by Yang and Kushner is the continuous-time analog of these techniques. The pathwise method is a continuous-time analog of the reparameterization trick. Existing methods in this regime all require simulating a forward SDE with prohibitive computational costs for high-dimensional systems. The adjoint approach, based on Euler discretization, stores intermediate values and uses reverse-mode automatic differentiation. This method, widely adopted in finance for calibrating market models, has high memory costs. The backward SDE for the stochastic adjoint process is based on two-sided filtrations, different from traditional backward SDEs. Forward-backward SDEs have been proposed for stochastic optimal control, but simulating them is costly due to estimating conditional expectations. This estimation arises from the appearance of an auxiliary process from the Martingale representation theorem. The appearance of an auxiliary process from the Martingale representation theorem leads to consequences in bounding I1 and I2, showing their convergence to 0 in probability as h approaches 0. The Stratonovich SDE with drift and diffusion functions is governed by parameters \u03b8. The augmented state satisfies a Stratonovich SDE with drift function f and diffusion functions. The dynamics for the adjoint process of the augmented state is characterized by a backward SDE. Assuming diagonal noise, the Jacobian matrix can be written out. Consider the adjoint process for the augmented state along with the backward flow of the backward SDE. The overall state X t = (Z t , (A z t ) , (A \u03b8 t ) ) satisfies a backward SDE with a diffusion function that satisfies the commutativity property. This property allows for efficient simulation of double It\u00f4 integrals by taking advantage of iterated integrals. The diffusion function is shown to satisfy the commutativity condition through various cases. The commutativity condition holds for the backward SDE with a diffusion function, allowing for efficient simulation of double It\u00f4 integrals. The Milstein scheme for the stochastic adjoint of diagonal noise SDEs can be implemented with a constant number of calls to vjp during each iteration of the backward solve. In numerical experiments, equations are duplicated to create a system of SDEs with varied parameter values. Training latent SDE models using an adjoint framework is done to recover specific processes like a 1D Geometric Brownian motion and a 3D stochastic Lorenz attractor. The study aims to recover a 1D Geometric Brownian motion and a 3D stochastic Lorenz attractor process. The learned posterior reconstructs the training data, and the learned prior exhibits stochastic behavior. Optimization is done on the variational free energy with respect to various parameters. The stochastic Lorenz attractor model can reconstruct data well and produce bimodal samples in both data and latent space. The space samples cluster around two modes, which cannot be achieved by a latent ODE. Figures 4 and 5 provide visualizations on synthetic datasets. The prior process accounts for most uncertainty in the initial latent state. The study uses a geometric Brownian motion SDE with specific parameters and corrupts data with Gaussian noise. The study utilizes a GRU-based latent SDE model with specific architecture and parameters to recover dynamics corrupted by Gaussian noise. The model includes time-inhomogeneous drift functions and a linear decoder mapping latent space to observation space. The observation model is fixed to be Gaussian with noise standard deviation 0.01. The model is optimized jointly for various parameters including the initial latent state distribution, drift functions, and encoder-decoder. A fixed discretization with step size of 0.01 is used, along with the Adam optimizer. The system is described by a stochastic Lorenz attractor SDE with specific parameters. We sample 1024 time series normalized by mean and standard deviation, corrupted with Gaussian noise. The latent SDE model uses a specific architecture with four neural networks for the diffusion function. Preprocessing follows previous studies, and a fully connected network encodes initial observations to predict the remaining sequence. The encoder may be extended for better performance. The architecture of the latent ODE and latent SDE models is similar to that of ODE 2 VAE with a fixed step size. The models are trained using the Adam optimizer with default hyperparameters and decayed learning rate. Validation is performed over training iterations, KL penalty, and annealing schedule. All models are trained for a maximum of 400 iterations to avoid overfitting. The latent SDE model uses an MLP encoder to predict the initial latent state distribution. The decoder architecture is similar to the ODE 2 VAE model, projecting a 6-dimensional latent state into a 50-dimensional observation space. The posterior drift function takes a 3-dimensional context vector from the encoder, while the prior drift only considers the current state and time. The diffusion function consists of small neural nets producing scalars for each dimension, resulting in diagonal noise for the posterior SDE. The model has a parameter count of 11605, smaller than the ODE 2 VAE model. The latent ODE baseline lacks diffusion and prior drift components, with a vector field defining the ODE. The model architecture includes an encoder, decoder, prior drift, posterior drift, and diffusion functions. It has fewer parameters (10573) compared to the latent SDE model. The main hyperparameter tuned was the coefficient for reweighting the KL. Training options included reweighting coefficients in {1, 0.1, 0.01, 0.001} with or without linear KL annealing. See Figure 6 for architecture details."
}