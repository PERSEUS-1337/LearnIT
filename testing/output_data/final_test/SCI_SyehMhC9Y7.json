{
    "title": "SyehMhC9Y7",
    "content": "Imitation learning is useful for autonomous control as it allows for demonstrations of preferred behavior from human experts, reducing the need for real-world data collection. However, policies learned through imitation learning lack flexibility. Model-based reinforcement learning offers more flexibility by using a predictive model to achieve various goals at test time. However, MBRL has shortcomings as the model does not help choose desired outcomes and requires additional online data collection for accuracy. In this paper, the authors propose imitative models that combine imitation learning and model-based reinforcement learning (MBRL) to plan expert-like trajectories for achieving goals. The method outperforms direct imitation and MBRL in a simulated autonomous driving task, can incorporate user-supplied costs at test-time, plan sequences of goals, and perform well with imprecise goals. Reinforcement learning (RL) algorithms offer the promise of learning behaviors from raw sensory inputs with minimal engineering but generally require online data collection. Incorporating expert demonstrations into a flexible robotic system, like an autonomous car, can be achieved through imitation learning (IL) or model-based RL (MBRL). However, both approaches are vulnerable to distributional drift when acting according to the learned model or policy. When using model-based RL or imitation learning with expert demonstrations in robotic systems like autonomous cars, distributional drift can occur, leading to ineffective actions in unseen states. MBRL algorithms require online data collection and training to address this issue, while IL algorithms can sometimes learn effective policies without additional data. However, standard IL has limited task flexibility as it only predicts low-level behavior. Some works have augmented IL with goal conditioning, but these goals must be predefined during training and are typically simple. In navigation tasks in CARLA BID5, a deep imitative trajectory model plans goals based on LIDAR input, with plans colored by ranking. The model can adjust planning based on test-time costs. A taxonomy of learning-based control methods is presented, focusing on avoiding online data collection from the policy being imitated. In navigation tasks in CARLA BID5, a deep imitative trajectory model plans goals based on LIDAR input, with plans colored by ranking. The model can adjust planning based on test-time costs. The goal is to devise a new algorithm that combines the advantages of IL and MBRL, capturing expert behaviors without manually designed reward functions. The method infers the most probable expert state trajectory to plan to a goal, incorporating a model-based representation for flexibility in achieving new user-specified goals at test time. Our method combines the advantages of imitation learning and model-based reinforcement learning for autonomous driving tasks. It generates interpretable trajectories that follow navigational goals and obey road rules without additional training. The approach outperforms both MBRL and IL in experiments, achieving near-perfect driving performance. The model combines imitation learning and model-based reinforcement learning for autonomous driving, outperforming MBRL and IL. It efficiently learns driving through the CARLA simulator from 7,000 trajectories. The model can achieve goals not seen during training and is robust to errors in navigation systems. The model conditions on observations to forecast expert trajectories and score them based on likelihood. It serves as a learned prior for imitating expert drivers and directing the agent towards desired goals. The goal is to reason automatically about mid-level details to achieve these goals. The driving task is defined by a set of goal variables G, with trajectory planning based on posterior distribution p(s 1:T |G, \u03c6) and MAP inference using prior q(s 1:T |\u03c6) and likelihood p(G|s 1:T , \u03c6). An example is waypoint planning towards a specific goal location, achieved by a goal likelihood function centered at the desired final state. The text discusses planning trajectories towards a desired goal location using a goal likelihood function. It mentions the ability to plan to successive states and propose waypoints for the robot to reach. Waypoint planning leverages conditional imitation learning, allowing communication of desired destinations without knowing the best actions. The planning-as-inference procedure generates paths similar to expert actions to reach a goal, providing interpretability and feasibility estimates. The model allows for user-specified costs at test time, offering more control over the plan. The Reparameterized Pushforward Policy (R2P2) is a deep generative model for Imitation Learning that computes q(s 1:T |\u03c6) and gradients \u2207 s 1:T q(s 1:T |\u03c6) for gradient-based optimization in planning. It efficiently minimizes false positives and false negatives by using pushforward distributions. R2P2 utilizes models BID21 BID3 to minimize false positives and false negatives by optimizing KL(p, q) to penalize mode loss. It approximates the unknown distribution of expert behavior with a spatial cost model, inducing q(s 1:T |\u03c6) through an invertible function f (z; \u03c6). The trajectory distribution in R2P2 is complex and multimodal, using RNN method with LIDAR features for planning trajectories. Three layers of spatial abstractions are used for planning routes in autonomous vehicle setups. The Imitative Planner generates paths based on waypoints provided by a route planner for autonomous driving. The model is trained on expert examples and repurposed for planning paths at test time. The Imitative Planner generates paths for autonomous driving based on waypoints from a route planner. A model-based conditional IL approach is proposed, offering flexibility for handling complex directives and ranking plans by objectives. The Imitative Planner generates paths for autonomous driving based on waypoints from a route planner. Our method ensures safety by never executing unlikely transitions and can stop the car if no safe plan is found. Online imitation learning is an active area of research, but our focus is on offline data collection. In autonomous driving environments, various methods predict the behavior of other vehicles or pedestrians using recurrent neural networks combined with Gaussian density layers or generative models. These methods lack the ability to evaluate trajectory likelihood or perform other inference tasks. Another approach involves inverse reinforcement learning to fit a probabilistic reward model to human demonstrations. The evaluation is done using the CARLA urban driving simulator, where the task is to drive to a goal location in Town01 or Town02 maps. The method utilizes three layers of spatial abstractions to plan the route. The study uses three layers of spatial abstractions for planning in autonomous driving: coarse route planning, path planning, and feedback control. A route to the goal location is computed using A* algorithm, waypoints are set along the route, and a PID-controller is used for steering. Four metrics are considered: success rate in reaching the goal, driving in the correct lane, crashes into obstacles, and passenger comfort. The study compares passenger comfort between different methods of autonomous driving control, including PID control, IL baseline, and MBRL baseline algorithms. The PID controller is effective at short distances but struggles at longer distances, leading to corner-cutting. The IL baseline algorithm is designed to control the vehicle using behavior cloning. The study compares passenger comfort between different autonomous driving control methods, including PID control, IL baseline, and MBRL baseline algorithms. IL involves behavior cloning to predict actions based on waypoints. Two baselines were designed, one with a branched architecture predicting actions based on commands and another predicting setpoints for the PID controller. The latter method was effective for stable control on straightaways but struggled with corners, requiring implicit path planning. The study compares passenger comfort between different autonomous driving control methods, including PID control, IL baseline, and MBRL baseline algorithms. A model-predictive control baseline is proposed, using a forwards dynamics model to plan a reachability tree through free-space to the waypoint while avoiding obstacles. The baseline uses a breadth-first search over steering angles and constant throttle to navigate over 20 time steps. The study evaluates autonomous driving control methods, including PID control, IL baseline, and MBRL baseline algorithms. The planned trajectory efficiently reaches the waypoint and avoids obstacles by converting LIDAR images into obstacle maps. Performance results comparing methods against baselines are presented in TAB1, with lower numbers indicating better performance. Success rate is defined as the proportion of episodes where the vehicle reaches the goal location without collisions. Collision impulse is also measured. The study evaluates autonomous driving control methods, including PID control, IL baseline, and MBRL baseline algorithms. Collision impulse is the average cumulative collision intensities over episodes. Passenger comfort is measured by recording derivatives of the position vector with respect to time. Lower numbers in TAB1 indicate a smoother driving experience. Poor performance of the PID baseline suggests insufficient information about driving direction. Imitation learning achieves better comfort levels than MBRL. Our method outperforms imitation learning and model-based RL in terms of success, comfort, and lane-obeyance metrics. It excels in incorporating test-time costs, as shown in a pothole collision experiment. Our method excels in incorporating test-time costs, achieving nearly perfect pothole avoidance and avoiding collisions with the environment. It drove closer to the centerline and occasionally dipped into the opposite lane to avoid obstacles not observed during training. The imitative model demonstrated robustness to decoy waypoints in an experiment where half of the waypoints were highly perturbed versions of the other half. However, a failure mode occurred when decoy waypoints confused the planner at intersections. Success rates and planning rounds were reported in TAB3, showing the method's ability to execute planning efficiently. Our method demonstrated robustness to decoy waypoints and systemic bias in the route planner. Despite receiving waypoints on the wrong side of the road, our method remained effective at navigating, staying near the distribution of expert behavior. The combination of imitation learning and model-based reinforcement learning proved successful in fitting a probabilistic model to learn preferred behavior. Our algorithm learns preferred behavior from expert demonstrations and plans paths to achieve user-specified goals while maintaining high probability under this distribution. It addresses the distributional drift issue in Model-Based Reinforcement Learning (MBRL) by preferring plans close to expert data, ensuring safety without needing negative examples. The method provides a safe way to generalize to new goals through planning in imitation learning scenarios. Our algorithm offers a safe way to generalize to new goals through planning in imitation learning scenarios. It produces an explicit plan within the distribution of preferred behavior, providing interpretability and feasibility estimates. The method is broadly applicable in settings with expert demonstrations, demanding flexibility to new situations, and critical safety requirements."
}