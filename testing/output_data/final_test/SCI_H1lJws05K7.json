{
    "title": "H1lJws05K7",
    "content": "The weight initialization and activation function in deep neural networks significantly impact training performance. Incorrect choices can result in information loss during forward propagation and gradient vanishing/exploding during back-propagation. Understanding the theoretical properties of untrained random networks is crucial for successful training, with specific hyperparameters at the 'edge of chaos' showing good performance. The analysis shows that information propagates deeper for activation functions initialized at the edge of chaos. A class of activation functions, including Swish, improves information propagation over ReLU-like functions. Elfwing et al. (2017) and Ramachandran et al. (2017) provide a theoretical grounding for the empirical performance of $\\phi_{swish}$. Using random initialization on the edge of chaos benefits deep neural networks, which are popular for achieving state-of-the-art performance in language processing and computer vision. Previous research has shown the exponential expressive power of neural networks with increasing depth. In this paper, the authors analyze the behavior of random networks in the context of infinite-width and finite-variance i.i.d. weights. They show that initializing a network on the 'edge of chaos' leads to deeper information propagation. For ReLU-like activation functions, the exponential depth scale is replaced by a polynomial depth scale, allowing information to propagate deeper when initialized on the edge of chaos. The paper discusses the behavior of random networks when initialized on the edge of chaos, showing that deeper information propagation occurs. It highlights limitations of ReLU-like activation functions and introduces conditions for activation functions to allow good information flow. The Swish activation function is identified as meeting these conditions, with experimental evidence suggesting it outperforms ReLU. The study provides a theoretical basis for these findings. Our paper provides a theoretical grounding for the benefits of initializing random neural networks on the edge of chaos. We assume independent Gaussian processes with covariance kernels for the network propagation. The approximation of y l i (.) by a Gaussian process was first proposed by BID12 in the single layer case and extended to multiple layers by BID9 and BID10. The expressions of the limiting Gaussian process kernels are recalled, providing a recursion to calculate the kernel \u03ba l. The kernel \u03ba l can also be expressed in terms of the correlation c l ab in the l th layer. The variance q l a is updated through the layers by a recursive formula, q l a = F(q l\u22121 a), where F is the 'variance function'. The text discusses the limiting behavior of q and c as the network depth increases, assuming \u03c6 has a second derivative. It defines Domains of Convergence for an activation function \u03c6 and discusses the variance function F. The variance q converges to a fixed point q when input data is rescaled. The text discusses rescaling input data to converge variance q, with conditions on (\u03c3 b , \u03c3 w ) for convergence of \u03c6. For ReLU activation, outputs become constant functions as network depth increases. BID16 established convergence of q and c as |q l a \u2212q| \u223c e \u2212l/ q and |c l ab \u22121| \u223c e \u2212l/ c. The text discusses depth scales q and c, representing variance and correlation propagation limits. The edge of chaos separates ordered and chaotic phases based on correlation \u03c71. In chaotic regime, correlations converge to random value c < 1, leading to different outputs for close inputs. In the chaotic phase, neural network output is non-continuous due to close inputs leading to different outputs. The Edge of Chaos (EOC) is defined by values of (\u03c3 b , \u03c3 w ) satisfying a specific condition. The correlation function f simplifies the analysis of correlations, showing asymptotic behavior similar to a dynamical system. Activation functions with exponential growth are not considered, and being on the EOC is indicated by f(1) = 1. The phase transition at the EOC is analyzed in detail for a broad class of cases. In the chaotic phase, neural network output is non-continuous due to close inputs leading to different outputs. The Edge of Chaos (EOC) is defined by values of (\u03c3 b , \u03c3 w ) satisfying a specific condition. Activation functions with exponential growth are not considered, and being on the EOC is indicated by f(1) = 1. In the next section, the phase transition at the EOC is carefully analyzed for a large class of activation functions, showing that correlations still converge to 1 even in the EOC regime, albeit at a slower rate. This class of activation functions has the interesting property of maintaining unchanged variance on the EOC. The EOC is characterized by activation functions that preserve variance across layers. Even in the EOC regime, correlations converge to 1 at a slower rate. The analysis focuses on ReLU networks and SELU activation for variance normalization. Correlations converge to 1 at a polynomial rate of 1/l 2. Proposition 4 introduces conditions for activation functions to slow down correlation convergence to 1. It states that with certain activation functions, correlations can be tuned to approach the identity function, leading to slower convergence rates. The next lemma and proposition provide conditions for activation functions to slow down correlation convergence to 1. The lemma shows that removing a certain condition results in useless activation functions, while the proposition gives sufficient conditions for bounded activation functions to satisfy the requirements. These conditions are easy to verify and are satisfied by Tanh and Arctan. Swish activation function, satisfying conditions for good information flow and avoiding vanishing gradient issue, is a promising alternative to ReLU. It maintains a gradient close to 1 for large inputs, preventing correlations from converging to 1. The Swish activation function with \u03c3 b = 0.2 shows more output variability than ReLU. Other activation functions like ELU, SELU, and Softplus also satisfy conditions for good information flow. Empirical results are demonstrated on the MNIST dataset. The Swish activation function with different initialization parameters affects the learning speed of the network. Initialization away from chaos leads to poor test accuracy as depth increases. Understanding the optimization algorithm is crucial for improving performance. The Swish activation function affects learning speed, with Tanh performing better than ReLU for shallow networks due to the vanishing gradient problem. However, for deeper networks, Tanh gets stuck at low test accuracy because of small gradients. Initialization on the EOC improves information propagation across layers. Initialization on the EOC enhances information propagation in neural networks. Activation functions like Tanh, Swish, and ELU satisfy conditions for improved information flow. This has implications for Bayesian neural networks as well. Our results suggest that assigning i.i.d. Gaussian prior distributions to weights and biases in neural networks leads to a concentration on constant functions, even on the edge of chaos (EOC) for ReLU-like activation functions. To achieve richer priors, it is essential to select parameters (\u03c3 b , \u03c3 w ) on the EOC along with an activation function that satisfies specific conditions. The supplementary material contains proofs of the propositions mentioned, along with additional theoretical and experimental results. The convergence to the fixed point is discussed in Proposition 1, where the asymptotic behavior of the function is analyzed. The function F is a contraction mapping, ensuring a unique fixed point q. Convergence of covariances is guaranteed for all inputs a, b \u2208 R d. The sequence q l converges to a limit c \u2208 [0, 1]. The derivative of the function at the limit is provided. The function F is a contraction mapping with a unique fixed point q. Convergence of covariances is guaranteed for all inputs. The sequence q l converges to a limit c \u2208 [0, 1], with the derivative of the function at the limit provided. In an illustration, the variance for different inputs is plotted, showing faster convergence for Tanh compared to ReLU. Proposition 2 states that for a ReLU-like function with specific parameters, the function F(x) = x for x \u2265 0. The proof involves showing the variance recursion and the correlation function. By analyzing the function f, it is shown to be differentiable and a Taylor expansion near 1 is derived. The sequence q l converges to a limit c \u2208 [0, 1], with details on the convergence process provided. The proof involves showing the variance recursion and the correlation function for a ReLU-like function with specific parameters. The sequence q l converges to a limit c \u2208 [0, 1], with details on the convergence process provided. The function F with parameters is non-decreasing and has a minimal fixed point q. On the edge of chaos, the EOC equation \u03c3^2wE[\u03c6(\u221aqZ)^2] = 1 holds, and as \u03c3b approaches 0, q approaches 0. This leads to the conclusion that q + 1 and equation 6 are valid. Additionally, for \u03c3b > 0, if (\u03c3b, \u03c3w) \u2208 EOC, then liml\u2192\u221e cl a,b = 1 due to the strict convexity of f. The correlation converges to the unique fixed point of f, which is 1, slowing down the convergence. Proposition 5 states conditions for a bounded function \u03c6 to satisfy Proposition 4. The function \u03c6 is bounded and satisfies certain conditions. By using equations and calculus, it is shown that the function e is increasing near 0. The correlation function's derivatives are calculated using Gaussian integration by parts. The function \u03c6 is bounded and satisfies certain conditions. Using Gaussian integration by parts, it is shown that \u03c6 is increasing near 0. The proof involves showing that \u03c6 is positive almost everywhere, and verifying assumption (iii) of Proposition 4. The function \u03c6 is defined as xe^x / (1 + e^x) and h as e^x / (1 + e^x), with \u03c6(0) = 0. Numerical analysis confirms the satisfaction of certain conditions. The function \u03c6 is bounded and satisfies certain conditions, including being positive almost everywhere. Numerical analysis confirms the satisfaction of these conditions. The function g(x) = x\u03c6(x)\u03c6(x) is defined, and it is shown that F > 0. Graphs of E[\u03c6(U1)\u03c6(U2(x))] for different values of q are shown, with f having small values when q is large. On the edge of chaos, F(q) \u2248 1. On the edge of chaos, the function \u03c6 satisfies all conditions for Proposition 4 if it meets specific criteria. The activation function's behavior for large x determines if the conditions are met, with implications for the quality of results. The activation function Hard-Tanh (HT) is compared to Tanh based on a metric, with explicit calculations provided for HT. The correlation behavior is studied assuming variance converges to q, with conditions for Proposition 4 met if the function \u03c6 satisfies specific criteria. The EOC curve shows that the function is non-decreasing and convex, with a limit of zero as \u03c3b approaches zero. A comparative analysis in TAB2 shows that ReLU outperforms Swish for smaller width and depth, but Swish has an advantage for deeper architectures."
}