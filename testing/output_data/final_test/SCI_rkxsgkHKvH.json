{
    "title": "rkxsgkHKvH",
    "content": "Activation functions are crucial for deep neural networks' convergence and performance. While ReLU is popular, higher-order polynomial activation functions can approximate real-valued functions within a range. However, using polynomials can lead to unstable performance due to exploding gradients. To address this, dynamic input and output scaling, along with lower learning rates for polynomial weights, are introduced. Our proposed method controls fluctuations in polynomials with lower learning rates, matching prior activation function performance in experiments on public datasets. Deep learning methods have excelled in visual understanding, recognition, speech, and natural language processing tasks. CNNs, recurrent neural networks, wavenet, and transformers with attention mechanisms are key algorithms. Deeper architectures are crucial for both theoretical and empirical evidence. The importance of deeper architectures in neural networks is supported by both theoretical and empirical evidence. Activation functions like ReLUs, sigmoid, and hyperbolic tangent enable nonlinearity. Recent advancements include derivatives of ReLU, Leaky ReLUs with adjustable slopes, and Parametric ReLU with trainable parameters for improved performance. Swish is a new activation function that shows promise. In this work, the focus is on using polynomials as nonlinearity functions in deep networks. The stability of polynomial orders 2 to 9 is demonstrated, showing competitive results with state-of-the-art activation functions on various architectures. The method allows each layer to find its preferred nonlinearity during training, offering insights into network design. Our method allows each layer to find their preferred nonlinearity during training, showing learned nonlinearities that are both monotonic and non-monotonic. Activation functions like Sigmoid, hyperbolic tangent, Softplus, ReLU, and Leaky ReLUs are discussed for neural networks, addressing issues like vanishing gradients and gradient flow. Leaky ReLUs (LReLU) and Parametric ReLU (PReLU) are extensions of ReLU with different properties. LReLU allows a fraction of negative values to speed up learning, while PReLU has a learnable parameter for better performance. Exponential Linear Unit (ELU) shifts the mean towards zero for improved learning, and Gaussian Error Linear Unit (GeLU) is a non-monotonic function. Scaled Exponential Linear Unit (SELU) offers self-normalizing properties for robust training of deeper networks. Swish, a novel activation function discovered through automated search techniques, has shown superior performance compared to ReLU in various experiments. It introduces a non-monotonic bump controlled by a parameter \u03b2, which can be constant or trainable. This work is the first to search for activation functions, using polynomials with trainable coefficients for learning. We introduce polynomials with trainable coefficients to learn activation functions that approximate continuous nonlinearities. The goal is to understand a network's nonlinearity preference. Higher order polynomials can lead to exploding gradients, which can be mitigated by using sigmoid, but limitations arise with weight values. To address exploding gradients and weights in higher order polynomials, dynamic input scaling g(.) is introduced. This scaling helps normalize outputs and limits weights to explore prior activations. Polynomial activations are defined with trainable coefficients to approximate nonlinearities in neural networks. The proposal explores newer nonlinearities by allowing the network to choose appropriate activation functions for each layer. Initialization of polynomial weights is crucial for network stability and convergence. Network-1 with polynomial activation of order 2 is trained using stochastic gradient descent with mixed precision training. Various combinations of weight initializations are tested to optimize performance. The weights of convolution and linear layers are initialized using He et al. (2015a) for each run. Networks initialized with f n (x) \u2248 x and f n (x) \u2248 \u2212x consistently achieve a test error of \u2264 5%. Experiment extended to n = 3 with observed results. Weights closer to 0 never converged, so weights approximating an activation function, F (x), were used for initialization. The weights are initialized to approximate activation functions like ReLU, Swish, and TanH for an n-th order polynomial. The goal is to minimize the sum of squared residuals within the bounds -5 \u2264 x \u2264 5. This approach helps in choosing an appropriate nonlinearity for the activation function. The weights are initialized to approximate activation functions like ReLU, Swish, and TanH for an n-th order polynomial. Evaluating stability and performance under different optimization settings and batch sizes, Swish approximations are more accurate compared to ReLU and TanH. Training 512 configurations per initialization on MNIST data. After initializing weights with activation functions like ReLU, Swish, and TanH for n-th order polynomials, stability and performance were evaluated under various optimization settings and batch sizes. Swish approximations outperformed ReLU and TanH in 167 experiments. Adjusting the learning rate for polynomial weights led to convergence of all 512 configurations. The results suggest that Swish approximations as initializations are marginally better. The study evaluates polynomial activations on public datasets and compares them with state-of-art activation functions. Polynomial activations are initialized with specific parameters for different functions. Models are implemented in PyTorch and tested on the MNIST dataset without augmentation. The study evaluates polynomial activations on public datasets with 60,000 training and 10,000 test samples. Four different networks with varying depths and convolutional layers are considered to understand the behavior of the proposed activation function. Each network includes convolutional layers, dropout, linear layers, and softmax layers. Training is done for 60 epochs using stochastic gradient. The study evaluates polynomial activations on public datasets with varying network configurations. Training is done for 60 epochs using stochastic gradient descent. Polynomial activations with order 2 and 3 perform well, while orders greater than three are unnecessary. Figure 2 shows the learned polynomial activation functions for different networks. The study evaluates polynomial activations on public datasets with varying network configurations, finding that higher orders tend to avoid most information due to data simplicity. Polynomial activations and PReLUs perform better on CIFAR-10 and CIFAR-100 datasets, with training data augmented for experiments on ResNet-164, Wide ResNet 28-10, and DenseNet 100-12 models. The architecture and training settings are replicated from original papers, with ReLU replaced by polynomial activations. The study evaluates polynomial activations on public datasets with varying network configurations, finding that higher orders tend to avoid most information due to data simplicity. The architecture and training settings are replicated from original papers, with ReLU replaced by polynomial activations. Lower learning rates are applied to polynomial weights and the polynomial activation is disabled before the average pool. Training is done using SGD with specific parameters. Results show comparable test accuracy for CIFAR-10 and CIFAR-100, with the best performance seen in Wide ResNet. Non-monotonic activations are prevalent, indicating their importance. Initial layers allow less information compared to deeper layers due to common residual connections. The study introduces a polynomial activation function for stabilizing networks, with trainable coefficients and lower learning rates for polynomial weights. The resulting nonlinearities are both monotonic and non-monotonic. Superior performance is achieved on MNIST experiments compared to various other activation functions. In CIFAR experiments, replacing ReLUs with polynomial activations in DenseNet, Residual Networks, and Wide Residual Networks showed comparable performance to eight state-of-the-art activation functions. The method is computationally expensive but can potentially improve accuracies by using simpler activations like ReLU for initial layers followed by polynomial activations."
}