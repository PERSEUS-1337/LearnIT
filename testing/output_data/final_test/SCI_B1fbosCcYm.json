{
    "title": "B1fbosCcYm",
    "content": "The ability to look multiple times through pose-adjusted glimpses is crucial for human vision, allowing us to understand complex visual scenes. Short term memory helps aggregate information from these glimpses. A new biologically inspired visual working memory architecture called Hebb-Rosenblatt memory is introduced, along with a Short Term Attentive Working Memory model (STAWM) that uses transformational attention to learn memory over each image. This model can provide goal-oriented latent representations for tasks like classification. Our model, STAWM, achieves competitive classification performance on MNIST and CIFAR-10 datasets. It also demonstrates effective visual reconstruction on the CelebA dataset by learning parts-based representations. The self-supervised representation from MNIST aligns with state-of-the-art models without using a visual attention mechanism. By training under dual constraints of classification and reconstruction, STAWM provides an interpretable visual sketchpad, offering insights into deep learning beyond statistical pattern recognition. In contrast, inspired by the human visual system, visual attention models utilize a glimpsing mechanism to filter model input and select portions of the image for processing. These models address key challenges in separating visual components and integrating pattern and texture with pose in a higher order model built in visual working memory. Various models have been proposed to address challenges in separating pose and object notions from visual features and modeling long-range dependencies. Examples include transformational attention models, Transformational Autoencoders, Capsule Networks, and the ubiquitous Long Short-Term Memory (LSTM) network. Short-term memories and the fast weights model have also been studied to improve the ability of Recurrent Neural Networks (RNNs) to learn long-range dependencies. Memory is crucial for enhancing deep networks' ability to attend to visual scenes. Synaptic plasticity, the basis of memory in neuroscience, explains how connections change with experience. Immediate, short, and long-term memories play a role in high-level cognition. Vision exemplifies this, with each eye movement triggering short-term changes that accumulate over time. Fast weights models enable recurrent networks to attend to recent past experiences. Incorporating visual working memory into transformational attention models aims to explore the potential benefits beyond efficiency and address challenges in modeling psychophysical concepts in deep networks. The study demonstrates classification performance on MNIST dataset and discusses the integration of Hebbian theory and differentiable plasticity in learning plastic networks. The study shows competitive classification performance on MNIST and CIFAR-10, superior to previous attention models, by incorporating a working memory. It demonstrates unsupervised learning of memory representation through image painting, similar to DRAW network. The model achieves competitive classification on MNIST with self-supervised features and learns a disentangled space on CelebA images. It can perform multiple tasks in parallel and utilize a visual sketchpad for interpretable classifiers. In this section, the discussion revolves around visual attention in deep models, distinguishing between hard attention and soft attention. Early hard attention models like RAM and DRAM used the REINFORCE algorithm for learning attention mechanisms. The terms motivation and inspiration are used interchangeably to highlight the influence of atypical factors on decision-making. Recent attention models like Spatial Transformer Networks (STNs), Recurrent STNs, and Enriched DRAM (EDRAM) use soft attention and are trained end-to-end with backpropagation. These models employ a two-layer LSTM to learn the attention policy, with the first layer aggregating information from glimpses and the second layer deciding where to look next. The DRAW network is a fully differentiable spatial attention model that updates a canvas to draw the input image. The canvas in the DRAW network acts as a working memory and output of the network. Inspired by Rosenblatt's perceptron, the Hebb-Rosenblatt memory architecture learns a Hebbian style memory representation. The Two Streams Hypothesis separates pose and feature information into 'what' and 'where' pathways in human vision. The DRAM model uses a multiplicative interaction between pose and feature information to emulate this separation. The concept of working memory is explored in deep architectures, drawing inspiration from the Baddeley model. Different types of memory, including immediate, short term, and long term memory, are compared to weights learned in neural networks. The activation in the hidden state of a recurrent network is likened to immediate memory. The missing component in current deep architectures is identified as working memory. In this study, the Baddeley model of working memory is used as inspiration to augment models of visual attention with a working memory component. The approach involves integrating 'what' and 'where' information to create and revisit mental images relevant to the task at hand. The 'plastic' Hebb-Rosenblatt network and STAWM model of visual attention are detailed to build up memory representations for each image, allowing for the projection of multiple query vectors through the memory for goal-oriented outcomes. In this study, the Baddeley model of working memory is used to augment models of visual attention with a working memory component. The approach involves integrating 'what' and 'where' information to create and revisit mental images relevant to the task at hand. A differentiable learning rule inspired by models of plasticity is derived to update memory space weights in a deep network during the forward pass. The rule is based on the Hebbian notion of synaptic plasticity, where synaptic weights increase if neurons activate for the same input and decrease otherwise. The spike train integral in a given window can approximate information encoded by timing of activations. A novel short-term memory unit can be integrated with networks trained with backpropagation. A two-layer network with input and output layers will activate based on an activation function of the projection. Synaptic weight increase is proportional to the product of input and output neuron activations. The increment expression in the memory model involves reducing weights matrix with decay rate \u03b4 and applying increment with learning rate \u03b7. Initialising weights to zero results in no learning, while a Gaussian initialisation may implant false memories. A solution from early multi layer perceptron models involves transmitting a fixed amount of input to a second layer counterpart. The Hebb-Rosenblatt rule, referred to as the STAWM model, is a biologically inspired learning rule that develops high values for consistently active features. It allows for the extraction of sub-images from input to iteratively learn a memory representation from a single image. This attention model is based on the Deep Recurrent Attention Model (DRAM) and uses components from Spatial. The STAWM model, based on DRAM, utilizes components from STNs and EDRAM to define an attention policy over input images. It employs a two-layer RNN to parameterize glimpses with an affine matrix, sampled from the RNN output, to construct a flow field for obtaining fixed-size glimpses. The STAWM model utilizes components from STNs and EDRAM to define an attention policy over input images. It employs a two-layer RNN to parameterize glimpses with an affine matrix, sampled from the RNN output, to construct a flow field for obtaining fixed-size glimpses. The glimpse features are combined with location features to update memory, and the context and glimpse CNNs are used to extract features from the image and glimpses. The context CNN establishes contextual information, while the glimpse CNN extracts features. The aggregator and emission RNNs formulate the glimpse policy over the input image. The aggregator and emission RNNs, depicted in Figure 2, create the glimpse policy over the input image. LSTM units are used for both networks due to their stable learning dynamics, with the same hidden size. These networks can be implemented as a two-layer LSTM. Two fully connected layers are used to reduce the output to six dimensions for each glimpse. The last layer has weights initialized to zero and biases initialized to the affine. The memory network in STAWM has weights initialized to zero and biases initialized to the affine identity matrix. It consists of a 'what, where' pathway, a Hebb-Rosenblatt memory, and three states: update, intermediate, and terminal. The memory is updated N times per image, once for each glimpse. The memory network in STAWM has weights initialized to zero and biases initialized to the affine identity matrix. It consists of a 'what, where' pathway, a Hebb-Rosenblatt memory, and three states: update, intermediate, and terminal. The update will happen N times per image, once for each glimpse, with different learning rates (\u03b4, \u03b7, and \u03b8) as hyper-parameters. The stability of the memory model depends on the choices of learning rate, and necessary initial conditions are derived in Appendix A. No update is made to the memory for signals projected through in the intermediate and terminal states. These states allow observation of the memory at different points during the glimpse sequence or after all N glimpses have been made. The intermediate or terminal states can be used to observe the latent space of the model conditioned on a query vector. The architecture of STAWM includes a memory network with different query vectors projected through the memory to obtain latent representations. The weights can be fixed for a self-supervised setting to prevent changes by the optimizer. Linear layers are used to project the output from the context CNN into different queries for the network's aims. Gradient of the image context is detached to prevent divergent updates. The STAWM architecture includes a memory network with query vectors projected through to obtain latent representations for classification and drawing tasks. The glimpse sub-spaces are constrained in a variational setting. For classification, the query vector is projected through the memory to derive a latent vector, which is then passed through a classification layer. The model is trained using backpropagation to minimize categorical cross entropy. To learn drawing, a novel approach is used to construct a visual sketchpad. Our approach for learning to draw involves using a memory independent from the canvas, allowing for more than just reconstruction contents. The drawing model utilizes intermediate memory states to update the canvas after each glimpse using a transpose convolutional network. The emission network further refines features observed under the glimpse transform. The emission network refines features observed under the glimpse transform by outputting parameters of an inverse transform. The sketch is warped before being added to the canvas using either the addition method or the Bernoulli method. The addition method involves adding updates to the canvas matrix and applying a sigmoid function to obtain pixel values. The final canvas expression is obtained after all glimpses, with the canvas set to black when no additions are made. Overlapping sketches may be needed for complex reconstructions. The Bernoulli method can help prevent issues with overlapping sketches on the canvas. To achieve a realistic painting effect, each brush stroke should replace what was underneath it. Adding an alpha channel to the output of the transpose CNN allows for masking out areas on the canvas before additions are made, ensuring precise replacement effects. The Gumbel-Softmax distribution is used to sample probabilities for a Bernoulli distribution in a differentiable manner. The canvas is constructed iteratively using elementwise multiplication. This approach helps prevent overlapping sketches on the canvas and ensures precise replacement effects. The variational approach in reconstructive models models the latent space as a distribution, employing a multivariate Gaussian with variance and mean. The model uses glimpse sub-spaces instead of a single latent space, concatenating them for the addition method and deriving a more appropriate objective for the Bernoulli method. In the STAWM model, outputs from the decoder are conditioned on the joint distribution of glimpse sub-spaces. The memory uses a ReLU6 activation function and memory learning rates are initialised to specific values. The code is implemented in PyTorch with torchbearer and can be found on GitHub. Examples in figures are not cherry-picked. The STAWM model achieves superior classification performance on MNIST compared to the RAM model, with the best model obtaining a test error of 0.31%. This suggests that visual attention enables the model to learn a more powerful representation of the image. The study experimented with classification on CIFAR-10 using different glimpse CNN models, with MobileNetV2 achieving a single run accuracy of 93.05%. For MNIST, three valid ways to draw using the addition method were observed, including compressing the image into a square equal to the glimpse size and learning to trace lines to contain object information in pose features. The model learned a pose invariant, parts-based representation with significant control over behavior based on glimpse size. Using N = 12 with S g = 6 achieved an appropriate balance, leading to a repeatable parts-based representation and implicit notion of class. Experimentation with painting images in CIFAR-10 was also conducted. Our model significantly outperforms the baseline with a terminal mean squared error of 0.0083 \u00b10.0006 vs 0.0113 \u00b10.0001 for the VAE. STAWM has learned to scan the image vertically and produce a series of slices, but has not achieved the desired parts-based representation. Different glimpse sizes were experimented with, but the desired behavior was not induced. The STAWM model learned to scan images vertically and produce slices, but did not achieve the desired parts-based representation. Overlapping sketches in the output will scale away from the target, unlike in MNIST where values only saturate the sigmoid. The model can be improved by fixing weights learned from drawing settings and adding a classification head to classify using self-supervised features. This allows the model to emphasize previous glimpses for better classification performance. The self-supervised performance on CIFAR-10 is competitive with state of the art models, showing improvement over baseline VAE. The drawing model uses the Bernoulli method to construct the canvas and can generate unsupervised segmentation masks. The model trained on CelebA dataset and adds KL divergence for joint distribution of glimpse spaces with a Gaussian prior. The STAWM model can separate salient face regions from backgrounds in an unsupervised setting. It learns to sketch increasingly complex areas with each new glimpse and can project different views on the latent space. The model can use both the drawing and classification networks together by summing their losses. The model combines the drawing and classification networks to minimize losses and avoid overfitting. The terminal classification error is 1.0% with S g = 8. The drawings reflect the model's predictions, sometimes closer to the predicted class than the target. Memories constructed as layer weights show promise for further research. The paper describes a novel short term attentive working memory model (STAWM) with impressive results on various tasks, including image reconstruction and foreground-background disentanglement. The model, augmented with a visual sketchpad, can provide interpretable descriptions of images. It also suggests the potential use of similar systems in future technologies to understand decision-making processes. The paper introduces a short term attentive working memory model (STAWM) with successful results in tasks like image reconstruction and foreground-background disentanglement. Future work will focus on exploring the model's potential for higher resolution images, understanding the dynamics of the Hebb-Rosenblatt memory, fusion of features from multi-modal inputs, and investigating the relationship between visual memories and saliency. Additionally, efforts will be made to address stability issues in the memory model. The paper discusses stability issues in the memory model, specifically the potential for gradient explosion causing damaging updates that halt learning. It explores properties of the learning rule and conditions for stable dynamics, using terms like stimuli, response, and projection through weights matrix. The paper discusses stability issues in the memory model, focusing on the change in weight of connections and the learning rule. It defines the response of neurons to stimuli and the latent representation derived from memory during the glimpse sequence. The sequence of stimuli is not bounded, but rather samples from affine transforms over the image. Equation 12 generalizes the change in weighted components for query stimuli over time steps. Equation 12 generalizes the change in weighted components for query stimuli over time steps, leading to Equation 13. The gradient of Equation 13 is derived by dividing by N \u2206t in the limit of \u2206t \u2192 0 (Equation 14). Stability conditions include a constant input to the memory network and nonnegative activation functions with an upper bound. The proof can be extended to incorporate recurrent networks, but that is beyond the scope of this paper. The section discusses the derivation of the joint KL divergence term for a glimpse sequence, focusing on the KL divergence between the posterior and prior distributions of the glimpses. The derivation is adapted from a previous work and involves a sequence of glimpse sub-spaces with K components. The joint KL term is simplified under certain assumptions of conditional independence. The joint KL term is derived for a glimpse sequence, focusing on the KL divergence between posterior and prior distributions. The prior is set as an isotropic unit Gaussian, leading to a simplified KL term. Different drawing modes are explored in experiments, showing compression and decompression processes for line drawings and images. The model can compress and decompress images efficiently, transitioning smoothly between reconstructing low and high-information regions. The CelebA dataset is used with the Bernoulli method for updating the canvas, resulting in a clear separation of background and foreground in the sketch sequence."
}