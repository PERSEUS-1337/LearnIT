{
    "title": "r1lYRjC9F7",
    "content": "Generating musical audio directly with neural networks is challenging due to the need to model structure at various timescales. By using notes as an intermediate representation, a new dataset called MAESTRO has been released, enabling models to transcribe, compose, and synthesize audio with coherent musical structure across different timescales. This advancement shows promise in creating expressive music. The dataset MAESTRO enables neural models to generate realistic music by capturing structure at different timescales, overcoming the challenges of modeling music dependencies. The WaveNet model by BID18 is a breakthrough in generating musical audio directly with a neural network. While it can create realistic piano audio at short time scales, it lacks longer-term structure. BID4 made progress in providing this structure by implicitly modeling the discrete musical events. The WaveNet model by BID18 generates realistic piano audio at short time scales but lacks longer-term structure. BID4 improved this by training a hierarchy of VQ-VAE models to capture long-term structure in piano audio, although the resulting sound has artifacts not present in real musical audio. The model learns a version of discrete structure from audio but is not perfect. In this work, a Music Transformer language model generates piano performance MIDI autoregressively, while a piano transcription model encodes piano performance audio as MIDI. The goal is to factorize the problem based on the generative process of performer and instrument, using a generative model with a discrete latent code of musical notes. The model is split into three modules: an Encoder for audio to MIDI transcription, a Prior for generating new performances in MIDI format, and a Decoder for audio synthesis based on MIDI. This process, called Wave2Midi2Wave, overcomes the lack of large-scale annotated datasets for training. A piano performance dataset is curated and released alongside the work. The method produces high-quality audio and symbolic performances on a larger scale than previous benchmarks. Training models based on natural musician/instrument division offers advantages such as better modularity and interpretability. Using an explicit performance representation allows for modeling structure at larger time scales and leveraging prior work in music generation. Additionally, a state-of-the-art music transcription model is utilized for improved results. By combining a transcription model, language model, and MIDI-conditioned WaveNet model, the study presents a factorized approach to musical audio modeling. They introduce a new dataset of piano performance recordings and achieve state-of-the-art results on a piano transcription benchmark. The dataset was obtained from the International Piano-e-Competition. The dataset MAESTRO contains paired audio and MIDI recordings from International Piano-e-Competition events, with key details such as key strike velocities and sustain pedal positions. The audio and MIDI files are aligned with high accuracy and annotated with composer, title, and year of performance. The MusicNet dataset contains high-quality uncompressed audio recordings of classical music performances, with a proposed train/validation/test split configuration to avoid repetition of compositions. Repertoire includes composers from the 17th to early 20th century, and the dataset includes recordings of human performances with separately sourced scores. The MAPS dataset contains Disklavier recordings and synthesized audio from MIDI files, which are not as natural as live performances captured in the MAESTRO dataset. The MAPS dataset contains synthesized audio and recordings of individual notes and chords, obtained from MIDI files and Disklavier recordings. To align the audio and MIDI files from the International Piano-e-Competition, an automated process was developed to ensure precise matching by minimizing the distance between CQT frames of real audio and synthesized MIDI. The MAPS dataset contains synthesized audio and recordings of individual notes and chords from MIDI files and Disklavier recordings. An automated process was developed to align the audio and MIDI files by minimizing the distance between CQT frames. The dataset is split into train/validation/test sets with specific criteria to ensure variety and accuracy in the compositions used. The MAESTRO dataset allows for training a piano music transcription model that achieves a new state of the art. The model is based on Onsets and Frames with modifications such as adding an offset detection head and increasing the size of LSTM layers and convolutional layers. The model for piano music transcription was improved by increasing layer sizes and units in the fully connected layer, stopping gradient propagation, disabling weighted frame loss, and using audio augmentation during training. This led to higher performance with the larger dataset and improved robustness to differences in recording environment and piano qualities. The model for piano music transcription was enhanced to be more robust to recording environment and piano qualities. Training results are summarized in Table 4, showing transcription precision, recall, and F1 results on the MAPS configuration 2 test dataset. Training was conducted on the MAESTRO dataset with audio augmentation. The transcription model enables training language and synthesis models on a large set of unlabeled piano data, using a Transformer BID20 for the generative language model. The decoder portion of a Transformer BID20 with relative self-attention was used to generate music with longer-term coherence. Two models were trained on MIDI data from the MAESTRO dataset and MIDI transcriptions inferred by Onsets and Frames from audio in MAESTRO. The models were evaluated on their respective validation splits. Transposition and time compression/stretching data augmentation were employed during training. The Music Transformer models were trained on MAESTRO dataset and MAESTRO-T, with validation NLL scores of 1.84 and 1.72 respectively. Commercial systems typically use concatenative methods to synthesize piano audio from MIDI, while a less common approach involves simulating a physical model of the instrument. This method requires significant engineering effort and research. Samples of Music Transformer outputs can be found in the Online Supplement. WaveNet can synthesize realistic instrument sounds directly in the waveform domain, but struggles with capturing musical structure at longer timescales. By providing a MIDI sequence as conditioning information, the model can focus on local structure like instrument timbre and note interactions. Conditional WaveNets excel at generating realistic speech signals from textual data, suggesting potential for music audio synthesis from MIDI sequences. The WaveNet model has a larger receptive field than previous models, enhancing its capabilities. The study by Oord et al. introduced a model with a larger receptive field for music audio synthesis. They found that a context stack with 2 stacks of 6 layers each worked better for the task. The model produces 16-bit output using a mixture of logistics. The input is a \"piano roll\" representation signaling the onset of keys on the keyboard. Three models were initially trained: Unconditioned, Ground, and Transcribed. The study by Oord et al. introduced a model with a larger receptive field for music audio synthesis, using a context stack with 2 stacks of 6 layers each. The model produces 16-bit output using a mixture of logistics. Three models were initially trained: Unconditioned, Ground, and Transcribed. The resulting losses after 1M training steps for the Transcribed model were 3.72, 3.70, and 3.84. The WaveNet model recreated non-piano subtleties of the recording, indicating potential for capturing the sound of more dynamic instruments. The study introduced a model for music audio synthesis with a larger receptive field. A model conditioned on a one-hot year vector produced consistent timbres. To avoid sonic crashes, the first 2 seconds of model outputs were trimmed. A listening study was conducted to assess the perceived quality of the method. To assess the effects of transcription, language modeling, and synthesis on listeners' responses, users were presented with various 20-second clips generated by different models described in the study. The samples demonstrate the end-to-end ability of inferring MIDI labels from unlabeled piano performances. Participants were presented with 20-second clips generated by different models in the study, showcasing the end-to-end ability of inferring MIDI labels from unlabeled piano performances. 640 ratings were collected, with each source involved in 128 pair-wise comparisons. Statistical analysis revealed significant differences between the models in terms of realism. The study tested participant ratings on real recordings and samples from WaveNet models, showing no significant difference. The Wave2Midi2Wave system for piano music modeling was demonstrated using the MAESTRO dataset. Future work will involve using piano recordings to train new models for generating compositions. The study demonstrated the Wave2Midi2Wave system for piano music modeling using the MAESTRO dataset. Future work includes extending the approach to other instruments and improving transcription performance. The new dataset is available under a Creative Commons license. In this appendix, the MAESTRO dataset was aligned and segmented by synthesizing MIDI and defining an audio-based difference metric. The alignment process aimed to recognize if two performances are of the same score based on raw audio. The Constant-Q Transform was computed for original and synthesized audio using librosa BID10. The initial alignment stage used a hop length of 4096 samples. In the alignment stage, a hop length of 4096 samples was chosen for speed and accuracy. The microphone setup varied, affecting frequency response and amplitude levels. The Constant-Q Transform was limited to 48 buckets aligned with MIDI notes C2-B5. Amplitude levels were converted to dB scale with a hard cut-off at -80 dB. The audio decay rate was normalized in the CQT arrays. MIDI files from a Disklavier covered hours of material. During alignment, the audio files were compared to synthesized MIDI files using mean squared error as the difference metric. The process involved aligning audio with MIDI notes, skipping over non-musical segments such as instrument tuning and applause. After aligning audio with MIDI notes and skipping non-musical segments, manual adjustments were made to concatenate short audio files, cut off events beyond shift tolerance, and repair MIDI files with clock errors. Tuning process parameters were adjusted to address misaligned audio/MIDI pairs, resulting in final metric values within a close range for each competition year. After aligning audio with MIDI notes and making manual adjustments, the segmentation stage involved further dividing the aligned pairs into individual musical pieces. The segmentation algorithm maintained intervals of start-end time offsets to split the data into train, validation, and test sets disjoint on compositions. In cases where timing information was missing, audio/MIDI pairs were sliced at the longest silences between MIDI notes. If expected piece duration data was available, a search with backtracking was applied to segment the pairs. The segmentation algorithm maintained intervals of start-end time offsets and expected piece durations to split the data into train, validation, and test sets. It picked the next longest MIDI silence, split the interval and corresponding durations, and continued to the next longest silence. The algorithm preferred splits that divided the piece durations evenly and backtracked if no suitable splits were possible. The algorithm successfully segmented audio/MIDI pairs based on intervals with one expected piece duration. It also trimmed non-music events and added padding before making the final cut. Dynamic Time Warping was then applied to account for any jitter in the recordings. In Python, librosa is used to load audio and resample it to 22,050Hz mono signal. The MIDI is synthesized at the same sample rate using FluidSynth. The sample arrays are padded to the same length, and CQTs are extracted with a hop length of 64 for a resolution of \u223c3ms. A C++ DTW implementation is used with cosine distances calculated within a Sakoe-Chiba band radius of 2.5 seconds. Sampling 100k random pairs, the mean of their cosine distances is used as a penalty value. By using a Sakoe-Chiba band radius of 2.5 seconds, we limit the number of calculations needed for alignment, as the sequences are already mostly aligned from the previous pass. This allows us to apply small sequence warps to adjust for any jitter."
}