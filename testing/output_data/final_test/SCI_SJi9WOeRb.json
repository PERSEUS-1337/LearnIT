{
    "title": "SJi9WOeRb",
    "content": "Implicit models are widely used in machine learning for generating samples but not for evaluating probabilities. Examples include data simulators, generative adversarial networks (GANs), and approximate inference techniques. Most approaches rely on approximating distributions for optimization, leading to inaccurate updates. This paper introduces the Stein gradient estimator to directly estimate the score function of implicitly defined distributions, improving model accuracy. The proposed estimator demonstrates empirical examples in meta-learning and entropy regularised GANs for improved sample diversity. Modelling is crucial for technological innovations in artificial intelligence, with a focus on developing prescribed probabilistic models and implicit probabilistic models for direct sample generation in various scientific and engineering research fields. Implicit models, such as generative adversarial networks (GANs), have gained interest in machine learning for image and text generation. They offer flexibility in modeling complex data structures, serving as approximate posterior distributions for Bayesian inference. Unlike prescribed probabilistic models, implicit models require more severe approximations due to their highly expressive nature. Implicit probabilistic models require severe approximations due to the intractability of the model distribution. Existing approaches approximate the model distribution or optimization objective function to learn parameters. Recent research on GANs suggests that restricting the representational power of the discriminator stabilizes training. The optimiser visits a finite number of parameter space locations, potentially leading to over-fitting if not regularised. Gradient approximation can be more accurate, estimating sensitivity of the loss function in a local region. This paper explores approximating the derivative of the log density, known as the score function, for training implicit models. The paper introduces the Stein gradient estimator for implicit models, comparing it with other estimators for tasks like gradient-free MCMC and meta-learning. It focuses on learning probabilistic models for datasets using generative processes. The model p(x) involves sampling a latent variable z and defining x = f \u03b8 (z), where f is a differentiable function parametrized by \u03b8. Conditional implicit models extend this by incorporating a supervision signal y to define a conditional distribution p(x|y). Wild variational inference uses implicit proposal distributions to approximate an intractable exact posterior p(z|x), providing flexibility but facing challenges in density evaluation. The intractability of density evaluation poses optimization challenges for implicit models like GANs, which rely on minimizing divergence measures. Approximations are crucial for learning complex data structures. Wild variational inference utilizes GAN-based techniques to construct density ratio estimators and approximate KL-divergence terms in variational lower-bounds. DISPLAYFORM0 BID22 and BID29 utilize the additive structure of KL divergence to improve density ratio estimation accuracy. Gradient-based optimization in algorithms involving minimax optimization can be unstable, but recent research focuses on stabilizing GAN training. An accurate approximation of intractable gradients could eliminate the need for a discriminator in gradient-based optimization tasks like variational inference. The text discusses the importance of accurate gradient approximation in optimization tasks like variational inference. Various gradient approximation techniques are mentioned, with a focus on kernel-based methods such as kernel density estimation and score matching. The Stein gradient estimator is proposed as a novel generalization of the score matching gradient estimator. Notation and the random variable x are introduced before presenting the main contribution of the paper. The text introduces Stein's identity for gradient approximation in optimization tasks, focusing on differentiable multivariate test functions and boundary conditions. The use of distribution q(x) and Monte Carlo methods for implicit models is emphasized for deriving gradient approximations. The text introduces Stein's identity for gradient approximation in optimization tasks, focusing on differentiable multivariate test functions and boundary conditions. It proposes the Stein gradient estimator by inverting Stein's identity and approximating it with Monte Carlo methods. Ridge regression is used to estimate G with a regularizer, leading to a simplified calculation for the gradient term. The text discusses the derivation of the Stein gradient estimator from a divergence minimization perspective, introducing the concept of Stein discrepancy measure using a unit ball in a reproducing kernel Hilbert space induced by a kernel. The kernelized Stein discrepancy (KSD) is defined for C0-universal kernels satisfying boundary conditions. The text discusses the derivation of the Stein gradient estimator using the kernelized Stein discrepancy (KSD) as a discrepancy measure for C0-universal kernels. BID12 further characterizes the power of KSD in detecting non-convergence cases. If the kernel is twice differentiable, KSD can be computed using FORMULA17, equivalent to the V-statistic of KSD. Minimizing the U-statistic of KSD provides gradient approximations, with other gradient estimators like the denoising auto-encoder (DAE) also available. The text discusses gradient estimators for fitting implicit distributions, comparing the denoising auto-encoder (DAE) with kernel-based gradient estimators. The focus is on quick approximations, with a naive approach involving estimating the intractable density and differentiating through a KDE estimate to obtain the gradient estimator. The KDE gradient estimator can be rewritten for translation invariant kernels. The Stein gradient estimator uses the full kernel matrix as a pre-conditioner, while the KDE method computes an averaged \"kernel similarity.\" This difference is crucial for the superior performance of the Stein gradient estimator compared to the KDE gradient estimator. The Stein method incorporates kernel similarity between all samples, making it more sample efficient and accurate with the same number of samples. The KDE gradient estimator indirectly approximates the gradient via density. The KDE gradient estimator approximates the gradient via density estimation, which can be inaccurate. An alternative approach directly approximates the gradient by minimizing the expected error. The objective can be reformulated using integration by parts, leading to the score matching gradient estimator. The Stein gradient estimator is a generalization of the score matching estimator, with the comparison between the two estimators being more complex. The Fisher divergence is stronger than KSD in detecting convergence, but direct gradient estimation is challenging due to non-differentiable Dirac kernel. Parametric approximation is needed to optimize parameters, potentially removing the advantage of using a stronger divergence. BID38 and BID45 proposed a parametric solution by approximating the log density and minimizing to obtain coefficients. Stein gradient estimator is a generalization of score matching estimator. The proposed Stein gradient estimator FORMULA18 is non-parametric and optimizes functions directly at specified locations, offering advantages over the score matching gradient estimator. It eliminates approximation errors from restricted parametric approximations and has a simpler form applicable to any kernel satisfying boundary conditions. Both estimators have similar computation complexity, involving kernel matrix inversions. The paper discusses the advantages of the proposed Stein gradient estimator, which is non-parametric and optimizes functions directly at specified locations. It highlights the limitations of early-stopping for inner-loop DAE fitting and the need for predictive estimators to address issues with gradient function predictions at samples drawn from distributions other than q. The paper introduces two predictive estimators, one derived from a nonparametric estimator and the other minimizing KSD using parametric approximations. It discusses using the non-parametric estimator for gradient approximation and demonstrates the computation of the non-parametric Stein gradient estimator. The practical implementation involves storing computed gradients, kernel matrix inverses, and parameters for the predictive estimator. The paper introduces two predictive estimators: one non-parametric and the other minimizing KSD using parametric approximations. The non-parametric estimator is computationally demanding and requires O(K^2 + Kd) time complexity and O(Kd) memory. To address this, minimizing KSD using parametric approximations is suggested. The paper introduces a non-parametric estimator and a method to minimize KSD using parametric approximations, which is computationally cheaper. The linear coefficients can be calculated analytically, making prediction faster and more memory-efficient. The paper introduces a non-parametric estimator and a method to minimize KSD using parametric approximations. Detailed settings are provided in the appendix. Implementation can be found at https://github.com/YingzhenLi/SteinGrad. The accuracy of the proposed gradient estimator is demonstrated using a synthetic example with a banana-shaped object. The approximate Hamiltonian flow is constructed using the same operator as in Hamiltonian Monte Carlo (HMC), with approximate gradients replacing the exact score function. The focus is on testing the accuracy of the gradient estimators. The paper introduces a non-parametric estimator and a method to minimize KSD using parametric approximations. The gradient estimators are fitted on 200 training datapoints from the target density using a Stein gradient estimator. The resulting Hamiltonian flows are HMC-like, but struggle to explore extremes due to lack of training data-points. The study used large bandwidths to explore extremes with few training data-points. Acceptance rates were high for gradient estimators, and KSD estimates indicated good sample quality. Non-parametric Stein gradient estimator was sensitive to hyper-parameters. Further improvements in sample quality and chain mixing time could be achieved with careful selection of kernel and hyper-parameters. In this section, the focus is on learning an approximate posterior sampler for Bayesian neural networks that generalizes to new datasets and architectures. The task involves defining an implicit approximate posterior distribution using a stochastic normalizing flow. The text discusses using a stochastic normalizing flow to update the location and direction in Bayesian neural networks efficiently. It proposes using variational inference to learn the variational parameters and approximate the gradient of the entropy term. The text discusses using a stochastic normalizing flow to update the location and direction in Bayesian neural networks efficiently. It proposes using variational inference to learn the variational parameters and approximate the gradient of the entropy term. The results are reported using non-parametric Stein gradient estimators with RBF kernel for gradient estimation. Six binary classification datasets are used for testing, with a small neural network trained on one dataset and a larger network used for generalization to other datasets. The study split datasets into training and test subsets for evaluating sampler performance. SGLD showed best results in KSD metric, while the Stein approach performed equally well or slightly better in test-LL and error. The KDE method was slightly worse, and score matching estimator produced considerably worse results. Future work may explore using advanced recurrent neural networks like LSTM for improved performance. Adding an entropy regularizer to the GAN generator loss can address issues like mode collapse. The generator's loss function is modified with the regularizer, and Monte Carlo is used to estimate the gradient. Empirical investigation on the BEGAN method with MNIST dataset shows promising results. The study uses a convolutional generative network and auto-encoder with specific hyper-parameters. The Epanechnikov kernel is applied to pixel values within a unit interval. Entropy regularization improves diversity in generated images. Four metrics are used to evaluate the trained models. The study uses a convolutional generative network and auto-encoder with specific hyper-parameters, applying the Epanechnikov kernel to pixel values within a unit interval. For each trained model, nearest neighbors are computed using l1 distance, resulting in a probability vector p. Results show the Stein approach outperforms others in learning a better generative model faster and more stably. The KDE approach achieves the lowest average l1 distance to nearest neighbors, possibly due to memorizing training examples. A fully connected network achieves 98.16% text accuracy on MNIST, with an empirical estimate of the inception score on generated images. The Stein approach outperforms others in learning a better generative model faster and more stably, with high inception score indicating realistic and diverse generated images. Computation speed is similar for all methods, dominated by kernel computations. The proposed entropy regularization adds little computational burden and can be applied to various GAN frameworks. The Stein gradient estimator is presented as a novel generalization to score matching. The Stein gradient estimator is a novel generalization of the score matching gradient estimator, showing efficacy in various learning tasks like gradient-free MCMC, meta-learning, and unsupervised image generation. Future work will focus on theoretical comparisons and practical improvements for gradient estimators in high dimensions. In this section, the focus is on providing discussions and analytical solutions for the score matching estimator, specifically deriving the linear coefficient for the Epanechnikov kernel. It is noted that de-noising autoencoders can be used to approximate the score function, as demonstrated in previous studies. This approach has been applied to train implicit models for tasks like image super-resolution. In this paper, kernel machines are used to provide analytical solutions for the score matching estimator in variational inference, avoiding the need for double loop optimization. Score matching can also be applied to learn parameters of an unnormalized density, with connections to contrastive divergence, pseudo likelihood estimation, and denoising autoencoders. Generalizations of score matching methods are also discussed. The derivations for the RBF kernel case are presented, along with the optimal solution for the coefficients. The Epanechnikov kernel and score matching objective are defined, with the V-statistic of KSD discussed in the context of samples from q. The V-statistic can be computed using matrix notations, and the U-statistic of KSD removes terms indexed by j = l. For most translation invariant kernels, the optimal solution of \u011c by minimizing KSD U-statistic is achieved. The non-parametric Stein gradient estimator can be applied to unseen data sampled from the q distribution. The estimator using V-statistic is defined with matrix calculations. For translation invariant kernels, the equation can be simplified further. The optimal solution for the U-statistic case is derived similarly. A parametric approximation is defined, and the optimal solution is shown by minimizing a specific equation. The gradient of the RBF kernel is considered, and the j, l indices are summed in the process. The detailed experimental set-up is described in this section, using Adam optimizer with standard parameter settings. Bayesian neural networks with binary classification are reviewed as an example. A deep neural network is constructed to predict y = f \u03b8 (x) for this task. In this task, a deep neural network is constructed to predict y = f \u03b8 (x), with network weights treated as random variables in a Bayesian framework. A prior distribution is attached to the weights, and the likelihood function is defined. The Bayesian approach finds the posterior of the network weights for future predictions, using uncertainty information for new input x*. The exact posterior is intractable. The exact posterior for a new input x* is intractable, so approximate inference is used to fit an approximate posterior distribution q \u03c6 (\u03b8). The predictive distribution is approximated using Monte Carlo methods. The approximate posterior is fitted using a one hidden layer neural network with 20 hidden units to compute noise variance and moving direction for updates. The training process involves simulating an approximate sampler for 10 transitions and summing over variational lower-bounds computed on samples of each step. The objective is maximized with T = 100, using Monte Carlo and data sub-sampling strategies to approximate the entropy term contribution. The DISPLAYFORM6 strategies approximate the entropy term contribution using KDE estimates of p \u03b8 (x) and gradient estimators. Experimental setup includes a deconvolutional net for the generator and a convolutional autoencoder for the discriminator with specific layer configurations. The generator and decoder have symmetric architectures with different convolutions. ReLU activation is used except for the last layer, which uses sigmoid. The minibatch size is 100, with a learning rate decay of 0.9 every 10 epochs. Different values of \u03b3 and \u03b1 are used for different approaches. The results use KDE estimator for entropy estimates. The Stein approach using the proxy loss shows slightly worse performance than the KDE entropy estimator. However, the advantage of the proxy loss is its direct relation to the approximate gradient. Additionally, the Stein approach demonstrates more robust performance with varying \u03b3 and \u03b1 values compared to other methods."
}