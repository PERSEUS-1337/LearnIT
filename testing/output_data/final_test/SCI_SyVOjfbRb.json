{
    "title": "SyVOjfbRb",
    "content": "Stochastic Gradient Descent (SGD) is a popular optimization algorithm for large-scale problems. While weighted non-uniform sampling can lead to faster epoch wise convergence, the cost of maintaining this adaptive distribution often outweighs the benefits. This can result in a false impression of faster convergence in iterations but slower convergence in time, creating a \"chicken-and-egg loop.\" A new sampling scheme, inspired by Locality Sensitive Hashing (LSH), offers superior gradient estimation without significantly increasing per-iteration sampling costs. Locality Sensitive Hashing (LSH) has recently emerged as a superior method for fast estimation, reducing running time for gradient descent algorithms like SGD and AdaGrad. In machine learning, the goal is to minimize the average loss function over training data, with labels being either continuous or discrete. The function to minimize is typically convex. SGD BID1 samples an instance x j uniformly from N instances and performs gradient descent update using step size \u2318 t. The gradient rf (x j , \u2713 t 1 ) is evaluated on x j only, making it an unbiased estimator of the full gradient. Despite slower convergence rate than full gradient descent, SGD still converges to local minima efficiently. SGD samples an instance uniformly and performs gradient descent update using a step size. Despite slower convergence rates, SGD is preferred in large-scale settings due to the prohibitively slow calculation of the full gradient. Efforts are ongoing to improve SGD by finding better sampling strategies for gradient estimation. Adaptive sampling methods aim to reduce variance by using weighted distributions instead of uniform distributions. However, these methods face challenges in maintaining the adaptive distribution, leading to high computational costs. A new sampling scheme is proposed in this work, leveraging Locality Sensitive Hashing for efficient adaptive gradient estimation with O(1) cost per iteration, comparable to SGD. Utilizing recent advances in sampling and unbiased estimation with Locality Sensitive Hashing, non-uniform sampling involves weighting each x i to minimize variance. Sampling based on the L2 norm of the gradient was shown to optimize distribution, but computing the weights w i is costly. Methods like exploiting cluster structure aim to reduce overhead. The text discusses the importance of non-uniform sampling in speeding up stochastic gradient descent. Methods like leverage score sampling and importance sampling are proposed to sample training instances based on Lipschitz constants or squared Euclidean norms. Maintaining the non-uniform distribution for estimation requires O(N) operations. In this work, a novel LSH-based sampler called LSD (LSH Sampled Stochastic gradient Descent) is proposed to break the computational chicken-and-egg loop in non-uniform sampling for stochastic gradient descent. The algorithm uses hash lookups with O(1) cost to generate samples, resulting in lower variance gradient estimates compared to single sample SGD. The sampling complexity is constant, maintaining the efficiency of SGD sampling. The LSD sampler accelerates gradient-based optimization algorithms like AdaGrad BID7 by reducing sampling complexity to the level of SGD. This leads to a faster convergence in terms of epochs and running time compared to traditional SGD. The focus should be on computational efficiency rather than just epoch-wise convergence. Comparisons should consider wall clock time and floating point operations for accurate conclusions. There is a lack of empirical comparisons between SGD and adaptive schemes in terms of accuracy improvement. LSH BID9 is a sub-linear time algorithm for approximate nearest-neighbor search, using hash functions to group similar items together. It is a recent advancement in sampling and estimation theory, heavily utilized in the proposal. LSH BID9 is an algorithm for approximate nearest-neighbor search using hash functions to group similar items together. It constructs L independent hash tables with a meta-hash function formed by concatenating K random independent hash functions. When a query is made, one bucket is collected from each hash table, and the union of L buckets is returned. This reduces false positives by making buckets sparse and increases potential matches to decrease false negatives. The L buckets reduce false negatives by increasing potential buckets for valid nearest-neighbor items. The candidate generation algorithm has two phases: Pre-processing Phase involves constructing L hash tables from the data, storing pointers to vectors. Query Phase searches for nearest-neighbors by probing L different buckets. The nearest-neighbor is computed by comparing distances between items in the candidate set and the query. Items returned from a (K, L)-parameterized LSH algorithm are sampled with probability 1 (1 p K ) L. The (K, L)-parameterized LSH algorithm samples elements with probability 1 (1 p K ) L, where p is the collision probability. This sampling approach has been used for adaptive sparsification of deep networks and unbiased estimation of partition functions in log-linear models. Random sampling can be outperformed with similar computational cost using this method. Recent advances in maximum inner product search (MIPS) using asymmetric locality sensitive hashing have enabled sampling large inner products efficiently. By utilizing a (K, L)-parameterized LSH algorithm with MIPS hashing, a candidate set S can be obtained for a collection C of vectors and query vector Q. This approach allows for adaptive sampling for query Q with minimal hash lookups after a one-time linear cost of preprocessing C into hash tables. The sampling for query Q requires few hash lookups and can compute the probability of getting x. The sampling scheme is not a valid distribution, but can still be used for unbiased estimation. The LSD algorithm leverages efficient estimations using locality sensitive hashing, beating random sampling estimators with near-constant sampling cost. The analysis of the proposal involves least squares regression with a loss function. By following procedures in BID17, a generic unbiased estimator can be shown via adaptive sampling. The L2 norm of the gradient can be written as an absolute value of inner product, with optimal sampling weight determined by the L2 norm of the gradient. Sampling x in proportion to the optimal weight is recommended for normalized data, despite the expensive nature of this sampling process due to weight changes. The proposal involves least squares regression with a loss function and adaptive sampling. A sampling process is designed to sample from a weighted distribution that is a monotonic function of the optimal weight. Monotonic sampling is argued to be a good choice for gradient estimation as it adapts with iterations and maintains preferences between data points. The proposal involves adaptive sampling for least squares regression, where monotonic sampling maintains preferences between data points. By preprocessing hash tables and querying efficiently, adaptive sampling allows for gradient estimation with minimal cost. The text discusses the use of adaptive sampling for least squares regression, maintaining preferences between data points through monotonic sampling. By hashing and querying efficiently, gradient estimation is achieved with minimal cost. The process involves creating hash tables using a feature expansion transformation, LSH sampling, and unbiased estimation of the full gradient. The text discusses using adaptive sampling for least squares regression, focusing on efficient hashing methods like simhash and LSH function h. By centering data in the LSH hash table, simhash query efficiency is improved. The computational cost of SGD sampling is minimal, while LSD sampling involves hash computations and random number generation. The goal is to design an adaptive sampling procedure with a cost not significantly larger than d multiplications. The scheme works for any K, with K = 5 in experiments. Sparse random projections reduce hash computation cost to a constant \u2327 d multiplications. Fast hash computation is crucial. Near-neighbor search is costly and sub-linear in N. LSH is used for near-neighbor search but in this case, it is used as a sampler. Efficient unbiased estimation is the key difference that makes sampling practical while near-neighbor query prohibitive. Sampling works for any K and l 1, leading to only approximately 1.5 times the cost of SGD iteration. Our estimator of the gradient is proven to be unbiased. Our estimator of the gradient is unbiased with lower variance than SGD for most real datasets. The sampling probability of x i is proportional to the L 2 -norm of the gradient. The advantage of LSD estimator comes from sampling x i under a distribution monotonic to the optimal one. The variance can be minimized with this approach. The LSD estimator has a smaller covariance trace than SGD's estimator, especially when data is not uniformly distributed. By setting small values for K and l, LSD can achieve a much smaller variance than SGD. This approach minimizes variance by sampling x i under a distribution monotonic to the optimal one. The effectiveness of the algorithm is tested on three large regression datasets in musical chronometry, clinical computed tomography, and WiFi-signal localization. The YearPredictionMSD dataset contains 515,345 instances from the Million Song Dataset with a dimension of 90, split into training and testing sets to avoid bias. The data for the algorithm testing was retrieved from a set of 53,500 CT images from 74 patients, with 385 features. 42,800 instances were used for training and 10,700 for testing. LSD samples based on gradient norm, while SGD samples uniformly. LSD queries points with larger gradient than SGD. Variance of norm and cosine similarity reduces with more samples. UJIIndoorLoc covers three buildings of Universitat Jaume I with 4 or more floors and 110,000 m2. The UJIIndoorLoc dataset consists of indoor location information from three buildings at Universitat Jaume I. It includes WiFi fingerprints, coordinates, and other attributes. The dataset was split equally for training and testing, preprocessed, and used in experiments with gradient descent algorithms. LSD and SGD differed in their gradient estimators, with fixed values for parameters K and L. Hash function used was simhash, with a low value for l in experiments. The study focused on comparing the computational efficiency of different gradient descent algorithms using simhash as the hash function. The goal was to show that no other adaptive estimation method could outperform O(1) SGD estimates in terms of wall clock speedup. The experiment compared vanilla SGD with LSD, demonstrating the performance of pure LSD in convergence with running time. The experiment compared vanilla SGD with LSD on a linear regression task. LSD showed faster convergence in both training and testing loss compared to SGD. LSD also demonstrated faster time-wise convergence on every dataset. LSD algorithm is shown to have faster convergence in training and testing loss compared to SGD. In a follow-up experiment using AdaGrad, LSD samples data points with probability monotonic to L2 norm of the gradient. The comparison of sampled gradient norm between LSD and SGD is shown in FIG2. In experiments comparing LSD and SGD, LSD sampled points consistently had larger gradient norms across all datasets. A novel LSH-based sampler was proposed for gradient estimation, showing that LSD estimated gradients were more aligned with the true gradient direction than SGD. The variance of norm and cosine similarity decreased when averaging over more samples. In this paper, a novel LSH-based sampler was proposed for gradient estimation, reducing variance by sampling with probability proportional to gradient norms. The method, LSD, is computationally efficient like SGD but achieves faster convergence. The Trace of the covariance of LSD's estimator is smaller than that of SGD's estimator."
}