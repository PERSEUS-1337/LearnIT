{
    "title": "BJepX2A9tX",
    "content": "Performance of neural networks can be enhanced by encoding known invariance for specific tasks, such as rotation invariance in image classification tasks like cellular imaging. A new method called Conic Convolution and DFT Network (CFNet) was introduced to improve rotation invariance in convolutional neural networks. CFNet outperformed standard CNN and group-equivariant CNN (G-CNN) in various image classification tasks, showing better classification accuracy, computational efficiency, and robustness to hyperparameter selection. CFNet is a new scheme that aims to improve imaging analysis applications by encoding invariance to uniformative augmentations of the data, such as rotation. This approach can enhance the effectiveness of neural networks in various computer vision settings, including satellite and microscopy imagery. Several proposed methods for encoding rotation equivariance and invariance have shown promise, with convolution over groups being one of the most effective approaches. The proposed conic convolution scheme encodes rotation-equivariance in neural networks by convolving rotated filters over conic regions of the input feature map. This method is computationally efficient and outperforms group convolution in various applications. The integration of the magnitude response of the 2D-DFT into a transition layer between convolutional and fully-connected layers encodes rotational invariance in CNNs. This approach differs from other rotation-invariance methods by preserving valuable pose information between filters. The 2D-DFT integrates pose information between filters, improving features for subsequent layers. CFNet enhances classification accuracy over standard convolution and G-CNN, while 2D-DFT boosts performance across diverse datasets. Source code for CFNet will be available on GitHub. BID6 introduced G-CNNs with convolution over groups for neural networks, maintaining equivariance and enforcing invariance. Steerable filters BID24 improved upon this design, allowing finer rotation sampling without artifacts. Rotational equivariance can also be achieved by transforming the image domain to an alternative domain like the log-polar domain BID23 BID11. Our proposed CFNet encodes global rotation equivariance without introducing distortion, helping mitigate susceptibility to translation. Spatial transform and deformable convolutional layers allow learning non-regular sampling patterns and potential rotation invariance. The RotEqNet and BID8 methods aim to achieve rotation equivariance and invariance in CNNs by storing maximal responses across rotations. These methods show improved results and storage savings compared to previous approaches like G-CNN. The proposed conic convolution method further saves on storage by applying each filter at the appropriate rotation within each conic region. Most previous methods enforce rotation invariance through permutation-invariant or pooling operations over rotations. The BID3 and BID4 methods aim to achieve rotation invariance in CNNs, but convolutional layers may hinder performance by not maintaining rotation equivariance. Previous methods like BID23 proposed the 2D-DFT for rotational invariance, but integrating it into a rotation-equivariant CNN has not been done. The formulation starts with quadrant convolution, a simpler version of conic convolution. The quadrant convolution involves rotating the filter by \u03c0/2 depending on the quadrant. Same-equivariance is associated with rotations in the output feature map. The group G consists of two-dimensional rotation matrices of \u03c0/2, parameterized by g(r), acting on points in Z2. Quadrant convolution in CNNs ensures rotational equivariance by weighting the convolution for each rotation with a function that selects the appropriate quadrant of the domain. The origin is handled by averaging the filter response at all rotations, and boundary values are assigned consistently. The equivariance property in CNNs is established by the placement of the equality for either u or v. The rotation angle in conic convolution can be decreased by an arbitrary factor of \u03c0/2R, instead of being fixed to \u03c0/2, defining conic regions emanating from the origin. The weighting function is changed to have value one only over this conic region. The group G R is defined with parameterization for r \u2208 {0, 1, . . . , 4R \u2212 1} and x = (u, v) \u2208 R 2. Rotation equivariance for arbitrary values of R cannot be guaranteed due to subsampling artifacts when discretizing R 2 to Z 2. Filters need to be interpolated for rotations not a multiple of \u03c0/2, with nearest neighbor interpolation chosen in experiments to preserve filter energy. Conic convolution maintains equivariance to rotations of \u03c0/2. In convolutional networks, finer angles of rotation can improve performance, and using a finer discretization of rotations for early layers is advantageous. Downsampling can be applied while maintaining rotational equivariance for rotations of \u03c0/2, regardless of the choice of parameter R. After downsampling, the result is passed through a non-linear activation function like ReLU with an added offset. Conic convolution in neural networks involves convolving over quadrants, multiplying by a weighting function, summing responses, and concatenating them. This process incurs minimal additional computation compared to standard convolution and produces one feature map per filter. Conic convolution in neural networks involves convolving over quadrants, multiplying by a weighting function, and summing responses to produce one feature map per filter. Fully-connected layers in a CNN do not maintain rotation equivariance or invariance properties. In a fully-convolutional network, downsampling is applied until the final convolutional layer results in a vector with dimensions equal to the number of filters. In fully-convolutional networks, the final convolutional layer acts as a fully-connected layer with N filters applied to the feature map generated by the penultimate convolutional layer. The transition to circular shift enables the use of 2D-DFT for encoding invariance, capturing mutual information across responses. Feature maps are represented as tensors in order to apply the DFT, which is defined for finite-length signals. The final convolutional layer in fully-convolutional networks acts as a fully-connected layer with N filters. The transition to circular shift enables the use of 2D-DFT for encoding invariance, transforming circular shifts to an invariant space. The result is then passed into fully-connected layers before the final output layer. The 2D-DFT is integrated into rotation-equivariant networks like G-CNN, encoding rotations in contiguous stacks of feature maps. The rotated MNIST dataset is used as a benchmark for rotation invariance studies. Tuning parameters involves training various models before the final output layer. The best CFNet architecture consisted of six convolution layers with specific parameters and a DFT transition layer. Changes were made to the G-CNN architecture by reducing the number of filters for each layer. The G-CNN architecture was modified by reducing the number of filters for each layer and adding a 2D-DFT transition. This resulted in a 0.28% reduction in test error, showcasing the importance of incorporating rotational information between filters for invariance. CFNet, with conic convolution, further reduced error by 0.25% compared to H-Net 2. Synthetic images based on Gaussian-mixture models were used to control rotational invariance manifestation. Gaussian-mixture models were used to generate synthetic images for training and testing. Each class had 50 distribution patterns with 10 Gaussians per class. Training included data augmentation with rotations and random jitter. Classification accuracies were compared for different numbers of training samples. The study compared the performance of different neural network models, including CFNet, G-CNN, and CNN. CFNet consistently outperformed G-CNN and CNN in terms of accuracy and speed of achieving accuracy. Even with suboptimal hyperparameters, CFNet performed better than the other models. Additionally, a network without DFT, called CNet, was included for comparison. The study compared different neural network models, including CFNet, G-CNN, and CNN. CFNet outperformed G-CNN and CNN in accuracy and speed. CNet, without the DFT, showed comparable performance to CNN with fewer parameters. Integration of DFT with conic convolution showed global rotation invariance. Real biomarker images of budding yeast cells were analyzed, with different cellular subcompartmental regions identified based on protein expression. The study compared different neural network models, including CFNet, G-CNN, and CNN, with CFNet outperforming the others in accuracy and speed. Modifications were made to the architecture to enhance performance, with CFNet showing superior results in protein localization classification. The study compared neural network models like CFNet, G-CNN, and CNN, with CFNet achieving higher accuracy and speed in protein localization classification. CFNet's performance was enhanced through architecture modifications, resulting in superior results. The 2D-DFT and proposed convolutional layers in CFNet showed gains in real-world microscopy data, outperforming non-deep learning algorithms. The transform T g distributes over multiplication and convolution, making the operation \u03a6 same-equivariant for rotation. Each class k is described by parameters \u039b of the GMM, with \u00b5 g \u2208 [\u22121, 1] 2. The generating distribution for sample images involves setting a background intensity, determining rotation with a random angle, drawing means for each Gaussian class, and generating points from Gaussians with small jitter. The image generation process involves drawing points from a Gaussian distribution, adjusting intensity values, smoothing with a Gaussian kernel, adding pixel noise, and simulating camera jitter with random offsets."
}