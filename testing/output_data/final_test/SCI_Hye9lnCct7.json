{
    "title": "Hye9lnCct7",
    "content": "Representation learning is crucial in various machine learning fields, including reinforcement learning. Prior work has focused on generative approaches to capture all underlying factors of variation in observation space. This paper aims to learn functionally salient representations that prioritize factors important for decision making, rather than capturing all variations. These representations are aware of the environment dynamics and only include elements necessary for decision making. The paper focuses on learning representations that prioritize factors essential for decision making in reinforcement learning. These representations improve exploration for sparse reward problems, enable long horizon hierarchical reinforcement learning, and serve as state representations for downstream tasks. The method is evaluated in simulated environments and compared to prior approaches in representation learning and exploration. The paper discusses learning functionally salient representations in reinforcement learning to improve decision-making. It aims to capture relevant factors for decision making rather than all factors of variation in the observation space. The focus is on learning controllers quickly for challenging tasks using unlabeled data effectively. The paper proposes learning a representation that understands the environment dynamics through a goal-conditioned policy. This policy helps in executing shortest paths between states and can be used for complex tasks. Extracting critical factors from state observations is essential for decision-making in reinforcement learning. Comparing actions taken for different goal states can help in identifying these factors. The paper proposes actionable representations for control (ARC) based on the principle that different goal states require different actions. These representations emphasize factors in the state that induce significant differences in actions, de-emphasizing irrelevant features. Functional differences between goal states are illustrated in Figure 1 with houses A, B, C reachable by different roads, highlighting the need for representations zA, zB, zC to differentiate between them. Representations z A, z B, z C are learned to differentiate between goal states A, B, and C while keeping B and C similar. Learning a goal-conditioned policy to extract such representations can be achieved through unsupervised exploration of the environment. Representations from goal-conditioned policies can improve learning of complex tasks beyond simple goal reaching. Learning dynamics-aware representations requires experiencing transitions and interactions, not just observing valid states. Learning goal-conditioned policies can lead to actionable representations that can be recursively improved. These representations are beneficial for various tasks in reinforcement learning, such as task-specific policies, hierarchical RL, and reward function shaping. They outperform representations from unsupervised generative models and predictive models. The performance of these actionable representations is analyzed in simulated robotic domains like wheeled locomotion, legged locomotion, and robotic manipulation. In RL, the goal is to learn a policy that maximizes the expected return. Training a policy conditioned on a goal allows for accomplishing a variety of tasks. Maximum entropy RL algorithms aim to maximize both reward and policy entropy. In maximum entropy RL, the solution is a stochastic policy that reflects the sensitivity of rewards to actions. The actionable distance D Act is defined by the divergence between goal-conditioned action distributions for states, used to learn an actionable representation for control (ARC). This representation distinguishes states based on the actions needed to reach them. To capture important state elements for decision making, actionable distances (D Act) between states are defined in ARC. These distances reflect differences in actions required to reach states, allowing for the extraction of a feature representation (\u03c6(s)) that is crucial for decision making. This is built upon the framework of goal-conditioned RL, where a maximum entropy policy is trained to start at any state and reach goals. The policy \u03c0 \u03b8 (a|s, g) can start at any state s 0 and reach a goal state s g. By analyzing how different goal states affect action distributions, actionable distances D Act can be determined. These distances indicate functional similarities or differences between states based on action distributions. The actionable distance D Act captures functional differences between states based on action distributions. To learn a representation \u03c6(s) that corresponds to actionable distances, optimization is done to emphasize functionally relevant elements of state. This yields good state representations that significantly affect actionable distance. The objective is to optimize representations of state by emphasizing functionally relevant elements, ensuring meaningful Euclidean distances. Leveraging a goal-conditioned policy captures local environment connectivity and dynamics. The representation is optimized to include only relevant state elements. Goal Conditioned Policy can be trained with existing RL methods, obtaining sparse task-agnostic rewards. RL methods (TRPO) can obtain a policy using a sparse task-agnostic reward, which is not difficult. Acquiring a functionality-aware state representation requires active environment interaction. ARCs help solve tasks beyond what a simple goal-conditioned policy can achieve. Goal-conditioned policies have limitations in generalizing to new states and are limited to solving goal-reaching tasks. In our empirical evaluation, we demonstrate that ARCs go beyond the limitations of goal-conditioned policies by generalizing to new tasks and regions of the environment. ARCs can help solve tasks that go beyond simple goal-reaching, such as tasks involving larger state space regions and temporally extended tasks with sequences of goals. Goal-conditioned policies are restricted to goal-reaching rewards and are unaware of other reward structures in the environment, making them unsuitable for tasks like navigation under non-uniform preferences or manipulation with motion quality costs. Using the ARC representation as input for a policy or value function can simplify learning tasks by prioritizing important parts of the state. ARC helps construct better-shaped reward functions by accounting for reachability, unlike Euclidean distance which may not be a meaningful metric in state space. This representation enables quicker learning for tasks involving navigation under non-uniform preferences or manipulation with motion quality costs. Using the ARC representation for reward shaping allows for better generalization and learning policies in the presence of sparse rewards. Compared to goal-conditioned policies, ARC exhibits superior generalization capabilities and can effectively shape rewards for challenging goal-reaching tasks. One approach to solving complex tasks in hierarchical reinforcement learning involves learning a high-level controller that generates goal states for a goal-conditioned policy to reach sequentially. This method addresses the challenge of disentangling relevant attributes in long-horizon reasoning for effective goal-reaching. In hierarchical reinforcement learning, two schemes are considered for using ARCs - learning a high-level policy in ARC space or a clustered latent space. The high-level controller generates a distribution over points in the latent space, which is then translated into a goal for commanding the policy. This approach eliminates the need to rediscover saliency in the goal space. Using ARC as a hierarchical goal space improves waypoint navigation tasks by utilizing clusters in ARC space for faster learning with a high-level controller generating distributions over discrete clusters. Using ARC as a hierarchical goal space improves waypoint navigation tasks by utilizing clusters in ARC space for faster learning with a high-level controller generating distributions over discrete clusters. A meta-policy is trained to output clusters instead of states, showing that performing RL in \"cluster space\" induced by ARC outperforms other representations due to the meaningful distance metric in ARC space. Deep neural network models have the advantage of learning effective representations implicitly or explicitly through various approaches like generative modeling. Generative modeling is a classic approach to representation learning, where latent variables are trained to model data distribution. While generative models are general and principled, they must also generate the input observation. Some methods perform representation learning without generation, often using contrastive losses. In contrast, ARCs are directly trained to focus on decision-relevant features of input, providing a broadly applicable objective that is selective about which aspects to consider. Representation learning methods in the context of RL and control have been used for various downstream applications, such as value functions and building models. Unlike other approaches, our method focuses on encoding only the features relevant for choosing actions, rather than all physically-relevant features of the state. Our approach also differs in that it determines which features are important for choosing controls, instead of identifying independently controllable features. The text discusses the relevance of features for choosing controls in representation learning methods for reinforcement learning. It compares different approaches, such as learning representations through goal-directed behavior and supervision through demonstrations. The aim is to study the properties of learned representations and their use in quickly learning policies on new tasks. The study explores the use of reward shaping with ARCs for faster learning and hierarchical RL in various simulated environments, including 2D navigation tasks, wheeled and legged locomotion tasks, and object pushing with a robotic gripper. The state spaces for each task are detailed, such as joint angles and center of mass for legged locomotion, and endeffector and object positions for manipulation tasks. The study focuses on representation learning challenges in various simulated environments, including 2D navigation, locomotion tasks, and object manipulation. The state representation must account for the structure imposed by walls and the saliency of different robot joints. A key challenge is distinguishing between pushing an object and moving the arm in free space. The study explores representation learning challenges in simulated environments such as 2D navigation and object manipulation. A stochastic goal-conditioned policy is trained using sparse rewards and Trust Region Policy Optimization. A dataset of 500 trajectories is collected to train the ARC representation through supervised learning. In Appendix A.2, the training procedure for ARC representation learning is outlined, including hyperparameters and architecture choices. Comparison is made with other representation learning methods like VAE, feature slowness, predictive model, inverse models, and a baseline using the full state space. Different approaches for downstream tasks are also compared, such as VIME for reward shaping and option critic for hierarchical RL. For hierarchical RL, comparisons were made with option critic BID20 and an on-policy adaptation of HIRO BID27, as well as model-based reinforcement learning with MPC BID28. To ensure a fair comparison, the same trajectory data was provided to all representation learning methods, with each representation trained on the same dataset of trajectories. The analysis focused on the structure of ARC space for specific tasks to identify emphasized factors and the impact of system dynamics. In 2D navigation tasks, ARC reflects system dynamics by showing points close in original state space are distant in representation space when functionally distinct. For room navigation, ARC separates rooms based on bottlenecks, emphasizing primary elements over secondary ones. In complex domains like wheeled or legged locomotion and block manipulation, VAE and naive state representations do not capture salient elements like robot CoM or object position. By analyzing how distances in the latent space change when perturbing different state elements, we aim to understand which factors are emphasized in the representation. In legged locomotion, CoM is important for decision making while joint angles are secondary. The CoM is the key factor in legged locomotion, while joint angles are less significant. In the wheeled environment, the car's position has a major impact, while orientation is less important. In object pushing, block position is crucial, with end-effector position being secondary. By projecting perturbed states into 2 dimensions, we can see that the ARC representation captures the important factors, with significant changes when the key factor is perturbed. The ARC representation captures key factors in different tasks, such as CoM for Ant, position for wheeled, and block position for block pushing. It outperforms VAE and original state space in capturing these factors. ARC distances can be used for reward shaping in tasks with sparse rewards, as shown in challenging exploration tasks for wheeled and legged locomotion. The learned representation guides learning via reward shaping to reach arbitrary goals in a larger region. The ARC representation optimizes for functional distances in latent space, leading to faster learning and better performance compared to other methods. This is especially beneficial for tasks with sparse rewards, as it guides learning through reward shaping to reach arbitrary goals in a larger region. The ARC representation, optimized for functional distances in latent space, outperforms other methods, particularly in tasks with sparse rewards. Incorporating dynamics information into representations improves exploration in large environments, as seen in the quadruped ant robot task. Policies using ARC features learn to solve tasks significantly faster than other methods, emphasizing important elements for multi-timestep control. This outperformance is attributed to the ability of ARC to optimize functional distances in latent space, particularly in tasks with sparse rewards. Using ARC representations for high-level controllers in navigation tasks can lead to faster learning and better performance compared to other methods. These representations emphasize important elements for control, reducing redundancy and noise in the input. This allows the RL algorithm to assign credit effectively and train multiple tasks quickly using the same representation. The agent navigates through 50 rooms or waypoints in sequence, receiving sparse rewards. Two hierarchical reasoning schemes are evaluated using ARCs: commanding directly in representation space or through k-means clustering. A high-level controller is trained with TRPO to output actions in latent space or cluster index, leading to better performance in capturing abstraction and environment dynamics. Commanding in representation space with ARCs enables more effective search and high-level control compared to learning from scratch via TRPO and standard HRL methods. The structured ARC space simplifies high-level planning by clustering semantically similar states, leading to better performance in capturing abstraction and environment dynamics. In this work, actionable representations for control (ARCs) are introduced to capture important state representations for decision making. ARCs are learned through a goal-conditioned policy and are useful for tasks such as learning policies, HRL, and exploration. The structured ARC space simplifies high-level planning by clustering semantically similar states, leading to better performance in capturing abstraction and environment dynamics. In this work, actionable representations for control (ARCs) are introduced to capture important state representations for decision making. ARCs are learned through a goal-conditioned policy using off-policy data, with the goal space coinciding with the state space. The process involves training a stochastic goal-conditioned policy using TRPO with an entropy regularization term. The sparse reward formulation is used for the Sawyer environment, while shaped rewards are used for other environments to improve sample efficiency. The goal-conditioned policy is trained using a sparse reward in free space and rooms environments. It is parameterized as a fully-connected neural network with three hidden layers. A dataset of 500 trajectories is collected to cover the entire state space for training representation learning methods. The learning algorithms are trained on a dataset covering the full state space, evaluating representations minimizing reconstruction error and performing one-step prediction. Each component is parametrized by a neural network with ReLU activations and linear outputs, optimized using Adam with a learning rate of 10^-3. Hyperparameter sweeps are conducted for all methods, including dimensionality of the latent state and network size. Objective functions for each representation are detailed in Appendix B. The reward shaping capabilities of learned representations are tested on navigation tasks in different sized regions. A goal-conditioned policy is trained on a small square space, then generalized to a larger square. Policy training involves using a \"shaped\" surrogate reward with weighted terms and an initialized policy. The policy is initialized to the parameters of the original goal-conditioned policy trained on a small region for exploration. Reward shaping with different representations is compared to a dedicated exploration algorithm, VIME, using TRPO as a base algorithm. Hyperparameter sweeps are performed for representation methods. The downstream task involves a \"reach-while-avoid\" task in the Ant environment. The downstream task in the Ant environment involves a \"reach-while-avoid\" task where the quadruped robot starts at (-1.5, -1.5) and must reach (1.5, 1.5) while avoiding a circular region at the origin. The reward function is based on the distance to the goal and origin. A policy is trained using TRPO with a stochastic policy \u03c0(a|\u03c6(s)), where the mean is a neural network and \u03a3 is a learned diagonal covariance matrix. Gradients do not flow through the representation, so only the policy is adapted. In the Ant environment, a high-level policy is learned to direct the agent in a waypoint reaching task with sparse rewards. The agent navigates through a sequence of 50 target locations, receiving rewards when reaching each checkpoint. Target locations are fixed and sampled uniformly at random within an 8x8 meter region. The policy outputs goals in latent space for execution. The high-level policy \u03c0 h (z h |s) outputs goals in latent space for execution by a goal-conditioned policy. The policy is trained with TRPO using a Gaussian distribution in the latent space. A reconstruction network \u03c8 is used to receive goal states g h = \u03c8(z h ). The reconstruction network \u03c8 is trained to minimize loss for the goal-conditioned policy. Comparisons are made using the learned representation to direct the policy in cluster space for navigation through rooms. Checkpoints are sampled uniformly with constraints in the rooms and wheeled rooms environment. The agent moves through different rooms in a fixed order, earning rewards for entering the correct room. A representation is learned from trajectory data and clustered using k-means. A high-level policy selects a cluster, and a low-level policy chooses a state within that cluster to guide the agent. The agent learns a high-level policy to select clusters and a low-level policy to choose states within those clusters. Loss functions for training representations are provided, and all representations are trained on trajectory data. The agent learns high and low-level policies for state selection within clusters. Loss functions for representation training are provided, trained on trajectory data. Neural networks are used to minimize mean squared error in various tasks. The agent learns high and low-level policies for state selection within clusters. Loss functions for representation training are provided, trained on trajectory data. Neural networks are used to minimize mean squared error in various tasks. \u03b2 is a hyperparameter balancing forward and inverse prediction error. 2D Navigation involves an agent navigating to points with acceleration control. Wheeled Navigation involves a car navigating to locations with 6-dimensional state space. Goal-conditioned policies are trained within a 2-dimensional action space. The agent learns high and low-level policies for state selection within clusters. Loss functions for representation training are provided, trained on trajectory data. Neural networks are used to minimize mean squared error in various tasks. \u03b2 is a hyperparameter balancing forward and inverse prediction error. 2D Navigation involves an agent navigating to points with acceleration control. Wheeled Navigation involves a car navigating to locations with a 6-dimensional state space. Goal-conditioned policies are trained within a 2-dimensional action space. The environment involves a Sawyer manipulator and a freely moving block on a table-top. The Sawyer environment consists of a manipulator and a block on a table. The state space is 6-dimensional, with Cartesian coordinates for the Sawyer and the block. The Sawyer is controlled using end-effector position control with a 3-dimensional action space. A shaped reward function is used for training a goal-conditioned policy. Hyperparameter tuning is done for representation parameters, reward shaping scaling factor, and the number of clusters for hierarchical RL. In the Sawyer environment, hyperparameter tuning is conducted for representation parameters, reward shaping scaling factor, and the number of clusters for hierarchical RL experiments. Parameters are optimized for legged and wheeled locomotion environments through a hyperparameter sweep on latent dimension and penalty terms. The ARC representation requires no additional tuning parameters beyond the latent dimension size. In the Sawyer environment, hyperparameter tuning is conducted for representation parameters, reward shaping scaling factor, and the number of clusters for hierarchical RL experiments. The penalty terms for comparison representations are evaluated with fixed values for downstream applications. Tuning for relative scales between sparse reward and shaping term is done over possible values for each representation on legged and wheeled locomotion environments. k-means clustering is performed with different values for each representation on room navigation tasks for 2D and wheeled navigation. In the Sawyer environment, hyperparameter tuning is done for representation parameters, reward shaping scaling factor, and the number of clusters for hierarchical RL experiments. k-means clustering is performed with different values for each representation on room navigation tasks for 2D and wheeled navigation, showing robustness to the number of clusters chosen."
}