{
    "title": "HklbIerFDS",
    "content": "Lifelong machine learning and few-shot learning are crucial for artificial general intelligence. Existing studies have limitations in practical settings. Inspired by human brain function, a novel model called Slow Thinking to Learn (STL) is proposed for making sophisticated predictions by considering interactions between current and previous tasks iteratively at runtime. The effectiveness of STL for lifelong and few-shot learning settings is empirically demonstrated through experiments. Deep Learning has been successful in various applications but still has room for improvement to match human's lifelong learning ability. Neural networks often require training on large datasets and suffer from catastrophic forgetting, unlike humans who can incorporate new knowledge from few examples throughout their lifetime. Efforts have been made to bridge this gap through few-shot learning studies. Efforts have been made to study few-shot learning and lifelong learning tasks, which are more complex than existing models. Lifelong learning models are usually trained with hyperparameters optimized for future tasks, which may not be practical in real-world applications. Sequential few-shot tasks pose a challenge for existing few-shot learning models, as they are typically trained on a large collection of tasks which is not available in lifelong learning scenarios. Humans can learn well with limited information, unlike current machine learning models. The key difference between human decision-making and NN-based models in making predictions lies in the deliberate and iterative process humans use to consider interactions between current and past knowledge. This slow, effortful process can help bridge the learning gap between humans and machines. A brain-inspired model called Slow Thinking to Learn is proposed to mimic this process. The Slow Thinking to Learn (STL) model is proposed for lifelong and few-shot machine learning tasks, with specialized modules for predictions. By enhancing the prediction process, the learning process is simplified, eliminating impractical settings. The interactions between slow predictions and fast learning are explicitly modeled and studied in this work. Our contributions focus on modeling interactions between slow predictions and fast learning to improve machine capability in solving lifelong and few-shot learning problems. The Slow Thinking to Learn (STL) model integrates shared SP and per-task FLs to enhance learning from feedback, aiming for proper training and accurate predictions for sequential tasks. The Slow Thinking to Learn (STL) model integrates a shared Slow Predictor (SP) network with task-specific Fast Learners (FLs) to make few-shot and lifelong predictions for sequential tasks. The FLs consist of an embedding network augmented with an external, episodic, non-parametric memory module for better storage efficiency. The Slow Thinking to Learn (STL) model combines a shared Slow Predictor (SP) network with task-specific Fast Learners (FLs) for few-shot and lifelong predictions. The FLs predict test instance labels using a single feedforward pass, while the SP predicts using a different method. The FLs grow with seen tasks to handle lifelong learning, with FLs for specific tasks stored on disk and loaded into memory when needed. The Slow Thinking to Learn (STL) model combines a shared Slow Predictor (SP) network with task-specific Fast Learners (FLs) for few-shot and lifelong predictions. The SP predicts the label of x with a slower, iterative process, adapting its weights to KNN(h) and making predictions after runtime adaptation. This slower decision-making process may seem unnecessary at first, but it minimizes losses for each seen task, making it a good bargain. The Slow Thinking to Learn (STL) model combines a shared Slow Predictor (SP) network with task-specific Fast Learners (FLs) for few-shot and lifelong predictions. The adapted SP's weights are recursively solved for each remembered (x, y) by the FLs using a gradient-based approach. The \u03b8 encodes invariant principles underlying hypotheses for different tasks, approximating FLs' hypotheses.\u03b8 and \u03b8 (t) 's relative positions on the loss surface after seeing different tasks are shown in Figure 3. The Slow Thinking to Learn (STL) model combines a shared Slow Predictor (SP) network with task-specific Fast Learners (FLs) for few-shot and lifelong predictions. The effective capacity of SP is the union of all possible points within a certain radius. To adapt to new tasks, the FLs need to adjust the embedding function. Using an expandable network can help address the challenge of unknown tasks arriving after training. The STL approach prevents the SP from directly learning tasks. The Slow Thinking to Learn (STL) model utilizes a shared Slow Predictor (SP) network and task-specific Fast Learners (FLs) for few-shot and lifelong predictions. The SP learns invariant principles behind tasks, with a postadaptation capacity that can exceed the training capacity. While FLs are augmented with external memory, additional training on existing few-shot tasks is necessary for optimal performance. The Slow Thinking to Learn (STL) model uses a shared Slow Predictor (SP) network and task-specific Fast Learners (FLs) for few-shot and lifelong predictions. The training objective of g(s) is defined for the current task T(s) with parameterized \u03c6(s) and augmented memory M(s). The feedback term encourages FLs to learn unique features for each task to help SP adapt effectively. The Slow Thinking to Learn (STL) model utilizes a shared Slow Predictor (SP) network and task-specific Fast Learners (FLs) for few-shot and lifelong predictions. The feedback term encourages FLs to learn unique features for each task to assist SP in adapting effectively. An alternate training procedure is used to train the SP and FLs, ensuring efficient learning with minimal examples and previous tasks. Refer to Section 2.3 of the Appendix for more details on the training process. The idea of adapting SP at runtime is similar to MbPA, where the output network adapts to examples stored in external memory for a previous task. However, there is no discussion on how runtime adaptation could improve a model's learning ability, which is the main focus of this paper. The concept of learning invariant representations in SP is akin to meta-learning, where a model learns good initial weights to speed up training for a new task with few shots. Few-shot learning involves using weights to speed up model training for new tasks with minimal data. This paper addresses Problem 1, which lacks the luxury of sampling tasks from the ground truth distribution. Memory-augmented networks, like the one used in this work, have fast-learning capabilities due to their nonparametric nature. Integration with other external memory modules is a potential future direction. Gidaris & Komodakis (2018) introduced FewShot Learning without Forgetting, a new approach in this field. In Few-shot learning, Gidaris & Komodakis (2018) proposed a method that avoids forgetting previous tasks when learning new ones. However, it requires training on numerous tasks to make predictions, limiting its applicability. The evaluation includes comparisons with Vanilla NN, EWC, Memory Module, and MbPA+ using TensorFlow. In Few-shot learning, Gidaris & Komodakis (2018) proposed a method to avoid forgetting previous tasks when learning new ones. They evaluated the method by comparing it with Vanilla NN, EWC, Memory Module, and MbPA+ using TensorFlow. MbPA+ is a memory-augmented model trained using MAML to improve prediction accuracy and space efficiency. Separate-MbPA is similar to Separate-MAML but does not train the SP for run-time adaptation. The models were tested on permuted MNIST and CIFAR-100 datasets to assess their ability to combat catastrophic forgetting and the impact of task-uncertainty on performance. In Few-shot learning, Gidaris & Komodakis (2018) proposed a method to prevent forgetting previous tasks when learning new ones. Memory-augmented models outperform Vanilla NN and EWC, showing no signs of forgetting. However, saving raw examples for an infinite number of tasks may not be feasible due to space constraints. Another setting involves saving only embedded examples to address this issue. In Few-shot learning, Gidaris & Komodakis (2018) proposed memory-augmented models to prevent forgetting previous tasks. The models do not forget even when saving embeddings, except for MbPA+ due to using the same embedder network for all tasks. CIFAR-100 dataset consists of 100 classes grouped into superclasses. Tasks are designed with transferable knowledge between tasks in the ground truth. In CIFAR-100 Hard tasks, class labels within a task belong to the same superclasses, while labels across tasks are from different superclasses, making lifelong learning more challenging. Memory-augmented models store embeddings in external memory, with 4 convolutional layers and one fully connected layer in the embedding networks. The SP model outperforms baseline models for both Normal and Hard tasks, with a 2-layer MLP output network. Task Uncertainty and Hyperparameters are studied to understand the SP's superior performance. The study investigates how model performance changes with capacity, specifically focusing on the SP model on the permuted MNIST dataset with limited external memory. Results show that the SP model performs well even with small memory sizes, indicating that the proposed STL can adapt to future tasks without the need for memory size customization. Additionally, experiments with different model architectures based on LeNet are conducted, with varying capacities and convolutional layers. The study explores how model performance is affected by capacity, focusing on different parametric models on the CIFAR-100 Normal tasks. Results show that EWC's performance is influenced by model size, MbPA forgets previous tasks regardless of model size, while STL performs well on both previous and current tasks, showcasing its runtime adaptation advantage. The experiment proves the advantage of SP's runtime adaptation ability in mitigating the need for carefully sized models for incoming uncertain lifelong tasks. A simulated few-shot task was introduced in the CIFAR Normal and CIFAR Hard sequential tasks, training models with sequential tasks similar to lifelong learning but with the last task being a few-shot task. Memory Module, Separate-MAML, and Vanilla NN were considered as baselines. In a study comparing different baselines for lifelong and few-shot learning, the Memory Module, Separate-MAML, and Vanilla NN were evaluated. Results showed that the Fast Learner and Slow Processor in the model outperformed other baselines. The Memory Module struggled without exposure to a variety of tasks during training, while Separate-MAML had unstable performance due to the lack of feedback. The interaction between fast-learning and slow-thinking modules was deemed crucial for performance. The interactions between the fast-leaning and slow-thinking modules are crucial for joint lifelong and few-shot learning. Comparing the performance of FLs and SP, FLs perform better with small training data, guided by invariant representations learned by SP. The predictive ability of FL and SP intersects within 48 to 192 examples. Visualizations in the Appendix show how SP guides FL representation learning. SP makes slow predictions due to runtime adaptation. The study focuses on the time required for the Slow Predictor (SP) to make predictions, with an average inference time of 0.24 ms for FL, 2.62 ms for SP, and 0.79 ms for Vanilla NN on CIFAR-100 tasks. The Space-Time Learning (STL) approach offers space efficiency and combines per-task FLs with a shared SP for improved lifelong and few-shot learning abilities. The study explores interactions between fast-learning and slow-prediction techniques to enhance machine learning capabilities for lifelong and few-shot learning. Future work will focus on integrating STL with different external memory types and evaluating its performance in real-world scenarios. Memory-augmented neural networks like MANN and Memory Module have shown promise in one-shot learning by utilizing external memory for retaining previously seen examples. Unlike MANN, Memory Module has a deterministic way of updating its memory, providing ease and efficiency in learning. Our proposed STL focuses on optimizing data usage stored in memory by enhancing interactions between per-task memory-augmented Fast Learners (FLs) and Slow Predictor (SP). This allows for better lifelong and few-shot predictions by improving representations. Various approaches in lifelong learning involve storing knowledge from previous tasks to prevent forgetting, such as task-specific models or storing raw data, the hessian of the task, or attention masks. Some recent models in lifelong learning draw inspiration from how the brain functions. Our proposed framework is closely related to dual-memory systems inspired by previous works. Our proposed framework is related to dual-memory systems inspired by the complementary learning systems theory. GeppNet and FearNet utilize external memory for rehearsal and memory replay, while STL does not require dedicated sleep for memory consolidation, making it suitable for continuous operation in frontline services. In this section, technical details about the design and training of STL are discussed. Different methods like MAML or Reptile can be used to solve Eq. (2) efficiently. MAML is chosen for its simplicity and efficiency, with the constraint \u03b8 \u2212 \u03b8 \u2264 R enforced by limiting adaptation steps. STL's FL is compatible with various memory modules, with the Memory Module selected in this paper for its clustering capabilities. The Memory Module (Kaiser et al. (2017)) is chosen in this paper for its clustering capabilities, optimizing FL based on it. The module has deterministic update and replacement rules for records, representing clusters of data points with shared labels. The Memory Module chosen in this paper has clustering capabilities for optimizing FL. The STL is trained sequentially for lifelong tasks, using alternate training for the current task. The training procedure involves updating weights and parameters, with a default alternate ratio of 1:1. The hyperparameter R affects the number of adaptation steps used by SP in the STL's training procedure. The hyperparameter R influences the number of adaptation steps in the Memory Module, impacting the stability of the model and its ability to guide future tasks. Experimentation showed that a small R, typically less than 5 adaptation steps, suffices for good performance. The Sequential Processor (SP) in the STL can work effectively with minimal external memory, maintaining performance even with a memory size of 10. This highlights the SP's ability to guide Future Learners (FLs) towards better representations. The Sequential Processor (SP) in the STL can guide Future Learners (FLs) to find better representations with high adaptation efficiency. It consumes less memory space compared to other models and maintains good performance even with a memory size of 10. The SP's invariant principles allow for quick adaptation with minimal examples. The Sequential Processor (SP) in the STL is beneficial on its own and requires few examples to generate a good hypothesis. The results of sequential few-shot learning on the CIFAR Hard dataset show that the STL model outperforms other baselines, even with limited examples. The interaction between FLs and SP is crucial for enhancing machine learning abilities in joint lifelong and few-shot learning tasks. The embeddings from different Few-Shot Learners (FLs) and the Memory Module are visualized in a 2D space using t-SNE algorithm. FLs learn task-specific features to help the Sequential Processor (SP) adapt quickly, stabilizing the system and making it an efficient guide for learning new tasks with limited examples."
}