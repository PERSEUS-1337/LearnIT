{
    "title": "Bylx-TNKvH",
    "content": "In this work, the redundancy of parameterisation in ReLU networks is explored. It is shown that for architectures with non-increasing widths, permutation and scaling are the only weight transformations that preserve the network's function. The proof is based on a geometric understanding of linear regions in ReLU networks. Deep learning has puzzled machine learning theorists due to contradictions in common sense, such as the effectiveness of deeper networks despite single-hidden-layer networks being sufficient. Recent research highlights the importance of overparameterisation in easier training and better generalisation of deep networks. The specific mechanism of over-parameterisation is still not fully understood. In this work, the study focuses on the ability of neural networks to represent a target function in various ways through parameter configurations. The research explores parameterisation redundancy in feed-forward ReLU networks, investigating transformations that preserve network output behavior for all inputs in a specific domain. The study examines parameterisation redundancy in feed-forward ReLU networks, focusing on transformations that preserve network output behavior within a specific domain. The main theorem shows that permutation and scaling are the only function-preserving parameter transformations for architectures with non-increasing widths. The study explores parameter redundancy in ReLU networks, showing that permutation and scaling are the only preserving transformations for networks with non-increasing widths. The proof relies on a geometric understanding of prediction surfaces, where folds carry crucial information about network parameters. This insight leads to the uniqueness of some networks based on the functions they implement. The concept of a fold-set is introduced in more detail in the following sections. The paper discusses the concept of a fold-set and its geometric structure for a subclass of ReLU networks. It also presents a proof sketch of the main result, with the full proof included in the Appendix. The functional equivalence of neural networks has been extensively studied in connectionist literature, with previous research showing that preserving transformations for feed-forward networks with the tanh activation function are compositions of permutations and sign flips. The paper explores the geometric structure of ReLU networks and their various parameterizations, focusing on networks with ReLU activation functions. Previous studies have shown that feed-forward networks with tanh activation functions can be transformed through permutations and sign flips. Our approach is based on the geometry of piece-wise linear functions, specifically the boundaries between linear regions. Previous studies have presented similar proof techniques, and the sets of equivalent parametrisations can be viewed as symmetries in the weight space. Multiple authors have observed that the naive loss gradient is sensitive to reparametrisation by scaling and proposed alternative, scaling-invariant optimisation procedures. This work focuses on general ReLU networks. General ReLU networks are characterized by non-degeneracy properties, such as weight matrices with non-zero entries and full rank. Most ReLU networks are general, with weights sampled from a distribution with density. These networks satisfy conditions where local optima do not have exactly zero values and exclude degenerate cases. Transparent networks, another class of ReLU networks, have significance that will be explained later. In this section, the concept of fold-sets is introduced, which is crucial for understanding ReLU networks and their prediction surfaces. Transparent networks are a class of networks with easily understandable fold-sets, defined as the set of points where a piece-wise linear function is non-differentiable. The lemma in this section shows that for transparent ReLU networks, knowledge of the end-to-end mapping provides explicit information about the network's hidden units. This information is based on observing the units' zero-level sets, which can determine the unit's parameters. Dealing with piece-wise linearity and disambiguating the union into its constituent zero-level sets remains a challenge for upcoming sections. The first-layer units of a network are linear, forming hyperplanes that partition the input space into regions with different activation patterns. The second-layer units are also linear within each region, creating piece-wise hyperplanes. Higher-layer units follow the same pattern, resulting in complex fold-sets. Piece-wise hyperplanes are defined with respect to a partition P of Z, where H is nonempty and can be represented as {z \u2208 P | w z + b = 0}. Understanding the parameterization of ReLU networks requires a precise characterization of the fold-set and the relationship between individual piece-wise hyperplanes. Comparing different layer units in Figure 1 and Figure 2 shows the variation in the fold-set structure. The fold-set structure in neural networks can be ambiguous when only considering the union of zero-level sets across layers. However, by partitioning hyperplanes based on layers, the folds can be disambiguated and assigned to specific layers. By partitioning hyperplanes based on layers, folds in neural networks can be disambiguated and assigned to specific layers. This procedure may not assign all folds to their original layers, but it is sufficient for mathematical purposes. Piece-wise linear surfaces can be denoted as kS, representing the largest subset of S that is a piece-wise linear surface of order at most k. This allows for the unique decomposition of S into different subsets based on the order of the piece-wise linear surface. The definition of piece-wise hyperplanes allows for the unique decomposition of S. A representation of S in terms of its hyperplanes assigns a unique 'layer' to each hyperplane. For architectures with non-increasing widths, a ReLU network h can be uniquely determined by its input-output mapping. The fold-set of h, if general and transparent, is a piece-wise linear surface. This surface contains information about the network's parameters. The zero-level sets of individual units in a network contain crucial information about the parameters, but disambiguating the union to obtain these sets is generally impossible. This is because a unit may contain more hyperplanes than expected, making it difficult to identify their origins. The key insight is that hyperplanes created by last-layer units cannot have dependencies or descendants in the dependency graph, as they are the final layer. This implies that any hyperplane with a chain of descendants must come from a layer at least m layers below the last one. The lemma states that for a ReLU network, the first-layer hyperplanes can be identified using the information in the fold-set. By constructing a well-connected dependency graph, the parameters of the network can be recovered up to permutation and scaling. This method can be extended to higher layers by considering a truncated network. The lemma shows that a network with a 'well connected' dependency graph exists, allowing recovery of network parameters up to permutation and scaling. This method extends to higher layers by considering a truncated network. The theorem's assumptions originate from Lemma 4, restricting the domain of h l:L to the interior of Z l\u22121 for fold-set simplicity. The architecture studied has non-increasing widths to avoid empty int Z l\u22121. Transparency is ensured by construction, with either h 1:l 1 (z) > 0 or h 1:l 2 (z) > 0 for each input z \u2208 Z and layer l \u2208 [L \u2212 1]. The boundedness of Z is necessary for the first layer, and a modified definition of transparency can lift this constraint. The proof extends to leaky ReLU activations, except for the diagonal entries of M l. The argument for positive diagonal entries does not apply to leaky ReLU, but a workaround may exist. This work demonstrates that certain ReLU networks are almost uniquely defined for architectures with non-increasing widths. Certain ReLU networks are almost uniquely identified by the function they implement, suggesting small function-equivalence classes. Possible explanations include easier achievement of approximate equivalence and the consideration of atypical networks in the study. The study suggests that certain ReLU networks are almost uniquely identified by the function they implement, with small function-equivalence classes. The analysis could be extended to convolutional and recurrent networks, as well as other activation functions like leaky ReLU. The authors highlight the importance of exploring architectures with non-increasing widths and wider intermediate layers for further research. The partition of Z induced by S, denoted P Z (S), consists of connected components of Z \\ S. A piece-wise hyperplane with respect to partition P is defined as H \u2286 Z = {z \u2208 P | w z + b = 0}. A piece-wise linear surface on Z of order \u03ba is a set S \u2286 Z that can be written as , with no smaller number than \u03ba admitting such a representation. If S 1 , S 2 are piece-wise linear surfaces on Z of order k 1 and k 2 , then S 1 \u222a S 2 is a piece-wise linear surface on Z of order at most max {k 1 , k 2 }. The text discusses the equality and inclusion relationships between different sets of piece-wise linear surfaces of varying orders. It also introduces the concept of piece-wise hyperplanes with respect to a partition. The text discusses the canonical representation of piece-wise linear surfaces, showing that every pwl surface has a canonical form. It also explains the inclusion relationships between different sets of piece-wise hyperplanes. The text discusses the canonical representation of piece-wise linear surfaces and ReLU networks on open sets in R^d0. It defines the dependency graph of a pwl surface and introduces the ReLU function. The text introduces activation indicators for functions and defines the fold-set of a continuous, piece-wise linear function on an open set. It also discusses the positive/negative behavior of a function in a neighborhood. The unit (l, i) fold-set of a ReLU network is defined as F. If z satisfies certain conditions, then z is in F(\u03c3 \u2022 f), otherwise z is in F(\u03c3 \u2022 f) c. The ReLU behavior is analyzed based on the value of f(z) in a neighborhood of z. The ReLU behavior is analyzed based on the value of f(z) in a neighborhood of z. If f(z) < 0, ReLU behaves like a constant zero on B(z), making \u03c3 \u2022 f differentiable at z. If f(z) > 0 and z \u2208 F(f), ReLU behaves like an identity on B(z). If f(z) = 0 and f is positive in the neighborhood of z, different cases are considered. Lemma A.9 states that for a set Z and continuous piece-wise linear functions f1,...,fn on Z, the proof is left as an exercise. Lemma A.10 discusses activation indicators and networks, proving a statement for a fixed activation indicator I. The rank of a matrix is analyzed in relation to zero rows, leading to the proof of the lemma. The matrix obtained by removing zero rows has a size of (rank(I k ), d k\u22121 ). For most W k, this matrix has a rank of min {d k\u22121 , rank(I k )}. Concatenating any r i -subset of rows of W i to the basis of ker(W i\u22121 ) results in linearly independent rows. The rank of I i W iWi\u22121 is at least min rank(W i\u22121 ), rank(I i ) r i. The rank of I i W iWi\u22121 is determined by the minimum rank between W i\u22121 and I i, denoted as rank(I i W iWi\u22121 ) = min(rank(W i\u22121 ), rank(I i ) r i). If rank(I i ) \u2264 rank(W i\u22121 ), the rows of I i W i are linearly independent, resulting in rank(I i W iWi\u22121 ) = r i. If rank(I i ) > rank(W i\u22121 ), the problem can be reduced by replacing some 1's with 0's in I i to match the ranks, leading to the same conclusion. The lemma states that for most values of \u03b8, if two units have the same activation indicators up to a certain point, then a certain condition holds for all scalars c. This condition is automatically satisfied when c = 0. The lemma assumes c = 0 and treats (l, i, I) and (k, j, J) symmetrically. The set \u0398 \u00ac of parameters for which the lemma does not hold needs to be shown as closed and zero-measure. By contradiction, if \u0398 \u00ac is positive-measure, there exist triples (l, i, I), (k, j, J), and a scalar c such that a certain condition is satisfied. Considering cases, if (l, i) = (k, j), then a contradiction arises, completing the proof. The lemma proves that a ReLU network is general if it satisfies certain conditions. It states that at least one of the parameters must be non-trivial, and provides a proof for this assertion. The ReLU network is proven to be general if it meets specific conditions regarding non-trivial parameters. The proof involves showing that certain units and activation indicators satisfy the lemma, with conditions for positivity and negativity in the network's neighborhood. The ReLU network is proven to be general by satisfying specific conditions for its units and activation indicators. The proof involves showing that certain parameters hold true, leading to the conclusion that the network is general. The ReLU network is proven to be general by satisfying specific conditions for its units and activation indicators. The function h 1:l+1 i is linear on regions of P, with slope and bias denoted by w(P) and b(P). The set F(h l:L | intZ l\u22121) is a piece-wise linear surface with a dependency graph containing directed paths. The construction ensures transparency in the networks through the definition of W l 2 and the identification of important vertices in the dependency graph. The ReLU network is proven to be general by satisfying specific conditions for its units and activation indicators. The function h 1:l+1 i is linear on regions of P, with slope and bias denoted by w(P) and b(P). The set F(h l:L | intZ l\u22121) is a piece-wise linear surface with a dependency graph containing directed paths. The construction ensures transparency in the networks through the definition of W l 2 and the identification of important vertices in the dependency graph. G contains the edge H, and there exists a sequence of points {z n } \u2286 P l such that z n \u2192z. By contradiction, it is shown that no two piece-wise hyperplanes in S are included in a single hyperplane. The ReLU network is proven to be general by satisfying specific conditions for its units and activation indicators. For any general ReLU network h \u03b7 : X \u2192 R, there exist permutation matrices P 1 , . . . P L\u22121 and positive-entry diagonal matrices M 1 , . . . , M L\u22121 such that h \u03b8 (x) = h \u03b7 (x) for all x \u2208 X."
}