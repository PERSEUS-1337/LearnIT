{
    "title": "HkeyZhC9F7",
    "content": "We demonstrate using deep reinforcement learning to learn efficient heuristics for backtracking search algorithms in quantified Boolean logics. The learned heuristic reduces execution time by a factor of 10 compared to existing handwritten heuristics, allowing for the solution of challenging logical problems. Automated reasoning algorithms have advanced significantly in recent decades, now being used in industry for hardware and software verification. Automated reasoning algorithms like logic solvers for SAT, SMT, and QBF rely on advanced backtracking search algorithms to solve complex formulas. Machine learning has made breakthroughs in various fields by training deep models on large datasets. In this paper, the focus is on combining machine learning and automated reasoning algorithms. The challenges of applying neural networks to logical formulas are discussed, along with using reinforcement learning to improve backtracking search algorithms in automated reasoning. The most prominent heuristic choice in CDCL-style search algorithms is the variable selection heuristic, such as VSIDS and its variants, used in competitive SAT and QBF solvers. Even small improvements like learning rate branching are of interest to the automated reasoning community. Designing heuristics for backtracking search algorithms can be counter-intuitive. Heuristics that prioritize causing conflicts over finding correct solutions can lead to better performance in search algorithms. This approach allows for clause learning, which deduces new constraints to prevent similar conflicts in the future, saving computation time. Machine learning can be applied to improve heuristics in backtracking search algorithms, aiming to solve more formulas in less time through reinforcement learning. In this work, the focus is on learning heuristics for the solver CADET BID36 BID38, a competitive solver for QBF formulas. The input to the neural network includes the current state of the algorithm and the formula, with a reward for quick termination. The unique challenges include the large size of the input with hundreds of variables and thousands of clauses. The heuristic addressed is the variable selection heuristic. The study involves solving QBF formulas using the CADET BID36 BID38 solver by learning heuristics. The input includes variables, clauses, and rewards for quick termination. The size of formulas varies greatly, with some large ones being easily solvable while smaller ones pose challenges. Episodes can exceed 100k steps, with some formulas never reaching a solution. The goal is to solve formulas faster, but the computational cost of decision-making may outweigh the benefits. Neural network inference times are typically in milliseconds. The study aims to create superior heuristics for automated reasoning algorithms using deep reinforcement learning, specifically focusing on Graph Neural Networks (GNNs). The approach overcomes challenges such as formula size, episode length, performance, and nameless variables by computing an embedding of each variable based on their context in the formula. The study focuses on creating better heuristics for automated reasoning algorithms using Graph Neural Networks. It addresses challenges like formula size, episode length, and performance by computing variable embeddings based on their context in the formula. This approach allows for scaling to formulas with any number of variables and shows great generalization properties. Propositional Boolean logic uses constants 0 and 1, variables, and standard Boolean operators. A formula is in conjunctive normal form (CNF) if it is a conjunction over clauses. Any Boolean formula can be transformed into CNF with a linear increase in size. The satisfiability problem of propositional Boolean logics (SAT) aims to find a satisfying assignment for a given Boolean formula. The satisfiability problem (SAT) aims to find a satisfying assignment for a Boolean formula. Backtracking search algorithms like DPLL gradually extend a partial assignment until a conflict is reached. Conflict-driven clause learning (CDCL) improves upon DPLL by analyzing conflicts. Conflict-driven clause learning (CDCL) enhances backtracking search algorithms like DPLL by analyzing conflicts through resolution steps, cutting off search space and improving efficiency. Top SAT solvers like Lingeling, Crypominisat, Glucose, and MapleSAT rely on CDCL to solve formulas with millions of variables for industrial applications. Quantifiers in QBF extend propositional logic, with formulas in prenex normal form having quantifiers at the beginning. 2QBF is a subset of QBF with one quantifier alternation, starting with universal quantifiers followed by existential quantifiers. In the context of QBF, 2QBF involves a sequence of quantifiers. The algorithmic problem for QBF is to determine the truth of a quantified formula (TQBF). CDCL-like algorithms, such as CADET, have been explored for QBF. CADET is a state-of-the-art solver for 2QBF that implements a generalized CDCL backtracking search algorithm. This solver ensures results can be proven, preventing loopholes in the environment for reinforcement learning algorithms. The agent interacts with environment E, a Markov decision process with states S, action space A, and rewards. The goal is to maximize expected reward. The environment is deterministic except for the initial state, where a formula is chosen randomly. The agent selects variables from available options based on observations received. The agent interacts with the environment, a Markov decision process with states, actions, and rewards. The agent selects variables based on observations. An episode is complete when the solver reaches a terminating state. VSIDS is a heuristic used in SAT solving. The VSIDS heuristic is a state-of-the-art method in SAT solving, maintaining activity scores per variable to choose the most active one. It uses the Jeroslow-Wang heuristic BID21 to select the polarity of the variable. The Random heuristic selects actions uniformly at random. Deep learning faces challenges in this setting due to large inputs and an infinite action space. Boolean formulas present unique challenges for deep learning models due to the vast differences in size and the infinite action space. Unlike natural language words, variables in Boolean formulas lack individual meaning and are independent across different formulas. This makes traditional embedding techniques ineffective, as models would need to remember thousands of variable names and their semantics to understand interactions. The semantics of Boolean formulas in CNF are represented as a graph using Graph Neural Networks (GNNs) to compute embeddings for literals. A policy network predicts the quality of each literal independently. The neural network architecture only requires learning two operators: one to compute a clause embedding based on literals and another to compute a literal embedding based on clause embeddings. The network is agnostic of variable names, making it applicable to large and unseen formulas. The GNN enforces network invariance under clause and literal reordering in Boolean formulas. It maps formulas and solver states to embeddings for each literal, considering only variable-specific information. The GNN input includes formula connectivity and solver state, representing variables' assigned values and selections. The GNN enforces network invariance under clause and literal reordering in Boolean formulas by mapping formulas and solver states to embeddings for each literal. It considers variable-specific information, including whether a variable has a value assigned, its selection history, and VSIDS activity score. The algorithm also tracks clause origins and whether they were derived from conflict analysis. Hyperparameters include embedding dimensions for variables and clauses, as well as the number of GNN iterations. Trainable parameters consist of a matrix and initial literal embedding. The policy network uses sparse matrices to represent adjacency information of nodes in the formula graph. It predicts the quality of each literal based on the literal embedding and global solver state. The global solver state includes essential solver state and statistical information. The policy network maps the literal embedding with the global solver state to a numerical value indicating the quality of the literal. The policy network has inputs from the literal and uses two fully-connected layers with ReLU nonlinearity. Predictions are turned into action probabilities through a softmax. Information flow to the policy network's judgement goes through the graph neural network. The design is simple and efficient, aiming to improve heuristics of the logic solver CADET through deep reinforcement learning experiments. Key questions addressed include learning to predict good actions for formulas and generalizing policies trained on short episodes to long episodes. The study aims to improve the logic solver CADET through deep reinforcement learning experiments. Key questions include generalizing policies trained on short episodes to long episodes and assessing the policy's performance on different families of formulas. Finding suitable data for the study proved challenging due to the varied statistical properties of different formula sets. The study aims to enhance the logic solver CADET through deep reinforcement learning experiments. They encountered challenges in finding suitable data due to the varied statistical properties of formula sets. A set of 4500 formulas, called Reductions, was selected for the study, containing varying levels of difficulty and sizes. 2500 formulas were filtered out as they could be solved without heuristic decisions. The study aims to enhance the logic solver CADET through deep reinforcement learning experiments. They selected a set of 4500 formulas, called Reductions, with varying levels of difficulty and sizes. 2500 formulas were filtered out as they could be solved without heuristic decisions. To address questions 2 and 3, they set aside a test set of 200 formulas, leaving a training set of 1835 formulas. The encoder and policy networks were jointly trained using REINFORCE. Rewards were assigned for solving formulas in fewer steps, and standard techniques were applied to improve training. The model was trained on the Reductions training set, resulting in the policy Learned. The cactus plot in FIG1 shows the growth in solved formulas for increasing decision limits. Lower lines in the plot indicate better performance, as they represent more formulas being solved with fewer decisions. The cactus plot in FIG1 illustrates that Learned outperformed baselines by solving significantly more formulas within a decision limit of 200 steps during training. The advantage of Learned over VSIDS is comparable to VSIDS over random choices, and this performance extends well beyond the training decision limit. The exponential growth in advantage over VSIDS with increasing decision limit suggests the effectiveness of small neural networks in achieving optimal results. Our best model uses \u03c4 = 1, \u03b4 L = 16, and \u03b4 C = 64, with merely 8353 parameters. The small neural network achieved the best results, with quick inference times. The learned heuristic solved 101 formulas, falling short of beating VSIDS. The generalization to a new formula set was not particularly strong. The learned heuristic, despite its performance disadvantage compared to the pure C implementation using VSIDS, reduces overall execution time by up to 90%. It also improves the number of solved formulas within the time limit significantly, which is the main performance criterion for logic. Our approach utilizes a GNN approach for logical formulas, allowing for variable embeddings based on context within the formula. GNNs have shown better scalability compared to previous LSTM and syntax-tree approaches. Our approach combines graph neural networks and logic solvers in a reinforcement learning setting to improve scalability and performance in predicting the satisfiability of logical formulas. Unlike previous approaches that focus on computationally cheap methods, we aim to leverage the strengths of logic solvers to handle larger formulas and ensure correct answers. Dai et al. used GNNs for combinatorial algorithms over graphs, but our focus is on enhancing the state-of-the-art in algorithmic problem-solving. The curr_chunk discusses the application of deep reinforcement learning in learning across multiple formulas in QBF algorithms, contrasting with other competitive algorithms. Janota also explored the use of classical machine learning techniques in QBF solvers. Reinforcement learning has been applied to logic reasoning tasks in various ways, such as learning linear policies for theorem proving and heuristics for program analysis. BID26 applied reinforcement learning to propositional logic, leading to improved scalability in existing solving algorithms. The curr_chunk discusses improving backtracking search algorithm heuristics for Boolean logic through deep reinforcement learning, addressing challenges like unbounded input-size and action space. The approach significantly reduces execution time of a competitive QBF solver by a factor of 10 after training on similar formulas, motivating further research efforts in logic solver performance enhancement. Challenges include limited transfer of learned insights between formula sets and difficulty in learning from long episodes. The curr_chunk discusses variables in Boolean logic and the limitations of assigning strings as names. The reasoning behind this decision is for computational backend use only. A model was trained on reduction problems training set. The model was trained on reduction problems training set using specific hyperparameters on an AWS server. Additional sets of formulas were considered and replicated the results from the main part. The study trained a model on reduction problems using specific hyperparameters on an AWS server. Additional sets of formulas, called Boolean and Words, were tested and showed improved performance compared to VSIDS. Boolean consists of random circuits with a fixed number of inputs, where AND-gates are added randomly up to a limit. The circuits are transformed into propositional Boolean formulas using the Tseitin transformation, with some random clauses added for irregularities. To convert these formulas into QBFs, 4 variables were universally quantified, resulting in an even split of true and false formulas with an average of 50.7 variables. The study trained a model on reduction problems using specific hyperparameters on an AWS server. Additional sets of formulas, called Boolean and Words, were tested and showed improved performance compared to VSIDS. Boolean consists of random circuits with a fixed number of inputs, where AND-gates are added randomly up to a limit. The circuits are transformed into propositional Boolean formulas using the Tseitin transformation, with some random clauses added for irregularities. To convert these formulas into QBFs, 4 variables were universally quantified, resulting in an even split of true and false formulas with an average of 50.7 variables. The Words data set consists of random expressions over (signed) bitvectors with specific characteristics. The study trained a model on reduction problems using specific hyperparameters on an AWS server. Additional sets of formulas, called Boolean and Words, were tested and showed improved performance compared to VSIDS. The formulas are simplified using the circuit synthesis tool ABC and turned into CNF using the Tseitin transformation. The resulting formulas have an average of 71.4 variables and are harder for both Random and VSIDS. For example, the first formula from the data set is \u2200z.\u2203x.((x \u2212 z) xor z) = z + 1, resulting in a QBF with 115 variables and 298 clauses. This statement is true and solved with just 9 decisions using the VSIDS heuristic. Training a new model on the Words dataset results in significantly improved performance. The study trained a model on reduction problems using specific hyperparameters on an AWS server. Additional sets of formulas, called Boolean and Words, were tested and showed improved performance compared to VSIDS. The resulting formulas have an average of 71.4 variables and are harder for both Random and VSIDS. An interesting observation is that models trained on sets of small formulas generalize well to larger formulas from similar distributions. A new dataset, Words30, was generated with larger formulas similar to the Words dataset, with expressions of size 30 and 186.6 variables on average. Testing the model trained on Words on Words30 showed promising results. In Figure 7, the model trained on small formulas shows good generalization to larger and harder formulas. The heuristics make local decisions due to the graph neural network approach, which helps in solving problems efficiently. This behavior likely extends well to larger formulas. The behavior of the model trained on small formulas likely generalizes well to larger formulas or longer episodes (Figure 7)."
}