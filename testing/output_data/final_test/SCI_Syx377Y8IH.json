{
    "title": "Syx377Y8IH",
    "content": "One fundamental problem in supervised classification and machine learning is modeling non-parametric invariances in data. Prior work focused on enforcing invariances to parametric nuisance transformations, but learning non-parametric invariances directly from data is still a challenge. This paper introduces a new layer for convolutional networks that can learn general invariances from data, called Permanent Random Connectome Non-Parametric Transformation Networks (PRC-NPTN). These networks are initialized with random connections, not just weights, which are a small subset of connections in a fully connected convolution layer. PRC-NPTNs have randomly initialized permanent connections that remain throughout training and testing, making them biologically plausible. These connections help learn invariances from data and outperform ConvNet baselines and NPTNs on benchmarks. They have an enhanced ability to learn non-parametric invariances. The presence of random connections in the cortex at a local level raises the question of whether it allows for enhanced learning of non-parametric invariances. Test data augmentation increased the complexity of the problem without altering the architecture. Test errors for this experiment are presented in a table."
}