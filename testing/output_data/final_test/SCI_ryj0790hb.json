{
    "title": "ryj0790hb",
    "content": "Deep Adaptation Networks (DAN) is a method proposed to learn new capabilities in a trained neural network without hindering performance on existing tasks. DANs constrain newly learned filters to be linear combinations of existing ones, requiring only a fraction of the parameters compared to standard fine-tuning procedures. When combined with network quantization techniques, the parameter cost can be reduced to around 3% of the original with no loss in accuracy. The learned architecture can switch between various representations. The learned architecture can switch between various representations, enabling a single network to solve tasks from multiple domains. Extensive experiments show the effectiveness of this method on image classification tasks. Typically, separate models are needed for different tasks, but related tasks can share the same architecture. The goal is to enable a network to learn related tasks one by one by augmenting a network learned for one task with controller modules. These modules utilize already learned representations for another task and are optimized to minimize loss on the new task without requiring the training data for the original task. The Deep Adaptation Networks (DAN) allow a single network to encode multiple tasks by adding controller modules to each layer. This enables seamless transition between tasks without the need for original task data. The method is effective in the Visual Decathlon Challenge, outperforming fine-tuning without doubling parameters. In this work, an improved alternative to transfer learning is introduced for image classification tasks. The method preserves old task performance, requires fewer parameters, and can switch between learned tasks. Two variants of the method are presented, one with full parameters and one with significantly fewer parameters that outperforms shallow transfer learning. The next section reviews related work, while Section 3 details the proposed method. In multi-task learning, a single network is trained to perform multiple tasks simultaneously, using a shared representation and multiple losses as a regularizer. Examples include facial landmark localization, semantic segmentation, 3D-reasoning, and object detection. Recent work explores the ability of a single network to perform various tasks on the same dataset. In contrast to multi-task learning, our method focuses on classifying images from multiple datasets one-by-one, without joint training. We compare our approach to Rebuffi et al. (2017) in Sec 4 and highlight the differences. Our method is similar to BID26 but does not require joint training and uses fewer parameters. Incremental learning is crucial to avoid \"catastrophic forgetting\" when adding new abilities to a neural network. The network's ability to perform well on old tasks can be maintained by fixing parameters and using the penultimate layer as a feature extractor for training a classifier. However, fine-tuning the entire architecture yields better results. Different methods for preserving old-task performance are discussed, including adding new representations alongside old ones without compromising old task performance. In BID33, new representations can be added alongside old ones without affecting old task performance, but this increases the number of parameters for each added task. BID18 lowers the learning rate of important neurons for the old task. Our method preserves the old representation while slightly increasing parameters for each added task. Various methods like network compression can be used in conjunction with ours to improve results. The proposed method involves modifying each convolutional layer of a base network by re-combining weights through a controller module for each new task. A binary switching vector controls the network output, with a \"Dataset Decider\" sub-network determining the source domain of the image. Different fine-tuning strategies are compared for transferability between datasets, showing better results for related domains. The text discusses using a deep convolutional neural net (DCNN) for image classification tasks. It mentions the structure of contemporary DCNN's with computational blocks and fully connected layers. The method applies to networks with or without residual connections. The text discusses augmenting a base network N for solving two tasks, T1 and T2, by attaching a controller module to each convolutional layer. This allows the network to solve T2 with the same architecture but different parameters. The text discusses augmenting a base network N for solving two tasks, T1 and T2, by attaching a controller module to each convolutional layer. Each controller module uses the existing weights of the corresponding layer of N to create new convolutional filters adapted to the new task T2. The text discusses adapting filters in a convolutional layer using a switching parameter \u03b1. A weaker variant forces the matrices to be diagonal, scaling the output of each filter. This variant is called \"diagonal\" compared to the full \"linear\" method. The text introduces the \"diagonal\" variant as a form of explicit regularization in adapting filters in a convolutional layer. It mentions the possibility of attaching multiple controller networks to a single base network, with each controller network being switchable. The notation for a network learned for a dataset/task S is N S, and a controller learned using N S as a base network is denoted as DAN S. In this work, a base network called DAN S (Deep Adaptation Network) is used for specific tasks. The method is applied to classification tasks but can be used for other tasks as well. The number of new parameters added for each task depends on the filters and fully-connected layers. The total number of weights required to adapt the convolutional layers combined with a new fully-connected layer amounts to about 13% of the original parameters. For VGG-B, this is roughly 21%. Constructing 10 classifiers using one base network and 9 controller networks requires 2.17 times the original parameters. When applied to VGG-B architecture, the method increases the number of parameters if the output filters exceed the input filter dimensions. Various experiments were conducted using different network architectures on classification benchmarks to analyze the method's performance. The study utilized the VGG-B architecture for analyses and experiments on various datasets, establishing baselines and testing different variants of the proposed method. Additionally, methods for predicting network performance and domain classification without manual parameter selection were discussed. Results on the Visual Decathlon Challenge using a different architecture were also presented, along with useful properties of the method. The study utilized the VGG-B architecture for experiments on various datasets, resizing images to 64x64 pixels and whitening them. Training was done with 80% for training and 20% for validation. The VGG-B architecture performed well on different datasets when trained from scratch. The study trained networks independently on 8 datasets using the Adam optimizer with varying learning rates. Most networks converged within 10-20 epochs. The top-1 accuracy (%) is summarized in Tab. 1. Performance was compared to training from scratch or pretrained networks on two base nets: DAN caltech\u2212256 (79.9%) and DAN sketch. The study compared the performance of different base networks on various datasets. DAN imagenet, based on VGG-B pretrained on ImageNet, showed improved performance on Caltech-256 and other datasets. DAN sketch performed better on Sketch and Omniglot datasets, which are domains of unnatural images. DAN imagenet was slightly inferior to non-pretrained VGG-B(S) in terms of performance. The study implemented a dual-controlled network using DAN caltech\u2212256 and DAN sketch as base networks, with controller networks attached. The resulting sub-networks' feature parts were concatenated before the fully-connected layer, achieving the same performance as DAN sketch alone. By using selected controller modules per group of tasks, performance was improved, with DAN imagenet used as the basis for control modules except for Omniglot and Sketch datasets. The study implemented a dual-controlled network using DAN caltech\u2212256 and DAN sketch as base networks, with controller networks attached. DAN imagenet+sketch network was created for 9 tasks, boosting mean performance to 87.76%. Starting from a randomly initialized base network, the method performed surprisingly well with a parameter cost of 0.22. The study implemented a dual-controlled network using DAN caltech\u2212256 and DAN sketch as base networks, with controller networks attached. The results showed that initializing with DAN caltech\u2212256 worked well, achieving a mean top-1 precision of 76.3%. Different weight initialization schemes were tested for the control-module, including setting W to an identity matrix, random noise, and linear approximation. Training DAN sketch\u2192caltech256 for one task helped find the best initialization scheme. From the experiments conducted, it was found that diagonal initialization is superior for training the controller modules in the dual-controlled network. This method allows for faster convergence by tuning the behavior of the base network. The residual adaptation unit in Rebuffi et al. FORMULA2 also shows similarities to the diagonal configuration. Starting with Caltech-256 as a feature extractor proves to be the most generic for transfer learning performance. Fine-tuning is best when initially training on the Sketch dataset. Choosing a good base-network for others involves testing the performance on other datasets. Transferability is defined as the accuracy attained by fine-tuning a network trained on a source task to perform on a target task. Three scenarios are tested: fine-tuning only the last layer, fine-tuning all layers, and freezing batch-normalization parameters. The results show that freezing batch-normalization parameters can be beneficial in some cases. There is a distinction between natural and unnatural images in terms of feature transferability. Training a network starting from Sketch or Omniglot works well for most datasets, both natural and unnatural. The mean transferability of datasets is determined by the transferability matrix. DAN Caltech\u2212256 is best for feature extraction, while DAN Plankton is best for full fine-tuning. DAN Sketch achieves high accuracy for controller networks. Transfer learning from the same network also performs well. Testing correlation between transferability and performance shows promising results. A single network for domain determination and classification is also explored. The network can determine the domain of an image and classify it accurately. By training a classifier on 8 datasets, the network quickly converges to high accuracy. A \"dataset-decider\" named N dc is used to set the controller scalar \u03b1 i of DAN sketch\u2192Di to 1 for images from the correct dataset. Increasing \u03b1 shifts the network towards learned tasks, gradually lowering performance on the base task and improving on the learned ones. The performance on sketch and Plankton datasets shows a similarity in learned representations. The goal is to achieve accurate classification on various datasets with a small model size. Each entry in the challenge is assigned a decathlon score to highlight methods that outperform the baseline on all 10 datasets. More details can be found on the challenge website. The study utilized a wide residual network with specific configurations and the YellowFin optimizer for training. Different learning rates were used for various datasets, with adjustments based on validation data. The study trained on the reduced resolution ImageNet from scratch and used the resulting net as a base for all tasks. Results were compared with baseline methods and those of Rebuffi et al. (2017), showing higher results with a stronger base architecture but at a higher parameter cost. The final column of the table shows the decathlon score, with the Residual Adapters method slightly below in terms of score and slightly above in terms of mean. The method discussed in this section highlights additional properties and benefits. Experiments were conducted using the same architecture as before, training only on the Visual Decathlon Challenge training sets and testing on validation sets. The proposed method (DAN) reduces parameters significantly and outperforms residual adapters, even when tasks are added independently. The study also examines the effects of network compression on performance. The study explores network compression techniques, specifically linear quantization of network weights using 4, 6, 8, 16, or 32 bits. Results show that using 8 bits incurs only a marginal loss of accuracy, allowing for learning new tasks with minimal parameter cost. Transfer methods like feature extraction and fine-tuning are compared, with some datasets maintaining performance even at 6 bits. Our method requires fewer parameters compared to fine-tuning, with the diagonal variant outperforming feature extraction when parameter usage is limited. The number of epochs needed to reach maximal performance is lower for our method, which converges faster and achieves slightly better performance despite network constraints. The method presented adapts an existing network to new tasks while preserving the existing representation. It may not work well if tasks require disjoint solutions. Future work should explore balancing feature reuse and learning new ones. Our method adapts an existing network to new tasks while preserving the representation, achieving better performance with fewer parameters. It converges quickly to high accuracy and can easily switch between learned tasks within a single network. The method allows for seamless switching between learned tasks using a control parameter \u03b1 as a real-valued vector. This enables smooth transitions between task representations, as shown in FIG2. By allowing tasks to use a convex combination of existing controllers, efficiency is increased and the number of controllers is decoupled from the number of tasks."
}