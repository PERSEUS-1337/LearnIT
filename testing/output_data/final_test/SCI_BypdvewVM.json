{
    "title": "BypdvewVM",
    "content": "The backpropagation of error algorithm (BP) is considered challenging to implement in a real brain, but recent success in deep networks has led to proposals on how the brain might learn across multiple layers. These proposals have not been rigorously evaluated on tasks where BP-guided deep learning is crucial. A biologically motivated model of deep learning has shown promising results on datasets like CIFAR-10 and ImageNet, demonstrating competitive performance with BP in training. The backpropagation of error algorithm (BP) has been questioned for its suitability in explaining learning in the brain. Different architectures or algorithms may be needed to scale approaches like DTP and our algorithm for training deep networks. Concerns include violation of Dale's Law, lack of cell-type variability, and the need for gradient signals to be both positive and negative. The backpropagation of error algorithm (BP) has faced serious objections, including the need for feedback connections to have the same weights as feedforward connections and the requirement for a distinct form of error propagation that does not affect neural activity. Researchers are seeking biologically plausible learning algorithms that address these flaws, with approaches like Contrastive Hebbian Learning and Generalized Recirculation using feedback connections to influence neural activity and approximate gradients. Many activity propagation methods, such as target propagation (TP), approximate gradients locally without explicit propagation through the network. These methods aim to address objections to backpropagation, but still require long settling phases for computing learning signals. Shortening these phases has proven challenging, limiting the scalability of algorithms relying on settling processes in machine learning. Target propagation (TP) is a practical algorithm that involves computing perfect inverses backwards through a feedforward neural network to infer activities that produce a desired output target. This method does not require settling dynamics and allows for layer-wise comparison of feedforward activations to compute weight changes. Target propagation (TP) algorithms compute forward passes and updates quickly without settling dynamics. One TP variant approximates weight changes computed by backpropagation (BP). However, questions remain about their applicability to the brain, as they have only been tested on simple datasets like MNIST and simple architectures like multi-layer perceptrons (MLPs). This work addresses these issues by examining learning and incorporating TP into more complex architectures. The study examines the learning and performance of Difference Target-propagation (DTP) on MNIST, CIFAR, and ImageNet. A variant called Simplified Difference Target Propagation (SDTP) is developed to address biologically implausible features. The role of weight-sharing convolutions in improving generalization and reducing memory and communication requirements is also investigated. The study compares STDP with BP in neural networks without weight sharing, showing that both methods perform worse than convolutional neural nets. The feed-forward neural network with L layers computes activations using a non-linear function on affine transformations of previous layer activations. The output layer parametrizes a predicted distribution over possible labels, with the learning signal provided by a loss incurred from making predictions. The learning signal in neural networks is provided by a loss incurred from making predictions, with the goal of adjusting parameters to minimize this loss. Backpropagation is a popular method for computing gradients with respect to layer parameters using the chain rule. SDTP differs from DTP and BP by not transporting gradients through error propagation steps. In target propagation, gradients are propagated backwards through the network using symmetric weights, inducing neural activity to match target outputs. This differs from backpropagation, where gradients are passed without inducing neural activity. In target propagation, neural activity is induced to match target outputs by propagating gradients backwards through the network using symmetric weights. This approach differs from backpropagation, where gradients are passed without inducing neural activity. The final output layer is trained to minimize loss directly, while other layers are trained to match their targets. Approximate inverse transformations can be learned to generate good targets in networks with invertible layers, eliminating the need for symmetric weight connectivity. Target propagation algorithms involve scheduled minimization of two types of losses for each layer. In target propagation, neural activity is induced to match target outputs by propagating gradients backwards through the network using symmetric weights. The final output layer is trained to minimize loss directly, while other layers are trained to match their targets. Approximate inverse transformations can be learned to generate good targets in networks with invertible layers. Two kinds of losses are involved for each layer in the training process. Target propagation involves computing targets by propagating higher layers' targets backwards through layer-wise inverses. Vanilla TP may struggle with different appearances of the same class, aiming to make their representations identical even in early layers. Difference target propagation updates output weights using the standard gradient rule, addressing biological concerns. Difference target propagation (DTP) updates output weights using the standard gradient rule, providing a stabilizing linear correction for imprecise inverse functions. In the original work, the penultimate layer target was computed using gradients from the network's loss to ensure diverse targets despite low-dimensional 1-hot targets at the output layer. Learning a good inverse mapping from 1-hot targets back to the hidden activity of the penultimate hidden layer may be problematic with a small number of classes. The penultimate hidden layer's activity may pose challenges in inverse mapping. Simplified Difference Target Propagation (SDTP) modifies DTP by computing targets for the penultimate layer, eliminating biologically infeasible gradient communication. However, the diversity and precision of these targets need empirical validation. In the original DTP algorithm, training of forward and inverse model parameters involved alternating optimizations. A variant was considered where both losses are optimized in parallel, reflecting simultaneous plasticity changes in the brain. Autoencoder training in the brain is de-noising, unlike the noise-preserving loss used in the algorithm. In the brain, autoencoder training involves de-noising to account for added noise downstream. Convolution-based architectures are crucial for image recognition but biologically implausible due to extensive weight sharing. Weight sharing is necessary for implementing convolutions in biology. The \"locally connected\" receptive field structure of convolutional neural networks mimics biological processes by sampling small areas of visual space to create feature detectors. Evaluating learning methods on networks with locally connected layers helps assess the role of convolutions in enhancing learning. The study focused on assessing biologically-motivated learning methods rather than achieving state-of-the-art results, using reasonable architectures for specific tasks without exhaustive architecture searches. The experiments involved a hyperparameter search for fixed architectures with different learning algorithms. Locally-connected architectures were used with specific layer specifications and Adam optimization. All networks utilized hyperbolic tangent as a nonlinearity between layers. The study compared different architectures using hyperbolic tangent as a nonlinearity between layers. The evaluation was done on the MNIST dataset with a fully-connected network of 7 hidden layers and a locally-connected architecture with 4 hidden layers. The setup aimed to understand the methods' effectiveness in learning in deep networks prone to learning signal issues. The study compared different architectures on the MNIST dataset, showing that SDTP performed competitively with DTP and BP. Locally connected architectures performed well with all variants of target propagation, but did not match the test accuracy of convolutional networks. The observed improvement in generalization was attributed to locally-connected layers. The study compared different architectures on the CIFAR-10 dataset, showing that target propagation had noisier and slower learning compared to backpropagation. However, with early stopping and hyper-parameter tuning, it performed competitively. Backpropagation achieved worse test error due to overfitting to the training set faster. The CIFAR-10 dataset consists of 32x32 RGB images of 10 categories of objects in natural scenes. In contrast to MNIST, CIFAR-10 classes lack a \"canonical appearance\", making them harder to classify with simple template matching. The study used fully-connected and locally-connected networks on the dataset, with results showing that different target propagation variants performed similarly to backpropagation. The study compared the performance of locally-connected networks with fully-connected networks on the ImageNet dataset. Data augmentation was crucial for the locally-connected network to outperform the fully-connected network. The need for biologically-plausible architectures to match modern convolutional networks was highlighted. The study also evaluated the methods on the ImageNet dataset, showing the first empirical study of biologically-motivated methods and architectures on a large-scale benchmark. The study conducted the first empirical evaluation of biologically-motivated methods and architectures on the ImageNet dataset, which consists of 1271167 training examples with 1000 object classes. The locally-connected architecture used in the experiment was inspired by the ImageNet architecture but with modifications to reduce computational complexity. The study evaluated biologically-motivated methods on the ImageNet dataset with locally-connected networks. Models were trained for 5 days with poor performance compared to MNIST and CIFAR datasets. A difference between BP and TP variants was observed, possibly due to the need for careful hyperparameter tuning in deeper networks. The study compared biologically-motivated methods on ImageNet with locally-connected networks, noting differences in performance compared to MNIST and CIFAR datasets. Challenges with deeper networks in TP were highlighted, including the need for careful hyperparameter tuning and difficulties in learning in the output layer. Improving performance in TP was suggested for future work, leaving the task of matching BP performance on ImageNet for further research. Recent progress in machine learning has revived the debate on whether backpropagation (BP) can shed light on learning in the brain. A variant of difference target-propagation was introduced, removing gradient propagation and weight transport, and tested on classifying CIFAR and ImageNet images. This approach has not been tested on datasets that convinced the machine learning and neuroscience communities to revisit these questions. The study focused on classifying CIFAR and ImageNet images using networks trained with SDTP without weight sharing. While SDTP performed well on CIFAR, BP outperformed both DTP and SDTP on ImageNet. The need for further research to understand this discrepancy at scale was highlighted, along with the importance of addressing biological constraints in activity-propagation-based algorithms. The question of spiking neurons was set aside to focus on scaling TP variants to solve complex problems. The study focused on classifying CIFAR and ImageNet images using networks trained with SDTP without weight sharing. SDTP performed well on CIFAR, but BP outperformed both DTP and SDTP on ImageNet. The importance of addressing biological constraints in activity-propagation-based algorithms was highlighted, along with the need for further research to understand this discrepancy at scale. The question of spiking neurons was set aside to focus on scaling TP variants to solve complex problems. Additionally, the research aimed at gaining algorithmic insight into the brain without tackling all elements of biological complexity simultaneously. The results offer a new benchmark for evaluating the effectiveness of potential biologically plausible algorithms in more powerful architectures and on difficult datasets. The implementation of locally-connected layers in neural networks poses challenges due to the high number of trainable parameters compared to convolutional layers. To address this, reducing the number of output channels in each layer was explored in experiments, although this may impact performance. Additionally, when training locally-connected layers with target propagation, implementing the inverse computation is necessary for training feedback weights. Further research is needed to optimize locally-connected architectures for scalability. The implementation of locally-connected layers in neural networks requires the computation of feedback weights. The forward computation involves a linear transformation with a sparse structure matrix. To efficiently compute the inverse operation, a trick from deconvolutional architectures is used. This involves defining a forward computation and its transpose matrix to quickly compute the gradient. The gradient dz dx (and its multiplication with y) can be computed quickly using automatic differentiation in deep learning frameworks. Implementation details include optimizing parameters for DTP and SDTP, using denoising training for model inverses, and finding the best hyperparameters through random searches. The study compared noise-preserving and denoising training methods in deep learning frameworks. Results showed no significant difference in practice for DTP and SDTP. The learning dynamics for both methods converged to similar train and test errors at a similar speed."
}