{
    "title": "rJgRMkrtDr",
    "content": "This paper introduces a self-supervised learning approach for video features that outperforms existing methods in tasks like video classification, captioning, and segmentation. The method extends the BERT model to real-valued feature vectors, using noise contrastive estimation (NCE) instead of softmax loss. Cross-modal training with sequences of visual features and words from ASR is also explored for improved performance. In this paper, a new method for self-supervised representation learning for videos is proposed, utilizing speech transcripts from automatic speech recognition. By fine-tuning linear decoders with self-supervised video representations, state-of-the-art results are achieved in tasks like video classification, segmentation, and captioning. The approach is based on the BERT model, using the Transformer architecture for encoding long sentences and training with the \"masked language modeling\" objective. The VideoBERT model applies vector quantization to ensure discrete tokens in the sequence. The VideoBERT model applied vector quantization to video frames before passing them to the BERT model. Recent papers proposed measuring visual similarity between frames using pre-trained visual encoders. This paper introduces a method called \"Contrastive Bidirectional Transformer\" (CBT) for training bidirectional transformer models on sequences of real-valued vectors without pre-trained visual encoders. It also combines video frames with optional discrete tokens derived from ASR. The proposed method introduces a cross-modal transformer to maximize mutual information between sequences x and y at the sequence level. It is robust to misalignments between words and visual frames. Visual representations are encoded using a 3D convolutional neural network S3D and passed to the CBT model for self-supervised pretraining with the NCE loss. The proposed method introduces a cross-modal transformer to maximize mutual information between sequences x and y at the sequence level. It is robust to misalignments between words and visual frames. Visual representations are encoded using a 3D convolutional neural network S3D and passed to the CBT model for self-supervised pretraining with the NCE loss. The method involves training and evaluation of various components in a supervised and self-supervised manner on different tasks and datasets. The proposed method utilizes a cross-modal transformer to maximize mutual information between sequences x and y at the sequence level. Visual representations are encoded using a pretrained S3D network and passed to the CBT model for self-supervised pretraining with the NCE loss on the unlabeled HowTo100M dataset. The method outperforms previous self-supervised methods on video classification tasks, showing significant improvements in performance. The curr_chunk discusses the gains achieved with cross-modal pretraining in video representation learning, highlighting the limitations of existing methods and the emergence of self-supervised approaches. The curr_chunk discusses self-supervised context modeling for language representations, focusing on the BERT model's pre-training approach and its adaptation to continuous video data using a contrastive loss. The curr_chunk discusses using contrastive loss and mutual information maximization for representation learning, utilizing BERT for bidirectional context encoding across different modalities in videos. It also mentions the potential of cross-modal learning from synchronized visual and audio signals in videos for self-supervised video representation. The curr_chunk discusses weak supervision in video and language modalities, leveraging joint embedding spaces and alignment between visual and textual data. Recent approaches extend BERT architecture to learn visual-linguistic representations with fixed visual encoders and visual MLM objectives based on similarities. The proposed CBT is the first to show the effectiveness of BERT-style pre-training in a self-supervised manner for video. It extends the BERT model to sequences of video frames and discusses learning from both textual and visual data, even when not perfectly aligned. The BERT model takes in discrete tokens, embeds them into dense vectors, and emits output vectors capturing semantic information about the input sequence. The BERT model captures semantic information from input sequences by minimizing pseudo negative log likelihood. For images and videos, a softmax version of noise contrastive estimation loss is proposed. This extends BERT-style pre-training to video frames, utilizing a 3D CNN for frame embeddings. The NCE loss encourages the model to learn to identify the correct frame compared to negative distractors, maximizing mutual information between frames. This loss has been used in self-supervised visual representation learning, such as in DIM and CPC papers. The context predictor in the CBT method is a bidirectional transformer applied to video frames to learn representations from sequences of visual features and ASR words. Sequences may not be perfectly aligned, so maximizing mutual information at the sequence level is prioritized over the frame level. Encoding sequences using CBT and BERT, concatenating them, and passing them to a shallow model is the approach taken. The model uses BERT and a shallow cross-modal transformer to compute an MI-like score. Training involves maximizing mutual information between video and ASR sequences. The focus is on instructional videos where spoken words and visual content are related. The model utilizes BERT and transformers to estimate mutual information between video and ASR sequences. Experiments are conducted to evaluate the learned representations for various tasks like action anticipation and video captioning. Ablations are also considered, such as turning cross-modal training on or off and varying the size of the visual transformer. In this section, self-supervised visual representation learning is evaluated for action recognition. The model is pre-trained on unlabeled RGB-only Kinetics videos using S3D feature extractor followed by CBT. Sliding windows are taken from original YouTube videos, and S3D features are used as input to a linear classifier for training on various datasets. For evaluation, the model uses UCF101 and HMDB51 datasets for action recognition test accuracy. The CBT model is pre-trained using a curriculum learning strategy on unsupervised clips with S3D features. Fine-tuning is done on the last blocks of S3D with a visual transformer using the CBT visual loss, resulting in improved downstream task performance. During pre-training, the model uses 2 visual transformer layers, 4 attention heads, and a hidden unit size of 768. 60-second sliding windows from Kinetics videos are broken into 1.5-second clips with 6 out of 40 locations masked. Video frames are resized to 112 by 112 before encoding with S3D. The model is trained for 500K iterations with a batch size of 128 and learning rate of 1e-5. Comparison of pre-training strategies shows improved performance using CBT visual loss compared to Shuffle&Learn and 3DRotNet proxy tasks. Our method outperforms existing training methods by a large margin, as shown in the comparison to various state-of-the-art self-supervised methods. We surpass 2DCNN approaches like Shuffle&Learn and OPN, attributed to our more powerful 3DCNN architecture for video action recognition. Additionally, we outperform approaches using 3DCNN architectures similar to our S3D. Our method outperforms existing training methods by a large margin, surpassing 3DCNN architectures similar to our S3D and even beating the most recent approach, DPC, by significant margins on UCF101 and HMDB51 datasets. This is attributed to better contextual features learned using the transformer model and NCE loss in self-supervised training. For self-supervised pre-training, unlabeled videos from the HowTo100M dataset are used, containing 1.22M instructional videos covering 23k visual tasks. Visual features are extracted by resizing videos to 224 by 224 and computing features over sliding windows of 1.5 seconds using an S3D network pre-trained on the Kinetics dataset. For self-supervised pre-training, visual features are extracted from unlabeled videos using an S3D network. Feature embeddings are taken from the final Mixed5c block and averaged spatio-temporally to get vectors of size 1024. Text features are extracted by converting the audio track into sentences using the YouTube ASR API and an LSTM-based language model. BERT preprocessing steps are followed for encoding ASR tokens with a vocabulary of 30,000 tokens. The pre-trained BERT-base architecture is used for encoding with 12 layers of Transformers, 768 hidden units, and 12 attention heads. To pre-train CBT with a cross-modal objective, paired inputs are constructed by concatenating sentence segments from the HowTo100M dataset until reaching a length of 48 tokens. Visual features are retrieved from videos, with 6 out of 48 features masked randomly. The model is trained using 32 Cloud TPUs, a batch size of 128, Adam optimizer with a learning rate of 1e-5, and trained for 2 million iterations over 2 days. The model is trained for 2 million iterations over 2 days. Evaluation of pre-trained temporal representations through transfer learning to action anticipation, video captioning, and action segmentation tasks on various datasets. The model is trained for 2 million iterations over 2 days. Evaluation of pre-trained temporal representations through transfer learning to action anticipation, video captioning, and action segmentation tasks on various datasets. Videos with 200 human action classes are used as inputs, with video segments up to T c seconds before the actual action starts. The outputs are categorical labels. Input sequence length is fixed at 72 seconds, with features zero-padded for videos shorter than 72 seconds. The outputs of CBT are transformed features, with a linear classifier used to predict future actions. Visual transformers are fine-tuned with the linear classifier for 5 epochs. Text transformers and cross-modal transformers are not used during fine-tuning. During fine-tuning, the model is trained for 5 epochs with a batch size of 32 using the Adam optimizer and an initial learning rate of 1e-3. Top-1 accuracy is reported on test sets for Breakfast and 50Salads, and on the validation set for ActivityNet. Comparison to existing methods shows outperformance of self-supervised approaches and recent deep classifiers by a large margin. CBT significantly outperforms average pooling and LSTM baselines on 50Salads Breakfast, 50Salads, and ActivityNet datasets, showing the impact of video length on performance. CBT outperforms LSTM and AvgPool on all datasets, showing better performance with longer video lengths. Ablation study on action anticipation task shows impact of pretraining data set on performance. The impact of cross-modal training on performance is significant, especially on smaller datasets like Breakfast and Salads. Model performance initially increases with the number of layers and attention heads for the visual transformer, but then starts to decrease due to the smaller unlabeled pre-training set compared to NLP-BERT. The technique is scalable, with a visual transformer having 15M parameters compared to NLP-BERT's 110M parameters. The results of using learned temporal representation are shown in Table 4. The results of using learned temporal representation for video captioning and action segmentation are presented in Table 4. Video captioning results on the YouCook2 dataset are compared with previous state-of-the-art methods, while action segmentation results on the COIN dataset are compared using a linear classifier on CBT output features. The BERT model is extended to learn representations from video in a self-supervised manner without requiring vector quantization or pre-trained visual features. In this section, the method is applied to video captioning using pretraining on HowTo100M and training on the YouCook2 dataset. The model learns features that outperform existing self-supervised methods for various video tasks. The goal is to scale to larger unlabeled video datasets and surpass supervised video pretraining. The dataset used for pretraining contains 2000 Youtube videos with segmentation boundaries and captions. The captioning model is a transformer with 2 layers and a hidden layer size of 128, trained for 10K iterations with specific parameters. Results are evaluated using BLEU, METEOR, and ROUGE metrics, showing superior performance compared to a simple baseline. Our model outperforms baselines and previous approaches on various metrics. The comparison to VideoBERT highlights the importance of fine-grained video representation. The difference between CBT and VideoBERT is smaller for YouCook2 dataset, possibly due to similar pre-training data. Our model is applied to temporal action segmentation using features pretrained on HowTo100M and trained on COIN dataset. The COIN dataset contains instructional Youtube videos with segment boundaries and class labels. Video features are extracted using S3D and fed to a visual transformer. A linear classifier is used for training with a batch size of 32 and Adam optimizer. The model is compared to existing approaches using frame accuracy as a metric. The model outperforms various state-of-the-art approaches in frame accuracy by a large margin (+19.6 points), including (Tang et al., 2019) and (Ding & Xu, 2018)."
}