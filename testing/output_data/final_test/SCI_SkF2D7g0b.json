{
    "title": "SkF2D7g0b",
    "content": "Existing black-box attacks on deep neural networks have largely focused on transferability, where an adversarial instance generated for a locally trained model can attack other learning models. This paper introduces Gradient Estimation black-box attacks for adversaries with query access to the target model\u2019s class probabilities, without relying on transferability. The proposed attack achieves close to 100% success rates for both targeted and untargeted attacks on DNNs, outperforming transferability-based attacks on MNIST and CIFAR-10 datasets. The study introduces Gradient Estimation black-box attacks, achieving high success rates on MNIST and CIFAR-10 datasets and against real-world classifiers. These attacks are effective even against state-of-the-art defenses, highlighting the challenges posed by adversarial examples in machine learning systems. In some commercial AI offerings, adversaries may only have black-box access to the model, limiting their knowledge of the learning system's parameters. Existing black-box attacks on DNNs have focused on transferability-based attacks, but exploration of other strategies is lacking in the literature. In this paper, new black-box attacks are designed using limited query access to learning systems to achieve high adversarial success rates. The proposed Gradient Estimation attacks on DNNs do not require access to a dataset or knowledge of the target model architecture. The adversary adds perturbations proportional to the estimated gradient, different from white-box attacks. The code to reproduce the results can be found at the provided link. In this study, Gradient Estimation attacks were conducted on DNNs without needing access to the dataset or target model architecture. Two query-reduction strategies were proposed: random feature grouping and PCA-based reduction. The attacks achieved high success rates of up to 90% for single-step attacks and 100% for iterative attacks on MNIST and CIFAR-10 datasets with just 200 to 800 queries. The Gradient Estimation attacks achieved high success rates with minimal queries, outperforming related attacks. It does not require training a local model, making it more efficient for real-world systems. Additionally, a practical black-box attack was conducted on NSFW and Content Moderation models by Clarifai. The Safe For Work (NSFW) classification and Content Moderation models developed by Clarifai have been deployed for real-world moderation, making black-box attacks especially harmful. Successful attacks have been demonstrated with minimal queries per image, showing the target model classifying adversarial images as 'safe' despite visible content that needs moderation. The full set of images used for experimentation can be found at a provided link. The study evaluates various black-box attacks on MNIST and CIFAR-10 datasets, including Gradient Estimation attacks which outperform others in attack success rate. The effectiveness of these attacks on DNNs with adversarial training is also assessed, showing vulnerability to iterative attacks despite some robustness against single-step attacks. The study evaluates black-box attacks on MNIST and CIFAR-10 datasets, showing vulnerability to iterative Gradient Estimation attacks despite some robustness against single-step attacks. Existing black-box attacks use genetic algorithms or hill climbing algorithms for crafting adversarial samples, but these methods are costly for high-dimensional data like images. BID13 proposed using queries to a target model to train a local surrogate model for generating adversarial samples. The only previous literature on query-based black-box attacks in deep learning is by BID10. They propose a greedy local search method using 500 queries per iteration, resulting in 75,000 queries per image. Our methods achieve higher attack success rates on MNIST and CIFAR-10 compared to BID10. Another black-box attack method named ZOO approximates the Adam optimizer. In this section, the paper introduces notation and evaluation setup for the attacks on classifiers. A classifier f(\u00b7; \u03b8) maps from X to Y, where Y = {0, 1} for binary classification. The parameters \u03b8 are associated with the classifier, and H represents the constraint set for adversarial samples. The paper introduces notation and evaluation setup for attacks on classifiers, focusing on neural networks. It defines the constraint set for adversarial samples and introduces notation specific to neural networks, such as logits and softmax layers. The empirical evaluation is conducted on state-of-the-art neural networks using the MNIST and CIFAR-10 datasets. The paper provides details on datasets, model architectures, and training for attacks on classifiers. Results for untargeted attacks are in the main body, while targeted attacks are in an appendix. Two loss functions, cross-entropy and logit-based, are used. Adversarial perturbations are constrained using L \u221e distance. Baseline black-box attacks and transferability-based attacks are also discussed in the appendices. Multiple attacks were evaluated on the MNIST dataset with scaled pixel values. Four models, A to D, were trained for the evaluation. In the study, four different models (Models A to D) were trained on the MNIST dataset, with perturbation budgets varied for attacks constrained by L \u221e distance. For the CIFAR-10 dataset, three model architectures were chosen, and perturbation budgets were also varied. Standard metrics were used to evaluate attack strategies, with metrics computed for single-step attacks on the MNIST dataset. For CIFAR-10 data, evaluations of targeted attacks involve choosing a random target T for each sample except the true class y. The attack success rate is the main metric, while average distortion is also considered using L2 distance between benign and adversarial samples. The L2 distance metric is used to compare distortion in attacks with similar success rates. The number of queries in black-box attacks impacts the attack cost. Query feedback allows adversaries to iteratively generate effective adversarial examples. The study explores various methods for black-box attacks, including Particle Swarm Optimization and Simultaneous Perturbation Stochastic Approximation. These methods were not effective, leading to the proposal of a Gradient Estimation black-box attack based on finite differences. This approach proved to be very effective in generating adversarial examples. The study proposes a Gradient Estimation black-box attack based on the method of finite differences to reduce the number of queries needed. The threat model assumes the adversary can obtain output probabilities for any input, allowing them to recover logits up to an additive constant. For untargeted attacks, the adversary only needs access to the output probabilities for the two most likely classes. The threat model assumes adversaries can query cloud-based ML services to carry out Gradient Estimation attacks. Adversarial examples created using this attack can be used against any client of the MLaaS provider. The method of finite differences is used for Gradient Estimation based attacks. The analysis and results focus on untargeted attacks but can be extended to targeted attacks easily. The finite difference method is used for Gradient Estimation attacks in the context of cloud-based ML services. It provides a two-sided estimation of the gradient of a function with respect to a vector x. The method is useful for black-box adversaries aiming to approximate a gradient-based attack, allowing estimation of the gradient with access to only function values. In untargeted attacks, the gradient is typically taken with respect to the cross-entropy loss between the true label of the input and the softmax probability vector. The FD-xent method generates adversarial samples by estimating the gradient of the loss using softmax probabilities. A loss function based on logits is also used for white-box attacks, with a confidence parameter \u03ba to control the strength of the perturbation. The loss function for untargeted adversarial attacks in white-box scenarios involves computing logit differences using the finite differences method. In black-box attacks with query-access to softmax probabilities, a similar approach is used. In black-box attacks with query-access to softmax probabilities, the adversarial sample is generated using the FD-logit attack. Table 1 shows the success rates of different attack methods on the model, with \u2206(X, X adv ) representing the average distortion. Gradient Estimation using Finite Differences matches white-box attacks' performance. The iterative variant of the gradient-based attack achieves higher success rates in the white-box setting. The iterative attack with estimated gradients, IFD-xent, and IFD-logit, outperforms single-step attacks. The most successful black-box attack is the Gradient Estimation attack using Finite Differences with the logit loss (FD-logit). The attack with query reduction using PCA (GE-QR) is also discussed. The Gradient Estimation attack with Finite Differences (FD-logit) is a successful untargeted single-step black-box attack for MNIST and CIFAR-10 models, outperforming transferability-based attacks. The Iterative Gradient Estimation attack with Finite Differences (IFD-logit) achieves 100% adversarial success rate across all models on both datasets. In targeted black-box attacks, IFD-xent-T achieves 100% adversarial success rates on most models, while FD-xent-T achieves about 30% success rates, similar to white-box attacks like FGS-xent-T and FGS-logit-T. Parameter choices include using \u03b4 = 1.0 for FD-xent and IFD-xent, and \u03b4 = 0.01 for FD-logit and IFD-logit. A larger \u03b4 value is needed for xent loss based attacks due to the less sensitivity of probability values in xent loss compared to logit loss. The Iterative Gradient Estimation attacks use \u03b1 = 0.01 and t = 40 for MNIST, and \u03b1 = 1.0 and t = 10 for CIFAR-10. This results in 62720 queries for MNIST and 61440 queries for CIFAR-10 per sample. Query reduction methods can achieve similar success rates with fewer queries. Black-box attacks require a large number of queries per adversarial sample. The number of queries needed for adversarial attacks is a concern due to high dimensions. Two techniques are explored to reduce queries by estimating gradients for groups of features. Feature grouping is justified by the relation between gradients and directional derivatives. The estimated gradient of any function can be computed using techniques to group features for estimation, reducing the number of queries needed for adversarial attacks. The simplest way is to choose a random set of features without replacement, with detailed algorithms provided in Appendix F. The estimated gradient of a function can be computed by selecting a set of indices to determine the directional derivative, which is an average of partial derivatives. To reduce the number of queries needed for adversarial attacks, directional derivatives along principal components from PCA can be computed. This requires access to representative data. More details on PCA and the Gradient Estimation attack using PCA components for query reduction are provided in Appendix F.2. The gradient estimation in Algorithm 2 approximates the true gradient using the sum of projections along the top k principal components. Adversarial attacks can reduce the number of queries needed by using directional derivatives along principal components from PCA. This approach can lower the queries to 2tk (k < d) to minimize the cost of iterative attacks. Gradient Estimation attacks with query reduction, specifically using PCA-based technique, maintain high success rates with close performance to other methods on MNIST and CIFAR-10 datasets. Iterative Gradient Estimation attacks achieve close to 100% success rates for untargeted attacks and above 80% for targeted attacks on Model A for MNIST. The effectiveness of gradient estimation attacks across models, datasets, and adversarial goals is shown in FIG3. Random grouping is not as effective as PCA-based method for Single-step attacks, but is equally effective for iterative attacks. Particle Swarm Optimization (PSO) was found to be slow and ineffective for constructing adversarial samples. The Simultaneous Perturbation Stochastic Approximation (SPSA) method, similar to Finite Differences, estimates the gradient of the loss along a random direction at each step. However, it requires a large number of steps to generate adversarial samples and has lower success rates compared to Gradient Estimation attacks on the MNIST dataset. SPSA is sensitive to the choice of parameters and results in higher distortion with lower attack success rates. The comparative evaluation of query-based black-box attacks for the MNIST dataset is presented in Table 2. The PSO based attack uses class probabilities for the loss function, outperforming the logit loss. The IGE-QR (RG-k, logit) attack balances speed and success. Detailed evaluation results are in Appendix I, covering baseline attacks, dimension effects, single-step attacks on defenses, and Gradient Estimation attack efficiency. Adversarial examples are shown in Appendix H. Black-box attacks against defenses based on adversarial training are evaluated, with details on adversarially trained models in Appendix B. The focus is on adversarial training based defenses to improve DNN robustness. Real-world attacks on Clarifai models are conducted, with a focus on Iterative Gradient Estimation attacks. Results show that Gradient Estimation attack with Finite Differences is the most effective single-step black-box attack on adversarially trained models on MNIST. Variants of Model A are trained with 3 adversarial training strategies using an L \u221e constraint of 0.3. The text describes adversarial training strategies for improving DNN robustness, focusing on attacks using an L \u221e constraint of 0.3. Different models are trained with various adversarial samples, including iterative black-box attacks. Results show high success rates for attacks against adversarially trained networks, with the best transferability attack achieving a success rate of 4.9%. The text discusses the success rates of black-box attacks against adversarially trained networks, with the best transferability attack achieving a success rate of 4.9%. Model A adv-iter-0.3 shows robustness against iterative attacks, with a black-box attack success rate of 14.5%. Additionally, the Iterative Gradient Estimation attack using PCA achieves high adversarial success rates against Model A adv-0.3 with just 4000 queries per sample. Iterative black-box attacks achieve high success rates against adversarially trained models for CIFAR-10. IFD-logit achieves 100% attack success rates against Resnet-32 adv-8 and Resnet-32 adv-ens-8, slightly dropping to 97% with IFD-QR (PCA-400, logit). IFD-QR (PCA-400, logit) also achieves a 72% success rate for targeted attacks. Resnet-32 adv-iter-8 has poor performance on both benign and adversarial samples, with an accuracy of 79.1% on benign data. IFD-xent achieves a 55% untargeted attack success rate on this model. Both single-step and iterative variants of the Gradient Estimation attacks outperform other black-box attacks on MNIST and CIFAR-10 datasets, achieving high success rates even on adversarially trained models. The attacks only require query-based access to the target model, making them applicable to public systems like Clarifai. Clarifai offers black-box access to models for practical applications like NSFW content detection and Content Moderation. Adversarial samples pose a threat, where attackers could manipulate images to evade classification. The Gradient Estimation method is used to evaluate attacks on Clarifai's models, querying the API for confidence scores on categories. Random grouping reduces queries for efficiency. The random grouping technique is used to reduce queries and the logarithm of confidence scores is taken for logit loss. Successful attack images can be found at a provided link, but are not included in the paper due to offensive content. An example attack on the Content Moderation API is shown, where an original image of drugs is classified differently after an adversarial image is generated with 192 queries. The Content Moderation model now classifies images as 'safe' with a confidence score of 0.96. The proposed Gradient Estimation attacks can generate adversarial examples that fool real-world systems without prior knowledge. The attacks achieve high success rates and outperform other black-box attacks. Random grouping and PCA methods reduce the number of queries needed. The effectiveness of the Gradient Estimation attack is demonstrated against a real-world classifier and defenses. Existing methods for generating adversarial examples involve adding small perturbations to benign samples to cause misclassification in targeted or untargeted attacks. Two baseline black-box attacks, random perturbations, and other methods can be carried out without knowledge of the target model. Defenses against Gradient Estimation attacks are crucial due to their high effectiveness in various settings. The attack involves adding random perturbations to input data, which can be generated by any distribution chosen by the adversary. These perturbations are not targeted and are denoted as Rand. While not optimal for DNNs, they serve as a baseline for comparison. Adversaries with access to training or test sets can carry out this attack, generating adversarial samples with L \u221e constraints. The attack involves adding random perturbations to input data, generated by any chosen distribution. Adversarial samples are created with L \u221e constraints, using methods based on iterative or single-step gradient minimization of neural network loss functions. The adversary needs knowledge of the model's gradient and access to a local model f s for generating adversarial samples. Transferability-based attacks involve generating adversarial samples for a local model f s, which can then be transferred to a target model f t. The Fast Gradient method constructs adversarial samples by performing a single step of gradient ascent for untargeted attacks, using L \u221e constraints. Iterative Fast Gradient methods are multi-step variants of this approach. Iterative Fast Gradient methods are multi-step variants of the Fast Gradient method, where the gradient of the loss is added to the sample for t + 1 iterations. These methods essentially carry out projected gradient descent with the goal of maximizing the loss. Black-box attacks assume the adversary has access to training data to train a local model. Adversarial samples transfer between networks, leading to the proposal of black-box attacks. The proposal of black-box attacks involves an adversary generating samples for a local network and transferring them to the target model, known as a Transferability based attack. These attacks use a surrogate local model to craft adversarial samples for causing misclassification. Different attack strategies can be used to generate adversarial instances against the target model. The ensemble of local models is used to compute the ensemble loss for attacks, with weights assigned to each model. Transferability attacks in the ensemble setting perform well in targeted attacks, while attacks with a single local model are usually effective for untargeted attacks. Adversarial samples are crafted using gradient directions from the ensemble to cause misclassification. Adversarial training methods like BID18 and BID0 modify the loss function to penalize neural networks during training for adversarial samples. Ensemble adversarial training, introduced by BID21, involves training the network with adversarial samples from multiple networks. Iterative adversarial training suggests training with adversarial samples generated using iterative methods like iterative FGSM. The MNIST dataset consists of 60,000 training examples and 10,000 test examples of handwritten digits. Each image is 28x28 pixels in grayscale, normalized, and centered. CIFAR-10 dataset contains color images from 10 classes. The models are trained on both datasets, with a focus on adversarial training using iterative methods like iterative FGSM attack. The accuracy of models A, B, C, and D on MNIST and CIFAR-10 datasets is compared. Models A and C have convolutional and fully connected layers with similar parameter magnitudes. Model B lacks fully connected layers and has fewer parameters. Model D lacks convolutional layers and has the fewest parameters. Models A, B, and C achieve over 99% accuracy on test data, while Model D achieves 97.2% due to the absence of convolutional layers. Adversarial training involves batches with equal numbers of benign and adversarial samples, with a weighting factor of 0.5. For ensemble adversarial training, FGSM samples are randomly chosen for each batch. Networks are trained for 12 epochs with standard and ensemble adversarial training, and 64 epochs with iterative adversarial training. Resnet-32 is a standard 32 layer ResNet, while Resnet-28-10 is a wide ResNet with 28 layers and width set to 10. Std.-CNN has two convolutional layers, max-pooling, normalization layers, and fully connected layers with weight decay. Each model architecture is trained in three ways: on CIFAR-10 data only, with standard adversarial training, and with ensemble adversarial training. The Resnet-32 and Resnet-28-10 models outperform the Std.-CNN model in accuracy on CIFAR-10 test set. Resnet-32 achieves 92.4% accuracy, Resnet-28-10 achieves 94.4%, while Std.-CNN only reaches 81.4%. Models were trained with batch size of 128 and Resnet-32 adversarial variant was trained for 80,000 steps. The fraction of inputs meeting the adversary's objective is measured based on correctly classified samples. This fraction represents truly adversarial samples misclassified due to perturbations, not the classifier's failure to generalize. Active queries to the model are assumed in a unified framework, with existing attacks making zero queries as a special case. The adversary iteratively adds perturbations based on a constraint set until the desired outcome is achieved. The adversarial constraint set H is used to iteratively add perturbations until the desired query results are obtained, generating the corresponding adversarial example x adv. Targeted and untargeted black-box attacks are defined based on this framework. Targeted attacks aim for a specific class T, while untargeted attacks aim to misclassify the input instance x. In zero-query attacks, the adversary aims to satisfy a target class without making any queries to the classifier. Various zero-query attacks have been conducted with different success rates. Targeted white-box and Gradient Estimation attacks are discussed, along with targeted transferability attacks using locally generated adversarial samples. Adversarial samples are generated using the Fast Gradient Sign (FGS) attack for targeted classes. The adversarial samples are generated using iterative FGS for targeted classes. Targeted black-box adversarial samples are created using Gradient Estimation method. The algorithm for query reduction using random grouping is detailed for Gradient Estimation with query reduction. Principal components of data samples are eigenvectors of a positive semidefinite matrix, decomposed into orthogonal matrix U and diagonal matrix \u039b. PCA minimizes reconstruction error using L2 norm, providing a basis for minimizing Euclidean distance in sample reconstruction. In this work, a taxonomy of black-box attacks based on the number of queries on the target model is proposed. Successful untargeted adversarial samples against Model A on MNIST and Resnet-32 on CIFAR-10 are shown, with minimal perturbation added by iterative attacks. White-box attack results for various cases are presented, matching previous work by Kurakin et al. (2016). The study presents successful untargeted adversarial attacks on Model A and Resnet-32 using minimal perturbations. The attacks utilize random perturbations chosen from a normal distribution and perturbations aligned with the difference of means. All attacks use the logit loss function. The study demonstrates successful untargeted adversarial attacks on Model A and Resnet-32 with minimal perturbations using logit loss. Single-step attacks result in smaller perturbations than iterative attacks. MNIST's '7' is misclassified as a '3' by single-step attacks and as a '9' by iterative attacks. CIFAR-10's dog is misclassified as a bird by white-box FGS and Finite Difference attacks, and as a frog by Gradient Estimation with query reduction. Adversarial samples from Rand. have lower success rates despite similar distortion levels compared to other black-box attacks on MNIST and CIFAR-10 models. The D. of M. method is effective at higher perturbation values for MNIST. The D. of M. attack is more effective than FD-xent for Models B and D on the MNIST dataset. However, for Model D, it outperforms transferability based attacks considerably. On CIFAR-10, Rand. outperforms D. of M. significantly as perturbation is increased. Models trained on MNIST have normal vectors aligned with the difference of means, while models on CIFAR-10 do not. Transferability experiments are conducted from Model B for MNIST and Resnet-28-10 for CIFAR-10. The untargeted white-box attacks for models with adversarial training show varying success rates and distortion levels. Adversarial samples generated using single-step methods and transferred between models have higher success rates when using logit loss compared to cross entropy loss. Targeted attacks have lower success rates than untargeted attacks, even with iteratively generated samples. The transferability rate of adversarial samples varies across models, with the highest rate achieved by IFD-xent-T at 100.0%. Using an ensemble of local models can improve transferability, but the rate may decrease if models have different architectures. It is crucial to use a local surrogate model similar to the target model for high attack success rates. The transferability rate of adversarial samples varies across models, with the highest rate achieved by IFD-xent-T at 100.0%. Using an ensemble of local models can improve transferability, but the rate may decrease if models have different architectures. It is important to use a surrogate model similar to the target model for achieving high attack success rates, as shown in experiments with Gradient Estimation attacks on different datasets. The effectiveness of single-step black-box attacks on adversarially trained models is analyzed in this section. The Gradient Estimation attacks using Finite Differences with random perturbations outperform other black-box attacks. Using PCA-based approach GE-QR, higher adversarial success rates are achieved with fewer principal components. The effectiveness of single-step black-box attacks on adversarially trained models is analyzed. Gradient Estimation attacks with random perturbations outperform other attacks, achieving the highest success rates. The attacks that perform the best against Resnet-32 are Random Perturbations, Difference-of-means, and Transferability attack from Resnet-28-10. An interesting effect is observed at = 4, where the adversarial success rate is higher than at = 8 due to overfitting. Gradient Estimation attack closely tracks the adversarial success rate of white-box attacks. Increasing effectiveness of single-step attacks using initial random perturbation. The addition of a random perturbation significantly improves the adversarial success rate of FD-logit on Model A adv-0.3, outperforming white-box attacks. This effect is also seen on Model A adv-ens-0.3, but Model A adv-iter-0.3 is resistant to single-step attacks. The attacks work well on DNNs with standard and ensemble adversarial training, achieving performance levels close to white-box attacks. On a GPU, models were run with a batch size of 100. Single-step attacks on Model A on MNIST data take around 6.2 \u00d7 10 \u22122 to 8.8 \u00d7 10 \u22122 seconds per sample. Iterative attacks without query reduction take about 2.4 to 3.5 seconds per sample. With query reduction, the time taken is just 0.5 seconds per sample. The fastest attack, ZOO-ADAM, takes around 80 seconds per sample for MNIST, much slower than other attacks. For Resnet-32 on CIFAR-10 dataset, various attacks take different times per sample. Iterative attacks with 10 iterations take roughly 30s per sample. IGE-QR with 10 iterations takes just 5s per sample. The fastest attack for CIFAR-10 dataset takes about 206 seconds per sample, much slower than other attacks. Parallelizing queries gives a 2 \u2212 4\u00d7 speedup. The model loaded on a single GPU limits speedup potential. Further optimization can achieve greater speedups. Adversarial attacks are efficient, allowing for quick generation of samples."
}