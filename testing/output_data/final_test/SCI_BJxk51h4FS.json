{
    "title": "BJxk51h4FS",
    "content": "Variational inference using chi-square divergence minimization (CHIVI) approximates a model's posterior and provides an upper bound on the marginal likelihood. However, CHIVI's performance is sensitive to initialization, requires a large number of samples, and may not always be a reliable upper bound. Suggestions for detecting and addressing these issues are provided in this empirical study on CHIVI's performance in synthetic inference tasks. Marginal likelihoods are used for computing posterior probabilities of model parameters and Bayesian model selection. Variational inference offers a scalable approximation by choosing a simple family of approximate distributions and optimizing their parameters to minimize divergence from the true posterior. Minimizing a \u03c7 2 divergence leads to a chi-divergence upper bound (CUBO), which can be combined with lower bound estimates to \"sandwich\" the model evidence for decision making. In practice, CUBO optimization for latent variable models can be challenging due to the need for Monte Carlo estimators to approximate integrals, leading to unreliable results with large variance even at modest sample sizes. Empirical evidence suggests that the bound may end up being too loose, especially when minimizing \u03c7 2 divergence using MC gradient estimates. The CUBO optimization for latent variable models can be challenging due to unreliable Monte Carlo estimators, leading to large variance even with modest sample sizes. The joint distribution of observed variables x and latent variables z is approximated through variational inference (VI) by minimizing the KL divergence D KL q(z; \u03bb)||p(z|x) to maximize the evidence. Our contributions include evaluating CUBO in simple scenarios, comparing it to other bounds, and reviewing best practices for testing new bounds. Minimizing KL divergence maximizes evidence lower bound (ELBO) on model evidence. \u03c7 2 variational inference minimizes \u03c7 2 divergence. Monte Carlo samples used for biased estimate. Two optimization strategies considered using reparametrization trick. Simple inference scenario: minimizing divergence between two univariate Gaussian. In a simple inference scenario, the divergence between two univariate Gaussian distributions is minimized. Two cases are considered: fixed variance with varying mean, or fixed mean with varying variance. Experiments were conducted using stochastic gradient descent and grid-searching the learning rate. The evolution of variational parameters over time is shown, with KL trajectories converging to true values while ChiSq parameters fail to converge in some cases. The CUBO MC estimator shows problematic behavior with 1M samples, with values below the true marginal likelihood. Warm initializations are recommended for CUBO optimization, especially for the Latent Dirichlet Allocation topic model. In single-document inference for Latent Dirichlet Allocation (LDA) topic model, the document length impacts posterior uncertainty about topics. With K=3 topics and V=3 vocabulary words, topic-word probabilities are given. Each document is represented by word counts and generated via a mixture of topics using variational inference. Two tasks are explored: estimating upper bounds on marginal likelihood with fixed q, and optimizing q to improve bounds. Four possible q distributions are fitted for upper bound estimation. The experiments conducted compare different optimization strategies for fitting Normal distributions in the context of Latent Dirichlet Allocation (LDA) topic modeling. The impact of initialization and optimization methods is analyzed through detailed experiments, highlighting issues with cold initializations in CHIVI parameter estimation. The experiments compare optimization strategies for fitting Normal distributions in LDA topic modeling. Issues with cold initializations in CHIVI parameter estimation are highlighted. CUBO estimators are overconfident, while KLpq estimators are better behaved. ELBO optimization followed by CUBO computation may be sufficient, suggesting direct optimization of CUBO may not always be necessary. Topic model case study in Table 2 shows bounds on marginal likelihood for a \"short\" toy document under an LDA topic model. In a topic modeling experiment, optimization strategies for fitting Normal distributions are compared. Issues with cold initializations in CHIVI parameter estimation are noted. CUBO estimators are overconfident, while KLpq estimators are more reliable. The experiments involve computing upper bounds on marginal likelihood using different estimators. The KLpq bound is considered reliable but computationally expensive. The \"KLpq\" upper bound can be approximated using S samples from the posterior in an LDA model. Samples are computed from a Hamiltonian Monte Carlo posterior using Stan. The entropy of the distribution is calculated, and a reparameterization trick is used to simplify the expectation term. The transformation from a real vector to the K-dimensional simplex via softmax is one-to-one invertible and differentiable. To generate a probability vector, \u03c0d, in an overcomplete representation of the K-dimensional simplex, a standard normal vector is drawn, scaled with parameters, and transformed using softmax. This overcomplete space includes K free parameters and an additional scalar random variable to augment the probability vector. The text discusses creating an invertible transformation between two K-length vectors, u and the augmented pair \u03c0, w. It explains the process of computing the Jacobian and determining its determinant as a function of \u03c0 and w. The number of swaps needed to create the Jacobian is always an even number, with each swap changing the sign but not the value of the determinant. The text discusses the Schur determinant formula for computing determinants of square matrices by manipulating subcomponent blocks. It also introduces the Jacobian and its determinant for variational inference. The approximate posterior for a document-topic vector is computed using an overcomplete logistic normal family. Samples can be drawn from this posterior in two steps, leading to a log probability density function over the joint space of \u03c0 and w. The generative model does not include the log-scale variable w d, but it can be given a N(0,1) prior and kept decoupled from the data."
}