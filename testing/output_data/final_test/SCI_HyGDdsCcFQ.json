{
    "title": "HyGDdsCcFQ",
    "content": "Memorization in over-parameterized neural networks can harm generalization due to mislabeled examples in large datasets. To address this, stochastic gradient descent with large learning rates is used to separate clean and mislabeled examples based on loss statistics. On-the-fly Data Denoising (ODD) algorithm is introduced to identify and discard mislabeled examples efficiently with minimal computational overhead. Empirical results show ODD's effectiveness on datasets with mislabeled examples, showcasing the generalization properties of over-parametrized deep neural networks. In this paper, the On-the-fly Data Denoising (ODD) method is proposed as a simple and robust approach to training with noisy data. The algorithm aims to improve generalization in large labeled datasets by efficiently identifying and discarding mislabeled examples. On-the-fly Data Denoising (ODD) is a method for training with noisy examples using stochastic gradient descent. It involves training residual networks with large learning rate schedules, identifying and removing mislabeled examples based on loss thresholds, and continuing training until convergence. ODD outperforms previous methods on datasets with artificial or real-world noise while maintaining accuracy on clean datasets. Our method, On-the-fly Data Denoising (ODD), is robust to hyperparameters and artificial noise levels. It can detect mislabeled examples in datasets without additional supervision. The goal of supervised learning is to find a function that describes the probability of a label given an input vector. The training dataset consists of correctly labeled examples and mislabeled examples. Our method, On-the-fly Data Denoising (ODD), aims to learn the function f from labeled examples sampled from P(X, Y) without knowledge about mislabeled examples sampled from another distribution Q(X, Y). Training on mislabeled examples can lead to undesired behavior of f, affecting generalization. Training only on mislabeled examples can significantly decrease validation error compared to training on clean data. Stochastic gradient descent with large learning rates can help separate mislabeled examples from clean ones. This is demonstrated in training deep residual networks on CIFAR-100 and ImageNet with different percentages of label noise. The implicit regularization of gradient descent is crucial for achieving better generalization on clean datasets. Large learning rates encourage solutions that are more robust to perturbations and less likely to overfit mislabeled examples. Classifying correct and mislabeled examples through loss statistics can help achieve better generalization. To improve generalization, selecting a reasonable threshold for classification is crucial. High thresholds may include too many examples from B, while low thresholds could prune too many examples from G. The loss distribution for B remains stable with different ratios of |B|/|D|, indicating little progress with a large learning rate. Characterizing the loss distribution of uniform label noise can be done through a generative procedure. The loss distribution of uniform label noise can be characterized through a generative procedure. An identity covariance matrix for x explains the noise distribution, which can skew left or right based on model behavior. This characterization is validated on CIFAR-100 and ImageNet datasets. A threshold can be defined using the p-th percentile of p n (l) to determine how many examples in B to retain if Q(Y |X) is uniform. The method discussed in Section 4.4 can identify different percentages of uniform label noise with high precision. On-the-fly Data Denoising (ODD) is introduced as a simple algorithm to remove harmful examples and improve generalization. ODD involves training all examples with large learning rates for E epochs, computing the p-th percentile denoted as Tp, removing examples with high average loss, and continuing training with the remaining examples. ODD introduces three hyperparameters: E, p, and h to determine the amount of training, trade-off between noisy and clean examples, and window of averaged loss statistics. The method introduced in Section 4.4, On-the-fly Data Denoising (ODD), uses a window of averaged loss statistics to reduce variance from data augmentation. It does not estimate noise in the dataset or assume a specific noise model. The threshold Tp can accurately predict the portion of noise in the dataset, even for different types of label noise. ODD is compatible with existing learning rate schedules and can improve generalization by removing harmful examples. In the presence of mislabeled examples, changes in optimization landscape could result in bad local minima. Larger batch sizes could mitigate this effect. Different methods exist for training with noisy labels, such as estimating noise distribution or cleaning noisy examples through predictions of auxiliary networks. Our method leverages the implicit regularization effect of SGD to identify noisy examples. Our method, ODD, leverages SGD to identify noisy examples without relying on noise confusion matrix estimations or additional trusted examples. It removes noisy examples on-the-fly with little computational overhead compared to standard SGD training. Evaluation on CIFAR-10, CIFAR-100, ImageNet, and WebVision datasets shows promising results. In our experiments, we exclude dropout BID29 and model ensembles BID12. We use h = 2 for ODD experiments, finding similar results with h \u2208 [2, 5]. Training on CIFAR-10 and CIFAR-100 datasets involves data augmentations like horizontal flips and random crops. We train the WRN-28-10 architecture for 200 epochs with specific parameters and a cosine annealing schedule for better performance. In experiments, different training schedules are compared for performance on clean datasets. Label noise is introduced by randomly replacing a percentage of training labels. Various baselines are used for comparison, including ORACLE, ERM, MENTORNET, REN, mixup, and Generalized. Neural networks trained with Generalized Cross Entropy (GCE) outperform other algorithms in the presence of artificial noise. ODD distinguishes mislabeled examples and improves generalization, even outperforming ERM in some cases. ODD prevents overfitting to noise and shows promising results in learning curves compared to ERM under label corruption. ODD outperforms ERM in handling label corruption by distinguishing mislabeled examples and improving generalization. Experiments show that ODD continues to improve generalization even as the learning rate decreases. Additionally, ODD shows promising results in learning curves compared to ERM under label corruption. In experiments comparing ERM and ODD on datasets with mislabeled examples, ODD consistently outperforms ERM. ODD is also effective in settings where the ratio of classes varies, such as in CIFAR-20 and CIFAR-50 tasks. Results in TAB2 show ODD's superiority over ERM when input examples are not uniformly distributed. ODD outperforms ERM in datasets with mislabeled examples, including the ImageNet-2012 classification dataset. Experiments show ODD's superiority in terms of top-1 and top-5 errors on mislabeled examples, while remaining competitive with clean data. The study verifies the effectiveness of their method on a real-world noisy dataset, WebVision-2017 BID17, containing 2.4 million noisy labels. Two architectures, ResNet-50 and Inception ResNet-v2 BID32, are trained without using a pretrained model or additional labeled data from ImageNet. Their ODD method removes around 9.0% of examples with ResNet-50 and 9.3% with Inception ResNet-v2 BID32 during training. Results show that their method outperforms ERM and MENTORNET on noisy training datasets. The study evaluates the ODD method on a noisy dataset, WebVision-2017, showing it outperforms ERM and MENTORNET. Results indicate ODD's ability to generalize well even with noisy examples, with optimal trade-off at p = 10. Sensitivity to p is low when there is no artificial noise, and p = 10 represents the best trade-off. The study evaluates ODD on CIFAR with input-agnostic label noise. Results show ODD can separate noisy and clean examples with small noise levels. Training error reflects the amount of noise in the dataset, demonstrating ODD's ability to characterize noise regardless of its percentage. ODD is a method for robust training with mislabeled examples, utilizing the implicit regularization effect of stochastic gradient descent to prune harmful examples. It outperforms related methods on various datasets with mislabeled examples and remains competitive with ERM on clean datasets. ODD can automatically detect mislabeled examples in CIFAR-100, opening up research directions for implementing robust algorithms. In addition to existing experiments, additional experimental results for ORACLE, ERM, and ODD with two stepwise annealing schedules are included. The learning rate is adjusted in stepwise schedules after specific epochs, and a cosine schedule is considered for better generalization performance on clean datasets. Different parameters are set for CIFAR-10 and CIFAR-100 datasets. The experimental results for ORACLE, ERM, and ODD with two stepwise annealing schedules are presented. Different parameters are set for CIFAR-10 and CIFAR-100 datasets, with p values adjusted accordingly. The cosine learning rate schedule generally outperforms the stepwise schedules, achieving good precision/recall with default hyperparameters. ODD achieves good precision/recall with default hyperparameters at different noise levels. Ablation experiments on ResNet-152 show p=10 as the best trade-off. The percentage of discarded examples by p=10 closely matches the actual noise level. Generalization performance is insensitive to hyperparameter p, except for p=1 which discarded 25.3% of examples. The study found that the hyperparameter p had little impact on the results, except for p=1 which discarded 25.3% of examples. WebVision had more discarded examples compared to ImageNet, indicating more mislabeled examples. Some examples labeled as \"leopard\" were actually images of tigers and black panthers. Examples labeled as \"leopard\" classified as noise often contain multiple objects, while those not classified as noise usually have only one less ambiguous object."
}