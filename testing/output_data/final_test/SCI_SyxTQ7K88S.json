{
    "title": "SyxTQ7K88S",
    "content": "The study introduces bio-inspired artificial neural networks with neurons characterized by spatial positions. By penalizing long connections and proximity of neurons, the network naturally forms clusters for different tasks, mirroring biological systems. This approach aims to mimic the grouping of neurons in the human brain, enhancing interpretability and continual learning in artificial neural networks. The study proposes artificial neural networks with neurons positioned based on spatial coordinates, mimicking biological systems. By penalizing long connections, the network forms task-specific clusters, improving interpretability and performance in classification tasks. The model is tested on MNIST and Fashion-MNIST datasets, showing that splitting the network into subnetworks based on weight structure maintains performance levels. Our work introduces spatial coordinates to neurons in artificial neural networks, focusing on cluster-forming properties. Unlike previous models, we use a simpler spatial loss function, investigate networks with multiple hidden layers, and emphasize the network's structure rather than just functional aspects. Our approach differs from others that find clusters based on neuron activations or correlation with the output, as we leverage the spatial placement of neurons in the network. Our approach utilizes the network's structure, focusing on spatial placement of neurons and connection strengths. It is related to continual and multi-task learning and parameter reduction models for deep neural networks. Different from other models, we introduce a biologically inspired mechanism for interpreting network predictions. In this section, the spatial network model is introduced, with neurons characterized by two-dimensional spatial features. The model includes additional loss components to encourage movement and grouping of neurons. A transport cost is implemented to penalize strong connections between neurons that are far apart in a layer. The spatial network model introduces a spatial L1 penalty to consider distances between neurons in a layer. A density cost is added to prevent neurons from being densely packed. These costs are applied to selected layers for flexibility. The final loss function includes the original loss function with hyperparameters \u03b1 and \u03b2 for weighting components. The spatial network model uses a sigmoidal activation function for rescaling invariance, allowing weights to be jointly rescaled without changing the network's final result. Preliminary experiments confirm the hypothesis, showing the network's ability to minimize loss without altering its spatial structure. Performance is tested on a double classification task involving MNIST and Fashion-MNIST datasets with similar structures but different semantics. The network splits its neurons into groups for each task, tested with different input methods: concatenated, mixed, and sequential. Outputs a 20-dimensional vector divided into two 10-dimensional vectors, with softmax applied separately. Code available at https://github.com/gmum/SpatialNetworks. The network architecture used for all tasks is n \u2212 128 \u2212 128 \u2212 128 \u2212 256 \u2212 20, with spatial penalties applied from the third hidden layer onwards. The network is split into two subnetworks for different tasks to observe the formation of disjoint regions. The network architecture used is split into two subnetworks for different tasks, with a simple greedy method of assigning neurons based on their connections. The assignment is based on the sum of absolute weights of connections to neurons in the next layer responsible for the task. The spatial network efficiently divides neurons into task-specific subgroups, placing them in separate clusters for easier network division. Classification accuracy before and after the split is compared, showing improved performance. Visual inspection confirms clear separation of neurons representing different tasks in the output layer. Neurons in the output layer are clearly separated into different task-specific groups, mirroring the arrangement in previous layers. The network's top-down view shows a division into two main groups with some outliers. Input presentation strongly influences results, with concatenated input leading to better performance post-split. The sequential task is more challenging as it hampers network performance due to shared neuron usage. This study connects neuroscience with machine learning. The study connects neuroscience with machine learning by proposing a spatial artificial neural network that mimics region-forming processes in the brain. Future work includes testing the model on a continual learning task and exploring spatial networks in relation to spiking neural networks. The model creates disjoint clusters of neurons for different tasks, allowing for learning new tasks without disrupting previous clusters. Additional constraints may be added to facilitate this process. The spatial artificial neural network mimics region-forming processes in the brain, suggesting similar properties to the brain's spatial structure of neurons. This makes the spatial network an interesting model for neuroscientific investigation."
}