{
    "title": "BJe55gBtvH",
    "content": "Telgarsky's seminal paper highlighted the benefits of depth in Deep Neural Networks (DNNs) by showing that DNNs can achieve zero classification error for certain functions, while shallow networks with fewer nodes incur constant error. However, the paper does not explain why some functions are difficult to represent with smaller depths, leaving it as an open question in deep learning and approximation theory. This work explores the connection between DNNs expressivity and Sharkovsky\u2019s Theorem, linking the depth-width trade-offs of ReLU networks to the presence of periodic points. By analyzing the eigenvalues of dynamical systems associated with functions, the study provides lower bounds for the width required to represent periodic functions based on depth. The research aims to understand the approximation of complex functions using simpler building blocks. Researchers have been exploring the approximation theory that governs neural networks, focusing on the question of neural network expressivity and how architectural properties affect the functions it can compute. Recent breakthroughs in deep learning have led to a rich history of understanding the expressive power of neural networks. Recent research by Cybenko, Hornik, and Fukushima in 1989 demonstrated the expressive power of neural networks, showing that even two-layered networks can approximate any continuous function on a bounded domain. However, the size of such networks may lead to overfitting and impracticality. Telgarsky's 2016 paper further revealed that there are functions representable by deep neural networks but not by shallow networks unless they are exponentially large. Telgarsky's 2016 paper demonstrated that neural networks with \u0398(k^3) layers, \u0398(1) nodes per layer, and \u0398(1) distinct parameters cannot be approximated by networks with O(k) layers unless they have \u2126(2^k) nodes. He uses the number of oscillations in functions to distinguish between deep and shallow networks' representation capabilities. This is achieved through the connection between the theory of dynamical systems and the representational power of DNNs via the notion of periodic points. The curr_chunk discusses the concept of fixed points and cycles of a continuous function, focusing on Lipschitz functions on the interval [0, 1]. It mentions the existence of periodic points of certain periods as a key factor in explaining certain phenomena. The curr_chunk discusses Sharkovsky's Theorem, which explains the ordering of periodic points for a continuous map on an interval. This theorem is important in understanding why depth is needed to represent functions with periodic points. Sharkovsky's Theorem states that if a function has a period of 3, it must also have points of any period. This theorem was later rediscovered in the USA in 1975 by Li & Yorke, who introduced the term \"chaos\" in Mathematics. In 1975, James Yorke and Tien-Yien Li rediscovered the concept of chaos in mathematics. They defined prime period of a function and gave an example with the function f(x) = 1 - x having prime period 2. They also presented an illustrative example connecting Telgarsky's triangle wave construction to DNNs' sensitivity to weight perturbations. The \"triangular wave\" function was a key component in Telgarsky's proof. Telgarsky's proof relied on the highly oscillatory behavior of the triangular wave function to show a separation for classification error in shallow vs deep neural networks. The function exhibits Li-Yorke Chaos and contains points of period 3, leading to complex compositions with exponentially many oscillations. However, it is unclear what would happen with a slightly modified version of the triangular wave function. The paper explores how modifying the weights of a neural network can affect its representational power, focusing on the triangle wave function and its chaotic behavior. The generalized triangle wave function parameterized by \u00b5 is closely related to the logistic map and exhibits various limiting behaviors. The triangle wave function, parameterized by \u00b5, exhibits various limiting behaviors such as convergence to a stable fixed point when \u00b5 \u2264 1 and chaos when \u00b5 = 2. Changing \u00b5 to 1 results in a different network behavior, with compositions showing different oscillatory patterns. The relative position of the map with the line y = x indicates the importance of fixed points and periodic orbits in function compositions. The behavior of these compositions can be characterized using tools from dynamical systems. The logistic map, studied by May and Feigenbaum, shows different behaviors with smoothly changing parameters. By adjusting the parameters, various compositions exhibit different oscillatory patterns. The depth-width trade-offs in representing the map are governed by a simple property of the function. Refer to Appendix C for figures illustrating these differences. The depth-width trade-offs in representing the logistic map are governed by a simple property of the function related to periodic points. Composing the function with itself multiple times results in exponentially many oscillations, leading to chaotic behavior. The number of oscillations a function has is connected to the depth-width trade-offs required. The depth-width trade-offs in representing functions are linked to chaotic behavior and periodic points. By analyzing matrices associated with periodic functions, connections with previous results are drawn. Understanding the benefits of depth in computational models is crucial for expressive power. Deep models have more power than shallower models in computational models like boolean circuits. Previous work has analyzed sumproduct networks and studied depth trade-offs. Chaotic behavior may be the reason for neural networks' failure to represent certain functions. The failure of neural networks to represent certain functions is attributed to chaotic behavior, requiring sufficient depth. Burns & Hasselblatt's proof of Sharkovsky's theorem introduces a covering lemma crucial for proving main results. The lemma defines a covering relation for functions with cycles of odd periods, leading to the existence of sub-intervals within closed intervals. The Covering Lemma introduces a covering relation for functions with cycles of odd periods, leading to the existence of sub-intervals within closed intervals. The lemma is used to show the exponential growth of the number of crossings in continuous functions. The text discusses the existence of cycles of odd periods in functions, leading to the exponential growth of the number of crossings. It introduces a sequence of vectors to count the number of times a function crosses specific intervals. The goal is to express these vectors recursively and show that they grow exponentially. The text analyzes the case of period three in a function, where numbers form distinct cycles. It discusses covering relations and intervals, showing exponential growth in crossings. The inequality arises from the Covering Lemma, ensuring at least one \"cover\" between intervals. Setting \u03b1 0 = \u03b4 0 and \u03b1 t+1 = A\u03b1 t, \u03b4 t \u2265 \u03b1 t for all t. The Fibonacci sequence F t+1 is related to \u03b1 t. Lemma 1 implies a subcollection of intervals I 0 , ..., I n\u22122 and J 0 , ..., J r. Interval J 0 involves self-loop covering. Define \u03b4 t in N r+1, capturing crossings of f t in J i. The adjacency matrix A defines a directed cycle graph with a self-loop at vertex J 0. The characteristic polynomial of A has a largest root \u03c1 r, which is a positive real number. The spectral radius of A is bounded by a root greater than one and less than two. The adjacency matrix A defines a directed cycle graph with a self-loop at vertex J 0. The spectral radius \u03c1 r is strictly decreasing in r, with smaller odd periods potentially having a faster growth rate in the number of crossings. The text discusses the growth rate of crossings in cycles of different periods, with smaller odd periods potentially exhibiting faster growth. The proof of Theorem 3.1 involves analyzing cases in Sections 3.1.1, 3.1.2, and Remark 3.2. Additionally, there are continuous functions with prime periods that are powers of two, where the number of crossings scales polynomially with t. The text discusses the representation power of different networks measured by classification error. Functions with cycles of period not a power of two will have compositions leading to a positive classification error. Theorem 3.1 states the existence of roots in polynomial equations. The text discusses the representation power of neural networks with ReLU activations. It is proven that a neural network with u ReLU units per layer and l layers is piecewise affine with at most (2m) l pieces. By choosing u to be at most 8, the classification error for any neural network g with u ReLUs and l layers is bounded from below by 1/4. This implies Theorem 4.1. The text discusses the classification error theorem for neural networks with ReLU activations. It states that for a function f of period m\u00d7p with p an odd number greater than one, a sequence of points can be constructed to achieve zero classification error for f mk. It also highlights that the classification error for any neural network with l layers and u nodes is a positive constant if u is constant and l is o(k), while the error decreases as p increases. The text discusses how classification error decreases as p increases for neural networks with ReLU activations. It suggests that functions with large odd periods are simpler than those with small odd periods, based on Sharkovsky's ordering intuition. The characteristic polynomial of matrix A is analyzed, showing that the eigenvalues must be roots of a specific equation. The eigenvalues of matrix A must be roots of a specific equation. Continuous functions with prime periods that are powers of two have a polynomial scaling of crossings. Examples include a function with prime period two and another with prime period four. The function defined has crossings that grow linearly with t, respecting Sharkovsky ordering. An example function with a point of period 5 is shown. The proof approach for odd periods is similar to period 3, using induced covering graphs. Compositions of the logistic map are illustrated as r varies slightly. The map's behavior changes with varying values of r: at r = 3.9, period 3 occurs, at r = 3.5 period 4, and at r = 3.2 period 2. Adding a bias term in the ReLU activation unit does not alter the results, but adding it to the function f itself leads to interesting outcomes, especially for non-power-of-two periods. Adding a bias term to the function can change its period, especially for non-power-of-two periods. For example, the triangle function with a bias term may lose its original period, becoming sensitive to numerical changes. In this section, experimental evidence is provided by training a neural network with increasing depth on a classification task resembling the n-alternating points problem. The goal is to show how classification error decreases with network depth for a fixed width, using 8000 points from [0,1] labeled 0 or 1. In this section, experimental evidence is provided by training a neural network with increasing depth on a classification task resembling the n-alternating points problem. The theory discusses the challenges of training deep and narrow networks with very few data points. Empirical results show that deep networks improve accuracy, with a deep network of 5 layers reaching close to 0 classification error. In experiments, deep networks with 5 layers can achieve 99.04% accuracy. The depth of the neural network is varied from 1 to 5, with 6 neurons in each layer. All activations are ReLU's, with the last layer using a sigmoid for output probabilities. The training error should tend to 0, using the \"ADAM\" optimizer for training. The study used the \"ADAM\" optimizer with 200 epochs to train various models and analyze training error saturation. The paper introduces a natural property of functions and explores depth-width trade-offs. It addresses questions raised in previous works and provides insights on functional complexity and depth benefits. The code is available in the supplementary material. The study explores depth-width trade-offs in neural networks, showing a depth separation argument for natural functions like triangle waves. Assessing the period for a specific prediction task is deemed challenging with current techniques. The characterization result provides qualitative and quantitative insights into representing certain functions. The characterization result provides insights into functions with complicated compositions, similar to problems in class NP. Finding certificates for arbitrary continuous functions is challenging, requiring deeper networks for prediction tasks inspired by physics. Examples include solving the 3-body problem or turbulent flows with complex dynamics. Empirical evidence suggests that solving complex physical processes like the 3-body problem or turbulent flows requires deep neural networks, as demonstrated by Ling et al. (2016) and Breen et al. (2019) using a 10-layered neural network."
}