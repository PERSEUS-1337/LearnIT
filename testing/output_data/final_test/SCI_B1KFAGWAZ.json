{
    "title": "B1KFAGWAZ",
    "content": "Recent research in artificial intelligence focuses on deep reinforcement learning for multi-agent domains, exploring both decentralized and centralized perspectives. A hierarchical master-slave architecture combining these perspectives shows promise, with key elements including composed action representation, learnable communication, and independent reasoning. This approach consistently outperforms other methods, leveraging the strengths of each perspective for improved performance. Our proposal outperforms competing methods in synthetic experiments and StarCraft micromanagement tasks. Reinforcement learning focuses on maximizing cumulative reward through agent actions in various environments. While most research has focused on single-agent problems, tasks like autonomous vehicle coordination, multi-robot control, and network packet delivery require collaboration among multiple agents. Multi-agent reinforcement learning methods are essential for addressing these challenges. Recent research efforts in multi-agent reinforcement learning (MARL) have shifted towards using deep neural networks for more complex environments and tasks. However, scaling deep reinforcement learning to multiple agents remains a challenge due to the difficulty of training and the large state-action space. Current methods in MARL focus on two perspectives in addressing these challenges. Recent methods in multi-agent systems focus on two perspectives: decentralized, where each agent has its own controller, and centralized, where there is a larger model controlling all agents. Learning in decentralized settings is challenging due to local viewpoints and non-stationary environments, while centralized settings require dealing with parameter search in a large state-action space. To address these challenges, the idea of a master-slave architecture is revisited to combine both perspectives effectively. The master-slave architecture combines decentralized and centralized perspectives in multi-agent systems, allowing the master agent to plan globally while slave agents optimize locally based on guidance. This concept is inspired by organized traffic systems and sports teams like football or basketball. Our work introduces a master-slave architecture for deep MARL using policy-based RL methods. A multi-agent policy network is created with slave agents and a master agent, all utilizing recurrent neural networks for policy approximation. Each agent has its own \"thoughts\" represented by hidden states, with slave agents considering local states and the master agent considering global states and messages from slaves. The final action output of each slave agent is a combination of inputs from both the slave and master agents, facilitated by a gated composition module. The proposed master-slave multi-agent RL solution, named MS-MARL, utilizes a gated composition module to process thoughts from slave and master agents for final action. Through synthetic experiments and StarCraft micromanagement tasks, MS-MARL consistently outperforms recent MARL methods. The effectiveness of learned policies is analyzed, showcasing interesting phenomena related to specific designs. The paper discusses related works, details the proposal, demonstrates effectiveness in challenging tasks, and concludes with findings and discussions. The paper discusses the proposed MS-MARL solution, which combines centralized and decentralized perspectives in deep MARL. Key factors for success include composable action representation, independent reasoning, and learnable communication. Empirical results show outperformance of recent methods in synthetic experiments and StarCraft tasks, making it a competitive MARL solution. Recent research has focused on deep multi-agent reinforcement learning (MARL) for complex tasks, inspired by the success of deep reinforcement learning (RL). Various approaches have been proposed, including learnable communications and centralized network structures. BID18 introduced bidirectional communication channels for effective coordination in tasks like StarCraft micromanagement. Recent research in deep multi-agent reinforcement learning (MARL) has explored different strategies for tasks like StarCraft micromanagement. BID26 and BID4 proposed episodic exploration and stabilizing experience replay, respectively. However, previous works like BID2, BID3, and BID11 have only focused on one perspective, missing out on the advantages of the other. BID22 introduced \"CommNet\" for communication among agents, but it lacks an independently reasoning master agent. BID3 and BID11 suggested a global critic for centralized level work, but critics are essentially value networks. The curr_chunk discusses the use of master-slave architecture in deep MARL, emphasizing independent master reasoning at a global level and each slave agent thinking at a local scale. Effective communication is achieved through back propagation to optimize rewards. Compared to existing works, this approach focuses on independent reasoning. The curr_chunk discusses the superiority of the master-slave architecture in deep MARL, highlighting independent reasoning and outperforming existing methods. It contrasts hierarchical RL methods by focusing on parallel agent-specific tasks rather than sequential sub-tasks. The key idea in RL methods is to have a master controller organizing agents globally while decentralized agents optimize their actions locally. This can be achieved through value-based, policy-based, or actor-critic methods. An example using policy gradient methods is introduced to learn a mapping from states to actions for multiple agents. The parameters \u03b8 represent the policy function approximator for all agents, including the master agent. The design allows independent states for the master agent and merges the master's action with the final actions of slave agents. This enables input of global states to the master agent and end-to-end training of the policy network with signals from actual actions. The architecture is illustrated in FIG0, showing the network structure at two consecutive time steps. The architecture includes slave agents represented as blue circles and a master agent as a yellow rectangle, all using RNN modules like LSTM cells. Agents also consider the hidden state of RNN h t\u22121 for reasoning. The master agent receives input from slave agents c i and broadcasts actions to all agents. A gated composition module merges actions from the master and slaves, resembling LSTM behavior. The GCM module integrates the master agent's states with action proposals from slave agents, allowing for different proposals to be provided. It enables generalization to heterogeneous agents and can also support unified action proposals to all agents. Learning is end-to-end through policy gradient application in a centralized perspective. Our MS-MARL proposal combines advantages from both centralized and decentralized perspectives by facilitating communication channels between slave agents and formulating an independent master agent reasoning based on all slave agents' messages and its own state. The algorithm updates network parameters using policy gradient and can handle both discrete and continuous action spaces. MS-MARL combines centralized and decentralized perspectives by utilizing a master-slave architecture. The master agent absorbs information globally, leading to more efficient reasoning and planning. It explores the large action space hierarchically, allowing for more stable and efficient training. The proposed master-slave architecture in MS-MARL combines centralized and decentralized perspectives, with the master agent focusing on base estimation while slave agents estimate advantages. Experiments were conducted on five multi-agent tasks, including traffic junction and combat tasks in the MazeBase environment. These tasks are widely used for evaluating MARL methods. The traffic junction task in MazeBase BID21 consists of four connected junctions with cars entering from eight directions. Cars can advance or stay still, with collisions occurring if they occupy the same cell. In experiments, collisions occur when cars occupy the same location. The combat task involves a battle between two teams in a 15x15 grid. Each team has 5 members with actions including moving, attacking, or staying idle. Attacks reduce an agent's health by 1 point, with initial health set at 3. Teams win by defeating all opposing members. Default settings from BID22 are used. StarCraft micromanagement tasks are based on the RTS game StarCraft. The micromanagement tasks in StarCraft involve local battles between two groups of units, with actions including move and attack. Unlike the combat task, the StarCraft environment has continuous state and action spaces, leading to a larger search space for learning. These tasks are considered \"hard combats\" and require a smart policy for AI bots to win. Independent policy methods have been found to be less effective, with MARL methods showing better results. In challenging StarCraft micromanagement tasks, MARL methods are essential for effective collaboration among agents. One such task involves controlling 15 Terran marines to defeat an AI controlling 16 marines. The key strategies include focus fire and avoiding overkill, as well as the policy of \"Spread Out.\" This task is more difficult due to the high number of agents and the controlled team having fewer units than the enemies. In challenging StarCraft micromanagement tasks, MARL methods are crucial for effective collaboration among agents. The \"Spread Out\" policy is captured in experiments involving Marines vs. Zerglings and Wraiths. Unlike ground units, Zerglings attack by direct contact, while flying units like Wraiths do not collide with each other. The \"Spread Out\" strategy is not effective against Zerglings, highlighting the importance of adapting strategies based on unit types. The \"Spread Out\" strategy may not be important for flying units like Wraiths since they can occupy the same tile. In a task involving Dragoons and Zealots, the strategy is to focus fire on enemy zealots first and then eliminate the Dragoons. The state-of-the-art performance on these tasks is still low, but the proposed MS-MARL method achieves higher results. The proposed MS-MARL method achieves higher winning rates on tasks involving {15M vs. 16M, 10M vs. 13Z, 15W vs. 17W and 2D3Z}. The implementation details include using LSTM for the master module and RNN for the slave module, with hidden states set to 50 for both agents. The \"Gated Composition Module\" is introduced in detail in section 3.1. The \"Softmax/Gaussian action modules\" are used for different tasks, with discrete and continuous action spaces. State features are applied differently based on the task, with specific representations for each. The proposed GCM module facilitates learning different strategies for various unit types, crucial for winning the \"2D3Z\" task. Action definitions vary based on the task, with continuous action spaces for specific scenarios. Reward definitions differ for the traffic junction task and combat. The reward definition for the traffic junction task and combat task is the same as in BID22. Rewards are defined for specific scenarios by simulating the design of the combat task. The reward is formulated to evaluate the current action of the agent. The training process involves the entire training procedure. The training process involves using a softmax policy for discrete tasks and a Gaussian policy for continuous tasks. The network output is the mean of the Gaussian policy distribution with a fixed variance. Different batch sizes are used for different tasks, with varying learning rates. The training process involves setting different learning rates for various tasks and adjusting the action variance gradually. Three state-of-the-art MARL methods are selected for comparison, including CommNet, which utilizes a simple communication strategy among agents. The GMEZO method was proposed for StarCraft micromanagement tasks in BID26, employing a greedy update over MDP for inter-agent communications. BiCNet uses a bidirectional RNN for communication among agents in BID18, showing success in handling micromanagement tasks and learning policies resembling expert human players. TAB2 demonstrates performance improvement compared to baselines. Our MS-MARL model outperforms CommNet on traffic junction and combat tasks. Adding an explicit global planner in our model improves collaborative policies compared to CommNet with global information. The GMEZO and BiCNet methods have not been tested on these tasks yet. The MS-MARL model shows superior performance on challenging StarCraft micromanagement tasks compared to baseline methods. The model facilitates independent thinking for the master agent, leading to better, faster, and more stable convergence. The training process is visualized in win rate curves, highlighting the effectiveness of the design. Individual components of the proposal are further analyzed in the combat task setting. In the combat task analysis, the MS-MARL model outperforms the CommNet model by allowing independent thinking for the master agent and enabling communication among agents. The inclusion of an explicit occupancy map as a state for the master agent leads to a significant improvement in performance. The extra information from the occupancy map is already included in each agent's state. The contributions of the master agent and slave agents to the final action are also observed. The master agent and slave agents contribute to final action choices in combat tasks. The master agent learns an effective global policy, leading the team towards enemies while slave agents adjust positions locally. This showcases different agent roles working together to attack enemies. The MS-MARL model combines centralized and decentralized perspectives, outperforming other baselines in training performance. In combat tasks, the MS-MARL model outperforms other baselines by more than 20 percent. Single master agent control is ineffective, while independent agents perform comparably to CommNet. A stronger global planner and gating mechanism are needed for multi-agent coordination. Our method shows more successful policies than CommNet in combat tasks. In combat tasks, the MS-MARL model outperforms other baselines by more than 20 percent. It shows successful policies like gathering all agents together before attacking enemies and spreading out in a half-moon shape to focus fire, similar to the military maneuver \"Pincer Movement\". This behavior contrasts with CommNet's limited local views and ineffective communications, leading to shorthanded losses in battles. The MS-MARL model demonstrates superior performance in combat tasks by utilizing successful strategies like the \"Pincer Movement\". The size of the \"pincer\" is crucial for winning battles with fewer units than the enemies. The master module contributes to higher performance, as shown in a visualization of hidden states. The distribution of hidden states indicates improved win rates with the GCM module and occupancy map compared to the original version. In this paper, the master-slave architecture for deep MARL is revisited, combining a centralized master agent with distributed slave agents. The master agent provides high-level instructions while local agents aim for fine-grained optimality. Empirical results show the superiority of this approach in challenging multi-agent tasks. The distribution of master's messages corresponds to successful policies, indicating that hidden states effectively carry meaningful messages. The master-slave architecture for deep MARL involves a centralized master agent providing high-level instructions to distributed slave agents for fine-grained optimality in challenging multi-agent tasks. This approach, instantiated with a policy gradient method, shows superiority in achieving successful policies through meaningful messages carried by hidden states. Other RL algorithms can also benefit from similar schemes."
}