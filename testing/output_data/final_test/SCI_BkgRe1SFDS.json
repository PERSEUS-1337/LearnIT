{
    "title": "BkgRe1SFDS",
    "content": "Efficiently learning to solve tasks in complex environments is a key challenge for reinforcement learning agents. A proposed approach involves decomposing the environment using task-agnostic world graphs, accelerating learning by focusing exploration on specific areas. This framework includes two phases: training a binary recurrent VAE to identify world graph nodes and edges, and using a hierarchical RL framework to bias exploration towards task-relevant areas. Results show significant acceleration in RL performance on 2D grid world tasks, with world graph integration doubling rewards on simpler tasks like MultiGoal. The integration of world graph in reinforcement learning accelerates learning by focusing exploration on specific areas, doubling rewards on simpler tasks like MultiGoal and solving more challenging tasks like Door-Key where baselines fail. This approach aims to improve efficiency in learning tasks in complex environments by utilizing a task-agnostic abstraction of the environment through world graph nodes. The world graph nodes serve as key states for agent trajectories, guiding exploration efficiently. Hierarchical RL agents use high-level policies to choose goal states from the world graph, aiding in task-relevant exploration. The framework involves two phases: task-agnostic phase for training world graphs using VAE with binary latent variables, and goal-conditioned policy for trajectory collection. The text discusses a task-agnostic unsupervised approach to learning world graphs using a recurrent VAE with binary latent variables and a curiosity-driven goal-conditioned policy. It also introduces an HRL scheme for the task-specific phase featuring multi-goal selection and navigation via world graph traversal. The text introduces a 2-phase framework for learning world graphs and hierarchical reinforcement learning (HRL) for task-specific navigation. It includes empirical evaluations on 2D grid worlds, showing improved sample efficiency and performance over baselines, especially with transfer learning and world graph traversal. The curr_chunk discusses the importance of understanding the environment and its dynamics for effective planning and control in model-based RL. It mentions strategies like active localization for robots to navigate unfamiliar regions and learning to represent the world with generative latent states. These latent states can assist in planning and model-based RL. In model-based RL, abstracting the environment's structure and dynamics into a graph representation with nodes as states and edges as transitions is crucial. Instead of selecting nodes based on state space coverage, a subset of states is identified to serve as waypoints for navigating complex environments. This method aids in automatic subgoal discovery and subpolicy learning for efficient task-solving. Subpolicy and subgoal learning are essential for efficiently solving complex tasks in reinforcement learning. Subpolicy learning involves identifying useful policies like option-based methods and subtask segmentations. On the other hand, subgoal learning focuses on identifying \"important states\" to reach. Various definitions of \"important\" states include frequently visited states, states with novel information, bottleneck states, and environment-specific heuristics. Drawing inspiration from unsupervised temporal segmentation and imitation learning, our work defines \"important\" states as critical waypoints. The text discusses the identification of \"important\" states as critical waypoints in learning a world graph for accelerating reinforcement learning of downstream tasks. The method involves learning a task-agnostic abstraction of an environment to capture its high-level structure and dynamics, with the waypoints serving as starting points for exploration. Feasible transitions between nearby waypoints are embedded as edges in the world graph. The text discusses defining important states in learning a world graph for summarizing the environment's structure and dynamics. It involves collecting state-action trajectories and using unsupervised learning to identify world graph nodes. A recurrent variational autoencoder is trained to infer binary latent variables for reconstructing action sequences from state-action pairs. The VAE infers binary latent variables to determine the importance of each state in the sequence for the generative decoder. It consists of an inference, generative, and prior network structured to process state-action pairs observed from the environment. The inference network approximates the posterior over a sequence of binary latent variables, the generative network computes a distribution over the action sequence using masked state sequences, and the prior network encodes the empirical state-conditioned latent variables. The prior network p \u03c8 encodes the empirical average probability that state s t is activated for reconstruction, encouraging inference to select a consistent subset of states for action reconstruction. The algorithm identifies waypoint states V p and learns a goal-conditioned policy \u03c0 g by initializing network parameters and performing T-step rollouts. The algorithm performs T-step rollouts using goal-conditioned policy \u03c0 g with goal g n. Rewards are relabeled with action reconstruction error as curiosity bonus. Policy gradient update of \u03c0 g is done using \u03c4 \u03c0 and r \u03c0. V is updated using \u03c4 r and \u03c4 \u03c0. V p is updated based on states with largest prior mean \u03b1s \u03b1s+\u03b2s. Beta distribution is used for the prior and Hard Kumaraswamy distribution for the approximate posterior to ensure differentiability. The algorithm uses a VAE with binary z to learn latent structure from trajectories. It includes stretch-and-rectify procedure, reparametrization trick, and regularization terms L0 and LT. The goal is to activate a targeted number of states for action reconstruction and encourage temporal separation among selected states. Training details are provided in Appendix A. The algorithm utilizes a VAE to learn latent structure from trajectories, emphasizing the importance of collecting a rich set of trajectories. A strategy is proposed to bootstrap trajectories by exploring the environment based on the current iteration's V p and updating the VAE and V p iteratively until action reconstruction accuracy plateaus. Exploration involves action replay to navigate the agent to a state from V p, with the ability to modify the resetting strategy if needed. Two rollouts are collected for each starting point, one for random exploration and the other for action execution. The algorithm uses a VAE to learn latent structure from trajectories, emphasizing the importance of collecting diverse trajectories. A strategy is proposed to explore the environment based on V p, updating the VAE iteratively until action reconstruction accuracy plateaus. GCP integrates intrinsic motivation like curiosity to generate diverse rollouts, using action reconstruction error as an intrinsic reward signal. The final stage involves constructing edges of G w to capture environment dynamics. Random walk rollouts are collected from each waypoint to estimate the underlying structure. The algorithm uses a VAE to learn latent structure from trajectories and explores the environment based on waypoints Vp. Random walk rollouts are collected from each waypoint to estimate the underlying adjacency matrix. Paths taken by \u03c0g are considered, and the shortest observed path is kept as a world graph edge transition. Dynamic programming is used to navigate between waypoints on the world graph. World graphs provide a task-agnostic abstraction of the environment through feasible transition routes between waypoints. The framework leverages world graphs for structured exploration in hierarchical RL, where high-level policy selects subgoals from Vp and traverses using world graph edges. RL agent's goal is to maximize cumulative expected return by incorporating world graphs with a hierarchical approach. The hierarchical approach based on the Feudal Network (FN) incorporates world graphs with RL. Tasks include collecting balls for rewards, avoiding lava blocks, picking up a key to open a door, and reaching the exit point. Visualizations are used to evaluate the benefit of using world graphs. The hierarchical approach based on the Feudal Network (FN) incorporates world graphs with RL. Tasks include collecting balls for rewards, avoiding lava blocks, picking up a key to open a door, and reaching the exit point. Visualizations in Appendix D decompose the agent's policy into two separate policies: a high-level policy (\"Manager\") proposes subgoals, and a low-level policy (\"Worker\") receives subgoals from the Manager and is rewarded for reaching them in the environment. The Manager and Worker operate at different temporal resolutions and do not share weights. Policies are trained using advantage actor-critic (A2C) for optimization. The Manager and Worker layers are initialized with corresponding layers from the GCP learned during world graph discovery. A Manager policy factorizes subgoal selection with wide and narrow policies, simplifying the search space. Waypoints are used as wide goals to leverage the world graph edges for planning and executing traversals. Traversal is initiated when the agent encounters a waypoint state with a feasible connection to the active wide goal. Planning occurs upon triggering a traversal. The agent uses dynamic programming planning to estimate the optimal route to the wide goal in the world graph. Execution of graph traversals depends on the environment, with the agent following action sequences or using a pretrained policy to reach intermediate waypoint states. If the agent fails to reach a waypoint within a time limit, a new goal pair is received from the Manager. World graph traversal allows task-relevant wide goals to be assigned by the Manager. The Manager assigns task-relevant wide goals that accelerate learning by focusing exploration. Ablation studies show improved efficiency and performance over baseline HRL models. Waypoints provide superior world representations for solving tasks. Implementation details and tasks are in Appendix C-D. In experiments with various reward structures and levels of stochasticity, each action incurs a negative penalty. A rigorous evaluation process is followed, with experiments repeated using different seeds for training, validation, and testing. Components in the framework are ablated and compared against non-hierarchical and hierarchical baselines, showing performance improvements. Wide and narrow goals are found to enhance performance over the baselines. The framework components improve performance over baselines. Using wide and narrow goals effectively structures Manager instructions, aiding Worker in task-solving phases. World Graph Traversal enhances performance with targeted exploration. Traversal speeds up convergence, V rand has higher variance than Vp. Initialization with \u03c0g on Vp shows benefits. Initializing on Vp, all models use WN. Task-specific phase benefits from task-agnostic goal-conditioned policy, boosting learning. G w traversal speeds up convergence, especially in larger mazes. Graph learning stage converges in 2.4K iterations. G w traversal exhibits superior sample efficiency and enables longer-horizon planning for Door-Key task. The agents improve success rates on Door-Key task by establishing waypoints learned by VAE as nodes for G w. Comparing against randomly selected states (V rand), V p generally outperforms V rand, except for Door-Key task where they match. V rand shows high variance, suggesting certain random states may be suitable for the task. Initialization with GCP benefits learning by transferring environment knowledge from weight initialization, allowing models to consistently perform well. Using learned waypoints outperforms random states for tasks, especially in environments like MultiGoal-Stochastic where lava squares can block traversal. World graphs are powerful abstractions that accelerate reinforcement learning. The proposed framework utilizes environment abstractions to accelerate reinforcement learning, with potential applications in real-world multi-task learning and navigation. It aims to generalize dynamic world graphs for evolving environments and apply them to multi-agent problems. The recurrent VAE's main objective is to derive the evidence lower bound for state-action pairs, utilizing inference and prior networks to approximate latent variables and state-conditioned priors. The proposed framework utilizes environment abstractions to accelerate reinforcement learning, with potential applications in real-world multi-task learning and navigation. It aims to generalize dynamic world graphs for evolving environments and apply them to multi-agent problems. The recurrent VAE's main objective is to derive the evidence lower bound for state-action pairs, utilizing inference and prior networks to approximate latent variables and state-conditioned priors. The state-conditioned prior p \u03c8 (s t ) is outputted at each time step, with Beta chosen as the prior distribution and the Kuma distribution used as the approximated posterior to relax discrete latent variables to continuous surrogates. The Kuma distribution, similar to Beta, ranges from bimodal to unimodal and has a simple Cumulative Distribution Function, making it suitable for the reparametrization trick. Fixing \u03b2 = 1 for the Kuma approximated posterior has shown empirically better performance. The KL-divergence between Kuma and Beta distributions can be approximated using the Digamma function and Euler constant. The Kuma distribution is made \"hard\" by stretching the support and rectifying non-eligible probabilities. Two additional regularization terms are imposed on the approximated posteriors to prevent overfitting. The optimization objective in the context of action sequence reconstruction involves constraints on the activation of z t and encourages temporally isolated activations. The objective weights \u03bb i are tuned using Lagrangian relaxation, treating them as learnable parameters. The optimization objective for action sequence reconstruction involves tuning the objective weights \u03bb i using Lagrangian relaxation. Initializing the Worker and Manager policies for downstream HRL with weights learned by \u03c0 g during world graph discovery is found to be efficient and stable for mini-batch training. The optimization objective for action sequence reconstruction involves tuning the objective weights \u03bb i using Lagrangian relaxation. Weight initialization from GCP improves performance. Model details shared in comments. Adam optimizer with mini-batches of size 128. Initial learning rate of 0.0001. HardKuma parameters set. BiLSTM sequence length is 25. Training iterations are 3600, converging around 2400. Networks are trained end-to-end. Lambda values initialized for Lagrangian Relaxation. The optimization objective involves tuning weights \u03bb i using Lagrangian relaxation. The total number of waypoints is set to 20% of the state space size. The Manager gives subgoal pairs, and the Worker takes actions based on policy \u03c0 \u03c9 to reach a new state. The Worker policy \u03c0 \u03c9 takes actions based on subgoal pairs given by the Manager. The Worker receives rewards for reaching goals g w and g n, with negative rewards for actions taken. The Manager renews subgoal pairs when g n is reached or the time limit is met. Training of the Worker policy follows the A2C algorithm. The Manager policy follows the A2C algorithm like the Worker policy. The Manager's value function regresses against the t m -step discounted reward. The policy gradient for the Manager policy becomes complex due to the large action space. Calculating the exact entropy for the Manager policy is computationally intractable. The Manager policy in HRL training inherits hyperparameters from the Worker policy. Training iterations are capped at 100K with early stopping for plateaued performance. Specific HRL hyperparameters include a horizon of 20 and a local attention range of 5 for small and medium mazes, and 7 for the large maze."
}