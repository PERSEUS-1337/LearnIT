{
    "title": "rylztAEYvr",
    "content": "Many challenging prediction problems involve creating complex structured objects as outputs. A simple iterative target augmentation scheme can effectively guide the training and use of generative models by leveraging the idea that evaluation is easier than generation. This scheme views the generative model as a prior distribution and uses a separately trained filter as the likelihood. The method is applicable in supervised and semi-supervised settings, yielding significant gains in molecular optimization and program synthesis. Our augmented model surpasses the previous state-of-the-art in molecular optimization by over 10%. Deep architectures excel at generating complex objects like images, text, molecules, or programs. Translation tasks, such as molecular optimization in drug development, face challenges due to the complexity of objects and limited training data. We propose a meta-algorithm to enhance translation quality by addressing these challenges. Our approach focuses on iteratively augmenting translation targets to improve performance on complex generation tasks, where each precursor corresponds to multiple possible outputs. By using a learned predictor of target object quality as a filter, we can effectively guide the generation process. This method has shown significant improvements in tasks like molecular optimization in drug development. Our approach involves constructing an external filter to guide the generative model in producing candidate translations that meet specific criteria. This filter is applied iteratively during training epochs to improve the model's performance. The generative model acts as a prior distribution over complex objects, with the filter serving as the likelihood. This approach is similar to self-training or reranking methods used in parsing tasks, but in our case, the candidate generator is complex while the filter remains relatively simple and fixed. Our meta-algorithm effectively improves translation quality in the supervised setting. It outperforms existing models on program synthesis and molecular optimization tasks, achieving state-of-the-art results. The iterative target augmentation significantly enhances the performance of graph-based methods. Molecular optimization aims to enhance chemical properties by modifying compounds. Previous approaches used reinforcement learning or graph-to-graph translation, but were limited by small training sets. Our method uses property prediction models to assess chemical properties of generated molecules, showing improved results even with reduced filter quality. Source side augmentation with unlabeled precursors can further enhance outcomes when combined with the target data augmentation framework. In this work, an off-the-shelf property prediction model is used to filter proposed translation pairs during data augmentation. Program synthesis involves generating a program based on input-output specifications, with correctness verified by executing it. The method is related to various approaches in semi-supervised learning. Our method in semi-supervised learning involves data augmentation and label guessing in image and text classification, as well as back-translation in machine translation. Unlike other methods, we work in the forward direction and use multiple iterations for data augmentation. Our method involves data augmentation and label guessing in various tasks, including image and text classification, as well as back-translation in machine translation. Unlike other methods, we use multiple iterations for data augmentation and improve the performance of neural models in different domains. Our method utilizes task-specific constraints for selecting correct outputs in conditional generation tasks, such as molecular optimization. The iterative target augmentation framework can be applied to any task with constraints, like program synthesis. Training the model on the dataset and applying iterative target augmentation while constraining the output to remain similar to the input are key steps in the process. Program synthesis is the task of generating a program Y that satisfies input specification X, such as a set of input-output test cases. The proposed augmentation framework involves an iterative procedure where candidate translations are generated and selected based on constraints. The proposed augmentation framework involves generating K distinct translations for each input satisfying a constraint and adding them to the training set. The model is then trained over this new set for one epoch. An external filter is used to remove outputs that violate the constraint. At test time, the model generates up to L outputs until one satisfies the constraint, or outputs the first attempt if all fail. Additionally, the augmentation step can be applied to unlabeled inputs without corresponding outputs. The iterative target augmentation framework involves generating multiple translations for each input that meets a constraint and adding them to the training set. An external filter is used to ensure outputs satisfy the constraint. The approach can be applied to unlabeled inputs without corresponding outputs, providing theoretical motivation for the framework. The iterative target augmentation framework optimizes a parametrically defined prior translation model to generate multiple acceptable translations for each input. The approach involves treating the output structure as a latent variable and using the EM algorithm to maximize the objective. The target augmentation step includes rejection sampling guided by posterior samples. The E-step involves drawing posterior samples with rejection sampling guided by the filter to approximate the posterior. The number of samples K controls the quality of approximation. An additional training step based on augmented targets corresponds to a generalized M-step. The augmentation step samples different candidates for each input using the old model and removes those violating constraints. Training maximizes the EM auxiliary objective via stochastic gradient descent, showing improved model performance with more iterations. The EM approach converges to a better-performing translation model. The iterative target augmentation converges to a better-performing translation model for molecular optimization and program synthesis tasks. The goal is to modify molecules to improve their chemical properties, with a focus on drug-likeness. This task is formulated as a graph-to-graph translation problem, similar to machine translation. Training involves a set of molecular pairs {(X, Y)} where X is the input molecule. Our method is suitable for molecular optimization tasks where the goal is to improve chemical properties of molecules. The training set consists of molecular pairs {(X, Y)} with X as the input molecule and Y as a similar molecule with enhanced properties. The constraint for this task includes ensuring that the chemical property of Y exceeds a threshold \u03b2 and the molecular similarity between X and Y surpasses a threshold \u03b4. Molecular similarity is measured using Tanimoto similarity on Morgan fingerprints. Ground truth values of chemical properties are typically determined through expensive experimental assays. In molecular optimization tasks, an in silico property predictor F1 is constructed to approximate the true property evaluator F0. The predictor is trained using molecules in the training set and their labeled property values. It is parameterized as a graph convolutional network and used to filter out molecules with predicted properties below a threshold \u03b2 during data augmentation. The evaluation setup follows Jin et al. (2019b) for QED and DRD2 optimization tasks. The text discusses the evaluation metrics for molecular optimization tasks, focusing on the probability of finding successful modifications for molecules and the diversity of these modifications. Different models' performance on QED and DRD2 optimization tasks is compared, with emphasis on the importance of iterative target augmentation for improved performance. The main metrics for evaluating molecular optimization tasks include success rate and diversity of translated compounds. Two model architectures, VSeq2Seq and HierGNN, are compared to demonstrate the effectiveness of the augmentation scheme across different neural architectures. The iterative augmentation procedure significantly improves the performance of VSeq2Seq and HierGNN models on molecular optimization tasks, surpassing VSeq2Seq by a wide margin. The augmentation paradigm increases translation success rates by over 10% on both datasets, with VSeq2Seq+ outperforming the non-augmented version of HierGNN. Adding more precursors during data augmentation significantly improves the VSeq2Seq model's performance on molecular translation tasks. For example, on the QED dataset, the translation success rate increased from 89.0% to 95.0% by adding test set molecules and 100K precursors from the ZINC database. This augmentation strategy also enhances the diversity of generated molecules, with a 100% relative gain in output diversity on the DRD2 dataset. The difference in evaluators used during data augmentation and test time does not affect the overall training process. The influence of property predictors in data augmentation was analyzed by using less accurate predictors in the external filter component. Despite deviating from the ground truth, strong performance was maintained on the DRD2 dataset, suggesting potential applicability to other properties. Ablation analysis of filtering at training and test time was presented in Table 2. The evaluation for VSeq2Seq(no-filter) is conducted after 10 augmentation epochs, as the best validation set performance only decreases over the course of training. Importance of External Filtering Our full model (VSeq2Seq+) uses the external filter during both training and testing. VSeq2Seq(test) is a version of our model trained without data augmentation but uses the external filter to remove invalid outputs at test time, performing significantly worse than our full model trained under data. In Table 2, VSeq2Seq(test) performs worse than the full model trained with data augmentation. VSeq2Seq(train) with data augmentation but no prediction time filtering also underperforms. A version without an external filter, VSeq2Seq(no-filter), shows declining performance with each data augmentation epoch, highlighting the necessity of the external filter in preventing poor targets from affecting model training in program synthesis. The method is suitable for program synthesis as the target program is not unique, with multiple programs consistent with the input-output specifications. Evaluation includes extra test cases, and prediction time filtering excludes held-out cases. Model performance on Karel program synthesis task is shown in Table 3. Our task focuses on Karel program synthesis using the Karel programming language. We have developed an augmented version of the MLE model for this task. Evaluation metrics include top-1 generalization, measuring the model's ability to generate a program passing input-output test cases. Our model is compared to the MLE baseline from previous studies. Our model, MLE+, is an augmentation of the MLE baseline for Karel program synthesis. It outperforms the base MLE model and the best reinforcement learning model from previous studies. The study presents an iterative target augmentation framework for generation tasks with multiple outputs, showing strong empirical results in molecular optimization and program synthesis. The approach is complementary to architectural improvements and can benefit other techniques like execution-based synthesis. It is applicable to various domains and demonstrates efficacy in maximizing generalization metrics. Our approach involves using augmented models with the same hyperparameters as their baseline counterparts. For the VSeq2Seq model, we use specific parameters such as batch size 64, embedding and hidden dimension 300, VAE latent dimension 30, and an LSTM with depth 1. The HierGNN model shares hyperparameters from a previous study. For training and prediction time filtering parameters, we set specific values for the tasks. For the Karel program synthesis task, we use hyperparameters from a previous model. At test time, we use a beam size of 64. At test time, a beam size of 64 is used. The baseline model is trained for 100 epochs, while the model with iterative target augmentation is trained for 15 epochs followed by 50 epochs of iterative augmentation. Each epoch of iterative augmentation uses 1/10 of the dataset, making 5 passes over the entire dataset. Training, validation, and test set sizes for different tasks are provided in Table 4. In molecular optimization tasks, the effect of modifying the number of new targets added per precursor during training is explored. Performance may suffer when the number of new targets is too small, but there is little change in performance for larger numbers. Additionally, a method is tested that continually grows the training dataset by keeping all augmented targets. In molecular optimization tasks, the impact of continually growing the training dataset by keeping all augmented targets is examined. Performance gains from this approach are minimal in comparison to discarding new targets at the end of each epoch. Additionally, training time iterative target augmentation and prediction time filtering are shown to improve model performance. Even without prediction time filtering, the model outperforms the best RL method from Bunel et al. (2018). The model, even without prediction time filtering, outperforms the best RL method from Bunel et al. (2018) in molecular optimization tasks. Ablation analysis in Table 7 shows the impact of filtering at training and test time, with improvements seen when using data augmentation during training and external filtering at prediction time."
}