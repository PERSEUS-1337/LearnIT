{
    "title": "r1eVMnA9K7",
    "content": "Learning to control an environment without hand-crafted rewards or expert data remains challenging in reinforcement learning research. An unsupervised learning algorithm is presented to train agents to achieve goals using only observations and actions. The agent learns a goal-conditioned policy and a goal achievement reward function, leading to a cooperative game and a learned reward function reflecting similarity in controllable aspects of the environment. The efficacy of the agent is demonstrated in reaching diverse goals in Atari, the DeepMind Control Suite, and DeepMind Lab. Deep reinforcement learning methods using deep neural networks have achieved human-level performance on challenging tasks. However, these methods learn differently from humans, who focus on mastering the environment rather than optimizing a single reward function. This approach allows for learning even in the absence of rewards. Reinforcement learning offers benefits such as learning without extrinsic rewards and the ability to reach arbitrary goals. Building agents that aim for environment mastery is a current challenge, with the use of environment models in model-based reinforcement learning proving to be difficult. The new model-free agent architecture DISCERN learns to control an environment in an unsupervised way by learning purely from observations and actions. It aims to learn a goal-conditioned policy to reach any goal state reachable from the current state by measuring similarity in controllable aspects of the environment. The DISCERN architecture can handle goal states that are not perfectly reachable, focusing on matching controllable elements with the goal state. It has been effective in Atari games, DeepMind Control Suite tasks, and DeepMind Lab, achieving visually-specified goals without external rewards. The agent interacts with the environment over discrete time steps, observing the current state and selecting actions accordingly. The agent interacts with the environment by observing the current state, selecting actions, receiving rewards, and transitioning to the next state. The aim is to maximize the expected discounted return without the need for extrinsic rewards. The focus is on learning a goal-conditioned policy to reach any reachable goal state from the current state. The agent receives a goal state at the start of each episode and acts according to a goal-conditioned policy to achieve the goal. Rewards are given based on the degree of goal achievement. Training in continuous high-dimensional environments is of interest. In continuous high-dimensional environments, learning goal-conditioned policies with hand-crafted reward functions limits generality. Learning a goal achievement reward function for high-dimensional state spaces is crucial. The goal is to simultaneously learn a goal-conditioned policy and a goal achievement reward function by maximizing mutual information between the goal state and the achieved state. The achieved state is represented as a random variable distributed according to the state distribution induced by running a policy for a certain number of steps. Previous work has shown how to learn abstract options by optimizing mutual information between an abstract option and the achieved state. The expectation term is simplified by rewriting it in terms of the goal distribution and the goal-conditioned policy, and by using a variational distribution to lower bound the expectation term. The entropy term is discarded as it does not depend on the policy parameters or the variational distribution. The overall objective is to optimize the distribution of trajectories by learning a finite set of possible goals that evolve over time. This approach simplifies the problem by focusing on modeling the conditional distribution of intended goals given an achieved state, rather than directly modeling the density of high-dimensional observations like images. This strategy avoids the need to model arbitrary statistical dependencies in observations. The DISCERN objective aims to optimize trajectories generated by a goal-conditioned policy in the environment. This involves updating policy parameters and variational distribution parameters to maximize rewards. Using a non-linear transformation for the reward function has shown practical benefits. Training the variational distribution function q \u03c6 involves learning to predict log q \u03c6 (s g |s T ). The DISCERN objective involves optimizing trajectories with a goal-conditioned policy by updating policy and variational distribution parameters to maximize rewards. Training the variational distribution function q \u03c6 entails learning a reward function through a cooperative communication game between an imitator and a teacher. The imitator communicates a goal state to the teacher by taking actions in the environment, and the teacher guesses the goal state based on the final state reached by the imitator. The DISCERN algorithm involves jointly learning a goal-conditioned policy and reward function through a cooperative communication game between an imitator and a teacher. Goals are proposed using a non-parametric approach with a fixed size buffer of past observations. The space of goals available for training drifts based on the agent's experience. The agent's training goals drift based on experience, leading to a natural curriculum. Goals are sampled randomly from a buffer, with a goal achievement reward function based on state similarity. The cosine similarity between states is used, with a learned embedding space. In our implementation, the final layer features of the convolutional network are used as h(\u00b7) to avoid learning a second network. The reward learner is regularized by keeping h(\u00b7) fixed during training, and \u03be \u03c6 (\u00b7) is trained with a goal-discrimination objective using a subset of goals for each trajectory. The set of possible classes includes the goal for the trajectory and we maximize the log likelihood. The objective aims to increase cosine similarity between embeddings and decrease similarity with decoy embeddings. Subsampling classes is a method for training a softmax classifier. The reward for reaching a state is determined by the goal. The reward function used in this paper is scaled to lie in [0, 1] and is less noisy compared to other suggested functions. The goal-conditioned policy is trained to optimize goal achievement reward, using a goal-conditioned action-value function trained with Q-learning and minibatch experience replay. Goal relabelling or hindsight experience replay is used to update parameters in both the policy and reward learner. In the DISCERN algorithm, the agent substitutes the goal with an observation from the final H steps of the trajectory with probability p HER. This encourages temporally consistent state embeddings and similar observations to have similar embeddings. The algorithm involves an experience-gathering actor process and a centralized learner process. The problem of reinforcement learning with multiple goals has been studied in grid worlds. The DISCERN algorithm involves substituting the goal with an observation from the final H steps of the trajectory with probability p HER. This encourages temporally consistent state embeddings. The algorithm includes an experience-gathering actor process and a centralized learner process. The reinforcement learning problem with multiple goals has been studied in grid worlds. The proposed generalized value functions (GVFs) and Universal Value Function Approximators (UVFAs) are used to represent knowledge about sub-goals. UVFAs BID37 use a function approximator to represent states and goal functions, allowing for compact representation and generalization across related goals. Our work includes learning a reward function for goal achievement alongside the policy, building on recent works in reward learning for goal achievement using Generative Adversarial Networks (GAN) like SPIRAL BID11 and AGILE BID2. Reward functions can be learned using a discriminator trained to distinguish states achieved by the policy from a dataset of instruction, goal state pairs. Various approaches such as adversarial network algorithms, time-contrastive networks, and Universal Planning Networks have been used for reward learning in imitation and goal-conditioned policy learning. Experiments have shown success in constructing reward functions for visually specified goals without expert trajectories. In the context of reward learning in imitation and goal-conditioned policy learning, models can be trained with supervised learning without expert trajectories. Zero-shot imitation of trajectories from images of a desired task is explored, along with unsupervised option discovery methods. Variational Intrinsic Control (VIC) leverages mutual information in an unsupervised control setting, aiming to maximize entropy of options while making them distinguishable from final states. In a hierarchical reinforcement learning context, low-level skills can be discovered through pre-training with simple proxy rewards. Various approaches involve unsupervised representation learning for goal sampling in simulated environments and robot control. In the context of model-free Q-learning for 3-dimensional simulations and robots, training goals are sampled from the model's prior and a reward function is derived from latent codes. A non-parametric approach is used for selecting goals, and the goal space is learned online with the policy. Goal relabelling, known as hindsight experience replay, is a key component of the method. BID45 introduces hindsight replay and all-goal update strategy in non-tabular environments for skill discovery and unsupervised pre-training. BID24 proposes a hierarchical Q-learning system with hindsight replay at different levels. BID31 uses a generalized goal relabeling scheme for training policies based on various possible goals. DISCERN's ability to achieve visually specified goals is evaluated in diverse domains like the Arcade Learning Environment, DeepMind Control Suite, and DeepMind Lab. DISCERN is compared to baseline methods for learning goal-conditioned policies in various environments like DeepMind Control Suite and DeepMind Lab. The comparison includes Conditioned Autoencoder and Conditioned WGAN Discriminator, with details on architecture, distributed training, and hyperparameters provided in the Appendix. The discriminator in the goal-conditioned policy learning process uses real and fake image pairs to generate rewards for the policy. A pixel distance reward based on L2 distance is compared to other baseline methods, all using the same goal-conditioned policy architecture and hindsight experience replay. DISCERN is compared to other methods in achieving visually specified goals in Atari games like Seaquest and Montezuma's Revenge. The simplicity of these games allows for handcrafted detectors to localize controllable aspects. Evaluation was done by running learned goal policies on fixed goals, measuring successful goal reach percentages with different goal buffer substitution strategies. DISCERN successfully achieved a large fraction of goals in Seaquest and Montezuma's Revenge in Atari games, while baselines failed to reliably achieve goals. The baselines struggled due to their objectives being too focused on visual similarity. DISCERN learned to match the position of the avatar in the goal image, ignoring non-controllable elements like fish in Seaquest. DISCERN learned to match the position of the avatar in the goal image, ignoring non-controllable elements like fish in Seaquest. Videos of the goal-conditioned policies learned by DISCERN on Seaquest and Montezuma's Revenge can be found at an anonymous URL provided. The DeepMind Control Suite consists of continuous control tasks, trained on pixel renderings of the scene without directly observing state variables. Actions are discretized to no more than 11 unique actions per environment for ease of implementation and comparison. DISCERN is compared to baselines on a set of 100 goals with 20 trials each, generated randomly after 25 environment steps. Average achieved frames for various tasks in different environments are shown. DISCERN outperforms baselines in goal achievement on multiple Control Suite domains, as shown in Figure 2. The results indicate superior performance in achieving goals, although not across all domains. Further analysis is needed to understand DISCERN's behavior compared to the baselines. DISCERN shows superior goal achievement compared to baselines on Control Suite domains, with a focus on individual dimensions of controllable states. Results indicate reliable matching of major dimensions like cart position and finger pose, while ignoring irrelevant steps. DISCERN outperforms baselines on Control Suite domains by reliably matching major dimensions like cart position and finger pose, while ignoring irrelevant steps. The method learns to match controllable state dimensions within 10% of the possible range, with varying success across different tasks. DISCERN shows strong performance in goal achievement on challenging tasks, as seen in the manipulator domain. The agent learned to achieve similar positions as in goal images in a 3D first-person reinforcement learning environment. Despite not reaching exact positions, the approach demonstrates the ability to learn a reasonable space of goals in various domains. The system presented can learn to achieve goals in various domains without extrinsic rewards or expert demonstrations. It uses a discriminative reward learning objective to recover controllability degrees. The fixed episode length assumption may limit achieving all goals in T steps, suggesting the need for early termination schemes based on embeddings. The system can learn to achieve goals without external rewards or expert guidance. It uses discriminative reward learning to determine controllability levels. Early termination schemes based on embeddings may be needed to address limitations in achieving all goals within a fixed episode length. The goal selection strategy is simplistic but effective, with potential for improvement through more advanced strategies. DISCERN's ability to identify controllable aspects of the observation space is valuable for robust low-level control. Incorporating DISCERN into deep hierarchical reinforcement learning is a natural progression. Incorporating DISCERN into deep hierarchical reinforcement learning involves learning a goal-conditioned state-action value function with Q-learning in a distributed reinforcement learning architecture. Each actor acts -greedily with respect to a local copy of the Q network and sends observations, actions, rewards, and discounts for a trajectory to the learner. Different values of are used for each actor to improve exploration. The learner batches re-evaluation of the convolutional network and LSTM according to action trajectories, periodically updating model parameters. Q-learning is used without off-policy correction, utilizing experience traces from actors. Actor-local replay buffers are maintained for standard and hindsight experience replay. Network architectures resemble those in previous work, with policy and value heads replaced by a Q-function. The same convolutional network is applied to both current and goal states, with final layer outputs concatenated. The convolutional network outputs for s g need only be computed once per episode. A periodic representation of the current time step is included as an extra input to the network. This allows the agent to become better at achieving goal states. The LSTM output is the input to a dueling action-value output network. Two strategies for updating the goal buffer were experimented with. The goal buffer was updated using two strategies: uniform and diverse goal sampling. In the diverse goal strategy, the current observation is considered for addition to the goal buffer with a certain probability, and if selected, a random entry from the buffer is replaced with the new observation based on proximity. The L2 distance in pixel space was used for the diverse sampling strategy. The diverse sampling strategy using L2 distance in pixel space greatly increased coverage of states in the goal buffer. Hyper-parameters used included weight matrix initialization, goal buffer size of 1024, and specific probabilities for goal replacement and addition. The teacher model consisted of an L2-normalized single layer with 32 tanh units. Hindsight experience replay involved substituting a highsight goal 25% of the time. The agent and teacher were trained jointly with RMSProp using a learning rate of 10^-4. Trajectories were 50 steps long for Atari and DeepMind Lab, and 100 steps for the DeepMind control suite. The environment was not reset after each trajectory, and preprocessing involved resizing to 84x84 pixels and scaling pixel values to [0, 1]. Control steps in the point mass domain were 5 times the default, and in other Control Suite domains, a different approach was used. The agent acts every fifth step in the Control Suite domains. The task uses fixed actuator semantics. Discrete action spaces allow for efficient computation of action values. Continuous maximization methods exist but restrict the functional form of Q. To simplify implementation, continuous actions are discretized. In the case of manipulator, the A-dimensional continuous action space is discretized into 3A discrete actions, with a \"diagonal\" discretization where each action sets one actuator to \u00b11 and others to 0. This choice is suitable for manipulator tasks as any position can be achieved by concatenating actuator actions. The Control Suite environments considered were chosen to ensure a reasonable size of the discretized action space. The study explored extensions to continuous domains in future work by running additional baselines on Seaquest and Montezuma's Revenge. One baseline trained the goal-conditioned policy only in hindsight, achieving 12% and 11.4% of goals, comparable to a random policy. Another baseline involved learning a goal achievement reward without hindsight experience replay, achieving 11.4% and 8% of goals. These results highlight the importance of combining hindsight experience replay and a learned goal achievement reward. DISCERN learns to reliably control more dimensions of the underlying state than any of the baselines, showcasing the importance of reward in goal achievement. FIG3 and Figure 4 provide visual representations of goal achievement curves on Control Suite domains using the \"diverse\" goal selection scheme."
}