{
    "title": "r1xQQhAqKX",
    "content": "Instance embeddings are a versatile image representation used for recognition, verification, retrieval, and clustering. The hedged instance embedding (HIB) model addresses uncertainty by treating embeddings as random variables and training under the variational information bottleneck principle. Results on a new N-digit MNIST dataset show the method hedges its bets in the embedding space when encountering ambiguity. The hedged instance embedding (HIB) model improves performance for image tasks by hedging bets in the embedding space when encountering ambiguity. Instance embeddings map inputs to vector representations for various tasks like image retrieval and face recognition. One drawback is the difficulty of modeling uncertainty induced by the input. The hedged instance embedding (HIB) model addresses aleatoric uncertainty induced by input factors like occlusion in images. HIB maps inputs to a region of space to represent uncertainty, spreading probability mass across locations. This method uses random variables to achieve this goal, as shown in Figure 1b with a two-component mixture of Gaussians covering ambiguous clusters. The HIB model uses a two-component Gaussian mixture embedding to address uncertainty in images, spreading probability mass across clusters. It outperforms point embeddings in downstream tasks and exhibits enhanced structural properties. The method described in this section focuses on improving verification for uncertain inputs, enhancing structural regularity in the embedding space, and predicting per-exemplar uncertainty. It involves using contrastive loss to encourage small distances between similar pairs and a margin for dissimilar pairs. The hyperparameter M is typically set heuristically or based on validation-set performance. Soft contrastive loss is a probabilistic alternative to contrastive loss, defining the probability of a pair of points matching using scalar parameters a and b, and the sigmoid function \u03c3(t). This formulation calibrates Euclidean distances into a probabilistic expression for similarity, with a and b serving as a soft threshold. The match probability p(m|z1, z2) is used to formulate the contrastive loss as a binary classification loss based on softmax cross-entropy. In HIB, embeddings are treated as stochastic mappings x \u2192 Z, with Z \u223c p(z|x). The match probability for probabilistic embeddings can be extended using Monte-Carlo sampling. Good results are obtained with K = 8 samples per input image. The computation of p(z|x) involves using a single Gaussian. The computation of p(z|x) involves using a mixture of Gaussians to represent embeddings, enabling easy backpropagation during training. To enhance computational efficiency, mappings share a common CNN stump and are branched with one linear layer per branch. Stratified sampling is used when approximating the match probability. The pipeline for computing match probability involves using stratified sampling with a mixture of Gaussians. The cost and space requirements for computing the stochastic representation are similar to point embedding methods. Training the stochastic embedding involves a soft contrastive loss and the VIB principle, extending the original VIB formulation. The latent encoding z in the information bottleneck principle maximizes mutual information I(z, y) \u2212 \u03b2I(z, x) to capture relevant parts of x for predicting y, while avoiding memorizing irrelevant parts. A tractable variational approximation is used to compute mutual information, with a lower bound derived under the Markov assumption. The VIB approach uses a decoder for distribution q(y|z) and an approximate marginal term r(z) set to a unit Gaussian. It has been shown to be robust to adversarial perturbations and useful for detecting out-of-domain inputs. The method is applied to learn a stochastic embedding by training a discriminative model on matching or mismatching pairs of inputs, minimizing a loss function that includes a negative log likelihood term and a KL regularization term. The VIB approach utilizes a decoder for distribution q(y|z) and an approximate marginal term r(z) set to a unit Gaussian. The optimization of the loss involves the embedding function (\u00b5(x), \u03a3(x)), as well as the a and b terms in the match probability. To address the imbalance of matching pairs, two streams of input sample images are used in each SGD minibatch. The embedding encodes uncertainty for inputs, with a proposed self-mismatch probability measure \u03b7(x) := 1 \u2212 p(m|x, x) \u2265 0. The self-mismatch probability \u03b7(x) quantifies the chance two samples of the embedding belong to different semantic classes. It can be computed for any distribution and is different from prior uncertainty measures for Gaussian embeddings. This section discusses related work in deep learning and probabilistic modeling. One common technique for estimating uncertainty in regression or classification models is Monte Carlo dropout. However, this method is not suitable for measuring input uncertainty, which is the focus of our model. Our approach involves using a parametric representation of uncertainty and conditioning the embedding distribution on the input. The VAE and VIB methods are used to measure input uncertainty. VAE is a latent variable model with a generative decoder and encoder network, while VIB approximates the information bottleneck objective. Point embeddings are trained with metric learning objectives like contrastive and triplet losses, requiring careful sampling schemes. In the context of training point embeddings with metric learning objectives, various alternatives have been explored to improve embedding quality, such as softmax cross-entropy loss with centre loss and clustering-based loss. In this work, a soft contrastive loss is used, building on the idea of probabilistic embeddings proposed by previous studies. In this section, the study proposes using multiple prototypes to represent each word and compares stochastic embeddings to point embeddings in two main tasks: verification and identification. Three methods are compared: baseline deterministic embedding, stochastic embedding with Gaussian embedding, and stochastic embedding with a mixture of Gaussians embedding. Additional details are provided in the Appendix, Section C. The study compares embeddings of different methods on a new dataset called N-digit MNIST, consisting of images with N adjacent MNIST digits. They use a shallow CNN model for computing the embedding function and conduct experiments on clean and corrupted images with N = 2 and N = 3 digits. The dataset will be made open source for reproducibility. The study uses a shallow CNN model with 2 or 3 dimensions for computing the embedding function on N-digit MNIST images. TensorFlow is used to build the networks, with a batch size of 128 and 500k training iterations. The KL-divergence hyperparameter \u03b2 is set to 10 \u22124 throughout the experiments. The study explores the effects of changing \u03b2 in the experiments, showing HIB 2D Gaussian embeddings for clean and corrupt test set subsets. Corrupt images generally have larger embeddings, indicating less certainty. Hedged embeddings capture uncertainty in the class label space, enhancing spatial regularity with classes aligning parallel to the x or y axis. The study examines the impact of covariance matrix parametrization on embeddings, showing improved performance on verification and KNN tasks with HIB embeddings. Hedged instance embeddings enhance spatial regularity, aligning classes parallel to x or y axis. Performance metrics are reported in Table 1, with HIB outperforming in corrupted test images. In the study, the impact of covariance matrix parametrization on embeddings is examined, showing improved performance on verification and KNN tasks with HIB embeddings. The results in Table 1 indicate that HIB outperforms in corrupted test images, with a significant advantage for corrupted input samples. The uncertainty measure \u03b7(x) is used to address the task of estimating when an input can be reliably recognized, with hedged embeddings comprising a single Gaussian. The study examines the impact of covariance matrix parametrization on embeddings, showing improved performance on verification and KNN tasks with HIB embeddings. The uncertainty measure \u03b7(x) is used to estimate when an input can be reliably recognized, with hedged embeddings comprising a single Gaussian. The utility of \u03b7(x) is measured for identification and verification tasks by sorting test examples into bins based on uncertainty levels and computing classification accuracy and match probabilities. Kendall's tau is applied to measure performance. The study explores the impact of covariance matrix parametrization on embeddings, showing improved performance on verification and KNN tasks with HIB embeddings. The uncertainty measure \u03b7(x) is used to estimate reliable recognition, with hedged embeddings comprising a single Gaussian. Kendall's tau is applied to measure the uncertainty-performance correlation. Results are reported for images with N digits and D embedding dimensions. The correlation between uncertainty measure and accuracy is observed, with accuracy generally decreasing as uncertainty increases. The model's uncertainty measure increases as performance drops off, as desired. The correlation between the uncertainty metric and performance metric is quantified using Kendall's tau correlation. Results show that the HIB uncertainty metric correlates with task accuracy, even within clean input images. Hedged instance embedding shows a correlation with task performance. Hedged instance embedding captures uncertainty in image mapping, improving performance on tasks like verification and identification, especially with ambiguous input. It also helps estimate embedding uncertainty correlated with downstream task performance. Future work includes exploring higher-dimensional embeddings and tougher datasets. In an early experiment, HIB is applied to cat and dog instance embedding with 20D embeddings. Consideration for the \"open world\" scenario, with novel classes in the test set, is also intriguing. Preliminary experiments suggest that \u03b7(x) correlates well with detecting occluded inputs, but does not work as well for novel classes. N-digit MNIST is a new dataset with an exponentially large number of classes based on MNIST, suitable for embedding-style classification methods. Further modeling of epistemic uncertainty is left for future work. The N-digit MNIST dataset is created by concatenating N MNIST digit images horizontally, respecting training and test splits. It is designed for easier evaluation of embedding algorithms with an exponentially increasing number of classes, making it more suitable for evaluation than other synthetic datasets. The N-digit MNIST dataset is created by concatenating N MNIST digit images horizontally for easier evaluation of embedding algorithms. Uncertainty is injected by randomly occluding regions of images during training and test time, with a 20% occlusion chance. Twin datasets, clean and corrupt, are prepared for testing with digit images either not corrupted or always occluded. Soft contrastive loss is proposed as a building block for the HIB. Soft contrastive loss has a conceptual advantage over vanilla contrastive loss as it eliminates the need to hand-tune the margin hyperparameter M. Results show that soft contrastive loss outperforms the vanilla version across a range of M values, with performance consistently better in both verification and identification tasks. This new formulation reduces the number of hyperparameters in the learning process without sacrificing performance. Additional results include task performance and uncertainty measure correlations for 4D embeddings with 3-digit MNIST, as well as 2D and 3D embeddings of digit images using a MoG representation. In Table 4, results of 4D embeddings with 3-digit MNIST are reported, showing task performance saturation with verification accuracy above 0.99 on clean input images. HIB provides a slight performance improvement over point embeddings for corrupt images, and task performance correlates with uncertainty. HIB was applied to learn instance embeddings for identifying pets using a dataset of cat and dog images, achieving good results without artificial corruption. The experiment evaluates verification task performance on a test set of pet images, achieving 0.777 balanced accuracy. Correlation between performance and task accuracy is high at 0.995, showing that HIB scales to real-world problems with sources of corruption. As hedged instance embedding training progresses, subsets of classes are strategically placed in the embedding space to impact organization. The organization of the embedding space is impacted by the probability mass, with class centers roughly axis-aligned. 2-digit MNIST is embedded into a single dimension to observe class placement along the number line. Hedged instance embedding reduces objective loss by grouping confusing categories together, encouraging nearby placement of classes that share a tens or ones digit. The resulting embedding space is assessed by deriving centroids for each of the 100 classes. The embedding space organization is impacted by probability mass, with class centers roughly axis-aligned. Hedged embeddings outscored point embeddings on four trials, with scores ranging from 76 to 80 versus 42 to 74. Runs of consecutive class pairs that share a ones or tens digit average 4.6 classes for hedged embeddings and 3.0 for point embeddings. KL divergence weight impacts the embeddings, with smaller weights approaching points and larger weights leading to ellipsoids. When the weight \u03b2 is too small (10 \u22126 ), embeddings approach points, and when too large (10 \u22122 ) embeddings approach the unit Gaussian. The regularization hyperparameter \u03b2 \u2265 0 controls the weight of the KL divergence regularization in the training objective. Increasing the KL term weight induces an overall increase in variances of embeddings. Mild improvements in main task performances are observed when using KL divergence regularization. Performances improve with KL divergence regularization, such as KNN accuracy increasing from 0.685 to 0.730 for N = 3, D = 3. Uncertainty quality also improves, with KNN accuracy and uncertainty becoming uncorrelated at \u03b2 = 0 (-0.080 for clean, 0.183 for corrupt inputs) and well-correlated at \u03b2 = 10 \u22124 (0.685 for clean, 0.549 for corrupt inputs). KL divergence aids generalization and enhances uncertainty measures by increasing overall variances of embeddings. The goal is to train a discriminative model for match prediction, not class label prediction. The VIB loss follows from the original VIB with additional independence assumptions. The VIB objective follows from the original VIB with two additional independence assumptions: samples in the pair are independent, and embeddings do not depend on the other input. The VIB objective is I((z1, z2), m) \u2212 \u03b2I((z1, z2), (x1, x2)). The first term is variational bounded using the approximation q(m|z1, z2) of p(m|z1, z2). The VIB objective is derived from the original VIB with additional independence assumptions. It involves variational bounds using the approximation q(m|z1, z2) of p(m|z1, z2) and is expressed as I((z1, z2), m) \u2212 \u03b2I((z1, z2), (x1, x2)). The inequalities in the equation are based on the non-negativity of KL-divergence and entropy. The second term is variational bounded using the approximation r(z i ) of p(z i |x i ). The VIB objective, derived from the original VIB with additional independence assumptions, is bounded from below by a specific expression involving KL-divergence terms and a loss function. This expression is equivalent to the loss function L VIBEmb in the previous equation."
}