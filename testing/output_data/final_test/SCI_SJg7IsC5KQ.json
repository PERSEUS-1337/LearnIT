{
    "title": "SJg7IsC5KQ",
    "content": "In this paper, the theoretical underpinnings of batch normalization (BN) are explored through a modelling approach using ordinary least squares (OLS). The analysis reveals interesting properties such as a scaling law, convergence for various learning rates, acceleration effects, and insensitivity to learning rate choices. These findings extend beyond OLS to more complex supervised learning problems, indicating a new direction in understanding the mathematical principles behind batch normalization. In this paper, the convergence and stability of gradient descent with batch normalization (BNGD) are studied through a modeling approach using ordinary least squares regression. The analysis aims to understand the effect of BNGD in a simplified supervised learning problem and serves as a mathematical model for general supervised learning tasks. The modeling approach in this study focuses on the convergence and stability of BNGD in least-squares regression. It is shown that BNGD converges for any constant learning rate \u03b5 \u2208 (0, 1], regardless of the conditioning of the regression problem, in contrast to GD. Insights from the analysis include the presence of a scaling law governing \u03b5 and the initial condition in BNGD, which is not present in GD. The analysis reveals that BNGD converges for any \u03b5 > 0 as long as \u03b5 a \u2208 (0, 1], with an acceleration effect and regions of \u03b5 where performance is insensitive to changes. These results are precise mathematical findings for the simplified model, and efforts are made to show they are general characteristics of BNGD. Batch normalization (BN) has become a crucial tool for enhancing the stability and efficiency of training deep neural networks. Recent studies have explored the convergence, stability, and scaling behaviors of Batch Normalization Gradient Descent (BNGD) on various datasets and model architectures. The insights from idealized analysis align with practical scenarios, showcasing the significance of BN in supervised learning problems. The authors proposed a variant of BN called diminishing batch normalization (DBN) algorithm, analyzing its convergence to a stationary point of the loss function. Recent studies have shown that higher learning rates of batch normalization have a regularizing effect. Another work considered the convergence properties of BNGD on linear networks and special problems like learning halfspaces. In the OLS case, BNGD converges linearly with an adaptive choice of dynamic learning rate schedule. The present research focuses on studying the BNGD algorithm itself, specifically with constant learning rates and gradient descent on rescaling parameters. Convergence is proven to occur even with arbitrarily large learning rates for \u03b5, as long as 0 < \u03b5 a \u2264 1. This analysis contrasts with previous studies on modified versions of BNGD, emphasizing the importance of analyzing the non-asymptotic, constant learning rate case in the context of BN. In this work, the analysis focuses on the BNGD algorithm with constant learning rates, highlighting the advantage of using a bigger learning rate compared to gradient descent. BN can be seen as a form of overparameterization, introducing additional parameters to improve algorithm convergence and stability. The paper outlines the OLS problem, compares GD and BNGD as solutions, and analyzes the convergence of BNGD for the OLS model. The study focuses on comparing the BNGD algorithm with gradient descent in a linear regression model. It assumes a noisy linear relationship between the input and output variables. The analysis simplifies by assuming a positive definite covariance matrix for the input and a zero mean. The eigenvalues of matrix H, denoted as \u03bb i (H), play a key role in optimization problems. The condition number \u03ba is defined as the ratio of the maximum and minimum eigenvalues of H. The OLS method for parameter estimation involves minimizing a certain function J 0, with the gradient \u2207 w J 0 (w) leading to the minimizer w = u. The GD method for optimization converges with a specific learning rate \u03b5, where the optimal rate is \u03b5 opt = 2 \u03bbmax+\u03bbmin. Chapter 4 of BID16 discusses the optimal learning rate \u03b5 opt = 2 \u03bbmax+\u03bbmin for convergence in optimization problems. Batch normalization is applied to the output z = x T w, with a normalization transform defined by \u03c3 := \u221a w T Hw. The objective function J(a, w) is no longer convex and has nontrivial critical points. The nontrivial critical points in the optimization problem are global minimizers, with degenerate Hessian matrices at each critical point. Saddle points are strict, simplifying gradient descent analysis. Batch normalization gradient descent (BNGD) is used with different learning rates for a and w. The iteration sequence is analyzed mathematically for BNGD on the OLS problem, establishing scaling properties and important results. The BNGD iteration possesses a scaling property and the dynamical properties are governed by a set of numbers. Two equivalent configurations have invertible linear transformations. The scaling property of BNGD ensures that equivalent configurations converge or diverge together with the same rate. It is independent of the loss function structure and is valid for general problems where BN is used. This property is crucial in determining the dynamics of BNGD. The scaling property of BNGD is crucial in determining its dynamics and convergence properties. The convergence result states that the iteration sequence converges to a stationary point for any initial value and \u03b5 > 0, as long as \u03b5 a \u2208 (0, 1]. Sufficient conditions for converging to global minimizers are also provided. The sequence { w k } in BNGD is shown to converge to a finite limit for any initial value and \u03b5 > 0. The proof is detailed in Theorem A.17 and preceding Lemmas. Additionally, the set of initial values converging to saddle points has Lebesgue measure 0 under certain conditions. BNGD converges for all step sizes \u03b5 > 0. BNGD converges for all step sizes \u03b5 > 0, independent of the spectral properties of H. The update coefficient in BNGD changes to a complicated term named the effective step size \u03b5 k, leading to a different convergence rate compared to GD. The effective learning rate \u03b5 k determines the convergence rate of e k in BNGD. Linear convergence is achieved when \u03b5 k \u2208 (\u03b4, 2/\u03bb max \u2212 \u03b4), with an acceleration effect over GD when close to a minimizer. The Hessian matrix for BNGD at a minimizer has better spectral properties than H, leading to acceleration effects. When close to a minimizer, the contraction coefficient can be improved, resulting in a lower coefficient for faster convergence. The Hessian matrix for BNGD at a minimizer has better spectral properties than H, leading to acceleration effects and a lower coefficient for faster convergence. The optimal BNGD could have a faster convergence rate than the optimal GD, especially when \u03ba * is much smaller than \u03ba. The effective learning rate \u03b5 k and convergence rate depend on \u03b5, with asymptotic estimates provided for small and large \u03b5 values. The step size in BNGD has order O(\u03b5 \u22121 ), with a range of learning rates where performance is insensitive to choice. BNGD allows for a variety of learning rates without affecting performance, unlike GD. This local insensitivity result has a large practical interval. The insensitivity of BNGD in practice is significant. Key findings include a scaling law governing BNGD, convergence for any learning rate \u03b5 > 0, and intervals where performance is not sensitive to \u03b5 choice. Validation of these claims on the OLS model is followed by insights applicable beyond OLS. Testing convergence and stability of BNGD for the OLS model is conducted. The scaling property of BNGD allows for setting the initial value w0 to have the same 2-norm as u. Testing the loss function of BNGD compared to optimal GD shows that BNGD can have a better convergence rate in certain configurations. This acceleration is attributed to the pseudo-condition number of H. The BNGD algorithm accelerates convergence by adjusting step sizes based on the pseudo-condition number of H. It extends the range of optimal step sizes, allowing for better convergence rates compared to optimal GD. Proper initialization of parameters is crucial for robust performance, with small initial conditions improving robustness. However, improper initialization weakens the algorithm's effectiveness. The BNGD algorithm accelerates convergence by adjusting step sizes based on the pseudo-condition number of H, extending the range of optimal step sizes for better convergence rates. Improper initialization of BN parameters weakens the power of BN, as seen in practical examples like BID3. Experiments are conducted on deep learning with standard classification datasets to explore key findings. The study explores the performance of different neural networks on MNIST, Fashion MNIST, and CIFAR-10 datasets. Various network architectures are used with specific loss functions and initialization methods. Batch normalization is applied to all layers with specific parameters set for optimal performance. The study examines the scaling property of batch normalization in neural networks, testing it with different learning rates. It also discusses the impact of regularization on the network's stability for large learning rates. The study analyzes the impact of batch normalization on neural network convergence rates. GD and BNGD with separate learning rates show different stability levels. Batch normalization accelerates convergence in deep networks. Batch normalization parameters are randomly initialized by the Glorot scheme and multiplied by a parameter \u03b7, while GD parameters are similar. The optimal learning rates in BNGD have a wide range, crucial for understanding its acceleration in deep neural networks. BNGD's convergence rate is less sensitive to the learning rate parameter compared to GD, potentially leading to faster convergence. Performance comparison of BNGD and GD on MNIST, Fashion MNIST, and CIFAR-10 datasets is shown in Figure 3. In this paper, the performance of BNGD and GD methods on MNIST, Fashion MNIST, and CIFAR-10 datasets is evaluated based on the loss value at epoch=1. The study investigates the dynamical properties of batch normalization using a modeling approach, with results showing interesting behavior such as scaling laws, robust convergence, acceleration, and insensitivity to learning rates. The findings are qualitatively valid for general models, beyond just the OLS model used as a reference point. The study evaluates the performance of BNGD and GD methods on various datasets, showing interesting properties of batch normalization like scaling laws and robust convergence. Future directions include extending results to more general settings and investigating better generalization errors. The objective function and gradients are discussed, with trivial critical points identified. The objective function J(a, w) has trivial critical points, but nontrivial critical points are global minimizers with specific properties. The Hessian matrix at saddle points has at least one negative eigenvalue, while the Hessian matrix at minimizers is positive semi-definite. Lemma A.1 provides an estimate of eigenvalues for a positive definite matrix H and its conjugate transpose H*. The eigenvalues of H and H* satisfy specific inequalities, with H* being positive semi-definite. The lemma also includes corollaries related to the spectral properties of H*. The H* -seminorm of a vector x is defined as x T H * x, where x H * = 0 if x is parallel to u. The pseudo-spectral radius of I \u2212 \u03b5H * is defined as \u03c1. Corollaries state that the pseudo-condition number of H * is less than or equal to the condition number of H, with strict inequalities under certain conditions. The dynamical system defined by equations is determined by a set of configurations and has a scaling property. The gradient descent method for a general problem and its BN version is given by an iteration formula. The BNGD method has certain properties when batch normalization is used. The BNGD method simplifies analysis by allowing specific parameter settings. The step size is bounded for certain values, ensuring the sequence remains within a constant limit. The iterations are defined by specific formulas and properties. The lemma A.11 ensures convergence of iterations to a minimizer under certain conditions. It proves the boundedness of w_k and the convergence of its direction to u. The proof involves showing that w_k is bounded and using estimates to demonstrate the boundedness of w_k. The lemma A.12 discusses the convergence of the sequence (a_k, w_k) to a global minimizer under specific conditions. It involves bounding a_k, defining \u03b5_0, and setting initial values for convergence. Lemma A.12 discusses the convergence of the sequence (a_k, w_k) to a global minimizer under specific conditions, with a focus on bounding a_k and defining \u03b5_0 for convergence. If the initial value satisfies certain conditions, the sequence converges to a global minimizer as well. Lemma A.13 states that if the step size satisfies certain conditions, the sequence (a_k, w_k) converges to a saddle point. The proof involves considering cases where a_k and w_T_k_g have different signs, leading to convergence. Additionally, Lemma A.14 discusses the convergence of positive series under specific conditions. Lemma A.14 discusses the convergence of positive series under specific conditions, where the series tends to zeros. Lemma A.15 introduces the separation property of a set S into two parts, S1 and S2, with distinct characteristics. The proof is based on the positivity of H and the geometric interpretation is illustrated in FIG4. The sequence (a_k, w_k) converges for any initial value (a_0, w_0). The convergence of w_k implies k a^2 k q_k is summable, leading to lim k\u2192\u221e w_k+1 \u2212 w_k = 0. The iteration of series (a_k, w_k) converges to a saddle point or a global minimizer. The convergence is guaranteed as long as \u03b5 a \u2265 1, with the set of initial values leading to saddle points being of measure zero. The proof involves analyzing the gradient descent on non-convex objectives and utilizing the real analytic property of the BN loss function. By showing that the Jacobian of T(x) is nonzero for almost all x in R^d, it is proven that the set {x : det(I - \u03b5\u2207^2J(x)) = 0} is of measure zero. If \u03b5 > 0, det(I - \u03b5\u2207^2f(x*)) = 0 and \u2207^2f(x*) has a negative eigenvalue, then there exists a neighborhood U of x* such that the set B has measure zero. The proof involves using the transform function F(x) := x - \u03b5\u2207f(x) and the central-stable manifold theorem. If \u03b5 = \u03b5 \u2265 1, the set of initial values for BN iteration converging to saddle points is of Lebesgue measure zero. The proof involves using Lemma A.18 and Lemma A.19 to show that the set of saddle points converges to saddle points is of Lebesgue measure zero. The saddle points set W is defined as {(a*, w*): a* = 0, w*Tg = 0}. For each saddle point x* of the BN loss function, \u03b5 \u2265 1 allows the use of Lemma A.19, leading to a neighborhood Ux* where a certain set Bx* is of measure zero. The neighborhoods of all saddle points in W form a cover of W, implying that a certain set A0 is also of measure zero. The convergence rate can be improved if H* has better spectral properties. The modification MBNGD improves convergence properties by enforcing a specific condition. The iteration sequence converges for any initial value and step size. The proof shows that the sequence is bounded and converges to a global minimizer unless a specific condition is met. The modification MBNGD ensures convergence to a global minimizer unless a specific condition is met. The sequence is bounded and converges to a vector parallel to u. The MBNGD method tests convergence and stability for the OLS model using a diagonal matrix H with an increasing sequence. The iteration is executed multiple times with randomly chosen initial values. The performance of GD and MBNGD is compared using effective step size, loss, and error values. The geometric mean is highlighted as more reliable than the arithmetic mean. The geometric mean (G-mean) is more reliable than the arithmetic mean (Amean) as it converges quickly with increasing tests. MBNGD shows better convergence rate than GD due to lower pseudo-condition number. BN extends optimal step size range, allowing for larger intervals with similar or better convergence rates. In a 100-dimension scenario with condition number \u03ba = 2000, MBNGD does not show significantly improved convergence rate compared to GD with optimal learning rate due to decreased difference between eigenvalues of H and H*. Larger dimensions result in larger intervals for 'optimal' step size, with the effect of condition number on the interval being small."
}