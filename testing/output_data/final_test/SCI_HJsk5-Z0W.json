{
    "title": "HJsk5-Z0W",
    "content": "Traditional factorization algorithms have limitations in supporting features and scalability, while neural methods offer flexibility but are often application-specific. Novel deep factorization methods proposed allow efficient feature representation, such as describing items with natural language linearly to vocabulary size. This enables prediction for unseen items and avoids cold start problem. The architecture shows improved training times and accuracy compared to shallow methods, with experiments suggesting generalization of previous neural architectures. Predictive tasks traditionally solved with factorization are now being explored within neural networks. Deep Structured Factorization Machine is a general-purpose factorization technique for neural networks that enables efficient factorization of datasets with complex feature sets, without losing interpretability. It addresses limitations of previous factorization methods and allows for scalable applications, such as natural language processing, where traditional methods struggle with large vocabularies. Factorization in natural language scales quadratically with vocabulary size, but our solution enables linear runtime complexity. Our Structured Deep Factorization method shows significant improvements in predictive accuracy and runtime compared to other models. Factorization Machine is a successful general-purpose method, formulated as an extension to polynomial regression. Factorization Machine is a general-purpose method that replaces individual pairwise parameters with shared parameters to encode interactions between features. It has a linear link for continuous outputs and a logistic link for binary outputs. The model has n \u00d7 r interaction parameters, where n is the total number of features and r is the rank of the vector of factors. Factorization Machine dramatically reduces the number of parameters by assuming strong parameter sharing for pairwise interactions. Inference runtime scales quadratically with the number of features. When using one-hot encoding for two categorical features, Factorization Machine is equivalent to Matrix Factorization. Softening the assumption of parameter sharing has not been studied in the literature before. In \u00a7 3.1, the study explores softening the assumption of parameter sharing, which has not been previously studied. In \u00a7 3.2, the issue of intractability for large feature sets in Factorization Machine is addressed by introducing Structured Factorization Machine, a model that only allows interactions between features from different groups. In this section, a model with four features is considered, with interactions defined between specific features. A comparison is made between existing factorization methods and novel models introduced. Polynomial Regression is contrasted with Factorization Machine, highlighting the advantages of the factorization assumption in cases of shared feature structure. The potential for overfitting in Polynomial Regression due to a larger number of parameters is also discussed. The text discusses the advantages of independent interactions in large feature sets and the ease of optimizing models with individual parameters. It introduces a hybrid factorized-independent method called Structured Deep-Out Factorization Machine, which combines the benefits of both models by factorizing embeddings and keeping parameters independent for each interaction group. Structured Deep-In Factorization Machine allows grouping features and building latent factors from them, treating each group as a single entity. The model extracts features from each group and projects them into embeddings, with the option to use different activation functions. The approach simplifies the interpretation of parameters by constraining them to be non-negative and choosing linear or ReLU activations. Future work may explore deeper networks. The Structured Deep-In Factorization Machine groups features into entities for higher abstraction levels. Feature extraction functions allow interactions within groups, while entities interact across groups via embeddings. This approach addresses the cold-start problem for large feature sets like natural language. Factorization Machine is not tractable due to the quadratic dependence on vocabulary size. Using matrix factorization with unique one-hot indices avoids this issue but leads to cold-start problems. The Deep-In model can overcome this by utilizing alternative item descriptions like images or text. The text discusses addressing the problem of quadratic dependence on vocabulary size in Factorization Machine by treating words as indexed features within a structured feature group. A Neural Factorization model with a learned neural function is shown to improve predictive accuracy for various tasks. The text discusses the drawbacks of using regular Factorization Machine for natural language features and proposes combining Neural Factorization with Structured Deep-In Factorization Machine to address the cold-start problem or quadratic dependence on vocabulary. The parameters of a deep factorization model can be learned using training data to minimize a loss function for labeling and classification tasks. The text discusses the optimization of regularization parameters for different tasks using binary cross-entropy and mean squared error. Neural models are trained using mini-batch updates with the Keras programming toolkit, which includes the ADAM optimization algorithm. For multi-label classification tasks, Structured Factorization Machine is used with binary output. The text discusses the optimization of regularization parameters for different tasks using binary cross-entropy and mean squared error. Neural models are trained using mini-batch updates with the Keras programming toolkit, which includes the ADAM optimization algorithm. For multi-label classification tasks, Structured Factorization Machine is used with binary output. The label to predict is associated with the input, and datasets contain only positive labels. Sampling strategies are chosen based on validation error, excluding actual positive labels for each training example. Development and test sets are defined for experiments, with a portion of the development set used for early stopping or validating hyper-parameters. The development set is used for early stopping and validating hyper-parameters. Results are reported on a single training-test split using the AUC metric for labeling and classification tasks. Sampling labels based on frequency ensures a model predicting popular labels would have an AUC of 0.5. Evaluation strategy may underestimate performance due to false negatives, but consistency allows for meaningful comparison of AUC values. In experiments, AUC is chosen as a metric for classification and ranking tasks, while MSE is used for regression. Regularization is avoided to prevent overfitting, relying on early stopping instead. Factorization algorithms are preferred for smaller feature sets to reduce overfitting. The study compares deep-out and shallow structured models under the same conditions to investigate the impact of relaxing factorization constraints on larger and more diverse features. Details on grid-search for reproducibility are provided in the appendix. In experiments, AUC is chosen as a metric for classification and ranking tasks, while MSE is used for regression. Regularization is avoided to prevent overfitting, relying on early stopping instead. Factorization algorithms are preferred for smaller feature sets to reduce overfitting. The study compares deep-out and shallow structured models under the same conditions to investigate the impact of relaxing factorization constraints on larger and more diverse features. Details on our grid-search are provided in Appendix A.1. We test this on classification, regression, and collaborative filtering tasks, including subscription prediction, airline delays, and course prediction tasks with specific dataset details. The dataset contains 36 million enrollment instances from 4.7 million students and 2,578 courses. Features include student id, course id, subject levels, university id, student year, and data source types. Grouping is done for continuous and categorical variables. Adding deep-out parameters improves forecasting quality. The deep-out approach significantly improves AUC by roughly 35% in subscription prediction tasks compared to shallow factorization. The deep method is almost twice as fast as the shallow method on the courses dataset, with some performance improvement. Using a Convolutional Neural Network (CNN) for feature extraction in natural language processing tasks, we experiment on two datasets to predict labels from text. The dataset contains job posting texts annotated with skills useful for predicting job requirements. Matrix Factorization is used for feature groups, outperforming Structured Deep-In Factorization Machine. Factorization Machine is intractable with a large vocabulary. Matrix factorization struggles with unseen items, compared to Collaborative Topic. Matrix factorization struggles with unseen items, so we compare it to Collaborative Topic Regression (CTR). We use the CiteULike dataset to predict users who may have added a given article to their library. We compare the performance of Structured DeepIn Factorization Machine with CTR using pre-defined cross-validation splits. On the warm-start condition, CTR has an AUC of 0.9356. Deep-In FM outperforms CTR on warm-start condition with an AUC of 0.9401, degrading to 0.9124 on unseen documents. It also performs well on cold-start prediction for Concepts and Skills dataset. Deep-In FM can be trained over ten times faster and further improvements are possible by optimizing architecture and hyper-parameters. DeepCoNN is a deep network designed for predicting customer ratings using textual reviews. It concatenates text from all reviews for an item and user, then feeds it into a feature extraction function followed by a factorization machine. Structured Deep-In Factorization Machine shows a significant performance increase compared to DeepCoNN on the Yelp dataset. The Structured Deep-In Factorization Machine offers a significant performance boost over Matrix Factorization in terms of mean squared error improvement. It is more efficient than DeepCoNN, especially for datasets with a large number of reviews per item and user. The method enables prediction for unseen items and addresses the cold-start problem by factorizing large feature sets. Future work may relax the requirement for domain knowledge in defining feature groups and extraction functions. Our methods generalize previously published single-purpose neural networks, offering more flexibility and making fewer assumptions about training data. The factorization hypothesis may be too restrictive, as evidenced by higher predictive performance when relaxed. Further experimentation can determine the conditions under which our methods are effective. The factorization hypothesis may be too restrictive, as shown by higher predictive accuracy and training speed improvement when relaxed. Experimental results outperform a text-specific algorithm, even with the same feature extraction CNN. This suggests the need for ad-hoc networks should be considered in relation to the enhancements over a general-purpose method. Our work introduces a general purpose factorization algorithm for efficient inference on arbitrary feature sets. Experimentation on the courses dataset involved killing experiments that took too long, with some shallow experiments not executed. Details of the feature extraction function \u03c6 used in labeling experiments are described in \u00a74.2.1 and \u00a74.2.2, with an overview of the network provided in FIG3. The text discusses building a vocabulary from popular words in datasets, converting words to one-hot encodings, and using convolutional filters to extract features from embeddings. The process involves assigning vectors to words, applying filters on groups of adjacent word embeddings, and passing the matrix through various layers for analysis. We apply filters on groups of adjacent word embeddings to extract features. The filters are matrices of learned parameters, and the output is passed through a ReLU activation. To handle inputs of different lengths, we use 1-max pooling to select the maximum value from the output vector of each filter. This results in a representation of the passage independent of its length. Higher-level features are learned using a fully connected layer with p units. The final embedding for x j is computed by a dense layer with r output units and an activation function, where r is the embedding size of indexable items. The vocabulary size is set to 100,000 words, and input embedding size to 50. Input word embeddings and label embeddings are initialized using Word2Vec. The architecture and hyperparameter settings were not varied, yielding good results on diverse datasets. The CNN hyperparameters for DeepCoNN were set with 1,000 convolutional filters, a dropout rate of 0.1, and PReLU activations for the final layer to prevent ReLU units from 'dying'. The architecture includes a word embedding lookup table, convolutional layer, 1-max pooling, and a fully connected layer with 100 convolution filters and 50 units. The authors used hyper-parameters for Deep-In FM with 100 convolution filters and 50 units for the fully connected layer. They set the word embedding size to 100, vocabulary size to 100,000, and maximum document length to 250. The comparison with Collaborative Topic Regression involved choosing the embedding size r from {5, 10, 15}. Other parameters were set as reported by the authors. Results are shown in TAB5."
}