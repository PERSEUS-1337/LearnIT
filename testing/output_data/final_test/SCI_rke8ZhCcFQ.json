{
    "title": "rke8ZhCcFQ",
    "content": "Graph convolutional networks (GCNs) are commonly utilized for node classification in graphs with limited supervision. In this paper, the vulnerability of GCNs to perturbations on adjacency and feature matrices of existing nodes is explored. A method attacking GCNs by adding fake nodes is proposed, using a Greedy-GAN algorithm to make fake nodes indistinguishable from real ones. The non-targeted attack reduces GCN accuracy to 0.10, while the targeted attack achieves a success rate of 0.99 for attacking the whole dataset and 0.94 on average for attacking a single node. Graphs are crucial in various real-world applications like social networks, biological networks, and attribute graphs. Node classification on graphs is a key task, where deep learning models like Graph Convolutional Networks (GCNs) have shown top performance. GCNs are also widely used in cybersecurity to autonomously label nodes, reducing the workload on security experts. This is particularly useful for managing dynamic networks like WiFi networks in universities and web services in companies. Recent studies have focused on the robustness of Graph Convolutional Networks (GCNs) in various applications. BID34 BID6 developed algorithms to attack GCNs by altering edges and features, reducing classification accuracy. However, changing existing nodes' edges or features is often impractical. Instead, adding fake nodes to the network can interfere with the classification results of existing nodes. Novel algorithms have been introduced to design fake nodes that successfully reduce GCN's performance on existing nodes. In this paper, two algorithms, Greedy attack and Greedy-GAN attack, are proposed to add fake nodes to attack GCNs. The challenges include designing adjacency and feature matrices for fake nodes in a discrete input space and making fake nodes indistinguishable from real ones. The contributions of the paper include being the first to study adding fake nodes to attack GCNs without manipulating existing nodes. The paper introduces a Greedy attack algorithm for adding fake nodes to attack GCNs without manipulating existing nodes. A Greedy-GAN algorithm is also proposed to optimize the discriminator and attacker simultaneously. Experimental results show successful targeted attacks on datasets like Cora and Citeseer. Adversarial attacks on deep neural networks have been extensively studied, with various algorithms proposed for generating adversarial examples in image classification tasks. These include FGSM, IFGSM, C&W attack, and PGD attack. In the black-box setting, attack algorithms using finite difference techniques have shown high success rates. Transfer and ensemble attacks have also been explored. Additionally, CNN-related attacks on semantic segmentation and object detection have been investigated. In the context of adversarial attacks on deep neural networks, attacks on semantic segmentation, object detection, image captioning, and visual QA have been studied. Attacks on text classification involve methods like FGSM, word deletion, replacement, and insertion. Black-box attacks involve developing score functions and adding misleading sentences. GANs are used to generate natural adversarial examples, and machine translation systems are targeted by changing words in text. Graph Convolutional Neural Networks (GCNs) are used for classification of graph nodes in cyber security and recommender systems. They aggregate information from nodes and neighbors to improve performance compared to traditional methods. Recent variations of GCNs have been proposed, with a focus on a commonly used structure. Adversarial attacks on GCNs involve changing nodes' links and features using methods like FGSM and optimizing a surrogate model named Nettack. In cyber security and recommender systems, Graph Convolutional Neural Networks (GCNs) are used for node classification by aggregating information from nodes and neighbors. Recent variations focus on a commonly used structure. Adversarial attacks involve changing nodes' links and features using methods like FGSM and optimizing a surrogate model named Nettack. BID6 showed vulnerability to attacks by manipulating graph structure, proposing a reinforcement learning-based attack method. Novel algorithms add fake nodes to interfere with GCN performance, tested in targeted and non-targeted attacks. Graph Convolutional Neural Networks (GCNs) are utilized for node classification by aggregating information from nodes and neighbors in cyber security and recommender systems. A common approach involves iteratively aggregating features from neighborhoods using an adjacency matrix and an activation function like ReLU. The model typically consists of multiple layers with a fully connected layer for classification. One application is semi-supervised learning node classification on graphs using a two-layer GCN. In this section, the model simplifies the formation of a target network for attacking Graph Convolutional Neural Networks (GCNs) by inserting fake nodes with fake features into the graph. This manipulation changes the adjacency matrix and feature matrix of the network. The attack can be either non-targeted, aiming to lower accuracy uniformly, or targeted, forcing the GCN to assign desired labels to nodes. The goal is to design fake nodes to lower classification accuracy on existing nodes in Graph Convolutional Neural Networks (GCNs). Two algorithms are presented: Greedy attack updates links and features individually, while Greedy-GAN attack uses a discriminator to generate unnoticeable fake node features. The objective function is defined to modify the graph and features by adding fake nodes. The objective is to add fake nodes to lower classification accuracy in Graph Convolutional Neural Networks (GCNs). A greedy approach is proposed, starting with all zeros for B, C, X fake, and adding one feature and edge at each step based on gradient information. Unlike images, graphs have discrete values, making gradient-based techniques like FGSM and PGD unsuitable. The Greedy attack algorithm aims to manipulate the adjacency matrix by maximizing elements in the gradient of J(A, X). It ensures no duplicate links or features are added and allows for adjusting the frequency of updates. Additionally, a Generative Adversarial Network (GAN) approach involves adding a discriminator to generate fake features resembling the original ones. The Greedy attack algorithm manipulates the adjacency matrix to maximize elements in the gradient of J(A, X). A Generative Adversarial Network (GAN) approach involves a discriminator to generate fake features resembling the original ones, using binary cross entropy loss for optimization. The objective function is determined by a parameter c, balancing the weight of the discriminator and GCN performance. The Greedy-GAN algorithm uses a GAN-like framework to train features and discriminator parameters iteratively, supporting both adding and dropping links and features based on the absolute gradient of elements. It extends to targeted attacks on GCNs by manipulating adjacency and feature matrices to misclassify nodes. The goal is to manipulate links and features of fake nodes to classify nodes as a desired class. Methods for attacking the whole dataset and a single node are presented. In targeted attacks, the objective function is defined based on the target label for adversarial attack. Greedy attack and Greedy-GAN attack are used to find fake nodes. For attacking a single node, fake nodes with target labels are added, and the objective function is updated by Greedy attack. Greedy-GAN attack is not performed due to the imbalance between real and fake nodes. Cora and Citeseer attribute graphs are used as benchmarks with a labeled/unlabeled split. The influence of adding fake nodes with fake features and labels is investigated. The results show the impact on GCN classification accuracy for non-targeted attacks. The effectiveness of random, Greedy, and Greedy-GAN algorithms in attacking a GCN is compared based on classification accuracy for non-targeted attacks. Greedy-GAN performs better than Greedy as it generates features closer to real nodes, resulting in lower attack success. Discriminators find it harder to differentiate between real and fake features under Greedy-GAN attacks. Targeted attacks by Greedy-GAN aim to make fake nodes indistinguishable from real ones, unlike Greedy attacks. Two experiments are conducted: attacking the whole dataset and attacking a single node. Both algorithms are susceptible to targeted attacks, with Greedy-GAN making fake nodes harder to detect. Adding fake nodes with target labels results in high success rates, with minimal impact on label distributions. On average, 13 edges and 10 features are added per fake node in the experiments. In experiments, 13 edges and 10 features are added per fake node on average. Greedy-GAN attack was not performed due to imbalance between real and fake nodes affecting discriminator accuracy. Attacking nodes in certain classes, like class 0 from Citeseer dataset, is challenging due to weak linkage and influence from other classes. Nodes with smaller degrees are easier to attack, while higher degree nodes are more resistant to fake node impact. In experiments, modifying features or edges in a Graph Convolutional Network (GCN) was studied. Modifying edges was found to have a more significant impact on classification accuracy compared to modifying features. Different normalization methods for the adjacency matrix in GCNs were discussed between Pytorch and TensorFlow implementations. In experiments, the impact of modifying edges in a Graph Convolutional Network (GCN) was studied. Different normalization methods for the adjacency matrix were compared, showing that row-wise normalized GCN is better than symmetric normalized GCN under small attacks, but the latter is more robust under large attacks. The impact of modifying edges in a Graph Convolutional Network (GCN) was studied, showing that both evasion and poisoning attacks can reduce the accuracy of GCN. Symmetric normalization is more robust under poisoning attacks. Increasing the number of fake nodes results in more effective attacks for both evasion and poisoning. In the study on Graph Convolutional Networks (GCN), it was found that increasing the number of fake nodes can enhance the effectiveness of both evasion and poisoning attacks. However, with 20% fake nodes and all nodes labeled, the attack is less effective compared to the default setting with 20% fake nodes and 5% labeled. The impact of different label rates on fake nodes was also discussed, revealing unexpected results such as the non-targeted attack having the largest effect when the label rate is 0%. In adversarial attacks on Graph Convolutional Networks (GCNs), two algorithms, Greedy and Greedy-GAN, were used to add fake nodes without altering existing edges or features. The success rate of targeted attacks was 0.81 with a normalized adjacency matrix, compared to 0.85 with labels. Adding a discriminator with Greedy-GAN made attacks less noticeable, but data cleaning before training was crucial. There is a trade-off between attack efficiency and the realism of fake nodes. In adversarial attacks on Graph Convolutional Networks (GCNs), the trade-off between attack efficiency and the realism of fake nodes' features is crucial."
}