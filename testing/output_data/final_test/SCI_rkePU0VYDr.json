{
    "title": "rkePU0VYDr",
    "content": "The paper presents an experimental analysis of perturbation defenses against adversarial examples in neural networks. It explores why small perturbations can recover correct predictions and investigates the effectiveness of these defenses in different settings. The attacks on Convolutional Neural Networks generate modifications that can be easily overcome by perturbations, leading to accurate predictions. The standard adversarial examples are easily dominated by perturbations, suggesting they are not robust. Defense techniques leverage this property, but there is a lack of understanding on the types of perturbations, their mechanisms, and counter-measures. Recent defenses involve input perturbations like feature squeezing, compression, randomized smoothing, and perturbation of network structure. Recent defense techniques involve input perturbations to enhance robustness against adversarial attacks. These perturbations induce small changes to the input data without affecting prediction accuracy on clean examples. The relationship between channel distortion and robustness remains consistent across different types of input perturbations. State-of-the-art attacks are sensitive to these perturbation-based defenses due to their optimization procedures. The optimization procedure in attacks aims to find an adversarial image close to the original image in terms of L1, L2, or L\u221e norm, resulting in higher instability. Perturbation-based defenses are vulnerable to similar attack strategies, with attackers aiming to minimize the distance from the original image. An attacker can exploit a strong lossy channel with additive Laplace noise for adaptive attacks. The community's understanding of adversarial sensitivity in neural networks is based on seminal work by Szegedy et al. Multiple works have explored linearity and over-parametrization as possible explanations. The connection between compression and adversarial robustness has been recognized, with defense strategies including defensive network distillation and feature squeezing. In our work, we unify methods based on compression and randomization applied to input images for adversarial robustness. Previous defenses like network distillation, feature squeezing, and JPEG compression have been broken, but the reasons for their initial success remain unclear. Szegedy et al. (2014), Xu et al. (2017), Buckman et al. (2018), and Zhang & Liang (2019) propose various methods to improve adversarial robustness in neural networks. Injecting Gaussian noise, without discretization, proves to be an effective defense mechanism. This defense strategy is related to gradient masking and obfuscation. In this work, the backward pass computation is perturbed to make it difficult for a gradient-based attack to synthesize an adversarial image. The study focuses on white-box attacks and their adaptive variants, emphasizing the power of noise injection as a defense mechanism. Many new defenses propose randomization through noise injection without considering adversarial training. Recent work has focused on combining randomized smoothing with adversarial training to achieve state-of-the-art provable robustness in convolutional neural networks. The approach involves injecting noise into inputs and layers of neural networks to improve defenses with theoretical guarantees. The method provides certified robustness up to a certain threshold of input distortion, utilizing inequalities from the differential privacy literature. Neural networks are parametrized functions between example and label spaces. An adversarial input is a perturbation of a correctly predicted example that is incorrectly predicted. Approximating a neural network with a less precise version can make it more robust against adversarial attacks. Popular defense methodologies can be characterized within this framework. Compression and randomization techniques can introduce a subtle trade-off between distortion and adversarial robustness in neural networks. By applying a noisy channel to input examples, such as deterministic or stochastic noise processes, the neural network's evaluation can be affected. This can be seen as a form of lossy compression, with techniques like color-depth compression for CNNs. These methods aim to balance the trade-off between distortion and adversarial robustness in neural network evaluations. Color-depth compression for CNNs involves converting integer inputs to floating point numbers using a normalization process. This process can be reversible but can also be made lossy by decreasing fidelity. Randomization in the compression function can play a significant role in enhancing defenses against adversarial attacks. Randomization in defending against adversarial attacks involves adding random noise to input pixels, which can partially recover accuracy loss by averaging over multiple perturbations. This approach can help in classification problems by selecting the most frequent label after multiple perturbation trials. The underlying mathematical mechanism for recovery in adversarial examples is not clear, but the hypothesis is that synthesized defenses can strategically counteract perturbations. The hypothesis is that synthesized adversarial examples have unstable predictions, where small perturbations can drastically change confidence values. By quantifying instability using a Taylor expansion of the function f, the change in confidence is bounded by the operator norm of the terms. This mechanism explains how randomization and adding noise to input pixels can help in defending against adversarial attacks and partially recover accuracy loss. The operator norm of terms in the function f is crucial for understanding instability around adversarial examples. Experiments with ResNet models on CIFAR-10 and ImageNet datasets show signs of instability in M1 and M2 values, suggesting a mathematical mechanism for recovery. Various attacks are evaluated using imprecision-based defenses, demonstrating the efficacy of these methods. In experiments with ResNet models on CIFAR-10 and ImageNet datasets, the foolbox library was used to measure test accuracy, prediction confidence, and distances between original and adversarial images. Various gradient-based attacks were explored, such as LBFGS and Carlini-Wagner L2, for non-targeted attacks inducing misclassification. The text discusses different types of adversarial attacks, including BIM L1, FGSM, PGD L\u221e, and Carlini-Wagner L2, used in experiments with ResNet models on CIFAR-10 and ImageNet datasets. The attacks aim to misclassify images by manipulating gradients and distances between original and adversarial images. In the experiment, 1000 random images from ImageNet and CIFAR-10 datasets are subjected to adversarial attacks using various methods. The adversarial images are then passed through different channels like Frequency Compression, Color Depth reduction, Uniform noise, Gaussian Noise, SVD-based compression, and Identity. Results show that while the identity channel always allows successful attacks, the imprecise channels can still recover a significant portion of the original labels. The experiment evaluated adversarial attacks on images from ImageNet and CIFAR-10 datasets using various methods. Different channels were used to process the adversarial images, with imprecise channels able to recover a substantial portion of original labels. Despite variations in attack methods, all channels could achieve similar high accuracy levels, indicating the effectiveness of imprecision in defending against attacks. Iterative attacks caused more accuracy decrease compared to attacks with higher distortions. The experiment analyzed adversarial attacks on CIFAR-10 and ImageNet datasets using different channels. The goal was to balance error from imprecise channels to counter adversarial perturbations while maintaining valid predictions. Various channels like FC, CD, Unif, Gauss, and SVD were studied, showing differences in accuracy for high distortions. Content-preserving FC and SVD compression channels had the lowest accuracy for high distortions, while Gaussian and Uniform channels performed better on ImageNet. The experiment analyzed adversarial attacks on CIFAR-10 and ImageNet datasets using different channels to counter adversarial perturbations while maintaining valid predictions. Results showed variations in accuracy for high distortions, with Gaussian and Uniform channels performing better on ImageNet. The base test accuracy is around 93.5% for CIFAR-10 and 83.5% for ImageNet. The experiment included C&W L2 and PGD attacks, with a table showing the transferability of adversarial images created against noisy channels to defenses with noisy channels. The experiment analyzed adversarial attacks on CIFAR-10 and ImageNet datasets using different channels to counter adversarial perturbations while maintaining valid predictions. Results showed variations in accuracy for high distortions, with Gaussian and Uniform channels performing better on ImageNet. The baseline test accuracy is 93.56%. Input transformation defenses can be broken if the attacker has full knowledge of the defense, leading to impervious attacks in the adaptive setting. In an adaptive setting, attackers do not need to be fully adaptive as adversarial inputs often transfer to other defenses. Even with a single adaptive step, deterministic channels are fully broken while randomized channels maintain high accuracy above 23.8%. Laplace attacked images transfer the best to other defenses, decreasing accuracy by at least 44.3%. FC attacked images do not transfer well, with a maximum drop in accuracy of 12.26% for other defenses. The adversarial images generated against the Uniform defense transfer well to other defenses compared to those generated against the Gaussian defense due to higher noise levels. This trend is observed for the ImageNet dataset using the ResNet-50 model with Gaussian noise. The maximum drop in accuracy for other defenses when attacked by FC is 12.26%. Increasing attack strength in consecutive subplots incurs higher distortion of adversarial image. L2 distance is increased by adding Gaussian noise for a single plot. Frequency count in Figure 9 shows model predictions for original, adversarial, or other class. Adversarial examples close to original image have wider recovery window for correct label. As the strength of the attack increases, the adversarial image experiences higher distortion. Adversarial examples closer to the original image have a wider window for correct label recovery. Different statistics can be used for stronger attacks, such as the RSE network with 0.2 noise. The RSE network with noise levels recommended by Liu et al. (2018) performs better for lower distortion levels (c value below 0.1) but deteriorates faster for higher distortion levels. Different channels like Laplace, CD, FC, SVD, Gauss, and Uniform show similar trends in accuracy. A simple image transformation by reducing brightness also affects accuracy. The comparison between complex and simple approaches reveals similar trends in sensitivity to perturbations in adversarial examples. Our experiments suggest that sensitivity to perturbations in adversarial examples arises from the optimization process that generates adversarial inputs. Adversarial examples are primarily caused by large gradients of the classifier, leading to increased loss in untargeted attacks and decreased loss in targeted attacks. In targeted adversarial attacks, the loss at a specific point x is decreased by the gradient norm. Recovered original images show lower gradient norms for both original and adversarial classes. Gradients change smoothly with added Gaussian noise. Adversarial inputs exhibit higher Hessian spectrum than original inputs, indicating less stable model predictions. The model predictions for adversarial inputs are less stable than for original images, making them vulnerable to small changes that can easily alter the classification outcome. Non-adaptive attacks are not robust, as random perturbations can override strategically placed perturbations. Results are consistent across deterministic and stochastic channels degrading input fidelity. The recovery window for attackers can be closed to minimize perturbation impact. The attacker's perspective on perturbation-based recovery techniques can be rendered ineffective by closing the recovery window. A perturbation analysis involves applying compression in the frequency domain to reduce input image precision using FFT for efficient computation. Band-limited spectrum and filter definitions are utilized, along with SVD transformation for image decomposition and reconstruction with dominant singular values. The basis used in SVD for image compression is adaptive and determined by the image itself, leading to higher quality at the same compression rate compared to FFT. However, SVD is more computationally intensive. Imprecise channel defense introduces errors regardless of adversarial examples, acting as an upper-bound for test accuracy under perturbations. Test accuracy comparisons for different levels of imprecision, noise settings, and noisy channels are presented for CIFAR-10 images on ResNet-18 architecture. The test accuracy of models can be improved by training with compression, such as using FFT based convolutions with 50% compression in the frequency domain increasing accuracy to 92.32%. Results are presented for six different noisy channels, including compression-based and noise-based channels with varying compression rates and noise strengths controlled by epsilon parameters. The test accuracy on clean data for CIFAR-10 is 93.56% and for ImageNet is 76.13%. The C&W attack shows sub-pixel level distortion for ImageNet. Recovery rates are high when rounding to the nearest 8 bit integers in the CD channel. Performance of defenses against different attacks is shown in Table 3. In Table 3, the accuracy of the classifier is evaluated over 1000 images using different perturbation channels and attacks. The channels include compression and noise, with comparisons made between clean and adversarial examples from CIFAR-10 and ImageNet datasets. The attacks used are C&W and PGD, with different channels applied to clean and adversarial images. The study evaluates the accuracy of channels on adversarial images generated by the C&W attack with SVD compression. The channel parameter is chosen based on the highest accuracy on adversarial images. Deltas are computed by subtracting original images from perturbed adversarial images, showing correct label recovery. The CD channel resembles a Uniform distribution, while FFT and SVD compression methods follow double-sided exponential distributions. The FFT and SVD compression methods are more related to Laplace distribution than Gaussian distribution. Using a stochastic channel as a defense can be effective against non-adaptive attacks. Running the defense multiple times improves efficacy, as shown with 1000 images from CIFAR-10 test set. Only 16 trials are needed for a strong defense, making randomized defenses difficult to attack. The use of randomized defenses can greatly reduce noise in predictions with a small number of trials. When the defense method is known, deterministic channels are easier for adversaries to fool but result in higher distortion compared to attacks on unprotected networks. Table 5 shows the results of an adaptive attack against the additive uniform noise channel. The adversary aims to send outputs through the stochastic channel multiple times to optimize the attack and evade detection. The attack is considered successful if the most frequent output label differs from the ground truth. Increasing the number of passes through the noisy channel strengthens the attack and reduces the L2 distortion. The attack against randomized defenses requires optimization of complex loss functions and incurs higher distortion. Running attacks and defenses on CIFAR-10 data with single and multiple iterations show varying levels of accuracy. Combining defenses can potentially enhance their effectiveness, as seen in random discretization. Our experiments contrast with previous work, showing little benefit to hybrid approaches in combining Frequency Compression (FC), Color Depth (CD) compression, and Uniform (Unif) noise. Results suggest that noisy channels do not exploit inherent image features, with noise addition being the mechanism for robustness. Composing different schemes increases noise, as seen in the recovery rate for various attacks on MNIST, CIFAR-10, and ImageNet datasets. The transferability of adversarial examples between different channels is explored, showing how adding Gaussian noise affects the gradient of the loss w.r.t. the input image. By systematically adding noise to an adversarial image, the norm of gradients for the adversarial class increases while decreasing for the original class, leading to correct label recovery. As more noise is added, classifier predictions become random. Adding noise to adversarial images affects the gradient of the loss, leading to random classifier predictions. Gradients for the original and adversarial classes are larger in untargeted attacks. Targeted attacks decrease loss for the target class, causing lower gradients for adversarial classes. The Hessian spectrum shows higher instability for adversarial images. In experiments using the foolbox library, attacks like LBFGS, PGD, Carlini & Wagner were conducted with specific parameters. For example, epsilon was set to 0.3 for PGD, and c was set to 0.01 for Carlini & Wagner. The LBFGS attack used epsilon of 0.00001 and up to 150 iterations. For the BIM L1 attack, epsilon is set to 0.3, step size to 0.05, and iterations to 10. Black box attacks operate without gradient or model knowledge. Band-limited CNNs discard noise by removing high frequency Fourier coefficients. Test accuracy is shown for images perturbed with uniform and Gaussian noise at different compression levels. Our study compared different compression levels and methods for robustness to noise. Models trained with higher compression showed more resilience to noise, with band-limited CNNs outperforming RPA-based methods. Input test images were perturbed with uniform or Gaussian noise, showing that more band-limited models were more robust. A black-box attack gradually distorts all pixels, but can be defended with Color Depth reduction until a certain epsilon value. In defending against the contrast reduction attack, running a high-pass filter in the frequency domain instead of a low-pass filter can be effective. Experiments on different models show that Color Depth (CD) defense helps to some extent, with models having fewer pixels per channel being more robust. Test accuracy for ResNet-18 on CIFAR-10 after 350 epochs is plotted against the contrast reduction attack. Varying the attack strength with epsilon from 0.0 to 1.0 shows the impact on the test set. The foolbox library supports single and multiple pixel attacks on CIFAR-10 test set. Spatial attacks involve adversarial rotations and translations during training. These attacks are not defended by removing high frequency coefficients or quantization. Rotation angles are varied from 0 to 20 degrees, and translations are applied within a certain range. The attack involves rotating the image by up to 20 degrees and translating it within a set pixel limit. It requires access to model predictions and probabilities to estimate gradients. The local search attack perturbs sensitive pixels until the image is misclassified, searching for critical pixels in the neighborhood. Experiments were conducted on 100 CIFAR-10 test images. For the CIFAR-10 dataset, multiple trials of uniform noise channels were run to improve overall accuracy significantly. After 128 trials, the best setting achieved accuracy within 3% of the overall model accuracy. The experiment involved running multiple noise levels and iterations, comparing defender and attacker performance on 100 images. The experiment involves running 100 images through 100 C&W L2 attack iterations, measuring changes in the L2 norm of the gradient of the loss for different classes. This is done by adding Gaussian noise to the adversarial image generated with the attack on 1000 images from the CIFAR-10 dataset."
}