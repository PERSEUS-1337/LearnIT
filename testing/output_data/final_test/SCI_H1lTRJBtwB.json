{
    "title": "H1lTRJBtwB",
    "content": "The study explores hierarchical policies in reinforcement learning to facilitate knowledge transfer across tasks, improving data efficiency in real-world robotics applications. The research demonstrates the benefits of using incentives to decompose task solutions, showing positive transfer and reduced interference. An RL algorithm is designed for stable and rapid learning of structured policies, enabling effective reuse of behavior components. The study focuses on hierarchical policies in reinforcement learning for knowledge transfer across tasks, enhancing data efficiency in real-world robotics. An RL algorithm is developed for rapid learning of structured policies and effective reuse of behavior components. The algorithm is evaluated in simulated and physical robot experiments, showing significant improvements in data efficiency compared to competitive baselines. In robotics, data efficiency is crucial, and human prior knowledge is often used. To reduce reliance on human accuracy, utilizing an agent's permanent embodiment and shared environment across tasks can be more scalable. Focusing on inductive biases that facilitate sharing and reusing experience and knowledge across tasks can lead to better results in transfer learning. In the realm of transfer learning, various methods have been explored to optimize initial parameters, share models and data across tasks, and utilize task-related auxiliary objectives. Transfer between tasks can result in either constructive or destructive outcomes for both humans and machines. The challenge lies in finding a mechanism that enables transfer without interference. This paper delves into the benefits and limitations of hierarchical policies in addressing this challenge. In this paper, the benefits and limitations of hierarchical policies in single and multitask reinforcement learning are explored. Models represent policies as state-conditional Gaussian mixture distributions, with low-level policies selected by the high-level controller. In the multitask setting, mixture components are shielded from task information to achieve more robust behaviors. Transfer is implemented through targeted exploration. In this study, a novel multitask actor-critic algorithm, Regularized Hierarchical Policy Optimization (RHPO), is developed to improve data-efficiency and robust policy optimization properties. The algorithm utilizes the multitask learning aspects of SAC to optimize hierarchical policies by sharing transition data across tasks and reusing low-level components. The generality of hierarchical policies for multitask learning is demonstrated by replacing MPO as the policy optimizer with another gradient-based, entropy-regularized policy optimizer. Regularized Hierarchical Policy Optimization (RHPO) is a novel multitask actor-critic algorithm that improves data-efficiency and robust policy optimization. It demonstrates improved performance, robustness, and learning speed in multitask domains compared to competitive continuous control baselines. Additional incentives are needed to encourage similar developments for single task domains. RHPO leads to a significant speed up in training on a physical robot for robotic manipulation tasks, enabling it to solve challenging stacking tasks. In a multitask reinforcement learning setting, an agent operates in a Markov Decision Process with state space S, action space A, transition probability p(s t+1 |s t , a t ), and a policy \u03c0(a|s). The joint transition dynamics and policy induce the state visitation distribution. Multitask learning involves a set of tasks with a common agent embodiment and shared state and action spaces. The discount factor and reward contribute to the expected reward value. Regularized Hierarchical Policy Optimization (RHPO) focuses on efficient training of modular policies by sharing data across tasks. It extends data-sharing and scheduling mechanisms from Scheduled Auxiliary Control with randomized scheduling (SAC-U). The objective is defined as maximizing the state-action value function Q \u03c0 (s, a, i) across tasks with shared state, action spaces, and transition dynamics. The text discusses combining MPO and SAC-U for training hierarchical policies in a multitask setting. It introduces a hierarchical policy class with high-level and low-level sub-policies. The method is robust with respect to the number of components in the policy. The method introduces a hierarchical policy class with high-level and low-level sub-policies, employing information asymmetry to enable task-independent behaviors. Sub-policies act as reflex-like control loops, modulated by higher cognitive functions. The equations underlying RHPO are presented, with optimization based on the MPO algorithm. Refer to the complete pseudocode algorithm in Appendix A.2.1. The method introduces a hierarchical policy class with high-level and low-level sub-policies, utilizing information asymmetry for task-independent behaviors. It builds on the MPO algorithm to optimize the policy class by decoupling policy improvement from hierarchical policy fitting. An intermediate non-parametric policy q(a|s, i) is introduced to optimize J while staying close to a reference policy \u03c0 ref (a|s, i). The policy is iteratively improved through policy evaluation and policy improvement steps. The method introduces a hierarchical policy class with high-level and low-level sub-policies, utilizing information asymmetry for task-independent behaviors. UpdateQ such thatQ(s, a, i) \u2248Q \u03c0 \u03b8 k (s, a, i). Policy Improvement involves obtaining q k = arg max q J(q) under KL constraints with \u03c0 ref = \u03c0 \u03b8 k and obtaining \u03b8 k+1 = arg min \u03b8 E s\u223cD,i\u223cI KL q k (\u00b7|s, i) \u03c0 \u03b8 (\u00b7|s, i). Multitask Policy Evaluation for data-efficient off-policy learning ofQ is based on scheduled auxiliary control with SAC-U, leveraging experience sharing and task switching within episodes. The method introduces a hierarchical policy class with high-level and low-level sub-policies, utilizing information asymmetry for task-independent behaviors. The retrace objective for learning Q is defined using rewards for all tasks as a vector in the buffer. Multitask Policy Improvement involves obtaining non-parametric policies by maximizing a specific equation and optimizing a temperature parameter alongside policy optimization. This policy representation is independent of the parametric policy form, making it easy to employ structured policies. The method introduces a hierarchical policy class with high-level and low-level sub-policies, utilizing information asymmetry for task-independent behaviors. It makes it easy to employ structured policies by fitting a policy to a non-parametric distribution obtained from sampling \u03c0 \u03b8 k and calculating the gradient of its log density. This step corresponds to maximum likelihood estimation (MLE) and introduces a trust-region constraint for regularization during training. The method introduces a hierarchical policy class with high-level and low-level sub-policies, utilizing information asymmetry for task-independent behaviors. It employs Lagrangian relaxation and gradient descent steps for optimization, demonstrating benefits for multitask learning in robotic manipulation tasks. RHPO significantly reduces platform interaction time in single and multitask domains. In the DeepMind Control Suite, a hierarchical policy class is introduced for robotic manipulation tasks, showing the need for additional incentives for component specialization to improve performance. Experiments on physical hardware with the Sawyer arm emphasize data-efficiency. The evaluation includes multitask domains with a distributed actor-critic framework for training agents. The study introduces a hierarchical policy class for robotic manipulation tasks in the DeepMind Control Suite, emphasizing the need for component specialization to enhance performance. Experiments with the Sawyer arm on physical hardware highlight the importance of data-efficiency and include multitask domains within a distributed actor-critic framework for agent training. The comparison between flat Gaussian and hierarchical policies for continuous control tasks shows that the hierarchical policy performs similarly to the flat policy, indicating a failure to decompose the problem effectively. The study introduces a hierarchical policy with distributed initial means for component specialization in robotic manipulation tasks. Experiments with the Kinova Jaco and Rethink Robotics Sawyer arms show improved performance in multitask scenarios. Physical robot experiments validate the findings, and further evaluation is done in two more complex multitask domains in simulation. In more complex multitask domains in simulation, the evaluation includes stacking blocks and opening a box with a total of 13 tasks. RHPO is compared against a flat policy shared across all tasks and policies with task-dependent heads. The baselines provide contrasting perspectives on transfer learning. The study compares RHPO with monolithic and independent policies in multitask domains, showing that hierarchical policies outperform baselines across tasks. Performance gains are observed with increasing task complexity, highlighting the advantage of composing learned behaviors. The study demonstrates the effectiveness of hierarchical policies in multitask domains, showing performance gains without additional incentives. Real-world experiments with a single robot show the benefits of RHPO in low data scenarios, focusing on stacking cubes with a distractor. The setup includes a Sawyer robot arm with a Robotiq 2F-85 gripper and three cameras for tracking. The setup for the real-world experiments includes a Sawyer robot arm with a Robotiq 2F-85 gripper and three cameras for tracking. The agent's action is five-dimensional, involving Cartesian translational velocities, wrist angular velocity, and gripper finger speed. Learning progress on the real robot shows RHPO with a hierarchical policy successfully learns the stacking task. RHPO with a hierarchical policy successfully learns the stacking task on a real robot after 15 thousand episodes of training. The components specialize, and tasks show varying levels of similarity in component usage. The algorithm RHPO demonstrates robustness in sequential transfer learning with pre-trained low-level components. Ablations show the importance of regularization and number of sub-policies, highlighting the relevance of hierarchical policies in data-limited domains like real-world robotics applications. In the context of machine learning, training a single model across different tasks can lead to interference, both positive and negative. Strategies to prevent negative transfer include reducing representational overlap and utilizing hierarchical classification models. Hierarchical approaches have been beneficial in reinforcement learning literature. Hierarchical approaches in reinforcement learning literature have a history of combining hierarchy with inductive biases for improved stability and data efficiency. Instead of introducing additional training signals, the benefits of compositional hierarchy for transfer between tasks are directly investigated. Trajectory modelling is used for behavior abstractions in reinforcement learning. Models act as inductive biases sharing behavior across tasks. Similar algorithms share a low-level controller but modulate behavior via continuous embedding. The options framework supports behavior hierarchies with a discrete set of sub-policies. Temporal abstraction in reinforcement learning has been explored through practical algorithms for learning option policies and criteria for option induction. This investigation focuses on hierarchical composition in single and multitask learning, highlighting the strength of hierarchical composition in domains with varying objectives. A hierarchical extension of SVG is introduced to explore similarities with the option critic, while KL regularization is used in RHPO for contextual bandits. Our approach in reinforcement learning (RL) focuses on contextual bandits and optimizing linearly parametrized mixture policies using a novel framework. We aim to tackle full RL problems in complex domains with long horizons and high-capacity function approximators like neural networks, requiring robust estimation of value function approximations and off-policy correction. Our method, RHPO, outperforms baseline methods in both simulation and real-world tasks. RHPO outperforms baseline methods in complex tasks with limited data, demonstrating hierarchical inductive biases for transfer learning. Performance improvements are seen when components specialize, and pre-trained specialized components can enhance learning new tasks. Identifying optimization methods is a key next step. Optimizing a basis set of components for transfer learning is crucial for improving performance when learning new tasks. Mixture distributions allow for marginalizing over components to optimize weighted likelihood, enabling multiple levels of hierarchy. While this approach helps mitigate negative interference between tasks in multitask learning, addressing catastrophic inference in sequential settings remains a challenge. RHPO combines multitask learning with hierarchical policy representations, robust optimization, and efficient off-policy methods for future practical work. In this section, detailed derivations for training hierarchical policies parameterized as a mixture of Gaussians are explained. The joint distribution over states and randomly sampled tasks is defined to obtain non-parametric policies. This approach aims to unlock further applications of reinforcement learning in both simulation and physical hardware. The text discusses the derivation of a dual function for training hierarchical policies parameterized as a mixture of Gaussians. The approach aims to expand the applications of reinforcement learning in simulation and physical hardware. The text discusses minimizing the dual function with respect to \u03b7 using samples from the replay buffer. Nonparametric policies are obtained and then a parametric policy is fitted to samples from these nonparametric policies using maximum likelihood estimation with regularization based on a distance function. Lagrangian Relaxation is employed to make the objective amenable to gradient-based optimization. The primal is solved for \u03b8 by iterating inner and outer optimization programs independently. The optimization process involves fixing Lagrangian multipliers and optimizing for \u03b8. One gradient step is performed in inner and outer optimization for each batch of data. Hierarchical policies are considered, with a mixture of Gaussians parametrization. The high level policy is a categorical distribution over low level Gaussian policies. A distance function is defined between old and new mixture of Gaussian policies. The distance function is defined between old and new mixture of Gaussian policies to control convergence and prevent premature convergence. The setting for policy improvement involves scheduled auxiliary control with uniform random switches between tasks. The retrace objective for learning Q is defined using data from all tasks, with importance weights calculated based on behavior policies. Parameters are fetched, new trajectories are collected from the environment, and active tasks are sampled from a uniform distribution. Trajectories are then sent in batches for processing. In practice, trajectories are collected and sent in batches for processing. The policy evaluation mechanism is similar to SAC-U, but the main difference lies in the policy parameterization. In practice, trajectories are collected and sent in batches for processing. The policy evaluation mechanism is similar to SAC-U, but the main difference lies in the policy parameterization. The Q-function in the multitask case is represented using the network architecture from SAC-X. The robot's proprioception, object features, and actions are fed into a torso network with specific layers and activation functions. The torso network in the multitask setting is used to share outputs with independent head networks for each task. Each head consists of two fully connected layers and outputs a task-specific Q-value. The policy architecture, different from the Q function, involves computing parameters for the desired policy distribution. This architecture, known as independent heads, is explored in the paper. The paper explores two policy architectures: monolithic and hierarchical. The monolithic architecture reduces the original policy to one head for all multitask experiments. The hierarchical architecture uses a shared torso and task-specific networks for parameterizing distributions. The final mixture distribution is task-dependent for the high-level controller and task-independent for the low-level policies. The hyperparameters for RHPO and baselines in single task and multitask experiments use feed-forward neural networks. A flat policy is represented by a Gaussian distribution with a diagonal covariance matrix, while a hierarchical policy is represented by a mixture of Gaussians distribution. The neural network outputs the mean and diagonal Cholesky factors for the flat policy, ensuring positive definiteness of the diagonal covariance matrix. The mixture of Gaussian policy includes multiple Gaussian components and a categorical distribution for component selection. The neural network outputs Gaussian components and logits for categorical distribution. Layer normalization and tanh are important for stability. RHPO's key hyperparameters are constraints in Step 1 and Step 2. SAC-U baseline uses SVG for policy optimization with a specific network structure and policy parameterization. The policy optimization function uses entropy regularization and gradient ascent. Experiments are conducted using a numerical simulator with access to CPUs and GPUs. In the real robot setting, cameras track the cube using fiducials for safety reasons. The real robot setup for the task involves measuring external forces at the wrist and terminating the episode if a threshold of 20N is exceeded on any axis. Object positions are randomized every 25 episodes using a hand-coded controller, with objects placed back in the basket if thrown out. The robot's starting pose is randomized each episode, similar to the simulation setup. The real robot setup includes a Sawyer robot arm with a Robotiq gripper, a basket with three cubes, proprioception information for the arm and gripper, wrist sensor readings, and cube poses provided to the agent. Historical observations and joint control commands are also given to account for communication delays. The real robot setup includes a Sawyer robot arm with a Robotiq gripper, a basket with three cubes, proprioception information for the arm and gripper, wrist sensor readings, and cube poses provided to the agent. Historical observations and joint control commands are also given to account for potential communication delays. The robot arm is controlled in Cartesian mode at 20Hz, with a 5-dimensional action space for the agent. Gripper movement is restricted to a cubic volume above the basket using virtual walls. In the Pile1 experiment, 7 different tasks are used to learn the final task of stacking the green cube on top of the yellow cube, following SAC-X principles. Tasks include minimizing the distance to the green cube, activating the grasp sensor, lifting the green cube, and achieving a sparse binary reward for stacking the cubes with specific tolerances. The Pile2 task involves stacking blue on red and red on blue cubes using a different robot arm setup to show RHPO's versatility across tasks and control modes. The agent controls a simulated Kinova Jaco robot arm with a Kinova KG-3 gripper to stack red and blue cubes in a basket. It receives proprioceptive information for the arm and fingers, touch sensor data for the gripper, and cube poses and velocities. No observation or action history is used in the experiments. The robot arm stacks red and blue cubes in a basket using proprioceptive information and touch sensor data. Cubes are randomly spawned on the table, and the robot hand is initialized above the table with a height offset. The robot arm is controlled in raw joint velocity mode, and the action space is 9-dimensional. Tasks include minimizing distance to cubes, moving cubes, and stacking them in specific orders. The robot arm stacks red and blue cubes in a basket using proprioceptive information and touch sensor data. Tasks include moving the red and blue cubes, adjusting their z-coordinate, bringing them close to each other, and placing them on another object. Sparse rewards are given based on the position of the cubes relative to each other. The Clean-Up task involves placing cubes inside a box in the workspace. The agent observes the lid's angle and velocity in addition to other observations. There are 13 different tasks for Clean-Up, with the main task being ALL_INSIDE_BOX. The tasks serve as auxiliary tasks for learning the main task. The Clean-Up task involves various actions such as moving cubes, lifting them, opening the lid, and placing them inside a box. Tasks include minimizing distances, lifting cubes, opening the lid, and positioning cubes above and close to the box. These tasks serve as auxiliary tasks for learning the main task of placing cubes inside the box. Coordinating convergence progress in hierarchical models can be challenging but can be effectively moderated by the KL constraints. Varying the strength of KL constraints on the high-level controller during training can lead to different behaviors. With a weak KL constraint, the high-level controller can converge too quickly, affecting the sub-policies and preventing successful convergence for the low-level policies. Strong KL constraints can prevent specialization of the low-level policies. In a distributed off-policy setting, evaluating the impact of different data generation rates on convergence speed is crucial. Lower data rates result in slower convergence, which is significant for real-world experiments with limited physical robots. To minimize computational costs, the comparison focuses on the simplest domain, Pile1. In the comparison of data generation rates for convergence speed in a distributed off-policy setting, the focus is on the simplest domain, Pile1. Agents are trained on related tasks in Pile1 and Cleanup2, with experiments showing up to 5 times more data-efficiency when reusing a policy trained on related tasks for the final task. Training a hierarchical policy structure for transfer learning in different domains, such as Pile1 and Cleanup2, shows that reusing low-level components from previous tasks can solve the final task if it is a composition of those tasks. The final task in Cleanup2 can be completed by sequencing previously learned components, unlike in Pile1 where certain actions are not required for earlier tasks. Models are trained to adapt to the final task by either training a high-level controller only or a high-level controller along with an additional component. Additional experiments using SVG instead of MPO show that the benefits of a hierarchical policy transfer can still be observed. The curr_chunk discusses modifying the structure of the MPO experiments to enable reparameterization with the Gumbel-Softmax trick, changing entropy regularization, and using a regularizer equivalent to the distance function. This extension of SVG is similar to a single-step-option version of the option-critic. The curr_chunk discusses the use of a single critic in the algorithm, showing mild improvements over standard SAC-U in a simple domain. The hierarchical policy leads to better final performance in a gradient-based approach, with all plots generated by running 5 actors in parallel."
}