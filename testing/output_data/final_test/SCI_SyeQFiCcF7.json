{
    "title": "SyeQFiCcF7",
    "content": "Capsule Networks have shown promising results on benchmark computer vision datasets like MNIST, CIFAR, and smallNORB. This paper introduces Siamese Capsule Networks for face verification tasks, using contrastive loss with l2-normalized capsule encoded pose features. The model performs well on pairwise learning datasets, especially in few-shot learning scenarios with unseen subjects. Convolutional Neural networks (CNNs) are effective at detecting local features but lose spatial relationships with pooling operations, leading to viewpoint invariance and loss of information about internal properties of entities in images. This issue can be addressed with large amounts of annotated data, but it is less efficient. From a psychology perspective of human shape perception, pooling does not account for coordinate frames in mental rotation to identify handedness. Spatial Transformer Networks address this issue by using dynamic spatial transformations to enhance geometric invariance. Capsule Networks have shown promising results in achieving translation equivariance. This paper extends Capsule Networks to the pairwise learning setting for few-shot learning and zero-shot prediction. The Siamese Capsule Network is trained using a contrastive loss with 2-normalized encoded features and demonstrated on face verification tasks. The Capsule Networks utilize a dynamic routing scheme to achieve translation equivariance and untangle the coordinate frame of entities through linear transformations, inspired by computer graphics. This approach aims to preserve information and achieve equivariance, unlike traditional neural networks that use pooling for viewpoint invariance. The brain is hypothesized to solve an inverse graphics problem by deconstructing images into hierarchical properties. Capsule Networks use a dynamic routing scheme with designated neurons called capsules, consisting of pose vector u and activation \u03b1. Two convolutional layers provide initial input representations for capsules, allowing reuse of learned knowledge. Capsule inputs are determined using an Iterative Dynamic Routing scheme, with transformations made to output vectors. The output vector u i of capsule C L i represents the probability of object detection and its state. It is transformed into a prediction vector \u00fb j|i = W ij u i, weighted by coupling coefficient c ij to obtain s j = i c ij \u00fbj|i. Routing By Agreement is performed using coincidence filtering to find tight clusters of nearby predictions. The entities output vector length is represented as the probability of an entity being present by using nonlinear normalization. Capsules are assigned high log-likelihood if densely connected clusters of predictions are found. Coincidence filtering can be achieved by traditional outlier detection methods. Vector normalization of instantiation parameters is used to preserve orientation. Reconstruction loss on images is used for regularization. In this paper, a variant of dropout is used instead of autoencoding for regularization. BID5 introduced matrix capsules that perform routing using the EM algorithm. Each parent capsule is considered a Gaussian, with pose matrices of child capsules as data samples. Capsule layers contain capsules with pose matrices and activations as outputs. A learned viewpoint invariant transformation is applied for pose matrix calculations. The matrix capsule network uses learned viewpoint invariant transformations for pose matrix calculations. It outperformed CNNs on the SmallNORB dataset. LaLonde & Bagci introduced SegCaps, which utilizes locally connected dynamic routing. SegCaps by LaLonde & Bagci uses dynamic routing to reduce parameters and deconvolutional capsules for segmenting pathological lungs from low dose CT scans. It outperformed baseline architectures with a significant reduction in parameters. Bahadori introduced Spectral Capsule Networks for medical diagnosis, showing faster convergence and spatial alignment of features using 1d convolution and residual layers. The model utilizes weighted votes obtained through a transformation that accounts for rotations in healthcare imaging. It decomposes the concatenated votes using SVD to capture variance, with the activation computed based on the ratio of variance explained. The training process maximizes log-likelihood, outperforming spread loss and preventing capsules from becoming dormant. The capsule routing strategy is formalized as an optimization of clustering loss and KL regularization term. The proposed objective function aims to minimize a specific loss function in a routing scheme that outperforms previous methods. The novelty lies in a pairwise learning capsule network scheme that focuses on comparing and aligning images while maximizing inter-class variations and minimizing intra-class variations. This approach differs from the current state-of-the-art work in face verification using Siamese Networks. State-of-the-art convolutional Siamese Networks have been used for face verification and recognition. BID24 introduced a joint identification-verification approach with contrastive and cross-entropy losses, balancing intra-personal and inter-personal variations with a weight parameter \u03bb. Optimal results were achieved with \u03bb = 0.05, maximizing intra personal variation while distinguishing between classes. The center loss function proposed by BID28 aims to improve discriminative feature learning in face recognition by minimizing intra-class variation and keeping features separable between classes. It penalizes the distance between class centers and computes centroids during stochastic gradient descent. BID15 introduced Sphereface, a hypersphere embedding using an angular softmax loss on a hypersphere manifold for face discrimination. The model achieves high accuracy on face datasets like LFW, YTF, and MegaFace using triplet similarity embedding and deep metric learning for face verification. The loss function combines logistic loss and regularization for optimal results. The model achieves high accuracy on face datasets like LFW, YTF, and MegaFace using triplet similarity embedding and deep metric learning for face verification. It combines SIFT descriptors, dense SIFT, and local binary patterns (LBP) for 90.68% accuracy on LFW dataset. BID18 and FaceNet BID23 use constraints on face features for improved performance. Siamese Inception Network and DeepFace network are also used for face verification. The DeepFace network, introduced by BID26, achieves human-level performance on the Faces in the Wild (LFW) dataset and outperforms previous methods. It is trained on a large dataset from Facebook (SFC) and utilizes manual steps for detecting, aligning, and cropping faces. The model normalizes images to avoid illumination differences and uses a 3D model for face detection and alignment. The proposed SCNs use a Capsule Network for face verification, which is more efficient and requires minimal preprocessing steps compared to previous methods. The network achieves state-of-the-art results with less data and no aligning or cropping of images. The Capsule Network for face verification aims to identify part-whole relationships of facial features and pose, improving similarity measures by aligning capsule features across paired images. The network consists of a 5-hidden layer architecture with tied weights, including convolutional and capsule layers. The Capsule Network for face verification consists of a 5-hidden layer architecture with tied weights, including convolutional and capsule layers. The network encodes paired images into vector pairs using pose vectors of capsules passed through a fully connected layer. Different distance measures are considered for capsule image encodings, with variations in routing iterations for different datasets. The Capsule Network for face verification uses a 5-hidden layer architecture with tied weights, including convolutional and capsule layers. Each input has a 20-dimensional representation of 32 capsule pose vectors, resulting in 512 features. The dropout probability rate is learned for each capsule to keep them active. The sigmoid function learns the dropout rate of the final capsule layer using Concrete Dropout BID4. Equation 2 shows the objective function for updating the concrete distribution, with a continuous relaxation approximation for dropout. The pathwise derivative estimator is used to continuously estimate the dropout mask with a weight \u03bb to prevent early deterioration of activity vector lengths. The overall loss is the sum of capsule losses. A spread loss BID5 maximizes inter-class distance on the smallNORB dataset. A contrastive margin loss BID2 ensures similar pose encodings are drawn together and dissimilar poses repulse. Equation 3 shows a pair of images passed to the SCN model. Equation 4 introduces a double-margin contrastive loss for better separation between matching and non-matching pairs in the SCN model. This loss is specifically used for LFW due to limited instances in AT&T dataset. The original reconstruction loss is not utilized in pairwise learning. The reconstruction loss is not used in pairwise learning, instead relying on dropout for regularization. Optimization convergence can be slow for face verification tasks, but AMSGrad improves adaptive learning rates by keeping long-term memory of past gradients. The AT&T dataset consists of 40 subjects with 10 gray-pixel images each, allowing testing of SCNs with little data. LFW dataset has 13,000 colored faces with 1680 subjects and varied characteristics like aging, pose, and lighting. Zero-shot pairwise prediction is performed during testing. In this work, images from the LFW dataset are resized to 100\u00d7100 and normalized. SCNs are compared against AlexNet, ResNet-34, and InceptionV3 for image recognition tasks. Best test results are obtained using contrastive loss with Euclidean distance for both AT&T and LFW datasets over 100 epochs. Different margin values are used for the contrastive loss. During 5-fold cross validation, grid searching was used to select a negative matching margin of 0.5 for SCN, which outperformed baselines on the AT&T dataset after 100 epochs of training. An adapted dropout rate was found to slightly increase contrastive loss due to the dataset's smaller size. Adding a reconstruction loss with \u03bb r = 1e \u22124 for paired images decreased performance compared to using dropout with a rate of 0.2 on all layers except the final encoding layer. For the LFW dataset, SCN and AlexNet achieved the best results, with SCN having 25% fewer parameters. Using a double margin improved standard SCN results but slightly decreased performance when combined with concrete dropout on the final layer. Figure 2 shows contrastive loss during training of 2-normalized features. Figure 2 illustrates the contrastive loss during training of 2-normalized features for different models on AT&T and LFW datasets. SCN shows faster convergence with Manhattan distance on AT&T, while Euclidean distance results in reduced loss variance and overall better performance. Batch normalization in convolutional layers improves SCN performance, reducing loss variance during training on both datasets. LFW test results show SCN takes longer to converge compared to AlexNet. The Siamese Capsule Network (SCN) has lower variance in predictions compared to other models, especially for Manhattan distance. It also has significantly fewer parameters than AlexNet and other baseline models. However, the routing iterations in Capsule Networks limit its speed, even with reduced parameters. The SCN introduces a novel architecture for pairwise learning. The Siamese Capsule Network introduces a novel architecture for pairwise learning, extending Capsule Networks with a feature 2-normalized contrastive loss. Results show improved performance in few-shot learning settings, particularly on the AT&T dataset, while remaining competitive on the Labeled Faces In The Wild dataset."
}