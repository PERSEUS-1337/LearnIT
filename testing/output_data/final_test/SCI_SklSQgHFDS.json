{
    "title": "SklSQgHFDS",
    "content": "Sparse reward reinforcement learning is a challenge, with state-of-the-art methods using intrinsic motivation to help the agent explore more effectively. Bonus rewards are commonly added to encourage exploration, but this can lead to a policy that struggles to balance exploration and task completion. In this paper, the authors propose a new approach called scheduled intrinsic drive (SID) agent, which learns separate intrinsic and extrinsic task policies to improve exploration efficiency. They introduce a new type of intrinsic reward called successor feature control (SFC) that considers complete trajectory statistics. The results show improved exploration efficiency in three different environments with pure visual inputs. The agent's performance can be viewed in a video available at https://gofile.io/?c=HpEwTd. The success of deep reinforcement learning (DRL) relies on carefully shaped dense extrinsic reward signals, which can support the agent in finding solutions efficiently. However, designing such rewards often requires substantial domain knowledge and ground truth state information, which is challenging to obtain in real-world robot scenarios. Carefully shaping rewards is crucial as poorly designed rewards can introduce bias in the learning process. In deep reinforcement learning, poorly designed reward shapes can introduce bias or distractions, hindering optimal solutions. Terminal reward RL settings offer the opportunity for agents to discover solutions beyond established domain knowledge, but sparse learning signals can be a challenge in real-world tasks. In intrinsically-motivated exploration, an intrinsic reward is added to the extrinsic reward to guide exploration. However, maximizing this combined reward signal can lead to a policy that does not prioritize either reward. Current methods focus on local information for estimating intrinsic rewards, which can lead to unstable learning. The proposed hierarchical agent scheduled intrinsic drive (SID) focuses on maximizing extrinsic and intrinsic rewards separately, allowing for pure exploration or task fulfillment. The high-level scheduler selects which policy to follow, increasing behavior policy space exponentially. The proposed method introduces successor feature control (SFC), an intrinsic reward based on successor features, enabling structured exploration in challenging environments. This approach is task-agnostic and can be easily integrated into existing DRL methods with minimal computational overhead. Experimental results in various environments validate the effectiveness of the proposed agent in visual navigation tasks. Intrinsic motivation, such as curiosity, guides exploration in challenging environments for DRL agents. The proposed agent utilizes an Intrinsic Curiosity Module (ICM) to predict future states and has shown success in visual navigation tasks. In visual domains, intrinsic motivation for exploration in DRL agents involves predicting features of the current state as a reward bonus. Different forms of curiosity, such as learning progress, have been connected to count-based exploration. Savinov et al. (2018) propose training a reachability network to give rewards based on state reachability within a certain number of steps. Our proposed Successor Feature Control (SFC) is motivated by choosing states for novel trajectories. We use a distance metric based on successor features, measuring states by the difference in average feature activations of future trajectories. Unlike other methods, our approach rewards states with high distance not only in the current episode but also from past trajectories. Auxiliary tasks like depth prediction and loop closure prediction have been suggested for learning more representative features. The UNREAL agent utilizes policies for pixel and feature control, but only to learn suitable features for the main task. Various hierarchical RL approaches have been proposed, with some incorporating feature control in a hierarchical setting. However, a flat policy trained on intrinsic bonus can achieve similar performance to hierarchical agents. Our hierarchical design is inspired by previous work and focuses on learning optimal policies rather than a set of options. The SAC agent aims to maximize extrinsic reward by scheduling between tasks and auxiliary tasks. This paper explores scheduling between extrinsic task and general intrinsic motivation. Successor Representation (SR) is used to accelerate exploration by extending its properties temporarily. In this study, the focus is on the temporarily extended property of Successor Representation (SR) to accelerate exploration. The proposed method utilizes the RL framework for learning and decision-making under uncertainty, formalized by Markov decision processes (MDPs). The agent samples actions according to a policy \u03c0 and receives rewards while transitioning between states. The agent's goal is to maximize the sum of discounted rewards in a Markov decision process. The study focuses on accelerating exploration in sparse reward scenarios, introducing intrinsic reward successor feature control (SFC) and a hierarchical framework called scheduled intrinsic drive (SID). The text discusses the use of scheduled intrinsic drive (SID) and successor representation (SR) to design intrinsic rewards for exploration in a Markov decision process. It introduces the concept of successor features and explores exploration strategies in the absence of extrinsic rewards. The text introduces the concept of successor features and compares exploration strategies using intrinsic rewards in a Markov decision process. The successor features are learned via random walk and Q-learning, showing improved exploration compared to a random agent. Successor features (SF) represent the average discounted feature activations when following a policy in a state. They can be learned without extrinsic rewards or a transition model, combining model-based and model-free RL advantages. SF can be learned efficiently via temporal difference updates and capture information from distant states for effective exploration. The successor distance (SD) metric measures state similarity based on SF, aiding in far-sighted exploration. The successor distance metric (SD) in the tabular case correlates to the shortest path between states. Using SD for intrinsic motivation involves choosing an anchor state, where distant states assimilate in SD. SD is higher for states in different rooms and increases when crossing rooms, capturing state space connectivity. Intrinsic reward successor feature control (SFC) is defined as the squared SD of consecutive states, indicating future feature activation changes. The text discusses the use of intrinsic rewards in reinforcement learning, highlighting the challenges of incorporating them into the overall reward signal. It mentions that intrinsic rewards can lead to instability in learning and sensitivity to reward scaling, requiring careful tuning for each environment. Scheduled Intrinsic Drive (SID) is proposed to address the challenges of incorporating intrinsic rewards in reinforcement learning. SID learns two separate policies for each reward signal and switches between them during episodes. This approach ensures exploration while maximizing extrinsic rewards without the need for scaling parameters. Additionally, scheduling allows for long-term exploration and minimizes disruption to the learning process. The Scheduled Intrinsic Drive (SID) framework allows for long-term exploration by switching between exploration policies during episodes. It helps the agent to avoid always choosing nearby small rewards and encourages it to seek out larger rewards further away. Various high-level schedulers were tested, but none consistently outperformed a random scheduler. The proposed method can be combined with any off-policy learning approach, such as Ape-X DQN with SFC as the intrinsic reward. The DRL algorithm Horgan et al. (2018) with SFC as the intrinsic reward consists of a Q-Net, SF-Net, a high-level scheduler, and N parallel actors. The SF-Net is trained with samples from the replay buffer, and the scheduler selects which policy to follow after a fixed number of environment steps. The DRL algorithm with SFC as the intrinsic reward includes a Q-Net, SF-Net, a high-level scheduler, and N parallel actors. Each actor has its own environment copy and learns from K-step targets stored in a shared replay buffer. The learner trains the Q-Net and SF-Net from the replay buffer, evaluating the proposed intrinsic reward SFC and hierarchical framework of intrinsic motivation SID in various simulated environments. The experiments compare different agent configurations using only raw pixels as input. These configurations include Ape-X DQN, ICM, RND, and the proposed SID framework with SFC reward. An ablation study is conducted to compare the performance of agents with intrinsic and extrinsic rewards. Plots and discussions are presented in Section 4.4 Appendix A. After parameter sweep, the scheduling agent is less sensitive to scalings compared to agents with added reward bonus. The SID setup requires an off-policy algorithm implemented under the Ape-X DQN framework. The number of scheduled tasks per episode is set to M = 8, with each episode divided into up to 8 sub-episodes. Additional information on experimental setups and model training details can be found in Appendices C and D. The experimental results of the baseline algorithms in \"DoomMyWayHome\" show that the M agent can solve the task without intrinsic motivations, but all types of intrinsic motivation help to solve the task more reliably and speed up learning. The method proposed in the study solves the task the fastest, with ICM and RND also learning to reach the goal reliably and efficiently. Testing on a more difficult VizDoom map was considered. The new map, FlytrapEscape, was designed to be more challenging for agents, with rooms laid out in a geometrically complex way to prevent easy navigation. The maze consists of 4 rooms separated by V-shaped walls, making it difficult to move between rooms without precise movements. Each episode starts with the agent at a red dot, and ends when the goal is reached, earning a reward of +1. The task in FlytrapEscape is to escape the fourth room. Experimental results show that ICM solves the task efficiently, while M and RND do not. The agents were tested in a 3D visual navigation simulator called DeepMind Lab, where they had to collect a faraway big reward amidst small distractive rewards in the level \"AppleDistractions\". In the challenging level \"AppleDistractions\" with a maximum episode length of 1350, the agent must navigate through corridors with different rewards. One corridor offers small rewards for each apple collected, while the other has a single big reward at the end. The optimal policy is to go for the big reward, but the challenge is to explore other areas enough to recover the optimal solution. Our method outperformed others in navigating to the large reward. The agent's ability to navigate to the large reward in the \"AppleDistractions\" level outperformed baseline methods by avoiding distractions and efficiently exploring the map. Scheduling intrinsic motivation allowed the agent to focus on exploration without being distracted by small rewards, reducing the risk of converging to bad local optima. This approach was shown to be generally applicable and beneficial for tasks involving ICM and RND. In a different domain, our methods were evaluated on the \"cartpole: swingup sparse\" task using third-person view images as inputs. The task is relatively easy and does not require intrinsic motivation, but our method still outperforms others by reducing interaction time. This demonstrates the general applicability of our approach, even in less challenging exploration tasks. The study conducted an ablation study on AppleDistractions, comparing agents with one policy (M+SFC, M+RND, M+ICM) and agents with two policies (SID(M,SFC), SID(M,RND), SID(M,ICM). The results showed that the SID(M, SFC) agent received the highest rewards on average, and scheduling helped both ICM and SFC agents find the goal more effectively. Additionally, scheduling improved the performance of the RND agent. The study proposes a hierarchical agent SID that schedules between extrinsic and intrinsic motivations. It introduces a new type of intrinsic reward SFC for exploration in DRL. The agent's behavior is influenced by the scheduling of intrinsic policy, allowing it to focus on discovering faraway big rewards and avoiding distractions from nearby small rewards. The intrinsic policy collects experiences of big rewards, which the extrinsic policy learns from since they share the same replay buffer. The study introduces a new intrinsic reward SFC for exploration in DRL, showing improvements in exploration efficiency. Possible research directions include designing more efficient scheduling strategies, testing the framework in other control domains, and extending evaluation to real robotics systems. Ablation studies were conducted to investigate the influence of scheduling on the proposed method and whether other reward types can benefit from scheduling. The study compares agent configurations with different reward bonuses (ICM, RND, SFC) and scheduling strategies for exploration in DRL. Results for AppleDistractions were shown in Section 4.4, while an ablation study for FlytrapEscape is presented in Fig.6 (left). In an ablation study for FlytrapEscape, agents with the ICM component performed poorly, with only 1 run of M+ICM successfully navigating to the goal. However, scheduling greatly improved performance for SFC agents, with SID(M,SFC) solving the task in 5 out of 5 runs. The superior performance of SID(M,SFC) compared to M+SFC may be due to the intrinsic policy of the scheduling agent. The extrinsic policy in the FlytrapEscape environment involves random exploration before receiving any reward signal, making it a good candidate for exploring new rooms from bottleneck states. The SID(M,SFC) agent efficiently explores from bottleneck to bottleneck, while the M+SFC agent struggles to benefit from different behaviors under extrinsic and intrinsic rewards. Scheduling did not help ICM or RND agents, possibly because they are not particularly attracted to bottleneck states. In the FlytrapEscape environment, the SID agent shows improved performance compared to ICM and RND agents. The SID agent with SFC converges to more stable policies, while reward bonus agents tend to oscillate. Experiments with SID show that the scheduler chooses 8 times per episode which policy to follow. Further experiments on \"DoomMyWayHome\" show how different numbers of switches per episode affect agent performance. The implementation details of the algorithm using SID with SFC for intrinsic reward are described. The off-policy learning strategy utilizes the Ape-X DQN algorithm with 8 actors and K-step target for bootstraping. Computational efficiency is achieved by setting parameters and implementing a prioritized version for faster learning. In the implementation of the algorithm using SID with SFC for intrinsic reward, a prioritized experience replay is utilized for computational efficiency. The replay buffer is split into two sizes, 40,000 and 10,000, with transitions pushed based on TD-error. A batch of size 128 in the learner consists of 96 transitions from the normal replay buffer and 32 from the high TD-error buffer. This work investigates learning \u03c8 without the auxiliary task of reconstruction on features. In this work, the focus is on learning the successor features (SF) \u03c8 without the additional reconstruction stream. The features \u03c6 are kept fixed during SF learning to ensure stability, as SF estimates the expected features under transition dynamics and policy. SF is learned from the same replay buffer as the Q-Net training, using K-step experience tuples. Intrinsic reward is calculated using the SFC reward formulation on K-step transitions. The behavior policy \u03c0 associated with SF is not explicitly provided. The behavior policy associated with the SF is a mixture of current and past policies from all actors. Network parameters are shared for estimating the expected return of rewards, with the scale of rewards influencing gradient scale. Normalization aims to bring intrinsic and extrinsic rewards into the same range by dividing intrinsic rewards by their standard deviation. In our experiments, we keep a running estimate of the mean of the normalized reward and scale the extrinsic task reward with a hyperparameter \u03b7. The discount rate for the intrinsic reward is denoted as \u03b3 I, and we found that setting \u03b7 = 3 works best for all algorithms with intrinsic rewards. The algorithms were evaluated on FlytrapEscape, and we also tried different values for \u03b7 such as {0.3, 1, 3, 10}. In experiments, the intrinsic rewards were normalized by dividing them by a running estimate of their standard deviation. Different scaling values were tried, but the approach did not work well due to varying scales during training. The model architecture remained consistent across all experiments, with ReLU activations after every layer except the last ones in each block. BatchNorm was added before activation for the ICM module, following the original code. The code was implemented in PyTorch, using a stack of 4 consecutive preprocessed observations for all experiments. In experiments, the model architecture remained consistent with ReLU activations after every layer except the last ones in each block. BatchNorm was added before activation for the ICM module. The environment settings included downscaled images to 84 \u00d7 84 pixels and grayscale conversion for observations. The action space for FlytrapEscape consisted of TURN LEFT, TURN RIGHT, MOVE FORWARD, MOVE LEFT, MOVE RIGHT actions. The DmLab environment produced 84 \u00d7 84 RGB images as observations. For the 'Cart-pole' domain in the classic control task, the environment was configured to produce 84\u00d784 RGB pixel-only observations from the 1st camera. The images were converted to grayscale, and a set of 8 predefined actions were used for the experiments. The continuous action spaces were discretized to {-0.5, -0.25, 0, 0.25, 0.5} to match the agent's discrete action space requirements. In the 'Cart-pole' domain, images were converted to grayscale and stacked for input. The agent could move 'up', 'down', 'left', or 'right' in the four-room domain. Successor features were calculated analytically using a formula. Agents completed 10000 episodes with 30 steps each to ensure challenging exploration. Machines with 4 GeForce Titan X GPUs were used for the experiments. The experiments were conducted on machines with 4 GeForce Titan X GPUs running Ubuntu 16.04. Each machine ran 4 experiments in parallel, with each experiment on a separate GPU. Successor features were visualized by discretizing the map into grids and positioning the trained agent at each grid. The l2-difference of the successor features matrix was calculated for different orientations. Additionally, the successor features of an agent trained with SID were visualized at the end of training. The SF are trained with experiences from the agent's behavior policy, showing good agreement with geometric distance. Big intensity changes are observed around bottlenecks in the heatmap, supporting the hypothesis that SFC leads the agent to bottleneck states. SF behavior in a first-person view environment is demonstrated for the first time. The evolution of SF over time can be seen in a video. Optimizing deep function approximators with algorithms like gradient descent requires stable TD-target values to ensure meaningful updates. In off-policy DRL, using a target network stabilizes updates but leads to incremental policy adaptation. However, for exploration, an opposite treatment of policy updates may be beneficial due to the non-stationary nature of intrinsic rewards and exploration policies. The common approach of using intrinsic rewards as a bonus to extrinsic rewards may require a balancing act for effective exploration. To address the issue of balancing exploration with stable learning, a hierarchical approach is proposed. This approach involves learning multiple policies optimized for different reward functions corresponding to extrinsic and intrinsic tasks. Each policy is trained on a different Markov Decision Process (MDP) using off-policy Deep Reinforcement Learning (DRL) algorithms. This allows for slowly changing target values while still enabling drastic behavior changes. The proposed hierarchical approach involves learning multiple policies optimized for different reward functions in various Markov Decision Processes (MDPs) using off-policy Deep Reinforcement Learning (DRL) algorithms. A high-level scheduler selects policies for the agent during episodes to maximize the extrinsic reward. By allowing the agent to follow one motivation at a time, a pool of behavior policies is created without creating unstable targets for off-policy learning. This approach increases the behavior policy space exponentially and strictly separates stationary and non-stationary behaviors. The study explored different high-level schedulers for optimizing behaviors in Deep Reinforcement Learning. A random scheduler performed well, indicating the benefits of stochastic scheduling in maximizing behavior policy space utilization. Three types of schedulers were investigated: Random, Switching, and Macro-Q Scheduler. Each scheduler had its own approach to task selection and learning from experience. The study investigated different high-level schedulers for optimizing behaviors in Deep Reinforcement Learning. The Macro-Q Scheduler stores subsampled experiences and additional macro-transitions in the shared replay buffer. The Threshold-Q Scheduler selects tasks based on the Q-value output of the extrinsic task head, using selection strategies like running mean and heuristic median. The study compared different high-level schedulers for optimizing behaviors in Deep Reinforcement Learning. The Threshold-Q Scheduler selects tasks based on a fixed value around the median of Q-values, choosing intrinsic when below and extrinsic otherwise. None of the scheduler choices consistently outperformed a random scheduler across all environments. This aspect is left for future work."
}