{
    "title": "BJRZzFlRb",
    "content": "Natural language processing (NLP) models require a large number of parameters for word embeddings, leading to high storage requirements. To address this issue, a method using few basis vectors and multi-codebook quantization is proposed. By learning discrete codes in an end-to-end neural network, a compression rate of 98% in sentiment analysis and 94% to 99% in machine translation tasks is achieved without performance loss. The proposed method improves model performance in tasks like sentiment analysis and machine translation by slightly reducing the compression rate. It is language-independent and does not require changes to the network architecture. Word embeddings are crucial in NLP models, but the large number of parameters in the embedding matrix can be a challenge. In an attempt to reduce the number of parameters in word embeddings without compromising model performance, this study focuses on compressing NLP models for deployment on devices with limited memory. The large storage footprint of neural networks poses challenges, especially when fitting into GPU memory during training. The goal is to address the issue of model size for low-latency neural computation on mobile platforms. In an effort to reduce the parameters in word embeddings for efficient deployment on memory-constrained devices, the study proposes using partially shared embeddings by assigning each word a compact code instead of a unique ID. This approach aims to capture similarities between words while still representing their syntactic differences. The study proposes using embedding vectors to represent codes in the vocabulary, creating multiple codebooks with codeword vectors. The embedding of a word is computed by summing up the codewords from the codebooks. This approach reduces the number of embedding vectors while aiming to maintain performance. The study suggests using embedding vectors to represent codes in the vocabulary by creating multiple codebooks with codeword vectors. The goal is to reduce the number of embedding vectors while maintaining performance. This is achieved by minimizing the squared distance between baseline embeddings and composed embeddings, selected from codebooks, in a compression-based source coding approach known as product quantization and additive quantization. The study proposes using compositional coding for constructing word embeddings with fewer parameters. A simple method using Gumbel-softmax trick in an end-to-end neural network is suggested to learn discrete codes. This approach allows for the use of differentiable loss functions like cosine similarity. The study proposes a coding approach for constructing word embeddings with significantly fewer parameters, achieving high compression rates in sentiment analysis and machine translation tasks. The approach involves direct learning of codes in an end-to-end neural network using a Gumbel-softmax layer. This method simplifies the computation graph for composing embeddings and does not require modifications to other parts of the network. Existing works for compressing neural networks include low-precision computation, quantization, network pruning, and knowledge distillation. The study proposes a coding approach for constructing word embeddings with fewer parameters, achieving high compression rates. Network quantization methods like HashedNet force weight matrices to have few real weights. DeepCompression groups weight values into clusters based on pre-trained matrices. Iterative pruning makes a network sparse by removing weight values below a threshold. The proposed method is compared with iterative pruning in this paper. In this paper, the proposed method is compared with iterative pruning for learning compact codes related to learning to hash. Previous works focused on efficient similarity search of image descriptors, while this work emphasizes reducing codebook sizes and learning efficient codes to avoid performance loss. The coding scheme of additive quantization is adopted for storage efficiency. In this work, a method is proposed to learn code assignment and codebooks simultaneously in an end-to-end neural network. The approach encourages discreteness using the Gumbel-Softmax trick for producing compositional codes. Other recent works in learning to hash also utilize neural networks to produce binary codes with binary constraints. The Gumbel-Softmax trick is used to produce compositional codes in a neural network. Character-level segmentation can reduce unique word types. Different approaches like character-based models and char-gram input features have been proposed. Our approach for compressing word embeddings does not have limitations like character-based models. The coding approach follows the scheme in additive quantization. The coding approach in additive quantization represents words with compact binary codes, making the learning problem equivalent to matrix factorization. This method produces shorter, more storage-efficient codes, with the model size dependent only on the number of basis vectors. The model size in the coding approach is dependent on the number of basis vectors. Short codes are crucial, with binary codes having N/2 bits for N basis vectors. Compositional coding uses M log 2 K bits for N basis vectors, with a 32 \u00d7 16 scheme producing 128-bit codes. The computation involves a summation over vectors, with different coding approaches compared. The comparison of coding approaches in computing embeddings shows that the compositional approach requires fewer vectors, resulting in shorter codes and lower computational costs. The original embedding matrix is represented by E \u2208 R |V|\u00d7H, with each vector having H dimensions. The objective function in Eq. 3 focuses on minimizing the reconstruction loss. The model uses the reconstruction loss as the objective function to learn discrete codes. It converts the problem into finding optimal one-hot vectors using a Gumbel-softmax trick. The neural network architecture includes a hidden layer with a fixed temperature of \u03c4 = 1. The end-to-end neural network is illustrated in FIG1, functioning as an auto-encoder with a Gumbel-softmax middle layer. The neural network for coding learning is an auto-encoder with a Gumbel-softmax middle layer. It has five parameters (\u03b8, b, \u03b8 , b , A) and can easily obtain the code C w for each word. For NLP tasks, compositional codes can be learned from GloVe vectors. However, for tasks like machine translation, word embeddings are jointly learned with the neural network. In such cases, a normal model is first trained to obtain baseline embeddings, and then task-specific codes are learned based on the trained embedding matrix. The code learning model cannot be jointly trained with the machine translation model. In experiments, the focus is on evaluating the compression rate of word embeddings for sentiment analysis and machine translation tasks. Model performance and embedding layer size are compared with a baseline model and iterative pruning method BID9. The size of other parts in neural networks is not included in the results. The method requires a set of baseline to learn efficient compact codes for each word. Our proposed method requires baseline embedding vectors for learning efficient compact codes for words. Codes are learned based on GloVe vectors for sentiment analysis and task-specific word embeddings for machine translation. The end-to-end network is trained to automatically learn the codes by optimizing network parameters to minimize reconstruction loss. Training involves sampling a small batch of embeddings from the baseline matrix, with a batch size of 128 and Adam optimizer with a fixed learning rate of 0.0001. Training runs for 200K iterations, saving parameters every 1,000 iterations if the loss decreases. The model training process involves examining loss on a fixed validation set and saving parameters if the loss decreases. Training is distributed to 4 GPUs using the nccl package, with each round taking around 15 minutes to complete. The dataset for sentiment analysis is the IMDB movie review dataset BID26, with 25k reviews for training and testing. Texts are lowercased and tokenized using the nltk package, and 300-dimensional uncased GloVe word vectors are used as baseline embeddings. The vocabulary includes around 75k words from both the IMDB dataset and GloVe vocabulary. Reviews are truncated to 400 words. The model architecture consists of a single computational graph with the only difference being the embedding layer. The model architecture for sentiment analysis involves a single LSTM layer with 150 hidden units and a softmax layer for binary label prediction. The embedding layer differs between the baseline model, which uses a 75K \u00d7 300 GloVe embedding matrix, and compressed models using a matrix of basis vectors. Training details include using Adam optimizer for 15 epochs with a fixed learning rate of 0.0001, and evaluating loss on a validation set at the end of each epoch. Results: Training the code learning network with different settings of the number of components M and the number of codewords K showed that increasing either M or K can effectively decrease the reconstruction loss. However, setting M to a large number will result in longer hash codes, significantly increasing the size of the embedding layer. It is important to choose correct numbers for M and K to balance performance and model size. The results using normalized product quantization (NPQ) are also shown. The study focused on quantization of filtered GloVe embeddings using codes provided by authors for model training. A 16 \u00d7 32 coding scheme achieved a 98.4% compression rate for the embedding layer. Improved classification accuracy was observed with slightly lower compression rates, possibly due to strong regularization. Experiments were conducted on IWSLT 2014 German-to-English and ASPEC English-to-Japanese translation tasks using small datasets. The moses toolkit was utilized for tokenization and lowercase processing. The study utilized moses toolkit and BID20 for tokenization and lowercase processing in machine translation. They concatenated TED/TEDx corpus for a test set and applied byte-pair encoding to transform texts to subword level. The ASPEC dataset with 300M bilingual pairs was used, with 150M pairs for training. English texts were tokenized by moses toolkit and Japanese texts by kytea BID33. Model architecture included a 32 \u00d7 16 coding scheme. The model architecture for the NMT model includes a bi-directional encoder with two LSTM layers and a decoder with two LSTM layers. Residual connection BID12 with a scaling factor of 1/2 is applied to the decoder states. LSTMs and embeddings have 256 hidden units in the IWSLT14 task and 1000 hidden units in the ASPEC task. Key-Value Attention BID30 is applied to the first decoder. Dropout with a rate of 0.2 is used, except in recurrent computation. The NMT model architecture includes a bi-directional encoder and decoder with LSTM layers. Training details involve using Nesterov's accelerated gradient with an initial learning rate of 0.25. The models are evaluated using smoothed BLEU on a validation set, with the learning rate reduced if no improvement is seen. Training involves distributing to 4 GPUs, with a baseline NMT model trained first to obtain task-specific embeddings. Hash codes and basis vectors are obtained through code learning, and NMT models using compositional coding are retrained with reconstructed embeddings. The experimental results show that iterative pruning achieves a 92% compression rate on the ASPEC dataset by removing 90% of connections. However, there is a slight performance loss on the IWSLT14 dataset with the same pruning ratio. Models using compositional coding achieve a 94% compression rate on IWSLT14 and 99% on ASPEC. Lowering the compression rate can lead to performance improvements. NMT models still have large sizes due to the softmax and recurrent layers. The model learned to assign similar codes to words with similar meanings, reducing redundancy in word embeddings and achieving a high compression rate. Analysis of code efficiency involves counting words containing specific subcodes in each component. In experiments with an 8 \u00d7 8 coding scheme, 31% of words have subcode \"0\" in the first component, while only 5% use subcode \"1\". The code learning model efficiently assigns codes without waste, with even the least popular codeword used by about 1000 words. A novel method is proposed to reduce parameters in word embeddings by composing embedding vectors using a small set of basis vectors. The proposed method aims to reduce the size of embedding layers by using a small set of basis vectors determined by the hash code of each word. By applying a compositional coding approach, redundancy in representing similar words is eliminated. The method involves learning discrete codes directly in a neural network, resulting in a significant reduction in the size of the embedding layer without impacting performance. The approach considers semantic inter-similarity among words and maintains a dense basis matrix, with potential for further compression through pruning. The proposed method aims to reduce the size of embedding layers by using a small set of basis vectors determined by the hash code of each word. By applying a compositional coding approach, redundancy in representing similar words is eliminated. The advantage of this approach is more significant when the size of the embedding layer is dominated by the hash codes. In both tasks, some hash codes are shared among multiple words, but this does not impact performance. The proposed method aims to reduce the size of embedding layers by using a small set of basis vectors determined by the hash code of each word. By applying a compositional coding approach, redundancy in representing similar words is eliminated. This approach is more significant when the size of the embedding layer is dominated by the hash codes. The model learns a set of codes using a 3 x 256 coding scheme to decompose each embedding into 3 vectors, maximizing compression rate. Codes like \"210\" for \"male\" and \"232\" for \"female\" can transform words like \"man/king\" to \"woman/queen\". This phenomenon can also be observed in city names."
}