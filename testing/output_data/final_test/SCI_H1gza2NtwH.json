{
    "title": "H1gza2NtwH",
    "content": "The geometric properties of loss surfaces in deep learning are linked to generalization. The True Hessian has eigenvalues of smaller absolute value than the Empirical Hessian, as shown using random matrix theory. This difference is supported for different SGD schedules on various neural networks. A faster spectral visualization framework based on GPU accelerated stochastic Lanczos quadrature is proposed for these experiments. The success of deep learning in computer vision and natural language processing has sparked interest in investigating the spectral properties of matrices through methods for spectral visualization. Research on the Hessian and its eigenspectrum aims to understand optimization procedures like SGD in training neural networks. The condition number, which is the ratio of the largest to smallest eigenvalues, plays a crucial role in determining convergence rates for optimization methods. The presence of negative eigenvalues indicates non-convexity even at a local scale, affecting generalization of solutions obtained through optimization methods like SGD. Batch size in SGD also influences the sharpness of solutions, with smaller batches leading to convergence to flatter solutions and better generalization. Geometrical insights have led to procedures and algorithms that optimize models for local flatness in weight space. In this paper, random matrix theory is used to analyze the spectral differences between the Empirical Hessian and the True Hessian in Bayesian neural networks. The differences in extremal eigenvalues depend on the ratio of model parameters to dataset size and the variance per element of the Hessian. The Empirical Hessian spectrum, compared to the True Hessian, shows broadening with larger largest eigenvalues and smaller smallest eigenvalues. Experiments on CIFAR-10 and CIFAR-100 datasets using a 110 Layer PreResNet support this theory. Key questions about flatness or sharpness of optima, local non-convexity, and rank degeneracy are investigated. Previous work used random matrix theory to study neural network Hessians, showing decreasing loss value difference between local and global minima. No theoretical work has been done on the difference in spectra between the True Hessian and Empirical Hessian in machine learning. Previous empirical work on neural network loss surfaces has focused on the Empirical Hessian. The spiked covariance literature studies learning the true covariance matrix from the noisy sample covariance matrix, with applications leading to improved results. In machine learning, previous empirical work has focused on the Empirical Hessian, but no theoretical work has been done on the difference in spectra between the True Hessian and Empirical Hessian. The spiked covariance literature studies learning the true covariance matrix from the noisy sample covariance matrix, with applications leading to improved results. In machine learning, previous empirical work has focused on the Empirical Hessian, but no theoretical work has been done on the difference in spectra between the True Hessian and Empirical Hessian. Using random matrix theory, the eigenvalue perturbations between the True Hessian and Empirical Hessian are analytically derived, showing a broadened spectrum for the Empirical Hessian. A visualization technique combining GPU acceleration and stochastic Lanczos quadrature is faster and requires fewer hyper-parameters, providing consistent results with observed moment information. This method can be used to compute spectra of large matrices in deep learning. In machine learning, the family of prediction functions is parameterized by a weight vector w. The goal is to minimize the loss over the data generating distribution by varying w. The true risk is the expected negative log likelihood per data point, while the empirical risk is based on the dataset. The Hessian describes the curvature in weight space, and the generalization gap is the difference between achieving low values of different equations. The Empirical Hessian can be rewritten in terms of eigenvalue, eigenvector pairs, representing curvature in weight space. The largest eigenvalue or Frobenius norm is used to define sharpness of an optimum. The normalized mean value of the spectrum has also been utilized in previous studies. The True Hessian, derived from the full data generating distribution, may offer better generalization with flat solutions. Reparametrization can create the illusion of flatness or sharpness in the Hessian. The True Hessian, derived from the full data generating distribution, may offer better generalization with flat solutions. Reparametrization can create the illusion of flatness or sharpness in the Hessian. The elementwise difference between the True and Empirical Hessian converges to a zero mean normal random variable, indicating that the true risk surface is flatter than its empirical counterpart. The True Hessian, derived from the full data generating distribution, may offer better generalization with flat solutions. The difference between the True and Empirical Hessian converges to a zero mean normal random variable, indicating that the true risk surface is flatter than its empirical counterpart. In deep learning, the network size often surpasses the dataset size by a large margin, leading to challenges in deriving analytic results. By moving to the large Dimension limit and utilizing random matrix theory, we can study the perturbations on the eigenspectrum between the True Hessian and Empirical Hessian. This approach differs from the classical statistical regime and focuses on the regime when the ratio of parameters to samples is greater than 1. Additionally, assumptions are made on the Gaussian distribution of the elements and the low rank of the True Hessian to make the analysis tractable. The extremal eigenvalues of the Empirical Hessian are larger in magnitude than those of the True Hessian, showing a spectral broadening effect. This effect has been observed when transitioning from training to test sets. The variance of the Empirical Hessian eigenvalues includes the variance of the True Hessian eigenvalues. The eigenvalues of the True Hessian and the variance of the Hessian are related. Extremal eigenvalues are determined by the noise matrix if a certain condition is not met. Results for the Wigner ensemble can be extended to non-identical variances and element dependence. Technical conditions can be extended for more general results. Assumption 2 can be relaxed under certain conditions. The Lanczos algorithm is used for spectral analysis on the Hessian of neural networks with millions of parameters. It approximates extremal and interior eigenvalues efficiently, requiring Hessian vector products with computational cost O(NP). The algorithm enforces orthogonality and stores the Krylov subspace to compute spectra of large matrices in deep learning. The Lanczos algorithm efficiently computes spectral density estimates by re-orthogonalizing at every step using random vectors to approximate the spectral density. It relates to Gaussian quadrature, with nodes given by Ritz values and weights by eigenvectors of the Lanczos matrix. The Lanczos algorithm efficiently computes spectral density estimates using random vectors to approximate the spectral density. The error between the expectation and the Monte Carlo sum can be bounded but the bounds are too loose to be practical. In the high dimensional regime, the squared overlap of random vectors with eigenvectors of H is approximately 1/P with high probability. The Lanczos algorithm can be easily parallelized. The Lanczos algorithm can be easily parallelized for computing eigenvalues of large matrices in deep learning. GPU parallelization is utilized for significant acceleration. Multiple random vectors are used to reduce variance in spectral evaluation, with a resulting discrete moment matched approximation smoothed using a Gaussian kernel. The Lanczos algorithm quickly converges to spectral outliers using a single random vector, avoiding kernel smoothing and increased computational cost. The discrete spectral density is plotted for a single random vector, allowing for the evaluation of eigenvalues near the origin. The Lanczos algorithm quickly converges to spectral outliers using a single random vector, avoiding kernel smoothing and increased computational cost. To determine the fraction of negative eigenvalues, the weight of negative Ritz values is calculated, well separated from the smallest magnitude Ritz value(s). The validity of theoretical results is tested by controlling the parameter q = P/N through data-augmentation and running GPU powered Lanczos method on the augmented dataset. Random horizontal flips, 4 \u00d7 4 padding with zeros, and random 32 \u00d7 32 crops are used for augmentation. We use data augmentation techniques like random horizontal flips, padding, and crops for training neural networks on CIFAR-10 and CIFAR-100 datasets. The Empirical Hessian is analyzed with a 110 layer pre-activated ResNet architecture using PyTorch. The training procedure for VGG-16 is detailed in the appendix. Different SGD decayed learning rate schedules are used to compare sharp minima with flatter ones in terms of generalization. The study analyzes the True Hessian by running two different SGD decayed learning rate schedules on a 110-Layer ResNet. Training curves, accuracies, losses, and eigenvalues are compared for Normal and Overfit schedules on CIFAR-100 and CIFAR-10 datasets. SLQ is used to approximate the spectrum of the Empirical Hessian and Augmented Hessian. The 110-Layer ResNet has over a million parameters for both datasets. The study compares the extremal eigenvalues of the Augmented Hessian with the Empirical Hessian for CIFAR-100 SGD. A sharp drop in negative spectral mass is observed, indicating that most of it is due to the perturbing GOE. The spectral density plots show the differences between the two Hessians, with the extremal eigenvalues shrinking for CIFAR-100. The extremal eigenvalues shrink for CIFAR-100/CIFAR-10, with the Overfit schedule augmented Hessians showing larger values than the Normal schedule. The Empirical Hessian has a fraction of negative Ritz values and relative negative spectral mass, while the Augmented Hessian has no negative Ritz values. The majority of eigen-directions for the Augmented Hessian are locally close to flat. The majority of eigen-directions for the Augmented Hessian are locally close to flat, with the spectral bulk carried by the 3 closest weights to the origin. Moving from q 1 regime to q < 1 results in a shift from symmetric to right skew bulk, corroborating previous results. The spectral analysis of the Augmented spectra shows a low effective dimension with significant negative spectral mass shrinking. The Overfit Schedule for CIFAR-100 has no spectral mass below 0, while CIFAR-10 has one negative Ritz value. The True Hessian is introduced to study the difference between true and empirical loss surfaces, with analytic forms derived for the perturbation between extremal eigenvalues. The geometric properties of loss landscapes in deep learning impact generalization performance. We developed a method for fast eigenvalue computation and visualization to approximate the True Hessian spectrum, showing smaller variation in eigenvalues compared to the Empirical Hessian. The Empirical Hessian is expected to have a greater negative spectral density than the True Hessian, which may explain the success of first-order methods on neural networks. Reported non-convexity and pathological curvature are worse for the empirical risk than the true risk, crucial for developing effective procedures for Bayesian deep learning. The geometric properties of the loss surface greatly influence the predictive distribution in Bayesian deep learning. Posterior representation for neural network weights with popular approaches like the Laplace approximation is defined by the Hessian curvature. Future work could explore using a white Wishart kernel to derive results for empirical Gauss-Newton and Fisher information matrices. Our approach to efficient eigenvalue computation and visualization can be used as a general-purpose tool to investigate spectral properties of large matrices in deep learning. The resolvent of a matrix H is defined with z = x + i\u03b7 \u2208 C, and the Stieltjes transform of \u03c1 is the normalized trace operator of the resolvent in the N \u2192 \u221e limit. The R transform of the Wigner ensemble is crucial for our calculations. Consider an n \u00d7 n symmetric matrix M n, known as a real symmetric Wigner matrix. Theorem 2 states that a sequence of Wigner matrices converges weakly to the semi. The property of freeness for non-commutative random matrices can be analogously considered to the moment factorization property of independent random variables. Matrices A and B are considered free if they satisfy certain conditions. The Stieltjes transform of Wigner's semi-circle law can be computed using the Blue transform. Perturbation theory can be used to find the Blue transform, which gives the leading-order result. The Blue transform can be used to find the leading-order result by setting \u03c9 = S M (z) using the ansatz. An extensive discussion on spectral broadening for sample covariance matrices can be found in (Ledoit & Wolf, 2004). The dispersion of Empirical Hessian eigenvalues around their mean equals the dispersion of True Hessian eigenvalues plus the variance of the Empirical Hessian. Theoretical assumptions in sections 4.1 and 4.2 allow for analytic results on this broadening. Theoretical assumptions in sections 4.1 and 4.2 allow for analytic results on spectral broadening. Using the Lanczos algorithm with computational cost O(N T m), properties of modern neural network spectra with millions of parameters are empirically analyzed. The Lanczos algorithm properties are summarized in theorems 3,4. The eigenvalues and eigenvectors of T k are related to the Gauss quadrature rule. The spectral density approximation using moments and the error in Monte Carlo simulations are discussed. In high dimensions, the overlap of random vectors is expected to decrease. In the high dimensional regime, the squared overlap of random vectors with eigenvectors of H is expected to be high. Analytical results for Gaussian vectors have been obtained. The training procedure for the network involves an initial learning rate of 0.05 and 300 epochs, resulting in a reduction in extremal eigenvalues."
}