{
    "title": "H1ets1h56E",
    "content": "Concerns about interpretability, computational resources, and principled inductive priors have led to efforts to engineer sparse neural models for NLP tasks. Research shows that well-trained neural models may naturally become sparse, with frequent input words leading to concentrated activations and frequent target words leading to dispersed activations but concentrated gradients. Gradients associated with function words are more concentrated than those of content words, even when controlling for word frequency. Despite advancements in deep learning, interest in sparse modeling for NLP persists. Sparse representations of language in NLP are motivated by concerns about computational resources and interpretability. Some research shows that sparse attention mechanisms and sparsified language models can outperform dense methods in NLP tasks. A study analyzes how sparsity arises in different layers of a neural language model in relation to word frequency, showing that word representation sparsity increases with exposure during training. Additionally, evidence of syntactic learning is found in gradient updates during backpropagation based on a word's part of speech. The language model is trained on a corpus of tokenized English Wikipedia, excluding rare words. A 2-layer LSTM LM is used with cross entropy loss for 50 epochs. The network backpropagates gradient updates based on word part of speech. The language model is trained on English Wikipedia data with a 2-layer LSTM LM. The model backpropagates gradient updates for word part of speech. The encoding layer has 200 units, recurrent layers have 200 hidden units each, batch size is 40, sequence length is 35, and dropout ratio is 0.2. Sparsity is measured using the Taxicab-Euclidean norm ratio. The Taxicab-Euclidean norm ratio, \u03c7(v) = v2/v1, illustrates the relationship between sparsity and concentration in vectors. When elements of v are close to 0, \u03c7 is higher, indicating a more concentrated vector. Sparsity affects model behavior, with highly concentrated activation vectors and gradients when only a few units contribute significantly to predictions. The relationship between sparsity and concentration in vectors is illustrated by the Taxicab-Euclidean norm ratio \u03c7(v) = v2/v1. Sparsity affects model behavior, leading to highly concentrated activation vectors and gradients when only a few units contribute significantly to predictions. Gradient sparsity increases with the frequency of a word in the corpus and overall training time, indicating that more exposure leads to sparser relevance. Activation sparsity is not correlated with target word frequency, as confirmed in experiments. The correlation between activation sparsity and target word frequency is not significant. The sparse structure in the gradient passed from softmax to the top LSTM layer is specific to how a word interacts with its context. The concentration of words from open and closed POS classes differs, with closed class words stabilizing while open class words continue to concentrate throughout training. This difference is attributed to the ambiguity of words in open classes. The importance of closed and open classes in shaping network structure during training is discussed. Function words show high sparsity in gradients, contrasting with the influence of content words on outputs. Previous studies on word impact did not consider word frequency, but this is addressed in the current work. In this work, correlations are measured rather than raw magnitude to analyze the representations of input words in the training process. The study reveals that the lower recurrent layer quickly learns sparse representations of common input words, while the higher layers show slower increases in correlation with word frequency. Gradient sparsity does not become significantly correlated with word frequency. The importance of individual units in feedforward networks is also explored by erasing dimensions and measuring the impact on target likelihood. The study explores the importance of individual units in neural networks by erasing dimensions and measuring the impact on target likelihood. It is found that importance is concentrated in a small number of units at lower layers and more dispersed at higher layers, possibly due to the sparsity of activations at lower layers. The trajectory over training is related to the Information Bottleneck Hypothesis, suggesting that early training focuses on effectively representing inputs, while later stages compress these representations. This compression may lead to the observed effect of an upward trend in \u03c1 \u2192 as extraneous units are removed. The study examines the concentration of importance in neural network units, with a focus on the trajectory over training related to the Information Bottleneck Hypothesis. The optimizer rescales step size, leading to an upward trend in \u03c1 \u2192 as extraneous units are mitigated. Common target words show concentrated gradients in the final LSTM layer, possibly due to high confidence from information about these words. Sparsity at the LSTM gradient is not directly affected by sparse logits, but the model may still be \"high confidence\" in assigning blame for errors during common events. According to the hypothesis, specialized neurons handle common words, which act as attractors and prototypes for other words. This process is similar to how humans learn language. Some neurons in a translation system specialize in specific word forms, shaping the handling of words like comparatives. Common words like 'better' trigger specialized units, while rarer words have more distributed representations. The gradient associated with common words in dense networks is more concentrated due to their interactions with recent words. Understanding where natural sparsity emerges in these networks can guide the application of sparsity constraints without affecting model performance. Understanding sparsity in neural networks can guide the application of sparsity constraints without impacting model performance. This knowledge can help identify when simple ensembling approaches are effective and accelerate the concentration of common words during training. Future work may explore how sparsity emerges in models for specific tasks and if concentration is a useful measure for information compression."
}