{
    "title": "rJxs5p4twr",
    "content": "The paper explores the main factors influencing a classifier's decision-making by analyzing latent codes from auto-encoding frameworks. A method is proposed to explain the classifier's behavior using examples that highlight semantic differences through interpolations in latent space. The concept of semantic stochastic paths and Lagrangians is introduced to emphasize differences in the classifier's decisions. The deep classification paradigm lacks transparency in decision-making, hindering its adoption in areas requiring explanation. Saliency maps are used to explain misclassifications, but recent research suggests they can be misleading. Some methods fail to provide accurate explanations due to sensitivity to feature space changes. In response to limitations of saliency maps in explaining misclassifications, a new formalism is proposed to generate sets of semantically-connected examples that highlight differences in a black-box model's decision. This approach aims to provide explanations for misclassifications in tasks like image classification, such as when a female is classified as male or a smiling face is labeled as not smiling. The proposed formalism aims to explain misclassifications in black-box models by generating sets of semantically-connected examples. This is achieved by changing features from a misclassified image to a correctly classified one, tracking the output probability along the way. The method uses a variational auto-encoder framework to create paths in feature space and optimize them based on the black-box classifier's decisions. This process highlights classification differences effectively. The approach involves creating stochastic semantic paths in feature space using an Auto-Encoder to highlight classification differences in black-box models. Sampling points along these paths generates examples that track the output probabilities of the classifier. The approach involves creating stochastic semantic paths in feature space using an Auto-Encoder to highlight classification differences in black-box models. The explanatory power of this approach is demonstrated on the MNIST and CelebA datasets, focusing on explaining specific decisions of black-box models by providing qualitative understanding of the relationship between data points and model predictions. The text discusses the importance of defining factors that can change a classifier's decision. By using a variational auto-encoder, one can find a code that describes data points, allowing for changes in the code to affect classification decisions. The goal is to provide model-agnostic, interpretable, and locally faithful explanations for classifier decisions. The text introduces the variational auto-encoder (VAE) to explain black-box classifier decisions by mapping data to a latent linear space of codes. This generative model minimizes distances between the data distribution and model distribution by using an encoder distribution Q \u03c6 (Z|X) and a decoder distribution P \u03b8 (X|Z) parametrized by neural networks. The VAE introduces an encoder distribution Q \u03c6 (Z|X) to approximate the true posterior distribution P \u03b8 (Z|X) and minimize the Kullback-Leibler divergence. The inferred latent code can be used to modify features of input data. A black-box model b(l, x) is defined as a classifier that assigns probabilities to data points belonging to classes. The black-box model b(l, x) assigns data points to classes. In a litigation case, x 0 was misclassified, and an explanation can be articulated using an example set E. The text discusses the concept of semantic changes in classification decisions, focusing on how factor changes impact the decision-making process. It introduces the idea of semantic interpolations in feature space to generate examples that provide local fidelity. The text introduces the concept of probability measures over semantic paths in X to constrain paths for classifier decision explanation. It highlights the VAE's ability to provide high-level semantic information through latent codes. Interpolations between latent codes can generate examples with common characteristics, leading to interpolating stochastic processes on feature space X. The focus is on linear latent interpolations to construct an interpolating stochastic semantic process. The text discusses the process of generating stochastic interpolations on feature space X through linear latent interpolations using VAE. It involves sampling latent codes at endpoints, choosing points along the line connecting them, and decoding to create a stochastic process. The procedure is detailed in subsection 5.2, with a visualization in Fig. 1b. The text discusses generating stochastic interpolations on feature space X through linear latent interpolations using VAE. It involves sampling latent codes at endpoints, choosing points along the line connecting them, and decoding to create a stochastic process. This process induces a continuous-time stochastic process on feature space X over the interval [0, T]. The text discusses generating stochastic interpolations on feature space X through linear latent interpolations using VAE. It involves sampling latent codes at endpoints, choosing points along the line connecting them, and decoding to create a stochastic process. The sample paths are interpolations that start and terminate at x0 and xT almost surely. The stochastic semantic process construction in Proposition 1 is one way to define such a process, with other natural options available. To generate examples explaining the defendant black-box model in the current litigation case, autoencoding mappings (P \u03b8, Q \u03c6) need to be discovered to highlight classification. To design auto-encoding mappings P \u03b8, Q \u03c6, an optimization problem is proposed to ensure semantic paths between data points x0 and xT highlight classification differences. The minimization problem can be viewed in the context of Lagrangian mechanics, introducing the model-b semantic Lagrangian for the semantic model action. This optimization aims to deliver meaningful paths, similar to those specified by the equations of motion in mechanics. The optimization problem involves finding encoding mappings P \u03b8, Q \u03c6 to create explainable semantic paths in a black-box model. The objective function includes the Lagrangian action and a hyperparameter \u03bb controlling the action's scale. The average over stochastic paths is taken with respect to the path integral, aiming to highlight classification aspects along the example set. The text discusses the PATH Auto-Encoder algorithm for creating explainable semantic paths in a black-box model. It involves sampling paths in feature space, evaluating semantic actions, and updating encoding mappings P \u03b8, Q \u03c6. The VAE loss function is trained on the entire dataset, while the action term only considers the start and end points. The algorithm aims to highlight classification aspects in the example set. The text discusses the Minimum Hesitant Path algorithm for generating example paths in a black-box model. It aims to find paths where the classifier's decisions change quickly, leading to highly certain regions in X. The algorithm uses Lagrangian functionals to define an objective value for explanations. In Figure 2, Probability Paths for the litigation case are shown, with the y-axis representing classification probability and the x-axis representing interpolation index. The path is forced to move through specific regions to achieve the desired outcome. The text discusses enforcing specific conditions on paths in a black-box model using Lagrangian functionals. Regularizers are introduced to ensure certain reconstruction levels at endpoints. The objective function includes hyper-parameters and actions associated with the minimum hesitant path. The study evaluates a method using VAE on real-world datasets MNIST and CelebA. Different VAE models are compared, including VAE-EDGE and PATH-VAE. A black-box classifier is defined as a deep network with convolutional layers. Architecture details and training procedures are provided in the Appendix. The study compares VAE models on MNIST dataset. VAE delivers uninformative interpolations, while PATH-VAE causes abrupt changes in output probability. Explanations suggest features like the lower stroke in digit two shifting up to coincide with digit seven, and the upper bar of digit seven having significant decision weight. The study compares VAE models on MNIST dataset, with PATH-VAE causing abrupt changes in output probability. Using Wasserstein Auto-Encoder (WAE) and Adversarial Auto-Encoder (AAE) as generative models, the models aim to control the path action while reducing variance within different paths. Theoretical details and architectures are provided in the Appendix, with action values defined over random litigation end pairs showing lower values for the PATH version of the model. The study compares VAE models on MNIST dataset, with PATH-VAE causing abrupt changes in output probability. The interpolation saliency method is defined to compare explanation models, showing minimum changes needed to alter classification decisions. The curvature of the lower bar is crucial for classifying as a two, while the style of the upper bar is important for a seven. A sanity check analysis is conducted to study the rank correlation between original and obtained saliency maps. In 2018, a study investigated the correlation between original and randomized saliency maps for a black-box classifier using the CelebA dataset. Specific misclassifications were examined, with interpolations by VAE models showing limited informative changes. The PATH-VAE model demonstrated sharp changes in probability along with visible teeth alterations. The PATH-VAE model shows significant changes in probability and visible teeth alterations, indicating the importance of teeth visibility in detecting a smile for black-box models. The model can influence classifier behavior along different paths, with experimental examples provided in the Appendix. In 2009, a study presented Probability Paths for detecting smiles in images of celebrities using interpolation index and classification probability. Various approaches were discussed to address gradient saturation and modify gradient explanations for classifier behavior. Various explanatory models, including Bayesian approaches and variational Bernoulli distributions, are used to obtain saliency of inputs in contrast to gradient-based methods. Substitution of black-box models with locally interpretable linear classifiers and selecting informative data points are successful methodologies. Auto-encoder interpolations aim to ensure reconstruction quality. Auto-encoder interpolations focus on reconstruction quality and latent space distortions. Adversarial methods aim to improve interpolation quality, but lack interpretable explanations for classifier decisions. The Auto-Encoder framework can also serve as a tool for dimensionality reduction by mapping data onto a manifold in feature space. Our approach extends the auto-encoder framework by focusing on interpolation paths in feature space to explain black-box classifiers. We train the auto-encoder to guarantee reconstruction quality and impose conditions on its interpolations to encode information about classification decisions. This formalizes the notion of a stochastic process induced in feature space by latent code interpolations. The methodology involves a stochastic process induced by latent code interpolations in feature space. The models were trained on MNIST images without preprocessing, using Adam optimizer with specific parameters. The encoder and decoder architectures used fully convolutional designs. The encoder and decoder in the architecture used fully convolutional designs with specific parameters for training. The CelebA images were pre-processed by cropping to 140x140 and resizing to 64x64. The models were trained with up to 100 epochs and mini-batches of size 128, using Adam optimizer with a learning rate of 10^-3. The VAE had a KL term weight of 0.5, path loss weight of 0.5, and edge loss weight of 10^-3. Path and edge loss were estimated during training by sampling 10 paths with 10 steps each. The encoder and decoder in the architecture used fully convolutional designs with specific parameters for training. The VAE model was trained with up to 100 epochs and mini-batches of size 128, using Adam optimizer with a learning rate of 10^-3. Path-VAE interpolation optimizes probabilities between 2 and 7 using a Lagrangian approach. The construction utilizes consistent measures and shows existence through Kolmogorov-Daniell extension. The text discusses the construction of consistent measures and shows existence through Kolmogorov-Daniell extension, employing central extension results. It defines notational remarks and restriction projections on measurable spaces, emphasizing consistency in finite measures. The text discusses the construction of consistent measures using Kolmogorov-Daniell extension, emphasizing compatibility conditions for finite measures. It also mentions the construction of Wiener measure and Brownian motion through this method. Proposition 3 states the existence of a continuous-time stochastic process where X0 is within a small ball B\u03b4(x0) with high probability, given a small reconstruction error. The proof involves applying Theorem 1 to a collection of consistent finite measures, resulting in a measure \u00b5 on the measurable space (S[0,T], B[0,T]). This leads to the definition of a stochastic process X. The statement is further supported by the Theorem of Kolmogorov-Daniell, ensuring the required expression for P((Xt1, Xt2, ..., Xtn) \u2208 A). The text discusses the regularity of sample paths in a stochastic process, emphasizing the difficulty in guaranteeing their regularity. It suggests considering a \"smooth\" version of the curve by sampling it through a decoder to alleviate this issue. The text discusses the regularity of sample paths in a stochastic process, suggesting a \"smooth\" version of the curve to address irregularity issues. It briefly mentions a result by Kolmogorov and Chentsov regarding the regularity of stochastic processes. An estimate on E [\u03c1(X s , X t ) a ] can measure the failure of Theorem 2. The stochastic process in Proposition 1 is considered, with points X s , X s+\u03b4 for a small \u03b4. The expectation in (38) is written using Euclidean distance. Assuming a deterministic encoder and Gaussian decoder, the integral simplifies to involve the covariance matrix \u03a3 s . H\u00f6lder regularity is verified if \u03a3 s+\u03b4 becomes small as \u03b4 approaches 0. The variance of the decoder indicates the stochastic process's deviation from H\u00f6lder continuity. Another stochastic process construction is based on It\u00f4 diffusion processes with path-regularity properties. Lagrangian theory provides a framework for optimizing functionals using Euler-Lagrange equations. The critical points of the variational problem are determined by the condition (\u2207B)(l T |x(t)) = \u03b1\u1e8b(t), while stochastic paths minimize distances on the manifold in feature space induced by the auto-encoder pair. The Riemannian metric g on the decoder's image in the feature space is induced by the ambient Euclidean metric, with g represented by the matrix J T J in the deterministic case. Suitable approximations can be used to obtain g in the stochastic case. In the stochastic case, suitable approximations can be used to obtain the matrix g, which represents the Riemannian metric on the data submanifold. Geodesic curves naturally arise as minimizers of a distance functional, ensuring classifier probabilities change monotonously along paths. This behavior is enforced to provide examples following a specific trend along disputed labels. In the WAE framework, monotonic behavior along paths is enforced with specific terms, and reconstruction accuracy of endpoints is required. Unlike VAE, WAE only requires sampling from certain distributions and is trained by minimizing a Wasserstein distance between input data distribution and latent variable model. The WAE framework enforces monotonic behavior along paths and requires reconstruction accuracy of endpoints. It minimizes a Wasserstein distance between input data distribution P D (X) and latent variable model P \u03b8 (X), with a loss function involving a distance function c and a divergence term D Z weighted by \u03bb. Minimizing the loss corresponds to minimizing the Wasserstein distance if the decoder is deterministic, or yields an upper bound if stochastic."
}