{
    "title": "rJgWiaNtwH",
    "content": "One challenge in training generative models like VAE is avoiding posterior collapse, especially with high latent dimensions and small datasets. A new objective function using MMD instead of KL divergence, along with latent clipping, helps control distance in latent space. The $\\mu$-VAE model outperforms ELBO and $\\beta$-VAE on MNIST datasets, showing less posterior collapse and improved generation capabilities. The $\\mu$-VAE is effective in avoiding posterior collapse and can generate high-quality reconstructions and new samples. Latent representations learned by $\\mu$-VAE are suitable for downstream tasks like classification. Autoencoders can be transformed into generative models using adversarial or variational training methods. Variational Autoencoders (VAEs) maximize the variational lower bound of p \u03b8 (x) and consist of an Encoder for approximate inference. Variational Autoencoders (VAEs) consist of two networks: Encoder - Approximate inference network, and Decoder - Generative network. The encoder maps input representations onto latent variables, while the decoder reconstructs input and generates new data. During training, the encoder learns to map data distribution to a simple distribution like Gaussian, and the decoder maps it back to the data distribution. The Variational Autoencoder (VAE) learns to map data distribution to a simple distribution like Gaussian, and the decoder reconstructs input data. VAE's objective function includes a log-likelihood term and a prior regularization term, maximizing the evidence lower bound (ELBO). Maximizing ELBO increases the probability of generating observed data and reduces the distance between estimated posterior and prior distribution, leading to less informative latent variables. Pushing the KL term to zero can cause posterior collapse in the model. The proposed method aims to mitigate posterior collapse in Variational Autoencoders by modifying the KL term of the ELBO to encourage samples from the latent variable to spread out while keeping the aggregate mean close to zero. This allows for learning a latent representation amenable to clustering similar samples, enabling the model to learn good generative models and data representations. In recent years, proposals to address posterior collapse in Variational Autoencoders include modifying the ELBO objective through various methods like annealing the KL term, lower-bounding the KL term, or controlling KL capacity. Other approaches involve constraining the model structure to reduce decoder capacity. Proposals to address posterior collapse in Variational Autoencoders involve modifying the ELBO objective through methods like annealing the KL term, lower-bounding the KL term, or controlling KL capacity. Other approaches include constraining the model structure to reduce decoder capacity, such as adding skip connections or imposing constraints on encoder structure. Recent proposals like \u03b4-VAE and SKIP-VAE introduce new methods to prevent KL term collapse, but may require careful tuning of hyperparameters and use dropout to regularize the decoder, potentially affecting decoder effectiveness during training. The proposal includes an additional constraint on encoder structure, named the anti-causal encoder. SKIP-VAE proposes to lower bound mutual information by adding skip connections from latent layers to each layer of decoder. One drawback is the introduction of additional non-linear layers per hidden layer, resulting in more parameters to optimize. The advantage is not clear when increasing decoder capacity by adding more units or channels instead of layers. The goal of training a VAE model is to reconstruct input well and generate high-quality samples by extracting salient features and storing them in latent space. Reconstruction involves extracting key data features and storing them in the latent variable. Generating good samples requires a generative model with a distribution close to the actual data distribution. There is a trade-off between reconstruction quality and sample quality due to the ELBO objective function, which aims to maximize the probability of generating observed data while minimizing the difference between the latent code and the prior distribution. The KL loss term can be problematic as it makes the latent code less informative. The KL term in optimization minimizes the variance log, pushing mean dimensions to zero and variance towards 1, making the posterior less informative. Increasing latent variable dimension or using a coefficient \u03b2 > 1 worsens the problem. MMD can match sample means for different input samples, offering an advantage over KL divergence. Modifying the KL term by changing the L2 norm of the mean can emulate MMD behavior. The KL term is modified by changing the L2 norm of the mean to ensure the aggregate mean of samples is zero while allowing them to spread out. Clipping the L2 norm of means for each sample by a predetermined value during optimization prevents individual mean estimates from becoming too high. This method, known as latent clipping, has been found to work well in general, with clipping by three times the square root of the latent dimension being effective. Larger values may improve results in tasks like classification. The KL loss terms in the \u00b5-VAE objective function are adjusted to optimize for a subset of terms, with log \u03c3 2 chosen for simplicity. The objective function includes a reconstruction loss term and batch size considerations. Latent clipping is visualized using a toy VAE model on the MNIST dataset with different activation functions. The latent layer is visualized to observe clustering of digits in three cases: Standard VAE objective with KL divergence, Standard VAE objective with KL divergence + latent clipping, and \u00b5-VAE objective function + latent clipping. Latent clipping may improve smoothness of latent space, even with standard VAE objective. A CNN-based VAE model is designed and trained on MNIST and MNIST Fashion datasets using \u00b5-VAE objective function. No regularization methods are used to show the advantages of the new approach. The model is trained with four different objective functions: VAE, \u03b2-VAE, \u00b5-VAE#1, and \u00b5-VAE#2. A three-layer fully connected classifier is also trained on a 10-dimensional latent variable. Evaluation of the generative model is done qualitatively by inspecting sample quality and diversity. Classifier parameters are updated separately from encoder and decoder parameters to avoid influencing how information is encoded on the latent variable. The model is trained with different objective functions to assess posterior collapse and sample quality. KL divergence is used to measure the accuracy of the latent variable representation, with higher values indicating better representation. The new objective function shows lower reconstruction loss, higher KL divergence, and improved classification accuracy, suggesting improved learning. The \u03b2-VAE performs poorly due to its less informative latent code encouraged by the \u03b2 factor, resulting in worse reconstruction quality. In contrast, the \u00b5-VAE clusters similar samples together and can control the distance between samples using an upper-bound on \u00b5. This can lead to improved clustering results in classification tasks. The \u00b5-VAE outperforms in clustering similar samples together by controlling the distance between them with an upper-bound on \u00b5, leading to improved classification accuracy. Sample distributions from different VAE models show varying degrees of similarity to the prior distribution, with \u00b5-VAE resulting in zero mean but more spread out samples. The \u00b5-VAE models result in zero mean sample distributions that are more spread out, controlled by an upper-bound on \u00b5 sample. Some distributions exhibit multi-modal behavior, with \u00b5-VAE providing the best reconstruction quality compared to VAE and \u03b2-VAE. Images generated from \u00b5-VAE models may have dark spots as the model tries to add texture, similar to VAE but less pronounced. \u03b2-VAE samples do not show this characteristic. The \u00b5-VAE models have samples that are more spread out with zero mean distributions. They learn diverse classes of objects and exhibit multi-modal behavior. A new objective function is proposed to mitigate posterior collapse in VAEs, improving reconstruction quality. The proposed method, latent clipping, controls distance between samples in VAEs to improve reconstruction quality and learn informative latent codes. It is robust to parameter tuning and can be used as a replacement for ELBO objective. The method is shown to work well in downstream tasks like classification. The proposed method, latent clipping, improves reconstruction quality and learns informative latent codes in VAEs. It uses Adam optimizer with high momentum, mean square error for reconstruction loss, and Xavier initialization for convolutional layers. Objective functions and model architecture are detailed in tables. The model architecture for the experiments includes a CNN-based encoder and decoder with a three-layer fully connected neural network classifier. Leaky Relu activation and a learning rate of 1e-4 are used throughout."
}