{
    "title": "BJeHFlBYvB",
    "content": "We study the BERT language representation model and the sequence generation model with BERT encoder for multi-label text classification. Experimenting with both models, we introduce a mixed model that combines them. Our experiments show that BERT-based models, especially the mixed model, outperform current baselines on various metrics, achieving state-of-the-art results on multiple datasets. Multi-label text classification has diverse applications in natural language processing. In text classification tasks, deep learning approaches have shown promise, with sequence-to-sequence modeling being a less studied but effective method. Using Seq2Seq architecture with GRU encoder and attention-based GRU decoder has shown improvements over standard models. Yang et al. (2018b) introduced the Sequence Generation Model (SGM) with BiLSTM-based encoder and LSTM for multi-label text classification. The Sequence Generation Model (SGM) with BiLSTM-based encoder and LSTM decoder has been enhanced by replacing the encoder with BERT, resulting in the BERT+SGM model. This model achieves good results with minimal training compared to standard BERT, showing improvements in accuracy on public datasets and datasets with hierarchically structured classes. The study introduces BERT as an encoder in a sequence-to-sequence framework for MLTC datasets, presents a novel mixed model, and fine-tunes BERT for multi-label text classification. It achieves state-of-the-art results on various datasets in English and Russian texts. In multi-label classification tasks, the goal is to learn a function that maps inputs to subsets of a label set. Deep learning models like recurrent neural networks and convolutional neural networks are commonly used for text classification tasks, where raw text is converted to fixed-size vector representations for classification algorithms. Recent advancements in NLP have shown significant improvements in text embeddings and language representation models. Models like ELMo, ULMFiT, OpenAI GPT, and BERT have enhanced NLP tasks. XLNet and RoBERTa have further improved results by addressing limitations of BERT. Seq2Seq modeling is a novel approach that considers dependencies between labels, originally used in neural machine translation. The framework introduced in the neural machine translation field involves modeling the distribution P(Y|X, \u03b8), learning parameters \u03b8, and performing inference to find \u0176. The MLTC problem can be treated as a sequence-to-sequence task with ordered labels. The primary approach decomposes the joint probability P(Y|X, \u03b8) into separate conditional probabilities, traditionally following a left-to-right order decomposition. Hierarchical text classification (HTC) involves sorting labels based on descending frequencies or a tree hierarchy. This structure helps discover similar classes and transfer knowledge, improving model accuracy, especially for labels with few training examples. Recent research efforts have focused on hierarchical text classification (HTC) in computer vision applications, with potential applications in natural language texts. New approaches include a Graph-based CNN architecture with a hierarchical regularizer and combining outputs from global and local classifiers. Reinforcement learning models with special reward functions have also been explored. BERT (Bidirectional Encoder Representations from Transformers) is a language representation model gaining attention. BERT is a language representation model pre-trained on unlabelled texts for text embeddings. It uses a special token [CLS] for classification tasks and has shown success in transfer learning for multi-class and pairwise text classification. Fine-tuning the model with an additional feedforward layer and softmax activation function yields state-of-the-art results. In a multi-label setting, the activation function is changed to sigmoid for independent label probability predictions. In a multi-label setting, BERT model is fine-tuned for independent label probability predictions using binary cross-entropy loss. The encoder utilizes BiLSTM with pre-trained word embeddings, while the decoder employs the outputs of the last transformer block in BERT as word vector representations. A dot-product attention mechanism is used for calculating decoder's hidden states. The process involves calculating decoder's hidden states and attention scores using Algorithm 1 and Figure 1. BERT parameters are not frozen to allow for fine-tuning. The model is trained to minimize cross-entropy loss for sequence generation. In the inference stage, predicted labels are used. Beam search is employed to find candidate sequences with minimal objective scores. Experimental results compare standard BERT and sequence generating BERT models. BERT and BERT+SGM models have advantages and drawbacks on different datasets. Combining them can help alleviate weaknesses, as BERT sometimes predicts excess labels while BERT+SGM is more restrained. A hybrid approach can complement each other well by exploiting label structure information. Inspired by the HMCN model, a mixed model ensemble is proposed to optimize both local and global classifiers. The proposed mixed model ensemble combines multi-label BERT and sequence generating BERT models to address the challenge of different model outputs. By computing probability distributions from the decoder and performing max-pooling, meaningful label sets can be obtained, although predictions may differ from the original BERT + SGM model. The proposed mixed model ensemble combines multi-label BERT and sequence generating BERT models to address the challenge of different model outputs. Table 1 summarizes the datasets, including the number of documents, labels, average words per sample, and average labels per sample. By obtaining probability distributions from both models and computing their weighted average, a final probability distribution vector is created for making label predictions with a 0.5 probability threshold. The trade-off parameter \u03b1 is optimized on the validation set, and the final procedure is presented in Algorithm 2. The study trained and evaluated models on public English datasets and private Russian datasets. Preprocessing involved lowercasing texts and removing punctuation. The RCV1-v2 dataset consists of 804,410 news stories categorized into 103 labels. Labels were sorted from most common to least common. In the multi-path case, labels were sorted from most common to rarest. The original training/testing split had 23,149 samples in training and 781,261 in testing. Recent research shifted towards using 802,414 samples for training and 1,000 for validation/testing. Maintaining the original split is important for consistent comparison of models. The dataset used for training/testing split includes Reuters-21578 with 10,787 articles and Arxiv Academic Paper Dataset with 55,840 research papers. Y.Taxi Riders dataset consists of 174,590 tickets from Yandex Taxi client support system. The Yandex Taxi client support system dataset contains 174,590 tickets labeled with 426 labels using a tree hierarchical structure. A similar dataset from Yandex Taxi drivers support system includes 163,633 tickets. In the drivers' version of the dataset, there are 163,633 tickets labeled with 374 tags. Experiments were conducted using PyTorch 1.0 on a GeForce GTX 1080Ti GPU. BERT models were used for English and Russian texts, outputting 768-dimensional hidden representation vectors. Adam optimizer with specific parameters was used for optimization. Results on five datasets are shown in Reuters Table 2. The dataset metrics were evaluated using various accuracy and f1 metrics. TextCNN and SGM models were used as baselines for the experiments. The study compared different models for hierarchical text classification on multiple datasets. BERT and BERT+SGM showed superior performance, especially on the RCV1-v2 dataset, outperforming other baselines significantly. The methods that considered class structure also had favorable results. The class structure and label frequency in BERT+SGM have the highest macro-F1 score. BERT sometimes outperforms the sequence-to-sequence version, especially on the Reuters-21578 dataset. BERT+SGM may have fewer samples, leading to better performance in some cases. On the RCV1-v2 dataset, BERT+SGM has a much larger macro-F1 score. The BERT+SGM model shows higher hamming accuracy and set accuracy on Yandex Taxi datasets in Russian. Mixing BERT and BERT+SGM can lead to better performance, with an average improvement of 0.4%, 0.8%, and 1.6% in miF1, maF1 on public datasets. On public datasets, BERT+SGM shows average improvements of 0.4% in miF1, 0.8% in maF1, and 1.6% in accuracy compared to BERT. For datasets with tree hierarchy, there are average improvements of 2.8% in maF1 and 1.5% in accuracy. BERT for multi-label text classification tasks requires more epochs to converge compared to BERT+SGM, which achieves decent accuracy faster. During training, both BERT and sequence generating BERT models showed similar performance on the validation set of Reuters-21578. The beam size in the inference stage had minimal impact on performance, with optimal results achieved with a beam size between 5 to 9. Even a greedy approach with a beam size of 1 yielded comparable results. This could be due to the simplicity of the label set generation task and the limited vocabulary size, which restricts the options for beam search. In this research, BERT and sequence generating BERT were examined on a multi-label setting. A mixed model combining vanilla BERT and sequence-to-sequence BERT outperformed current baselines, achieving state-of-the-art results on multiple datasets. Multi-label BERT required several dozen epochs to converge, while the mixed model showed decent results after just a few hundred iterations. The success of the mixed model may be attributed to the different views on text from the two models. The success of the mixed model combining vanilla BERT and sequence-to-sequence BERT in a multi-label setting may be due to their different views on text features. Visualization of word importance scores shows BERT+SGM is more selective, but sequence generating BERT has more accurate predictions. The models were trained on AAPD dataset and applied to BERT paper, achieving state-of-the-art results. Visualization of label clusters from multi-label BERT fine-tuned on AAPD dataset is shown in Figure 6, indicating close clusters of labels based on word similarity."
}