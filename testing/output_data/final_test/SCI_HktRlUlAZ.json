{
    "title": "HktRlUlAZ",
    "content": "Convolutional neural networks (CNNs) are inherently equivariant to translation. The Polar Transformer Network (PTN) expands equivariance in CNNs by incorporating rotation and scale equivariance. PTN achieves state-of-the-art results on rotated MNIST and SIM2MNIST datasets. The ideas of PTN are extendable to 3D with the Cylindrical Transformer Network. The quest for (in/equi)variant representations in computer vision has a long history. State-of-the-art approaches like SIFT produce equivariant descriptors for scale and rotation invariance. Steerable filtering allows for transformed filter responses to be interpolated from a finite number of filter responses, proven for rotations of Gaussian derivatives and extended to scale and translations. The use of the orbit and SVD to create a filter basis was proposed, alongside advancements in achieving equivariance. The quest for (in/equi)variant representations in computer vision has a long history. State-of-the-art approaches like SIFT produce equivariant descriptors for scale and rotation invariance. Steerable filtering allows for transformed filter responses to be interpolated from a finite number of filter responses, proven for rotations of Gaussian derivatives and extended to scale and translations. Recent advancements include the proposal of using the orbit and SVD to create a filter basis, as well as the development of methodologies for computing the bases of equivariant spaces. The scattering transform has also been introduced, offering representations invariant to translation, scaling, and rotations. The consensus now is that representations should be learned rather than designed, with approaches to equivariance of more general deformations still evolving. Main veins of research include the Spatial Transformer Network (STN) which learns a canonical pose and produces an invariant representation through warping. The Polar Transformer Network (PTN) combines ideas from STN and canonical coordinate representations to achieve equivariance to translations, rotations, and dilations. The network identifies the object center and transforms the input into logpolar coordinates, where planar convolutions correspond to group-convolutions in rotation and scale. PTN produces a representation equivariant to rotations and dilations. The Polar Transformer Network (PTN) extends the concept of equivariance in CNNs by incorporating rotations and dilations of arbitrary precision. It achieves state-of-the-art performance on rotated MNIST and SIM2MNIST datasets. The PTN architecture learns image representations that are invariant to translation and equivariant to rotation and dilation through the use of a polar transformer module. The Polar Transformer Network (PTN) utilizes a differentiable log-polar transform for backpropagation training. The transform origin is learned as the centroid of a single channel heatmap predicted by a convolutional network. Various equivariant feature extraction schemes have been proposed in the past, including the Fourier-Mellin transform which is invariant to rotation and scale. Invariances of integral transforms have been developed through methods based on Lie generators. The work discusses steerability in response interpolation to group actions, including rotational steerability for Gaussian derivatives and the shiftable pyramid for rotation and scale. A method for approximating steerability by learning a lower dimensional representation of image deformation was proposed. Recent advancements combined steerability with learnable filters, while the scattering transform composed rotated and dilated wavelets for equivariant representations. The approach discussed in the current text chunk involves transforming BID30 with rotated and dilated wavelets to achieve translation invariance and local rotations and scalings equivariance. Methods for enforcing equivariance in CNNs fall into two main categories: constraining filter structure like Lie generator based approaches BID29 and BID10, or using filters derived from complex harmonics like Harmonic Networks BID36. The approach discussed involves achieving rotational and translational equivariance using filter orbits. Various methods like BID3, BID4, BID7, BID38, BID21, and BID18 are used to produce oriented feature maps and rotationally invariant features. The PTN approach expands CNN equivariance to include scaling by employing log-polar coordinates for rotation-dilation. PTN employs log-polar coordinates for rotation-dilation group-convolution, achieving equivariance similar to BID11. In 3D object classification, representations equivariant to rotations are learned by transforming input to cylindrical coordinates. In 3D object classification, representations equivariant to rotations are learned by transforming input to cylindrical coordinates about a predicted axis. This section discusses equivariance and group-convolutions, using the 2D similarity transformations group, SIM(2), for translational convolution. Equivariant representations encode class and deformation information predictably, with invariance as a special case. In the context of image classification and CNNs, equivariance to image deformations can be achieved through group-convolutions. The inherent translational equivariance of CNNs is independent of the convolutional kernel. Group-convolution requires integrability over a group and identification of the appropriate measure dg. Group-convolution in CNNs requires integrability over a group and the appropriate measure dg. Equivariance to SIM FORMULA2 is achieved by learning the center of the dilated rotation and transforming the image to canonical coordinates. The standard translational convolution is equivalent to the dilated-rotation group-convolution. The origin predictor is an application of STN to global translation prediction. The application of STN to global translation prediction involves transforming the image to canonical coordinates and performing group-convolutions in SO(2) \u00d7 R +. The convolution is computed using canonical coordinates for Abelian Lie-groups, resulting in an equivariant output. Rotational group-convolution with an arbitrary filter results in an equivariant representation. Filter response is shifted by 90\u00b0. Group-convolutions in SO(2) \u00d7 R + show a shift corresponding to input image deformation. The dilated-rotation equivariant representation responds to input deformation. The network constructs a translational network using planar convolution. The network constructs a translational network using planar convolution. It consists of two main components: the polar origin predictor and the classifier. The building block of the network is a 3 \u00d7 3 \u00d7 K convolutional layer followed by batch normalization, ReLU, and occasional subsampling through strided convolution. The polar origin predictor operates on the original image and comprises a sequence of blocks followed by a 1 \u00d7 1 convolution. The network uses a fully convolutional network with a polar origin predictor to output a single channel feature map. Training neural networks to predict coordinates in images can be challenging, with some approaches using heatmaps to improve accuracy. The network architecture includes rectifier and pooling layers to preserve equivariance. The input image is processed by a fully convolutional network called the polar origin predictor, which generates a heatmap. The centroid of the heatmap, along with the input image, is fed into the polar transformer module for a polar transform. This transformation makes the object location invariant, treating rotations and dilations as shifts. The approach avoids the argmax gradient problem by supervising the gradient of the output coordinates with respect to the heatmap. The centroid of the heatmap serves as the polar origin, ensuring a constant and nonzero gradient for all points. The polar transformer module uses a constant and nonzero gradient for learning, transforming the origin prediction and image into logpolar representation. It employs differentiable image sampling techniques to express output coordinates in terms of input and sample point coordinates. The log-polar transform maintains feature map resolution and avoids zero-padding issues by being periodic about the angular axis. The log-polar transform maintains feature map resolution by being periodic about the angular axis. Wrap-around padding is used on the vertical dimension, with zero-padding on the horizontal dimension. To improve robustness, a random shift is added to the regressed polar origin coordinates during training. Performance gains of this augmentation are quantified in TAB5. Different network architectures are briefly defined, with CCNN being a conventional fully convolutional network, PCNN applied to polar images, and STN being their method. PTN is our polar transformer networks, and PTN-CNN is a combination of PTN and CCNN. Rotation augmentation is performed for polar-based methods to account for interpolation and angle discretization effects. Results are shown in TAB1, comparing smaller networks without rotation augmentation to those without restrictions. The Harmonic Network BID36 outperforms the PTN with almost 4x more training time due to costly convolutions on complex variables. STN struggles without augmentation, while PTN-B variants excel when combined with CCNN and test time augmentation. PCNN achieves high accuracy due to centered digits, but our method surpasses it by a large margin, showing the effectiveness of finding an origin away from the image center. In experiments with various MNIST variants, including SIM2MNIST, different methods are compared. PTN performance matches STN on MNIST R and RTS, but outperforms on SIM2MNIST due to more challenging deformations and smaller training set. HNet performs best overall, emphasizing the importance of predicting the best polar origin. Our method, more efficient in parameters and training time, is equivariant to dilations and achieves superior performance. No augmentation is used with SIM2MNIST. We visualize network activations to confirm invariance to translation and equivariance to rotations and dilations. The network learns to reject clutter and find a suitable origin for the polar transform, with the representation presenting the claimed properties. Visualizations confirm preservation of properties in deeper layers. The study confirms the preservation of properties like equivariance to rotations and dilations in deeper layers. The model is extended to 3D object classification from voxel occupancy grids, utilizing random rotations for transformation. The prediction of an axis aids in achieving equivariance to rotations. The cylindrical transform involves channel-wise polar transforms on a 3D voxel grid. Anisotropic probing is used to predict the axis, with 2D convolutional layers collapsing to a single heatmap centroid. A 3D CNN classifier operates on the cylindrical representation, which is equivariant to input rotations. The cylindrical coordinates representation in the novel network is equivariant to input rotations. Experimental results on ModelNet40 show superior classification performance compared to voxel-based methods. The network's output is invariant to translations. The proposed novel network is invariant to translations and equivariant to dilations/rotations. It combines the idea of learning translation with equivariance for scaling and rotation, avoiding fully connected layers. Equivariance to dilated rotations is achieved through convolution in a group, improving performance on rotated MNIST and a new dataset called SIM2MNIST. The approach is expected to be applicable to problems with different orientations and scales that hinder conventional CNN performance. The proposed novel network is invariant to translations and equivariant to dilations/rotations, improving performance on rotated MNIST and SIM2MNIST datasets. Different architectures are implemented for comparison, including Conventional CNN (CCNN), Polar CNN (PCNN), Spatial Transformer Network (STN), and Polar Transformer Network (PTN). PTN is the proposed method with a polar origin predictor comprising three blocks of 20 filters each. The network architecture includes small and big networks with different numbers of filters and blocks. Training and test time rotation augmentation is denoted by + and ++ respectively. Additional blocks are added at the beginning to handle larger input resolutions. The cylindrical transformer network includes axis prediction with 2D and 3D convolutional blocks. The rotated MNIST dataset consists of 360\u00b0 rotated images of handwritten digits. MNIST R and MNIST RTS are variations of the dataset with rotated digits. The dataset includes rotated versions of MNIST digits with various transformations such as scaling and shifting. Another challenging dataset, SIM2MNIST, perturbs MNIST images with random transformations. Additionally, experiments are conducted on the Street View House Numbers dataset and a rotated version (ROTSVHN). The study uses a ResNet32 model to analyze rotated digits in house numbers, specifically focusing on the impact of rotations on performance. Results show that the PTN method is more robust to perturbations compared to conventional ResNet, with lower error rates even when removing certain classes from the dataset. The PTN method is more robust to perturbations and shows lower error rates compared to conventional ResNet. Performance boost is quantified with various augmentations, and results are based on the PTN-B variant trained on Rotated MNIST. Removing any operation leads to a consistent drop in performance, indicating the importance of all operations. TAB5 displays the results."
}