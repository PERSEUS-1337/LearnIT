{
    "title": "B1eZxbU9DE",
    "content": "The proposed framework aims to incrementally learn a PPDDL representation of the domain model using experiences from current planning problems. It introduces the concept of reliability as intrinsic motivation for reinforcement learning to improve learning efficiency and goal-directedness. Experimental results in three planning domains validate the approach. In this work, an incremental learning model (ILM) is presented to learn action models incrementally over planning problems using reinforcement learning. The model addresses the challenge of acquiring training data through acting or executing actions, especially in nonstationary domains where distributions of stochastic dynamics shift over time. The ILM aims to improve learning efficiency and goal-directedness by incrementally learning a PPDDL representation of the domain model. The incremental learning model (ILM) presented in this work focuses on learning action models incrementally over planning problems using reinforcement learning. PPDDL is used for planning, while a rules-based representation is used for the learning process. Prior action models, either learned or hand-coded, are provided to subsequent planning problems to bias learning and reduce exploration. Empirical estimates of learning progress guide the process. The incremental learning model (ILM) focuses on learning action models incrementally using reinforcement learning. It uses empirical estimates to guide the search for models and as intrinsic motivation in reinforcement learning. The model can learn from failure by recording failed executions and preventing similar attempts, increasing exploration efficiency. The paper discusses the Incremental Learning Model (ILM) for learning action models incrementally using reinforcement learning. It extends the rules learner from BID15 to represent probabilistic actions with a relational representation, allowing generalization of the state space. The training data consists of state transitions, and the model is evaluated in three planning domains. Our work focuses on learning probabilistic actions incrementally, revising relational rules representing actions based on contradicting examples. Unlike other approaches, we only use current training data and can incorporate prior knowledge with incomplete action models. This is related to model-based reinforcement learning, such as R-MAX, which efficiently balances exploration and exploitation. The text discusses the R-MAX approach for balancing exploration and exploitation in planning problems with large state spaces. Additional assumptions are often made to make R-MAX practical, such as known structures of dynamic Bayesian networks. Reliability is used as intrinsic motivation and to quantify prior knowledge in learning progress. The text discusses using rules-based representation for learning action models in PPDDL. Rules consist of the action name, precondition, and effect, with the key difference being the addition of noise effects to avoid rare effects and simplify planning. In PPDDL, rules are used to represent action models with noise effects to simplify planning. Rules cover state-action pairs and propositional rules are obtained by grounding relational rules. MDPs model fully observable problems with uncertainty using a tuple format. The objective is to find a policy that maximizes expected rewards in Reinforcement Learning. Model-based reinforcement learning is used when transition functions in MDPs are unknown. Exploration and exploitation are balanced in reinforcement learning. A new approach called ILM for incremental learning across planning problems is proposed, consisting of a rules learner and a reinforcement learning framework. The concept of reliability is introduced in both components. The reliability of learned action models is crucial in reinforcement learning. It is defined by various parameters and is updated with each action execution. The framework extends the rules learner from BID15 to consider the prior action model and its reliability when learning new rules. In exploration, less reliable actions are preferred. The reliability of the prior action model is determined by a discount factor \u03b3, which decreases its importance with new data. Success Rate is defined as the likelihood of recent executions being successful, with a high success rate indicating small errors. Volatility measures how much a set of rules representing an action changes after learning, with low volatility suggesting convergence to the true action model. The volatility of a set of rules after learning is computed recursively, indicating convergence to the true action model. It is defined by the normalized difference between two sets of rules, where the difference is calculated based on the number of predicates in each rule. Exposure measures the variability of pre-states in training data by calculating the average pairwise difference weighted by the number of state transitions. It considers probabilities of effects inferred from successful state transitions, preventing double-counting of unique pre-states. The rules learner from BID15 applies a search operator to modify rules, yielding new rules using a score function as heuristics. A deviation penalty is introduced to refine action models, preventing scenarios where reliability does not reflect learning progress. The score function considers probabilities of effects from state transitions, with parameters like exposure and training data. The deviation penalty in the rules learner model penalizes complex rules to prevent over-specialization. It increases when rules deviate further from the original rules. The penalty is scaled by the reliability of the prior action model and the inverse of exposure, aiming to limit deviation if the prior model is reliable. The deviation penalty in the rules learner model penalizes complex rules to prevent over-specialization, scaling with the reliability of the prior action model and the inverse of exposure. The main algorithm for ILM involves learning, planning, and acting, with exploration and exploitation at each iteration. If an action fails, ILM learns from the failure by recording it in tabu. The algorithm for ILM involves learning, planning, and acting, with exploration and exploitation at each iteration. Failed instances are recorded in tabu to prevent over-specialization. Synthetic state transitions are generated to augment training data and aid in learning preconditions. Learning is delayed until certain criteria are met to ensure correctness of learned rules. The algorithm for ILM involves learning, planning, and acting with exploration and exploitation. Learning new rules and updating values are based on successful and failed transitions. The algorithm terminates when the goal is reached or after a maximum number of iterations. The balance between exploration and exploitation is implemented in EE(s, g, R, RE, tabu, \u03b6) by computing counts for applicable actions using a context-based density formula. Relational generalizations in ILM involve reducing exploration as states unknown in propositional representations become known in relational representations. Count-action pairs are sorted by reliability, with less reliable actions explored more. A state is known if counts exceed a threshold or if all action reliabilities surpass a constant. Exploitation is done using Gourmand if the state is known, otherwise exploration is attempted. The plan involves exploration if the state is unknown or exploitation fails. Grounded actions are enumerated and a random one not in tabu is selected. Failed executions are recorded in tabu. Algorithm 2 checks if an action is in tabu before execution. The plan involves exploration if the state is unknown or exploitation fails. Grounded actions are enumerated and a random one not in tabu is selected. Failed executions are recorded in tabu. Algorithm 2 checks if an action is in tabu before execution. In the example, moveCar(l31, l13) is in tabu, as are all grounded actions of moveCar. The tabu list contains failed executions to prevent repeated actions. The algorithm checks if an action is in tabu before execution to ensure completeness. If tabu is empty, the algorithm is incomplete. Only correct actions are added to tabu, making the algorithm sound. In experiments, planning problems in different domains were attempted sequentially with increasing scale. The machine used had a four-core Intel i5-6500 processor. Three planning domains were used: Tireworld, Exploding Blocksworld, and Logistics. In Tireworld, a car may get a flat tire when moving, leading to a dead end. In Tireworld, a flat tire can lead to a dead end if no spare tires are available. In Exploding Blocksworld, blocks may detonate, rendering them inaccessible. Logistics problems involve trucks, airplanes, and parcels with potential loading failures. The models for all domains are stationary with constant action probabilities. Performance evaluation includes correctness of the learned model and goal-directedness. Comparisons are made with R-MAX and two variants of ILM. ILM, ILM-R, and ILM-T are compared with R-MAX in terms of goal-directedness and model correctness. ILM-R does not use reliability or delay learning, while ILM-T does not learn from failure. Variational distances for different domains are shown in Figure 2. ILM learns action models incrementally, with variational distances decreasing from rounds 1 to 10. ILM-R performs slightly worse due to training data with low variability, causing variational distances to increase in rounds 4 and 6. Learning from failure is crucial, as shown by larger variational distances for ILM-T and R-MAX. Table 3 displays the average number of successful trials for different domains. In the domain of Exploding Blocksworld, R-MAX outperforms ILM after four rounds of training due to learning from a larger training set. The complex actions in this domain require more training data to learn the preconditions effectively. In contrast, in the Logistics domain, ILM consistently performs the best in all rounds. ILM had the best performance in all rounds, with large variational distances for ILM-T due to difficulty in learning driveTruck. The goal-directedness is evaluated by the number of successful trials, shown in Table 3 for three domains. Prior knowledge in rounds 2 and 3 led to more successful trials, as ILM exploits learned models from previous rounds. ILM-R outperforms ILM in rounds 1 to 3 by exploiting learned models from previous rounds. The goal state can be reached by executing moveCar repeatedly as long as the tire is not flat along the way. Exploration or exploitation may not make a significant difference in small-scale planning problems, but as the scale increases, the probability of getting a flat tire along the way also increases. In small-scale planning problems, exploration or exploitation may not make a significant difference, but as the scale increases, the probability of getting a flat tire along the way is higher. ILM outperforms ILM-R in rounds 4 to 10, while ILM-T and R-MAX did not perform well. Dead-ends are often caused by failing to reach the goal state, with irreversible changes leading to dead-ends. ILM had the most successful trials in all rounds, while R-MAX did not learn from failure. Figure 5 compares the number of successful actions in each round using ILM and ILM-T, with ILM-T showing significantly fewer successful executions. The frequency of exploration decreases over rounds while exploitation increases, as action models are learned incrementally. The use of tabu in ILM is shown in FIG3, with a decline in entries added after round 1. The number of successful trials increases even with larger planning problems. In small-scale planning problems, driveTruck repeatedly failed to execute successfully until round 3, with only two out of 432 grounded actions succeeding. This led to a subset of the state space not being reached, affecting the execution of loadTruck, unloadTruck, loadAirplane, and unloadAirplane actions. The action model for driveTruck was learned from a partially successful execution in round 3, as shown in Figure 8. The action model for driveTruck in the Logistics domain had an extraneous precondition causing it to be erroneously inapplicable in some parts of the state space. This led to loadTruck and unloadTruck actions not being attempted due to being in tabu. Learning extraneous predicates for preconditions can have adverse impacts on the execution of actions. The new measure reliability influences learning and planning by reducing exploration for reliable action models. ILM learns from failure by checking state-action pairs for failed executions. Experimental results show improved correctness and goal-directedness with reduced failed executions. More training data is needed for complex domains, as past training data is not effective for non-stationary domains. To address issues with learning action models in non-stationary domains, it is suggested to learn distributions from current training data only and maintain a fixed size of training data by replacing older experiences. This approach aims to maximize exposure and variability of the training data for better performance."
}