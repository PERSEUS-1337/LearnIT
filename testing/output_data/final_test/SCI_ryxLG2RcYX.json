{
    "title": "ryxLG2RcYX",
    "content": "In high-dimensional reinforcement learning, exploring sparse rewards is a challenge. Model-based approaches aim to improve exploration through planning but learning a reliable MDP in high dimensions is difficult. The proposed solution involves learning an abstract MDP with fewer states for effective exploration. A manager maintains the abstract MDP over a subset of states while a worker policy navigates between abstract states. Our approach outperforms the previous state-of-the-art by over a factor of 2 in three of the hardest games from the Arcade Learning Environment: Montezuma's, Pitfall!, and Private Eye. In this paper, the exploration problem in high-dimensional, sparse-reward reinforcement learning tasks is addressed by investigating model-based reinforcement learning as a potential solution. The difficulty lies in effective exploration without demonstrations, as even state-of-the-art intrinsically-motivated RL agents achieve only a fraction of an expert human's score. Model-based reinforcement learning is considered as a solution to the exploration problem in high-dimensional settings. However, imperfect models can lead to errors compounding over time, making planning over many steps challenging. Even with a perfect model, computing the optimal policy in high-dimensional state spaces is intractable. Prior work has focused on learning more accurate models and combining model-based with model-free approaches to address these challenges. Instead of directly learning a model over the concrete state space, a new approach inspired by hierarchical reinforcement learning (HRL) is proposed. This approach involves abstract states represented by a white grid superimposed on the original game, with the agent navigating transitions between these abstract states following a plan made by the manager. The manager guides the agent to the fringe of the known set and then explores new transitions near that area. The worker extends the abstract MDP by learning to navigate to newly discovered abstract states. The abstract MDP is accurate and near deterministic on the known set for efficient planning. In our implementation, the worker policy learns to transition between abstract states efficiently by assigning skills driven by deep neural networks. This approach ensures monotonic progress in learning the abstract MDP, allowing for efficient exploration via planning. This method differs from previous HRL work by simplifying the planning problem for the manager and avoiding compounding errors. Our approach in HRL work differs from previous methods by learning skills and operating on latent abstract state spaces without forming an MDP. We achieve significant improvements in challenging games like MONTEZUMA'S REVENGE, PITFALL!, and PRIVATE EYE, surpassing prior state-of-the-art approaches. Our model-based approach allows for generalization to new rewards without re-training, achieving superior performance even on unseen reward functions. In MONTEZUMA'S REVENGE, the reward depends on abstract states, not concrete states. Model-based approaches struggle in high-dimensional spaces, so we propose operating on a low-dimensional abstract MDP instead of the concrete MDP. In MONTEZUMA'S REVENGE, the reward depends on abstract states, not concrete states. To address the challenges in high-dimensional spaces, we propose planning in an abstract MDP, which consists of a known set of abstract states and a worker policy for navigating transitions between these states. The worker abstracts away the complexity of the concrete MDP, allowing other parts of the system to operate on the abstract MDP. The core idea is to construct the abstract MDP by training the worker to grow the action set and known set. The manager maintains estimates of reward and transition dynamics to solve the abstract MDP efficiently. The abstract MDP grows capturing more of the concrete MDP, enabling the manager to recover a better policy. The action set consists of reliable actions transitioning with high probability. The manager estimates success rates to simplify notation. The abstract MDP grows by adding new actions for the worker to learn reliable transitions, aiming to construct the abstract MDP with all abstract states. The worker learns skills for each transition, freezing parameters once reliably traversed, to compute a high-reward policy on the concrete MDP. The manager in the MDP trains the worker to navigate new transitions successfully. It involves discovering and training on new transitions, updating dynamics estimates, and choosing between exploration and training. The process includes constructing a list of exploration goals, learning transitions, and exploring abstract states to find nearby transitions. The manager in the MDP navigates to abstract states by planning with dynamics models, executing plans, and updating models based on worker attempts. It adds successful transitions to the action set and known set for training the worker on new transitions. The manager navigates to abstract states using dynamics models, executes plans, and updates models based on worker attempts. It discovers new transitions through randomized exploration of nearby abstract states with fewer than N visit times. The policy \u03c0 d outputs randomized concrete actions conditioned on past states and actions for T d timesteps. The manager navigates abstract states using dynamics models, executes plans, and updates models based on worker attempts. It discovers new transitions through randomized exploration of nearby abstract states with fewer than N visit times. The policy \u03c0 d outputs randomized concrete actions conditioned on past states and actions for T timesteps, recording transitions and rewards to update its models. If exploration continues after T timesteps, a new exploration direction is chosen. The manager selects exploration goals from candidate exploration goals, including exploration candidates for transition discovery and candidate transitions for the worker to learn. The worker imposes a heuristic on its transition learning process to maintain the Markov property of the abstract MDP. The manager also considers \"long-distance\" transitions to prevent getting stuck. The manager prioritizes exploration goals based on priority scores, choosing goals in a specific order to facilitate the worker's learning process. The manager considers all pairs of states to determine the shortest path for transition discovery. The manager heuristically computes the easiness of learning transitions based on the worker's success and failure rates. Additionally, the manager prioritizes exploration goals to facilitate the worker's learning process. The manager prioritizes exploration goals by computing the usefulness of learning transitions for the worker. This is done by considering the reward achieved and the potential for new candidate transitions in abstract states. The worker forms an action set by learning subtasks to navigate between abstract states. The worker forms an action set by learning subtasks to navigate between abstract states. The worker maintains an inventory of skills, where each transition is learned by a single skill to preserve the Markov property. The worker uses these skills to form the action set of the abstract MDP. The worker selects skills from the inventory to traverse transitions in the abstract MDP. If the success rate exceeds a threshold, the action is added to the MDP. Each skill is a goal-conditioned subpolicy that produces actions based on the current state and goal state. The worker selects skills from the inventory to traverse transitions in the abstract MDP. If the success rate exceeds a threshold, the action is added to the MDP. Each skill is a goal-conditioned subpolicy that produces actions based on the current state and goal state. When learning a new transition, the worker tries to reuse already learned skills from the inventory. If no skill can reliably traverse the new transition, a new skill is created and trained to navigate the transition by optimizing intrinsic rewards. The worker must carefully traverse transitions in the abstract MDP, ensuring not to violate the Markov property due to lost history-dependent information. For example, in the task of jumping over a hole, incorrect assumptions may lead to unreliable traversal. The worker navigates transitions in the abstract MDP, avoiding violations of the Markov property by using the holding heuristic to check for history-dependent consequences. If the worker can stay in a state for many timesteps without significant changes, it declares success. The worker uses a holding heuristic to navigate transitions in the abstract MDP, declaring success if it accumulates a certain reward. Skills are trained using Dueling DDQN and self-imitation for faster learning. The sample complexity is of interest for learning a policy close to optimal with high probability. Our approach can learn a near-optimal policy on a subclass of MDPs in time and space polynomial in the size of the abstract MDP, by operating on the abstract MDP with a neural network policy class rich enough to represent all necessary skills. Our approach learns subtasks with a time horizon for navigating abstract states. Empirically evaluated on challenging games like MONTEZUMA'S REVENGE, PITFALL!, and PRIVATE EYE. Experiments use standard ALE setup, ending episodes on agent life loss. Rewards reported every 4000 episodes, averaged over 4 seeds with 1 standard deviation error bars in training curves. Our experiments on challenging games like MONTEZUMA'S REVENGE, PITFALL!, and PRIVATE EYE use the same hyperparameters tuned on MONTEZUMA'S REVENGE. The state abstraction function extracts the agent's location and inventory from the RAM state. The abstract state function does not specify the meaning of each part, and the agent does not know the entire abstract state space beforehand. In MONTEZUMA'S REVENGE, we compare with SmartHash BID44, a count-based exploration approach using RAM state information. In PITFALL!, we compare with SOORL BID18, a planning approach requiring extensive engineering to extract objects on the screen. In PRIVATE EYE, SOORL learns quickly by pooling data from similar objects. The final average and best performance over 100 runs are reported. Comparison is made with DQNPixelCNN for count-based exploration. AbstractStateHash is also compared, showing improvement over prior state-of-the-art results in MONTEZUMA'S REVENGE. Our approach outperforms the prior best on MONTEZUMA'S REVENGE and PITFALL! and matches the prior state-of-the-art on PRIVATE EYE. AbstractStateHash shows improvement in MONTEZUMA'S REVENGE but performs poorly in PRIVATE EYE and PITFALL!. Our method achieves a final average reward of 11020 in MONTEZUMA'S REVENGE after 2B training frames, surpassing SmartHash's average reward of 5001. Our approach achieves superhuman performance on PIT-FALL! with a final average reward of 9959.6 after 2B frames of training, outperforming SOORL and Ape-X DQfD. In PRIVATE EYE, our approach achieves a mean reward of 35636.1, more than double the reward of DQN-PixelCNN. Our results demonstrate stability and outperformance compared to prior state-of-the-art approaches in deep RL. Even our worst seed surpasses the mean performance of previous methods and achieves competitive rewards in various games. Our best seeds set new peak performances in each game, showcasing the effectiveness of our approach using the abstract MDP. Our approach demonstrates stability and outperformance in deep RL compared to prior state-of-the-art methods. It sets new peak performances in each game by using the abstract MDP to quickly generalize to new tasks in the same environment. After training on the original reward function in MONTEZUMA'S REVENGE, our approach outperforms SmartHash on unseen reward functions, achieving about 3x as much reward. This showcases our approach's ability to generalize to new tasks with the same dynamics. Our approach demonstrates stability and outperformance in deep RL compared to prior methods. It sets new peak performances in each game by quickly generalizing to new tasks in the same environment. Performance degrades slightly on the stochastic version of PRIVATE EYE, but both versions outperform the prior state-of-the-art method. The worker successfully abstracts away stochasticity from the manager in the game. Our state abstraction function buckets the agent's coordinates, varying in coarseness by adjusting the bucket size. Results show our method outperforms the prior state-of-the-art approach across different bucket sizes, indicating it does not require a highly tuned state abstraction function. Optimism in the face of uncertainty (OFU) methods achieve near-optimal policies by providing reward bonuses for exploring uncertainty. However, these methods do not scale well to deep RL settings with large state spaces. Other methods apply optimism reward bonuses in high-dimensional state spaces but may not guarantee optimality. Model-based RL succeeds in tabular settings and small state spaces but struggles with exponentially large state spaces. Our work addresses the challenges of learning models in exponentially large state spaces by creating an abstract MDP with a smaller state space and learned skills. Unlike prior works using manually engineered skills, our approach learns skills. This work is related to hierarchical reinforcement learning (HRL), which also operates on abstract states with learned skills or subgoals. Our work constructs an abstract MDP to enable targeted exploration via planning and avoid large state histories. Unlike other works, our approach outperforms by saving worker parameters as transitions become reliable, preventing catastrophic forgetting and allowing growth of the abstract MDP. Our approach constructs an abstract MDP to facilitate targeted exploration and prevent catastrophic forgetting, enabling growth of the abstract MDP. It addresses long-horizon, sparse-reward tasks by reducing state space dimensionality and handling model errors. The framework performs well in hard exploration tasks and guarantees near-optimality, but relies on prior knowledge in state abstraction function. Our approach constructs an abstract MDP to facilitate targeted exploration and prevent catastrophic forgetting, enabling growth of the abstract MDP. It addresses long-horizon, sparse-reward tasks by reducing state space dimensionality and handling model errors. The framework performs well in hard exploration tasks and guarantees near-optimality, but relies on prior knowledge in state abstraction function. Experiments involve downsampling and cropping pixel concrete states to 84 by 84, converting them to grayscale, and stacking past four frames for velocity information. Future work could focus on automatically learning state abstraction from visible pixels and refining representation iteratively based on reward discovery. The worker in the framework receives past frames as input and repeats actions 4 times. MONTEZUMA'S REVENGE and PITFALL! are deterministic, allowing the manager to navigate to known set fringes using saved skills. States at the fringes are saved to minimize training time, enabling teleportation instead of re-simulation. This only affects wallclock time and does not benefit the agent. Hyperparameters are tuned on MONTEZUMA'S REVENGE. In MONTEZUMA'S REVENGE, hyperparameters are tuned with the Adam optimizer BID19. Abstract states consist of bucketed agent coordinates, room number, inventory, room objects, and inventory history. In PITFALL!, abstract states include agent coordinates, room number, and picked up items. In PRIVATE EYE, abstract states are represented as tuples including agent coordinates, room number, inventory, inventory history, and tasks completed. The architecture uses Dueling DDQNs to produce state-action values and recover a policy by selecting actions with the highest Q-value. The skill in PRIVATE EYE uses the BID24 architecture to represent states and transitions. It computes pixel and transition embeddings using convolutional layers and rectified linear layers. The pixel embedding is obtained by applying three convolutional layers with rectifier non-linearities and a final layer with output size 512. The transition embedding is computed by concatenating reward and state difference embeddings, passing them through rectified linear layers to get a final output size of 64. The skill in PRIVATE EYE uses the BID24 architecture to represent states and transitions with pixel and transition embeddings. The skill passes through linear layers to obtain A(s,s)(x,a) and V(s,s)(x). A sliding window estimate of success rate is kept to prevent rapid skill changes. The DDQN loss function is used for policy updates, with rewards clipped between 0 and R hold for stability. Some skills are easy to learn without pixel inputs. The worker in PRIVATE EYE uses pixel-blind skills for simple transitions, only computing e(s,s) and passing it through a final layer to compute advantage and value functions. If pixel-blind skills fail, pixel-aware skills are attempted. Skills use epsilon-greedy exploration and once frozen, exploration is set to 0. The number of episodes needed to learn each skill is unknown. The number of episodes required to learn each skill is not known in advance, as some skills require more episodes to learn than others. Using a fixed epsilon schedule for decay may not be sufficient, as it can waste training time for simple skills or prevent difficult skills from learning. To address this, an epsilon schedule is created based on the doubling trick in online learning to accommodate skills with varying learning speeds. The skills in the training process have varying learning speeds, so an epsilon schedule is used to adjust exploration over different horizons. Additionally, count-based exploration and self-imitation techniques are employed to enhance learning efficiency. The training process utilizes an epsilon schedule for adjusting exploration over different horizons. Self-imitation techniques are used to decrease learning time by adding successful trajectories to a replay buffer for imitation learning on optimal skill trajectories. The skills periodically sample from this buffer to update on an imitation loss function. The training process uses self-imitation techniques to speed up learning by replaying successful trajectories for optimal skill learning. Skills are encouraged to replay actions that led to success, with simpler skills being reused more frequently than esoteric ones. The agent's skill execution involves jumping over monsters, climbing ladders, and using different skills to avoid obstacles. The approach is tested on new reward functions, such as picking up keys in different rooms, to evaluate its generalization ability. The agent in MONTEZUMA'S REVENGE receives rewards for specific tasks like killing a spider, entering a room, and completing goals. The approach is trained on the basic reward function for 2B frames before observing new rewards. After being trained on the basic reward function for 2B frames, the agent in MONTEZUMA'S REVENGE quickly adapts to new reward functions with little additional training. Compared to SmartHash, our approach achieves 3x more reward on different reward functions, with an average reward of 716.7 out of 1000 across 3 tasks. SmartHash only achieves an average reward of 220 even when trained directly on the new reward function. Our approach shows near-linear progress in learning transitions, with consistent learning rates in MONTEZUMA'S REVENGE and PIT-FALL!. By training a single seed on PITFALL!, it achieved rewards of 26000 at 5B frames and 35000 at 20B frames. Adjusting a hyperparameter improved performance on PRIVATE EYE, surpassing human performance on 2 of 4 seeds. Decreasing the manager's exploration around abstract states from 500 to 10 visits in PRIVATE EYE further enhances performance. Decreasing N visit from 500 to 10 improves performance by preventing unnecessary exploration, enabling more transitions to be learned in fewer frames. With N visit set to 10, the approach achieves a final average performance of 60247 after 200M frames of training, exceeding average human performance in some cases. Hierarchical policy over skills may not always be near-optimal due to certain trajectories being impossible to follow using the skills. The approach achieves near-optimal policy on the original MDP in polynomial time with high probability, under certain assumptions. Lemmas are used to prove this, showing that the policy is near-optimal on the abstract MDP and achieves the same expected reward as on the concrete MDP. The approach achieves near-optimal policy on the original MDP in polynomial time with high probability, under certain assumptions. Lemmas are used to prove this, showing that the policy is near-optimal on the abstract MDP and achieves the same expected reward as on the concrete MDP. The known set grows to cover all abstract states, ensuring suboptimal behavior on the concrete MDP. The abstract states that the policy is near-optimal on the concrete MDP. Lemmas are used to prove this, showing that the policy achieves the same reward on both MDPs. The model errors are bounded, and the abstract MDP is Markov, allowing for the application of the simulation lemma. The policy is near-optimal on the concrete MDP, with Lemmas proving it achieves the same reward on both MDPs. Model errors are bounded, and the abstract MDP is Markov, allowing for the application of the simulation lemma. If certain conditions are met, the policy optimizing the MDP is at most suboptimal."
}