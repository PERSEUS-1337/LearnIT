{
    "title": "Syx4wnEtvH",
    "content": "Training large deep neural networks on massive datasets is computationally challenging, leading to a recent interest in using large batch stochastic optimization methods like LARS. However, LARS performs poorly for attention models like BERT. This paper introduces a new technique called LAMB, which shows superior performance across various tasks including BERT. The results demonstrate the superior performance of LAMB in tasks like BERT and ResNet-50 training with large batch sizes. By increasing batch size to the memory limit of a TPUv3 Pod, BERT training time can be reduced significantly. Training state-of-the-art models like BERT and ResNet-50 is time-consuming, leading to a need for optimization solutions like LAMB. The paper investigates optimization techniques to accelerate training large deep neural networks, focusing on variants of SGD. Traditional approaches use distributed asynchronous setup to improve training time, but this leads to degraded performance due to implicit staleness. Recent hardware advances allow for computing gradients on large minibatches in parallel, leading to a resurgence of using synchronous SGD with large minibatches. Recent advances have shown that using synchronous SGD with large minibatches can be an effective alternative to asynchronous SGD. However, simply increasing the batch size can lead to a decrease in generalization performance and computational benefits. Synchronous SGD on large minibatches benefits from reduced variance of stochastic gradients, allowing for larger learning rates. Recent works have demonstrated that linear scaling of the learning rate with minibatch size can further speed up training, but a hand-tuned warmup strategy is necessary during the initial phase. Using synchronous SGD with large minibatches can reduce training time for deep neural networks. A hand-tuned warmup strategy is needed initially, and linear scaling of the learning rate can be detrimental beyond a certain batch size. The LARS algorithm has been successful in training ResNet-50 on ImageNet in just a few minutes, but its performance gains are not consistent across tasks. Variants of SGD with adaptive learning rates have been proposed to address the challenges of large batch learning. The LARS algorithm has shown inconsistent performance gains, especially for attention models like BERT. To address this, new approaches tailored for large batch learning have been developed. These include a general adaptation strategy inspired by LARS and a new optimization algorithm called LAMB, which ensures adaptivity of learning rate in SGD. Convergence analysis for both LARS and LAMB in nonconvex settings has been provided, highlighting their benefits for large batch settings. Empirical results demonstrate the strong performance of LAMB. LAMB demonstrates strong empirical performance in large batch settings, scaling batch size in BERT training to over 32k without performance degradation, reducing training time from 3 days to 76 minutes. It is the first work to achieve BERT training in less than a couple of hours. LAMB also shows efficiency in training state-of-the-art image classification models like RESNET, outperforming adaptive solvers like Adam. In large batch settings, learning focuses on convex models with benefits from larger batch sizes and appropriate learning rates. Similar improvements are seen in nonconvex settings, but with concerns about generalization and computational performance. Prior works hand-tune hyperparameters to prevent performance degradation, while scaling the learning rate linearly with batch size can help avoid optimization instability. In large batch settings, learning focuses on convex models with benefits from larger batch sizes and appropriate learning rates. Goyal et al. (2017) proposed a hand-tuned learning rate strategy involving warm-up and linear scaling to train RESNET-50 with batch size 8192 without loss in generalization performance. However, Shallue et al. (2018) found that learning rate scaling heuristics may not hold across all problems or batch sizes. Recent works have explored adaptive learning rates for large batch training to reduce hyperparameter hand-tuning and successfully scaled batch sizes without performance degradation. Recent works have explored adaptive learning rates for large batch training to reduce hyperparameter hand-tuning and successfully scaled batch sizes without performance degradation. The fastest training result for RESNET-50 on ImageNet is achieved by Ying et al. (2018) in 2.2 minutes using the LARS optimizer and a batch size of 32K on a TPUv3 Pod. However, these performance gains do not hold in other tasks such as BERT training. In this paper, nonconvex stochastic optimization problems are studied, where a smooth function and a probability distribution on the domain are involved. The function is assumed to be L i -smooth with respect to x (i) , and various norms are used to denote properties of vectors. Stochastic gradient descent (SGD) is a simple first-order algorithm for solving nonconvex optimization problems. It involves updating iterates using random samples drawn from a distribution. With large batch settings, SGD iterates can converge to an optimal solution. In large batch settings, tuning the learning rate in SGD can be challenging, especially due to the dependence on L \u221e for smoothness across dimensions. To address this issue, algorithms have been developed to adapt the learning rate. The focus is on training deep neural networks, with a discussion on a general strategy for updating iterates in the small batch setting using algorithms like SGD or ADAM. In large batch settings, the update rule for deep neural networks is modified by normalizing the update to unit l2-norm and scaling the learning rate layerwise. This modification results in a biased gradient update but helps in addressing challenges in tuning the learning rate for SGD. In large batch settings, normalizing the update to unit l2-norm and scaling the learning rate layerwise helps address challenges in tuning the learning rate for SGD. This normalization provides robustness to exploding and plateauing gradients, ensuring faster convergence in deep neural networks. The scaling term involving \u03c6 ensures the update's norm is of the same order as the parameter, with \u03c6(z) = min{max{z, \u03b3 l }, \u03b3 u } found to work well in practice. The LARS algorithm, based on momentum optimizer, reduces variance in stochastic gradients with little bias. It was proposed for large batch learning for RESNET on ImageNet. The convergence analysis for LARS in a nonconvex setting is provided, with a focus on the case where \u03b2 1 = 0 and \u03bb = 0. The LAMB algorithm extends the analysis of LARS to the general case. It incorporates ADAM as the base algorithm and introduces adaptivity through per dimension and layerwise normalization. The pseudocode for LAMB is provided in Algorithm 2. When \u03b2 1 = 0 and \u03b2 2 = 0, the algorithm reduces to Sign SGD. The LAMB algorithm extends LARS to the general case by incorporating ADAM and introducing adaptivity through normalization. The convergence rate for LAMB in nonconvex settings is discussed, with bounds provided for x t generated using LAMB. Comparison of convergence rates between LARS and SGD is also discussed, highlighting differences in convergence criteria. The LAMB algorithm extends LARS by incorporating ADAM and introducing adaptivity through normalization. The convergence rate of LARS is better than SGD when the gradient is denser than curvature and stochasticity. The convergence rate of LAMB depends on L avg instead of L \u221e and is significantly better than that of SGD. Empirical results comparing LAMB with SGD are presented. The LAMB algorithm is compared with existing optimizers on large batch training tasks like BERT and RESNET-50. Minimal hyperparameter tuning is used for LAMB, with parameters \u03b2 1 and \u03b2 2 set to 0.9 and 0.999. A polynomially decaying learning rate is employed, and hyperparameters are not tuned for BERT and RESNET-50 training with increasing batch size. The LAMB algorithm was used without hyperparameter tuning while increasing batch size. LR scaling rule and linear-epoch warmup scheduling were employed. TPUv3 with 1024 chips was used for experiments. Hyperparameters for ADAM, ADAGRAD, ADAMW, and LARS were tuned using grid search. Experimental details are in the Appendix. The study focused on speeding up BERT training using the SQuAD task 2 dataset. In this paper, the focus is on the SQuAD task 2 dataset and the F1 score metric. Training BERT involves 900k iterations with a sequence length of 128, then switching to 512 for the last 100k iterations. The baseline BERT model achieves an F1 score of 90.395. By adjusting the learning rate, a higher F1 score of 91.688 can be obtained. By using the LAMB optimizer, a higher F1 score of 91.688 was achieved for a batch size of 16K, compared to the untuned version's score of 91.345. Switching to a 32K batch size with LAMB optimizer reduced BERT training time from 3 days to around 100 minutes, resulting in a 49.1 times speedup. Researchers achieved a 49.1 times speedup in BERT training time, reducing it from 3 days to around 100 minutes using synchronous data-parallelism. The efficiency was 76.7%, with communication overhead from gradient transfers. RESNET-50 showed 90% scaling efficiency due to fewer parameters compared to BERT. Mixed-Batch Training with LAMB was used for further improvements, with different sequence lengths for training stages. In experiments using TPUv3 Pods, a batch size of 32768 is recommended. A larger batch size of 131072 can be used for the first stage due to shorter sequence length, but no speedup was observed. The batch size is restricted to 65536 for this stage to fully utilize hardware resources. F1 score on SQuAD-v1 is used as the accuracy metric, with experiments running on TPUv3s following a specific setting. In experiments using TPUv3 Pods, a batch size of 32768 is recommended. A larger batch size of 131072 can be used for the first stage due to shorter sequence length, but no speedup was observed. The batch size is restricted to 65536 for this stage to fully utilize hardware resources. F1 score on SQuAD-v1 is used as the accuracy metric, with experiments running on TPUv3s following a specific setting. The experiments used a sequence length of 512 and all ran the same number of epochs. Dev set refers to the test data. Manual tuning of hyperparameters can lead to better results. Increasing batch size helps warm-up and stabilize optimization, while decreasing it can cause chaos and divergence. A technique was found to stabilize the second stage optimization by re-warming up the optimization and ramping up the learning rate from zero again in the second stage. This method reduced the number of iterations needed for BERT training to 8599 and finished in 76 minutes with 100.2% efficiency. Comparison with ADAMW and LARS was also conducted. In experiments with TPUv3 Pods, a batch size of 32768 is recommended, with a larger batch size of 131072 for the first stage. Manual tuning of hyperparameters can lead to better results. Increasing batch size helps warm-up and stabilize optimization. A technique was found to stabilize the second stage optimization, reducing the number of iterations needed for BERT training to 8599 and finishing in 76 minutes with 100.2% efficiency. Comparison with ADAMW and LARS was also conducted, showing that LAMB performs better than LARS for all batch sizes. Training with ResNet-50 is an industry standard metric used in MLPerf. Successful implementations are based on momentum SGD or LARS optimizer. Previous studies did not achieve state-of-the-art accuracy with ADAM, ADAGRAD, or ADAMW optimizer. Even with hyper-parameter tuning, these optimizers only reach 55.38% to 67.27% accuracy. Implementing a learning rate scheme improved accuracy to 72.0% to 73.48%, still lower than the baseline of 76.3%. LAMB optimizer can achieve the target accuracy. Table 3 displays LAMB's superior accuracy compared to momentum and LARS at various batch sizes. LAMB achieves 77.11% top-1 accuracy at a batch size of 2K, surpassing LARS's 76.6%. Additionally, at a batch size of 32K, LAMB achieves 76.4% while LARS achieves 76.3%. The target accuracy for LAMB is around 0.763. The convergence analysis of LAMB for general minibatch size is discussed here. The update of LAMB is analyzed based on the smoothness of the function f and the Lipschitz continuous nature of the gradient. By rearranging terms and dividing by a specific factor, the convergence of LAMB is proven. The convergence analysis of LAMB for general minibatch size is discussed based on the Lipschitz continuous nature of the gradient. By bounding term T 1 and using telescoping sum, the convergence of LAMB is proven. Inspired by previous comparisons, LARS convergence rate can be determined. Comparing SIGN SGD with SGD, the convergence rate of LARS is discussed based on the gradient's denseness, curvature, and stochasticity. N-LAMB and NN-LAMB show comparable accuracy to LAMB optimizer, outperforming momentum solver. Nadam's learning rate is adjusted for stability and target accuracy. Nesterov's accelerated gradient (NAG) is also mentioned. Nesterov's accelerated gradient (NAG) is found to be superior to regular momentum for convex, non-stochastic objectives. Nadam optimizer, incorporating Nesterov's momentum, improves convergence speed and model quality in various applications. N-LAMB and NN-LAMB algorithms, utilizing Nesterov's momentum in LAMB optimizer, demonstrate comparable accuracy to LAMB. NN-LAMB algorithm, incorporating Nesterov's momentum for both first and second moments, achieves comparable accuracy to LAMB optimizer. Experimental results show improved performance compared to momentum solver. Adam-correction operation has a similar effect as learning rate warmup. The warmup function in modern deep learning systems allows for the removal of adam-correction from the LAMB optimizer. Different norms were tried in the LAMB optimizer, but no significant difference in validation accuracy was observed for ImageNet training with ResNet-50. The default norm used is L2. DavidNet is the fastest model for the CIFAR-10 dataset according to DAWNBench. The baseline for CIFAR-10 training with DavidNet is the momentum SGD optimizer. The PyTorch implementation of DavidNet achieves 94.06% accuracy on GPUs in 24 epochs, while the Tensorflow implementation on TPUs achieves 93.72% accuracy. The LAMB optimizer outperforms other adaptive optimizers and momentum SGD with a 94.08% test accuracy in 24 epochs. LAMB also performs better than existing solvers on tasks like MNIST training with LeNet. The momentum optimizer and weight decay term were carefully tuned for CIFAR-10 training with DavidNet on a cloud TPU. Various solvers were tuned with different learning rates. LAMB optimizer has specific hyper-parameters like \u03b21 and \u03b22 for gradient averaging. Default settings include weight decay rate \u03bb=0.01 and \u03b21=0.9. In experiments with different optimizers, tuning the learning rate was found to be crucial for efficiency and accuracy. Validation loss was not reliable for large-batch training, so test/val accuracy or F1 score was used instead. BERT training with ADAMW optimizer showed the importance of tuning parameters for optimal performance. The tuning information from BERT training with ADAMW optimizer shows that the target F1 score is 90.5, while LAMB achieves a F1 score of 91.345. The loss curves of BERT training by LAMB for different batch sizes indicate good scalability with the batch size. Additionally, LAMB can make the training converge smoothly at a batch size of 64K, achieving scaling efficiency of 76.8% and 101.8% with mixed-batch training. From the tuning information, it is evident that different ratios are used for various layers in LAMB to aid slow learners in training faster. Implementing learning rate warmup and decay schemes suggested by Goyal et al. (2017) can enhance ImageNet classification accuracy. Techniques like 5-epoch warmup and adjusting the learning rate at specific epochs have been incorporated into Adam/AdamW/AdaGrad tuning, resulting in an accuracy improvement to around 73%. Despite these efforts, the target validation accuracy has not been achieved yet. The tuning information for different optimizers like Adagrad, Adam, and AdamW with various learning rate schemes is provided in tables and figures. Tuning sets based on AdamW with default and disabled L2 regularization are discussed, aiming to improve performance. The tuning information for different optimizers like Adagrad, Adam, and AdamW with various learning rate schemes is provided in tables and figures. Figures 17-20 show tuning results for LAMB optimizer, while Figure 6 demonstrates the training loss curve of LAMB. Additional tuning sets based on AdamW+ with default and disabled L2 regularization are discussed in Figures 21-25. The study concludes that existing adaptive solvers do not perform well on ImageNet training. The difficulty in tuning adaptive solvers like Adagrad, Adam, and AdamW was highlighted in a conference paper presented at ICLR 2020."
}