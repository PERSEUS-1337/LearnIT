{
    "title": "BJfIVjAcKm",
    "content": "In the context of neural network verification, we focus on co-design to train deep neural networks that are robust to adversarial perturbations and easier to verify. We highlight the importance of weight sparsity and ReLU stability in reducing verification complexity. Improving weight sparsity alone makes verification problems manageable, while enhancing ReLU stability further speeds up verification times by 4-13x. Our methodology is universal and compatible with various training and verification methods. Deep neural networks (DNNs) have achieved success in various domains, including image classification, face and speech recognition, and game playing. The reliability and robustness of these models, especially their resilience to adversarial attacks, are key concerns. Adversarial examples are imperceptibly modified inputs that can cause models to misclassify with high confidence. While there are defenses against such attacks, recent research has shown that most of them are ineffective. Recent research has shown that many defenses against adversarial attacks are ineffective, including the defense of BID19. There is a need for verification of networks to ensure resilience to all attacks. Exact verifiers have been designed for this task, but the process is often slow and computationally infeasible. This work aims to address the intractability of exact verification by decoupling model training and verification. The paper introduces the concept of co-design during model training to align training and verification stages. By improving weight sparsity and ReLU stability, models can be both robust and easy to verify. Techniques like 1-regularization can speed up the verification process significantly. The paper focuses on improving weight sparsity and ReLU stability to speed up the verification process for exact verification of ReLU networks. By minimizing branching through maximizing stable ReLUs, the complexity of verification can be reduced significantly. By maximizing stable ReLUs, we introduce a regularization technique to induce ReLU stability, enabling faster verification of weight-sparse networks for MNIST and CIFAR-10. Combining weight sparsity methods with adversarial training, we achieve almost 90% faster verification compared to previous techniques. Additionally, our regularization technique speeds up verification by 4-13x while maintaining high accuracy on MNIST and CIFAR models. Our network achieves provable robustness results for \u221e norm-bound adversaries, with the best provable adversarial accuracies yet for = 0.2 and = 0.3 on MNIST. We also achieve the first nontrivial provable adversarial accuracy results for CIFAR-10 using exact verifiers. Our training techniques focus on improving input for verification, compatible with current and future verification methods. Our code is available at https://github.com/MadryLab/relu_stable. Verification of neural networks, especially those with ReLUs, is challenging due to the exponential growth in cases to consider. Exact verification has been proven to be NP-complete, leading to the development of formal verification methods in recent years. Formal verification methods using SMT and MILP solvers struggle with scaling issues when verifying networks with many ReLUs. An alternative approach focuses on certification methods that rely on solving a relaxation of the verification problem by overapproximating the adversarial polytope. These methods make certification easier by training models in a specific manner. Certification methods make verifying models easier by solving a relaxation of the original verification problem. However, they may fail to certify inputs that are actually robust. Certification approaches work well only on models specifically trained for that purpose, leading to a high rate of false negatives when used on arbitrary models. Our methods are designed to be universal and can be combined with any standard training procedure for networks to improve verification speed. They also reduce overapproximation in certification methods. Weight sparsity and ReLU stability are key properties that enhance verification speed. Natural regularization methods induce weight sparsity, while a new method induces ReLU stability. Applying interval bound propagation during training with MILP-based verification leads to provably robust networks. Our focus is on inducing weight sparsity and ReLU stability in deep neural network models, specifically k-layer fully-connected feed-forward DNN classifiers with ReLU nonlinearities. These methods significantly speed up verification while maintaining accuracy. The network's output is defined as a function f(x, W, b), where W and b are weight matrices and biases. Adversarial robustness is crucial for network reliability. Adversarial robustness is essential for network reliability, aiming for predictions to be robust to adversarial perturbations. The focus is on \u221e norm-bound perturbations, but methods can be applied to other norms. Verification involves finding perturbations that cause misclassification by altering logits. The task of finding adversarial perturbations is expressed as an optimization problem. Adversarial accuracies are defined as the model's robustness to perturbations. Evaluations against specific attacks provide upper bounds, while certification methods provide lower bounds. Exact verifiers can prove robustness or find perturbations causing misclassifications, determining true adversarial accuracy. Certification methods, however, cannot exactly determine true adversarial accuracy. The provable adversarial accuracy is computed as the fraction of test set inputs for which the verifier can prove robustness within a time budget. Weight sparsity is a property of network models that can improve the speed of exact verification. The MILP exact verifier of BID29 is used in experiments for its speed and open-source nature. Weight sparsity is crucial for speeding up exact verification of models, as LP or MILP systems benefit from fewer variables. Regularization methods like 1-regularization and small weight pruning significantly enhance verification speed, making it possible to prove robustness of adversarially-trained models on a higher percentage of inputs within a shorter time frame. Weight sparsity is essential for speeding up model verification. Techniques like 1-regularization and small weight pruning enhance weight sparsity, making networks easier to verify. Different methods were incrementally added to improve weight sparsity and ReLU stability, leading to improved adversarial accuracy and verification solve times. The control model for MNIST with a timeout of 3600 seconds and verification of only the first 100 images was used. The primary speed bottleneck of exact verification is the number of ReLUs the verifier has to consider. In our paper, we focus on reducing the number of ReLUs verifiers need to branch on, known as inducing ReLU stability. Verifiers must consider pre-activations of ReLUs, leading to exponential increases in verification complexity. Models with fewer ReLUs requiring branching are easier to verify. In our paper, we focus on reducing the number of ReLUs verifiers need to branch on, known as inducing ReLU stability. Verifiers must consider pre-activations of ReLUs, leading to exponential increases in verification complexity. Models with fewer ReLUs requiring branching are easier to verify. Among them, only 100 ReLUs requiring branching are easier to verify than 200 ReLUs that all require branching. The goal is to maximize the number of stable ReLUs on an input x with allowed perturbation set Adv(x). Verifiers compute upper and lower bounds of pre-activations to determine stability, replacing ReLUs with identity or zero functions if necessary. If branching is needed, it is determined based on the bounds. In Section 3.3.2, methods for determining upper and lower bounds for ReLUs are discussed. A smooth approximation function is used as a regularizer to encourage ReLU stability, decreasing the number of unstable ReLUs. The RS Loss objective effectively speeds up verification by determining bounds for each ReLU. The bounds for inputs are simple, aiding in the process of maximizing stable ReLUs on input x with allowed perturbations. Interval arithmetic approaches can efficiently estimate bounds for ReLUs, using either naive or improved versions. The improved version provides tighter estimates but requires more resources and is best suited for smaller networks. These approaches work well with backpropagation as they involve matrix multiplications, unlike exact verifiers which solve LPs or MILPs. Interval arithmetic may overestimate unstable ReLUs, but minimizing them based on IA bounds gives an upper bound compared to exact verifiers. Improved Interval Arithmetic (IA) efficiently estimates bounds for ReLUs, penalizing unstable ReLUs accurately. Experimental results show that RS Loss regularization enhances ReLU stability and speeds up verification times significantly. Comparing networks with different RS Loss weights reveals a trade-off between stability, verification speed, and test set accuracy. The RS Loss regularization improves ReLU stability and speeds up verification times significantly. Increasing the weight on RS Loss enhances ReLU stability, leading to faster verification and potentially better provable adversarial accuracy. However, excessive weight on RS Loss can reduce model capacity and impact adversarial accuracy. Careful selection of the RS Loss weight is crucial for achieving high provable adversarial accuracy and faster verification speeds. RS Loss improves ReLU stability and speeds up verification times significantly, leading to higher provable adversarial accuracy. By carefully selecting the weight on RS Loss, models can achieve both high accuracy and faster verification speeds. RS Loss significantly speeds up verification times and improves provable adversarial accuracy, especially for the hardest verification problems. The results are compared with state-of-the-art certifiable defenses in TAB3. Our method demonstrates robustness against strong adversarial attacks and provable adversarial accuracy. Results show better performance on MNIST compared to CIFAR-10, indicating the need for more unstable ReLUs for CIFAR classifiers. Verification times were faster with RS Loss, and detailed experimental results are provided in Appendix E. In experiments, robust adversarial training BID10 was used against a strong adversary to train DNN classifiers on MNIST and CIFAR datasets. Weight on RS Loss was determined via line search, and different ReLU architectures were employed. The network architectures included 2x2 strided convolutions with 16 and 32 filters, a 100 hidden unit fully connected layer, and a larger architecture with 4 convolutional layers and 2 fully connected layers. The model used 64 filters and 2 fully connected layers with 512 hidden units each. Verification was done using the exact verifier from BID29, parallelized over 8 CPU cores. Build times for control and \"+RS\" models on MNIST were between 4 and 10 seconds. The paper focuses on developing training methods that prioritize verification, showing faster verification of trained models. Our method improves a network's ReLU stability, making verification 4-13x faster. It can be added to any training procedure and scales verification to larger networks. The regularization methods compress networks into simpler forms, aiding in achieving high accuracy. The use of techniques from model compression can enhance network verification by improving weight sparsity and ReLU stability. Additional objectives beyond weight sparsity and ReLU stability may also be important for verification speed, suggesting a potential direction for future exploration. Techniques such as 1-regularization and small weight pruning can improve provable adversarial accuracy and verification speed without compromising test set accuracy. The use of 1-regularization and small weight pruning techniques can enhance network verification by improving weight sparsity and ReLU stability. These techniques improve provable adversarial accuracy and verification speed without compromising test set accuracy. The text discusses how ReLUs that are rarely used can be pruned without significantly affecting the network's behavior. Experimental evidence on an adversarially trained MNIST model supports this idea. Pruning ReLUs that are active on less than 10% of the training set or inactive on less than 10% is considered reasonable. Adversarial training against norm-bound adversaries also improves weight sparsity, making networks easier to verify. Adversarial training improves weight sparsity in networks, making verification easier. For linear networks, minimizing the ||W||1 term promotes weight sparsity. Adversarial attacks are stronger when the 1-norm of weight matrices is higher. Nonlinear networks also benefit from adversarial training, leading to weight-sparse layers in models. Adversarial training alone improves weight sparsity but is not enough for efficient verification. Additional regularization like 1-regularization and weight pruning further promote weight sparsity. Naive interval arithmetic determines bounds based on previous layers, but can be conservative for deeper networks. Exploiting ReLUs that are always active improves upon naive interval arithmetic. Exploiting ReLUs that are always active allows for canceling out equivalent symbols from earlier network layers, leading to tighter bounds in interval arithmetic. This concept is illustrated using a neural network with one hidden layer and scalar inputs. The active part of matrix W is defined as W A, while the non-active part is defined as W. An improved version of IA uses information from previous layers. Additional notation is defined to extend the method to any number of layers. The function f n takes in sequences of upper bounds, lower bounds, weights, and biases to output the current layer's bounds. The method involves computing bounds for each layer of a neural network efficiently by utilizing information from previous layers. This approach only requires O(d^2) additional matrix multiplications for a d-layer neural network, making it feasible for most DNNs. Improved Interval Arithmetic (IA) method provides more accurate estimates of ReLU upper and lower bounds for neural networks compared to naive IA. Despite the slight slowdown and higher memory usage due to matrix-matrix multiplications, the trade-off is worth it for most networks. Empirical evidence shows that the number of unstable ReLUs estimated by improved IA closely aligns with the exact verifier, outperforming naive IA. The method is compared on MNIST networks, demonstrating the effectiveness of improved IA in estimating the average number of unstable ReLUs. Improved Interval Arithmetic (IA) method provides accurate estimates of ReLU bounds for neural networks. The upper and lower bounds computed by IA are conservative, correctly identifying unstable ReLUs. However, stable ReLUs can be mislabeled as unstable, leading to unnecessary regularization. By using RS Loss combined with IA bounds, the number of ReLUs labeled as unstable can be reduced without compromising test set accuracy. This approach achieves similar performance to models trained without RS Loss, demonstrating the effectiveness of IA in decreasing the number of unstable ReLUs. IA bounds are conservative, but it is possible to decrease the number of unstable ReLUs without significantly affecting test set accuracy by using RS Loss. Experimental results show a decrease in the average number of unstable ReLUs from 290.5 to 105.4 with just a 0.26% loss in accuracy for MNIST and = 0.1. This trend holds for deeper networks as well. Adversarial training with BID10 was used to train various DNN classifiers, with a linear increase in adversary strength during the first half of training. The training schedule was optimized for MNIST and CIFAR datasets, using different batch sizes and optimization techniques. RS Loss regularization was added in the last 20% of training on CIFAR to speed up the process. Suitable weights for RS Loss were found via line search, and the same weights were used for each ReLU. The MILP-based exact verifier of BID29 uses a two-step process for every input, involving model-building and solving. The verifier computes upper and lower bounds on each ReLU using LP, with IA as an alternative option. The speed of the model-build step depends on the tightening algorithm and the sparsity of the weights. The MILP verifier utilizes LP to compute bounds on ReLUs, with the number of variables impacting build times. The solver used is Gurobi, with speed influenced by the number of binary variables and total variables in the MILP. In exploring properties of neural networks that correspond to MILPs easy or hard to solve, the exact verifier from BID29 was used with default settings. Verification of each input datapoint was allotted 120 seconds, using Gurobi Solver (version 7.5.2) parallelized over 8 CPU cores. Computers with 8-32GB RAM were used, part of an OpenStack network. Full results on natural improvements, control networks, and \"+RS\" networks were analyzed. The study evaluated the robustness of neural networks using a verifier to determine upper and lower bounds on adversarial accuracy. The verifier can prove if a network is not robust to perturbations on certain inputs. Timeouts were recorded for inputs that the verifier could not prove in 120 seconds. Build and solve times were reported in seconds, with timeouts contributing 120 seconds to the total solve time. The \"Adversarial Training\" network uses a 3600-second timeout and is only verified for the first 100 images due to long verification times. The \"+RS (Large)\" networks are verified for the first 1000 images because of long build times. Wong et al. (2018) and BID20 do not report results on MNIST with \u03b5 = 0.2. Certification methods are important for verifying properties of neural networks, such as adversarial robustness. The certification methods for adversarial robustness exploit a trade-off between provable robustness and speed, providing certificates quickly but potentially missing some robust inputs. Exact verifiers always give the correct answer but can take hours to verify robustness on a single input. A new paradigm called \"co-training\" involves training a robust neural network and then formally verifying its robustness in two steps. In the paradigm of \"co-training,\" exact verification methods are used as step 2 to ensure accuracy in assessing robustness, despite potential slowness. Techniques to induce weight sparsity and ReLU stability in step 1 aim to speed up the verification process. Developing effective methods for step 1 is crucial, as step 2 focuses on exact verification. Developing effective methods for inducing ReLU stability is crucial for tightening the relaxation of certification approaches. Techniques for inducing weight sparsity and ReLU stability can speed up the verification process, making them useful for certification. While most works focus on verifying adversarial robustness, our techniques can be applied to verify other properties as well."
}