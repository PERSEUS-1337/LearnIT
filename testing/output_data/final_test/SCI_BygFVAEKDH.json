{
    "title": "BygFVAEKDH",
    "content": "Non-autoregressive machine translation (NAT) systems use knowledge distillation from pretrained autoregressive models to improve generation speed and accuracy. Systematic experiments show that knowledge distillation reduces data complexity, helps model output variations, and correlates with optimal data complexity for translation quality. Based on findings, approaches are proposed to improve NAT model performance and achieve state-of-the-art results. Traditional NMT systems generate sequences in an autoregressive manner, while NAT models aim to take advantage of parallelism for faster inference. Non-autoregressive translation (NAT) models trade decoding efficiency for model capacity, predicting multi-token chunks simultaneously. However, NAT models generally perform worse than standard autoregressive (AT) models. To mitigate this, training data is created through knowledge distillation, specifically using sequence-level knowledge distillation during NAT model training. In this paper, the focus is on understanding how knowledge distillation reduces \"modes\" in training data for non-autoregressive translation (NAT) models. Key questions include measuring this reduction quantitatively, exploring the relationship between NAT and autoregressive (AT) models, and closing the performance gap with standard AT models. The paper aims to answer key questions about knowledge distillation for non-autoregressive translation models by conducting empirical analysis on various AT and NAT models. The contributions include visualizing mode reduction through synthetic experiments, proposing metrics for complexity and faithfulness, and finding a correlation between NAT model performance and capacity. Systematic analysis over different teacher and student models on a translation benchmark reveals a strong relationship between NAT model capacity and dataset optimization. The experiments show a correlation between NAT model capacity and dataset complexity for optimal translation quality. Proposed approaches adjust distilled data complexity to match model capacity, achieving state-of-the-art performance. NMT models generate output tokens based on previous ones, using autoregressive factorization. Greedy decoding and beam search are common methods for translation generation. Non-autoregressive translation (NAT) models factor the conditional distribution independently to make predictions for the entire sequence in one pass. However, they struggle with capturing dependencies between output tokens, leading to errors like repeated token outputs. Recent works aim to improve multi-modality handling in NAT models. Recent works have improved non-autoregressive translation models by incorporating approaches like relaxing the fully non-autoregressive restriction, using latent variables or structured information, and training with objectives other than maximum likelihood. Existing NAT models rely on training with data distilled from a pre-trained autoregressive model to achieve competitive performance. Knowledge distillation was originally proposed for training a weaker student classifier using label probabilities from a stronger teacher model. It has been effective in adversarial defense, neural network compression, and fast speech synthesis. In sequence generation, knowledge distillation has been extended to the sentence level using \"hard targets\" from a pretrained large teacher model to train a small sequence generation model. The teacher distribution q(t|x) is approximated by its mode, with objectives related to target sequences. This is a form of distillation where the most likely translation is predicted by the teacher using beam search. Existing literature trains NAT models using knowledge distillation from a pre-trained AT model. The teacher model is typically a Transformer with a similar number of parameters as the NAT model. This paper explores the knowledge distillation process in this context. In this paper, the knowledge distillation process is studied in the context of NAT models. The difficulty of NAT models in capturing multi-modality in output data is investigated using a synthetic setup with three language pairs from the Europarl parallel corpus. Multiple modes are explicitly included in the training data to test the hypothesis about the mechanism of knowledge distillation for NAT models. In this study, English input sentences correspond to target sentences in three languages without explicit signals. Two models, AT and NAT, are trained on concatenated data and compared. The AT model is based on the Transformer model, while the NAT model is a simplified version. Both models are trained for 300,000 steps and used to translate English sentences in validation and test sets. The study compares two models, AT and NAT, trained on concatenated data for 300,000 steps to translate English sentences into three languages. The AT model uses the Transformer model, and visualization of its outputs shows the probability distribution of language classes for each translated sentence. The AT model prefers generating sequences in one language, while the NAT model fails to capture language modes and predicts tokens mixed with multiple languages. Creating datasets with fewer modes than the original dataset is the next step. The AT model prefers generating sequences in one language, while the NAT model fails to capture language modes and predicts tokens mixed with multiple languages. Creating datasets with fewer modes than the original dataset is essential for the NAT model to select one mode (language) when generating translations. Training with reduced modes improves the clustering of points, indicating that systematic selection of language for training sentences is more effective than random assignment. In this section, quantitative measures are proposed to analyze the complexity and faithfulness of parallel data for NAT models. A measure of translation uncertainty, operationalized as conditional entropy, is used to assess complexity. The computation involves assumptions on the conditional distribution of target tokens given the source sentence. The conditional entropy of a sentence can be simplified by assuming independence of target tokens given the source sentence. The complexity of a parallel corpus is measured by calculating the conditional entropy of all sentences and averaging over source tokens. This metric is used to assess complexity and faithfulness of parallel data for NAT models. The conditional entropy of the distilled data is smaller than that of the original concatenated data and random-selection data. The En-Es and En-Fr conditional entropy are similar, while En-De is larger. A simpler data set is preferred for NAT models, but faithfulness to real data distribution is also important. In this section, an extensive study is conducted on non-autoregressive (NAT) models trained from autoregressive (AT) teacher models using the WMT14 English-German (En-De) dataset. Different Transformer models with varying parameter sizes are utilized, and a byte-pair encoding (BPE) vocabulary of 37,000 is learned on the tokenized data. The faithfulness of the parallel corpus is measured using KL-divergence, and newstest2013 is used as the validation set while newstest2014 is used as the test set. The study focuses on non-autoregressive (NAT) models trained from autoregressive (AT) teacher models using the WMT14 English-German dataset. Different Transformer models with varying sizes are used, and a BPE vocabulary of 37,000 is learned. The models are trained using the Adam optimizer with a maximum of 300,000 steps. The resulting AT models are used to decode the training set with beam size 5 to create a new parallel corpus. Various NAT models, from vanilla to state-of-the-art, are considered, all based on the Transformer backbone and implemented using Fairseq. The study focuses on non-autoregressive (NAT) models trained from autoregressive (AT) teacher models using the WMT14 English-German dataset. Different Transformer models with varying sizes are used, and a BPE vocabulary of 37,000 is learned. The models are trained using the Adam optimizer with a maximum of 300,000 steps. Various NAT models, from vanilla to state-of-the-art, are considered, all based on the Transformer backbone and implemented using Fairseq. Similarly to \u00a73.1, a simplified version is used where the decoder's inputs are directly copied from the encoder without considering latent variables. FlowSeq adopts normalizing flows as latent variables to model the mappings from source sentences to a latent space. iNAT extends the vanilla NAT by iteratively reading and refining the translation with 10 decoding iterations. InsT adopts a similar architecture as iNAT while generating the sequence by parallel insertion operations. MaskT adopts a masked language model to progressively generate the sequence from an entirely masked input with 10 iterations. The LevT model, similar to InsT and MaskT, generates based on insertion and deletion operations. It includes base and big models, with parameters, performance, and decoding speed summarized in a table. NAT models are trained from autoregressive models with greedy decoding for fair comparison. The LevT model, InsT, and MaskT use insertion and deletion operations for generation. Different search algorithms are compared, with some models based on iterative refinement. The capacity of the AT model affects the complexity and fidelity of the distilled data, as shown in Fig. 3. Higher-capacity teacher models result in higher BLEU scores for the distilled data compared to the real data set. The BLEU score of distilled data from a higher-capacity teacher model is higher, in agreement with results on KL divergence. Fuzzy reordering score shows less reordering in distilled data compared to real parallel sentences. Distilled data from a weaker teacher is more monotonic. This simpler reordering may benefit NAT models but could hinder learning for other models. Decoding strategies impact the performance of NAT models when creating distilled data. Beam search yields the best results, reducing complexity while maintaining faithfulness. Greedy decoding also simplifies real data effectively. Decoding strategies like beam search and greedy decoding impact the performance of NAT models in creating distilled data. Beam search reduces complexity while maintaining faithfulness, while greedy decoding simplifies real data effectively. The relationship between NAT students and distilled training data from different AT models is examined, with results shown in Fig. 4 for NAT models of varying capacity. The performance of NAT models trained on distilled data from different capacity AT models on WMT14-ENDE newstest 2014 test sets shows that changing the distilled data set can significantly improve results. For example, FlowSeq improved to 22 by switching from Transformer(base) to Transformer(small). LevT achieved the best performance when trained with distilled data from a big Transformer. In this section, three techniques are introduced to adjust the distilled data to match the capacity of NAT models. Born-Again Networks (BANs) are used to simplify the dataset for NAT models, MoE can simplify data for lower-capacity models, and Interpolation can increase faithfulness for higher-capacity models. The study introduces a technique (Gu et al., 2018) that uses the output distribution of a trained model to train the original model. Repeated training of new AT models with decoded sentences from the previous iteration results in k distilled data sets. Experiments show that the performance of the vanilla NAT model can be improved by 2 BLEU when using the distilled data from reborn iteration 6. The complexity of distilled data decreases as iterations continue, quality compared to real data also decreases. Mixture-of-Experts model learns diverse experts for machine translation. Performance of best expert in MoE decreases with more experts. NAT model performance improves by 1.21 BLEU using distilled data from MoE model with 3 experts. The NAT models like MaskT and LevT benefit from sequence-level interpolation to learn from datasets closer to real data, resulting in improved performance. Interpolation selects sentences with the highest BLEU score from beam search hypotheses, enhancing LevT's performance by approximately 0.4 BLEU compared to standard distillation methods. In this paper, the authors examine why knowledge distillation improves NAT model performance. They conducted experiments with autoregressive teacher models and a wide range of NAT models, finding that higher-capacity NAT models require more complex distilled data for better performance. Techniques are proposed to adjust data complexity to match model capacity. The AT models are implemented based on the Transformer model using fairseq, following examples from Vaswani et al. (2017). The authors used AT models based on the Transformer model with specific hyperparameters and training details. They employed beam-search for decoding and tested various NAT models, noting differences in architecture. The NAT models, except FlowSeq and LevT-big, have similar architecture and hyper-parameters as the Transformerbase. LevT-big is an extension of the original LevT model with parameters similar to Transformer-big. FlowSeq-base is used for FlowSeq, while a simplified version of Gu et al. (2018) is used for the vanilla NAT model. Additional modules are required for all models except InsT to predict output sequence length or placeholders. LevT also has a binary classifier for predicting token deletions during training. All NAT models are trained using the Adam optimizer. The NAT models are trained using the Adam optimizer with specialized settings for each model. For example, the iNAT model is trained jointly with 4 iterations of refinement, while the InsT model uses slot-loss based on the uniform loss function. Training batch sizes for NAT models are typically larger than AT models to ensure sufficient learning. Transformer (InsT) uses slot-loss based on the uniform loss function. MaskT follows a typical masked language model approach. LevT focuses on sequence generation tasks, learning insertion and deletion. Decoding is done using greedy decoding for all NAT models without advanced methods. The decoding steps for iNAT and MaskT are fixed at 10, while InsT and LevT decode adaptively with a maximum of 10 iterations. A special penalty for generating short sequences is tuned based on the validation set. Final results are evaluated using tokenized BLEU score. The dataset split for WMT14 En-De is detailed in Tab. 6. The histogram in Fig. 7 shows the distribution of conditional entropy in real and distilled data sets, indicating differences in complexity values. In experiments with WMT'14 En-De dataset, various metrics were considered alongside BLEU scores to capture system changes. Metrics like METEOR, RIBES, ChrF, TER, and BEER showed strong correlation with BLEU scores. Bayesian decision theory is used for pattern classification, providing a principled approach using probability and losses. In structured prediction, x is the input sequence and y is the output label sequence. H represents all hypothesis functions from input to output space. The goal is to find a hypothesis function h that minimizes the Bayes risk by minimizing the conditional risk for each input x. The Bayes optimal classifier is achieved by minimizing the overall risk R. Two loss functions are considered, including sequence-level loss L seq. The Bayes classifier determines the most probable output label sequence given the input sequence. It considers sequence-level loss and token-level loss, finding the most probable label at each time step. A Hidden Markov Model (HMM) is constructed to study the impact of training data on a weaker classifier, with parameters sampled uniformly. Higher values of transition and emission probabilities indicate higher uncertainty in the HMM model. The \"Bi-LSTM\" classifier uses a low-dimension Bi-LSTM to encode input sequences and individual softmax functions to predict labels independently. It cannot model dependencies between output labels in the HMM. Training data is generated by sampling from the true HMM, and token-level and sequence-level accuracies are evaluated on test data. Distillation labels are generated from the true HMM using h * seq (x). The training data is generated using h * seq (x) to create distillation labels y from the true HMM. Training data sets D seq and D tok are created using h * seq (x) and h * tok (x) respectively. The Bi-LSTM classifier is trained using three different training data sets (D real, D tok, D seq) and evaluated for token-level and sequence-level accuracies. Models trained with D tok perform the best in terms of token-level accuracy, while models trained with D seq perform the best in terms of sequence-level accuracy. The Bi-LSTM classifier trained with D seq yields the best performance in terms of sequence-level accuracy, as it can better capture the distribution of P(yt|x) compared to the true data distribution defined by an HMM. This is because D seq and D tok define simpler conditional distributions over the input data. Models trained with D tok perform best in terms of token-level accuracy. When using distilled data from a pretrained autoregressive model with beam-search decoding, NAT performs better on the test set with the BLEU score metric compared to modeling the real conditional distribution of true sentence pairs."
}