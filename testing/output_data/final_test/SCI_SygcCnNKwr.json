{
    "title": "SygcCnNKwr",
    "content": "State-of-the-art machine learning methods struggle with compositional generalization, lacking realistic benchmarks to evaluate improvements. A novel method is introduced to create benchmarks by maximizing compound divergence while ensuring small atom divergence. A natural language question answering dataset is presented using this method, revealing machine learning architectures' limited compositional generalization. The method shows a strong negative correlation between compound divergence and accuracy, highlighting the challenges in generalizing compositionally. The method introduced maximizes compound divergence while ensuring small atom divergence, creating new compositionality benchmarks on top of the existing SCAN dataset. This confirms the strong negative correlation between compound divergence and accuracy, revealing challenges in generalizing compositionally. Human intelligence shows systematic compositionality, the ability to understand and create infinite novel combinations of known components. This is seen in compositional generalization, where individuals can generalize to test examples after being exposed to training components. This ability is demonstrated in various domains like natural language understanding and visual scene understanding. For instance, learning the meaning of a new word allows for understanding its usage in different contexts. State-of-the-art machine learning methods often struggle to capture compositional structures, hindering their ability to generalize effectively. This is due in part to a lack of realistic benchmarks that comprehensively measure compositional learning in practical scenarios. Compositional generalization can be evaluated by splitting training and testing based on observable properties that correlate with underlying compositional structures. In this paper, the authors introduce distribution-based compositionality assessment (DBCA) as a method to assess dataset splits for measuring compositional generalization. They also present the Compositional Freebase Questions (CFQ) dataset designed for this purpose and conduct experiments using the DBCA method. The authors introduce the DBCA method for measuring compositionality on CFQ and SCAN datasets. They analyze the performance of ML architectures and propose a setup where train and test sets have different distributions to assess generalization. The test set contains novel compounds, representing new ways of composing atoms from the train set. For example, questions like \"Who directed Inception?\" and \"Did Christopher Nolan produce Goldfinger?\" are used to measure compositional generalization. The atoms are primitive elements like predicates and question patterns, while compounds are combinations of these atoms found in examples. The curr_chunk discusses the concept of compositionality experiments in datasets like CFQ and SCAN, where examples are created by applying rules. It emphasizes the importance of atoms and compounds in measuring compositional generalization. The ideal experiment should adhere to these principles. An ideal compositionality experiment should adhere to two principles: 1. Similar atom distribution between train and test sets. 2. Different compound distribution to ensure compositional challenge. The experiment evaluates adherence to compositionality principles by analyzing frequency distributions of atoms and compounds in a sample set. Subgraphs of rule application DAGs are weighted to avoid double-counting correlated compounds. The experiment evaluates adherence to compositionality principles by analyzing frequency distributions of atoms and compounds in a sample set. The divergence of weighted distributions is measured using the Chernoff coefficient. Different divergence values are used for atom and compound distributions to make them as similar as possible. Based on principles of compositionality, a preferred benchmark for dataset accuracy is suggested to be based on splits with maximum compound divergence and low atom divergence. The Compositional Freebase Questions (CFQ) dataset is introduced as an example designed to measure compositional generalization using the DBCA method. CFQ is a large dataset of natural language questions and answers with corresponding SPARQL queries against the Freebase knowledge base, suitable for semantic parsing tasks. In this paper, the focus is on automated rule-based dataset generation for measuring compositional generalization using the DBCA method. The approach offers benefits such as scalability, scope control, and error avoidance. By tracking the sequence of rule applications, the method allows for precise tracking of atoms and compounds in each example. The generation rules are designed to have few but meaningful atoms to enable a wide variety of compounds and compound divergences. The paper focuses on automated rule-based dataset generation for measuring compositional generalization using the DBCA method. The rules aim to create a wide variety of compounds while maintaining atomic behavior. The rules are categorized into grammar, inference, and resolution rules for generating natural language constructs and mapping to SPARQL. The paper discusses automated rule-based dataset generation for measuring compositional generalization using the DBCA method. It categorizes rules into grammar, inference, and resolution rules to generate natural language constructs and map them to SPARQL queries. The generation algorithm produces triples of question, logical form, and SPARQL query in a top-down and bottom-up fashion, along with a normalized DAG of rule applications. The approach described in the paper involves generating a diverse set of questions by sampling randomly and then subsampling to maximize diversity of rule combinations. The complexity of examples is measured by the number of rule applications, with a limit set on maximum complexity. Examples of generated questions at varying complexity levels are shown in Table 1. Additional details on data items, data quality analysis, and the generation algorithm are provided in the appendices. The dataset focuses on semantic parsing and provides natural language answers for each question. Ambiguity is avoided in the questions, and specific language features are selected as building blocks. Knowledge base features include roles. The dataset focuses on semantic parsing, providing natural language answers for questions with specific language features as building blocks. Knowledge base features include roles, verbs, types, and adjectives from well-represented domains in Freebase. The internal logical form adopts a variation of description logic EL augmented with additional constructors for mapping to linguistic structures. Grammar rules use a unification-based syntax with support for disjunction, negation, absence, and default inheritance for compact representation. The dataset focuses on semantic parsing with natural language answers using specific language features from Freebase. The examples generated by CFQ rules contain entity placeholders instead of MIDs. To make the questions natural, placeholders are replaced with specific entities by executing SPARQL queries against Freebase. Candidate MID combinations are used as substitutes, with a random selection for positive answers. If no candidates are found, the question is abandoned as unnatural. In generating questions for semantic parsing, CFQ rules create variations with \"Yes\" and \"No\" answers by mixing in MIDs from other substitutions. Unnatural questions are filtered out using semantic and structural rules, based on analysis of datasets like WebQuestionsSP and ComplexWebQuestions. CFQ contains the most query patterns compared to other semantic parsing datasets and significantly more queries and questions. The DBCA principles enable a method for constructing compositionality experiments using an iterative greedy algorithm. The iterative greedy algorithm constructs compositionality experiments by alternating between adding examples to train and test sets while maintaining desired ratios. Random choices are made to avoid local optima, allowing for the removal of examples. Multiple splits can satisfy compound and atom divergence, reflecting different occurrences of compounds in train and test sets. The goal is to measure compositional generalization accurately by constructing maximum compound divergence (MCD) splits with low atom divergence. A comparison of compound and atom divergence is made for three MCD splits against a random split baseline and other compositionality experiments. The experiments are based on specific train and validation/test sizes for CFQ and SCAN datasets. The MCD splits aim to optimize compound divergence across all compounds directly, leading to higher divergence compared to other experiments. Despite this, MCD splits still correlate with aspects of compositional generalization targeted by other experiments. The train set in MCD splits contains shorter examples on average than the test set, and only a small fraction of input and output. The MCD splits aim to optimize compound divergence across all compounds directly, leading to higher divergence compared to other experiments. Despite this, correlations with aspects of compositional generalization targeted by other experiments are less pronounced and vary significantly across different MCD splits. This illustrates the comprehensive coverage of MCD splits in various aspects of compositional generalization, making examples in train and test sets look fairly similar. Three encoder-decoder neural architectures are used as baselines, with hyperparameters tuned using a CFQ random split. The hyperparameters are kept fixed for both CFQ and SCAN experiments, with a constant number of training steps. Each experiment is replicated 5 times, and the mean accuracy with 95% confidence intervals is reported. It is suggested to tune hyperparameters on a random split for measuring compositional generalization with respect to an unknown test distribution. Tuning on a validation set with the same distribution as the test set may lead to optimizing for a specific type of compound divergence. The text discusses anonymizing Freebase names and MIDs in textual input and SPARQL output to focus on the relation between compound data split divergence and accuracy in model configurations. The text discusses the relation between compound data split divergence and model accuracy, showing that models achieve high accuracy on random splits but low accuracy on MCD splits. The study reveals a strong negative correlation between compound divergence and model accuracy, indicating that baseline models capture superficial but not compositional structure. Varying compound divergence directly impacts accuracy, highlighting the challenge for ML architectures to generalize compositionally. Output-length experiment shows lower accuracy than expected due to test distribution differences from training. The study shows a correlation between compound divergence and model accuracy, indicating that baseline models struggle to generalize compositionally. Length ratios have a weaker impact on accuracy compared to compound divergence. Error analysis is conducted for split MCD 1. Error analysis was conducted for split MCD 1, showing accuracies between 29% and 37% on the test set. The systems made similar errors, with 68% occurring on the same samples. The most common error was the omission of a clause in the output, present in 43%-49% of test samples. For example, the best system ignored \"executive produced\" in one instance and \"female\" in another. We recreated the SCAN dataset to compare compositional generalization abilities of baseline systems. Compound divergence predicts mean accuracy, with systems reaching close to 100% accuracy for compound divergences up to around 0.2. CFQ is more complex than SCAN, with 443 rules compared to 38 rules in SCAN. The total number of rules used in generating SCAN is 38, compared to 443 rules in CFQ. Appendix G compares experiments with different atom distributions, showing lower accuracies but maintaining the correlation between accuracy and compound divergence. Finegan-Dollak et al. (2018) propose a query split to measure compositional generalization for semantic parsing to SQL, which is harder to learn than a conventional split. Lake & Baroni (2018) introduce the SCAN dataset, with further analyses by Bastings et al. (2018) and Loula et al. (2018). The curr_chunk discusses extending analyses on the SCAN data with CFQ providing richer annotations and a comprehensive score for assessing compositionality. The mathematics dataset contains 112M samples in 56 sub-tasks focusing on mathematical reasoning. The breakdown of generation rules per train sample allows for a more precise compositional generalization analysis. The curr_chunk discusses datasets related to CFQ and compositional generalization analysis, including ComplexWebQuestions and CLEVR. These datasets offer different approaches to dataset creation and semantic parsing, with a focus on capturing compositional structure and visual reasoning. In the area of visual reasoning, datasets like CLEVR capture structural information of questions and are linked to question patterns. Researchers explore generalization to new visual attributes using specific train-test splits. Various studies propose neural-symbolic architectures and compositional attention networks for structured learning on CLEVR data. Additionally, a visual question answering dataset with functional programs shows good performance with neural state machines. Specific train-test data splits are also utilized in these studies. The use of specific train-test data splits in visual data analysis is highlighted in recent studies. For instance, Agrawal et al. (2018) propose a split algorithm to maximize test concept coverage in the train set. Bahdanau et al. (2019) introduce a visual question answering dataset called SQOOP for testing learners' abilities. The additional annotation in CFQ allows for in-depth analysis of compositionality beyond accuracy metrics. Several ML approaches have been developed for semantic parsing, including Key-Value Memory Networks and end-to-end architectures. Battaglia et al. emphasize the importance of combinatorial generalization in achieving human-like abilities, while Andreas discusses measuring the compositionality of trained representations. In this paper, the authors present a new benchmark for compositional generalization in natural language understanding (NLU). They introduce a new dataset and a method for splitting the dataset to optimize performance. The study shows promising results with three baseline models in a realistic NLU scenario. The study introduces a new benchmark for compositional generalization in NLU, showing that state-of-the-art learning systems struggle to generalize compositionally even with large training data. The mean accuracy is strongly correlated with compound divergence, inspiring further research to improve learning systems' generalization capabilities through methods like unsupervised pretraining and diverse learning architectures. The authors suggest applying the benchmark to other domains like visual reasoning and exploring new compositionality benchmarks. The study aims to evaluate current architectures on compositionality benchmarks, focusing on natural language question-answering tasks in CFQ. They plan to expand the approach to include ambiguous constructs, negations, quantification, comparatives, additional languages, and other domains. The data item includes question text, answer, SPARQL query, tracked statistics, rules, and rule tree. During the development of the data generation pipeline, manual quality checks were conducted on the generated examples. Below is a random selection of 50 examples. During the development of the data generation pipeline, manual quality checks were conducted on a random selection of 50 examples from the final CFQ dataset. Some questions in the dataset have unnatural combinations of roles due to incorrect data in Freebase. The text discusses issues with data quality in the CFQ dataset, particularly related to Freebase data. It mentions how the phrasing of questions can be affected by Freebase modeling choices, leading to unnatural combinations of roles. The analysis shows that the most frequent answers in CFQ are entities related to movies in Freebase. Subsampling is used to balance the distribution of questions in CFQ, increasing the frequency of rarely used rules and decreasing commonly used ones. The text discusses data quality issues in the CFQ dataset related to Freebase data. It mentions how questions can be affected by Freebase modeling choices, leading to unnatural role combinations. Subsampling is used to balance question distribution in CFQ, increasing rare rule frequency and decreasing common ones. The difference between train and test examples is less observable in divergence-based splits compared to traditional compositionality experiments. The frequency of atoms and compounds in the train and test sets of the CFQ data diverges significantly, making it challenging to distinguish between the two sets. While atoms show alignment, compounds exhibit varying frequencies between the two sets, with some appearing in both but with different frequencies. The experiments were conducted using the tensor2tensor framework with hyperparameters tuned using a previous version of the dataset. Default hyperparameter sets were used, and errors in baseline models on CFQ were analyzed, categorized into SPARQL property clause error, SPARQL filter clause error, and malformed SPARQL query. The total number of test set examples with clause or filter errors is reported, along with the number of insertions, deletions, and substitutions in the model's output compared to the correct query. Property clause substitution errors are categorized based on the correctness of the property, subject, and object. The accuracy metric requires exact match between model response and correct answer. Errors include cases where clauses are in a different order or repeated, despite being equivalent in meaning. The error analysis of machine learning models reveals that a small percentage of errors occur in the test set. Anonymized instances where all models fail are examined, with a focus on a specific SPARQL query. The analysis of the training set suggests that sufficient information is provided for correct answers, despite some subqueries not being explicitly present in the training data. The analysis of the training set reveals that some subqueries, like \"sibling of Mx\" and \"Mx's parent\", occur frequently but are not explicitly present in the training data. The model 'forgets' to include the relation between the director and movie M1 in the inferred SPARQL query. The model 'forgets' to include the relation between the director and movie M1 in the inferred SPARQL query, as shown by the analysis of subqueries and their occurrence count during training. Various subqueries occur often, but \"edit and direct\" have not been shown together frequently. This lack of training instances may be why all systems fail on certain examples, despite the belief that a compositional learner should be able to generalize correctly. Examples include queries about male film directors influencing art directors, film producers editing and directing movies, screenwriters working on sequels, and Chinese male directors editing multiple movies. Figure 7 displays accuracy and divergence measurements for different splits of the SCAN dataset, based on existing work by Lake & Baroni (2018) and Loula et al. (2018). The splits include various experiments such as random splits, splits by action sequence length, and adding templates or more training examples. The data splits used are available on GitHub, and the authors analyze atom and compound divergences across these splits. In Figure 7, accuracy and divergence measurements are shown for different splits of the SCAN dataset, following previous work by Lake & Baroni (2018) and Loula et al. (2018). The point labels in the plot indicate experiment names, parameter values, and baseline system abbreviations. There is a strong correlation between accuracies and compound divergence of the data split, suggesting a relationship with baseline architectures. For experiments on the SCAN dataset, accuracy drops faster with increasing compound divergence. The 'primitive<jump>' experiment highlights a problematic scenario where the jump command is poorly represented in training data but occurs in all test examples. This results in low accuracies and higher atom divergence values. Comparisons to other experiments show varying levels of accuracy. In contrast to 'jump', the action 'turn left' is generated by other inputs, leading to higher accuracy. The experiment also shows lower atom and compound divergence, covering a larger portion of the data in the train set. Different systems achieve 100% accuracy on the few-shot task, while transformer models struggle with the length split. Figures 8 and 9 demonstrate a strong correlation between accuracy and compound divergence for various training sizes. As training size increases, the difference in accuracies between sizes decreases. Figures 10 and 11 show that accuracy improvement levels off around 80k for CFQ and 6k for SCAN, suggesting further increasing training size may not significantly improve performance. Further increasing the training set size may not significantly improve performance on compositionality experiments. The logical form used is based on the description logic EL with additional concept and role constructors. These constructors are determined by the generation rules of the CFQ dataset. New strings are generated using a special function to create unique variables for SPARQL constraints. The format of rule types for generating the CFQ dataset includes conventions for variable and concept names, function names, and string literals.Variables are prefixed with '$', concept names are in camel case, and function names for logical forms are also in camel case. String literals are enclosed in single quotes and functions for converting logical forms to SPARQL are written in lowercase with underscores. The CFQ grammar uses unification-based rules to generate pairs of strings and logical forms. It follows a syntax similar to Prolog extension GULP 3.1, with support for disjunction, negation, absence, and default inheritance of features. The rules are notated as variations of context-free phrase-structure rules. The CFQ grammar uses unification-based rules to generate pairs of strings and logical forms, following a syntax similar to Prolog extension GULP 3.1. The rules are variations of context-free phrase-structure rules, with each syntactic non-terminal and terminal augmented with feature lists in parentheses. Features are represented as attribute-value pairs separated by a colon, and shared values in feature structures are represented through variables. The CFQ grammar utilizes unification-based rules with feature lists in parentheses to generate pairs of strings and logical forms. Features are represented as attribute-value pairs separated by a colon, with shared values represented through variables. The notation allows for default inheritance of features and emphasizes the relationship to context-free phrase-structure rules. The CFQ grammar uses unification-based rules with feature lists to generate strings and logical forms. Features are represented as attribute-value pairs, allowing for default inheritance and emphasizing the relationship to context-free phrase-structure rules. The rules include disjunctive, negated, and absence of features specifications. The CFQ grammar uses unification-based rules with feature lists to generate strings and logical forms. Features are represented as attribute-value pairs, allowing for default inheritance. Logical forms are represented using a variation of description logic. Unification of logical forms is based on achieving structural concept equality after variable replacement. The CFQ knowledge rules output expressions representing known facts, which can be used as preconditions for other rules. These rules define a knowledge base denoted as KB CFQ. Knowledge in CFQ is represented in a specific form. The CFQ knowledge base, KB CFQ, contains knowledge represented as predicates with logical forms or raw strings. Knowledge rules in CFQ transform logical forms based on preconditions and output expressions. The CFQ knowledge base, KB CFQ, contains predicates with logical forms or raw strings. CFQ resolution rules transform SPARQL expressions based on knowledge preconditions. The rules index describes the format for applying these rules, which involve replacing variables with logical forms, strings, or expressions. The generation algorithm produces triples consisting of a question, logical form, and SPARQL query. The generation algorithm produces triples of question, logical form, and SPARQL query using a top-down and bottom-up approach. The process involves applying grammar and inference rules to create pairs of question and logical form. The algorithm applies grammar rules top-down to generate a syntactic parse tree, then bottom-up to create text, logical form pairs. Inference rules are greedily applied after each grammar rule, ensuring interleaved execution. This process continues until a question, logical form pair is generated for the S nonterminal, marking the start of the second phase of the algorithm. After generating a question, logical form pair for the S nonterminal, the algorithm proceeds to the second phase where resolution rules are applied to create a corresponding SPARQL query. This phase involves top-down application of rules to transform the logical form into a sequence of text literals representing the query, followed by bottom-up construction of a rule application DAG. The resolution phase is more deterministic compared to the grammar phase. During the resolution phase, CFQ resolution rules ensure each question yields a single SPARQL query. To maintain consistency, an arbitrary deterministic ordering is applied when multiple valid orderings exist. The final SPARQL query is normalized by alphabetically sorting clauses and re-numbering variables. This normalized question, logical form, SPARQL query triple is added to the CFQ dataset. During the resolution phase, CFQ resolution rules ensure each question yields a single SPARQL query. An example-independent rule, JOIN_BY_LOGICAL_FORM, describes the handover process between the grammar and resolution phases in CFQ. This rule is included in the rules list for every example in CFQ. The JOIN_BY_LOGICAL_FORM rule is included in the rules list for every example in CFQ and is the head of the rule application tree. The semantic parsing task would need to find the set of rules producing the desired input text, but the set of examples generated by exhaustively combining rules is infinite or prohibitively large. To address this, a subset of examples is chosen to maximize the diversity of rule combinations for comprehensive measurement of compositional generalization. To maximize rule combination diversity in CFQ, a large sample set is generated with random rule applications. A subset is then selected to maximize entropy of subgraph distribution using a greedy algorithm. The subsampling process in CFQ assigns elements to a set while maximizing entropy at each step, starting with examples of smallest complexity and progressing to larger levels. The maximum number of examples per level is capped to ensure a uniform distribution, and complexity levels are limited to maintain natural questions. Examples of generated questions at varying complexity levels are shown in Table 1. Figures 12 through 14 depict the rule application DAG for the question \"Who directed [entity]?\", illustrating the combination of grammar, inference, and knowledge rules to generate text and logical forms, as well as the use of resolution rules to create the SPARQL query. Nodes in the DAG represent rule applications, while edges indicate dependencies among the rules. The DAG is normalized to ensure a consistent representation of rule combinations across examples. Only \"minimal dependencies\" are included in the DAG, meaning edges are only produced for rules that strictly depend on each other. This normalization, along with deterministic rule application measures, allows for meaningful comparison of entropy and divergence across subgraphs. The normalized rule application DAG for \"Who directed [entity]?\" includes ObjectUndergoerVerb and E1 = Entity('?E1'). Entity placeholders are initially used in question generation and replaced with specific entities later. The DAG describes the process of generating questions with placeholders or with final entity MIDs. The DAGs for rule application are structurally identical, differing only in entity-related rules. Subgraphs in Figure 15 illustrate sampling and weighting of compounds, with examples of linear and non-linear subgraphs highlighted. Weight calculation for each sample involves subgraph occurrences and empirical probabilities. The weight of a subgraph G is determined by its occurrence as a supergraph over the full sample set. The weight is computed based on the complement of the maximum empirical probability of G being a supergraph of g. This ensures that the weight reflects the interestingness of the occurrence of G in the sample. The weight of a subgraph G is determined by its occurrence as a supergraph over the full sample set, ensuring that it reflects the interestingness of the occurrence of G in the sample. If G occurs in all cases, then w(G) will be high in all samples where it occurs, impacting compound divergence calculations."
}