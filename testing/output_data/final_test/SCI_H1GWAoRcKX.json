{
    "title": "H1GWAoRcKX",
    "content": "In this paper, the focus is on reducing the computational cost of video classification by using a Teacher-Student network. The teacher examines all frames in the video, while the student only looks at a small fraction. The student is trained to minimize the difference between its final representation and the teacher's, or the difference between their predicted distributions. This approach allows for a smaller student network with fewer computations. The proposed student network in the Teacher-Student model reduces inference time by up to 30% with minimal performance drop. Video content is prevalent on the internet, impacting education, entertainment, and communication. Automatic video processing tasks include identifying activities, generating textual descriptions, and answering questions. Large-scale datasets enable training complex models with high memory and computational requirements. With a focus on video classification, the challenge lies in balancing the need for complex models with high computational requirements and the demand for running them on low-power devices with strict constraints. Current models treat videos as sequences of images processed by RNNs, but there is a need to reduce computational costs at inference time for longer videos. To reduce computational costs for processing longer videos in video classification, a distillation approach is proposed. A teacher network processes all frames in the video to compute a representation, while a student network is trained to process only a few frames and produce a similar representation. This allows for a less expensive model to be used at test time, balancing the need for complex models with low-power device constraints. The distillation approach involves training a student network to produce a representation similar to that computed by a teacher network, by minimizing the difference between their output distributions. This allows for faster video processing at inference time. Two methods of training the Teacher-Student network are experimented with, including Serial Training where the teacher is trained independently before the student is trained to match the teacher. The teacher and student are trained jointly using classification and matching loss in Parallel Training. The smaller student network reduces inference time by up to 30% while maintaining classification performance close to the teacher network. The YouTube-8M dataset contains videos with an average length of 200 seconds, represented by frames. The authors proposed a baseline model using LSTM to encode one-second frame representations of videos. Various classification models were evaluated on the dataset, exploring feature aggregation, label interactions, and non-linear units. A hierarchical model's performance was comparable to a single model. The authors proposed a hierarchical model with performance comparable to a single best performing model. They used Knowledge Distillation to train a shallow student network to mimic a deeper teacher network, ensuring similar output representations. Knowledge Distillation is a technique used to train a student network to mimic a deeper teacher network by using soft targets generated by the teacher. Variants of this technique include using intermediate representations learned by the teacher as additional hints. Knowledge Distillation has been applied in various domains, such as pruning networks and model compression for object detection and image classification. The Teacher-Student network for video classification aims to identify classes in a video by processing frames. This approach differs from previous works by varying the number of time steps processed. The goal is to predict the probability of each class using a neural network that analyzes all frames. The focus is on designing a simpler network for video classification that leverages information from all frames during training but only looks at a fraction of frames during inference. This is achieved through a teacher-student network where the teacher has access to more frames than the student. The model assumes videos are sequences of equal-sized blocks, each containing a sequence of frames. The model for video classification utilizes a teacher network that encodes video frames into blocks and then into the video itself. The teacher network processes all frames to compute an encoding fed into a neural network for classification. The network parameters are learned using a multi-label classification loss. The student network in video classification processes every j th frame of the video to compute a representation, using a hierarchical recurrent neural network. The student is trained to minimize the squared error loss between its representation and the teacher's, with shared output layer parameters. The student network is trained to minimize the difference between its representation and the teacher's, with shared output layer parameters. This includes ensuring similarity in intermediate representations and class probabilities predicted by the teacher and student. The student network is trained to minimize the difference between its representation and the teacher's, with shared output layer parameters. This involves training the student to mimic the teacher's representations without focusing on final classification loss. In the second stage, the student is trained to minimize representation loss and classification loss simultaneously, improving performance. Performance of proposed Teacher-Student models is compared using different StudentLoss variants. The text chunk discusses training a student with different loss functions, including learning from soft targets obtained from the teacher. It also mentions experimenting with training the teacher and student in parallel. The dataset, hyperparameters, baseline models, and the effect of different loss functions and training methods are described. The original validation set was used as a test set, and results were reported on it. For training the models, 48,163 examples were randomly sampled for validation data. The Adam Optimizer with a learning rate of 0.001 was used, along with a batch size of 256. Models were trained for 5 epochs with dropout (0.5) and L2 regularization penalty of 2. The best model was selected based on validation performance, with no benefit observed beyond 5 epochs. The teacher network had a value of l (number of frames per block) set to 20, while the student network had l set to 5 or 3. The student network had a value of l set to 5 or 3, and different metrics were used to evaluate model performance, including GAP and mAP. Various values of k frames were considered in the video sequence, ranging from 6 to 30, to compare the student networks with other models. The student networks were evaluated with different models processing various numbers of frames in the video sequence (6 to 30). Models included Teacher-Skyline, Uniform-k, Random-k, First-k, Middle-k, Last-k, and First-Middle-Last-k. The experiments compared different frame selection methods for the student network, with Uniform-k showing strong performance. The results indicated that equally spaced k frames outperformed other baselines, especially when k is small. This led to the decision to feed equally spaced k frames to the student network for all experiments. Uniform-k was considered a strong baseline for comparison. The Uniform-k method effectively reduces the number of frames processed. Teacher-student models outperform this baseline, especially with limited training data. Comparing Serial and Parallel training shows little difference in performance. After comparing the performance of teacher models in Parallel and Serial training setups, it was observed that the Parallel teacher reaches similar performance to the Serial teacher after 3-4 epochs. The Parallel student then learns from this trained teacher and almost matches the performance of the Serial student. This trend holds across different loss function combinations. Additionally, the representations learned by the teacher and student were evaluated by visualizing TSNE-embeddings of the top-5 classes in the Youtube-8M dataset. The TSNE-embeddings of teacher and student representations for the same video in the Youtube-8M dataset showed close similarity. Matching intermediate representations did not provide any benefit according to Table 2. The aim of the study was to minimize computational cost and time. The study aimed to minimize computational cost and time by reducing the number of FLOPs at inference time. Results showed a 30% drop in inference time and a 90% reduction in FLOPs when k=30, without affecting model performance. The proposed teacher-student model effectively reduces computational cost and time for video classification through distillation. The study introduced a teacher-student model for video processing tasks, where the student network processes only k frames of the video. Different loss functions are used to ensure that the student's final representation and output probabilities are similar to the teacher's. The proposed model outperforms baseline and skyline models, reducing computational time and cost significantly. Evaluation on the YouTube-8M dataset shows a 30% reduction in computation time with similar performance to the teacher network. Future work includes testing the model on other video processing tasks. Future work includes experimenting with different teacher networks for video processing tasks such as summarization, question answering, and captioning. Additionally, there is a plan to independently train an agent to select the most favorable frames of a video instead of using equally spaced frames."
}