{
    "title": "SJx4Ogrtvr",
    "content": "Edge intelligence, particularly binary neural networks (BNN), has gained attention in the AI community for reducing computational costs and model size. Despite these benefits, there is still a performance gap compared to full-precision neural networks with ReLU activation, which we attribute to the geometry of BNNs. The comparison between full-precision neural networks with ReLU activation and their binarized counterparts suggests random bias initialization as a remedy for activation saturation. This leads to improved BNN training, confirmed by numerical experiments. ReLU-style activation functions are widely used in deep neural networks for various tasks like computer vision and natural language processing, known for their accuracy and convergence speed. Binary Neural Networks (BNNs) use low-bit quantization for deep neural networks on edge devices. BNNs only keep weight signs and compute activations using the Sign function. They use Straight-Through-Estimator (STE) for backward propagation. BNNs with hard hyperbolic tangent activation are compared to full-precision networks with htanh activation for performance improvement. The bias initialization is crucial for mimicking ReLU geometric behavior in networks with htanh activation, challenging the common practice of deterministic bias initialization for neural networks. This conclusion applies to BNNs using STE and other saturating activations like hyperbolic tangent and sigmoid. The novelties include analyzing the geometric properties of ReLU and htanh activation, proposing a bias initialization strategy for bounded activations, and supporting findings with experiments. The study proposes random bias initialization as a remedy for bounded activations like htanh, aiming to reduce the performance gap with ReLU activations in full-precision networks. This strategy also benefits Binary Neural Networks (BNNs) and addresses the lack of focus on bias term initialization in neural networks. Previous works have highlighted the advantages of ReLU activation in deep neural networks, while hyperbolic tangent has limitations in training deep networks. Since AlexNet, most successful neural network architectures use ReLU activation or its variants. The reason for ReLU's superior performance remains an open question. Automatic search techniques have found novel activation functions with an asymmetric saturating regime similar to ReLU. Penalized tanh activation, proposed by Xu et al., achieves performance comparable to ReLU in CNNs. Similar ideas have been explored in related works. In related works, ideas similar to ReLU activation have been explored, such as adding random noise to improve performance. Binarized neural networks limit weights and activations to -1 and +1, reducing memory usage and computational cost significantly. Initial work on BNN was tested on the VGG-7 architecture, showing promising results for CIFAR 10. The BNN was tested on the VGG-7 architecture for CIFAR 10 dataset, showing a performance gap compared to full-precision with ReLU activation. XNOR-Net introduced the idea of BNN and suggested using non-Binary activation after binary convolution layer for better training. Later, PReLU was used instead of ReLU in XNOR-Net to improve accuracy. However, storing full-precision activation maps during inference makes memory usage larger than pure 1-bit solutions like vanilla BNN. Neural networks are trained using back-propagation with forward pass and backward propagation. Weight vectors are assumed to be normalized. Neuron responses in each layer are computed by projecting input data points onto weight matrices. During backward propagation, gradients are updated using ReLU activation function to allow gradients to propagate and update. ReLU activation function allows gradients to propagate and update the hyper-plane, with three key properties that set it apart from other activations: diversity of activated regions, equality of data points, and equality of hyper-planes at initialization. Weight initialization techniques like Xavier and MSRA help maintain output variance. The activated regions of hyper-planes depend on the direction of the weight vector, allowing for different hyper-planes. The ReLU activation function allows gradients to update hyper-planes with key properties like diversity of activated regions, equality of data points, and hyper-plane equality. This ensures that backward gradients from all data points can pass through the same amount of activation function, speeding up convergence and facilitating model optimization. The htanh activation function has less diverse activated regions compared to ReLU, covering only areas close to the origin. Data points closer to the origin activate more hyper-planes, leading to unfair treatment of data points. The imbalance in backward gradients due to data points close to the origin activating more hyper-planes affects model generalization. A simple initialization strategy is proposed to address this issue for activation functions like ReLU and htanh. The proposed initialization strategy aims to improve activated region diversity for the htanh activation function by using bias initialization with a uniform distribution. This approach helps achieve data equality during backward propagation but may lead to hyper-plane inequality if biases are not set to zero. The optimal value of the hyper-parameter \u03bb is crucial in balancing hyper-plane equality and diversity. The optimal value of \u03bb is crucial for balancing hyper-plane equality and data equality. Empirical experiments suggest that a \u03bb value of around 2 works well with batch normalization. Small \u03bb values also improve the performance of ResNet architecture. The proposed bias initialization method is evaluated on CIFAR-10 using VGG-7 and ResNet architectures. In full-precision experiments, BatchNorm layers were moved back to their original position, improving the baseline model's error rate from 9.0% to 6.98%. This is close to the best CIFAR 10 error rate of 6.50% reported on VGG-9 architecture. VGG-9 is kept as the baseline for full-precision VGG due to its lower parameter count. The training recipe for VGG involved using SGD optimizer with momentum and weight decay. More training epochs were used to address slow convergence with htanh activation. Bias initialization strategy reduced performance gap between htanh and ReLU activation. Similar effects were seen with ResNet architectures. Results showed bias initialization improved htanh accuracy. Binary training with STE showed similar results to htanh activation. Validation error rates were summarized in tables. In Binary VGG-7 experiments, accuracy gap between full-precision network with ReLU activation and BNN was reduced. The bias initialization strategy effectively reduced the accuracy gap between full-precision network with ReLU activation and BNN from 4% to 1.5% on binary ResNet architecture, even when the full-precision model under-fits on CIFAR10 data."
}