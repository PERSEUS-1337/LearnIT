{
    "title": "B1l4SgHKDH",
    "content": "Text generation in NLP tasks like summarization, dialogue, and machine translation often relies on locally normalized models predicting one word at a time. However, these models suffer from exposure bias. This study explores un-normalized energy-based models (EBMs) operating at the sequence level, leveraging pretrained contextual representations like BERT and RoBERTa. Experiments show that residual EBMs have lower perplexity than locally normalized models, and generation through importance sampling is efficient. Locally normalized baselines are efficient for text generation, but have drawbacks such as the need to specify token generation order. Large neural auto-regressive models are dominant in parametric text generation, trained efficiently via maximum likelihood. They rely on local normalization and conditional distributions for each token in the sequence. Despite drawbacks, these models can generate high-quality samples. Energy-based models (EBMs) offer a more general framework for text generation, addressing issues like exposure bias and lack of long-range coherency. They do not require local normalization and instead define an energy function over the entire input sequence, shaping it during training to score the input as a whole. EBMs are ideal for modeling text as they can score the entire input at once and are not affected by label bias. Energy-based models (EBMs) are beneficial for text generation as they can score the entire input at once, avoiding label bias. However, their limited application in text generation is due to the intractability of sampling from the model and maximum likelihood training. The challenge lies in shaping the energy function by adjusting model parameters to decrease energy at training data points and increase it at other data points. Gradient-based MCMC methods and Gibbs sampling are impractical for text generation, making it difficult to generate negatives efficiently. Recent research has focused on training discriminators to distinguish human written text from language model generations. Bakhtin et al. (2019) found that discriminators can generalize well to weaker language models when training/test corpora match. They also noted that discriminators are not robust to random perturbations and operate in the \"residual\" space of the language model. Grover et al. (2019) proposed a method to \"de-bias\" a generator by training a discriminator. This work builds upon these findings by formalizing the residual interpretation by Bakhtin et al. The residual interpretation by Bakhtin et al. (2019) formalizes a generative model using a locally normalized language model and an energy function. This formulation has multiple benefits including leveraging advancements in language modeling, efficient training with conditional noise contrastive estimation, and enabling efficient evaluation and generation. The formulation by Hyv\u00e4rinen (2010) enables efficient evaluation and generation through importance sampling. It allows estimating perplexity of the residual EBM, leading to improved generation ability compared to auto-regressive language models. EBM generations are preferred by humans, showing superiority over strong baselines in terms of perplexity and human evaluation. Training in machine learning has a long history with key challenges including mining for good negatives and sampling from the distribution induced by the model. Gradient-based MCMC approaches are not applicable for discrete inputs like in text applications, leading to the investigation of other methods such as Gibbs sampling and variants of auto-encoders. Our approach in text generation involves a generative model with parameters tuned to match data distribution, different from discriminative reranking methods. It is also distinct from sequence level training objectives, focusing on improving baseline models. Energy Networks have been utilized for sequence modeling. Energy Networks have been used for sequence modeling, with prior works using residual forms and NCE for training the model. Different architectures like LSTM and CNN-LSTM have shown gains in speech recognition. Our work introduces new bounds for log-probability under the joint model, improving perplexity with a focus on conditional generation. The residual EBM approach allows for natural integration of BERT in language modeling, showing empirical success. BERT is effectively used for language modeling, outperforming current baselines. Generative Adversarial Networks relate to Energy-Based Models, with the pretrained locally normalized language model acting as a fixed generator. Other works aim to improve sampling from the generator using the discriminator. Our work adapts the use of the discriminator to de-bias the pretrained generator for text generation. Our work adapts the conditional noise contrastive estimation (NCE) objective to a residual model energy function for text generation. We apply this method to produce whole sequences at once using a pretrained auto-regressive language model as the noise distribution. The focus is on conditional generation of discrete sequences with a given prefix, aiming to model the probabilities of generating sequences longer than the prefix. The partition function \u03b8 (x 1 , \u00b7 \u00b7 \u00b7 , x p ) is a normalizing factor in the joint model. It is intractable to compute due to exponential growth with sequence length. Training aims to learn parameters of the energy function to align the joint model distribution with the data distribution. Maximum Likelihood Estimation (MLE) requires samples from the model distribution when the partition function is intractable. Training large bidirectional transformer models for text applications is costly with traditional methods like MCMC or mean field inference. Instead, Noise Contrastive Estimation (NCE) is used to train the residual energy function efficiently. Training the residual energy function efficiently is done using Noise Contrastive Estimation (NCE), specifically its conditional version. NCE trains a binary classifier on the log-probability scores difference between the model distribution (P\u03b8) and a noise distribution (PLM). The objective function involves a positive sequence from the training set and a negative sequence from PLM, reducing to log P\u03b8 - log PLM = -E\u03b8. The training of the energy function involves training a binary classifier to distinguish between real text and text generated by a language model. The goal is to assign negative energy to real data and positive energy to machine-generated data. The objective reaches its maximum when the language model distribution matches the data distribution. Theorem 2 states that two estimators are derived for the log-partition function based on previous work. These estimators can be used to estimate the lower and upper bounds of the partition function, but they are only true asymptotically. Theorem 2 provides estimators for the log-partition function, giving lower and upper bounds asymptotically. To improve numeric stability, a leave-one-out strategy is used for estimating Tn-1. Top-k Joint Sampling algorithm is used for sampling from P LM, with step-wise probabilities calculated using importance sampling. The basic P LM distribution is adjusted by the probability assigned to token x t by the energy function, with additional marginalization over subsequent tokens up to the horizon T. Log probability can be calculated by exhaustive enumeration at t = T. Generating from the joint model is challenging, and auto-regressive sampling is expensive and impractical. Generating efficiently from the joint model involves using self-normalizing importance sampling. Sampling proceeds by first sampling from the auto-regressive language model and then resampling according to the energy function, with an optional top-k constraint to improve sample quality. In this section, the experimental setup and results of using the residual EBM for text generation are described. Two datasets are considered: the Toronto Book Corpus and CC-News. The former dataset consists of fiction books in 16 genres, totaling half a billion words, while the latter is a subset of the CommonCrawl news dataset, totaling 16 billion words. The book corpus is more challenging due to its diverse range of styles and topics compared to CC-News. The experiments use a prefix of 120 tokens and generate the following 40 tokens. For training joint models, 16/128 samples per prefix for CC-News/Book Corpus are generated offline and sampled uniformly at training time. Baselines include a transformer language model with 12 layers, h = 16, d model = 1024, d f f = 4096. The joint model parameters are the sum of the base LM and energy network parameters. Two additional baselines with the same parameters as the joint model are considered. P \u03c6 is an auto-regressive language model trained with parameters. The second baseline model, BALM, is an auto-regressive language model with 12 layers, h = 16, d model = 1568, d f f = 6272, trained with token level cross-entropy loss. The residual EBM architecture consists of two versions based on transformers, one using causal self-attention and the other derived from a unidirectional transformer. The second version is bi-directional (BIT) with an energy function computed by projecting hidden states. Two variants, BIT-BASE and BIT-LARGE, are considered. Parameters are initialized with a trained BERT, and external data is marked with *. The model fine-tunes bidirectional pretrained models for text generation. UNIT is also considered to compare to the RALM baseline. The model differs in training parameters and includes local normalization. Trained on 8 DGX nodes with Nvidia V100s using Adam optimizer. Top-k sampling with k=10 for generation. Residual EBMs with causal attention outperform baseline models in perplexity on both datasets. Non-residual baseline performs similarly. By using an EBM approach, the joint model JOINT BITRANSF-BASE achieves better performance than baselines and JOINT UNITRANSF. Initializing from pretrained bi-directional transformers RoBERTa-Base and RoBERTa-Large further improves performance. Deeper language models like BALM-24L result in lower perplexity, but training JOINT BITRANSF-BASE on the residual of a deeper language model yields even lower perplexity with fewer parameters. The evaluation protocol for the EBM approach shows that despite fewer parameters, it achieves lower perplexity. The estimated bounds align well with exact values at the final generation step, indicating reasonable PPL estimates. However, better perplexity results do not always translate to better generations due to approximation errors in sampling from the joint distribution. The human evaluations compared generations from the residual EBM model to baseline language models, showing that the generation quality of JOINT BIT-BASE and JOINT BIT-LARGE is better. The preference rates confirm that the joint model is preferred between 56% and almost 60% of the times, regardless of the model variant. Interestingly, there is no clear preference for BALM over base LM or JOINT UNIT over BASE LM. In analyzing the results obtained, it was found that despite lower perplexity scores, JOINT UNIT is not preferred over BASE LM. It is suggested that generation artifacts in unidirectional scoring functions and auto-regressive models may overshadow improvements from perplexity gains. The analysis includes checking the number of samples used for perplexity estimates, assessing repetition levels in the joint model compared to the base language model, and comparing statistics of model and data distributions. The upper estimate of perplexity stabilizes beyond 20,000 samples, and density plots show differences in log-probability scores between the base language model and the joint model. The joint model outperforms the base language model in fitting real samples. Despite improvements, the joint model still exhibits repetition of phrases. It shows a slightly higher percentage of unique n-grams compared to the baseline model. Matching the data distribution is crucial for the model's performance. The joint model, when compared to the baseline language model, better matches the real data distribution. The histogram of samples from the joint model closely aligns with the real data distribution, with a smaller difference in means compared to the baseline model. The residual EBMs have shown strengths in simplicity, efficiency, and improved perplexity scores, but there are limitations to consider for future research. The current approach uses negatives generated from a pretrained auto-regressive language model to make training efficient. The model's performance is dependent on the quality of the base language model from which samples are drawn. If the base language model is poor, the generation from the joint model will also be poor, and training will be trivial. The energy function in the loss equation of the base language model can minimize loss by modeling either true data or negative samples, leading to importance sampling. However, a poor base language model results in inefficient search due to the need to sample a large number of negatives. This work assumes a strong base language model for optimal performance. The EBM trained on the residual of a pretrained autoregressive language model scores sequences holistically using an energy function. Training involves a binary classification task between positives and pregenerated negatives, resulting in a model with lower perplexity than the base language model. This approach offers an efficient way to finetune a large language model. In this subsection, the joint model BIT-BASE is factorized auto-regressively and compared with BASE LM. Approximations are made to estimate probabilities, and future research will explore different ways to generate negatives and loss functions for text generation applications. The joint model BIT-BASE is compared with BASE LM by estimating probabilities for text generation. The Global Compact aims to establish benchmarks for migration practices and reduce irregular flows. The Global Compact contains ten guiding principles for migration, emphasizing fair and sustainable development. Optimization settings and human evaluation experiments were conducted for text generation. In text generation experiments, a qualification task was performed to assess the quality of received ratings. Qualified turkers were used for further evaluation, with attention checks conducted during the actual experiment. Examples of when BASE LM outperforms BALM and vice versa were presented based on human evaluation."
}