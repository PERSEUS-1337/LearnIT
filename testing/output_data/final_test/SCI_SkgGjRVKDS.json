{
    "title": "SkgGjRVKDS",
    "content": "Batch Normalization (BN) is a widely used technique in Deep Learning, but its performance degrades with small batch sizes, limiting its use in tasks like computer vision. Modified normalization techniques have been proposed, but they either don't fully restore BN's performance or introduce additional operations. This paper discusses extra batch statistics in BN's backward propagation that can affect deep neural network training. A novel method, Moving Average Batch Normalization (MABN), is proposed to address these issues. Moving Average Batch Normalization (MABN) is a method that can restore the performance of Batch Normalization (BN) in small batch cases without adding extra nonlinear operations. It has been proven effective in computer vision tasks like ImageNet and COCO, addressing the challenges of using BN with extremely small batch sizes. The code for MABN is available on GitHub. Many modified normalization methods have been proposed to address issues with batch normalization in small batch cases, divided into two categories: some correct batch statistics but fail to fully restore performance, while others use instance-level normalization to avoid batch statistics instability and partially restore performance. In small batch cases, instance-level normalization methods struggle to meet industrial needs due to increased computational complexity. Batch normalization, on the other hand, is a linear operator that can be merged with convolution layers during inference, resulting in faster inference times. The challenge lies in restoring batch normalization performance in small batch training without introducing nonlinear operations in the inference procedure. In this paper, the formulation of vanilla BN is analyzed, revealing 4 batch statistics involved in normalization during FP and BP. The additional 2 batch statistics in BP are related to gradients, playing a role in regularization. The variance of batch statistics associated with gradients in BP is larger due to small batch size, impacting BN performance. A new normalization method, Moving Average Batch Normalization (MABN), is proposed to address this issue. The Moving Average Batch Normalization (MABN) method addresses small batch issues by replacing batch statistics with moving average statistics. The modification reduces the number of batch statistics, centralizes convolution kernel weights, and utilizes a renormalizing strategy to prevent training collapse. The theoretical analysis proves the stability of the modified normalization form over the vanilla form. The Moving Average Batch Normalization (MABN) method is proven to be more stable than the vanilla form. It shows effectiveness in various vision datasets and tasks, achieving comparable performance with small batch sizes. MABN has the same inference consumption as vanilla BN and has been validated through ablation experiments. Batch Renormalization (BRN) and EvalNorm correct batch statistics during training and inference, respectively, to improve BN performance. Instance Normalization (IN), Layer Normalization (LN), and Group Normalization (GN) handle small batch issues by computing mean and variance across multiple GPUs. These methods do not fully solve the problem and require significant resources. Some approaches normalize convolution weights to address the issue. Weight Standardization (Qiao et al., 2019) centralizes weights before dividing by standard deviation, often combined with Group Normalization (GN) for small batch cases. Batch Normalization (Ioffe & Szegedy, 2015) normalizes feature maps using batch statistics \u00b5 Bt and \u03c3 Bt, with scaling and shifting parameters \u03b3, \u03b2. Batch Normalization (BN) involves using batch statistics \u00b5 Bt and \u03c3 2 Bt in backward propagation (BP). The formulation of BP in BN includes computing partial gradients using g Bt and \u03a8 Bt. Ioffe & Szegedy (2015) suggest normalizing feature maps using mini-batches in stochastic gradient training to estimate mean and variance for each activation. This simplification is practical for stochastic optimization. The batch statistics \u00b5 Bt and \u03c3 |\u0398 t ] are crucial for estimating population statistics accurately in order to regulate model gradients during weight updates. The variance of batch statistics increases significantly with smaller batch sizes. Regular batch statistics are considered a good approximation for normalization in training. During training on ImageNet, small batch statistics are unstable and contain notable errors compared to regular batch statistics. This instability can lead to slow convergence and inconsistency between training and evaluation procedures, resulting in poor model performance. During training on ImageNet, small batch statistics are unstable, leading to poor model performance on evaluation data. To restore performance, the key is to solve the instability of small batch statistics. Two ways to handle this are using moving average statistics to estimate population statistics and modifying the normalization formulation to reduce the number of statistics. Simple moving average statistics (SMAS) and exponential moving average statistics (EMAS) are more stable than batch statistics under mild conditions. The proof of theorem 1 in appendix A.1 shows that moving average statistics have lower variance than batch statistics. EMAS is better than SMAS with large momentum \u03b1. Using SMAS and EMAS require different conditions, but under the assumption of a small learning rate, SMAS can be used. SMAS and EMAS can replace batch statistics with minimal bias. The effectiveness of moving average statistics as substitutes for small batch statistics is proven by equation 11 in theorem 1. Batch Renormalization replaces batch statistics with exponential moving average statistics during training. However, small batch statistics associated with gradients remain during backward propagation, hindering the restoration of performance. Batch Renormalization replaces batch statistics with exponential moving average statistics during training to stabilize the training procedure in small batch cases. By modifying the normalization of feature maps X using EX 2, the backward propagation is improved with only two batch statistics left during forward and backward propagation. The modification reduces the variance of the gradient by eliminating batch statistics g B during backward propagation. This adjustment theoretically proves to be beneficial in reducing instability in the normalization layer compared to the vanilla normalizing form. The modification in Batch Renormalization improves backward propagation by reducing the variance of the gradient through the elimination of batch statistics. Centralizing weights of convolution kernels, known as Weight Centralization (WC), compensates for the absence of centralizing feature maps. Further ablation studies are conducted to evaluate the effectiveness of WC. The ablation study clarifies that Weight Centralization (WC) has little benefits for vanilla normalization but significantly improves modified normalization. The proposed method MABN utilizes a clipping and renormalizing strategy to prevent collapse during training. The formulation includes EMAS and SMAS computations, with a renormalizing parameter set for ImageNet and COCO datasets. The main results of MABN on ImageNet, COCO, and Cityscapes datasets are presented in this section. Experiment results can be found in appendices B.2, B.3, and B.4 respectively. Evaluation of computational overhead and memory footprint is also included. The proposed method is tested on ImageNet classification datasets with ResNet-50, comparing with other normalization methods. Instance-level normalization counterparts are not included due to their non-linear nature during inference time and failure to match BN performance. The performance of BN, BRN, and MABN was compared on ImageNet, with MABN showing the best results regardless of batch size. Ablation experiments on ImageNet showed that MABN outperformed BRN in restoring performance, with EMAS showing some improvement but still falling short. The modified normalization outperforms BRN in small batch cases, but still fails to completely restore performance. EMAS in FP further reduces error rates, but still falls short of complete restoration. SMAS in BP finally fills the remaining gap, almost completely restoring performance in small batch cases. The experiments conducted on Mask R-CNN benchmark using FPN followed basic settings. Training networks from scratch independently led to proven results. The condition for uniformly bounded \u03be t implies convergence. The memory size and t relationship were established, proving the effectiveness of the approach. The experiments on Mask R-CNN benchmark using FPN showed proven results. The variance of partial gradients w.r.t. inputs x is analyzed, showing that small batch statistics differ from regular batch statistics. The variance of \u03c3 and \u03c7 are considered fixed numbers due to their small values. The experiments on ImageNet with 8 GPUs involved training models with a gradient batch size of 32 images per GPU. To simulate small batch training, samples on each GPU were split into groups based on the normalization batch size. Weight initialization and decay details were provided, along with the training iterations and learning rate adjustments. The models are evaluated by top-1 classification error on center crops of 224 \u00d7 224 pixels in the validation set. In MABN, the momentum \u03b1 = 0.98 is used. Ablation studies are conducted for instance segmentation. MABN is used on heads, and a mask-rcnn baseline is built using a Feature Pyramid Network backbone. Training involves 2\u00d7 iterations with 4conv1fc instead of 2fc as the box head. Batch statistics in normalization layers are used on head during the first 10,000 iterations when training models with MABN. SyncBN is compared with MABN based on one-stage pipeline. In the experiments, MABN is compared with SyncBN on one-stage models using RetinaNet benchmark for object detection. Results show MABN performs comparably to SyncBN and outperforms BN on COCO dataset. Semantic segmentation is evaluated on Cityscapes dataset using PSPNET baseline with ResNet-101 backbone. The backbone model is re-pretrained on Imagenet due to centralized weights of convolutional kernel using MABN. Fine-tuning involves a learning rate increase for 3 epochs followed by a \"poly\" learning schedule. In the experiments, MABN is compared with SyncBN on one-stage models for object detection using RetinaNet benchmark. MABN performs comparably to SyncBN and outperforms BN on COCO dataset. The training process of maskrcnn on COCO is monitored, showing memory footprint and training speed. MABN's training speed is slightly slower than BN and GN due to unoptimized implementation."
}