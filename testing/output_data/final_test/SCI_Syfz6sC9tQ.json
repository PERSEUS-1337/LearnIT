{
    "title": "Syfz6sC9tQ",
    "content": "Our approach, Generative Feature Matching Networks (GFMN), utilizes pretrained neural networks for feature extraction. Experimental results show that by matching first order statistics, our approach achieves state-of-the-art results on challenging datasets like CIFAR10 and STL10. The focus in unsupervised learning is on training generative models that can capture data distribution, with recent advancements in approaches like GANs and VAEs. Recent advancements in generative models like GANs and VAEs have led to the development of Feature Matching GANs (FM-GANs) to address issues such as training instability and blurred image generation. FM-GANs utilize the discriminator network as a feature extractor to guide the generator in generating data that matches the feature statistics of real data. This approach minimizes the mean squared error of average features from generated and real data batches, improving the stability of GANs. Generative Feature Matching Networks (GFMN) is a new approach that focuses on training effective generative models by performing feature matching on features extracted from a pretrained neural network. It aims to answer whether adversarial training of the feature extractor along with the generator is necessary for training effective generators. Generative Feature Matching Networks (GFMN) is a novel approach for training generative models using features from pretrained neural networks. The method avoids the min/max game and offers benefits such as direct correlation between loss function and image quality, no mode collapsing issues, versatility with different datasets, and the ability to use both supervised and unsupervised models as feature extractors. Extensive experiments show state-of-the-art results on challenging benchmarks like CIFAR10 and STL10 by matching first order statistics from pretrained Imagenet classifiers. The proposed Generative Feature Matching Networks (GFMN) approach trains generative models using features from pretrained neural networks. It avoids adversarial learning, achieves state-of-the-art results on benchmarks like CIFAR10 and STL10, and demonstrates the effectiveness of using pretrained autoencoders and deep convolutional net (DCNN) classifiers as feature extractors. The method includes a new feature matching-based training approach, an ADAM-based moving average method for training with small minibatches, and utilizes a generator (G) and pretrained neural network (E) for training. The proposed approach involves training a generator by minimizing a loss function with hidden layers. The network is trained using mini-batches of true and generated data through stochastic gradient descent. The encoder is used for feature extraction and is kept fixed during training. An autoencoder framework is used for unsupervised feature extraction. The encoder network is designed to be a good feature extractor for image generation. The autoencoder is pretrained using mean squared error or Laplacian pyramid loss, which helps in learning high frequencies of images and avoiding blurry results. The Laplacian pyramid loss has been shown to produce better results than simple MSE loss. The decoder network can also initialize the parameters of the generator, making training easier. The generator's parameters can be initialized by the decoder network to facilitate training. Using features from DCNNs pretrained on ImageNet has shown significant improvements in various tasks. Experimenting with different pretrained DCNNs as feature extractors, we aim to train cross-domain generators through feature matching. Large mini-batches are required for accurate mean feature estimation when training with feature matching loss. To address memory issues when using large images and DCNNs, the proposal suggests using moving averages of feature means differences between real and generated data. This approach replaces the expensive feature matching loss with moving averages, providing a better estimate of population feature means. The proposal suggests using moving averages of feature means differences between real and generated data to address memory issues with large images and DCNNs. This approach provides a better estimate of population feature means compared to using a large minibatch with feature matching loss. The moving average is updated using the ADAM optimizer to improve estimation accuracy. The ADAM optimizer, specifically the ADAM Moving Average (AMA), promotes stable training with small minibatches by using adaptive first and second order moments. This non-stationary estimation ensures a stable estimation of moving averages, particularly beneficial for online and non-stationary losses. Experimental results support the memory advantage of AMA formulation in feature matching over naive implementation, as well as the stability advantage and improved generation results. DCNN features from ImageNet are commonly used for transfer learning in computer vision tasks. Previous work has utilized DCNN features for image generation and transformation, combining feature-based loss with adversarial loss to enhance image quality in VAEs. Another study proposes a feature-based loss using VGG-16 neural network features for tasks like style transfer and super-resolution, confirming that initial network layers are more content-related. The proposed approach is related to MMD-based generative models, specifically focusing on matching the first moment of transformed data. It differs from GMMN+AE by mapping directly from the z space to the data space, similar to GANs. The proposed method involves mapping from the z space to the data space, unlike GMMN+AE, and utilizes a frozen pretrained feature extractor for moment matching. It differs from previous MMD-based generative models by incorporating adversarial learning and online learning of moments. Our work introduces a generative approach that optimizes model parameters and noise input vectors jointly. We achieve competitive results on CelebA and LSUN datasets without adversarial training, using Laplacian pyramid loss to enhance non-adversarial methods. Our method involves a pretrained autoencoder to create a generative model, where the encoder acts as the discriminator in a GAN. The text discusses using a pretrained autoencoder as the discriminator in a GAN setting to improve GAN stability. Different approaches are mentioned, such as replacing the discriminator with an autoencoder and including a feature reconstruction loss term in the GAN generator training. The proposed approach is evaluated on various datasets including MNIST, CIFAR10, STL10, CelebA, and ImageNet. The text discusses using a pretrained autoencoder as the discriminator in a GAN setting to improve GAN stability. Different approaches are mentioned, such as replacing the discriminator with an autoencoder and including a feature reconstruction loss term in the GAN generator training. The proposed approach is evaluated on various datasets including MNIST, CIFAR10, STL10, CelebA, and ImageNet. The experiments involve rescaling images to different sizes and using specific architectures for the generator network. Autoencoder features are extracted using an encoder network with varying latent code sizes for different datasets. The text discusses using pretrained autoencoder features in GAN training for improved stability. Experiments are conducted on datasets like MNIST, CIFAR10, and CelebA, with encoder networks like VGG13. Classifier features from pretrained ImageNet models are also utilized. GFMN is trained with an ADAM optimizer, fixed hyperparameters, and a latent code size of 100. In this section, experimental results are presented on using pretrained encoders as feature extractors in GFMN training. The performance in terms of Inception Score (IS) and Fr\u00e9chet Inception Distance (FID) for CIFAR10 is shown when a pretrained decoder is used to initialize the generator, resulting in a significant boost in both IS and FID. Visual comparisons of generated samples for MNIST, CIFAR10, and CelebA datasets are also provided. Using pretrained encoders as feature extractors in GFMN training shows a boost in performance when ImageNet classifiers are used instead of autoencoders. The initialized generator improves results, with the best performance achieved using both VGG19 and Resnet18 as feature extractors for CIFAR10. Using more layers in the VGG19 and Resnet18 feature extractors significantly improves performance, with the best results achieved when the maximum number of layers is used. Training for a longer number of epochs also leads to better performance. The study explores the impact of using different numbers of layers in feature extractors, specifically VGG13 architecture, on image generation. Pretraining the autoencoder with CIFAR10 and ImageNet datasets shows some improvement in Inception Score (IS) and Fr\u00e9chet Inception Distance (FID), but not as significant as using a VGG19 classifier. Experimental results show the advantage of ADAM moving average (AMA) over simple moving average (MA) for stable training with small minibatches in GFMN, which requires a large number of features from DCNNs. The Pytorch BID34 implementation of GFMN can handle minibatches up to size 160 with VGG19 as a feature extractor on a Tesla K40 GPU. In experiments using CelebA as the training set, a smaller feature extractor is used to allow for minibatches of size up to 512 for image size 64\u00d764. Results show generated images from GFMN trained with different batch sizes using either MA or AMA. The minibatch size significantly impacts the quality of generated images when training with MA or AMA. GFMN produces better images with AMA compared to MA, even with a minibatch size of 64. Increasing the minibatch size to 512 does not improve image quality with AMA. Using a stronger feature extractor like VGG19 ImageNet classifier also enhances image quality with AMA. The use of multi-GPU setups for training with larger minibatches is not feasible for many practitioners. GFMN shows training stability and avoids mode collapsing. GFMN VGG19 was trained on different portions of the ImageNet dataset, generating images of various dog breeds. The GFMN VGG19 model was trained on ImageNet subsets containing dog breeds and daisy images. Results show superior performance in unconditional generation compared to other approaches for CIFAR10 and STL10 datasets. Additional experiments with a WGAN-GP architecture also demonstrated promising results. The experiment evaluates if WGAN-GP benefits from DCNN classifiers pretrained on ImageNet. Different hyperparameter combinations were tried, but training with VGG19 or Resnet18 discriminators was unsuccessful. The study aims to train effective generative models using feature matching from pretrained neural networks, avoiding adversarial training. Successful non-adversarial training is achieved by introducing key ingredients. The study introduces key ingredients for successful non-adversarial training of generative models, including using ADAM optimizer for computing moving average of mean features, utilizing features from all layers of pretrained neural networks, and initializing the generator network. Results show that the proposed GFMN approach achieves comparable or better results than state-of-the-art methods like Spectral GAN. GFMN presents important distinctions compared to GMMN and GMMN+AE, achieving far better results due to the use of a strong kernel function and AMA trick for training with small minibatches. The Gaussian kernel used in GMMN requires a large minibatch size, which is impractical. GFMN outperforms recent adversarial MMD methods like MMD GAN and BID2 by achieving similar results to MoLM with significantly fewer features. While MoLM-1536 requires 42 million moments for training, GFMN's best model only needs 850 thousand moments, making it suitable for single GPU environments. Despite using supervised feature extractors, GFMN's success lies in its cross-domain feature extractor and lack of reliance on target labels. In this work, GFMN is introduced as an effective non-adversarial approach to training generative models. It achieves state-of-the-art results without the need for defining and training an adversarial discriminator. The feature extractors used are easily obtained and enable robust and stable training of the generators. Future research could explore alternative feature extractors beyond those mentioned. In this work, GFMN is introduced as a non-adversarial approach to training generative models. It achieves state-of-the-art results without the need for an adversarial discriminator. Open questions include the type of feature extractors and architecture designs suitable for GFMN. GFMN VGG19 is trained with different portions of the ImageNet dataset, generating images of various sizes like pizza, daisy, breeds of dogs, and Persian cats. Generators are trained from scratch without initialization. The experiments involve using a Resnet generator for ImageNet, with specific neural net architectures detailed in TAB4. Different models achieve top 1 accuracy of 29.14% for VGG19 and 39.63% for Resnet18. Data augmentation techniques like random cropping and flipping are used during classifier training. Features from VGG19 and Resnet18 are utilized in GFMN as feature extractors. In GFMN, VGG19 and Resnet18 are used as feature extractors, with features extracted from multiple layers. Evaluation is done using Inception Score (IS) and Fr\u00e9chet Inception Distance (FID) metrics. IS is calculated for 50K generated images, while FID is computed using 5K and 50K generated images, with statistics from CIFAR10 training data. When evaluating GFMN, VGG19 and Resnet18 are utilized as feature extractors, with features extracted from multiple layers. Inception Score (IS) and Fr\u00e9chet Inception Distance (FID) metrics are used for evaluation. FID is computed using 5K and 50K generated images, with statistics from CIFAR10 training data. For STL10, FID computation is repeated 3 times and the average is reported, showing very small variance in the results. Pretrained decoder D boosts performance when initializing the generator, as demonstrated in a visual comparison in Fig. 7. The pretrained decoder D significantly improves image quality when initializing the generator. Using a VGG19 pretrained on ImageNet as a feature extractor for GFMN generator training on the CelebA dataset was successful. In a cross-domain setting, the impact of pretraining autoencoder-based feature extractors for GFMN generator training on the CelebA dataset is investigated. Results in Tab. 6 show performance differences for different combinations of cross-domain feature extractors and G initialization for STL10 and CIFAR10. CIFAR10 using E STL and D STL performs similarly to using E CIFAR and D CIFAR, with a drop in performance when using E CIFAR and D CIFAR for training a STL10 generator. This drop is believed to be related to the training set size difference between STL10 and CIFAR10. The experiments evaluated the use of WGAN-GP with DCNN classifiers pretrained on ImageNet. Different discriminator architectures were tested, but training with VGG19 or Resnet18 discriminators was unsuccessful due to the pretrained discriminators quickly distinguishing between real and fake images, limiting gradient reliability. The reliability of gradient information from the discriminator in GAN training is limited when it can perfectly distinguish between real and fake images early on, making it challenging for the generator to learn properly. This imbalance in the min/max game results in degenerate models, as shown in FIG8 with examples of unsuccessfully trained models. The comparison between simple moving average (MA) and ADAM moving average (AMA) is presented in this context, using VGG19 ImageNet classifier as a feature. The experiment uses ADAM moving average (AMA) with a minibatch size of 64 and shows a positive effect on image quality. A visual comparison between images generated by GFMN and GMMN is presented, highlighting differences in generated images. The experiment uses ADAM moving average (AMA) with a minibatch size of 64 to improve image quality in GFMN generators. Generator initialization with pretrained decoders enhances the implicit generative model, but it is not essential for GFMN's effectiveness. The impact of computing mean features of real data in a minibatch-wise vs. global manner is assessed in this appendix. In the experiment, the impact of computing mean features of real data in a minibatch-wise vs. global manner is assessed using GFMN generators trained with either simple Moving Average (MA) or Adam Moving Average (AMA). The results show that using global mean features does not improve performance when training with MA. In a comparison between autoencoder and VGG19 features for the CelebA dataset, it was found that VGG19 features lead to better image generation quality than autoencoder features, especially when using simple moving average (MA). Images were generated using models that perform feature matching with either VGG19 or autoencoder features. VGG19 features outperformed autoencoder features, especially with simple moving average (MA)."
}