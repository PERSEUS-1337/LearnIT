{
    "title": "SJzqpj09YQ",
    "content": "Spectral Inference Networks is a framework for learning eigenfunctions of linear operators through stochastic optimization. It generalizes Slow Feature Analysis to various operators and is related to Variational Monte Carlo methods. This tool can be used for unsupervised representation learning from video or graph data. Training Spectral Inference Networks is formulated as a bilevel optimization problem, enabling online learning of multiple eigenfunctions. Results show accurate recovery of eigenfunctions and interpretable representations from video in an unsupervised manner. Spectral algorithms are essential in machine learning and scientific computing, used for tasks like PCA and solving eigenfunctions of linear operators. Full eigendecomposition can be done in O(n^3) time for reasonably-sized matrices, while iterative algorithms are used for larger matrices. Eigenvectors may not be stored explicitly in memory for large-scale systems. In many applications in quantum physics and machine learning, eigenvectors cannot be stored explicitly in memory due to the large or continuous state space. Eigenfunctions are approximated from a fixed number of points, and the Nystr\u00f6m method is used for points not stored in memory. Function approximation is necessary for large datasets, and using a function approximator like convolutional neural networks can bias the learned representation towards reasonable solutions. Spectral Inference Networks (SpIN) propose a method to approximate eigenfunctions of linear operators on high-dimensional function spaces using neural networks. The networks are trained via bilevel stochastic optimization and can find correct eigenfunctions in quantum physics problems and discover interpretable representations from video data. This approach significantly advances unsupervised learning without a generative model and has potential applications in scaling spectral methods across various domains. The paper outlines related work on spectral learning and stochastic optimization, defines the objective function for SpIN, and describes the algorithm for training these networks. Spectral Inference Networks propose a method to approximate eigenfunctions using neural networks through bilevel optimization. Spectral methods are widely used in machine learning applications like spectral clustering and Laplacian eigenmaps. Additional materials include custom gradient updates and a TensorFlow implementation of the algorithm. Spectral algorithms are used for precise estimation of parametric models like hidden Markov models and latent Dirichlet allocation by computing the SVD of moment statistics. In reinforcement learning, spectral decomposition of predictive state representations is proposed for learning environments for planning and control. Proto-value functions (PVFs) are eigenfunctions of symmetric transition functions and have been suggested for discovering subgoals in reinforcement learning. PVFs have also been proposed as a model for grid cells in the entorhinal cortex. Spectral learning with stochastic approximation has a long history, with early work on stochastic PCA like \"Oja's rule\" converging to the first principal component. Various online SVD algorithms have since emerged. Most focus on learning fixed-size eigenvectors, while our concern is with cases where eigenfunctions are too large to be efficiently represented with a fixed-size vector. Slow Feature Analysis (SFA) is a related work in machine learning, optimizing for the slowness of eigenfunctions. Spectral methods like Laplacian eigenmaps and Slow Feature Analysis (SFA) have been used in machine learning to optimize for the slowness of features. SFA is typically applied to train shallow or linear models, while SpIN allows for simultaneous learning of all eigenfunctions. Spectral networks combine spectral methods with deep learning, generalizing convolutional neural networks to graph and manifold structured data. Our work uses neural networks to solve large-scale spectral decompositions, specifically in the field of Variational Quantum Monte Carlo (VMC) for approximating eigenfunctions of a Hamiltonian operator. This approach differs from using spectral methods to design or train neural networks, as seen in previous works. Neural networks are used for calculating ground and excited states in Variational Quantum Monte Carlo (VMC). Stochastic optimization for VMC involves importance sampling to eliminate bias. Eigenvectors of a matrix A are defined as vectors u such that Au = \u03bbu, with the largest eigenvector being the solution to the Rayleigh quotient. To find the top N eigenvectors of a matrix A, we can minimize the Rayleigh quotient. Algorithms like power iteration can converge to the global solution despite it being a nonconvex problem. By solving a sequence of maximization problems, we can compute the eigenvectors. If orthogonality is not required, the problem can be reframed as a single optimization problem. In the context of finding top N eigenvectors of a matrix A, the text discusses breaking symmetry in the case where A and u are too large to represent in memory. It introduces a symmetric kernel k(x, x) in a measurable space \u2126 with a probability distribution. A symmetric linear operator is constructed to compute top N eigenfunctions. The text discusses solving optimization problems for function spaces using covariance and kernel-weighted covariance. It introduces the use of a graph Laplacian for simplification in certain cases. The graph Laplacian is equivalent to the total number of neighbors of x and can be written as a kernel that penalizes differences between neighbors. This leads to Slow Feature Analysis (SFA), a special case of SpIN. The algorithm for learning in SpIN enables online learning of SFA with arbitrary function approximators. The Laplace-Beltrami operator generalizes to generic manifolds as a purely local operator. The Laplace-Beltrami operator is a purely local operator that can be used to solve optimization problems. Various methods, such as constrained optimization and constructing an orthonormal function basis, can be used. However, difficulties arise when the distribution is unknown or when working with large discrete spaces. Optimizing the quotient in Eq. 6 allows for learning eigenfunctions in large discrete spaces. By masking the flow of information from the gradient correctly, all eigenfunctions can be learned simultaneously. The objective in Eq. 6 can be rewritten to conveniently depend on the first n functions u 1:n. By masking gradients and combining them into a single masked gradient for optimization, all eigenfunctions can be learned simultaneously. Orthogonalizing u(x) by multiplication results in true ordered eigenfunctions of K. The closed form expression for the masked gradient is provided in the supplementary material. The gradient for Spectral Inference Networks involves passing the error back to parameters \u03b8. The expression is a nonlinear function of multiple expectations, making unbiased gradient estimates challenging. Reframing as a bilevel optimization problem can help address this issue. Bilevel stochastic optimization involves solving two coupled minimization problems simultaneously. This approach is common in machine learning and can be applied to actor-critic methods, generative adversarial networks, and imitation learning. By optimizing the coupled functions on two timescales, the optimization process can converge to simultaneous local minima. Optimizing coupled functions on two timescales can lead to convergence to simultaneous local minima. By replacing certain terms with moving averages, learning Spectral Inference Networks can be framed as a bilevel problem. This approach involves updating moving averages and parameters alternately to solve the optimization problem efficiently. A Spectral Inference Network is a machine learning algorithm that minimizes an objective function through stochastic optimization over deep neural networks. It uses modified gradients to impose feature ordering and bilevel optimization to address bias from finite batch sizes. The algorithm for training these networks is provided in Alg. 1, with TensorFlow pseudocode in the supplementary material. The algorithm for training Spectral Inference Networks involves computing the Jacobian of the covariance with respect to parameters at each iteration. While proper learning rate schedules are important in theory, constant values of \u03b1 and \u03b2 are often sufficient in practice for good performance. In experiments, constant values of \u03b1 and \u03b2 are used. Empirical results on a quantum mechanics problem with a known solution are presented. The Schr\u00f6dinger equation for a two-dimensional hydrogen atom is solved using SpIN. The equation describes wavefunctions \u03c8(x) with unique energy E. The solutions are eigenfunctions of the Hamiltonian operator. The Schr\u00f6dinger equation for a two-dimensional hydrogen atom is solved using SpIN to find eigenfunctions with unique energy E. A neural network was trained to approximate the wavefunction \u03c8(x) with different energy solutions. Details of the training network and experimental setup are provided in the supplementary material. It was important to set the decay rate for RMSProp slower than the decay \u03b2 for the moving average of the covariance in SpIN. Small batch sizes were chosen to investigate biased gradients and demonstrate SpIN's ability to correct them. The effectiveness of SpIN in accurately estimating eigenfunctions and converging to true eigenvalues for a two-dimensional hydrogen atom is demonstrated. The learned eigenvalues show high accuracy, supporting the correctness of the method. The bias correction term in SpIN helps in achieving accurate results, especially for nearly degenerate eigenfunctions. In vision representation learning, a convolutional neural network was trained to extract features from videos using Slow Feature Analysis. The model had 12 output eigenfunctions with similar decay rates. The training setup details are provided in Sec. C.2, with training curves in FIG4. The order of eigenfunctions often switched during training, with lower eigenfunctions taking longer to fit than higher ones. Most eigenfunctions encode ball positions independently, with the first two separating up/down and left/right. Higher eigenfunctions capture more complex joint statistics. One eigenfunction, outlined in green in FIG2, does not relate to ball position but encodes if balls are crowded in the lower right corner. The feature encoding whether balls are crowded in the lower right corner is a fundamentally nonlinear one, not discoverable by shallow models. Higher eigenfunctions may encode even more complex relationships. None of the eigenfunctions investigated seemed to encode velocity due to rapid changes caused by collisions. A different kernel choice could yield different results. Stochastic gradient descent can compute spectral decompositions in a unified framework, allowing learning of eigenfunctions over high-dimensional spaces without Nystr\u00f6m approximation. The Spectral Inference Nets can be applied to complex physical systems for which computational solutions are not available. The learned representations from video data show sensitivity to scene properties and can be used for tasks like object tracking and gesture recognition. The framework addresses biased gradients due to finite batch size and requires computing full Jacobians at each time step. Improving training scaling is a potential future research direction. The framework presented allows for faster exploration and subgoal discovery in reinforcement learning. It discusses breaking the symmetry between eigenfunctions and the use of masked gradients to optimize functions. The approach explores a rich space of possible kernels and architectures to combine and investigate. The matrix can be diagonalized to transform eigenfunctions, but small numerical errors can contaminate results. The masked gradient approach improves numerical robustness in learning degenerate eigenfunctions. The Cholesky decomposition of a matrix A is discussed, showing that \u039b 1:n,1:n is independent of u n+1:n (x). The unique lower triangular matrix with positive diagonal LL T = \u03a3 is explained, along with its blocks. The inverse of a lower triangular matrix is also lower triangular. The upper left block of the inverse of a lower triangular matrix can be written as \u039b 1:n,1:n, ordered by eigenvalue. By induction, maximizing the Rayleigh quotient for the top eigenfunction leads to u 1:n (x) being a maximum of n i=1 \u039b ii. Training with the masked gradient \u2207 u Tr(\u039b) maximizes \u039b (n+1)(n+1), resulting in u 1:n+1 (x) being a maximum of n+1 i=1 \u039b ii. The text discusses orthogonalizing eigenfunctions and computing gradients using matrix operations. It explains how to subtract components of lower eigenfunctions to find the (n+1)th eigenfunction. The reverse-mode sensitivities for matrix inverse and Cholesky decomposition are also mentioned. The gradients can be computed in closed form by applying the chain rule. Matrices \u2206 k and \u03a6 k are defined to simplify notation. The text discusses orthogonalizing eigenfunctions and computing gradients using matrix operations. Matrices \u2206 k and \u03a6 k are defined to simplify notation. The unmasked gradient with respect to u is expressed as a row vector. To zero out relevant elements of the gradient, \u2207 u \u039b kk can be right-multiplied by \u2206 k. The masked gradients can be expressed in closed form using TensorFlow implementation. The text discusses updates in TensorFlow algorithms, including custom gradients for covariance and eigenvalues operations. The code provided is not executable as is, but key details of the updates are explained. The discussion involves matrix operations for orthogonalizing eigenfunctions and computing gradients, with masked gradients expressed using TensorFlow implementation. The text provides pseudocode for creating a moving average operation in TensorFlow. It also includes a function for learning in SpIN, involving two minibatches, a neural network, a kernel function, and TensorFlow parameters. To solve for the eigenfunctions with lowest eigenvalues, a neural network with 2 inputs, 4 hidden layers with 128 units, and 9 parameters is used. The function involves two minibatches, a neural network, a kernel function, and TensorFlow parameters. The neural network used for solving eigenfunctions had 4 hidden layers with 128 units and 9 outputs. A softplus nonlinearity was chosen over ReLU due to the Laplacian operator. RMSProp with specific parameters was used for training, and points were sampled uniformly. To prevent degenerate solutions, the network output was modified by a factor. The neural network architecture for solving eigenfunctions had a block-sparse structure to separate different eigenfunctions. Weight matrices were split into overlapping blocks, allowing features to be shared between eigenfunctions in lower layers while separating distinct features in higher layers. The neural network for solving eigenfunctions had a block-sparse structure, with weights split into blocks to share features in lower layers and separate distinct features in higher layers. Training was done on 200,000 frames using a network with 3 convolutional layers and a fully-connected layer before outputting 12 eigenfunctions. The weights had sparsity between entire feature maps for the convolutional layers, and training was done with RMSProp for 1,000,000 iterations. The neural network used a block-sparse structure to solve eigenfunctions, with weights split into blocks for sharing features. Training was conducted on 200,000 frames with 3 convolutional layers and a fully-connected layer outputting 12 eigenfunctions. The weights had sparsity between entire feature maps, and RMSProp was used for training for 1,000,000 iterations. In contrast, for next-frame prediction, a network was trained on 500k frames of a random agent playing a game, with a batch size of 32 and RMSProp optimization. The network aimed to minimize the difference between consecutive frames by using two consecutive frames as input and predicting the next frame. Additionally, another network was trained to compute successor features and eigenpurposes were derived by applying PCA to the successor features on held-out frames. The convolutional network architecture used for training had a batch size of 32 and RMSProp with a learning rate of 1e-4 for 300k iterations. The successor features were mean-centered to improve results, making the baseline stronger than the original publication. A spectral inference network was trained with 128 hidden units and 5 non-constant eigenfunctions. SpIN was tested on 64k held-out frames using the same training parameters and kernel as before. Features learned by each method were compared by averaging 100 frames from the test. The features learned by each method were compared by averaging 100 frames from the test set. Results showed that SpIN encoded more distinguishable features compared to successor features, with nearly all eigenfunctions capturing features like the presence/absence of sprites or different sprite arrangements. SpIN can encode distinguishable features without pixel reconstruction loss, unlike successor features which require training from distinct losses and computing eigenpurposes. Future research will explore the usefulness of these features for exploration and learning options as rewards."
}