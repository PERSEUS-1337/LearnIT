{
    "title": "SyeKGgStDB",
    "content": "Our novel approach uses reinforcement learning to train a natural media painting agent that imitates human drawing strokes. The agent computes actions representing painting strokes, trained using a constrained learning method. Results show the agent can handle various painting media and constraints, collaborating with humans or other agents effectively. Throughout history, painting has been a crucial part of artistic creation, with various styles like watercolor, oil painting, and sketching. Advances in image processing and computer graphics have led to efforts in simulating these styles using non-photorealistic rendering techniques. Recent developments in machine learning have significantly improved computer-based painting systems, including visual generative methods based on generative adversarial networks. Recent developments in machine learning have significantly improved computer-based painting systems, including stroke-based rendering tasks. Various studies have demonstrated promising results in this area. In this paper, the focus is on training a natural media painting agent for interactive applications, aiming to imitate human drawing strokes without supervision. The technique presented in this study trains an agent to manipulate natural painting media like charcoal, pencil, and watercolor without supervision. A model-based natural media environment is created using deep CNN, and a painting agent is trained using model-based reinforcement learning. Constraints are introduced to enable interactive applications, allowing the agent to generate various styles without retraining the model. Key contributions include training an agent with constraints on actions such as start location, stroke width, and color. The study focuses on training a painting agent to manipulate natural painting media like charcoal, pencil, and watercolor without supervision. The agent can generate various styles by incorporating constraints on actions such as start location, stroke width, and color. The algorithm allows for interactive applications and high-resolution stylized images using different paintbrush configurations. The evaluation highlights the benefits over prior reinforcement learning methods. Stroke-based approaches, including heuristics-based and machine learning techniques, can generate intermediate painting states for interactive purposes and different artistic effects. Heuristics-based methods like those by Winkenbach & Salesin (1996) and Hertzmann (1998) can produce stylized illustrations and colorful paintings in various styles, but are limited by hand-engineered features. Machine learning techniques enable the agent to learn the painting policy, allowing for more flexibility in generating different painting styles. Machine learning techniques enable agents to learn painting policies for generating natural results. Various approaches, such as using RNNs or deep reinforcement learning, have been explored in this domain. Unlike prior methods relying on reward functions or expert demonstrations, a more general setup is utilized here without the need for rewards engineering. The reinforcement learning framework is used to train a painting agent with a differentiable environment model. The approach involves proximal policy optimization with curriculum learning and self-supervision to increase the action space and positive rewards. A model-based RL using a differentiable environment and DDPG is employed, with limitations due to the renderer and uncontrollable agent. The training phase includes extracting the neural network as the painting agent for the roll-out process. Symbols used in the paper are highlighted in Table 1, and details of the natural media painting agent with an environment model are presented in section 4. In section 4, details of a natural media painting agent with an environment model are presented. The approach utilizes Deep Deterministic Policy Gradient (DDPG) for training the model-based painting agent. Constraints are identified using an unconstrained agent and encoded for training the constrained painting agent. An actor-critic-based reinforcement learning framework is used to train the painting agent, with observations based on the current states of the canvas and reference image. The painting agent predicts actions for the neural renderer to update the canvas. The actor neural network serves as the painting agent, using a natural media painting renderer for training. The canvas is generated based on the given action, implementing blending functions and synthetic programs. Unlike previous approaches, a natural media painting renderer MyPaint is used for the experiment. The painting agent uses MyPaint for rich visual effects and blending functions. The action space follows Ganin et al.'s stroke definition using a quadratic Bezier curve. Pressure affects blending function and transparency. The painting agent uses MyPaint for rich visual effects and blending functions, with strokes defined by quadratic Bezier curves. Pressure influences transparency and stroke width. The agent's observation includes the reference image, current canvas, and constraints. The action space is continuous, allowing precise control over the agent. The painting agent uses MyPaint for visual effects and strokes defined by Bezier curves. The reward metric guides the policy network in reproducing the reference image. Loss functions like l2 and WGAN are used to calculate rewards. The objective is to maximize discounted accumulative rewards in reinforcement learning. In the painting environment, the accumulative rewards are calculated using a discount factor. The transition function is represented by the rendering function R. The real environment renderer Rr is modeled using a neural network Rn. Simplifying the environment model can save training time and computational resources. Modeling the transition function of states using neural networks makes the framework differentiable, allowing backpropagation through the renderer network. The training process uses the environment model Rn, while the real environment Rr is used for the roll-out process. The blending function is synthesized by adding stroke images to the current canvas, starting from a blank canvas for prediction. Different paintbrushes are treated as separate models, and paired datasets are collected by sampling actions and synthesizing corresponding images. The training process uses model Rn, while the roll-out process utilizes the real environment Rr. The goal is to make the painting agent controllable for interactive scenarios and stylized image generation. A modified action space can constrain the agent, but it is challenging to generalize to different constraints. A new approach is proposed to create a constrained painting agent that can follow any constraint drawn from the action space. The constraint for the painting agent is defined as a vector in the constraint space C, a sub-space of the action space A. Different constraints can be applied by selecting and combining dimensions in A, such as color, position, and stroke width. The constrained agent takes the reference image, current canvas, and constraint as observations to achieve complex effects. The constrained agent uses a constraint vector c t drawn from the action space sub-space as an additional observation. The constraint is encoded as a bitmap and upsampled to match the size of the reference and current images. The policy network decodes the constraint and outputs a constrained action, which is formed by concatenating the action and constraint. The constrained agent uses a constraint vector c t as an additional observation for training. The constraint is encoded as a bitmap, upsampled, and concatenated with the action to form a constrained action. This training scheme involves using an unconstrained agent to generate constraints and cascading them together. The constrained agent is trained by cascading an unconstrained agent, where a subspace of the action space is identified to extract the constraint c from the action a. The constraint is upsampled and passed as an observation to the constrained agent using a renderer. The roll-out process involves extracting the constraint from either user-defined constraints or the previous action. The constrained painting agent replaces the unconstrained agent in the training scheme, with constraints extracted from human commands or a painting agent's output for each step. The painting agent can place a paint stroke with specified position, color, and stroke width. The action space is split into two agents for exploration. Different paintbrushes are treated as different models for training in the natural media painting environment. Various datasets are used for training the unconstrained painting agent. Deng et al. (2009) trained painting agents using different strokes for various image datasets. They measured l2 loss between reference and reproduced images, showing results in Figure 5 and Table 3. The agents were trained on MNIST, KanjiVG, CelebA, and ImageNet images. A coarse-to-fine strategy was employed for roll-out process to increase image resolution. The constrained painting agents are trained using various constraints such as color, stroke width, and start position. The learning parameters are the same as unconstrained painting agents, and the l2 distance is computed between reference images. The agents use a pencil as a paintbrush and employ a coarse-to-fine strategy by dividing reference images into patches. The results are shown in Figure 2, with learning curves in Figure 4 and l2 loss in Table 3. The trained painting agents use a coarse-to-fine strategy by dividing reference images into patches and constraining color and stroke width. They collaborate with humans and other agents to create artistic paintings using natural media. We build a model of natural media environment using deep CNN and train a painting agent through model-based reinforcement learning. Our algorithm can reproduce reference images in various artistic styles using different paintbrushes and constraints. Future work includes extending the algorithm for different paintbrush configurations and training a hierarchical agent for interactive painting systems. Using various paintbrushes in MyPaint, natural media painting agents were trained and rolled out to reproduce Van Gogh's Starry Night. The coarse-to-fine strategy was incorporated by dividing the reference image and canvas into patches for higher resolution images."
}