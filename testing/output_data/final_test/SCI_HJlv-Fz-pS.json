{
    "title": "HJlv-Fz-pS",
    "content": "Learning Pretrain-KGEs is a training framework that leverages linguistic knowledge from pretrained language models to improve entity and relation embeddings for knowledge graph completion. This method can be applied to various KGE models and consistently achieves state-of-the-art performance on benchmark KG datasets. Knowledge graphs (KGs) are essential for NLP tasks like question-answering and entity linking. They consist of triplets (h, r, t) representing relations between entities. KG completion is crucial due to KGs' incompleteness, with knowledge graph embedding (KGE) models being a prominent approach. Various KGE models like TransE and QuatE are used across benchmark KG datasets for link prediction and triplet classification tasks. Recent KGE models have been designed to model entities and relations in vector spaces based on a score function for triplets. Traditional models suffer from limited knowledge representation, but recent works aim to enrich representation using both fact triplets and textual descriptions. However, contextual information from textual descriptions is often not exploited in these models. The research focuses on a model-agnostic pretraining technique for Knowledge Graph Embedding (KGE) models. The PretrainKGEs framework includes fine-tuning, initializing, and training phases using pretrained language models like BERT to enhance entity and relation representations with textual descriptions. Unlike previous works, this approach leverages syntactic and semantic information for better understanding textual descriptions. The research introduces a model-agnostic training framework for learning knowledge graph embeddings using BERT pretraining. It enhances entity and relation representations with linguistic knowledge, initializes embeddings with rich information, and trains the KGE model to achieve state-of-the-art performance in link prediction and triplet classification tasks. The research presents a framework for learning knowledge graph embeddings by leveraging pretrained language models. Results on benchmark datasets demonstrate improved performance in link prediction and triplet classification tasks. The method, Pretrain-KGEs, outperforms baselines, especially with fewer training triplets and low-frequency entities. KGE models are optimized to score true triplets higher than false ones. They can be divided into translational models like TransE and semantic matching models. Translational models address different relational patterns with models like TransH, TransR, TransD, RotatE, and TorusE. Semantic matching models define a score function for knowledge graph embeddings. Semantic matching models like RESCAL, DistMult, SimplE, and ComplEx use a bilinear approach to model entities and relations in knowledge graph embeddings. ComplEx represents entities in complex space, while DistMult, SimplE, and RESCAL embed entities in the real number field. QuatE, the recent state-of-the-art model, represents entities as hypercomplex-valued embeddings and relations as rotations in quaternion space. These models learn entity and relation embeddings from triplets but do not incorporate external knowledge resources. The current approach in Knowledge Graph Embeddings (KGEs) does not utilize external knowledge resources, leading to limited knowledge representation in entity and relation embeddings. A unified approach is proposed to incorporate linguistic knowledge from pretrained language models, leveraging textual descriptions of entities and relations in knowledge graph datasets. Previous methods have used word averaging to represent entities, with recent works also enriching knowledge representation through textual descriptions. Our method utilizes pretrained language models like BERT to incorporate rich contextual information from textual descriptions for entity and relation representation in knowledge graphs. This approach improves upon previous methods that only used word averaging for entity representation. Our proposed method, Pretrain-KGEs, utilizes pretrained language models like BERT to improve entity and relation representations in knowledge graphs. The framework includes three phases: finetuning, initializing, and training. By incorporating rich knowledge from BERT, our method enhances KGE models by exploiting contextual information from textual descriptions. Our training framework, Pretrain-KGEs, enhances KGE models by improving entity and relation representations using pretrained language models like BERT. It involves three phases: finetuning, initializing, and training. The entity and relation descriptions are encoded into vectors, projected into separate spaces, and scored to measure plausibility. In the initializing phase of our training framework, entity and relation embeddings are initialized using knowledgeable representations instead of random initialization. Entity embeddings E and relation embeddings R are concatenated into matrices, with k and l denoting the total number of entities and relations respectively. The embedding dimension is denoted by d, with E i and R j representing the embeddings of entities and relations respectively. During this phase, the representation vector of entity with index i is encoded by the entity encoder Enc e (\u00b7). In the training phase, entity and relation embeddings are initialized with knowledgeable representations instead of random initialization. The KGE model is then trained using a score function and loss function, such as TransE and max-margin loss with negative sampling. The entity embedding E and relation embedding R are optimized using the same loss function of the corresponding KGE model. To improve knowledge representation in KGE models, textual descriptions of entities and relations are encoded using Bert. This helps capture rich contextual information from large-scale corpora, enhancing the understanding of entities and relations. The textual descriptions can include words, phrases, or sentences providing information such as names or definitions. The textual descriptions of entities and relations are encoded using Bert to improve knowledge representation in KGE models. This involves converting the descriptions into entity and relation representations in a vector space, which are then projected into separate vector spaces through linear transformations. The entity and relation representations are used to initialize embeddings for a KGE model, and the framework is evaluated on four benchmark KG datasets. In experiments on four benchmark KG datasets, link prediction tasks are performed with triplet classification. Corrupted false triplets are generated for evaluation metrics like Mean Rank (MR) and Mean Reciprocal Rank (MRR). Triplet classification uses accuracy metric (Acc) for evaluation. Multiple public KGE models are selected as baselines to evaluate the universality of the training framework Pretrain-KGEs. The training framework Pretrain-KGEs evaluates multiple public KGE models as baselines, including translational models like TransE and RotatE, as well as semantic matching models like DistMult. Detailed dataset statistics are provided in the appendix. The Pretrain-KGEs algorithm evaluates various KGE models, including DistMult, ComplEx, and QuatE. Results are presented in three tables showcasing link prediction, WordNet comparison, and the proposed method's performance in link prediction and triplet classification tasks. Our unified training framework can be applied to multiple variants of KGE models, achieving improvements over various models on most evaluation metrics. It incorporates rich linguistic knowledge to learn better representations for embedding initialization. Additionally, it facilitates further improvements over QuatE in link prediction and triplet classification tasks, demonstrating its effectiveness. In this section, the effectiveness of the proposed training framework is verified through analysis of Pretrain-KGEs' performance in scenarios with fewer training triplets, low-frequency entities, and out-of-knowledge-base (OOKB) entities. The effects of knowledge incorporation into entity and relation embeddings are evaluated by examining MR and MRR metrics sensitivity and visualizing the knowledge incorporation process. The training framework is also tested on WordNet with fewer training triplets and on entities of varying frequency in test triplets on FB15K, as well as on OOKB entities in test triplets on WordNet. Our Pretrain-TransE model incorporates rich linguistic knowledge from BERT to initialize entity and relation embeddings, improving performance with fewer training triplets compared to traditional TransE. This approach enables better learning of knowledgeable KGEs and enhances fine-tuning effectiveness during the training phase. Our training framework, incorporating BERT for entity and relation embeddings, outperforms Baseline-TransE on fewer training triplets. It learns knowledgeable representations and utilizes word senses for improved performance, especially on entities with varying frequency in training triplets on FB15K. Our training framework, utilizing BERT for entity and relation embeddings, performs better than Baseline-TransE, particularly on infrequent entities. Traditional methods struggle with infrequent entities due to inadequate dataset information. Our method addresses the Out-of-Knowledge-Base (OOKB) entity problem by initializing entity embeddings with knowledgeable representations, leading to improved performance on WordNet dataset. The framework effectively addresses the OOKB entity issue on the WordNet dataset by utilizing BERT for encoding textual descriptions of entities. Different methods such as TransE baseline, word averaging model, and Pretrain-TransE with entity names and definitions are compared, showing the superiority of the proposed method. The training framework offers advantages over traditional methods by enhancing entity and relation representations through BERT. The effectiveness of knowledge incorporation during fine-tuning phase is verified by comparing Baseline-TransE and Pretrain-TransE on WN18RR. The changing trend of MR and MRR metrics is analyzed, showing that MR is more sensitive to tricky triplets. Performance is better on high-frequency triplets compared to low-frequency ones. According to Theorem 2, MR is more sensitive to low-frequency triplets while MRR is more sensitive to high-frequency triplets. Pretrain-TransE shows increasing MR performance due to its knowledge learning process, distinguishing between different supersenses in WN18. Baseline-TransE, on the other hand, does not differentiate between certain supersenses in its embeddings. Our Pretrain-TransE can distinguish embeddings between different supersenses, especially related to human beings, by incorporating rich linguistic knowledge into entity and relation representations during initialization. However, during training, the model gradually learns different knowledge graph embeddings, leading to an increase in MR results. During training, Pretrain-TransE shows lower MR and MRR values compared to TransE baseline, indicating better entity and relation representation learning. The proposed method incorporates linguistic knowledge from BERT initially and then learns from datasets, retaining some BERT knowledge in embeddings. Pretrain-KGEs is a general pretraining technique for knowledge graph embedding models. The Pretrain-KGEs technique enhances entity and relation representations in KGE models by leveraging pretrained language models like BERT. Extensive experiments show state-of-the-art performance on benchmark datasets, especially with fewer training triplets and infrequent entities. The method's effectiveness is verified through improved MR and MRR metrics, showcasing the benefits of knowledge incorporation. Our implementations of various knowledge graph embedding models are based on frameworks provided by different authors. In the fine-tuning phase, we use a non-linear pointwise function with different hypercomplex-value units. The score functions of the baseline models are listed in Table 4, showcasing the different mathematical formulations used in each model. The curr_chunk discusses the implementation of knowledge graph embedding models using different mathematical formulations and hypercomplex-value units. It includes details on score functions, entity embeddings, and constraints on the pRotatE model. Additionally, it mentions utilizing WordNet for entity representation. The curr_chunk discusses training methods for entity and relation embeddings in knowledge graph models, utilizing WordNet for entity representation. It includes details on score functions and the optimization process during pretraining and training phases. The curr_chunk evaluates a training framework for knowledge graph models on four benchmark datasets: WN18, WN18RR, FB15K, and FB15K-237. Detailed dataset statistics and hyper-parameters are provided in tables."
}