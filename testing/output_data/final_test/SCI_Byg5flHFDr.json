{
    "title": "Byg5flHFDr",
    "content": "In recent years, neural networks have been extensively studied for structured data like graphs, with a focus on static graphs. However, real-world networks are dynamic, making the prediction of their evolution a significant task in graph mining. This paper proposes a model using a graph neural network and recurrent architecture to capture temporal evolution patterns of dynamic graphs. Additionally, a generative model is employed to predict the topology of the graph at the next time step. The proposed model is evaluated on artificial and real-world datasets with common network evolving dynamics. Results show the effectiveness of Graph Neural Networks (GNNs) in analyzing graph-structured data, with applications in graph classification, link prediction, and community detection. While GNNs have been successful in static graph tasks, their utility in dynamic graphs remains uncertain. Some models focus on predicting low-dimensional graph representations for the next time step, while neglecting the task of predicting the graph's topology. Graph generation, an important task in graph mining, aims to create graphs with specific properties like degree distribution and community structure. Traditional models like the Erd\u0151s-R\u00e9nyi model are used for graph generation. The proposed framework in this paper introduces a novel approach for predicting the evolution of network topology over time using neural networks. This framework utilizes an encoder-decoder architecture with a Graph Neural Network (GNN) to generate low-dimensional representations of input graphs, capturing structural information. The proposed framework introduces a novel approach for predicting the evolution of network topology over time using neural networks. It utilizes an encoder-decoder architecture with a Graph Neural Network (GNN) to generate low-dimensional representations of input graphs. The model is evaluated on synthetic and real-world datasets, showing competitive performance compared to baseline methods. The paper discusses a novel approach for predicting network topology evolution using neural networks. It outperforms competing methods and is organized into sections detailing related work, preliminary concepts, model components, evaluation on tasks, and conclusions. The work is related to random graph models like Erd\u0151s-R\u00e9nyi, preferential attachment, and Kronecker graph models. Parameters of these models can be estimated based on observed graph instances to predict graph structure evolution over time. The proposed model focuses on predicting the topology of graphs, unlike previous works that mainly predict node or graph representations evolution over time. It introduces EvoNet as a step towards this objective, combining GNNs and RNNs for feature extraction and sequence learning. The text introduces basic concepts from graph theory and defines notation. It presents EvoNet, a framework for predicting graph evolution, detailing its components. It defines a graph G = (V, E) with nodes V and edges E, and discusses the topology described by the adjacency matrix A\u03c0. In graph theory, networks are represented by a triplet G = (A, X, L) where nodes and edges have feature vectors. Evolving networks change over time by adding or removing nodes/edges. An evolving graph is a sequence of graphs {G0, G1, ..., GT} where Gt = (At, Xt, Et) represents the state at time step t. In graph theory, networks are represented by a triplet G = (A, X, L) where nodes and edges have feature vectors. Evolving networks change over time by adding or removing nodes/edges. The evolving graph at time step t is denoted as G t = (A t , X t , E t ). The focus is on predicting the topology of G t based on its history within a fixed window of size w. The proposed architecture aims to predict what \"comes next\" in the sequence of graphs. The proposed architecture aims to predict the topology of evolving graphs by using a framework consisting of a graph neural network (GNN), a recurrent neural network (RNN), and a graph generation model. This model treats graphs as sequences and generates vector representations for each graph instance to predict the graph topology at the next time step. The proposed EvoNet model utilizes a network to map graph sequences into vectors and predict the next graph representation. Graph Neural Networks (GNNs) have become popular for machine learning on graphs, employing message passing to update node representations. After k iterations, each node captures structural information within its neighborhood, leading to a feature vector for the entire graph. The proposed EvoNet model uses Graph Neural Networks (GNNs) to compute feature vectors for graphs. The learning process involves aggregation, update, and readout phases. Aggregation computes messages for each node by aggregating neighbor representations using a permutation invariant function. The update function combines feature vectors using a Gated Recurrent Unit for end-to-end trainability. The EvoNet model utilizes Graph Neural Networks (GNNs) to compute feature vectors for graphs. The Gated Recurrent Unit is employed for aggregation and update steps, generating node representations that are aggregated into a single vector representing the graph topology. Set2Set is used to generate the final graph representation. The EvoNet model uses Graph Neural Networks (GNNs) to compute feature vectors for graphs. A recurrent neural network (RNN) is then used to process the sequence of graph vectors. The RNN can predict the next element in the sequence by learning the conditional distribution. In this implementation, a Long Short-Term Memory (LSTM) network is used to read sequentially the vectors. The LSTM network in the EvoNet model reads vectors produced by the GNN to generate an embedding of the next graph. This embedding includes topological information and serves as input to the graph generation module. The architecture acts as an encoder network, projecting input graphs into a low-dimensional space. The framework for learning generative models of graphs is utilized to generate a graph that evolves from the current instance in an autoregressive manner. The generative model aims to maximize the likelihood of observed graphs by considering them as sequences of adjacency vectors. A hierarchical RNN is used to generate new nodes and links in an autoregressive manner. The model utilizes a hierarchical RNN to learn the probability distribution of edges in a graph sequence. It generates nodes and links in an autoregressive manner, predicting the topology for the next time step by minimizing cross-entropy loss. In this Section, EvoNet's performance on predicting graph topology evolution is evaluated using synthetic and real-world datasets. Different strategies for node ordering can impact model efficiency, with the Breadth-First-Search scheme being one option. The nodes in the sequence of graphs are distinguishable, allowing for a consistent node ordering across time steps. New nodes are added at the end of the ordering. The study evaluates EvoNet's performance in predicting graph topology evolution using synthetic and real-world datasets. Synthetic datasets consist of sequences of graphs with specific patterns of graph structure changes at each time step, while real-world datasets involve graphs decomposed into snapshots based on timestamps. Path graphs are discussed, denoted as Pn with n nodes, and two scenarios are considered. In the study, two scenarios are considered for graph evolution. The first scenario involves adding a new node at each time step with an edge to the last node. The second scenario involves removing a node every three steps. Cycle graphs, denoted as Cn with n nodes, are also discussed, with the first graph being C3. In the first scenario, the cycle size increases by adding a new node and two edges at each time step. The ladder graph L n is a planar graph with 2n vertices and 3n \u2212 2 edges, formed by attaching rungs to a ladder structure. At each time step, one rung is added to the ladder. EvoNet is evaluated on real-world datasets derived from the Bitcoin transaction network, where nodes represent users and edges indicate trades. Each edge is annotated with a rating between -10 and 10. Data is collected from Bitcoin OTC 2 and Bitcoin Alpha 3 platforms. The study compares EvoNet with traditional graph models like Erd\u0151s-R\u00e9nyi, Stochastic Block, Barab\u00e1si-Albert, and Kronecker Graph models to analyze the topology evolution of temporal graphs. Graph kernels are proposed as a method to compare graphs effectively. In computer science, graph kernels are used to compare graphs and evaluate their quality. The Weisfeiler-Lehman subtree kernel is employed in experiments, normalizing values between 0 and 1. Datasets consist of sequences of graphs evolving over 1000 time steps, with 80% used for training and the rest for testing. The window size is set to 10 for feeding data. The window size for feeding data is set to 10 consecutive graph instances. The Weisfeiler-Lehman subtree kernel measures the similarity between test set graphs and predicted graphs. EvoNet's hyperparameters are chosen based on validation set performance. Random graph model parameters are set to match ground-truth graph properties. The model uses an MLP to predict graph properties based on past data, enabling accurate generation of graphs similar to ground-truth graphs on synthetic datasets. Comparison of predicted graph sizes against ground-truth sizes shows accurate predictions for path graphs and slight variance for ladder graphs. The model accurately predicts graph sizes for path graphs and shows slight variance for ladder graphs. However, there is a mismatch at the beginning of the sequence for small size ladder graphs, possibly due to their complex structure. The model fails to reconstruct the cycle structure in cycle graphs, with all predicted graphs being path graphs. The performance of the model on real datasets Bitcoin-OTC and Bitcoin-Alpha is analyzed, showing similarities between real and predicted graphs. Our proposed method EvoNet outperforms traditional random graph models in predicting the evolution of dynamic graphs. EvoNet achieves an average similarity of 0.82 on BTC-OTC dataset and 0.55 on BTC-Alpha dataset, surpassing all other methods. Experiments demonstrate the advantage of EvoNet over traditional models, showcasing its effectiveness on both synthetic and real-world datasets."
}