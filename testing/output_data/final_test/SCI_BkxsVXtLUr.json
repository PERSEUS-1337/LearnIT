{
    "title": "BkxsVXtLUr",
    "content": "The visual cortex processes retinal images via two interconnected networks: the ventral pathway for object-related information and the dorsal pathway for motion and transformations. An unsupervised approach to feature learning is explored, inspired by the cortical division of labor. A new convolutional bilinear sparse coding model is proposed for learning object features and transformations from natural videos. The model can process large images and learn groups of features and their transformations directly from videos in an unsupervised manner. The model presented in the curr_chunk demonstrates unsupervised learning of primary \"capsules\" and has connections to the Lie group approach to visual perception. Current unsupervised models lack interpretability or hierarchical depth, unlike the proposed model which captures the statistics of transitions between video frames. The model in the curr_chunk involves bilinear sparse coding with object and transformation coefficients interacting multiplicatively. Sparsity is enforced on either r or both r and x via a penalty. The model is trained using pairs of video frames with fixed r and inferred x to account for frame differences, connected to the Lie group approach to vision. The model in the curr_chunk introduces a new bilinear sparse coding model for images, referred to as IBSC. It allows features to have independent pose parameters for transforming independently from frame to frame using transposed convolutions. The reconstruction-based loss function for consecutive frames of a video includes mean-squared reconstruction error, sparsity penalty, and weight decay. Inference for BSC involves initializing x to a canonical vector and iteratively updating it. The IBSC model introduces a new bilinear sparse coding approach for images, allowing features to have independent pose parameters. Inference involves optimizing r and x alternately until convergence, using iterative thresholding for r and projected gradient descent for x. Experiments were conducted on YouTube videos converted to grayscale and scaled down for analysis. The frames from YouTube videos were converted to grayscale and scaled down to 236 \u00d7 176 pixels. Sequences of 5 consecutive frames were extracted and normalized using subtractive normalization. Frames with significant differences in Euclidean norm were excluded to avoid sudden camera changes or scene transitions."
}