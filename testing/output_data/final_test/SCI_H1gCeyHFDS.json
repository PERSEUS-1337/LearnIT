{
    "title": "H1gCeyHFDS",
    "content": "The paper proposes a novel Gram-Gauss-Newton (GGN) algorithm for training deep neural networks for regression problems with square loss. Inspired by the connection between neural network optimization and kernel regression, GGN offers better convergence rates than typical second-order methods with minimal computational overhead. The method is shown to have a quadratic convergence rate for sufficiently wide neural networks and provides a convergence guarantee for mini-batch GGN algorithm. The paper introduces the Gram-Gauss-Newton (GGN) algorithm for training deep neural networks, providing a convergence guarantee for the mini-batch version. Preliminary experiments show that GGN converges faster and performs better than Stochastic Gradient Descent (SGD) for training standard networks. SGD is computationally efficient but GGN offers better performance and faster convergence. Efficient first-order methods beyond SGD are crucial for training overparameterized deep neural networks. Second-order methods have better convergence rates but are rarely used due to prohibitive computational costs. Computing second-order information like the Hessian matrix and its inverse is challenging for deep neural network training. Second-order methods like the GGN have simpler update rules and better convergence guarantees for neural networks compared to traditional methods like backpropagation. Our method offers a quadratic convergence rate guarantee for overparameterized networks, with a focus on practical and theoretically sound optimization. The relation between our method and NTK kernel regression allows for easy generalization guarantees. Another related work proposed a similar update rule for Q-learning algorithms. The GGN method offers a simpler update rule without the need for a learning rate term, unlike other methods. Nonlinear least squares regression aims to optimize a class of nonlinear functions, such as neural networks. Previous work has shown that optimization with gradient flow on neural networks with infinite width involves a special kernel known as the neural tangent kernel (NTK). The NTK is further extended in relation to optimization, with Lemma 1 stating the evolution of network outputs during gradient descent. Key ideas from previous works emphasize the importance of the Gram matrix with respect to the NTK. The NTK is closely related to the Gram matrix and its evolution during training. Arora et al. (2019a) shows that a wide ReLU neural network is equivalent to kernel regression. The NTK concept involves a linear approximation using first-order Taylor expansion, with \u2207 w f (w 0 , x) representing the feature map at x. The linear approximation in the NTK involves a first-order Taylor expansion, with the parameter w \u2212 w 0 in the RKHS induced by NTK. The Gauss-Newton method uses a similar linear approximation for accelerating the solution of nonlinear least squares problems. The parameter update can be achieved by solving a specific problem involving the Jacobian matrix. The Gauss-Newton method utilizes a linear approximation for nonlinear least squares problems, achieving superlinear convergence rates when the model is not highly nonlinear. The Gram-Gauss-Newton (GGN) method, inspired by NTK kernel regression, addresses challenges with overparameterized neural networks without needing to compute the Hessian matrix. The GGN method, inspired by NTK kernel regression, shows quadratic convergence for wide neural networks. It has minimal additional computational cost compared to SGD. The method accelerates optimization by solving the kernel regression problem explicitly at each step. Our Gram-Gauss-Newton (GGN) method accelerates optimization by directly solving the NTK kernel regression with the Gram matrix G at each time step. The feature map of NTK can be expressed as x \u2192 \u2207 w f (w t , x), and the linear parameters in RKHS are w \u2212 w t. The kernel regression solution involves the matrix of features J t,S computed on the training data set S, the vectorized outputs f t,S, and the Gram matrix of the NTK on S. The Gauss-Newton method re-derives with minimum norm solution for overparameterized models in kernel learning. The solution of kernel regression lies in n-dimensional subspace of RKHS and minimizes RKHS norm. Using mini-batch instead of full batch for derivatives computation is crucial for good model generalization. The proposed mini-batch version of GGN updates using the Gram matrix, reducing computational cost significantly. The proposed mini-batch version of GGN introduces a variant using the Gram matrix to reduce computational cost. For two-layer neural networks, Full-batch GGN converges with a quadratic convergence rate, while Mini-batch GGN converges linearly. The GGN algorithm is designed based on the fact that for wide neural networks, the output is close to a linear function with respect to the parameters. The neural network structure consists of a two-layer network with input x, network width M, weights initialized from a standard Gaussian distribution, and activation function \u03c3(\u00b7). The GGN algorithm is designed for wide neural networks where the output is linear with respect to the parameters. The network has two layers with input x, width M, weights initialized from a Gaussian distribution, and Lipschitz activation function \u03c3(\u00b7). The Gram matrix G has an asymptotic limit under certain initialization conditions, leading to a positive definite matrix. The full-batch GGN theorem shows quadratic convergence on overparameterized neural networks under specific assumptions. The GGN algorithm is designed for wide neural networks with linear output. The full-batch version of GGN converges to zero with second-order convergence. The mini-batch version performs serial subspace correction similar to the Gauss-Siedel method. The mini-batch GGN algorithm converges to zero with second-order convergence and performs serial subspace correction. The convergence is highly related to the spectral radius of A, with an assumption on the iteration matrix being diagonalizable. The mini-batch GGN algorithm converges to zero with second-order convergence and performs serial subspace correction. The convergence is highly related to the spectral radius of A, with an assumption on the iteration matrix being diagonalizable. We choose an arbitrary diagonalization of A as A = P \u22121 QP and denote Assumption 2 for simplicity. Theorem 2 states the convergence of Mini-batch GGN on Overparameterized Neural Networks under certain conditions. The mini-batch GGN algorithm converges to zero with second-order convergence and performs serial subspace correction. The convergence is related to the spectral properties of matrix A. Theorems 1 and 2 provide bounds on the update norm W t \u2212 W t+1 F, showing that matrices J, G, etc. remain close to their initialization. The full proof is in the appendix. The accelerated convergence of the GGN algorithm is linked to the local linearity and stability of the Jacobian and Gram matrix. The theorems presented serve as motivation for the algorithm's performance in practice, even under less stringent conditions. For overparametrized deep neural networks, full-batch GGN demonstrates a quadratic convergence rate. The computational cost of GGN is compared to SGD, with two major steps involved in each iteration: forward and backpropagation for the Jacobian matrix computation, and using the Jacobian for updates. The GGN algorithm computes the Jacobian matrix efficiently, with small computational complexity compared to SGD. It tracks derivatives for nodes and parameters, leading to faster training times than SGD. The GGN algorithm efficiently computes the Jacobian matrix with small computational complexity compared to SGD. It tracks derivatives for nodes and parameters, leading to faster training times. The computational cost in step (B) is small compared to step (A). The GramGauss-Newton method requires O(b^2m + b^3) for computing the Gram matrix and a matrix inverse. The proposed GGN algorithm is compared with baseline algorithms in real applications for regression tasks like AFAD-LITE and RSNA Bone Age. The training data for the AFAD-LITE task includes 60k facial images with corresponding ages. ResNet-32 is chosen as the base model architecture, with two variants: ResNetBN and ResNetFixup. SGD is used as the baseline algorithm with specific hyperparameters. The inconsistency of batch normalization with the regression function assumption is noted. The GGN algorithm does not directly apply to ResNetBN, so it is tested on ResNetFixup only. The RSNA Bone Age task involves 12,611 labeled images of left hand radiographs. ResNetBN and ResNetFixup are chosen for the experiment, with warm-start initialization. SGD is used with lr=0.01 and momentum=0.9, while GGN is used with \u03bb=1 and \u03b1=0.1. Batch size is set to 128, and mean square loss is used for training. Our proposed GGN method converges much faster than baselines using SGD on AFAD-LITE and RSNA Bone Age tasks. The training loss decreases quickly to nearly zero in 30 epochs for AFAD-LITE, while baselines decay much slower. GGN also shows advantages on the RSNA task. Generalization performance needs evaluation, where our method trains faster than other baselines. The GGN algorithm accelerates training and learns better models compared to baselines on the RSNA Bone Age task. Different hyper-parameters were tested, showing that the model converges faster with a \u03bb value close to 1. The value of \u03b1 in GGN can be considered as the inverse of the learning rate in SGD. The Gram-Gauss-Newton (GGN) method accelerates training for regression problems with square loss using overparameterized neural networks. Despite being a second-order method, GGN has low computation overhead compared to SGD and can achieve quadratic convergence rate with wide neural networks. Experimental results show GGN outperforms SGD on regression tasks with standard network architectures, indicating the potential of second-order methods in learning deep neural networks. The GGN method can compete with first-order methods for training deep neural networks with a large number of parameters. It can be applied to regression tasks and easily generalized to other tasks like classification. While most first-order methods struggle to utilize computational resources in parallel settings, GGN can exploit this ability, leading to faster training. The GGN method can accelerate convergence speed by refining gradients with second-order information, unlike increasing batch size which harms generalization. Future work includes studying GGN's application to classification problems. Notations are used for gradients, Jacobians, activation functions, and parameter initialization. The analysis is based on the fact that G stays close to its infinite-width limit, a positive definite matrix with least eigenvalue denoted \u03bb 0. Lemma 2 provides bounds on norms at initialization, showing that with probability at least 1 \u2212 \u03b4/2, certain conditions hold. By choosing appropriate parameters, specific norms can be estimated with high probability. The analysis focuses on the concentration of a Lipschitz function over Gaussian variables, ensuring certain conditions hold with high probability. Additionally, a lemma discusses the least eigenvalue of the Gram matrix at initialization, showing its proximity to a positive definite matrix. The lemma discusses the concentration of a Lipschitz function over Gaussian variables, ensuring certain conditions hold with high probability. It also bounds the relevant norms and the least eigenvalue of the Gram matrix inside a specific range of optimization trajectory. Lemma 4 provides bounds on norms and the Gram matrix during optimization, ensuring stability and invertibility of G_W. It establishes conditions for the optimization trajectory to maintain certain properties with high probability. In this section, parameter W is represented as W_t after t iterations. If G_t is invertible, we control the first term on the right side based on Lemma 4 for second-order convergence. By setting R_t = W_t - W_t+1 F, we prove the existence of a sufficient constant M. Induction on t shows that W_t is within a certain bound B(R) as long as a certain condition is met. After t iterations, parameter W is denoted as W_t. If G_t is invertible, Lemma 4 controls the first term for second-order convergence. By setting R_t = W_t - W_t+1 F, a constant M ensures W_t is within bound B(R) as long as a condition is met. The update rule involves solving for variables using the Gauss-Siedel method, introducing matrix (15). The update rule involves solving for variables using the Gauss-Siedel method, introducing matrix (15). The formula for the update of f \u2212 y satisfies certain conditions, with the index going in decreasing order from left to right. The proof involves showing that D t \u2212 L t = U t0 by induction on i. The convergence of the algorithm is closely related to the eigenvalues of the iteration matrix A. Lemma 7 provides bounds on the spectral radius of A and its convergence on perturbed matrices. The spectral radius of A, denoted \u03c1(A), is shown to satisfy certain conditions based on the initial Gram matrix and the maximum norm of eigenvalues of A. Lemma 8 states that if the iteration matrix A is diagonalized as A = P^-1 QP and satisfies certain conditions, then the convergence of perturbed matrices can be bounded. Additionally, Lemma 9 provides useful bounds for the norms and eigenvalues of relevant matrices in the optimization scope. Lemma 8 and Lemma 9 provide conditions for convergence and bounds on relevant matrices in optimization. The proof of Theorem 2 involves selecting R to keep W ti in B(R) for convergence. The logic follows a similar approach as in the proof of Theorem 1, using induction on the pair (t, i). Theorem 2 states that there exists a constant in the range B(R) such that all requirements for M in Lemma 2-9 can be satisfied. By induction on (t, i), it is shown that certain conditions hold for W ti in B(R), leading to invertibility of G ti,ii. Theorem 2 proves the existence of a constant in the range B(R) satisfying requirements for M in Lemma 2-9. By induction on (t, i), conditions for W ti in B(R) are shown, ensuring invertibility of G ti,ii. Additionally, test performance curves for AFAD-LITE dataset and baseline results on RSNA Bone Age dataset are provided, with implementation of Group Normalization (GN) to improve Adam's performance. Grid search is used to find optimal hyper-parameters for experiments conducted with batch size 128. The experiments were conducted with batch size 128, input size 64*64, and weight decay 10^-4. Different hyper-parameters were used for various models such as SGD+ResNetBN, Adam+ResNetBN, SGD+ResNetGN, Adam+ResNetGN, K-FAC+ResNet, and GGN+ResNetGN. Convergence results are shown in Figure 4 with a logarithmic scale for training curves."
}