{
    "title": "SJxE3jlA-",
    "content": "Humans rely on episodic memory for various tasks, such as remembering names, movie plots, and locations. Endowing reinforcement learning agents with episodic memory is crucial for achieving human-like general intelligence. A new form of external memory called Masked Experience Memory (MEM) is designed to mimic human episodic memory. An RL task based on the game of Concentration is used to evaluate episodic memory, showing that a MEM RL agent outperforms baseline agents. Episodic memory is essential in human life for recalling information and navigating daily tasks. Episodic memory is crucial for storing and recalling novel information, like movie plots or where a car is parked. In advanced Alzheimer's disease, the loss of episodic memory severely impairs a person. While current Reinforcement Learning agents have procedural and semantic memory, they lack episodic memory. The goal is to enhance RL agents by giving them episodic memory, which allows them to leverage past observations similar to current ones. Episodic memory is essential for storing and recalling new information. Reinforcement Learning agents need to be designed with episodic memory to leverage past observations similar to current ones. This involves storing details of old observations, comparing new observations with stored ones, retrieving similar details, and utilizing this information to pursue rewards. The Concentration task evaluates an agent's episodic memory by requiring it to recall past observations of cards and their locations to identify matching pairs among a set of face-down cards. Intelligent agents in the Concentration task use unique randomly generated cards to test episodic memory. Deep RL agents have shown success in surpassing humans in Atari games, but still struggle with challenges like sparse rewards and partial observability. Equipping RL agents with memory is a promising approach to address these challenges and has gained interest in the research community. Memory augmented neural networks, like the differentiable neural computer (DNC), offer storage capabilities beyond LSTMs for handling memory-based tasks. However, the DNC struggles with reusing elements of its memory matrix in tasks like Concentration. Key contributions include proposing a definition of episodic memory for RL. The paper introduces a new architecture called Masked Experience Memory (MEM) for RL agents to have episodic memory. MEM enables agents to solve the Concentration task by remembering one-shot experiences, which baseline RL agents struggle with due to slow neural network weights. Fast weights are a method for storing information from discrete samples in neural network weights. While many external memory architectures have been proposed for supervised learning tasks, there are no published evaluations of fast weights on episodic memory tasks. Some memory architectures have shown improvements in deep reinforcement learning tasks by increasing sample efficiency, but none have been evaluated specifically on episodic memory tasks. Despite the lack of evaluations on episodic memory tasks, some memory architectures have shown improvements in deep reinforcement learning tasks by increasing sample efficiency. The differentiable neural computer (DNC) demonstrated that an RL agent could use a memory matrix to handle previously unseen observations in tasks like Concentration and Mini-SHRDLU. The Mini-SHRDLU puzzle involves data buffering and problem-solving subtasks, not requiring human-like episodic memory. Episodic memory tasks are a form of transfer learning, involving novel task evaluation. Concentration is an episodic one-shot learning task requiring correct memorization and recall of novel observations. Previous work on few-shot image classification utilized learned metric spaces and siamese networks. The Masked Experience Memory (MEM) architecture in RL agents leverages past observations similar to the current one, differentiating it from DNC. DNC writes abstract vectors to memory, while MEM focuses on comparing observations. DNC's memory mechanism is more general than MEM's, making it a valuable baseline for comparison. Each MEM memory write operation copies the last observation into a fixed-size memory store. The Masked Experience Memory (MEM) architecture in RL agents copies the last observation into memory, dropping the oldest one. This design assumes recent history is most relevant for selecting actions. Memory read operations compare current observations with past ones, returning a weighted sum of memories. This method is similar to content-based addressing memory architectures. The Masked Experience Memory (MEM) architecture in RL agents stores recent observations in memory and uses a weighted sum of memories for read operations. The similarity between memories and the current read key vector is calculated using vector quadrance scaled by a learned mask vector. The mask weight vector and attention distribution parameters are trained by gradient descent to determine which memory dimensions are used for lookup operations. The Masked Experience Memory (MEM) architecture in RL agents uses memory dimensions as lookup keys for memory read operations, determining attention distribution over memories. A mask vector is used to calculate vector similarity, avoiding noise associated with cosine similarity. Cosine similarity is ideal for comparing word count vectors from documents of different lengths. The read operation in content-based addressing replaces zeros in the read key with values from memory that best match the key. Cosine similarity can add noise to the similarity calculation, especially when the key vector has a small non-zero portion. MEM uses an explicit mask to select vector elements for similarity measurement, avoiding this issue. In this section, we discuss how to use MEM in an RL agent to maximize its expected long-term return by acting in an unknown environment. The use of memory architecture is crucial in making decisions in partially observable environments like the game of Concentration. The use of memory architecture, such as LSTM and DNC, in RL agents is crucial for making decisions in partially observable environments. Different approaches, including using LSTM inside actor and critic networks, separate actor and critic DNCs, and a proposed architecture based on MEM, are illustrated. The memory store is cleared at the beginning of each episode for episodic RL tasks. The LSTM controllers in RL agents receive current observations, last action taken, and rewards as inputs. They also receive input from the memory store (MEM or DNC). A new task was designed to evaluate episodic memory in RL agents, inspired by the children's memory game of Concentration. The Concentration game involves matching pairs of cards on a table. Players take turns flipping two cards at a time to find matches. The winning strategy is to remember card locations. The game is converted into a single-player RL task with an agent navigating a grid to find matching pairs. The Concentration game is converted into a single-player RL task where an agent navigates a grid to find matching pairs of cards. The agent can take steps in four directions and flip over cards. Matching cards result in a reward and removal from the grid. The episode ends when all cards are matched or a certain number of time steps have passed. The challenge lies in representing the agent's observation of card faces to test episodic memory. The Concentration game is converted into a single-player RL task where an agent navigates a grid to find matching pairs of cards. The agent can take steps in four directions and flip over cards. Matching cards result in a reward and removal from the grid. The episode ends when all cards are matched or a certain number of time steps have passed. The challenge lies in representing the agent's observation of card faces to test episodic memory. The agent can solve the task without relying on episodic memory by dedicating a different unit to every possible card-plus-position combination in its network. Each card face could be represented by a complex image processed by a convolutional neural network, allowing the agent to toggle the activation state of the unit representing a particular card and position when a card is revealed. To test episodic memory in a single-player RL task based on the Concentration game, new card face images are generated at the start of each episode. Each card face is represented by a random vector of length 6, ensuring that no image appears in more than one episode. This prevents the neural network from learning each image persistently. The images are created by randomly selecting six real numbers in the range [0, 1]. During training, agents are evaluated based on card-pair matches per flip in a Concentration task with 8 cards on a 3x3 grid. Counters track the number of cards flipped and matches obtained by 16 worker agents, with performance calculated every 100,000 steps. This allows for a trailing estimate of the agent's task performance. The agent's performance on the Concentration task was evaluated using hyper-parameter settings, with key results summarized in FIG2. The MEM agent showed near-optimal policy learning, while the LSTM-A3C agent struggled to remember card locations. The Sonnet LSTM agent outperformed the TensorFlow LSTM agent slightly, despite both being based on the same model. The colorblind MEM agent performed better than LSTM agents, even without seeing card faces. The colorblind MEM agent 3b outperformed LSTM agents in a Concentration task, indicating a different strategy. DNC agent slightly outperformed LSTM-A3C but worse than Sonnet LSTM. Ablation studies on MEM architecture showed the mask and Euclidean distance squared were essential for improvement. Similarity strength feature did not provide benefits. The MEM architecture outperformed other agents in a Concentration task by effectively using episodic memory to remember card locations. The similarity strength feature did not provide any benefits, but the mask weights for card face dimensions were crucial for improvement. The code for replicating this work will be made public before the conference. The MEM architecture, which outperformed other agents in a Concentration task by effectively using episodic memory to remember card locations, plans to enhance memory in various ways: 1. Making mask weights context-sensitive for read key vectors. 2. Expanding memory dimensions to include recurrent network activations. 3. Implementing memory deletion based on importance. 4. Introducing a mask for write operations to cover specific dimensions. The differentiable neural computer (DNC) uses a memory matrix for combining neural and computational processing. DNC's performance deteriorates with episode length due to difficulty reusing memory locations. This issue was demonstrated in a modified copy task. The DNC's GitHub repository included a modified copy task with a 16x16 memory matrix for Figures 4 and 5. Reusing memory locations was necessary to achieve zero error in two out of five runs. DNC struggled with learning to reuse its memory matrix, as shown in FIG4. Additionally, DNC was evaluated on Mini-SHRDLU, consisting of data buffering and puzzle solving tasks. Constraints were fed to the RL agent before working on the puzzle. The DNC used external memory to buffer constraint information and achieve better results on the combined buffer-puzzle task compared to a baseline LSTM-based RL agent. The Mini-SHRDLU task was used to test an RL agent's episodic memory, with the buffer and puzzle sub-tasks evaluated separately to determine the impact of DNC's external memory on puzzle-solving. The Mini-SHRDLU task tested an RL agent's episodic memory without the data-buffering subtask. The LSTM-A3C agent read constraints from a simple array at its own pace, achieving results comparable to DNC. Numeric comparisons between the two performance curves are not meaningful due to differences in training methods. The LSTM-based RL agent learned to solve Mini-SHRDLU problems as well as DNC without using a differentiable memory matrix, demonstrating that external memory was not necessary for problem-solving."
}