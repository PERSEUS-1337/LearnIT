{
    "title": "rkl8DES3nE",
    "content": "In a study comparing SGD and Adam for estimating a rank one signal in matrix or tensor noise, it was found that Adam gets stuck in local minima when polynomially many critical points appear in the matrix case, while SGD escapes them. However, both algorithms get trapped when the number of critical points degenerates to exponentials in the tensor case. Warm starting was shown to be beneficial in these situations, as stochasticity in gradients alone cannot replace it to find the basin of attraction in this class of problems. Reductionism involves breaking down complex systems into atomic components. The behavior of stochastic gradient descent algorithms like SGD and Adam in non-convex problems is still not fully understood. Studying their performance in simple instances can help clarify phenomena like escaping critical points and generalization. Adaptive stochastic optimization methods like Adam are popular in deep learning for fast training, but they may not generalize as well as SGD solutions. The assumption that SGD's randomness helps escape local critical points is widely spread. Our contribution is to experiment with a simple model to challenge claims about randomized gradient algorithms. The landscape of non-global critical points in toy datasets is linked to neural nets' empirical loss functions. The statistical properties of optimizers like SGD and Adam are well understood in neural nets' loss functions. Comparisons with spectral and power methods reveal multiple regimes: strong signal regime, polynomial critical points where SGD outperforms Adam, and exponentially many critical points where all algorithms fail unless properly initialized. Single spiked models and stochastic gradients are also discussed in non-convex loss analysis. Principal component analysis (PCA) and the convergence of power method to the leading principal component are well-established in statistics and machine learning. Single spiked models have shed light on the problem difficulty as a function of the signal-to-noise ratio. The phase transitions in non-convex loss functions are studied using stochastic gradients and simplified models. The noise matrix spectrum forms a semi-circle between -2 and 2. Signal dilutes in weak signal-to-noise ratios, but pops out above critical value. Power method converges logarithmically with problem dimension. Objective function is minimized with theoretical leading eigenvalue. Random noise is added to gradient. The function \u03b3 = 2 for \u03bb < 1 or \u03bb + \u03bb \u22121 for larger \u03bb, with random normal noise added to the gradient for stochasticity. The Hessian is constant and positive semi-definite when \u03b3 is equal or larger than the leading eigenvalue of A. Data abundance explains the success of first order methods in solving the problem with weak signals accumulating in large datasets. The counter part to the strong requirement \u03bb d 1/4 is that accumulation of observations compensate low signals. The accumulation of observations compensates for low signal-to-noise ratio in each individual sample, as conjectured and proven in previous studies. Warm started power iteration can produce a vector u with high probability, relaxing the requirement for SNR in the average tensor case. This result is established using specific theorems and considerations from previous research. When solving a tensor PCA problem with a quadratic growth in observations, spectral warm start can reliably compute the solution. Numerical results compare algorithm performance in matrix and tensor PCA simulations, measuring signal recovery quality with cosine or u values and maximizing the Rayleigh log-likelihood objective function. Higher values are preferred for both metrics. Theoretical maximum value of the objective is the operator norm of the observed tensor A, compared with training loss. Signal to noise ratio \u03bb compared with number of observations in supervised learning. Stochasticity of gradients \u03c3 compared with number of sample points in minibatch. Plots show objective function values and cosine of ground truth with solution over iterates. Comparison made at different SNR values. Learning rate and stochasticity set by generating instances with noise matrices for optimization comparison. The plots compare optimization algorithms' performance and solution quality. Adam gets stuck quickly, while SGD outperforms Adam across different SNR values. Power method shows similar convergence rates to first-order methods. Gradient descent is equivalent to power iteration on a shifted matrix. The objective function is strongly convex, leading to stable convergence rates. In the tensor setting, spectral initialization is used to improve optimization performance. It helps locate the initial point in a basin of attraction for better solutions when \u03bb is large enough. Spectral initialization benefits optimization iterates by placing them in a convex problem area. In the basin of attraction, Adam struggles to find a solution as good as SGD's. Adding noise to gradients does not improve the problem difficulty. First-order methods fail to find the best basin of attraction, while spectral initialized methods succeed. The study focuses on optimizing non-convex objectives on simple synthetic datasets. The study focuses on optimizing non-convex objectives on simple synthetic datasets to rigorously study the properties of the problem, optimizer behavior, and solution. Toy data sets align with deep learning loss function properties, with resulting tensor problems appearing harder. Proper initialization is crucial for finding good solutions, while stochasticity in gradient estimates may not compensate for poor initialization. Each column in the algorithm iterations represents different values. The study emphasizes the importance of proper initialization in optimizing non-convex objectives on synthetic datasets. Different values are represented in each column of the algorithm iterations. Spectrally initialized SGD outperforms randomly initialized SGD and Adam in high SNR scenarios."
}