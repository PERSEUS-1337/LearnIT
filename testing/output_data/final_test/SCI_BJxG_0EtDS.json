{
    "title": "BJxG_0EtDS",
    "content": "Many real-world sequential decision-making problems can be formulated as optimal control with high-dimensional observations and unknown dynamics. A promising approach is to embed observations into a lower-dimensional latent representation space, estimate the latent dynamics model, and use it for control. The focus of this paper is on learning representations for locally-linear control algorithms like iterative LQR (iLQR). Three principles for the learned representation are accurate prediction in the observation space, consistency between latent and observation space dynamics, and low curvature in the latent space transitions. The paper focuses on learning representations for locally-linear control algorithms like iLQR by incorporating three principles: accurate prediction in the observation space, consistency between latent and observation space dynamics, and low curvature in the latent space transitions. A new variational-PCC learning algorithm is proposed, which shows improved stability and reproducibility in training, leading to superior control performance. The importance of all three PCC components for learning a good latent space for control is supported by ablation studies. The recent advancements in generative models have enabled successful dynamics estimation of high-dimensional decision processes, which can be used in conjunction with various decision-making techniques, including optimal control and reinforcement learning. One promising approach in the field of dynamics learning and control is learning controllable embedding (LCE). This involves learning a low-dimensional latent embedding of the observation space to conduct control. Two main approaches exist: defining a cost function in the high-dimensional space and learning the embedding through reinforcement learning, or first learning the embedding and then defining a cost function for control. In this paper, the focus is on determining the desirable traits that a latent embedding should exhibit for effective control/learning algorithms, specifically locally-linear control (LLC) algorithms. Three key properties are identified: prediction, consistency, and curvature. The Prediction, Consistency, and Curvature (PCC) framework is proposed for learning a latent space suitable for LLC algorithms. A latent variable model adhering to the PCC framework is designed, with a novel curvature loss for transition dynamics. The framework aims to minimize approximation error and optimize the solution of the LLC algorithm in the latent space. We propose a direct amortization of the Jacobian calculation in the curvature loss to improve training efficiency. The PCC model consistently outperforms E2C and RCE on control-from-images tasks. We focus on controlling non-linear dynamical systems with high-dimensional observations of each state. In controlling non-linear dynamical systems with high-dimensional observations, a stochastic optimal control problem is solved to minimize expected cumulative cost. The observations are generated by a stationary Markov process, and immediate costs are defined in the observation space X. In visual servoing, costs are bounded by c max and Lipschitz with constant c lip. The goal is to minimize tracking error by controlling the robot to reach the goal observation x goal. Algorithms based on learning a low-dimensional latent space Z have been developed to address the intractability of solving the optimal control problem in high-dimensional observation spaces. The LCE approach involves developing algorithms like E2C, RCE, and SOLAR to learn a triplet of encoder, dynamics, and decoder for mapping in latent space Z. It aims to solve the optimal control problem in the latent space by learning a mapping P through a loss function. The regularizer R2 is crucial in bridging the performance gap between two states (SOC1 and SOC2) by aligning their evolution paths through a principled selection process. This regularization term plays a key role in learning the latent space Z by incorporating prediction, consistency, and curvature components in the loss function of the PCC model. The PCC model involves two evolving spaces, X and Z, governed by dynamics P and F respectively. Learning P requires prediction and consistency terms to ensure consistent evolution paths in both spaces. These terms are crucial for solving general SOC problems and are further explored in the context of LLC algorithms in Section 3.3. In Section 3.3, the focus is on LLC algorithms like iLQR to solve SOC, incorporating the third term, curvature, in learning P. Transition in observation space under P and P is shown in Figures 1(a)(blue) and 1(c)(red). Instead of minimizing mismatch with P, learning P involves solving SOC with loss function similar to (SOC1). Lemma 1 explains setting the regularization term R3 in (SOC3) for control sequence from solving (SOC3). Recipe for learning P is provided in Section 3.1 through an intermediate (SOC3) evolving in observation space X according to dynamics P. In this section, the connection between (SOC2) operating in Z and (SOC3) operating in X is established. Lemma 2 provides guidance on setting the regularizer in (SOC2) to achieve similar performance to (SOC3) under their respective dynamics models. The expectation in Eq. 2 is over the state-action stationary distribution of the policy used for training samples. R2(P) can be seen as a measure of the transition probability over the next latent state given the current observation and action in (SOC2) and (SOC3). In this section, the connection between (SOC2) and (SOC3) is established. R2(P) measures the transition probability between latent states in the models. The regularizer in (SOC2) cannot be computed directly from the data, so a computable regularizer is proposed. Corollary 1 bounds the performance loss when using R2(P) instead of R3(P), showing it is still a reasonable choice. In this section, a loss function is derived to learn the latent space Z, focusing on locally-linear control (LLC) algorithms for solving (SOC2). The LLC algorithms iteratively compute an action sequence to improve the trajectory by linearizing the dynamics and adding a third term for curvature regularization to the loss function of the PCC model. The text discusses using locally-linear control algorithms to optimize the trajectory by linearizing the dynamics and imposing a penalty on the curvature of the latent space transition function in the context of solving a specific SOC problem. The text discusses using locally-linear control algorithms to optimize the trajectory by linearizing the dynamics and imposing a penalty on the curvature of the latent space transition function in the context of solving a specific SOC problem. The solution of (SOC-LLC) has similar performance to (SOC1), making it a reasonable optimization problem to learn P and the latent space Z. Instead of solving (SOC-LLC) jointly for U and P, it is treated as a bi-level optimization problem. In this section, a variational approximation is proposed to solve the outer optimization problem, min U L(U, F * ,c, z 0 ), to obtain the optimal control sequence U * with a large regularization parameter \u03bb LLC. The regularization parameters (\u03bb p , \u03bb c , \u03bb cur ) are left as hyper-parameters, and the loss function enforces prediction accuracy, consistency in latent state prediction, and low curvature over f Z, referred to as the prediction-consistency-curvature (PCC) loss. The PCC-Model objective introduces the optimization problem min P \u03bb p R 3 ( P ) + \u03bb c R 2 ( P ) + \u03bb cur R LLC ( P ). In this section, a variational approximation is proposed for the negative log-likelihood and batch-consistency losses, along with an efficient approximation of the curvature loss. The negative log-likelihood is bounded using Jensen's Inequality, assuming a recognition model employing bottom-up inference. A backward-facing model is chosen to account for noise in dynamics, with expectations estimated through Monte Carlo simulation to reduce estimator variance. The R 3,NLE-Bound is decomposed further for analytical tractability of Entropy and Kullback-Leibler terms when Q is restricted to a suitable variational. The consistency loss R2 can be analytically tractable with a suitable variational family for Q. The derivation is provided in Appendix C.1, where the distribution matching of zt+1 | xt+1 with zt+1 | xt is discussed. By making simplifying assumptions, a tractable variational bound can be constructed. A variant of the curvature loss is used in practice, which involves evaluating Taylor expansions and gradients. When nz is large, computation through Jacobians can be slow. See Appendix C.2 for a detailed derivation. When n z is large, evaluation and differentiation through Jacobians can be slow. To address this, the Jacobians can be amortized by treating them as coefficients of the best linear approximation at the evaluation point. This results in a new amortized curvature loss involving function approximators A and B to be optimized. The goal is to find the best linear approximation induced by A(z, u) and B(z, u) for any given (z, u) pair, making the behavior of F \u00b5 approximately linear in the neighborhood. PCC stands out from previous works like E2C and RCE by using a nonlinear latent dynamics model with an explicit curvature loss. In the E2C/RCE formulation, A and B cannot be treated as Jacobians of the dynamics, leading to difficulty in controlling stability during training. PCC separates prediction and curvature, demonstrating the importance of curvature loss for iLQR. RCE lacks PCC's consistency loss and all three frameworks are Markovian encoder-transition-decoder models. The reliance on minimizing prediction loss is a common factor among them. To reduce discrepancy between training and test-time usage, PCC minimizes consistency loss. E2C includes a regularization term similar to PCC but does not properly minimize prediction loss. Training and test sets are generated with triples (x t , u t , x t+1 ). To generate training and test sets, triples (x t , u t , x t+1 ) are sampled with an underlying state s t. Actions u t are sampled and next state s t+1 is obtained with noise added. Data is uniformly distributed by sampling state-action pairs. Models are evaluated under deterministic and stochastic scenarios with varying noise levels. Tasks have unobservable start and goal states, algorithms have access to corresponding observations. The algorithms have access to start and goal observations for control using the iLQR algorithm with a specific cost function. Performance is measured by the percentage of time spent in the goal region. A reproducible experimental pipeline involves training 10 models independently and solving 10 control tasks per model, totaling 100 tasks. Statistics are reported averaged over all tasks and the best performing model. The study evaluates the performance of the model over 10 tasks using a reliable evaluation pipeline. The RCE implementation optimizes the ELBO loss and explores different approaches. Results show that PCC outperforms baseline algorithms in noiseless dynamics. The study compares the performance of different algorithms on control tasks, showing that PCC has a more interpretable representation than other baselines for Planar and Inverted Pendulum Systems. PCC demonstrates superior interpretability for Planar and Inverted Pendulum Systems compared to other baselines in both noiseless and noisy dynamics cases. It also shows faster training with a 64% improvement over RCE and 2% improvement over E2C. Ablation analysis highlights the importance of consistency loss in maintaining control performance. The consistency loss is crucial for maintaining control performance in latent dynamics models. Removing the curvature loss leads to decreased control performance due to errors in the iLQR control algorithm. Models trained without amortized curvature loss perform slightly better than those with it. In this paper, the authors argue that learning a latent representation for control should prioritize good prediction in the observation space and consistency between latent transition and embedded observations. They found that using an amortized curvature loss in model training results in speed-ups of 6%, 9%, and 15% for Planar System, Inverted Pendulum, and Cartpole, respectively. The low-curvature dynamics are desirable when using iterative LQR as the controller, and all three elements of their PCC models are critical for stability and performance. The authors suggest that different controllers will have varying requirements for learned dynamics. They propose investigating biases for effective embedding and latent dynamics in other model-based control methods. The text discusses inequalities based on mathematical results and the expected cumulative KL cost with respect to control action sequences. The KL cost is calculated for a nonstationary state-action mapping \u03c0, with the expectation taken over the state-action occupation measure induced by data-sampling policy U. The proof involves inequalities related to deterministic policy \u03c0, sampling policy dU, and importance sampling factor bounds. The second part of the proof considers the optimality condition of the problem, leading to a chain of inequalities. The proof involves inequalities related to deterministic policy \u03c0, sampling policy dU, and importance sampling factor bounds. The first part of the proof shows a chain of inequalities using results from previous optimizations. The second part extends the argument to show optimality conditions for the problem. The proof involves inequalities for solutions of (SOC3) and (SOC2), connecting them with total-variation distance bounds and batch consistency regularizer analysis. By applying triangle inequality and Pinsker's inequality, the proof establishes a chain of inequalities. The proof involves inequalities connecting solutions of (SOC3) and (SOC2) with total-variation distance bounds and batch consistency regularizer analysis, establishing a chain of inequalities based on control action sequences and expected costs. The proof involves inequalities connecting solutions of (SOC3) and (SOC2) with total-variation distance bounds and batch consistency regularizer analysis, establishing a chain of inequalities based on control action sequences and expected costs. By applying Jensen's inequality and optimality conditions, the lemma shows that a LLC solution to (SOC-LLC) and a solution to (SOC1) satisfy certain conditions with high probability. The lemma discusses the effect of \u03b4 on LLC Performance by considering the proximity of the nominal state and actions to the optimal trajectory. It introduces a performance bound of the LLC algorithm in terms of the regularization loss R LLC, using Mahalanobis distance to measure the distance to a Gaussian distribution. The condition helps in choosing the parameter \u03b4 based on the trade-off between designing the nominal trajectory and optimizing R LLC. When \u03b4 is large, the R LLC regularizer enforces global linearity over a large state-action space, leading to higher loss and degraded performance. Conversely, a small \u03b4 results in lower loss and a tighter performance bound, but only if the state-action pairs are close to the optimal trajectory at each time-step. The analysis focuses on the noiseless deterministic dynamics case. In the noiseless deterministic case, the analysis focuses on the Lipschitz property of the immediate cost and the value function. Using Bernstein's inequality, one can establish bounds with probability 1 - \u03b7. In the noiseless deterministic case, the analysis focuses on the Lipschitz property of the immediate cost and the value function. Using Bernstein's inequality, bounds with probability 1 - \u03b7 are established. The value function is also Lipschitz with a constant (T \u2212 t + 1)c lip. At any state-action pair (z,\u0169), the value function inequality holds with Gaussian perturbations. The optimal control w.r.t. the Bellman operator at any latent statez is considered, along with the LLC loss function and Bellman operators w.r.t. latent SOC and LLC. The Bellman operator w.r.t. latent SOC and LLC is considered in the analysis. The inequality in (21) is further expressed, showing that all latent states are generated by encoding observations. By applying dynamic programming results, a bound in the value function is derived with probability 1 - \u03b7. The proof is completed by utilizing the Union Probability bound. The proof is completed by combining the results with Lemma 3. The control scheme follows iLQR solver to plan in the latent space using start and goal observations. A random trajectory is initialized and fed to the solver, with actions applied sequentially until the end of the problem horizon using a receding window approach. The solver optimizes for a fixed length of action sequences in the latent state SOC problem. The value function is determined by the nonlinear latent space dynamics model. The deterministic dynamics model is smooth, leading to well-posed Jacobian terms. The value function at each time instance satisfies a recursive fixed point equation. In the iLQR algorithm setting, a trajectory of latent states and actions is utilized. The iLQR algorithm utilizes a trajectory of latent states and actions to find an optimal policy and generate perturbed actions for local improvement. It updates the nominal trajectory by applying the sequence of actions to the environment. The algorithm aims to find a locally optimal control action sequence based on deviations of state and control actions. The iLQR algorithm aims to find a locally optimal control action sequence by computing a locally optimal perturbed policy that minimizes a second-order Taylor series approximation. The optimal perturbed policy has a closed-form solution with controller weights, leading to first and second order approximations of the Q-function. The iLQR algorithm computes a sequence of optimal control actions using first and second order approximations of the Q-function. However, it has limitations as it only finds open-loop optimal control actions and requires knowledge of a nominal trajectory at every iteration. In extending the iLQR paradigm to closed-loop RL, the iLQR-MPC procedure utilizes model predictive control (MPC) to generate a nominal trajectory and derive bounds for conditional log-likelihood and consistency loss. The text discusses the data collection process, domains, and implementation details for experiments involving generating training and test sets with state-action pairs. The observation-action data is uniformly distributed, and the robustness of each model is tested in both deterministic and stochastic scenarios. In the experiments, models are evaluated in deterministic and stochastic scenarios. The task involves navigating an agent in a 2D plane with obstacles using pixel images. The agent's goal is to reach the bottom-right corner within a certain distance. Agent's Objective: Control an inverted pendulum from 48x48 pixel images by swinging it up and balancing it. The system's state includes angle and angular velocity, with the goal of reaching the top position. Observations consist of two images from consecutive time-frames to maintain the Markovian property. CartPole is a visual task involving balancing a pole on a moving cart while avoiding boundaries. The control is 1-dimensional, with a 4-dimensional state indicating pole angle, angular velocity, cart position, and velocity. Observations are two 80x80 pixel images. The agent's objective is to keep the pole within a certain angle range. The goal is to move a 3-link manipulator from a downward resting position to a top position and balance it. The system's state is 2N-dimensional, indicating the relative angle and angular velocity at each link. The system's state is 2N-dimensional, representing the relative angle and angular velocity at each link. The observation state x t is a stack of two 80 \u00d7 80 pixel images of the N-link manipulator. The task takes place in the TORCS simulator on the michigan f1 race track. The goal is to balance the 3-link manipulator by keeping the sum of all poles' angles within \u00b1\u03c0/6 from an upright position. In the TORCS simulator, the task involves controlling a car to stay in the middle of the lane by steering actions. Observations are pre-processed into binary images, and the car's velocity is maintained around 10. The car initially steers strongly left or right for 20 steps, causing it to drift from the lane center. The TORCS simulator task involves controlling a car to stay in the middle of the lane by steering actions. The car initially drifts away from the center of the lane and needs to recover to stay within a certain distance from the middle. Various architectures and hyper-parameters were used for training the algorithms. The training process for the TORCS simulator involved tuning various parameters such as \u03bb values for different loss terms, adding a small coefficient term to stabilize the latent space, and incorporating a deterministic loss term based on cross entropy. The training process for the TORCS simulator involved tuning parameters like \u03bb values for loss terms and stabilizing the latent space with a small coefficient term. A deterministic loss term based on cross entropy was also added. Results for noisy dynamics across different domains were presented, with performance metrics shown in Table 3. The text chunk depicts 5 instances of learned latent space representations for noiseless and noisy planar systems from PCC, RCE, and E2C models."
}