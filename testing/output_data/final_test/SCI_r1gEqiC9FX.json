{
    "title": "r1gEqiC9FX",
    "content": "Modern neural networks are over-parametrized, allowing for modification of hidden units by adjusting input and output weights. A fast iterative method inspired by the Sinkhorn-Knopp algorithm minimizes the l2 norm of weights, improving test accuracy when interleaved with SGD during training. This approach provides an alternative to batch- and group-normalization on CIFAR-10 and ImageNet with a ResNet-18, enhancing performance in tasks like image classification, image segmentation, speech recognition, natural language processing, and playing the game of Go. Deep neural networks (DNNs) have achieved success in various tasks like image classification and playing Go, driven by large datasets, computational power, and deeper models. However, DNNs are prone to overfitting and underfitting, which can be mitigated using techniques like regularization, dropout, data augmentation, and stochastic gradient descent algorithms with carefully tuned learning rates. Training deep networks is challenging due to the vanishing/exploding gradient problem, studied for RNNs and feedforward networks. Gradients become too small or large, hindering convergence, alleviated by using non-saturating activation functions and better initialization schemes. Techniques like Batch Normalization have allowed vision models to achieve \"super-human\" accuracy. Batch Normalization (BN) was developed to train Inception networks, introducing intermediate layers that normalize features within the current batch. BN reduces training time, improves generalization capabilities, and diminishes the need for careful initialization. Network architectures like ResNet and DenseNet use skip connections with BN to enhance information flow during forward and backward passes. In this paper, the authors address the limitations of Batch Normalization (BN) in handling biases, convolutions, max-pooling, and skip-connections in modern CNN architectures. BN works well with large batch sizes, but for sizes below 16 or 32, the test error increases significantly due to high variance in batch statistics. This restricts the use of higher-capacity models, especially in applications like video recognition and image segmentation where batch sizes are limited. The authors introduce Equi-normalization (ENorm) as a novel algorithm to improve training speed and generalization accuracy of networks by utilizing over-parameterization to regularize them. ENorm focuses on positive-rescaling equivalent neural networks, scaling consecutive matrices with rescaling coefficients to minimize the joint p norm. This re-parameterizes the network under the constraint of implementing the Equi-normalization principle. The authors introduce Equi-normalization (ENorm) as a novel algorithm to improve training speed and generalization accuracy of networks by re-parametrizing the network within the space of rescaling equivalent networks. This algorithm focuses on architectures like ResNet18, converging with learned skip-connections, and introduces an iterative, batch-independent approach to ensure smooth gradient propagation during training. The paper introduces Equi-normalization (ENorm) as a novel algorithm to improve training speed and generalization accuracy of networks by re-parametrizing the network within the space of rescaling equivalent networks. ENorm extends to modern convolutional architectures, showing lower computational overhead compared to Batch Normalization (BN) and Group Normalization (GN). Applying ENorm after each SGD step outperforms BN and GN on CIFAR-10 and ImageNet datasets. The paper is organized into sections reviewing related work, defining the ENorm algorithm for fully-connected networks, adapting ENorm to convolutional neural networks, employing ENorm for training neural networks, and presenting experimental results. The code is available at https://github.com/facebookresearch/enorm. The section reviews methods for improving neural network training by normalizing activations and controlling the weight space geometry. Batch Normalization normalizes activations using batch statistics, leading to smoothing effects on the optimization landscape. Batch Normalization (BN) normalizes activations using batch statistics, but its performance suffers with small batch sizes. Batch Renormalization (BR) reduces sensitivity to batch size, while Layer Normalization and Instance-Normalization operate on different dimensions. Group Normalization (GN) divides channels into groups for effective replacement of BN with small batch sizes in computer vision tasks. Weight normalization techniques aim to replace Batch Normalization for small batch sizes in computer vision tasks. BID14 proposes a re-parametrization of weights to separate direction from weight norm. Optimization landscape considers anisotropic relation between model parameters and loss function for better training efficiency. Anisotropic relation between model parameters and loss function led to the introduction of natural gradient methods. These methods involve storing and inverting the curvature matrix, with various approaches to approximate its inverse. Equi-normalization focuses on weight balancing independently of batch concepts, aiming for a balanced network with good gradient backpropagation through an iterative algorithm. Equi-normalization introduces a method to balance the network using an iterative algorithm, different from implicit regularizers. It can be adapted to convolutional cases and has lower computational complexity compared to Batch Normalization or Group Normalization. The algorithm is inspired by Sinkhorn-Knopp and aims to balance the energy of the network by minimizing the global norm of matrices. Equi-normalization balances the energy of a network by minimizing the p-norm of its weights while preserving its function. It defines a fully connected feedforward neural network with q linear layers and ReLU activation. The network parameters are denoted by \u03b8, and rescaling equivalence is defined for them. Equi-normalization balances network energy by minimizing weight p-norm while preserving function. Rescaling equivalence for parameters \u03b8 involves positively scaling incoming weights and inversely scaling outgoing weights. Functional equivalence includes permutations of neurons within a layer. Objective is to find canonical parameter vector rescaling equivalent to given vector. Functional equivalence constraint is not addressed due to cases where it does not apply. The paper discusses minimizing the p norm of a neural network's weights within an equivalence class to find a unique canonical element. The ENorm algorithm uses block coordinate descent to generate a sequence of rescaling coefficients. The rescaled weights are minimized based on the rescaling coefficients. ENorm generates rescaling coefficients \u03b4 (r) through initialization and iteration steps. Algorithm 1 provides the pseudo-code for ENorm, balancing the network from 1 to q-1. The main convergence result for Equi-normalization is stated, with the proof relying on a coordinate descent Theorem by Tseng. ENorm generates rescaling coefficients \u03b4 (r) that converge to \u03b4 * as r \u2192 +\u221e. The rescaled weights minimize the global p norm and are unique regardless of the starting point. Matching biases are defined to preserve the input-output function in the network. In Section 6, Equi-normalization is adapted to introduce a depth-wise penalty on each layer by modifying the rescaling coefficients. Two ways of defining the weighting factors are explored: uniform and adaptive setups. The uniform setup penalizes layers exponentially based on depth, while the adaptive setup weights the loss by the size of the matrices. ENorm is extended to CNNs, focusing on the ResNet architecture and adapting it to convolutional or max-pooling layers. In Section 6, Equi-normalization introduces a depth-wise penalty on each layer by modifying rescaling coefficients. ENorm is extended to CNNs, focusing on the ResNet architecture and adapting it to convolutional or max-pooling layers. The rescaling of two consecutive convolutional layers is explained in Figure 2, preserving the function implemented by the composition of the layers. The MaxPool layer operates per channel by computing the maximum. Refer to Appendices C and E.1 for more details and implementation checks. The MaxPool layer in convolutional cases rescales activations to maintain functional equivalence with interleaving convolutional layers. ResNet-18 architecture uses learned 1x1 convolutions for shortcuts. Rescaling two consecutive blocks requires defining the process and computing coefficients. After updating the gradients, the network is balanced periodically without changing its function. Momentum is updated using a specific formula and matrices. ENorm cycles are performed after each SGD step. A method to jointly learn rescaling coefficients and weights with SGD is explored in the appendix. The computational advantage over BN and GN is shown in TAB2, detailing the number of elements accessed during normalization. ENorm is a method that is theoretically faster than BN and GN for a ResNet-18, requiring no extra-learnt parameters. It involves rescaling weights of convolutional layers to preserve the CNN function. The approach is analyzed and implemented using a tensor library. We analyze our approach by conducting experiments on MNIST, CIFAR-10, and ImageNet datasets. ENorm refers to Equi-normalization with p = 2. Input data is normalized by subtracting the mean and dividing by standard deviation. The encoder and decoder structures are FC(784, 1000)-ReLU-FC(1000, 500)-ReLU-FC(500, 250)-ReLU-FC(250, 30) with He's initialization for weights. Training parameters include learning rates {0.001, 0.01, 0.1}, batch size of 256, SGD with no momentum, and weight decay of 0.001. Path-SGD implementation follows the original paper closely, while GN cross-validates the number of groups {5, 10, 20, 50}. WN uses BN and a greedy layer-wise initialization. Results show that ENorm alone is competitive with BN and GN, while ENorm + BN outperforms all methods, including WN + BN. The adaptive setup excels without BN due to network structure, but with BN, results are less sensitive to parameter values. Poor performance results in no normalization and Path-SGD not being displayed. Initial experiments involve a fully-connected architecture for image input. The first experiment involves a fully-connected architecture with normalized input data and He's scheme for weight initialization. Training is done for 60 epochs with SGD, batch size of 256, and weight decay of 10^-3. ENorm outperforms BN and GN, while ENorm + BN is the best method for more than 11 intermediate layers. Path-SGD, GN, and WN are learned as detailed in Section 6.1. The CIFAR-NV architecture, described by Gitman & Ginsburg (2017), is used for training with normalized images. Random crops and horizontal flips are applied during training, while center crops are used at test time. The training set is split into one training set (40,000 images) and one validation set (10,000 images). Training is done for 128 epochs using SGD with a cross-validated initial learning rate and weight decay. The learning rate is decayed quadratically to 10^-4. Various parameter studies show similar results to the MNIST setup. The learning rate is decayed to 10^-4 quadratically. Different batch sizes and momentum are compared. Weight initialization follows He's scheme. BatchNorm layer is used for stabilization. ENorm + BN performs best on CIFAR-10 fully convolutional results. ENorm with c = 1.2 outperforms WN + BN. Parameter study for asymmetric scaling values can be found in the appendix. The dataset contains 1.3M training images split into 1000 classes. The dataset consists of 1.3M training images and 50,000 validation images divided into 1000 classes. The ResNet-18 model with type-C learnt skip connections is used for training. The experimental setup is similar to GN (Wu & He, 2018), training on 8 GPUs with batch mean and standard deviation computed per GPU. Kaiming initialization is used for weights, and data augmentation follows the standard scheme of BID19. Models are trained for 90 epochs using SGD with a momentum of 0.9. The learning rate is scaled linearly with an initial rate set to 0.1B/256, where batch size B is 32, 64, 128, or 256. Weight decay is adjusted based on batch size, and the learning rate is decayed quadratically to 10^-5. Results show that Top 1 accuracy on a ResNet-18 model is compared using different normalization schemes: Baseline, BatchNorm (BN), GroupNorm (GN), and ENorm. The performance of BN decreases with small batches, while ENorm shows promising results without the need for cross-validation of the number of groups. Additionally, a BatchNorm is added at the end of the network for stability and faster training. The results in Table 4 show that our method outperforms GN and BN for batch sizes 32 to 128. ENorm achieves better results after convergence compared to BN and GN, with less overfitting. However, applying ENorm to a deeper ResNet-50 model yielded unsatisfactory results, possibly due to difficulties in training without BN. Further investigation is needed. Equi-normalization (ENorm) is a method that balances weight energy in a network while maintaining its function. It converges towards a unique network with minimized weight norms, suitable for modern CNN architectures. ENorm incurs less computational overhead than Batch Normalization (BN) or Group Normalization (GN) and shows competitive performance. Further research is needed to optimize ENorm's convergence and training properties. Initial application to a fully connected network with 20 layers showed promising results. The network with 20 intermediary layers is artificially unbalanced by adjusting weights in specific layers. The ENorm algorithm naturally re-balances the network without training it. Assumptions are made regarding the differentiability of the function and compactness of X. The coordinate descent algorithm generates a bounded sequence leading to local minimizers of the function. Theorem 2 is applied to the function \u03d5 based on verified assumptions. Theorem 2 is applied to function \u03d5 with verified assumptions. For hidden layers, neurons are connected to input and previous layers. A ball exists where \u03d5 has a unique minimum, satisfying Assumption (2). The network naturally rebalances without training. The existence and uniqueness of the minimum in function \u03d5 is ensured by the connectivity of hidden neurons to input and output neurons. Each hidden neuron is connected to at least one input and one output neuron, leading to non-zero norms in the weight matrices. The proof shows that \u03d5 has at most one stationary point under specific connectivity assumptions. The proof shows that function \u03d5 has at most one stationary point on the set of edges with non-zero weights. To avoid the complexity of directly computing the gradient of \u03d5, a change of variables is defined using operators like S, P, and S H. This change of variables leads to a new function \u03c7, which is strictly convex and has at most one stationary point. The proof shows that function \u03d5 has at most one stationary point on the set of edges with non-zero weights. A change of variables using operators like S, P, and S H leads to a new function \u03c7, which is strictly convex. The sequence \u03b4 (r) converges as shown in Step 3, where it is bounded and has at least one cluster point. The injectivity of S H is proven by induction on the hidden layer index k, showing that x \u03bd = 0 for every neuron \u03bd at layer k. The proof demonstrates that function \u03d5 has only one stationary point on edges with non-zero weights. Using operators like S, P, and S H, a new strictly convex function \u03c7 is obtained. The convergence of sequence \u03b4 (r) is shown in Step 3, where it is bounded and has at least one cluster point. The injectivity of S H is proven through induction on the hidden layer index k, establishing x \u03bd = 0 for every neuron \u03bd at layer k. In the neural network, momentum is updated using Equation FORMULA1 and weights are updated using Equation (2). The weight tensors of consecutive convolutional layers are rescaled using rescaling matrices to preserve the function implemented by the composition of the layers. This rescaling procedure can be applied to any two consecutive convolutional layers with different parameters. The weight tensors of consecutive convolutional layers are rescaled using rescaling matrices to maintain functionality. Rescaling coefficients are calculated between blocks to adjust the weights of Conv1, Conv2, and ConvSkip layers in a ResNet architecture. The rescaling coefficients between convolution layers inside the same block and between two blocks are calculated to adjust the weight tensors in a ResNet architecture. Equivalent block weights are computed using electrical network analogies to deduce these coefficients. The most accurate method involves expressing the succession of convolution layers as a single ConvEquiv layer and then combining ConvEquiv and ConvSkip layers into a parallel equivalent weight. The text discusses the approximation of equivalent weights in convolutional neural networks using an iterative algorithm called Implicit Equi-normalization. It involves minimizing the global p norm of the network by performing alternative SGD and ENorm steps during training. The loss function of the network includes cross-entropy loss and weight decay regularization, with both weights and rescaling coefficients considered as learnable parameters. Automatic differentiation packages are used to compute derivatives of the loss function with respect to the weights. Implicit Equi-normalization (ENorm-Impl) is used in training neural networks by minimizing the global p norm through iterative SGD steps. The derivative of C with respect to rescaling coefficients is zero. Training time for ResNet-18 increased by 30% using PyTorch 4.0 BID11. ENorm-Impl generally performs better than the baseline but is not as effective as explicit ENorm, especially in deep networks. Rescaling coefficients are initialized to one and \u03bb is cross-validated in experiments. The Equi-normalization algorithm is applied to a ResNet architecture by integrating various methods. Sanity checks are performed before experiments, including verifying output probabilities and observing decreasing weights in the network during ENorm cycles. ENorm-Impl underperforms compared to ENorm due to not learning weights and rescaling coefficients at different speeds. The Equi-normalization algorithm is applied to a ResNet architecture, showing decreasing weights in the network as proven theoretically. Comparisons are made between training with or without ENorm, with ENorm consistently leading to lower energy levels. Different setups and values are tested, with the adaptive setup outperforming others, especially when using Batch Normalization. In testing different values of c without Batch Normalization (BN), c = 1, c = 0.8, and c = 1.2 were evaluated. The adaptive setup was also tested. Results showed that c = 1.2 performed the best, possibly due to ReLUs cutting energy during forward passes. Results with BN were less sensitive to c values."
}