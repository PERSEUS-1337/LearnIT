{
    "title": "rkgCJ64tDB",
    "content": "Encoding input scale information into a CNN benefits vision tasks with multiscale input signals. A scale-equivariant CNN architecture with joint convolutions across space and scaling group is necessary for scale-equivariant representations. To reduce complexity, convolutional filters are decomposed under separable bases and truncated to low-frequency components. This approach improves deformation robustness and performance in multiscale image classification. Improved performance in multiscale image classification and better interpretability than regular CNNs at a reduced model size. CNNs have achieved success in machine learning tasks like image classification, object detection, and semantic segmentation. Regular CNNs are translation-equivariant but not equivariant to other group transformations like rescaling and rotation. Encoding group information explicitly into the network representation can be beneficial for certain applications. Several network architectures have been designed to achieve this. Several network architectures have been designed to achieve roto-translation-equivariance (SE(2)-equivariance) using group convolutions and steerable filters. Scaling-translation-equivariant (ST -equivariant) CNNs have also been studied in the existing literature. A joint convolution across space and the scaling group S has not been proposed before to achieve equivariance in the most general form. Dealing with the scaling group presents difficulties due to its acyclic and unbounded nature, leading to increased model parameters and computational burden. Additionally, the deformation robustness of equivariant representation under imperfect scaling transformations has not been extensively studied in prior works. The paper proposes a general ST-equivariant CNN architecture with joint convolution over R2 and S, necessary for achieving equivariance. It also introduces a truncated decomposition of convolutional filters to reduce model size and computational cost. The representation stability of the architecture under equivariant scaling action is proven, addressing challenges posed by the non-compact scaling group S. Incorporating multiscale information into a CNN representation has been studied in many existing works, such as the Inception net and dilated convolutions. The challenges posed by the non-compact scaling group S have led to the introduction of a general ST-equivariant CNN architecture with computational efficiency and proved representation stability. Group-equivariant CNNs, proposed by Cohen & Welling (2016) and generalized in Cohen et al. (2018), encode group information into network representation, specifically for rotation groups SO(2) and SO(3). ST-equivariant CNNs have also been explored in literature but in a less general setting. Previous works have explored group-equivariant CNNs for rotation groups SO(2) and SO(3), while some have looked into ST-equivariant CNNs in a less general setting. Our proposed architecture introduces joint convolutions over R 2 \u00d7 S to impose equivariance, offering representation stability to input deformations. The scattering transform computes translation-invariant representations using predefined wavelet transforms and modulus poolings. DCFNet combines a pre-fixed filter basis with learnable expansion coefficients in a CNN architecture for data adaptivity and representation stability. This idea is extended to produce SE(2)-equivariant representations that are Lipschitz continuous to input deformations modulo a global rotation. A theoretical analysis of the deformation robustness of a ST-equivariant CNN has not been studied yet. Group-equivariance is the property of a mapping to commute with group actions on the domain and codomain. G-invariance is a special case of G-equivariance. Learning tasks where features change equivariantly to a group action should be equivariant to that action. In this paper, the focus is on constructing scaling-translation group (ST)-equivariant CNNs for tasks like image segmentation. The goal is to find an architecture that ensures each trained network commutes with the group action on the input and output. Inspired by previous work, the approach involves considering ST-equivariant CNNs with an additional scaling group index. The l-th layer output x (l) (u, \u03b1, \u03bb) is defined with spatial position u, scale index \u03b1, and unstructured channels \u03bb. The group action T \u03b2,v on the output involves scaling-translation in space and shift in scale channel. ST-equivariance is achieved through joint convolutions over S \u00d7 R 2. A feedforward neural network is ST-equivariant only if joint convolutions are conducted over S \u00d7 R 2. The joint convolution reduces to a spatial convolution, where feature maps at different scales do not transfer information among each other. Previous works on ST-equivariant CNNs are based on this special case. The joint convolutions on R 2 \u00d7 S provide a general way of imposing ST-equivariance, but they increase model size and computational burden. To address this, a truncated decomposition of convolutional filters under a separable basis is proposed, using eigenfunctions of the Dirichlet Laplacian. The spatial basis is the Fourier-Bessel (FB) basis, and spatial \"pooling\" is equivalent to rescaling the convolutional filters in space. The spatial \"pooling\" operation rescales convolutional filters in space by using expansion coefficients. During training, only the expansion coefficients are updated, leading to a reduction in network parameters and computational burden. Truncating the expansion to low-frequency components further decreases the number of trainable parameters. This approach has been considered in prior works and results in a more efficient ST-equivariant CNN. In an ScDCFNet, the number of parameters is reduced by using truncated basis expansion. For example, with specific values for L, L \u03b1, K, and K \u03b1, the reduction in parameters can be up to 19.2%. The computational cost in a forward pass of ScDCFNet is also reduced compared to a regular ST-equivariant CNN. The computational cost of an ScDCFNet is significantly reduced by truncating the filter decomposition, improving deformation robustness. This is beneficial as scaling transformations are often imperfect in practice due to local distortions. The norm of x (l) is used to quantify the distance between different feature maps at each layer. The norm of x (l) is defined to quantify the distance between feature maps in ScDCFNet. It adopts the L \u221e norm for \u03b1 due to the scaling group S having infinite Haar measure. Representation stability is quantified under assumptions on convolutional layers and input deformations, including non-expansive activation functions and bounded convolutional filters. The boundedness of A l in ScDCFNet is ensured by truncating basis decomposition to low-frequency components. Proper initialization of coefficients satisfies certain bounds on convolutional filters, making ScDCFNet layerwise non-expansive. Input deformation is assumed to undergo a global scale change. The representation stability of an ScDCFNet to input deformation and global scale change is demonstrated. The ScDCFNet remains approximately equivariant even after numerical discretization, handling both scale changes and nonlinear spatial deformations. This is important in theory and practice for object scaling. In numerical experiments, ScDCFNet demonstrates ST-equivariance, outperforming regular CNNs in multiscale image classification with reduced model size. Trained ScDCFNet auto-encoder can reconstruct rescaled input by applying group actions on image codes, encoding scale information explicitly. Tested on Scaled MNIST and Scaled Fashion-MNIST datasets by rescaling original images. The implementation details of ScDCFNet are explained in Appendix B.1, including how to discretize the integral and mitigate boundary effects. Modifications to maintain ST-equivariance are also discussed. Feature maps comparison with a regular CNN is detailed in Appendix B.2. Figure 2 displays feature maps of an original image and its rescaled version using different architectures. The images show rescaled layer features and the difference in padding for the scale channel. Figure 3 shows the numerical error in equivariance as network depth increases, with options to mitigate the error. The numerical error in equivariance can be mitigated by using joint convolutional filters with a smaller support in scale and replicate-padding instead of zero-padding. Even with numerical discretization, ScDCFNet remains approximately ST-equivariant. The error in equivariance increases as the network gets deeper, but can be alleviated by choosing smaller support filters. The improved performance of ScDCFNet in multiscale image classification is demonstrated through experiments on SMNIST and SFashion datasets. The network utilizes convolutional layers with specific architectures and feature map computations within a truncated scale interval. The comparison with regular CNNs shows the effectiveness of joint convolutional filters with smaller scale support and replicate-padding. ScDCFNet outperforms regular CNNs in multiscale image classification, especially with limited training data. The model achieves higher accuracy with fewer parameters by incorporating scale information directly into its representation. Even with data augmentation, regular CNNs fall short compared to ScDCFNet's performance. The ScDCFNet outperforms regular CNNs in multiscale image classification, achieving higher accuracy with fewer parameters. Data augmentation further improves ScDCFNet's performance, with accuracies reaching 94.30% and 80.62% on SMNIST and SFashion datasets. The model incorporates scale information directly into its representation, leading to superior results compared to regular CNNs. The ScDCFNet auto-encoder successfully encodes scale information directly into the representation, generating rescaled versions of the original input. This ability is illustrated in experiments where reconstructions using rescaled image codes show superior performance compared to regular CNN auto-encoders. The ScDCFNet auto-encoder encodes scale information into the representation, allowing for reconstructions of rescaled versions of the original input. This is demonstrated through experiments showing superior performance compared to regular CNN auto-encoders. The ScDCFNet auto-encoder incorporates scale information into the representation, enabling reconstructions of rescaled input. It utilizes joint convolutions across R2 and the scaling group S to impose ST-equivariant network representation, leading to improved deformation robustness. Experimental results demonstrate enhanced performance in multiscale image classification with greater interpretability and reduced model size compared to regular CNN models. The ScDCFNet model incorporates scale information into the representation, improving deformation robustness and enabling reconstructions of rescaled input. Future work includes exploring applications in object detection and pose estimation, as well as optimizing memory usage and computational efficiency through alternative implementation methods. The ScDCFNet model incorporates scale information into the representation, improving deformation robustness and enabling reconstructions of rescaled input. In a regular ST-equivariant CNN, the l-th convolutional layer is computed as a feedforward neural network with an extra index \u03b1 \u2208 S. The proof of the Theorem involves showing the sufficient and necessary parts for different values of l. The computational cost of a regular ST-equivariant CNN and an ScDCFNet with separable basis truncation is compared. The ScDCFNet reduces the computational cost by a factor of KK\u03b1 L2 L\u03b1. Lemma 1 and Lemma 2 are discussed before proving Proposition 1. The proof involves simplifying notation and showing that certain inequalities hold. The computational cost of ScDCFNet is compared to a regular ST-equivariant CNN. Lemma 1 and Lemma 2 are discussed before proving Proposition 1. The proof involves simplifying notation and showing that certain inequalities hold. To prove the case for any l > 1, we first recall certain equations and claim that for any \u03b1, certain conditions hold. The proof of Proposition 2 focuses on the case l > 1, simplifying notation and defining certain variables. The proof of Proposition 1 involves estimating E1 and E2 individually by bounding E2 and using gradient descent for training networks with specific architectures. The network architecture for the ScDCFNet and regular CNN auto-encoders is detailed in Table 3, including convolutional layers, average-pooling, fully connected layers, and batch normalization. Training involves using SGD with a decreasing learning rate over 20 epochs. Filter expansion in the ScDCFNet auto-encoder is limited to K = 8 and K \u03b1 = 3. The network architecture for the auto-encoders includes convolutional layers, transposed-convolutional layers, and spatial upsampling. Batch-normalization is used after each convolutional layer."
}