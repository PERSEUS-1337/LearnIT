{
    "title": "ByQZjx-0-",
    "content": "ENAS is a faster and less expensive approach to automated model design compared to previous methods. It involves a controller learning to discover neural network architectures by searching for an optimal path within a larger model. The controller is trained with policy gradient to select a path that maximizes the expected reward on the validation set. ENAS achieves state-of-the-art results on the Penn Treebank dataset and is more efficient than standard NAS, being over 10x faster and 100x less resource-demanding. Neural architecture search (NAS) is computationally expensive and time-consuming, requiring 450 GPUs and 3-4 days to train. The main bottleneck is training each child model to convergence. The goal is to remove this inefficiency by optimizing the search space and model selection process. The goal of this work is to improve the efficiency of neural architecture search by enabling more sharing between child models. This is similar to weight inheritance in neuro-evolution. The standard NAS involves training an RNN controller to search for a good architecture. All graphs in NAS can be seen as sub-graphs of a larger graph, represented as a single DAG. This design allows for parameter sharing among all architectures in the search space. There is a growing interest in improving neural architecture search efficiency. Various methods like learning curve prediction, iterative search for complex architectures, and hierarchical representation have been proposed. Weight inheritance in neuroevolution has shown positive effects. Recent approaches like ConvFabrics and SMASH are more computationally efficient but have limitations in flexibility. Recent approaches like SMASH and PathNet BID10 use hypernetworks and evolution to design architectures, but they may lead to misleading signals for reinforcement learning. ENAS introduces a search space for designing recurrent cells and convolutional architectures efficiently. ENAS introduces a search space for designing recurrent cells and convolutional architectures efficiently. The Directed Acyclic Graph of ENAS has N nodes where the controller RNN decides which edges are activated and computations are performed at each node. To create a recurrent cell, the controller RNN samples N blocks of decisions. The ENAS method introduces a search space for designing efficient recurrent cells and convolutional architectures. The controller RNN decides which edges are activated and computations are performed at each node in a Directed Acyclic Graph with N nodes. The controller samples N blocks of decisions to create a recurrent cell, where different activation functions are chosen for each node. The output is obtained by averaging all loose ends, nodes that are not input to any other nodes. In ENAS, the controller network decides which parameter matrices are used for recurrent cells in a search space with an exponential number of configurations. The controller network samples decisions via softmax classifiers in an autoregressive fashion using a two-layer LSTM BID16. The controller network in ENAS samples decisions in an autoregressive manner using softmax classifiers. It has two sets of learnable parameters: \u03b8 for the controller LSTM and \u03c9 for the shared parameters of child models. The training procedure alternates between training \u03c9 on the training data set and training \u03b8 for a fixed number of steps. In experiments with the Penn Treebank dataset, \u03c9 is trained for 450 steps with SGD on batches of 64 examples. In training the shared parameters \u03c9 of the child models, the policy \u03c0(m; \u03b8) is fixed and stochastic gradient descent (SGD) updates are performed on \u03c9 to minimize the expected loss function. The gradient is computed via the Monte Carlo estimate, providing an unbiased estimate of the gradient. With an appropriate learning schedule, the SGD updates of \u03c9 converge almost surely, although they have a larger variance than SGD on a fixed model m. In training the shared parameters \u03c9 of the child models, stochastic gradient descent (SGD) updates are performed on \u03c9 to minimize the expected loss function. The updates on \u03c9 have a larger variance than SGD on a fixed model m. However, using M = 1 works fine, updating \u03c9 with the gradient from any single model m sampled from \u03c0(m; \u03b8). The policy parameters \u03b8 are then updated to maximize the expected reward using the Adam optimizer with REINFORCE and a moving average baseline. The reward is computed on the validation set to select models that generalize well. The perplexity is computed on a minibatch sampled from the validation set. In experiments with image classification, the reward function is the classification accuracy on a minibatch of images from the validation set. Novel architectures are derived from a trained ENAS model by sampling models from the policy \u03c0 (m, \u03b8) and selecting the one with the highest reward to re-train. The search space for convolutional architectures involves sampling decisions for connecting to previous nodes and choosing activation functions. The model samples decisions for connecting to previous nodes and choosing computation operations to construct convolutional layers. Instead of deciding on specific operations like convolution or pooling, a mask is selected over all choices, including convolutions, average pooling, and max pooling. This approach allows for greater flexibility in designing convolutional architectures. The network uses conv7x7, average pooling, and max pooling operations with convolutional parameters for each layer. Skip connections are sampled by the controller network, concatenating outputs from previous layers. Training of the controller LSTM and child models alternates in 2000 steps with the Adam optimizer and REINFORCE. The network uses conv7x7, average pooling, and max pooling operations with convolutional parameters for each layer. Skip connections are sampled by the controller network, concatenating outputs from previous layers. Training of the controller LSTM and child models alternates in 2000 steps with the Adam optimizer and REINFORCE. For each phase of training the shared parameter \u03c9, we train it for 450 minibatches, each has 100 images, using Nesterov Momentum BID32. An alternative to designing the entire convolutional network is to design smaller modules and then fit repeats of them together BID40. FIG3 illustrates this approach, where the convolutional cell and reduction cell architectures are to be designed. We discuss how to use ENAS to search for the architecture of these cells by sampling both the convolutional cell and reduction cell using an RNN controller. The network utilizes separable convolution with kernel sizes 3x3 and 5x5, along with average pooling and max pooling with kernel size 3x3. Each cell in the network receives input from two nodes, indexed as node 1 and node 2. The controller is depicted in FIG4 operating in a search space with 4 nodes. ENAS is used to design recurrent cells for language modeling on the Penn Treebank dataset and convolutional architectures on the CIFAR-10 dataset. An ablation study demonstrates ENAS's role in discovering novel architectures and its efficiency. ENAS is applied to language modeling using the Penn Treebank dataset. The controller is trained with specific parameters and sampling techniques. A novel recurrent cell is discovered by ENAS, outperforming models with similar parameters. Highway connections are added between nodes in the recurrent cell for improved performance. The ENAS algorithm is applied to language modeling on the Penn Treebank dataset, discovering a novel recurrent cell that outperforms models with similar parameters. The shared parameters of the child models are trained using SGD with specific learning rate and decay factors. Results show that ENAS finds the recurrent cell in less than 10 hours, with performance compared to other methods presented in Table 1. The ENAS algorithm discovers a novel recurrent cell that outperforms other methods in language modeling. The cell has interesting properties with all non-linearities being ReLU or tanh. Experimentation shows that changing non-linearities affects perplexity. The CIFAR-10 dataset is used for training and testing with standard data pre-processing techniques. The study explores data pre-processing and augmentation techniques for training images, including padding and cropping. A search space for convolutional architectures is presented, allowing decisions on skip connections and channel masks. Different versions of the search space are explored to improve results. The masks over channels are searched for in a 12-layer network using a fixed pattern of skip connections. The study focuses on exploring data pre-processing and augmentation techniques for training images, including skip connections and channel masks in a 12-layer network. Training details include parameters trained with Nesterov momentum and cosine schedule, with architectures recommended by the controller also trained using the same settings. The study explores data pre-processing and augmentation techniques for training images in a 12-layer network. Training parameters include using the Adam optimizer with a learning rate of 10^-3. Techniques to prevent premature convergence of REINFORCE are applied, such as adjusting temperature and tanh constants, adding entropy term to the reward, and enforcing sparsity in skip connections. Tricks for stabilizing and improving training performance are also discussed, including the structure of convolutional layers with batch normalization and ReLU layers. The operation in the method involves batch normalization followed by a ReLU layer. The alternate setting of batch norm-conv-ReLU yields worse results. Stabilizing Stochastic Skip Connections involves concatenating outputs from multiple layers, followed by a 1x1 convolution, batch normalization, and ReLU layer. Global Average Pooling averages activations before passing them to the Softmax layer to reduce parameters and avoid overfitting. These tricks are crucial for reducing variance in gradient updates of shared parameters. ENAS successfully found architectures that outperform other automatic model designing approaches with the same computing resource usage. In the general search space, ENAS takes 15.6 hours to achieve a 4.23% error rate on CIFAR-10, outperforming most models while using significantly less time and computing resources. In the restricted search space, ENAS takes 11.6 hours to achieve a 4.35% error rate, with the resulting model often having 64 or 96 channels at each branch and layer. The controller in ENAS activates specific blocks in each layer to prevent overfitting. ENAS can discover skip connection patterns comparable to human-designed architectures. It takes 12.4 hours to find skip connection patterns in a restricted search space. ENAS outperforms other methods on CIFAR-10 with a 4.23% error rate in the general search space. ENAS outperforms various approaches in designing networks on CIFAR-10, including MicroNAS and achieves similar performance to NASNet-A. The model forms skip connections more densely at higher layers, resulting in a test error of 5.04%. Increasing the number of output channels to 512 reduces the error to 3.87%. In the search space over cells, ENAS takes 11.5 hours to discover optimal configurations. ENAS achieves a test error of 3.87% on CIFAR-10. It takes 11.5 hours to discover optimal configurations in the search space over cells. Replicating the convolutional cell 6 times results in a 3.54% test error, comparable to NASNet-A. With CutOut, ENAS's error decreases to 2.89%. Ablation studies show that randomly selecting configurations leads to a model with 47.1M parameters and an error rate of 5.86%, significantly worse than ENAS-designed models. Neural Architecture Search (NAS) is a significant advancement for faster neural network design, but its computational expense limits widespread adoption. ENAS, an alternative method, requires significantly fewer resources and time by sharing parameters across child models during architecture search. Proper training of the ENAS controller is crucial for optimal performance, as shown by the higher error rates when not updating the controller. Neural Architecture Search (NAS) involves sharing parameters across child models during architecture search. The method works well on CIFAR-10 and Penn Treebank datasets. Shared parameters include matrices and activation functions selected by the controller. Batch normalization is added to stabilize training. The method of Neural Architecture Search (NAS) involves sharing parameters across child models during architecture search, working well on CIFAR-10 and Penn Treebank datasets. Each branch configuration in the network has its own embedding and softmax head, with specific matrix sizes for prediction. Layers 4 and 8 are max pooling layers that reduce spatial dimensions, using 3x3 convolutions with 48 output channels. The controller RNN guides the search process. The controller RNN in Neural Architecture Search uses 3x3 convolutions with 48 output channels at all layers. It has the ability to form skip connections between layers and pads outputs to maintain spatial dimensions. The first convolutional cell is created by performing a 3x3 convolution on the image. The first 6 convolution cells have 32 output channels each, with reduction cells applied similarly but with a stride of 2. The final models are trained for 310 epochs using a cosine learning schedule and an auxiliary head is inserted before the second reduction cell BID35."
}