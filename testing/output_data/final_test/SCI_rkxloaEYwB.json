{
    "title": "rkxloaEYwB",
    "content": "Planning in high-dimensional space remains a challenging problem despite recent advances in algorithms and computational power. Inspired by neuroscience theories, our goal is to enable agents to create mental models of their environments for planning. We emulate the cerebellum using a two-stream, fully connected predictor network that receives efference and current state features as inputs. By utilizing knowledge distillation methods, we generate a compressed representation of the current state for fast search using classical graph search algorithms. Our approach is demonstrated to be effective on a viewpoint-matching task using a modified best-first search algorithm. When manipulating an object, our visual system helps us predict its appearance after an action. Agreement between our experience and prediction indicates understanding. Recent work focuses on compact state representations for efficient planning in lower-dimensional latent spaces. The CNS sends commands to the motor system and cerebellum for coordination. The cerebellum receives sensory information and compares it with intended consequences to make adjustments for future movements. It learns by processing a large amount of input data and outputting a smaller amount, allowing for planning in a low-dimensional space. This helps reduce the complexity of tasks and enables classical AI methods to operate effectively. The text discusses a link between efference theory and classical planning, introducing a search method for reduced state-space search. Experimental validation is done on visual data with categorical actions, creating a manipulation task using the NORB dataset. The dataset can be embedded on a cylinder or sphere, visualizing actions as traversing the embedded manifold to approximate a Markov process. The text discusses the use of reinforcement learning in predicting future states without assuming a reward signal. Unlike previous works, the approach does not rely on a reward signal or operate within a reinforcement learning setting during training. Similar to Oh et al. (2015), the method predicts future frames in ATARI environments conditioned on actions to improve exploration by informing agents of more likely actions. Our work focuses on maneuvering within the latent space rather than the full input space, combining vision, memory, and controller modules to learn a model of the world. We train a predictive model in an unsupervised manner, using pre-trained networks for transfer learning to apply the method to a generic representation. This approach allows the agent to learn policies entirely within its learned latent space representation of the environment. The text discusses knowledge distillation, where a smaller model learns from a larger model's output. It also mentions a method called Causal InfoGAN for learning plannable representations using generative adversarial networks. This approach extends the idea by incorporating actions as input to the predictor network. Our method involves training a simple forward model without reconstruction, inspired by the concept of efference copies from neuroscience. Efference copies are signals sent from the central nervous system to the peripheral nervous system, allowing for prediction of sensory outcomes by minimizing errors between predictions and actual sensory inputs. This process distinguishes between exafference and reafference signals, providing a basis for training the model. By creating an efference copy and training the forward model, we can distinguish between exafference and reafference signals. The motor system and sensory system are assumed to be fixed, with the motor system issuing efference commands. Pre-trained CNNs provide powerful descriptors while reducing computational load. A motor command is issued by the CNS, with a copy sent to a forward model to predict sensory outcomes. The forward model is updated to minimize discrepancies between predicted and actual outcomes. A predictor is trained using a training set to enable efficient planning in a latent space. The predictor enables efficient planning in a latent space by finding a path between a start state and a goal state, treating the environment as a graph with states as nodes and actions as edges. A modified best-first search algorithm is used with EfferenceNets to determine the sequence of actions connecting the start node to the solution node. The algorithm aims to find the optimal path from the start node to the solution node by avoiding previously evaluated states and simplifying paths to reduce redundancies. This process is crucial in solving temporally-extended Markov decision problems efficiently. The algorithm finds a plan to reach a goal state by exploring representations of states and adapting the strategy to optimize the reward signal in reinforcement learning. This process requires generating samples or having exact knowledge. In this paper, a one-step prediction model is used to make decisions based on predicted outcomes from one-step lookaheads. The method, known as hillclimber or best-first search, relies on heuristics like Euclidean distance to guide decision-making towards a known target efficiently. The effectiveness of using Euclidean distance as a heuristic for decision-making is dependent on how well it encodes the actual distance to the goal state. Pre-trained VGG16 models can already provide sufficient guidance for this purpose. However, for a more suitable representation that considers the topological structure of the data manifold, spectral embeddings like Laplacian Eigenmaps are recommended. These methods may only be applied in an in-sample fashion due to limitations in out-of-sample embedding. In experiments, the method is combined with graph search algorithms for manipulation tasks using the NORB dataset. Actions involve turning a turntable, moving the camera, and changing illumination. EfferenceNet is trained for viewpoint matching tasks to find a sequence of actions transforming start to goal states with different azimuth and elevation configurations. EfferenceNet is a neural network trained to predict feature map changes after actions are performed in a state. The network architecture consists of two streams with dense layers and BatchNorm, followed by 3 more dense layers with ReLU activation. It is optimized with Nadam and converges in two hours on a Tesla P40. The EfferenceNet neural network predicts feature map changes after actions in a state. It uses a Laplacian Eigenmap for in-sample search and VGG16 for out-of-sample search. The network is trained on 9 car toys from the NORB dataset and tested on one. The Laplacian Eigenmaps embed toy configurations in three dimensions, encoding elevation and azimuth angles. The EfferenceNet neural network uses Laplacian Eigenmaps to predict feature map changes after actions in a state. The network is trained on 9 car toys from the NORB dataset and tested on one, encoding elevation and azimuth angles in three dimensions. The training results show monotonically decreasing distance as the prediction gets closer to the target, which can be effectively used for a greedy heuristic. Visualizations include heat maps of similarities and a histogram illustrating search accuracy. The elevation changes are more fine-grained than azimuth changes by a factor of 4. Heat maps can reveal attractor states during the search, with the goal and its flipped version being attractors. Lighting conditions can also affect the aggregate heat map. The focus is on learning a transition model for control after learning. In this work, the focus is on learning a transition model for control after learning. The approach allows for quick learning of new reward functions and accommodates evaluations of reward trajectories with arbitrary discounts. It is useful in sparse or late reward settings and not restricted in optimizing discounted or undiscounted expected returns. Simulation-based planning methods can optimize arbitrary functions of predicted reward trajectories. Using a pre-trained network for flexibility in predicting reward trajectories may lead to irrelevant features being encoded, affecting the accuracy of informed search methods like best-first search. Visualizations show gradients towards the goal state and visually similar distant states, with varying levels of similarity hindering planning. The EfferenceNet, paired with a generic feature map, allows for accurate search in manipulating unseen objects. Inspired by the cerebellum's neurology, this method shows promise for future work. Validation is done on a viewpoint-matching task from the NORB dataset. EfferenceNets calculate features of the current state and action in deterministic environments, paving the way for future research combining them with successor features. Effective feature maps are crucial in this line of work. In this line of work, feature maps are crucial. Laplacian Eigenmaps and pre-trained deep networks are utilized. While end-to-end training could enhance system performance, focusing on generic multi-purpose representations is deemed more promising. Slow Feature Analysis (SFA) is also considered as a potential method for improving the system."
}