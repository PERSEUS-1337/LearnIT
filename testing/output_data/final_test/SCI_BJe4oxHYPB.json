{
    "title": "BJe4oxHYPB",
    "content": "The Lottery Ticket Hypothesis by Frankle & Carbin (2019) suggests that small sub-networks of neural networks can train faster and perform better than the original models. The Iterative Magnitude Pruning (IMP) algorithm consistently finds sub-networks with significantly fewer parameters (90-95%) that outperform the overparameterized models they were extracted from, with potential applications in transfer learning. The paper introduces Continuous Sparsification, a new algorithm that removes parameters from a network during training to find winning tickets. It outperforms Iterative Magnitude Pruning, providing faster search and addressing issues of overparameterization in deep neural networks. Recent research has shown that overparameterization is crucial for network capacity, generalization, and training dynamics. However, it is uncertain if overparameterization is essential for achieving state-of-the-art performance. Empirical methods have successfully identified less overparameterized neural networks through post-training reduction or more efficient architectures. New methods combine these approaches to optimize efficient architectures. Pruning already trained networks maximizes parameter efficiency, but pruned networks are challenging to train from scratch, indicating that overparameterization may not be necessary for model performance. Recent research suggests that overparameterization may not be necessary for successful network training. Pruned networks trained with the same parameter initialization as the original models have shown faster training and superior performance. The search for winning tickets, defined as sub-networks with randomly-initialized parameters, has become a topic of interest in transfer learning. The standard algorithm to find winning tickets is Iterative Magnitude Pruning (IMP), which involves a 2-stage procedure alternating between parameter optimization and pruning. The Iterative Magnitude Pruning (IMP) procedure involves parameter optimization and pruning, requiring multiple rounds of training and pruning to find winning tickets in deep neural networks. However, IMP's reliance on a pruning strategy and resetting parameters back to initialization at each iteration make it time-consuming. To address this, a novel method called Continuous Sparsification continuously removes weights from the network during training, aiming to speed up the search for winning tickets. Continuous Sparsification is a method that approaches the search for sparse networks as a 0-regularized optimization problem, offering superior performance in pruning VGG and finding winning tickets in Residual Networks trained on CIFAR-10. Continuous Sparsification efficiently discovers winning tickets in fewer iterations compared to Iterative Magnitude Pruning, without the need for parameter rewinding. This method supports the Lottery Ticket Hypothesis by demonstrating the existence of sparse sub-networks that outperform the original network with less training. Iterative Magnitude Pruning (IMP) involves training, pruning, and re-initializing a network to find winning tickets that outperform dense networks. These sub-networks can generalize across datasets and training methods, making IMP a promising tool for knowledge transfer applications like transfer or meta learning. Zhou et al. (2019) conducted experiments to better understand the Lottery Ticket Hypothesis and proposed a method to learn. The authors propose a method to learn binary masks through SGD instead of magnitude-based pruning, showing that learned masks encode valuable information about a problem's solution. Finding a sparse sub-network that performs well compared to its dense counterpart is crucial, achieved through pruning methods like IMP. The iterative algorithm outperforms \"one-shot pruning\" by continuously removing parameters during training based on a criterion. It approximates 0 regularization on network weights, eliminating the need for complex pruning strategies. Gradient estimators are used to train both weights and sparsity parameters, automatically removing components that do not contribute to the network. This approach aims to find winning tickets efficiently by sparsifying networks. Sparsity in networks is crucial for finding winning tickets efficiently. Unlike Iterative Magnitude Pruning, 0-regularization methods optimize a clear trade-off between sparsity and performance by continuously sparsifying networks during training. This approach ensures that the number of removed parameters is maximized without compromising the model's performance. Performing sparsification continuously is done automatically by the optimizer, framing the search for sparse networks as a loss minimization problem with regularization. The problem becomes combinatorial due to constraints, but can be addressed using the sigmoid function and temperature parameter. The problem of finding sparse networks can be addressed by using a sigmoid function and a temperature parameter. Gradients of \u03c3(\u03b2s) vanish as \u03b2 increases, suggesting slow annealing during training. A stochastic mapping s \u2192 m can be used to avoid the binary constraint, with a deterministic parameterization m = b(s) considered as an alternative approach. The problem of finding sparse networks can be addressed by using a smooth relaxation of a step function b(z) with a sigmoid function \u03c3(z). By controlling a temperature parameter \u03b2, we can interpolate between the smooth function \u03c3(s) and the original goal b(z), illustrating computational challenges. Increasing the temperature parameter \u03b2 while optimizing s and w with gradient descent can lead to sparsification of the network. As \u03b2 approaches infinity, negative components of s are mapped to 0, effectively removing their corresponding weight parameters. This process results in actual sparsification during training, as long as \u03b2 is sufficiently large. By minimizing the loss function L \u03b2 (w, s) for T parameter updates with gradient descent and annealing \u03b2, sparse networks can be learned. Increasing the temperature parameter \u03b2 during optimization can lead to network sparsification by mapping negative components of s to 0. This process results in actual sparsification during training, as long as \u03b2 is sufficiently large. The method presented offers an alternative to magnitude-based pruning for finding winning tickets, with a strict constraint that the learned mask m must be binary to avoid learning the magnitude of weights. The method involves incorporating techniques from successful methods for learning sparse networks and searching for winning tickets. It includes enabling \"kept\" weights to be removed from the network at a later stage based on positive components of s after many iterations. Additionally, parameter rewinding is performed to reset soft mask parameters for remaining weights without interfering with removed weights. The proposed algorithm, \"Continuous Sparsification,\" aims to find winning tickets in neural networks by comparing different methods. Parameter rewinding is mentioned as a key component, but it is optional in the algorithm. The evaluation focuses on generalization performance and the cost of the search procedure. The proposed algorithm, \"Continuous Sparsification,\" compares different methods to find winning tickets in neural networks. It evaluates generalization performance and the cost of the search procedure by comparing it to Iterative Magnitude Pruning and Iterative Stochastic Sparsification (ISS). ISS uses a stochastic re-parameterization method to remove weights from the network. The hyperparameters used in this section were chosen based on analysis presented in Appendix, where we study how the pruning rate affects IMP, and how \u03bb, s 0 and \u03b2 T interact in CS. Training a neural network with 6 convolutional layers on the CIFAR-10 dataset, the network consists of 3 blocks of resolution-preserving convolutional layers followed by max-pooling, with fully-connected layers and ReLU activations. The network is trained with Adam with a learning rate of 0.0003 and a batch size of 60. Learning a \"supermask\" involves training a binary mask that yields competitive performance when applied to a network with randomly initialized weights. Two methods, ISS and CS, are compared over 100 epochs. ISS is equivalent to Stochastic Sparsification, controlling sparsity with varying parameters. Continuous Sparsification varies \u03bb to achieve sparsity levels. Continuous Sparsification (CS) outperforms Stochastic Sparsification (SS) in terms of training speed and mask quality. CS achieves over 75% sparsity with over 75% test accuracy, while SS's performance decreases with sparsity over 50%. CS also shows faster progress in training, indicating that optimizing a deterministic mask is quicker than learning a distribution over masks. Parameters are trained using Adam with a learning rate of 3 \u00d7 10 \u22124, and pruning rates of 15%/20% are used for convolutional/dense layers in CS. In Continuous Sparsification (CS), pruning rates of 15%/20% are used for convolutional/dense layers. The Bernoulli parameters of ISS are initialized with s 0 = 1 and trained with SGD. CS anneals the temperature from \u03b2 0 = 1 to \u03b2 0 = 250 and limits each run to 4 iterations. Six runs of CS are performed with different mask initialization values. The study compares Stochastic Sparsification (SS) and Continuous Sparsification (CS) on a 6-layer CNN on CIFAR-10. CS learns masks faster with similar performance, achieving sparser masks while maintaining test accuracy. CS outperforms SS in finding tickets on both CNN and ResNet 20 models. Experiments show that tickets found by Continuous Sparsification (CS) outperform those found by IMP. CS consistently finds winning tickets in 2 iterations and outperforms IMP in terms of test accuracy. The quality of tickets found by CS is illustrated by the Pareto curve, showing superior performance compared to IMP for tickets with less than 97% sparsity. Ticket performance with Continuous Sparsification (CS) is superior to IMP, even in scenarios where specific sparsity is desired. CS is more time-efficient than IMP, especially when searching for tickets in realistic models like ResNets. IMP struggles to find winning tickets in ResNets without adjustments, leading to subpar performance. The authors propose a modification to IMP for successful ticket search on complex networks by initializing tickets with weights from early training. Continuous Sparsification outperforms IMP in finding winning tickets in ResNet-20 on CIFAR-10, even in scenarios where specific sparsity is desired. The network is trained with SGD, a learning rate of 0.1, and momentum of 0.9 for 85 epochs per iteration, with a batch size of 128. The authors propose a modification to IMP for successful ticket search on complex networks by initializing tickets with weights from early training. Continuous Sparsification (CS) outperforms IMP in finding winning tickets in ResNet-20 on CIFAR-10. CS runs for a total of 5 iterations with varying mask initialization values. The Continuous Sparsification (CS) method transfers weights between iterations, eliminating the need for network re-training. CS outperforms IMP in finding winning tickets with varying sparsity in under 5 iterations. CS quickly sparsifies the network in a single iteration and typically finds better tickets than IMP after only 2 rounds. Additionally, running CS for 2 iterations in parallel outperforms IMP. Not rewinding in CS leads to a degradation in performance for highly sparse tickets after 2 or more iterations. The performance of tickets with high sparsity degrades quickly after 2 or more iterations of Continuous Sparsification (CS). Not rewinding between iterations causes a significant increase in distance between parameter iterates, leading to sub-optimal masks for weight values. Continuous Sparsification efficiently finds winning tickets without the need for re-training, preventing overtraining and maintaining model performance. In evaluating a pruning technique for finding tickets quickly, a VGG is trained on the CIFAR-10 dataset using various methods like Continuous Sparsification, Magnitude Pruning, and Stochastic Sparsification. The network is sparsified after 160 training epochs and fine-tuned for 40 epochs to assess the quality of learned masks. Sparsification methods like ISS, IMP, and CS are used for global pruning in training a VGG on CIFAR-10 dataset. Different sparsity levels are evaluated with global pruning rates ranging from 50% to 99.75%. Continuous sparsification outperforms stochastic sparsification in performance. The deterministic re-parameterization proposed in this study outperforms magnitude pruning, suggesting that 0-based methods could advance pruning techniques. Sparse sub-networks, known as winning tickets, can be trained from scratch, challenging the belief that overparameterization is necessary for neural network optimization. The search for winning tickets is an underexplored area, with potential to reduce resources needed for training deep networks. In the search for winning tickets, Continuous Sparsification algorithm is proposed as a method to find sparse sub-networks by removing parameters continuously during training. It questions the effectiveness of post-training pruning and highlights the need to quickly find winning tickets in overparameterized networks. The Continuous Sparsification algorithm aims to find sparse sub-networks by adjusting hyperparameters like final temperature, penalty strength, and mask initial value. The study focuses on how these hyperparameters impact the performance of Continuous Sparsification when training a ResNet 20 on CIFAR-10. The study explores the impact of varying hyperparameters on the performance of Continuous Sparsification when training a ResNet 20 on CIFAR-10. Different settings for fixed hyperparameters result in varying sparsity and performance of the found tickets. Varying \u03bb between 0 and 10^-8 for different settings shows little impact on performance or sparsity, except for one case where slightly increased sparsity is observed. The study suggests using higher values for \u03b2 T when pruning or performing ticket search, with recommended values around 150 or 200. Varying the initial mask value s 0 between -0.3 and +0.3 has a strong effect on sparsity, with proper tuning recommended to achieve specific sparsity levels. Binary search over s 0 values can be performed to reach desired sparsity levels. In this experiment, the running time of Iterative Magnitude Pruning is evaluated by increasing the amount of parameters pruned at each iteration. A ResNet 20 is trained on CIFAR-10 with IMP for 30 iterations, showing that the performance of tickets found by IMP decreases when the pruning rate is increased to 40%. The final performance of tickets found by IMP decreases with more aggressive pruning rates, indicating IMP is not compatible with higher pruning rates."
}