{
    "title": "B1gskyStwr",
    "content": "Model-based reinforcement learning, specifically the Dyna architecture, is effective in improving sample efficiency by integrating learning and planning. A key component in Dyna is search-control, which involves generating state or state-action pairs to query the model for simulated experiences. In this work, a novel search-control strategy is proposed based on searching high frequency regions on the value function. The strategy is inspired by Shannon sampling theorem, suggesting that high frequency functions are harder to approximate. This approach aims to improve learning efficiency by focusing on states in high frequency regions. The text suggests a search-control strategy based on measuring the frequency of a function by gradient norm. This approach is applied to search-control in Dyna for model-based reinforcement learning, showing effectiveness on benchmark domains. The Dyna agent uses real experience to learn a model and improve policy through simulated experiences. Experience replay (ER) is a related method that randomly samples recorded experiences to update the policy. The search-control mechanism in Dyna selects states or state-action pairs to query the model for simulated experiences. The search-control mechanism in Dyna is crucial for the model-based agent's efficiency. Sampling visited states or state-action pairs for search-control may not outperform a model-free agent using experience replay. Model errors can lead to performance deterioration without an effective search-control mechanism. The search-control mechanisms explored include prioritized sweeping, local linear models for optimal trajectories, and sampling from regions where the value function is difficult to estimate. This paper proposes sampling more frequently from challenging state space regions to improve search-control strategy. The paper proposes a search-control mechanism that focuses on sampling from regions of the state space where the value function is difficult to estimate. This is based on the connection between a function's local frequency and its gradient and Hessian norm, using a hill climbing approach for sampling. The paper introduces a search-control mechanism for sampling in challenging regions of the state space to improve the efficiency of learning the value function. It includes a review of MBRL, signal processing concepts, and experiments in supervised learning. The method measures the frequency of points in a function's domain and is adapted for the Dyna architecture using a hill climbing approach. Experiments on benchmark domains demonstrate the effectiveness of the proposed method. The curr_chunk discusses reinforcement learning problems formulated as Markov Decision Processes (MDPs) and the components involved such as state space, action space, transition function, reward function, and discount factor. It also mentions the agent's observation of states, taking actions, receiving rewards, and maintaining a policy. Value-based RL methods learn the action-value function to act effectively. Model-based RL involves creating a mapping that predicts future states based on current state-action pairs. The model can be local or global, deterministic or stochastic, and can operate on different types of features or observations. In model-based RL, various approaches have been explored, including decision-aware models and RKHS embedding for environment dynamics. The one-step environment dynamics model considered takes a state-action pair and outputs the next state and reward. Relevant works include the classical Dyna architecture and its hill climbing variant, HC-Dyna, which uses a search-control mechanism based on hill climbing on value estimates. HC-Dyna maintains an ER buffer and performs hill climbing on the learned value function by following the natural gradient. The search-control queue is populated with states generated during the hill climbing procedure. The distribution of samples in the search-control queue is related to the value function estimate. During the planning stage, states from the search-control queue are paired with on-policy actions selected by the current Q network. The HC-Dyna model uses a search-control mechanism based on the value function estimate to guide the querying of the model for next-state and reward information. This helps the agent gather more samples from high-value regions of the state space for training the value function estimator. The estimated value function is incorrect, leading to fast correction in low-value regions. Sampling more from difficult learning regions improves performance by focusing on high frequency areas. The norm of the gradient is proposed as a measure of local frequency. The norm of the gradient is proposed as a measure of the local frequency of a function, establishing a theoretical connection with the local frequency. This criterion forms the basis for a frequency-based search-control method. The goal is to improve the quality of the learned function by selecting distributions of samples in difficult-to-approximate regions of the domain. The text discusses quantifying the difficulty of approximating a function using the norm of the gradient as a measure of local frequency. It introduces learning curves showing testing error based on biased and unbiased training datasets, referencing the Nyquist-Shannon sampling theorem in signal processing for reconstruction based on regular samples. The text discusses the application of the sampling theory in machine learning algorithms, highlighting the need for more training data in regions with high-frequency signals. It compares the frequency ratios in different intervals to emphasize the importance of sufficient training data for accurate reconstruction. In a simple regression task with the target function f sin, biased training data sampling towards high frequency regions speeds up learning, as shown in regression learning curves with different sampling ratios. This supports the need for more training data in regions with high-frequency signals in machine learning algorithms. The heuristic to assign more data to high frequency regions speeds up learning. Identifying these regions is challenging without access to the target function and due to frequency being a global property. A simple criterion using function approximation and local frequency information is needed for practical use. The criterion for identifying high frequency regions in function approximation involves calculating the first order derivative ratio and integrals of squared gradient norm. This method efficiently characterizes local frequency information and can be used practically for speeding up learning. The integral of squared gradient norm and Hessian norm over a bounded domain reflects high frequency behavior. Weighting schemes proportional to n^2 or n^4 emphasize higher frequency terms. Empirical demonstration shows that regions with large gradient and Hessian norm correspond to high frequency regions. Biasing training dataset towards high gradient and Hessian norm leads to better learning results. Biasing the training dataset towards high gradient and Hessian norm leads to better learning results. Biased-HessianNorm involves sampling proportional to Hessian norm for 40% of training data, resulting in denser points around spikes. This leads to faster learning compared to Biased-GradientNorm, as shown in Fig. 2 (a). The superior performance on the dataset biased by Hessian norm is attributed to the difficulty in approximating areas around spikes due to sharp changes in the underlying function. Based on passing a \"sanity check\" of calculations and experiments, a method is proposed to measure the local frequency of a function around a point x using the gradient norm and Hessian norm. The local frequency is claimed to be proportional to g(x), with a theoretical justification provided. The theory establishes a connection between local gradient norm, Hessian norm, function energy, and frequency distribution. The local Fourier transform around x for any function in Euclidean spaces is defined, assuming finite local function energy. The proof can be found in Appendix A.2. The local frequency distribution of a function around a point x is defined using the gradient norm and Hessian norm. The distribution characterizes the frequency behavior, with higher norms indicating higher frequencies. The Fourier coefficient is normalized to eliminate the impact of function energy, and the proportion of frequency k is calculated. The integral of the distribution reflects the contribution of high frequency terms in the local function. The integral of \u03c0f(k) \u00b7 k2 reflects the contribution of high frequency terms in the local frequency distribution of a function. Theorem 1 suggests that regions with large gradient norm can have large absolute value function or high local frequency. To prevent finding regions with only large negative value function, considering both gradient norm and value function is reasonable. The Uncertainty Principle states that a function cannot be too concentrated in both spatial and frequency space. The Dyna architecture is presented in the next section. The Dyna architecture with frequency-based search-control (Algorithm 1) is introduced in this section to draw samples from high-frequency regions of the state space. Implementation details are omitted, and the goal is to query the model more often from regions where the local frequency of the value function is higher. This search-control mechanism aims to address the difficulty of learning the function in those regions. The Dyna architecture introduces a frequency-based search-control mechanism to draw samples from high-frequency regions of the state space, where learning the value function is more difficult. To populate the search-control queue with states from these regions, hill climbing on the gradient of the value function can be used. Theorem 1 suggests that states with large gradient norms can have large absolute values or high local frequencies. To avoid sampling from regions with large negative value states, a combination of hill climbing methods is proposed. This method combines hill climbing on the gradient with hill climbing on the value function to generate samples from high value states. The Dyna architecture introduces a frequency-based search-control mechanism to draw samples from high-frequency regions of the state space. Hill climbing is used to populate the search-control queue with states from high value regions, with different methods for value function and g(s). Additional intuitive mechanisms are discussed in Appendix A.4. In the work by Pan et al. (2019), the state-value function is obtained by maximizing the estimated action-value. During the planning stage, multiple mixed mini-batches are sampled to update parameters and address off-policy sampling issues. This approach was also used in the work by Gu et al. (2016). In the study by Pan et al. (2019), the state-value function is obtained by maximizing the action-value. During planning, mixed mini-batches are sampled to update parameters and address off-policy sampling issues. This method was also utilized by Gu et al. (2016). The algorithm's properties are examined on the MountainCar benchmark domain and applied to a MazeGridWorld domain to showcase the practical implications of sampling from high frequency regions. The focus is on search-control rather than learning. The MountainCar domain is well-studied with sharp changing regions in the optimal value function. The agent aims to reach the goal state quickly due to a penalty per time step. The study focuses on search-control over model learning, with experiments to verify performance against competitors and robustness to noise. Competitors include Dyna-Frequency using the proposed search-control strategy. Additional experiments and reproducibility details are provided in appendices. The study compares different algorithms in the MountainCar domain, including Dyna-Frequency, Dyna-Value, PrioritizedER, and ER. Results show that the proposed algorithm benefits from more planning updates, with clear differences between Dyna-Frequency and Dyna-Value. Our algorithm outperforms PrioritizedER and Dyna-Value in the presence of noise, utilizing samples from high frequency regions for superior performance. The method is illustrated in a challenging MazeGridWorld domain with continuous state space. In a MazeGridWorld domain with continuous state space, the agent navigates through walls with holes using four discrete actions. Model-free methods fail, leading to the study of a new algorithm and Dyna-Value. Evaluation curves show the robust policy learned by the new algorithm, with further investigation into state distribution differences between the algorithms. The search-control queue in our algorithm has a high density around bottleneck areas with holes, crucial for the agent to reach the goal state. This distinguishes our approach from previous work, which focuses on states near the goal area. The state distribution in our algorithm at 50k environment time step shows a high density around squares representing holes, with 25.3% and 11.7% of points falling inside a 0.1 radius ball in Dyna-Frequency and Dyna-Value, respectively. The text discusses a new method for search-control in a function approximation context. The method identifies high frequency regions of a function and is integrated into the Dyna architecture for empirical testing. The results show competitive learning performances in a challenging domain. Future research directions include exploring different search-control strategies and incorporating active learning methods into model-based reinforcement learning algorithms. The text discusses designing a search-control mechanism in MBRL algorithms by establishing a connection between local gradient norm, Hessian norm, and local frequency. It introduces a definition of \"local frequency\" to build this connection. The text introduces the concept of \"local frequency distribution\" \u03c0f as a probability distribution over R n, characterizing the proportion of a frequency component within an unit ball. The proof involves using a local Fourier transform, calculating the gradient/Hessian norm, and integrating over the unit ball to connect with function energy. The proof establishes the first and second order connections by utilizing local Fourier transform, gradient norm calculation, and integration over the unit ball to relate local gradient information with energy and frequency distribution. The Hessian matrix H l,: can be written as \u2202\u2207f (y) \u2202y l, where e l is a one-hot vector. The norm of the vector H l,: is calculated using complex conjugate. The square of Frobenius norm of the Hessian matrix is denoted as ||H||. Integration of ||H|| 2 F over y variable within a ball with center x and unit radius is obtained. In this section, the vanilla Dyna and Hill Climbing Dyna algorithms are presented. Dyna is a model-based reinforcement learning architecture where real experiences are used to improve policy/value estimates and learn the environment dynamics model. Hill climbing on V(s) and g(s) have different insights, with the former propagating value information from high to low value regions. The approach of storing states along the trajectory in low value regions is empirically verified by Pan et al. (2019). The search-control queue distribution and state distribution in the queue are connected in value-based hill climbing through Langevin dynamics. The hill climbing process in value-based hill climbing through Langevin dynamics tracks a discretized version of a stochastic differential equation, with a state distribution approximately proportional to exp(V(s)). The search-control queue distribution resembles a state distribution with terms exp(V(s)) and exp(g(s)), where removing the first term leads to a sampling distribution similar to supervised learning experiments biased towards gradient norm. Future research could explore the theoretical connection between sampling distribution and sample complexity, as well as the effect of hill climbing on gradient norm or Hessian norm. In this section, the study explores the effect of hill climbing on either gradient norm or Hessian norm. The search-control strategy can be applied to continuous control algorithms as well. The design helps navigate regions with varying gradient and Hessian magnitudes on the value function surface. Results using only gradient norm or Hessian norm are shown in Fig. 6 for MountainCar and GridWorld domains. In this section, the study demonstrates the application of a search-control strategy on MountainCar and GridWorld domains. The performance of Dyna-HessNorm is poor due to frequent zero vectors, while Dyna-GradNorm shows slightly better performance. A more general form of the function g(x) is proposed, introducing additional meta-parameters. Additionally, the method is adapted to a continuous control setting for further exploration. The study applies a search-control strategy to Hopper-v2 and Walker2d-v2 domains using NAF algorithm. The algorithm parameterizes the action value function and utilizes the value function V(s). Results show that DynaNAF-Frequency outperforms model-free NAF. Implementation details include the use of tensorflow 1.13.0 and Adam optimizer for DQN update. In experiments, parameters are initialized using Xavier initialization. Neural networks with specific units and activation functions are used for model learning and supervised learning. Testing error is computed every 20 iterations. Points are sampled for Fig. 2 using even spaced points on a domain. The experiment on MountainCar involves using a 32x32 tanh layer with specific parameters like target network moving rate, learning rate, and exploration noise. Prioritized experience replay is implemented without importance ratio, and Dyna-Value and Dyna-Frequency use specific parameters for gradient ascent and hill climbing. The hill climbing rules are fixed across all experiments. For the MazeGridWorld experiment, hill climbing rules are selected with equal probability. The walls have specific dimensions and each hole is located at different coordinates. Actions result in a small perturbed move. Parameters for DynaValue and Dyna-Frequency are similar to MountainCar, with differences in Q network units and planning updates. State distribution from ER buffer is provided as a supplement. The ER buffer has a different state distribution compared to the search-control queue. Algorithm 4 provides pseudo-code for the hill climbing rules used in the experiment. Frequency-based search-control uses an empirical covariance matrix \u03a3 s and parameters \u03b7 = 0.01, \u03b1 = 0.01. The projection step was omitted as it was found unnecessary. The Dyna architecture with Frequency-based search-control is detailed with B s as the search-control queue, B as the experience replay buffer, M as the environment model, and m as the number of search-control samples fetched at each step. The environment model uses search-control samples to fetch at each step with a probability of choosing a value-based hill climbing rule. The mixing factor in a mini-batch is determined by a parameter \u03b2. Planning steps are determined by the number of state variables. The update target network Q is done every \u03c4 updates. Gradient ascent hill climbing is used with a probability of p, otherwise, hill climbing is chosen. Mini-batches are used for parameter updates in the DQN algorithm. The DQN algorithm uses mixed mini-batches for parameter updates, with the target network Q updated every \u03c4 updates."
}