{
    "title": "SyxJ2y2qaE",
    "content": "Deeper neural networks are harder to train than shallower ones. The Hessian eigenvalue distribution in deeper networks has heavier tails, making optimization with first-order methods more challenging. Adding residual connections mitigates this effect, improving training. In deeper neural networks, training becomes harder. Residual connections in CNNs alleviate this issue by reducing the flatness of the landscape and decorrelating gradients with depth. The slowing down of training is not due to gradient explosion or vanishing as commonly believed. The slowing down of training in deeper neural networks is not caused by gradient explosion or vanishing. Instead, the Hessian eigenvalue density is studied to understand the optimization rate, which is dependent on the spread of eigenvalues. By examining the skewness and kurtosis of the Hessian eigenvalues, it is observed that models with residual connections mitigate the presence of large outliers. Batch normalization is hypothesized to suppress these outliers, leading to faster training. In this paper, evidence is presented that residual connections speed up training similar to batch normalization. Deeper CNNs are known to be harder to train, with gradient explosion or vanishing as a common explanation. However, this explanation is challenged in the study comparing gradient norms of depth 80 residual and non-residual networks. The study compares gradient norms of depth 80 residual and non-residual networks. It shows no exponential increase or decrease in gradient norms as hypothesized in gradient explosion explanations. Residual connections do not consistently affect gradient norms, with 49.4% of variables having lower gradient norms in residual networks. The Hessian of the training loss function measures loss curvature, impacting optimization characteristics in the convex setting. The Hessian spectrum of the loss curvature influences training speed in neural networks. Large differences among eigenvalues slow down optimization, especially when there are outliers. Heavy-tailed eigenvalue distributions lead to slower training, as shown in FIG1 for CNNs of increasing depth. The loss Hessian of deep neural networks shows most eigenvalues concentrated near zero, indicating a relatively flat loss surface. However, as the network depth increases, outliers with higher magnitudes appear, making training more challenging. Skewness and kurtosis are computed to quantify the extent of these outliers, where kurtosis greater than 3 indicates heavy-tailed distributions. The Hessian eigenvalues exhibit heavy tails with kurtosis greater than 3, indicating extreme skewness. Increasing model depth results in a significant increase in both skewness and kurtosis. Residual connections in deep models are expected to mitigate the extreme values of the largest eigenvalues. The addition of residual connections reduces outliers in the Hessian spectrum. Models with residual connections show lower skewness and kurtosis compared to models without. Residual connections mitigate the increase in outlier eigenvalues with depth in neural networks. Behrooz Ghorbani was supported by NSF grants. A class of networks with 6n layers trained on CIFAR-10 is studied, with different sizes of feature maps and filters per layer. The network includes batch-normalization and can be either 'simple-6n + 2' without residual connections or ResNet-6n + 2 with residual connections. SGD with momentum is used for training both types of networks. For CIFAR-10, networks with 6n layers were studied, including 'simple-6n + 2' and ResNet-6n + 2. Training both networks for 100k steps with SGD and momentum. Deeper CNNs are harder to train, skip connections help at depth. Simple models train well for small n, but become slower with increased depth."
}