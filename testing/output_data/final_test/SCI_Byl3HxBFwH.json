{
    "title": "Byl3HxBFwH",
    "content": "Supervised deep learning requires a large amount of training samples with annotations, which are costly and time-consuming to obtain. A novel training framework is proposed to adaptively select informative samples based on a hardness-aware strategy to address the challenges of expensive annotations and loss of sample informativeness. The proposed training framework utilizes adaptive selection based on hardness-aware strategy in the latent space constructed by a generative model. Experiments on MNIST, CIFAR-10, and IVUS datasets show superior performance compared to random sampling. Deep learning advancements have led to state-of-the-art results in various fields, attributed to network designs, optimization techniques, and availability of large annotated datasets. Training deep neural networks with massive datasets like COCO and Cityscapes is challenging due to the expensive and time-consuming annotation process, especially in fields like medical imaging. An alternative approach is to select a subset of the most representative samples for network training, allowing for learning new behaviors or concepts through typical cases rather than accumulative learning. In this paper, a model state-aware framework for data-efficient deep representation learning is proposed. The main focus is on selecting a subset of representative samples for network training to improve data efficiency and model performance. The goal is to utilize annotated samples more efficiently to achieve a predefined performance measure. The framework aims to mine 'harder' training samples progressively on the data manifold based on the current parameter. The proposed framework focuses on selecting representative samples for network training to improve data efficiency and model performance. It involves training a variational auto-encoder (VAE) using unannotated samples and mining harder training samples based on error information propagated backward via the target model and decoder. Annotating these samples helps in selecting plausible harder samples that yield higher loss, estimated through backpropagation. A generative model is used to embed data into a low-dimensional latent space for selecting harder training samples. Two sampling strategies, sampling by nearest neighbor (SNN) and sampling by interpolation (SI), are investigated. The framework's data efficiency is evaluated on MNIST, CIFAR-10, and IVUS datasets. Major contributions include a novel framework for model state-aware sampling and deep representation learning, and the introduction of a generative model for proposing informative training samples. The model introduces informative training samples through two latent space sampling strategies. It is applicable for sampling on existing datasets, suggestive annotation, and synthesizing new samples. The framework is demonstrated in a biophysical simulation task, where artificial samples are synthesized from the latent space. Few-shot learning focuses on learning the relevance of objects from a limited number of samples. Various methods have been proposed, such as Siamese networks for classification and neural networks for mapping samples into a metric space. Some approaches aim to avoid fine-tuning for new class types. While most methods focus on classification, some recent ones address tasks like object detection. Schwartz et al. (2018) introduced few-shot object detection by jointly learning an embedding space and data distribution of categories. Hardness-aware learning involves selecting training samples that contribute most to the training process, as seen in works by Schroff et al. (2015) and others. The curr_chunk discusses the use of a triplet training approach with a triplet loss to minimize distances between anchor and positive points while maximizing distances between anchor and negative points. Smart sampling methods are used to address issues with gradients. The work is related to active learning, with references to previous studies on core-set selection and addressing similar issues. The proposed framework consists of two stages: in the first stage, a VAE-based generative model is trained using unannotated samples, while in the second stage, 'hard' samples are mined iteratively in the latent space for training. Sampling strategies are employed to select these samples. The proposed framework involves training a VAE-based generative model with unannotated samples in the first stage. In the second stage, 'hard' samples are selected iteratively in the latent space using sampling strategies based on error information of the neural network model. Sampling strategies include 'sampling by nearest neighbor (SNN)' and 'sampling by interpolation (SI)'. The sampling and training process continues until a pre-set amount of training samples is reached. The proposed framework involves training a VAE-based generative model with unannotated samples in the first stage. 'Hard' samples are selected iteratively in the latent space using sampling strategies based on error information of the neural network model. Incremental subsets of informative samples are mined and added to the train set progressively to train the model. The method involves selecting informative samples for training by identifying embedding positions in latent spaces and calculating gradients using backpropagation. 'Harder' samples are identified in the latent space to train the model effectively. The method involves selecting informative samples for training by identifying embedding positions in latent spaces and calculating gradients using backpropagation. 'Harder' samples are identified in the latent space to train the model effectively. Randomly select a subset and calculate gradients for each point using backpropagation. Identify harder points and their nearest neighbors to construct a new incremental set for training. Two sampling strategies, 'sampling by nearest neighbor' and 'sampling by interpolation', are proposed for scenarios with different labeling capabilities. The generator G produces synthesized samples following a sampling strategy for training the neural network model. Initially, variables are drawn from a Gaussian distribution in latent space. The synthesized samples are annotated with ground truth labels and used as the initial train set. In subsequent rounds, the incremental subset of synthesized samples is added to the train set for training. Gradients are calculated using backpropagation to update the model. The gradients \u2202L \u2202z are obtained from a loss function using backpropagation. Eq. 2 is used to find a z for the generator G to produce more informative samples. The proposed framework was evaluated on MNIST, CIFAR-10, and IVUS datasets. In IVUS, y is provided by a labeling tool. The framework involves training F with a randomly selected subset and calculating \u2202L \u2202z for each z using backpropagation. Harder points are identified for each z. The IVUS dataset consists of 1,200 slices of 2D gray-scale images of coronary plaques and vessel wall segmentation. A deep neural network is trained to approximate finite element analysis for structural stress prediction from medical images. The vendor provides an in-house Python package 'VasFEM' for labeling by solving PDEs on segmentation masks. Experiments aim to show framework effectiveness with less training data. Proposed framework selects or generates harder samples progressively, unlike baseline method. An independent evaluation was conducted on test sets from three datasets: MNIST, CIFAR-10, and IVUS. Sampling methods varied for each dataset, with MNIST and CIFAR-10 using nearest neighbor sampling and IVUS using interpolation. A vanilla VAE was trained on MNIST and IVUS datasets without annotations, while \u03b1-GAN was used for CIFAR-10 due to difficulty in reconstruction. The \u03b1-GAN model, proposed in Rosca et al. (2017), was utilized for CIFAR-10 image reconstruction. It consists of an encoder, a generator, and a code discriminator. CNNs were used for classification tasks with cross entropy loss. For IVUS dataset, deconvolution layers and MSE loss were employed. Adam optimizer was used for all tasks, with experiments increasing train set size and evaluating accuracy and MSE on independent test sets. The proposed framework, utilizing an annotating system, showed better performance compared to the baseline method until reaching a bottleneck. However, marginal improvement was observed on MNIST and IVUS datasets due to task simplicity. The framework integrates a labeling tool into the training process actively. The framework integrates an annotating system to actively incorporate more informative samples into the training process, particularly beneficial in scenarios with high annotation costs like medical image segmentation. Performance initially drops when new samples are introduced but rebounds to a higher level as the neural network learns from them. Hardness-aware sampling results in a deeper drop followed by a higher rebound, indicating the mining of more informative samples along the gradients of the loss function. The framework integrates an annotating system to actively incorporate more informative samples into the training process. A 2D latent space for MNIST embeddings is visualized, showing trajectories that explore areas of high uncertainty. This exploration strategy encourages sampling points to walk around the boundary between classes, where ambiguous samples are located. The choice of dimensionality of latent space is crucial for generating diverse and plausible samples. Higher dimensions encourage diversity but may lead to sparse sampling points, making it difficult to identify nearest neighbors. Undesired trajectories exploring outside boundaries should be avoided by periodically re-selecting points for exploration within existing training samples. The proposed framework addresses sparse sampling in the latent space by introducing two sampling strategies: Sampling by nearest neighbor for datasets without external labeling tools, and Sampling by interpolation for datasets with external labeling tools. This model state-aware framework aims to efficiently annotate and learn from training samples. The proposed state-aware framework efficiently annotates and learns by mining hard samples in a low-dimensional latent space. It can be applied to various machine learning applications and realistic scenarios with a labeling tool. A Python package named 'VasFEM' is used for image-based biomechanical analysis of coronary plaques, providing structural stress maps based on segmentation masks. The plaque material is assumed to be incompressible and non-linear, described by a modified Mooney-Rivlin strain energy density function. The proposed framework uses a modified Mooney-Rivlin strain energy density function to analyze coronary plaques. Material parameters for blood vessels are derived from previous experimental work. The finite element method is employed to solve plane-strain equations, with a focus on data efficiency. A template pulsatile blood pressure waveform is applied, and a structural stress map is extracted for analysis. The proposed framework simplifies the simulation by resampling the segmentation mask into a 64x64 pixel image size, reducing the simulation time to two mins. An example of the input image and output stress map is shown in Fig S1."
}