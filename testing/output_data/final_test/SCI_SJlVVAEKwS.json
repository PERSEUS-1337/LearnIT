{
    "title": "SJlVVAEKwS",
    "content": "Deep learning models are vulnerable to adversarial attacks, with current methods requiring pre-trained models and lots of queries. A novel adversarial imitation attack is proposed, using a generative model to create examples that confuse the target model. This approach requires less training data compared to current substitute attacks. The imitation attack proposed in this study requires less training data compared to black-box substitute attacks, achieving a high attack success rate close to white-box attacks on unseen data without queries. Adversarial examples exploit vulnerabilities in deep neural networks, leading to incorrect predictions and prompting the development of attacks and defenses to enhance model robustness. White-box attacks rely on training data and gradient information, while black-box attacks do not require knowledge of model structures or weights. Substitute black-box attacks use pre-trained models to generate adversarial examples and target models without gradient information, achieving high attack success rates. These attacks exploit the transferability property of adversarial examples, affecting different models regardless of architecture. Black-box attacks on models rely on transferability of adversarial examples. Score-based attacks access output probabilities iteratively, while decision-based attacks require less information. Practical attacks should minimize knowledge of models. Substitute black-box attacks need pre-trained substitute models. In this paper, the authors propose an adversarial imitation training method as a practical attack mechanism for black-box attacks. The method involves a two-player game with a generative model G and an imitation model D, where G generates examples to make the predicted label of the attacked model T different from D. This approach aims to minimize the need for pre-trained substitute models and reduce the number of queries required to generate adversarial examples. The proposed adversarial imitation attack involves a two-player game with generative model G and imitation model D, where D aims to output the same label as attacked model T. This method requires less training data and does not need labels for the data. Adversarial examples generated by D are used to fool T, reducing the need for pre-trained substitute models and queries for generating attacks. The proposed adversarial imitation attack involves a two-player game with generative model G and imitation model D, where D aims to output the same label as attacked model T. This method requires less training data and achieves state-of-the-art performance compared to current substitute attacks and decision-based attacks. The attack mechanism needs less training data but achieves a high success rate close to white-box attacks. It requires the same information as decision attacks during training but is query-independent during testing. Adversarial attacks occur in white-box and black-box settings, with the former having complete access to the attacked model's internals and data. The black-box attack method utilizes the transferability property of adversarial examples and only needs labeled training data. Various methods for generating adversarial examples have been proposed, such as FGSM, BIM, DeepFool, and Carlini & Wagner attacks. These attacks require different levels of prior knowledge of the attacked models and have varying success rates. Carlini & Wagner (2017b) and Rony et al. (2018) developed strong attacks on models by minimizing perturbation and L F norm. Liu et al. (2017) found targeted adversarial examples lack transferability and proposed ensemble-based methods for stronger transferability. Various black-box attacks like ZOO and decision-based attacks have been proposed by different researchers. Decision-based attacks were first proposed by et al. (2017) without relying on gradients. Cheng et al. (2018) and Chen et al. (2019) improved query efficiency. Adversarial defenses, such as adversarial training and defenses based on gradient masking, aim to increase model robustness. Random transformations and thermometer encoding are also used to enhance model security. In response to existing defenses against attacks, some researchers focus on detecting adversarial examples using neural networks to distinguish between clean and adversarial examples. Others focus on achieving statistical properties to detect adversarial examples. This section introduces the definition of adversarial examples and proposes a new attack mechanism based on adversarial imitation training. The text discusses adversarial attacks on machine learning models, where the objective is to generate imperceptible perturbations to fool the model. Different attack methods include white-box attacks, substitute attacks, and the key to a successful attack is the transferability of adversarial examples. Researchers are exploring ways to improve the transferability of these examples and avoid detection. The text introduces adversarial imitation training to improve the transferability of adversarial examples. It utilizes an imitation network to mimic the characteristics of the attacked model without the need for additional queries. Inspired by GANs, a two-player game-based approach is proposed to replicate the information of the attacked model. The proposed adversarial imitation attack involves a two-player game between players G and D to generate adversarial examples without the need for additional queries. The imitation network mimics the characteristics of the attacked model, aiming to achieve the same success rate as a white-box attack. In a two-player game, the adversarial imitation attack involves players G and D generating adversarial examples without extra queries. The key to training an efficient imitation network D lies in setting a proper upper bound for G(X). Strong transferability of adversarial examples occurs when D's characteristics are more similar to the attacked model. The loss functions for D and G are J_D = V_G,D and J_G = e^(-V_G,D) respectively. The global optimal imitation network D is achieved when D(X) = y_T(X), resulting in J_D = 0 and J_G = 1. This ensures a controllable loss value for G during training. During training, the loss of G is kept controllable. Adversarial examples from a well-trained D have strong transferability. The attack perturbation is small to limit the search space of G. The G and D are alternately trained in mini-batches using L2 penalty. The procedure is shown in algorithm 1 for mini-batch stochastic gradient descent training of the imitation network. The experiments evaluate a proposed method on MNIST and CIFAR-10 datasets, using test sets divided for training and evaluating attack performance. The imitation network is trained using only the output label of the attacked model T, without prior knowledge or pre-trained models. The experiments evaluate a proposed imitation attack method on MNIST and CIFAR-10 datasets using different network architectures. Various attack methods are compared for generating adversarial examples, and the performance is evaluated against decision-based and score-based attacks. The experiments compare different attack methods for generating adversarial examples on MNIST and CIFAR-10 datasets using various network architectures. Evaluation criteria include non-targeted and targeted attacks, with success rates calculated based on the number of adversarial examples that fool the attacked model. The proposed adversarial imitation training is utilized in this subsection. In this subsection, adversarial imitation training is used to train imitation models and evaluate attack success rates. The medium network and VGG-16 are attacked on MNIST and CIFAR-10, respectively. Substitute adversarial examples are generated using a pre-trained large network and ResNet-50. Imitation networks are obtained using the proposed training method, with architectures based on the large network and ResNet-16 for MNIST and CIFAR-10. The imitation models are trained on a small subset of the test dataset and evaluated on a separate set of samples. The experiments show that the proposed imitation attack outperforms substitute attacks in terms of success rates. It requires fewer training images but achieves a success rate close to white-box attacks. Adversarial samples from a well-trained imitation model have higher transferability. The method is compared with decision-based and score-based attacks using MNIST and CIFAR-10 datasets. Our imitation attack outperforms substitute attacks in success rates, requiring fewer training images but achieving results close to white-box attacks. It shows higher transferability of adversarial samples and achieves state-of-the-art performance in decision-based methods, even outperforming score-based attacks in perturbation distance and success rate on MNIST and CIFAR-10 datasets. The imitation attack demonstrates high success rates on unseen data and can be applied to query-independent scenarios. Different network capacities were studied for imitation performance, showing that models with lower capacity than the attacked model can still achieve good imitation results. The attack success rates of imitation models surpass substitute attacks, indicating the effectiveness of the imitation approach. The imitation models outperform substitute attacks in various experiments, with higher capacity leading to higher attack success rates in some cases. However, the performance is not solely dependent on the imitation model's capacity but also on the capability of the attacked model. Training the imitation network with a limited number of images from MNIST and CIFAR-10 test sets shows promising results in comparison to Practical Attack on other images from the same test sets. The practical attack uses output labels to train substitute models, generating adversarial examples to fool attacked models is challenging due to limited training samples. Adversarial imitation attack produces substitute models with higher accuracy and attack success rates than Practical Attack in scenarios with infinite queries. Performance comparison with limited queries is shown in Figure 6. In this study, a new attack mechanism called imitation attack is proposed to generate adversarial examples fooling deep learning models efficiently. This attack requires less data than the training set of the model being attacked and does not need labels of the training data. The generated adversarial examples have strong transferability and achieve state-of-the-art performances. The imitation attack is query-independent on the testing stage and outperforms score-based and decision-based attacks. The proposed imitation attack generates adversarial examples that can fool deep learning models with limited unlabeled images. Future work will evaluate the attack on tasks other than image classification. Adversarial examples can deceive the model with a small perturbation."
}