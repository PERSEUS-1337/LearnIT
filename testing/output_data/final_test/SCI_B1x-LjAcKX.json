{
    "title": "B1x-LjAcKX",
    "content": "This paper introduces a novel approach to training deep neural networks by utilizing local critic networks in addition to the main network model. The method aims to unlock layer-wise dependencies in backpropagation training, leading to improved performance in various applications. Experimental results demonstrate the effectiveness of the proposed approach and provide guidelines for parameter selection. The success of deep neural networks lies in their ability to extract information progressively through layered structures. Complex network structures are developed to solve real-world problems. Training involves backpropagation, which updates weight parameters by propagating error gradients through layers. Layer-wise dependencies can cause issues like locking, especially in systems with interacting models or distributed computing nodes. The method proposed in BID9, called decoupled neural interface (DNI), directly synthesizes estimated error gradients, known as synthetic gradients, for deep learning architectures like convolutional neural networks (CNNs). The decoupled neural interface (DNI) synthesizes synthetic gradients using a small neural network to train layer weights independently. However, this method shows performance degradation compared to regular backpropagation. Another approach in BID2 involves additional modules trained to approximate main model outputs, not error gradients, but does not address update locking or non-sequential learning algorithms. In this paper, a novel approach called local critic training is proposed for non-sequential learning. Additional modules, known as local critics, are used to indirectly deliver error gradients to the main neural network model for training without backpropagation. This eliminates the need for backpropagation, allowing feedforward operations and gradient-descent learning to be performed independently. The method involves training a local critic at a specific layer group so that its output derivative serves as the error gradient for training corresponding layers' weight parameters. The proposed method of local critic training offers insights into network structure influences, update frequency, and total local critics. It enables training without locking and structural optimization of neural networks. Additionally, it allows for progressive inference with reduced computational complexity and ensemble inference for improved classification performance. The proposed approach introduces local critics alongside the main network model to improve classification performance. Each local critic network approximates the output of the main network, allowing for training without locking and structural optimization. This method enables progressive inference with reduced complexity and ensemble inference for better classification results. The proposed approach introduces local critics alongside the main network model to improve classification performance by training the weight parameters of each network using error gradients obtained through differentiation of the loss function. The final layer group does not require a local critic network and can be trained using regular backpropagation. The update of each network can be performed independently, simplifying operations compared to subsequent networks. The proposed approach introduces local critics alongside the main network model to improve classification performance by training the weight parameters of each network using error gradients obtained through differentiation of the loss function. The local network c i is trained so that its training loss targets the training loss for c i+1, allowing for independent updates of each network. Comparing with the existing DNI approach, the proposed architecture resolves the issue of locking BID9 by using an indirect, cascaded approach for training c i. The proposed approach introduces local critics alongside the main network model to improve classification performance by training the weight parameters of each network using error gradients obtained through differentiation of the loss function. In many cases, determining an appropriate structure of neural networks for a given problem is not straightforward and is usually done through trial-and-error, which is time-consuming. Structural optimization in deep learning is critical due to the risk of overfitting in large-sized networks. The proposed approach introduces local critics alongside the main network model to improve classification performance by training the weight parameters of each network using error gradients obtained through differentiation of the loss function. It is desirable to find an optimal network structure that is small yet performs well, especially for resource-constrained cases like embedded and mobile systems. After training, different networks with similar input-output mappings but varying structures and accuracies are obtained, allowing for the selection of a structure-optimized network. The proposed approach introduces local critics to improve classification performance by training weight parameters using error gradients. After training, different networks with varying structures and accuracies are obtained, allowing for the selection of a structure-optimized network. A simple yet effective way called progressive inference is proposed to utilize sub-models for computational efficiency. This approach can finish classification with a small sub-model if its confidence is high enough, avoiding the need for a full feedforward pass. The proposed approach introduces local critics to improve classification performance by training weight parameters using error gradients. A progressive inference method is used to utilize sub-models for computational efficiency, where the confidence level determines if a full feedforward pass is needed. Ensemble approaches are popular in deep learning systems, and the sub-models can be combined with the main model for ensemble inference. The proposed method introduces local critics to enhance classification performance by training weight parameters using error gradients. It utilizes a VGG-like CNN architecture with batch normalization and ReLU activation functions on CIFAR-10 and CIFAR-100 datasets. The method combines multiple networks' outputs through summation for ensemble classification. It does not require specific network structures and can work with generic CNNs. The method utilizes a VGG-like CNN architecture with batch normalization and ReLU activation functions on CIFAR-10 and CIFAR-100 datasets. It combines multiple networks' outputs through summation for ensemble classification. The training process involves stochastic gradient descent with momentum, Adam optimization, L2 regularization, cross-entropy, and L1 loss functions. Experiments are conducted using TensorFlow with specific batch size, training iterations, and learning rate adjustments. The experiments were conducted five times with different random seeds to report the average accuracy. The local critic networks successfully approximate the main network's loss with high accuracy during training. The proposed local critic training approach's classification performance was evaluated and compared with other methods in Table 1. The proposed method involves using different numbers of local critic networks, each composed of a convolutional layer and an output layer. Results show successful decoupling of layer group training with a slight decrease in accuracy compared to backpropagation. The proposed method involves using different numbers of local critic networks to decouple layer group training, resulting in a slight decrease in accuracy compared to backpropagation. The accuracy decrease is more significant for complex problems like CIFAR-100, with a trade-off between accuracy and unlocking effect. The method shows performance improvement over the critic training method for both datasets. The study examines the influence of the structures of local critic networks in the proposed method, changing the number of convolutional layers in each network. Results show a slight increase in accuracy for CIFAR-100 as the number of convolutional layers increases. The study explores the impact of varying the number of convolutional layers in local critic networks on CIFAR-100 and CIFAR-10 accuracy. Increasing layers improves CIFAR-100 performance but decreases CIFAR-10 accuracy, suggesting a need for more complex structures closer to the input side of the main model. The study examines the effect of increasing convolutional layers in local critic networks on CIFAR-100 and CIFAR-10 accuracy. Updating local critic networks periodically, rather than at every iteration, reduces computation and communication burden without significantly impacting accuracy. The study shows that decreasing the update frequency of local critic networks results in a slight decrease in accuracy for CIFAR-10 and CIFAR-100 datasets. Larger sub-models perform better than smaller ones, with the largest sub-model achieving similar accuracy to the main model but with reduced complexity. The study demonstrates that reducing the update frequency of local critic networks leads to a slight accuracy drop for CIFAR-10 and CIFAR-100 datasets. Larger sub-models outperform smaller ones, with the largest sub-model achieving comparable accuracy to the main model but with significantly reduced complexity. For CIFAR-10, the computational and memory complexity is reduced by about 30%, with sub-models trained using local critic training outperforming those trained with regular backpropagation. The study shows that a structurally optimized network can be achieved with local critic training, resulting in reduced computational complexity for CIFAR-10. By using a progressive inference algorithm, accuracy loss is minimal when the threshold is set to 0.9 or 0.95, with significant reductions in complexity compared to the main model. The ensemble of sub-models and main model improves classification accuracy compared to the main model alone. Complementarity among models, especially those with fewer shared layers, enhances performance. Sub-model 1 and the main model show significant performance improvement due to classifying more data differently. There is potential for a better method to combine the models than simple summation. In this paper, a local critic training approach was proposed to remove inter-layer locking constraints in training deep neural networks. The method was applied to structural optimization, progressive inference, and ensemble classification, showing successful training of CNNs with simple local critic networks. Various aspects were evaluated, including network structures, update intervals, and layer group sizes. The approach allows for direct performance of structural optimization, progressive inference, and ensemble classification without additional steps. The proposed approach includes Sobolev training to minimize loss and derivative in critic training. Results show no significant difference compared to backpropagation. Deep supervision algorithms BID2 and BID18 were also tested with similar performance. The method was effective for larger networks like ResNet-50 and ResNet-101. The proposed method using three local critic networks shows slight performance improvement for complex networks like ResNet-101 and ResNet-50 on the ImageNet dataset. The representational dissimilarity matrix (RDM) is obtained from each layer to analyze the trained networks, showing high correlation between samples from the same class. The figure illustrates that samples from the same class have highly correlated representations in the last layers. Different training methods result in networks with distinct internal characteristics, particularly in layers where local critic networks are attached. These layers act as intermediate layers and are forced to learn class-specific representations to some extent. The loss values of local critic networks for individual data over training iterations are examined. Initially, there are differences between local critic networks and the main network, but they converge to small values later on. Test performance of sub-models and their accuracy over training iterations for CIFAR-10 are also shown. Test accuracy for CIFAR-10 is compared between local critic training and backpropagation in Figure 7, showing faster convergence with local critic training."
}