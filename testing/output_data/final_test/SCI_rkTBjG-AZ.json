{
    "title": "rkTBjG-AZ",
    "content": "In deep learning, performance is greatly influenced by architecture and hyperparameters. Manual selection of architecture is common due to limited exploration in complex spaces. A framework is proposed for automatic design and training of deep models using a modular language to represent search spaces efficiently. The search spaces are tree-structured for easy traversal and model compilation into computational graphs. Leveraging the search space structure allows for different model search strategies. The framework allows for different model search algorithms like random search, Monte Carlo tree search (MCTS), and sequential model-based optimization (SMBO). Experiments on CIFAR-10 show MCTS and SMBO outperform random search. MNIST experiments demonstrate near state-of-the-art performance with a few samples, indicating effective model discovery with minimal human effort. Deep learning has gained popularity for applications like computer vision and natural language processing. Recent work emphasizes the importance of complex architectures for high performance. Improving neural network performance requires clever ideas beyond just adding more layers or parameters. Techniques like dropout and batch normalization may not always lead to better performance and need to be applied judiciously. Choosing appropriate values for architectural hyperparameters in deep learning models currently requires manual supervision by a human expert. This process involves trial and error, guided by intuition, to specify a model, train it, and evaluate its performance. The expert must make numerous choices that interact in complex ways and significantly impact the model's performance. It would be beneficial for experts to automate the search for optimal architectures, similar to how simple scalar hyperparameters are optimized. The main contribution of the work is a language for representing search spaces over models, giving control to experts in selecting model variations and enabling automatic search for performant models. This language allows models to be compiled to computational graphs without additional coding. The language developed in this work enables automatic search for performant models by representing search spaces over models and compiling them to computational graphs without additional coding. The focus is on searching over deep architectures, allowing experts to set up a search space, search over it, and compile models automatically. The language is modular, composable, and extensible, emphasizing the problem of searching over deep architectures. The language developed in this work combines deep model specification and hyperparameter search functionalities. Traditional methods for model search are limited to Euclidean space, but in today's context, discrete architectural choices are crucial. Current hyperparameter optimization algorithms require experts to simplify structural choices into scalar hyperparameters, limiting the consideration of complex architectural options. The search for optimal model architectures is often limited by simple global hyperparameters, restricting the exploration of complex variations that could improve performance. Neuroevolution techniques, such as genetic algorithms and evolutionary algorithms, have been used to evolve models based on validation performance. Recent approaches also involve reinforcement learning for architecture search. The search for optimal model architectures is often limited by global hyperparameters. Recent approaches involve reinforcement learning for architecture search, but existing methods have fixed model search spaces that hinder human expert input. Evolutionary approaches require handcrafted encodings and genetic operators, making it difficult to flexibly change the search space. The Tree of Parzen Estimators (TPE) algorithm is proposed as a hyperparameter search method that offers better control over model search spaces compared to existing approaches like reinforcement learning. It has been successfully used to tune hyperparameters for Deep Boltzmann Machines and computer vision systems, outperforming previous methods. However, using TPE requires significant effort from human experts to distill and express the hyperparameters of the search space. The Tree of Parzen Estimators (TPE) algorithm offers better control over model search spaces compared to existing approaches. It requires significant effort from human experts to distill and express the hyperparameters of the search space in Hyperopt BID32. The language is modular and composable, simplifying the process of constructing search spaces and mapping models to computational graphs automatically. The model search space specification language is modular and allows for easy expression of complex design choices. Complex modules can be created compositionally from simpler ones, with behavior generated automatically. The language is extensible, enabling the implementation of new module types. The model search algorithm determines how the search space is explored and allocates effort based on previous model performance. It requires a model evaluation algorithm to compute model performance and interacts with the search space through a minimal interface. Different search algorithms like Monte Carlo tree search and Sequential Model Based Optimization are experimented with. Based Optimization BID13 involves evaluating fully specified models by training them on a training set and evaluating on a validation set. The training procedure includes tuning hyperparameters like optimization algorithm and learning rate schedule. Hyperparameters for the evaluation algorithm can be introduced and searched over using a specification language. Components can be changed, improved, or extended while keeping others fixed, allowing for framework extension and reuse. DeepArchitect is a platform for deep learning research and hyperparameter tuning. Computational modules are defined as functions with input dimensionality, hyperparameter values, parameters, and output dimensionality. The hyperparameter set can be structured or a cross product of scalar sets. The number of parameters and output dimensionality can be functions of the input dimensionality. The output dimensionality and number of parameters of a module are functions of input dimensionality and hyperparameter values. For example, in an affine module with h hidden units, output dimensionality is h and number of parameters is (n + 1)h. Similarly, in a convolutional module, parameters and output dimensionality depend on input dimensionality, number of filters, filter size, stride, and padding scheme. This observation allows for architecture search based on fixed input dimensionality and hyperparameter values. The module's computation structure is determined, and this information can be shared with other modules. A module is fully specified when all hyperparameters are chosen, and input dimensionality is known. We focus on single-input single-output modules with no output sharing for simplicity. The single-output case with no sharing is simpler to develop and exemplifies the main ideas for automatic architecture search. The ideas extend to multiple-input multiple-output cases with sharing. Modules can be represented by defining new modules that encapsulate signal paths. Top performing architectures like AlexNet, VGG, and ResNet are captured in this framework. Basic and composite computational modules are distinguished. The search space for automatic architecture design is defined using composite and basic modules like Concat, MaybeSwap, and Optional. These modules encode different models and paths in the tree structure, with binary hyperparameters controlling their behavior. The search space for automatic architecture design is defined using composite and basic modules with binary hyperparameters. Different values for hyperparameters affect the resulting architecture structure. The search space is characterized by input, output, parameters, and hyperparameters. The set of composite modules is not minimal. The search space for automatic architecture design is defined using composite and basic modules with binary hyperparameters. Composite modules like Optional and MaybeSwap are defined based on basic modules like Empty and Or. A tree is built over fully specified models by sequentially assigning values to hyperparameters. Internal nodes represent partial assignments, while terminal nodes represent fully specified models. The search space for automatic architecture design is defined using composite and basic modules with binary hyperparameters. A tree is built over fully specified models by sequentially assigning values to hyperparameters. Internal nodes represent partial assignments, while terminal nodes represent fully specified models. The branching factor of a node corresponds to the number of possible values for the hyperparameter under consideration, and traversing a specific edge from that node to a child corresponds to assigning the value encoded by that edge to the hyperparameter. Each branch in the tree corresponds to the assignment of some value to some hyperparameter. The search space for automatic architecture design involves making decisions on hyperparameters at different nodes in a tree structure. Each node represents a choice between different options, such as the number of filters, filter size, batch normalization, and dropout. The decisions at each node are conditional on previous choices, leading to a fully specified model at the terminal node. The traversal functionality of the search space is derived from the component modules' functionality. The traversal functionality of the search space is derived from the component modules' functionality. Modules need to implement three local operations for traversal: checking if fully specified, returning the hyperparameter being specified and its possible values, and making a choice for the current hyperparameter. This allows for sequential specification of modules in the search space. The search space traversal functionality is derived from component modules' operations, allowing for sequential specification of modules. Compilation of the fully specified model to its computational graph is achieved through recursive mapping of simpler modules. Various search algorithms are built on this functionality. Random search and Monte Carlo tree search (MCTS) are algorithms used for model search in tree-structured search spaces. Random search selects architectures uniformly at random, while MCTS is an approximate planning technique that uses information to guide the search. These algorithms are useful when training deep models is expensive and computational resources are limited. Monte Carlo Tree Search (MCTS) is an effective algorithm that uses information to guide the search in tree-structured spaces. It maintains a search tree expanded incrementally, using tree and rollout policies to evaluate and update node statistics. This approach is beneficial when training deep models is costly and computational resources are constrained. The tree policy in Monte Carlo Tree Search uses an upper confidence bound (UCB) approach to select the best child node for exploration. The UCB score is determined by the number of visits and average scores of the children nodes, with a trade-off parameter for exploration and exploitation. Unexpanded children always take precedence over expanded children in the selection process. When MCTS visits a node in the expanded part of the tree, it must expand all children of that node before expanding any children of its currently expanded children. To address hyperparameters that have similar performance with numeric values, restructuring the tree branches through bisection can be done. This involves sequentially committing to values of hyperparameters rather than directly choosing a value at a node. Tree restructuring in MCTS involves sequentially committing to values of hyperparameters by narrowing down choices through bisection. This tradeoff between depth and breadth allows for better properties and more sharing between different hyperparameter values. The restructured tree can effectively consider hyperparameters with a large number of possible values. Sequential Model Based Optimization (SMBO) BID13 introduces a surrogate function to address the problem of limited sharing between different hyperparameter values in MCTS. This allows for capturing relationships between models and improving information sharing in the tree structure. The surrogate function introduced in SMBO captures relationships between models and helps in selecting which model to evaluate next by optimizing the surrogate function over a search space. Random rollouts are used to evaluate models based on their scores from the surrogate function. An exploratory component is also introduced to enhance the selection process. In SMBO, a surrogate function is used to select models for evaluation based on their predicted performance. An exploratory component involves choosing between evaluating a random model or the best model according to the surrogate function. The surrogate function is updated after each evaluation using a ridge regressor to predict model performance. Features are based on n-grams of sequences of basic modules, ignoring hyperparameter values. More complex features and training losses could improve search performance in the future. The model's score is determined by training it on a training set and evaluating it on a validation set. The training process involves various hyperparameters like optimization algorithm, learning rate schedule, early stopping criteria, and data augmentation strategies. The evaluation algorithm's behavior is defined by experts for the task, including user hyperparameters in the search space. The framework can search over all model hyperparameters, including architecture and training hyperparameters. The search space focuses on deep convolutional models, considering factors like depth, batch normalization, and dropout. Models are evaluated on CIFAR-10, with training hyperparameters like using ADAM or SGD. The training hyperparameters considered include using ADAM or SGD with momentum, initial learning rate, learning rate reduction multiplier, and rate reduction patience. Standard data augmentation techniques are applied to CIFAR-10 images. Search algorithms are compared based on the best model found in terms of validation performance. All algorithms find performant solutions after 64 evaluations. In the study, different model search algorithms are compared based on their performance in finding optimal solutions after a certain number of evaluations. Both SMBO and MCTS with bisection outperform random search, with MCTS starting to outperform around 32 evaluations and SMBO around 16 evaluations. MCTS without restructuring does not outperform random search, possibly due to the large number of possible values for the initial hyperparameters. MCTS with bisection and SMBO can identify and focus on high-performance regions of the search space earlier. In a study comparing model search algorithms, MCTS with bisection and SMBO outperform random search by evaluating a larger fraction of high-performance models. The goal is to show that complex algorithms leverage the search structure better. Using the same search space on MNIST with a larger time budget leads to close to state-of-the-art performance, with a different data augmentation scheme. Randomly sampling 16 models in the search space, they are trained for up to 3 hours. The best model among 16 randomly sampled models in the search space achieved a test accuracy of 99.72%, close to the state-of-the-art. A simple majority voting ensemble of the 5 best models matched the validation accuracy of the best single model and improved test accuracy to 99.75%. Model diversity can enhance ensemble performance, suggesting that defining effective search spaces can lead to successful ensembles. The framework described consists of three components: model search space specification language, model search algorithm, and model evaluation algorithm. It allows for defining expressive search spaces over architectures and exploring them with model search algorithms. This approach can significantly reduce the burden on human experts in designing and training deep models. In Section 7, a search space of deep convolutional models with structural hyperparameters is considered, including depth of the network, batch normalization, dropout, convolutional filter parameters, and training hyperparameters. Experiments on CIFAR-10 compare model search algorithms, showing that MCTS with tree restructuring and SMBO outperform random search. The framework and code for experiments are publicly available, aiming to advance automatic architecture search. In Section 7, a search space of deep convolutional models with structural hyperparameters is considered, including depth of the network, batch normalization, dropout, convolutional filter parameters, and training hyperparameters. The LISP-like pseudocode and corresponding Python implementation for the search space are shown in FIG3 and FIG4, respectively. The model search space in FIG4 closely matches the pseudocode in FIG3 in both semantics and length. Our implementation closely matches the LISP-like pseudocode in FIG3, with code modularity and reusability benefits. We can define auxiliary functions to instantiate modules, as shown in FIG4. Performance profiles of models from the search space are illustrated in Figure 6a, with the best model's architecture and training hyperparameters in Figure 6b. Various basic and composite modules have been implemented in our framework, making it simple to define new modules. Various basic and composite modules have been implemented in our framework, allowing for the definition of new modules easily. Basic modules like Affine and UserHyperparams have specific hyperparameters and parameters, while Composite modules take submodules as arguments, with behavior dependent on them. A composite module specifies its submodules based on hyperparameters. The module interface in Python includes methods for initialization, output dimension, specification, choices, and compilation. New modules can be implemented by following this interface. The module interface in Python includes methods for initialization, output dimension, specification, choices, and compilation. To implement a new type of module, a human expert only needs to implement the module interface, which includes functions like initialize, get outdim, is specified, get choices, and choose. The module interface in Python includes methods for initialization, output dimension, specification, choices, and compilation. The module assigns values to hyperparameters and transitions to the next one. Compilation creates the computational graph of the model in deep learning languages like Tensorflow or PyTorch. Composite modules rely on calls to submodules for implementation. A module in Python can implement its own interface by calling submodules. New modules can have complex signal paths as long as they are encapsulated. The module interface only needs to be implemented by a human expert, with arbitrary single-input single-output submodules within the module. NewModule's interface can be implemented using its submodules M4 and M5. Choosing values for hyperparameters g1 and g2 fully specifies NewModule. Testing if submodules M1, M2, M3, M4, and M5 are fully specified can be done by calling is specified. The output dimensionality of NewModule depends on hyperparameters g2, M5, and M4. The output dimensionality of M5 and M4 can be obtained by calling get outdim. To choose values for hyperparameters, keep track of which submodule they belong to (M1, M2, M3, M4, M5, g1, g2). Use get choices for M1-M5 and compile functionality for g1 and g2 in NewModule."
}