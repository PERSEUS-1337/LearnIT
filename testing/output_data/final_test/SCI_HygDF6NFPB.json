{
    "title": "HygDF6NFPB",
    "content": "Experimental reproducibility and replicability are crucial in machine learning, with concerns raised about their absence in scientific publications. The graph representation learning field has garnered significant attention from researchers, leading to a surge in works. Several Graph Neural Network models have been developed for graph classification, but experimental procedures often lack rigor and reproducibility. To address this issue, we conducted over 47000 experiments to re-evaluate five popular models across nine benchmarks. By comparing GNNs with structure-agnostic baselines, we found evidence that some datasets do not fully utilize structural information. This study aims to improve the evaluation of graph classification models and contribute to the development of the graph learning field. The text discusses flaws in scholarship related to experimental reproducibility in machine learning and science in general. It highlights issues such as ambiguity in experimental procedures and improper comparison of machine learning models. The study focuses on benchmarking state-of-the-art models in graph representation learning, specifically Graph Neural Networks (GNNs), which have become standard tools for machine learning on graphs. Graph Neural Networks (GNNs) combine node features and graph topology to create distributed node representations. They are used for tasks like node classification, link prediction, and graph classification. Despite theoretical advancements, reproducibility issues exist in experimental settings, particularly concerning hyperparameter selection and data splits. Standardization of evaluation code and experimental procedures is lacking in the field. The text discusses the lack of standardized experimental procedures in the graph learning community, leading to doubts and confusion among practitioners. It emphasizes the importance of separating model selection and assessment phases to avoid biased performance estimates. The primary contribution is to provide a fair performance comparison among GNN architectures in a reproducible experimental environment. In a rigorous experimental framework, various models were compared using the same features and data splits. Surprisingly, structure-agnostic baselines outperformed GNNs on some datasets, highlighting the need for caution in reporting improvements. Additionally, the study found that including node degrees as features in social datasets can improve performance and impact the number of GNN layers required for good results. The code for this research is publicly available. The text discusses the release of code and dataset splits for reproducibility of results in evaluating Graph Neural Networks. It aims to establish a standardized evaluation framework for GNNs to enable fair comparisons with existing architectures. GNNs compute node states in a graph, updating them iteratively based on neighboring nodes, utilizing layering or recursive schemes for information propagation. GNNs are popular for efficiently extracting features from graphs, replacing non-adaptive and computationally expensive kernel functions. They use convolution over node neighborhoods for tasks like graph classification, with models like GraphSAGE performing neighborhood aggregation and linear projection for node representation updates. Graph Isomorphism Network (GIN) extends GraphSAGE with arbitrary aggregation functions on multi-sets, proven to be as powerful as the Weisfeiler-Lehman test. Wagstaff et al. (2019) provided an upper bound on hidden units needed for permutation-invariant functions. Edge-Conditioned Convolution (ECC) learns parameters for each edge label, weighting neighbor aggregation accordingly. Deep Graph Convolutional Neural Network (DGCNN) proposes a convolutional layer similar to Kipf & Welling (2017). Some models like ECC and DiffPool use pooling schemes to reduce graph size after convolutional layers. DiffPool combines a differentiable graph encoder with its pooling strategy for end-to-end training. DGCNN sorts and aligns nodes using SortPool. Shchur et al. (2018) compare GNNs on node classification tasks. The curr_chunk discusses the importance of evaluating Graph Neural Networks (GNNs) on multiple test splits to achieve a fair comparison. It also mentions the criticism of neural recommender systems for lack of reproducibility. The risk assessment and model selection procedures are outlined for the experimental procedure in the paper. The risk assessment procedure involves using k-fold Cross Validation to estimate model performance without using test data for model selection. Model selection aims to choose the best hyperparameter configuration for a specific validation set. The main contribution of this work is to clearly separate model selection and model assessment estimates, addressing biases in validation performances. This is motivated by issues in experimental setups and reproducibility of results in recent papers on GNN models. The study selected GNN works based on specific criteria such as performance with 10-fold CV, peer-reviewed status, architectural differences, and popularity. Models like DGCNN, DiffPool, ECC, GIN, and GraphSAGE were chosen. Criteria for evaluation and reproducibility included providing code for data preprocessing, model selection, and assessment, data splits with stratification technique, reporting results of 10-fold CV with standard deviations, and focusing on model evaluation rather than selection. The authors evaluated GNN models based on reproducibility criteria, with findings summarized in Table 1. The model was evaluated using 10-fold CV, with fixed architecture but tuning learning rate and epochs on one random fold. However, the lack of code for model selection reproduction and running CV 10 times may lead to sub-optimal performance. The authors ran 10-fold cross-validation to reduce variance in estimates, but did not apply the same procedure to other competitors. CV data splits are correctly stratified and publicly available for reproducibility. It is unclear if reported results are from a test or validation set. Standard deviations for DiffPool and competitors are not reported. Early stopping was applied to prevent overfitting, but model selection code and validation splits are not available. Data is randomly split without stratification or setting a random seed. The ECC model evaluation lacks standard deviations in 10-fold CV results. Hyper-parameters are fixed without clear model selection process. GIN lists tuned hyper-parameters but reports validation accuracy, not model evaluation. GraphSAGE is not tested on graph classification datasets in the original paper. Our analysis reveals that GNN works often lack good machine learning practices in evaluation and reproducibility. This highlights the importance of re-evaluating models in a rigorous and reproducible manner. In our main experiment, we re-evaluate GraphSAGE, DiffPool, and GIN on 9 datasets, implementing two baselines to assess the utilization of structural information. The study re-evaluated GraphSAGE, DiffPool, and GIN on 9 datasets using Pytorch Geometrics library for graph processing. Discrepancies between papers and code were resolved by following paper specifications. GraphSAGE was adapted for graph classification using max-pooling global aggregation. Various publicly available graph datasets were used, including molecular and social graphs. In the study, various datasets were used for binary and multi-class classification, including molecular and social graphs. GNN literature commonly augments node descriptors with structural features, such as degree and clustering coefficient. Different models have different approaches to incorporating node degrees, with a trade-off between performance and generalizability. Good experimental practices recommend consistent comparison of models with the same input. In the study, models are evaluated using the same input representations for consistency. For the chemical domain, nodes are labeled with atom type encoding, while for social graphs, either an uninformative feature or node degree is used. The focus is on the model's ability to learn structural features and the impact of node degrees on machine learning models for graphs. In this study, the impact of node degrees on performances for social datasets is investigated. Two distinct baselines are adopted for chemical and social datasets. Molecular Fingerprint technique is used for chemical datasets, while permutation-invariant functions over sets of nodes are used for social domains and ENZYMES. Both baselines do not leverage graph topology. The curr_chunk discusses the importance of using structure-agnostic baselines for evaluating the effectiveness of Graph Neural Networks (GNNs) on specific datasets. It highlights that if GNN performances are similar to baselines, it could indicate either the task doesn't require topological information or the GNN isn't utilizing graph structure effectively. Factors like training data, architecture bias, and hyperparameters play a role in assessing this. Significant improvements over baselines suggest successful exploitation of graph topology. The experimental approach involves using a 10-fold CV for model assessment and an inner holdout technique with a 90%/10% training/validation split for model selection. After each model selection, the best configuration is used to evaluate the external test fold. Training is done three times on the whole training fold, with early stopping applied by holding out 10% of the data. This process helps smooth the effect of random weight initialization. The evaluation process involves three runs to mitigate the impact of random weight initialization on test performances. The final test fold score is the mean of these runs, with early stopping implemented to prevent overfitting. Data partitions are pre-computed, ensuring consistent model selection and evaluation on stratified data splits. Hyper-parameter tuning is done through grid search. Hyper-parameter tuning is performed via grid search, involving a large number of training runs with grid sizes ranging from 32 to 72 configurations. The total effort exceeded 47000 single training runs to assess models. Various hyper-parameters are selected for all models, including the number of convolutional layers, embedding space dimension, learning rate, and early stopping criterion. Model-specific parameters like regularization terms and dropout are also considered. The study involved over 47000 training runs using parallelism on CPU and GPU to assess models. Training on a single hyper-parameter configuration could take over 72 hours. Results showed GIN to be effective on social datasets. Our experiments showed that GIN is effective on social datasets. However, on D&D, PROTEINS, and ENZYMES, none of the GNNs improved over the baseline. In contrast, on NCI1, the baseline was clearly outperformed, indicating that GNNs can utilize the graph's topological information in this dataset. Additionally, we found that an overly-parameterized baseline struggled to overfit the NCI1 training data, while GIN could easily overfit it. The structural information significantly impacted the ability to fit the training set. On social datasets, adding node degrees as features was beneficial, especially for REDDIT-BINARY, REDDIT-5K, and COLLAB. Our results suggest that current state-of-the-art GNN models struggle to fully utilize structural features in datasets like D&D, PROTEINS, and ENZYMES. Small performance gains on these datasets may be attributed to factors like random initializations rather than successful exploitation of the structure. It is recommended for GNN practitioners to include baseline comparisons in future works to gain better insights. Based on the results, using node degrees as input features significantly improves performance on social datasets, with an increase of approximately 15%. However, adding node degrees is less impactful for most GNNs as they can infer this information from the structure. DGCNN is an exception, requiring node degrees to perform well on all datasets. The ranking of models changes significantly with the addition of degrees, prompting further investigation into the impact of other structural features on performance. Adding node degrees as input features improves performance on social datasets by approximately 15%. Most GNNs can infer this information from the structure, except for DGCNN, which requires node degrees to perform well. The addition of degrees changes the ranking of models, prompting further investigation into the impact of other structural features on performance. GraphSAGE is an exception, where the addition of the degree reduces the number of layers needed by \u2248 1. In this paper, the authors emphasize the importance of rigorous model selection and assessment protocols in evaluating Graph Neural Networks (GNNs). They highlight the ambiguities in experimental settings of previous studies and propose a clear and reproducible procedure for future comparisons. The authors conducted a complete re-evaluation of five GNNs on nine datasets, demonstrating the time and computational resources required for such evaluations. The authors conducted a re-evaluation of five GNNs on nine datasets, highlighting the importance of structure-agnostic baselines and the effect of degree features on model performance. They aim to provide reliable results for the graph learning community and release a library for comparison. The authors conducted a re-evaluation of five GNNs on nine datasets, emphasizing structure-agnostic baselines and the impact of degree features on model performance. They use external k outfold CV for generalization performance estimation and a hold-out technique for hyper-parameter selection. Additionally, they mention the option of inner k inn -fold CV for Nested Cross Validation, increasing computational costs. A table of hyper-parameters for model selection is provided."
}