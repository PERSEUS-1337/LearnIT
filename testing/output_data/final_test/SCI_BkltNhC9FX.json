{
    "title": "BkltNhC9FX",
    "content": "Modern neural architectures heavily rely on attention for mapping structured inputs to sequences. This paper demonstrates that existing attention architectures do not effectively capture the relationship between attention and output tokens in a predicted sequence. Posterior Attention Models propose changes in attention architecture by marginalizing attention at the output instead of the input, leading to improved performance on translation and morphological tasks compared to existing models. Attention is crucial for sequence learning tasks like translation and speech to text conversion. Many variants of attention have been proposed, including soft, sparse, local, hard, monotonic hard, hard non-monotonic, and variational. Soft attention is the most prevalent, computing attention for each output as a multinomial distribution over input states. It is end-to-end differentiable, easy to implement, and widely popular. Hard and sparse attentions are less popular due to implementation difficulties. This paper revisits the statistical soundness of soft attention and other variants in capturing dependencies between attention and output variables. The Posterior Attention Model (PAM) introduces a principled approach to attention variables in sequence prediction. It combines output and attention variables in a joint distribution for efficient training and inference. Unlike existing models, PAM directly couples output and attention, leading to benefits of hard attention without computational challenges. It also introduces the concept of posterior attention, improving the overall model performance. The Posterior Attention Model (PAM) introduces a new approach to attention distribution, conditioning subsequent attention on the output corrected posterior attention for improved performance in translation and morphological inflection tasks. The model shows higher alignment accuracy, better input coverage, and lower entropy compared to soft attention models. The Posterior Attention Model (PAM) improves attention distribution by considering the output token. It aims to model the conditional distribution Pr(y|x) of output sequence y given input sequence x. Each output y depends on a specific part of the input x, determined by the attention variable a. During training, x and y are observed, but a is hidden. The attention distribution Pr(y|x) is modeled by the Posterior Attention Model (PAM) to improve attention. Existing Encoder-Decoder (ED) networks factorize Pr(y|x 1:m ) by applying chain rule on y variables. The distribution of each attention variable a t is computed as a function of the decoder state and encoder state. An attention weighted sum of the input states is computed to obtain the input context c t. The attention distribution is improved by the Posterior Attention Model (PAM) in Encoder-Decoder networks. Attention variables are computed from decoder and encoder states to obtain input context c t. Attention is modeled as latent-alignment variables in the joint distribution Equation 1, making the model more tractable. The Posterior Attention Model (PAM) improves the attention distribution in Encoder-Decoder networks by considering the interaction of attention and output variables. This model allows for more realistic latent behavior compared to previous models that ignore relationships between attentions and output variables. The Posterior Attention Model (PAM) aims to improve attention distribution in Encoder-Decoder networks by considering the interaction of attention and output variables. This approach allows for more realistic latent behavior by expressing the joint distribution as a product of tractable terms computed at each time step. The Posterior Attention Model (PAM) improves attention distribution in Encoder-Decoder networks by considering the interaction of attention and output variables. It simplifies the joint distribution into factors at each decoding step, conditioning only on previous outputs and attention. The attention at each step is termed as prior attention or posterior attention based on whether it is conditioned on previous outputs or observed output labels, respectively. The Posterior Attention Model (PAM) improves attention distribution in Encoder-Decoder networks by considering the interaction of attention and output variables. It simplifies the joint distribution into factors at each decoding step, conditioning only on previous outputs and attention. The attention at each step is termed as prior attention or posterior attention based on whether it is conditioned on previous outputs or observed output labels, respectively. The attention at step t should be computed from the posterior attention of the previous step, as it reflects an alignment of the input and output, improving its distribution when the output is known. The methods explored for computing P(a_t|s_t, a_t-1, y_t-1) are designed to be light-weight with minimal extra parameters. The simplest method involves using the same decoder RNN to absorb the posterior attention of the previous step. The function is linearized using the first order Taylor expansion to efficiently approximate computation of Prior t (a). The decoder RNN state is updated based on the context computed from the previous step's posterior attention. The ED model updates the RNN with posterior attention, leading to improved accuracy. Models like Prox-Postr-Joint focus on adjacent attention positions, while Monotonicity biased attention differs in coupling definition. The Mono-Postr-Joint model introduces a monotonic bias in attention by defining coupling energy differently. This bias benefits tasks with natural monotonic attention. The overall architecture, called Posterior Attention Model (PAM), computes a joint distribution of output and attention at each step, leading to a mixture of multiple output distributions. The Mono-Postr-Joint model, known as Posterior Attention Model (PAM), introduces a monotonic bias in attention by defining coupling energy differently. This leads to a mixture of multiple output distributions, with each focused on one input. The attention distribution propagated to the next step is posterior to observing the current output, resulting in significant accuracy gains. The model computes a joint distribution of output and attention at each step, but may require selecting top-K attentions to handle large vocabulary sizes efficiently. The Posterior Attention Model (PAM) introduces a monotonic bias in attention by updating the decoder state differently. Significant accuracy gains were observed by propagating posterior attention to subsequent steps. The model computes a joint distribution of output and attention, augmenting posterior attention with input from standard attention to address sparsity issues. The prior attention distribution is conditioned on previous attention to incorporate natural biases like proximity and monotonicity. The Posterior Attention Model (PAM) introduces a monotonic bias in attention by updating the decoder state differently to achieve accuracy gains. It computes a joint distribution of output and attention, augmenting posterior attention with input from standard attention to address sparsity issues. The model factors the joint distribution as the product of local distributions at each time step for efficient gradient updates and minimal changes to existing beam-search inference. The standard forward algorithm is used to handle Markovian dependencies in the model. The encoder-decoder model is the standard for sequence to sequence learning via neural networks. Various attention models have been proposed, including Soft Attention for seq2seq learning and Hard Attention for explicit focus. Hard Attention focuses on one input state for an output, proving useful in model adaptation and catastrophic forgetting. Hard Attention is used in sequence to sequence learning to focus on specific input states for an output. Training Hard Attention requires the REINFORCE algorithm and can be subject to high variance. Different approaches, such as using monotonic hard attention or sparse/local attention, have been proposed to address these challenges. In sequence to sequence learning, Hard Attention is utilized to focus on specific input states for an output. Sparse/local attention methods aim to make attention sparse using sparsity inducing operators. Yang et al. (2016) previously modeled the relationship between attentions at different time steps using a recurrent history mechanism. Our model proposes that posterior attention should be the link to the next attention, incorporating dependence on history. The model factorizes the joint distribution and models the posterior attention distribution and attention coupling. Unlike other models, it uses the actual posterior for computing attention. Structured Attention Networks interpret attention as a latent structural variable and implement segmental and syntactic attention using graphical models. These works focus on attention at each step independently. In this paper, the focus is on modeling the dependency among adjacent attention using a posterior attention framework. Results in Table 1 show that models with posterior attention outperform those with prior attention, as well as soft and sparse attention. The model uses a 2-layer bi-directional encoder and decoder with LSTM units and dropout. Word-level encoding is used for all translation tasks. The study focuses on the performance advantage of joint modeling and posterior attention in translation tasks. Different coupling models are explored, with Mono-Postr-Joint showing slight improvements for language pairs with natural monotonic alignment. The model is also applied to generating morphological inflections beyond translation tasks. The study explores generating morphological inflections for German Nouns and Verbs using a one layer encoder and decoder model. Joint modeling shows significant gains in accuracy compared to task-specific hard-monotonic attention. Mono-Postr-Joint model with a bias towards monotonic attention achieves even higher improvements. The study aims to understand why posterior attention models outperform soft attention in end-to-end tasks. The study demonstrates that posterior attention models outperform soft attention in end-to-end tasks due to better alignment of input and output. Through experiments in a teacher forcing setup, it is shown that posterior attention is more focused, provides better alignment accuracy, and improved input coverage. Anecdotal examples illustrate the difference in attention between Postr-Joint and SoftAttention, with Postr-Joint correcting mistakes and providing appropriate context for the next step. Postr-Joint corrects mistakes and provides context for the next step, showing better alignment and accuracy compared to Soft-Attention. Attention entropy affects Soft-Attention more than other models, indicating that posterior attention can learn the distribution more easily. The accuracy decay is smoother in Postr-Joint, especially when the input is 'pure' rather than diffused via pre-aggregation. The accuracy of Postr-Joint, Prior-Joint, and Soft-Attn models on the EnglishGerman pair decreases as attention uncertainty increases. Soft attention models have a higher number of cases with high attention uncertainty, leading to lower performance. Joint models outperform soft-attention by reducing the number of such cases and showing a smoother accuracy decay with attention uncertainty. In cases of high attention certainty, Postr-Joint slightly underperforms Prior-Joint. Based on the accuracy of Postr-Joint, Prior-Joint, and Soft-Attn models on the English-German pair, Postr-Joint slightly underperforms Prior-Joint in cases of high attention certainty. However, Postr-Joint shows stabler behavior overall. The failure of attention to produce corresponding linguistic structures has been noted, leading to the hypothesis that Posterior Attention may produce better alignments. The alignment accuracy was tested using the RWTH German-English dataset, comparing Soft, Prior-Joint, and Postr-Joint attentions. The AER metric was used to compare these alignments against expert alignments. In an experiment, the reliability of different attention models was indirectly assessed by measuring if attention covered the entire input sequence. Soft attention model often neglects some sentences during decoding. Prior-Joint and Postr-Joint outperform soft attention significantly. Existing attention models do not adequately capture the relationship between output and attention for sequence prediction tasks. A new factorization approach is proposed. The proposed factorization of the joint distribution allows for decomposition over output tokens, similar to existing attention mechanisms. This leads to more principled probabilistic modeling of the dependency structure, with important differences such as aggregating predictions across all attention and conditioning subsequent attention distribution on posterior attention. Additionally, attention coupling enables the incorporation of task-specific biases and prior knowledge into attention. Our work explores incorporating task-specific biases and prior knowledge into attention, leading to boosts in related tasks. Future work could scale these techniques to large-scale models and multi-headed attention, as well as incorporate more complex biases like phrasal structure or image segments into joint attention models."
}