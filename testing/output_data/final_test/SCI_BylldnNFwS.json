{
    "title": "BylldnNFwS",
    "content": "This work uses tropical geometry to characterize the decision boundaries of neural networks with piece-wise linear activations. The decision boundaries are shown to be a subset of a tropical hypersurface related to a polytope formed by the convex hull of two zonotopes. The generators of the zonotopes are functions of the neural network parameters, providing new insights into the lottery ticket hypothesis. The decision boundaries of neural networks are characterized using tropical geometry, providing new insights into the lottery ticket hypothesis. This characterization is used as tropical regularizers in network pruning and generating adversarial attacks by perturbing decision boundaries. Previous works have shown the importance of decision boundary smoothness and curvature in network robustness. The decision boundaries of neural networks are studied using tropical geometry, revealing insights into the lottery ticket hypothesis. Previous research has highlighted the significance of decision boundary smoothness and curvature for network robustness. Additionally, studies have shown that decision boundaries of deep neural networks can converge to a linear SVM under certain conditions. The decision regions of DNNs with smaller width than input dimension are found to be unbounded. The piecewise linear ReLU activation function has sparked interest in studying this class of DNNs. In this section, the decision boundaries of a network in the form (Affine, ReLU, Affine) are analyzed using tropical geometry. The network's functional form is defined as f(x) = Bmax(Ax + c1, 0) + c2, where max(.) is an element-wise operator. The outputs of the network f are the logit scores. The parameters A, B, c1, and c2 are specified for ease of notation. In this section, we analyze the decision boundaries of a bias-free neural network with two outputs using tropical geometry. The network's outputs are expressed as tropical rational functions. The main results are presented for a network without biases, with the decision boundary defined as {x \u2208 R^n : f_1(x) = f_2(x)}. The decision boundaries of a bias-free neural network with two outputs are analyzed using tropical geometry. The decision boundary is defined as {x \u2208 R^n : f_1(x) = f_2(x)}. Theorem 2 states that the decision boundaries are linear pieces separating classes C1 and C2, with the dual subdivision being the convex hull between zonotopes Z G1 and Z G2. The normals to the dual subdivision are in correspondence with the tropical hypersurface T(R(x)), which is a superset of the decision boundaries B. The dual subdivision for a neural network's decision boundaries is the convex hull of two zonotopes, denoted as Z G1 and Z G2, and is a function of network parameters A and B. Theorem 2 bridges the gap between decision boundaries B and the polytope \u03b4(R(x)), which is the convex hull of two zonotopes. Zhang et al. (2018) discussed a special case of Theorem 2 for a neural network with a single output and a score function s(x). This work is the first to propose a tropical approach in this context. This work introduces a tropical geometric formulation for a super-set containing the decision boundaries of a multi-class classification neural network. Theorems show that altering the network while preserving the tropical hypersurface can help maintain decision boundaries. The dual subdivision, a well-structured geometric object, can be utilized to preserve decision boundaries in bias-free networks. The tropical hypersurface is the skeleton of the dual to \u03b4(R(x)), with normal lines corresponding to the tropical hypersurface. Zonotopes can be defined using a generator matrix U \u2208 R p\u00d7n or as a Minkowski sum of line segments. The number of vertices of a zonotope is polynomial in the number of line segments. Theorem 2 establishes a strong relation between a polytope and decision boundaries. The text discusses the construction of zonotopes using line segments and the relationship between polytopes and decision boundaries. It presents a method to efficiently compute zonotopes with arbitrary end points of line segments. The text discusses the construction of zonotopes using line segments and the relationship between polytopes and decision boundaries. It presents applications for Theorem 2, including leveraging geometric structure to reaffirm the lottery ticket hypothesis proposed by Frankle & Carbin (2019). The hypothesis suggests the existence of sparse trainable sub-networks within dense, randomly-initialized networks that perform as well as the original network after pruning and reinitialization. The text discusses leveraging geometric structure to reaffirm the lottery ticket hypothesis by proposing a pruning algorithm that results in a smaller network with similar accuracy. The algorithm involves standard network pruning and initializing the pruned network with the same initialization as the original training. The orientation of decision boundaries polytope is preserved after pruning with this proposed initialization algorithm. The text discusses training a neural network with 2 inputs, 2 outputs, and a single hidden layer with 40 nodes. The network is pruned by removing the smallest x% of weights and then trained with different initializations. The orientation of decision boundaries polytope varies significantly with different initialization schemes. The text discusses how the orientation of decision boundaries polytopes varies significantly with different initialization schemes when pruning a neural network. The focus is on preserving decision boundaries throughout the evolution of pruning, with further analysis left for the appendix. Network pruning is an effective approach to reduce computational cost and memory usage during network inference. The task is to find a smaller subset of network parameters while maintaining similar decision boundaries and accuracy. A new geometric approach towards network pruning is discussed, showing that preserving certain polytopes can preserve decision boundaries. Theorem 2 states that preserving the polytope \u03b4(R(x)) preserves decision boundaries in a neural network. Pruning a neuron may not affect the dual subdivision polytope or accuracy. The question arises whether a sparser network can preserve decision boundaries. The optimization problem aims to determine if a smaller network with fewer line segments can preserve decision boundaries. By approximating the generators, the decision boundaries are maintained. The matrix mixed norm encourages row sparsity in the weight matrix. The optimization problem aims to encourage row sparsity in the weight matrix by solving it with alternating optimization over variables\u00c3 and B. The extension to deeper networks involves pruning consecutive blocks, and experiments evaluate the proposed pruning approach against classical methods on various architectures and datasets. The tropical pruning approach is compared to classical methods like Class Blind, Class Uniform, and Class Distribution on different architectures and datasets. Class Blind prunes parameters with the smallest magnitudes across all nodes, while Class Uniform prunes parameters per node. Class Distribution prunes parameters based on the standard deviation of magnitudes per node. The focus is on pruning fully connected layers in deep neural networks like AlexNet and VGG16. We focus on pruning fully connected layers in deep neural networks like AlexNet and VGG16. Training these networks on SVHN, CIFAR10, and CIFAR100 datasets, we can prune over 90% of classifier parameters without affecting accuracy. By retraining network biases only, we can further increase the pruning ratio without accuracy loss. Our proposed approach focuses on pruning fully connected layers in deep neural networks like AlexNet and VGG16. By utilizing the area under the curve (AUC) metric, we can achieve up to 90% parameter reduction without sacrificing accuracy. The algorithm for identifying rows and elements to prune is detailed in the appendix. Our method outperforms other popular pruning schemes, particularly excelling in AUC by 3% for AlexNet. Our approach focuses on pruning fully connected layers in deep neural networks like VGG16, achieving better performance in AUC compared to other methods. We emphasize a new geometry-based perspective to network pruning, with experiments showing that retraining biases can preserve decision boundaries. In this work, a tropical geometric view is provided on adversarial attacks in neural networks. The authors leverage Theorem 2 to construct a tropical geometric based targeted adversarial attack, offering a dual view to the problem. The text discusses a dual view approach to adversarial attacks on neural networks, where instead of perturbing the input sample, the network parameters are perturbed to move decision boundaries. This alternative method aims to alter the classification region of the input without changing the input itself. The text discusses a dual view approach to adversarial attacks on neural networks, where parameters are perturbed to move decision boundaries. Theorem 2 provides means to construct attacks by perturbing decision boundaries, resulting in minimal perturbations to change network predictions. The problem is formulated with constraints to ensure the desired target class when the input is perturbed. The text discusses perturbing parameters in neural networks to manipulate decision boundaries for adversarial attacks. Constraints ensure the desired target class when input is perturbed, with a focus on limiting perturbations and maintaining feasibility. The function D2 captures perturbations in the dual subdivision polytope, extending to multi-class neural networks. The derivation, discussion, and extension of (6) to multi-class neural networks is left for the appendix. Problem (5) is solved with a penalty method on linear equality constraints. Training a single hidden layer neural network with 50 hidden nodes and 2 outputs on a simple dataset shows perturbing decision boundaries by perturbing the first linear layer results in misclassification. Perturbing different decision boundaries corresponds to perturbing different edges of the dual subdivision. Perturbing decision boundaries in neural networks through alterations in the dual subdivision polytope can lead to misclassification of inputs. Adversarial examples designed to misclassify MNIST images show changes in network predictions for specific digits. The approach leverages tropical geometry to characterize decision boundaries, offering a new perspective in the field of adversarial attacks. The text discusses using tropical geometry to characterize decision boundaries in neural networks, linking it to geometric objects like zonotopes and polytopes. This perspective supports the lottery ticket hypothesis, network pruning, and adversarial attacks. Future work includes extending this analysis to convolutional neural networks and graphical convolutional networks. The text discusses using tropical geometry to characterize decision boundaries in neural networks, linking it to geometric objects like zonotopes and polytopes. The Minkowski sum P+Q is defined between sets P and Q. For a bias-free neural network f(x) : R^n \u2192 R^2 with decision boundaries given by set B, we have B \u2286 T(R(x)). The tropical hypersurface of R(x) is where the maximum is attained by two or more monomials. The tropical hypersurface of R(x) is where the maximum is attained by two or more monomials in the decision boundaries. The operator+ indicates a Minkowski sum between sets, resulting in geometric objects like zonotopes. \u03b4(Q2(x)) is the Minkowski sum of points in R^n, while \u03b4(H1(x))+\u03b4(Q2(x)) is a Minkowski sum between a zonotope and a single point. In this section, Theorem 2 for a neural network in the form of (Affine, ReLU, Affine) with non-zero biases is derived. The presence of biases does not affect the results, only increasing the dimension of the space where the polytopes exist. The first linear layer for x \u2208 R^n is discussed, with coordinates and \u2206(Q1i) as a point in (n + 1) dimensions at (A - (i, :), 0). The extension to networks with multi-class output is mentioned, where the analysis can be applied to studying decision boundaries between any two classes. The geometrical representation of the output of the neural network layers remains unchanged under projection \u03c0, even with the addition of biases. The Minkowski sum of line segments results in a zonotope for the output of the network. The Minkowski sum of line segments forms a zonotope in (n + 1) and n dimensions for the first and second expressions respectively. The second part involves a Minkowski sum of points, resulting in a single point in (n + 1) and n dimensions. The biases in the second linear layer do not affect the geometrical representation of decision boundaries under projection \u03c0. The zonotope formed by line segments in Rn is equivalent to the zonotope formed by the line segments. The zonotope formed by line segments in Rn is equivalent to the zonotope formed by the line segments. The problem of updating rows of \u00c3 independently exhibits a closed form solution. The problem is separable in the coordinates of B + (1, :) and can be solved using projected gradient descent. Theorem 2 describes a superset to the decision boundaries of a binary classifier through the dual subdivision R(x). For a neural network with k classes, a natural extension is to analyze the pair-wise decision boundaries of all k-classes. The set S represents all possible pairwise combinations of the k classes. The generator Z(G(i,j)) is the zonotope with the generator matrix. The text discusses the computational efficiency of calculating G(i+, j-) in neural networks with k classes. It suggests representing G(i+, j-) as a Minkowski sum of two zonotopes, leading to a concatenation between G(i+) and G(j-). The objective is expanded to approximate the generators, with a counting argument used to solve the objective for multi-class networks in experiments. The text discusses the computational efficiency of calculating G(i+, j-) in neural networks with k classes by representing it as a Minkowski sum of two zonotopes. The objective is expanded to approximate the generators, with a counting argument used to solve it for multi-class networks in experiments. The overall objective includes enforcing sparsity constraints for pruning purposes, and updates for A and B are derived. The parameters of the pruned network are then constructed accordingly. The function D2(\u03beA) captures perturbations in the dual subdivision polytope to ensure similarity between networks with and without perturbations. This extends to multi-class networks with k classes. Linear equality constraints are enforced using a penalty method with ADMM updates. The augmented Lagrangian and ADMM update steps are outlined for solving the optimization problem. The ADMM updates involve updating \u03b7, w, and z in a separable manner. The linearized ADMM converges for some non-convex problems by adding a Bergman divergence term. The proximal gradient method is used to solve the problem, with hyper-parameter settings and experimental results discussed. The importance of investigating the tropical geometrical perspective of decision boundaries is highlighted. In this section, an experiment is conducted to differentiate between investigating the tropical geometrical representation of the functional form of a network and the shape of decision boundaries. By training a single hidden layer neural network on a simple dataset and performing pruning iterations, it is shown that changes in the zonotopes do not affect the shape of the decision boundaries. The experiment conducted in this section focuses on the decision boundaries of a neural network and the impact of lottery ticket initialization when pruning and retraining the model. The lottery ticket initialization is shown to preserve the shape of the decision boundaries polytope the most. The tropical pruning method involves controlling two hyper-parameters, the number of iterations and the regularizer coefficient \u03bb, to achieve 100% pruning. The experiment focuses on pruning neural networks with the tropical pruning method, achieving 100% pruning by controlling hyper-parameters. The algorithm removes non-effective line segments without changing non-deleted segments, resulting in new sparse matrices. Additional results show pruning of AlexNet and VGG16 on various datasets, as well as attacking decision boundaries of synthetic data and MNIST images. The authors thank R3 for reviewing the paper and address concerns about the use of tropical geometry in theoretical analysis. R3 questions the benefits and novelty of using tropical geometry, urging the authors to clarify their motivation and contribution in the paper. The paper discusses the use of tropical geometry in theoretical analysis, redefining concepts and transforming algebraic problems into combinatorial problems on polytopes. This approach promises a powerful element in solving piecewise linear problems. Zhang et. al. 2018 rederived classical results on the number of linear pieces in DNNs using a simpler analysis. This work focuses on studying decision boundaries of piecewise linear DNNs through tropical geometry, providing a new geometrically motivated approach for training DNNs. The authors focus on studying decision boundaries of piecewise linear DNNs through tropical geometry, proposing new optimization problems based on Theorem 2. They show a relation between network perturbations and adversarial attacks. In experiments on Tropical Pruning, they compare their approach against other methods. The authors corrected a typo in Figure reference and added the definition of pruning methods. They are questioned about proposing a new attack method and the use of different attack settings. The authors discuss the use of a new geometric view for constructing adversarial attacks, emphasizing that the decision boundaries are a function of network parameters, not the input space. They question how adversarial attacks can be framed in this new setting, as it pertains to perturbing the parameters space of the network rather than the input space. In the context of constructing adversarial attacks, the authors propose a method that perturbs the network parameters space to flip predictions. By solving a linear system, one can find an equivalent pixel adversarial attack that flips the prediction of the original network. This approach incorporates geometric information and is solved using a mix of penalty and ADMM algorithm. The authors propose an algorithm (a mix of penalty and ADMM) to construct adversarial attacks that flip network predictions on the MNIST dataset. They thank R2 for reviewing the paper and addressing all comments/suggestions in the revised version. The paper discusses the complexity of the decision boundary of deep neural networks using tropical geometry perspective. In our work, we use tropical geometry to analyze decision boundaries in deep neural networks. This analysis involves representing decision boundaries through a solution set related to a geometric structure known as the decision boundaries polytope. This approach is different from previous works that focused on the convergence of decision boundaries to SVM classifiers or the unbounded nature of decision regions in neural networks with smaller widths. The polytope constructed using tropical geometry represents decision boundaries in deep neural networks. The normals to the edges of this polytope preserve the direction of decision boundaries, leading to new insights in network pruning and adversarial attacks. This new representation offers potential for geometrically inspired network regularizers and applications beyond. Efficiently extending tropical pruning to convolutional layers is a nontrivial direction. Convolutional layers naturally fit the framework since a convolutional kernel can be represented with a structured topelitz/circulant matrix. However, the question of efficiency remains as constructing the underlying structured matrix is still necessary. The tropical formulation of network pruning problem focuses on avoiding the need to construct dense representations of convolutional kernels. Comparing decision boundaries between different architectures can be challenging, but a proposed approach involves computing the distance between dual subdivision polytopes representing the architectures. The proposed objective involves comparing the orientation of polytopes by approximating the distance between their generators. Test accuracies are compared across architectures based on pruning ratio, but similar accuracies do not necessarily imply similar decision boundaries. Adversarial examples aim to generate misclassified instances resembling real data for pre-trained neural networks. The new approach aims to generate adversarial examples by altering the decision boundary of a pre-trained model. The decision boundaries are geometrically represented as a convex hull between two zonotopes, dependent only on network parameters. This method introduces a fresh perspective on adversarial attacks, challenging the traditional approach. The adversarial attacks problem in the new tropical setting involves perturbing the network parameters to flip the prediction, closely related to perturbing the geometric structure in the first layer. This method challenges the traditional approach by altering decision boundaries based on network parameters. The paper introduces a new method involving a linear system that flips network predictions, similar to classical adversarial attacks. The approach successfully flipped predictions on the MNIST dataset, aiming to provide a novel geometric perspective in the field of adversarial attacks. The method challenges traditional approaches by altering decision boundaries based on network parameters. In this paper, the authors introduce a method that analyzes decision boundaries in adversarial attacks. They propose a geometric characterization of decision boundaries that is solely dependent on network parameters, not input space. This approach offers a dual view to adversarial attacks, focusing on perturbing network parameters to maximize perturbation to decision boundaries. The authors focus on a new geometric polytope representation of decision boundaries in the network parameter space, different from prior work. They address concerns from reviewers and have made revisions to improve clarity and exposition in the paper. The authors have made revisions to improve clarity and exposition in the paper, including merging paragraphs and adding references. They have addressed suggestions such as adding information about the semiring and tropical quotient in the revised version. The focus is on a new geometric polytope representation of decision boundaries in the network parameter space. The upper faces of a polytope are defined as the faces that can be seen from \"above\". The formal definition involves a canonical vector and is illustrated in Figure 2. The paper has addressed issues with Theorem 2 and clarified the relationship between decision boundaries and the tropical hypersurface. The color map in Figure 2 represents compression percentage. The second figure in Figure 2 shows the polytope of the dual subdivision with overlapping vertices and small edges. The normals to the major edges in the polytope are parallel to decision boundaries in the \"Input Space\" figure. The orientation of the polytope is preserved for the lottery ticket initialization, unlike other types of initialization. The decision boundaries in the paper are subsets of the tropical hypersurfaces, as shown by Maclagan & Sturmfels (2015). The red structures in the convex hull visualization are normals, although not formally defined in the paper. The decision boundaries in the paper are subsets of the tropical hypersurfaces, with normals parallel to the edges. For more information, refer to the work of Erwan Brugall and Kristin Shaw. The discussion on the functional form is elaborated in section F. The decision boundary polytope remains unchanged with pruning, while the zonotopes show differences. The decision boundary polytope remains unchanged with pruning, while the zonotopes (dual subdivisions of tropical polynomials) vary significantly. Different tropical polynomials can represent the network's functional form while having the same structure for the decision boundary polytope. This observation warrants further investigation in future work. Multiple experiments were conducted in Section 4, and more experiments of this nature would be beneficial to explore claims over different repetitions and larger architectures. Additional experiments have been included in the appendix and based on suggestions. The decision boundaries polytope for larger architectures is more difficult due to generic properties beyond Affine-ReLU-Affine structures. Enumerating vertices becomes computationally intractable, especially for networks with more than 3 dimensional inputs. Formalizing the claim that orientations are preserved is crucial, with investigations into metrics like histograms of oriented normals. The study explores using histograms of angles for normals to edges in polytopes and investigating Hausdorff distance as a metric between polytopes. Future work includes designing distance functions capturing orientation information for applications like tropical pruning. The experiments currently focus on approximating distances between sets of generators of zonotopes in a Euclidean sense. Additional information on Minkowski sum and other pruning methods has been included in the revised version. In the revised version, the experiments involve training base networks like AlexNet and VGG16 on datasets like SVHN, CIFAR10, and CIFAR100 to achieve state-of-art results. Pruning schemes, including the tropical approach, are applied with varying pruning ratios without re-training the networks. Experiments in the appendix involve fine-tuning biases after each pruning step to consider reporting averaged results. In the final version, results will be reported averaged over multiple runs. The comment on normals generating a superset to decision boundaries was clarified. Perturbing decision boundaries is a dual view for adversarial attacks, allowing for flipping network predictions by either perturbing the sample or decision boundaries. In a feasibility study, it was shown that decision boundaries can be perturbed by altering the dual subdivision polytope. This perturbation can lead to changes in network predictions, as discussed in the subsection \"Dual View to Adversarial Attacks\". An objective function and algorithm were proposed to address this issue, providing an input perturbation that alters the network prediction. The new framework allows for incorporating geometrically motivated objective functions for classical adversarial attacks. Future extensions include investigating efficient ways to extend results to convolutional layers and exploring the potential of GCNs. Minor style issues have been addressed in the revised version."
}