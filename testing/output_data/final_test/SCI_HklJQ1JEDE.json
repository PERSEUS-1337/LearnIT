{
    "title": "HklJQ1JEDE",
    "content": "Regularization normalization is proposed as a reparameterization of neural network activation, considering statistical regularity in the implicit space. It involves a model selection process constrained by a normalizing factor, the minimum description length of the optimal universal code. An incremental method for computing this code, called normalized maximum likelihood, is introduced. This method shows flexibility in incorporating data prior and can be integrated into batch normalization and layer normalization. Initial results indicate its superiority in handling limited and imbalanced data from a non-stationary distribution in computer vision tasks. The Minimum Description Length (MDL) principle is proposed for unsupervised learning algorithms, treating neural networks as systems communicating input data. It aims to maximize regularity extraction for optimal data compression, prediction, and communication. Further research is needed to explore different scenarios and variants of this biologically plausible normalization in various real-world settings, including reinforcement learning with sparse and non-uniform rewards. Neural network training can be seen as an optimization process in a communication system, with inputs at each layer described in a low-dimensional constraint space. The constraint space includes input-vector, hidden-vector, and implicit spaces, representing underlying variability dimensions. For example, images of the same object can be represented in a 2D implicit space with orientation and size dimensions. Constraints on the implicit space help minimize the system's description length. Neural network training is like an optimization process in a communication system with constraints on implicit spaces to minimize system description length. High-level brain areas send feedback to lower-level areas to select relevant information, similar to a communication system. Neural coding during hierarchical processing involves adaptation in vision neuroscience. Relevant information with higher \"information\" depends on statistical regularity, matching neuron firing behavior. In this paper, the concept of minimum description length (MDL) is introduced to relate the activation of neurons to the code length of the model. The code length assigned to a neuron or neuronal population is shorter for regular inputs and longer for rare inputs. The implicit space concept from neuroscience is extended to neural network optimization in both supervised and unsupervised settings. Each layer of neural networks is formulated as a module selection process during training. In this paper, the concept of minimum description length (MDL) is introduced to relate the activation of neurons to the code length of the model. Each layer of neural networks is formulated as a module selection process during training, where the description length is computed at the unit of each layer and the optimization objective includes minimizing the description length as part of the normalization procedure. The minimum description length in each layer is computed to reassign weights based on statistical regularities using an optimal universal code obtained from batch input distribution. In section 2, the problem setting in neural network training is framed as a layer-specific model selection process under the MDL principle. The proposed regularity normalization (RN) method is introduced, along with variants like regularity batch normalization (RBN) and regularity layer normalization (RLN). Additionally, saliency normalization (SN) is included as a top-down attention mechanism. Preliminary results on the imbalanced MNIST dataset show the advantages of this approach over existing normalization methods. The paper concludes by pointing out future research directions. The research introduces regularity normalization methods in neural network training under the MDL principle. It discusses variants like regularity batch normalization and regularity layer normalization, along with saliency normalization as a top-down attention mechanism. The approach shows advantages over existing normalization methods on the imbalanced MNIST dataset. Future research directions are also highlighted. In the next step of the research, a universal code is defined for efficient data compression and maximum likelihood estimation. The solution involves a universal code, P(x), defined for a model class \u0398. The normalized maximum likelihood probability minimizes worst-case regret, with the minimax optimal solution given by a specific formula. Model selection in neural networks is illustrated in Figure 2. Model selection in neural networks involves choosing the optimal model from a model class \u0398 i for each layer of the neural network. The normalized maximum likelihood is computed by selecting the \"optimal\" model with the shortest code length given the data. This process is formulated as a partially observable problem in the context of a feedforward neural network example. Model selection in neural networks involves choosing the optimal model from a model class \u0398 i for each layer of the neural network. The normalized maximum likelihood is computed by selecting the \"optimal\" model with the shortest code length given the data. This process is formulated as a partially observable problem in the context of a feedforward neural network example. Regularity normalization is outlined in Algorithm 1 for updating the distribution of optimized parameters for sequential data samples. Regularity normalization in neural networks involves updating parameters after each batch using Algorithm 1. The normalization factor is computed based on the log sum of two values, stabilized with the log-sum-exp trick. The NML distribution can include a data prior function, s(x), which can emphasize certain inputs or address imbalanced data issues. Regularity normalization in neural networks involves updating parameters after each batch using Algorithm 1. The procedure can be more strategic with additional information, such as addressing imbalanced data problems or focusing on specific features of the input. Saliency normalization (SN) is formulated to incorporate this additional functionality, where P N M L is computed with a pre-specified s(x). The normalization is computed elementwise in the current setup, considering the implicit space of the model parameters to be onedimensional. Regularization normalization in neural networks can be multidimensional to enhance method expressibility. Regularity layer normalization (RLN) and regularity batch normalization (RBN) can inherit advantages from BN and LN. A biologically plausible normalization method considers activation distribution regularity in the implicit space, adjusting activation weights based on frequency. Inspired by MDL principle, this method aims to optimize neural code adaptation. The concept of neural network training as a model selection problem is introduced based on the MDL principle. An optimal universal code length is computed using normalized maximum likelihood in an incremental manner. The implementation can be easily integrated with batch normalization and layer normalization methods. Saliency normalization is proposed to enhance representation learning with top-down attention and data prior. The method focuses on maintaining low model complexity and short universal code length through incremental updates of normalized maximum likelihood. Initial results show that the approach outperforms existing normalization methods, especially in scenarios with imbalanced or limited data. The research explores regularity-based normalization methods for imbalanced or limited data scenarios. Variants of regularity normalization are being experimented with, along with top-down attention mechanisms. The method is being applied to convolutional and recurrent neural networks, as well as state-of-the-art architectures in various datasets and reinforcement learning settings. Test errors for different data scenarios are being analyzed in the study. In a study on regularity-based normalization methods for imbalanced data, seven methods were tested on an imbalanced MNIST benchmark. These methods included batch normalization, layer normalization, weight normalization, regularity normalization, saliency normalization, regularity layer normalization, and a combined approach. Regularity normalization was expected to better adapt to the data distribution, particularly in non-stationary and highly imbalanced scenarios. In a study on regularity-based normalization methods for imbalanced data, seven methods were tested on an imbalanced MNIST benchmark. The methods aimed to tackle the imbalanced data issue by adjusting the activation of rare and dominant sample features. Changes in the context distribution were simulated by randomly selecting classes for training. The study focused on the short-term sensitivity of normalization methods on neural network training. In a study on regularity-based normalization methods for imbalanced data, seven methods were tested on an imbalanced MNIST benchmark. The data sets were shuffled into training, validation, and testing sets with different distributions. Stochastic gradient descent was used with specific parameters. Different scenarios were defined based on the imbalance level in the data, such as fully balanced, rare minority, highly imbalanced, and dominant oligarchy scenarios. These scenarios are common in real-life situations. In a study on regularity-based normalization methods for imbalanced data, different scenarios were tested, including rare minority and highly imbalanced scenarios. The proposed regularity-based method performed the best in these scenarios, allocating learning resources to rare and out-of-normal-range cases successfully. In the study on regularity-based normalization methods for imbalanced data, LN performs the best in the \"dominant oligarchy\" scenario, while LN+RN also shows good performance. LN captures features of rare classes well compared to other methods. Hybrid methods like RLN and LN+RN excel in imbalanced scenarios, suggesting combining normalization methods is advantageous. Further analysis is needed to understand long-term behavior. Batch normalization (BN) and layer normalization (LN) perform global normalization in different ways, with BN normalizing along the batch dimension and LN normalizing over all neurons in a layer. Weight normalization (WN) applies normalization over incoming weights. These normalization techniques show promise in learning from extreme regularities and stabilizing hidden state dynamics in recurrent networks. Our proposed method introduces description length to quantify neural network simplicity and optimize communication of weights. It considers implicit spaces for hidden units to minimize model cost under the Minimum Description Length (MDL) principle. This approach involves reparameterization at both the layer and batch dimensions, with variants like regularity batch normalization (RBN) and regularity layer normalization (RLN). The proposed method introduces description length to quantify neural network simplicity and optimize weight communication. It considers implicit spaces for hidden units to minimize model cost under the MDL principle, using reparameterization at layer and batch dimensions. Unlike previous approaches, it directly computes the minimum description length with an optimal universal code obtained incrementally."
}