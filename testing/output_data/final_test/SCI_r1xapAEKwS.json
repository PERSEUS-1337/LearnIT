{
    "title": "r1xapAEKwS",
    "content": "In probabilistic classification, a sparse discriminative Gaussian mixture (SDGM) model is proposed, trained using sparse Bayesian learning to automatically determine the number of components. This model can be embedded into neural networks and prevents overfitting by obtaining sparsity, outperforming fully connected layers in certain cases. In supervised classification, probabilistic classification assigns class labels by estimating posterior probabilities. Discriminative and generative models optimize different distributions, but are related. Generative models can derive corresponding discriminative models. The discriminative models corresponding to the Gaussian mixture model (GMM) offer more flexible fitting capability than the generative GMM. They have been successfully applied in fields like speech recognition. The challenge in GMM models is determining the number of components M, traditionally done using Akaike's information criterion and Bayesian information criterion, which are computationally intensive. In this paper, a novel GMM called sparse discriminative Gaussian mixture (SDGM) is proposed. It combines sparsity and discriminability, trained using sparse Bayesian learning to improve generalization capability and automatically determine the number of components. The SDGM can be integrated into neural networks for end-to-end training, offering a unique approach not seen in existing GMM models. The study introduces a novel sparse classifier, the Sparse Discriminative Gaussian Mixture (SDGM), which combines sparsity and discriminability in a Gaussian mixture model. It automatically determines the number of components and can be integrated into neural networks for improved performance in tasks like image recognition. The Sparse Discriminative Gaussian Mixture (SDGM) is a classifier that takes a continuous variable as input and outputs posterior probabilities for each class. It acquires a sparse structure through sparse Bayesian learning, removing redundant components while maintaining discriminability. Training involves removing unnecessary components from the model, with posterior probabilities calculated based on the number of components and mixture weights. The Sparse Discriminative Gaussian Mixture (SDGM) classifier uses weight vectors to represent Gaussian components, with weight updating and convergence criteria. Posterior probabilities are calculated based on Gaussian distribution, leading to sparse structure and discriminability. The Sparse Discriminative Gaussian Mixture (SDGM) classifier calculates posterior probabilities using Gaussian distribution with weight vectors representing components. The algorithm optimizes weights for maximum a posteriori solution and introduces binary variables for sample classification. The Sparse Discriminative Gaussian Mixture (SDGM) classifier uses binary variables for sample classification and optimizes weights for maximum a posteriori solution. It employs Gaussian distributions with zero mean for weight optimization and updates precision parameters during learning. The joint probability of weights is represented using vectors w and \u03b1, with a sparse solution obtained by optimizing \u03b1. The expectation of the log-likelihood function over z is defined using training data matrix X and matrix T. The training data matrix X contains x T n in the n-th row. The variable r ncm in the right-hand side corresponds to P (m|c, x n ) and can be calculated as r ncm = P (c, m|x n )/P (c|x n ). The posterior probability of the weight vector w is described as follows: An optimal w is obtained as the point where (10) is maximized. The denominator of the right-hand side in (10) is called the evidence term, and we maximize it with respect to \u03b1. However, this maximization problem cannot be solved analytically; therefore we introduce the Laplace approximation described as the following procedure. With \u03b1 fixed, we obtain the mode of the posterior distribution of w. The solution is given by the point where the following equation is maximized: where A = diag \u03b1 cmh . We obtain the mode of (11) via Newton's method. The gradient and Hessian required for this estimation can be calculated as follows: (13) Each element of \u2207J and \u2207\u2207J is calculated as follows: where \u03b4 cc mm is a variable that takes 1 if The posterior distribution of w is approximated by a Gaussian distribution with mean \u0175 and covariance matrix \u039b. The updating rule for the evidence term is derived by calculating its derivative with respect to \u03b1 cmh. The mixture weight \u03c0 cm can be estimated using r ncm, and a sparse solution is obtained by alternately updating hyper-parameters and estimating the posterior distribution of w using the Laplace approximation. The {c, m}-th component is eliminated if \u03c0 cm becomes 0 or all the weights w cmh become 0. The characteristics of the SDGM were evaluated through classification experiments using synthetic data with two classes sampled from a Gaussian mixture model. Error rates, number of components, nonzero weights, and weight reduction ratio were calculated for training and test data. Changes in learned class boundaries were observed based on the number of initial components. The SDGM was evaluated through classification experiments using synthetic data with two classes sampled from a Gaussian mixture model. The decision boundary becomes more accurate with an increase in the number of components. The model learns as a discriminative model, obtaining appropriate decision boundaries even with fewer components. Evaluation results show recognition error rates, number of components, nonzero weights, and weight reduction ratio. The recognition error rates for training and test data decrease with an increase in the number of initial components. The SDGM prevents overfitting by using sparse Bayesian learning. The number of components after training corresponds to the initial components until eight. The SDGM can reduce unnecessary components by decreasing the number of components after training. The complexity of the trained model depends on the number of initial components, with a higher number leading to more weight reduction. The weight reduction ratio is greater than 99% in all cases. The SDGM can prevent overfitting by achieving high sparsity and reducing unnecessary components. A classification experiment was conducted using benchmark datasets including Ripley's synthetic data and four datasets from R\u00e4tsch et al. (2001) - Banana, Waveform, Titanic, and Breast Cancer. These datasets varied in size and dimensionality, with experiments repeated 100 times. The study conducted experiments on benchmark datasets to compare the performance of the SDGM with sparse learning against SVM and RVM classifiers. Results showed that the SDGM achieved similar or better accuracy in recognition error rates and sparsity compared to SVM and RVM on average. The SDGM, a Gaussian mixture model, showed comparable or better accuracy than SVM and RVM. It demonstrated sparsity and generalization capability, with a small number of nonzero weights due to sparse Bayesian learning. The SDGM was embedded into a deep neural network, functioning similarly to the softmax function for calculating posterior probabilities. The SDGM, a Gaussian mixture model, was compared to the softmax function in image classification experiments using a CNN. The experiment used the MNIST dataset with 60,000 training images and 10,000 testing images. The study utilized CNNs with different output dimensions for image classification on Fashion-MNIST and CIFAR-10 datasets. Fashion-MNIST consists of 10 classes of fashion images, while CIFAR-10 has 10 classes of color images. DenseNet was used for CIFAR-10 with specific parameters. DenseNet was trained with specific parameters for image classification on the CIFAR-10 dataset. The network utilized a batch size of 64, 100 epochs, and a learning rate of 0.01. Different feature embeddings were observed when using softmax versus SDGM, with SDGM showing distinct class distributions with margins. Recognition error rates on each dataset were also presented in Table 2. SDGM outperformed softmax in recognition error rates on various datasets by creating margins between classes through a Gaussian shape. The study combines discriminative model, Gaussian mixture model, and Sparse Bayesian learning, expanding knowledge in these areas. SDGM can be seen as an extended RVM using a GMM, enhancing classification capabilities. In this paper, a sparse classifier named SDGM is proposed based on a GMM. Sparse Bayesian learning has been applied in various fields, including EEG classification. Sparse methods offer benefits such as improved generalization, memory reduction, and interpretability. Previous attempts have adapted sparse learning to deep NNs, such as spatially-sparse convolutional neural networks. The SDGM is a sparse classifier based on a GMM trained with sparse Bayesian learning. It automatically determines the number of components and can be embedded into NNs. Experimental results show that SDGM outperforms conventional sparse classifiers and fully connected layers in deep NNs. One limitation is that sparse Bayesian learning is only applied when SDGM is trained independently. In future work, a sparse learning algorithm will be developed for the entire deep neural network structure, enhancing CNN's classification capabilities for larger datasets."
}