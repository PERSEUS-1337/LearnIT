{
    "title": "rydeCEhs-",
    "content": "Designing deep neural network architectures requires expert knowledge and substantial computation time. A technique called SMASH accelerates architecture selection by using an auxiliary HyperNet to generate weights for a main model based on its architecture. This allows for efficient searching over a wide range of architectures with just one training run. SMASH is validated on various datasets and achieves competitive performance with hand-designed networks. The cost of engineering and validation for deep neural network architectures is high due to intricate design patterns. A proposed technique called SMASH uses an auxiliary HyperNet to generate weights for a main model, allowing for efficient exploration of different architectures with just one training run. The SMASH technique uses a HyperNet to generate weights for different architectures, allowing for efficient exploration with just one training run. It achieves competitive performance on various datasets like CIFAR-10 and Imagenet32x32 BID6. Methods for optimizing hyperparameters include random search and Bayesian Optimization (BO), treating model performance as a black box. Bandit-based methods provide efficient exploration of hyperparameter space with an adaptive early-stopping strategy. Evolutionary techniques offer a flexible approach for discovering models but struggle to scale to deep neural nets. Reinforcement learning methods have been used to train an agent to generate network definitions using policy gradients. These methods start from trivial architectures and discover models that achieve high performance. Our method replaces random weights with weights generated through HyperNets, allowing for efficient exploration of architectures. Our method uses HyperNets to dynamically adapt weights for a wide range of model configurations, focusing on connectivity patterns, depth, and width. It does not address hyperparameters like regularization or learning rate schedule. Unlike evolutionary or RL methods, we explore a pre-defined design space and cannot discover entirely new structures independently. Our method dynamically generates weights for different model configurations, focusing on connectivity patterns. Unlike evolutionary or RL methods, it explores a pre-defined design space and does not discover entirely new structures independently. Stochastic regularization techniques like Dropout or Convolutional neural fabrics resemble our method but our weights are dynamically generated, not fixed. Our method in SMASH aims to rank neural network configurations based on validation performance using weights generated by an auxiliary network. We train the system end-to-end by sampling architectures, generating weights with a HyperNet, and evaluating performance on a validation set to select the best architecture. SMASH comprises two core components: a novel method for sampling architectures using a \"memory-bank\" view of feed-forward networks and a HyperNet that generates weights for a given architecture. The HyperNet learns to map directly from binary architecture encoding to weight space, aiming to correlate validation error with performance using normally trained weights. The SMASH network utilizes a \"memory-bank\" view of feed-forward networks to explore various architectures with variable depth, connectivity patterns, and layer sizes. This approach allows for flexible definition of architectures encoded into a conditioning vector for the HyperNet. The SMASH network utilizes a memory-bank view for exploring different network architectures. Each layer reads from and writes to memory banks, with different architectures like DenseNet and FractalNet having unique read-write patterns. The base network structure consists of blocks with memory banks at varying spatial resolutions, using downsampling via 1x1 convolution and average pooling. The SMASH network utilizes a memory-bank view for exploring different network architectures, with convolution and fully-connected output layers freely learned. Different read-write patterns and ops are randomly selected within each block, with concatenation of read tensors and addition to write tensors in banks. Each op includes a 1x1 convolution followed by convolutions interleaved with nonlinearities. The SMASH network uses a memory-bank view to explore network architectures, with randomly selected ops and read-write patterns within each block. Each op consists of a 1x1 convolution followed by convolutions interleaved with nonlinearities. The weights for the 1x1 convolution are generated by the HyperNet, while other convolutions are learned parameters. Variable depth is ensured by learning a single set of 4 convolutions for each block, shared across all ops. Transition convolutions and output layers adjust based on the number of memory banks. In designing the SMASH network, the majority of the network's capacity is placed in the HyperNet to minimize static learned parameters. BatchNorm is used at downsample layers and before the output layer. WeightNorm was experimented with but found to be unstable, so a simplified version is employed where each generated 1x1 filter is divided by its Euclidean norm. This approach works well for SMASH with only a minor drop in accuracy. A HyperNet BID11 is a neural net used to parameterize the weights of another network, the main network, in fixed-architecture networks. The weights W are generated based on a tensor encoding of the main network architecture c in a Dynamic HyperNet. The goal is to learn a mapping W = H(c) that is close to the optimal W for any given c, allowing for ranking based on validation error using HyperNet-generated weights. Our HyperNet enables sampling of architectures with variable topologies and interpretable dimensions. The output tensor W varies with the input c, which is a 4D tensor in Batch x Channel x Height x Width format. Each slice of W corresponds to a subset of c, with information embedded in the channel dimension. The HyperNet employs a channel-based weight-compression scheme to reduce the size of the output tensor c while maintaining its representational power proportional to that of the main networks. This allows for sampling of architectures with variable topologies and interpretable dimensions. The HyperNet uses a weight-compression scheme to reduce the size of the output tensor c while maintaining its representational power. The scheme involves adjusting the spatial extent of c to a fraction of the size of W, reshaping the tensor, and choosing a depth compression hyperparameter. Details of this encoding strategy are available in Appendix B. SMASH is applied to various datasets for benchmarking and investigating the behavior of SMASH networks, particularly focusing on the correlation between the validation error of a network using SMASH-generated weights and a normally trained network. The publicly available code in PyTorch BID22 leverages dynamic graphs to define sampled networks in line with the memory-bank view. Hyperparameter details are omitted for brevity, with full details in the appendices. A SMASH network is trained for 300 epochs on CIFAR-100, followed by the evaluation of 250 random architectures on a validation set. The architectures are sorted based on their SMASH score. For SMASHv1 architectures, a fixed memory bank size is used with a single 3x3 conv in the main body. The architecture also includes a fixed bottleneck ratio of 4 and variable elements like read-write pattern and dilation factor. A correlation between SMASH score and validation performance is observed, indicating the potential for rapid architecture comparison. The correlation between SMASH score and validation performance suggests potential for rapid architecture comparison. Further investigation is needed to determine the key variables affecting this correlation, including the capacity of the HyperNet and the ratio of HyperNet-generated weights to freely learned weights. The study explores the impact of the ratio of HyperNet-generated weights to freely learned weights on architecture adaptability. Different HyperNet configurations and g hyperparameter values are tested to determine the optimal balance for dynamic weight generation. Multiple experiments are conducted with varying ratios and configurations, followed by training and evaluation of SMASH networks to identify the most effective architectures. The study evaluates the correlation between SMASH score and validation performance for different HyperNet configurations and g hyperparameter values. Results show that the strength of correlation increases with increased HyperNet capacity, with g values of 4, 8, and 16 corresponding to different weight ratios. The correlation between HyperNet capacity and strength varies with different g values. For g=4, correlation increases with capacity up to 12M parameters, but breaks down for larger architectures. For g=8, correlation is weaker than g=4 but varies little with architecture changes. For g=16, there is a complete breakdown of correlation, indicating too much capacity in a single set. The HyperNet capacity correlation varies with different g values. For g=16, there is a breakdown of correlation, suggesting too much capacity in a single set. The HyperNet has learned a mapping from architecture to weights, as validated by SMASH performance. Preliminary tests on guiding architecture search through gradient descent-like procedure did not improve SMASH scores. Models with weights learned on large datasets may outperform those trained from scratch on smaller datasets, suggesting similar behavior for architectures. Testing was done on STL-10 dataset. In experiments on the STL-10 dataset, the best-found architecture from CIFAR-100 outperformed the best-found architecture from STL-10, achieving 17.54% and 20.275% error rates, respectively. This contrasts with baseline WRN models achieving 15.43% and 16.06% errors. The best architecture found on CIFAR-100 outperformed the one on STL-10, indicating that larger training sets benefit architecture search more than domain specificity. The CIFAR-100 architecture achieved 93.28% accuracy on ModelNet10, compared to 93.61% for a hand-designed model on ModelNet40. Inception-ResNet BID4 with 18M parameters trained on ModelNet40 dataset. SMASH on CIFAR-10 and 100, expanding search space to include variable filter sizes, groups, and op structure. Best SMASHv2 networks' final test performance on CIFAR-10 and 100 in TAB2. SMASHv2 architecture from CIFAR-100 trained on STL-10 and ImageNet32x32 BID6. Comparison with Wide ResNet baselines in TAB3. SMASHv2 nets with 16M parameters achieve final test errors. Our SMASHv2 networks with 16M parameters achieve test errors of 20.60% on CIFAR-100 and 4.03% on CIFAR-10, outperforming Large-Scale Evolution despite requiring less time and compute. The method lags behind Neural Architecture Search but does not need postprocessing through grid search. This work opens up future research paths with its simplistic elements. The SMASH method can be improved by using Bayesian Optimization or HyperBand to guide sampling for better performance. Adding a parallel worker for constant validation evaluation and changing the optimization objective can help maximize performance while minimizing computational costs. Combining this technique with RL methods and using policy gradient for sampling guidance are also suggested. Another option is to use HyperNet-generated weights for initialization. Our code supports using HyperNet-generated weights to initialize the network and accelerate training. Architecture exploration involves variable layer sizes, skip connections, and potential variations in activation, operation order, convolution numbers, and block types. Network-wide design patterns can be considered, along with varying elements generated by HyperNet. The memory-bank view offers new possibilities for network design. The memory-bank view in network design allows for dynamic read and write operations using a learned softmax attention mechanism. This approach does not use memory in the traditional sense but can easily incorporate memory augmentation. Different operations like adding to, overwriting, or multiplying tensors in memory banks can be explored. The technique accelerates architecture selection by learning a model over network parameters conditioned on the network's parametric form. Flexible network connectivity patterns are introduced for architecture exploration. Our method introduces a flexible scheme for defining network connectivity patterns and generating weights for variable architectures. Results show a correlation between performance using suboptimal weights from an auxiliary model and fully-trained weights. The SMASHv1 network has memory banks with N = 6 channels each, a maximum of 240 memory banks per block, and a depth compression ratio of D = 3. Each layer's units are sampled between 6 and 42, and dilation factor between 1 and 3. The HyperNet-generated 1x1 convolution output is always 4 times the number of output units. The main network has a maximum budget of 16M parameters, with rarely more than 5M parameters sampled. SMASHv2 networks have variable memory bank sizes constrained to multiples of N = 8 up to Nmax = 64. Filter sizes are sampled from [3, 5, 7] with dilation values ensuring a max spatial extent of 9. Convolutional groups are sampled as factors of the base N value [1, 2, 4, 8]. Hand-designed priors are placed on op configuration preferences, favoring all four convolutions active. The HyperNet is a DenseNet with specific design features, including multiple Dense Blocks with varying numbers of convolutional layers and growth rates. Leaky ReLU with a negative slope of 0.02 is used in the network. The output of the 1x1 convolution is capped at a maximum bottleneck ratio of 2. The main network's hyperparameters are constrained for compatibility with the embedding tensor layout and convolutional toolbox. Output units must be divisible by memory bank size N and less than Nmax, input units divisible by D. This reduces the embedding vector size by DN^2. The input to a standard 2D CNN is x \u2208 R B\u00d7C\u00d7H\u00d7L. The conditional embedding tensor c in a standard 2D CNN has 2M + dmax channels, representing memory banks read from and written to, as well as dilation factors for convolutions. The height dimension corresponds to units at each layer, and the length dimension to network depth in terms of input channels. The Batch dimension is kept at 1 to prevent signal propagation. The HyperNet has 4DN 2 output channels, reshaped to W \u2208 R Nmax\u00d74Nmaxn ch \u00d71\u00d71, allowing it to predict weights at a given layer based on nearby layers. The receptive field determines how far up or down the network it can look to predict parameters. The weights for the entire main network are generated in a single pass, with the HyperNet slicing W based on incoming channels and layer width. At each training step, a network architecture is sampled block-by-block with a computational budget. SMASHv1 uses memory banks with 6 channels each, constraints on incoming memory banks and output units, and a 26 layer DenseNet HyperNet without bottleneck blocks or normalizers. SMASHv2 evaluates 500 random architectures, selecting the highest scoring one for further evaluation. The evaluation process involves perturbing the architecture with random resampling, evaluating perturbations in a Markov Chain, and training the resulting network with freely learnable parameters and BatchNorm. Experimentation with SMASH generated weights for initialization was not as effective as standard strategies. The exploration of the SMASH design space is kept minimal, with brief experimentation on different settings for N and D. We experimented with different settings for N and D using a simple DenseNet architecture for the HyperNet. We used Adam BID17 for training SMASH and Nesterov Momentum for training resulting networks. Different training epochs were used for CIFAR, ModelNet10, ImageNet32x32, and STL-10 datasets. For STL-10, training is done for 300 epochs with the full set and 500 epochs with 10-fold splits. ModelNet-10 tests use 3x3x3 filters to reduce parameters to 8M. Networks are pre-activation with options for BatchNorm or WeightNorm. Initial experiments guide further investigation on SMASH correlation. The study investigates the impact of a smaller HyperNet on the correlation between SMASH score and true performance. Results show a breakdown of the correlation, with variations observed across training runs. Additionally, a high-budget SMASH network with a majority of non-generated weights is trained to explore model capacity under different conditions. The study explores the impact of a smaller HyperNet on the correlation between SMASH score and true performance. Results show a breakdown of the correlation, with variations observed across training runs. Additionally, a high-budget SMASH network with a majority of non-generated weights is trained to explore model capacity under different conditions. The validation errors achieved with SMASH-generated weights are lower than with an equivalent SMASH network with the typical ratio, but the resulting top models are not as performant. The SMASH score did not correlate with true performance in the limited correlation tests conducted, highlighting potential pitfalls in comparing architectures based on SMASH scores. The study found that a network with more fixed weights may achieve better SMASH scores, even if the resulting nets are not necessarily better."
}