{
    "title": "ryetZ20ctX",
    "content": "Neural network quantization is essential for deploying deep learning models efficiently on various hardware platforms. Conventional quantization methods are vulnerable to adversarial attacks, leading to a need for more secure approaches. A novel Defensive Quantization (DQ) method is proposed to optimize the efficiency and robustness of deep learning models by controlling the Lipschitz constant to mitigate the impact of adversarial noise during inference. Our new quantization method, Defensive Quantization (DQ), aims to defend neural networks against adversarial examples by controlling the Lipschitz constant to limit the adversarial noise impact during inference. Extensive experiments show that DQ outperforms full-precision models in robustness on CIFAR-10 and SVHN datasets, while maintaining hardware efficiency. Additionally, DQ can enhance the accuracy of quantized models without adversarial attacks. Adversarial attacks involve subtle perturbations on input images to deceive deep learning models, posing security risks in safety-critical scenarios like autonomous driving. Quantization, while typically leading to inferior adversarial robustness, has shown effectiveness in defending against adversarial examples in input image space. The effectiveness of quantization on intermediate DNN layers remains a question to be analyzed. The error amplification effect of adversarial perturbations in deep neural networks is more significant in deeper layers. Quantization can reduce errors caused by small perturbations but can amplify errors with larger perturbations, leading to inferior robustness. This phenomenon is the main reason for the decreased effectiveness of quantization in defending against adversarial attacks. Defensive Quantization (DQ) is proposed to address the inferior robustness of quantized models by turning activation quantization into a defense method that boosts adversarial robustness. Inspired by the success of image quantization, DQ aims to keep the magnitude of perturbations small to defend against attacks. Previous works have attempted to control the Lipschitz constant of the network to reduce error propagation. Defensive Quantization (DQ) aims to enhance the robustness of deep learning models by quantizing feature maps into low-bit representations while controlling the Lipschitz constant to keep noise within a small magnitude. This method reduces perturbation errors, improving security and efficiency. DQ offers three key advantages: boosting model robustness while maintaining efficiency, serving as a generic building block for adversarial defense, and providing state-of-the-art robustness when combined with other defense techniques. Our method enhances deep learning model robustness by quantizing feature maps into low-bit representations while controlling the Lipschitz constant. This reduces perturbation errors and improves security. Quantization makes inference more efficient and requires less memory, but quantized models are more vulnerable to adversarial attacks. Activation quantization using ReLU6 is computationally efficient and widely adopted. Defensive quantization with Lipschitz regularization is used in quantized convolutional networks to improve robustness against attacks. Previous work suggests binary quantized networks can enhance robustness, but randomized quantization is not practical for real deployment. The use of Tanh-based quantization in defensive methods for quantized convolutional networks is not practical for real deployment due to hardware constraints. This method leads to severe gradient masking issues during adversarial training, resulting in no improvement in robustness against black-box attacks. Previous work has not studied quantized robustness under real inference settings for both black-box and white-box scenarios. The study focuses on raising awareness about the security of deployed models by exploring adversarial attacks on neural networks. Various attack and defense methods are introduced, including Random Perturbation and Fast Gradient Sign Method (FGSM) & R+FGSM. These methods aim to find perturbations that are hardly visible to humans, with FGSM being a fast and effective attack method. The study explores adversarial attacks on neural networks, introducing methods like FGSM and R+FGSM to calculate adversarial noise. FGSM can suffer from sharp curvature near data points, leading to false directions, so R+FGSM adds a random step. BIM is a stronger variant of FGSM, applying it iteratively with a small step size. In our experiments, we used the PGD attack, a general first-order attack, to test the model's robustness. Unlike BID16, we followed BID14 and BID26 by using \u03b1 = 1 and a variable number of iterations. Several defense methods were introduced, such as BID31 which detects adversarial images by processing the input image with color depth reduction and smoothing. The robustness of neural networks against adversarial attacks was tested using various defense methods, including adversarial training and PGD training. These methods were combined with a DQ method to enhance robustness. Conventional neural network quantization was found to be more vulnerable to attacks compared to the proposed defense techniques. Experiments were conducted on VGG-16 and a Wide ResNet on CIFAR-10 dataset. The study tested neural network robustness against adversarial attacks using defense methods like adversarial training and PGD training combined with a DQ method. Experiments on VGG-16 and Wide ResNet on CIFAR-10 dataset showed that conventional quantization is vulnerable to attacks, with accuracy dropping significantly under adversarial attacks even with 5-bit quantization. The error amplification effect prevents activation quantization from defending against attacks effectively. Input image quantization is an effective defense method against adversarial attacks, but it does not work when applied to hidden layers and can even worsen the model's robustness. The study analyzed the impact of quantization on different perturbation strengths by generating adversarial samples and feeding them to a Wide ResNet model. The results showed that quantization can make the model more vulnerable to attacks, raising security concerns. The study analyzed the impact of quantization on adversarial samples fed through a Wide ResNet model. Results showed that quantization can worsen model robustness, especially with small magnitude perturbations. Error amplification effect is attributed to the increased relative perturbed distance in the network. Defensive Quantization (DQ) is proposed to defend adversarial examples for quantized models by suppressing noise amplification effects and keeping the perturbation magnitude small. This helps mitigate the increased relative perturbed distance in the network, especially with small magnitude perturbations. Defensive Quantization (DQ) aims to defend against adversarial examples in quantized models by controlling the neural network's Lipschitz constant to suppress noise amplification effects. This helps maintain robustness by ensuring that adversarial perturbations do not amplify errors, but rather reduce them. Defensive Quantization (DQ) aims to defend against adversarial examples in quantized models by controlling the neural network's Lipschitz constant to suppress noise amplification effects. Specifically, a feed-forward network composed of various functions where the Lipschitz constant of each layer needs to be kept \u2264 1 to prevent exponential growth and perturbation amplification. Regularization terms are used to maintain a small Lipschitz constant for the entire network. Defensive Quantization (DQ) aims to control the Lipschitz constant of neural networks in quantized models to prevent noise amplification effects. The Lipschitz constant of each layer should be \u2264 1 to avoid perturbation amplification. Regularization terms are utilized to maintain a small Lipschitz constant for the entire network. Linear layers with weight W are considered under the || \u00b7 || 2 norm, where the Lipschitz constant is defined as the spectral norm of W. By keeping the weight matrix row orthogonal, the singular values are naturally equal to 1, satisfying non-expansive requirements. The optimization objective involves a regularization term to ensure W T W \u2248 I, where I is the identity matrix. For convolutional layers, a similar regularization approach is applied to maintain Lipschitz constraints. Defensive Quantization (DQ) focuses on controlling the Lipschitz constant of neural networks in quantized models to prevent noise amplification effects. The regularization term suppresses noise amplification by maintaining a small Lipschitz constant for the entire network, ensuring perturbations stay within a certain range. This method not only addresses the drop in robustness caused by quantization but also enhances robustness by using quantization as a defense mechanism. Experimental results show the advantages of Defensive Quantization in increasing robustness. Defensive Quantization (DQ) can maintain model robustness with low-bit quantization, is a versatile defense method that can be combined with other techniques, and improves accuracy on clean images. Experiments with Wide ResNet BID32 on CIFAR-10 dataset using ReLU6 activation quantization show increased adversarial robustness with more bits. Defensive Quantization (DQ) can maintain model robustness with low-bit quantization. The robustness of quantized models increases with the number of bits and with \u03b2. Lipschitz regularization improves model robustness, and combining it with quantization further enhances robustness. Conventional quantized models are less robust compared to models with Lipschitz regularization. Combining Lipschitz regularization with quantization enhances model robustness. The study highlights the importance of checking model robustness under black-box attacks. Results show that the proposed method improves black-box robustness compared to vanilla quantization. The approach is consistent for both white-box and black-box settings, avoiding gradient masking. Experiments were conducted on the Street View House Number dataset. Experiments were conducted on the Street View House Number dataset (SVHN) and CIFAR-10 dataset using Wide ResNet models. The models were trained with specific parameters and optimization techniques for each dataset. The model is trained for 160 epochs on the dataset with specific parameters. DQ with bit=1 and \u03b2 = 2e-3 is used for robustness. Other defense methods are combined with DQ for further robustness. Feature Squeezing BID31 is used with 5 bit for color reduction and a 2 \u00d7 2 median filter. Adversarial R+FGSM training is used to address gradient masking issues. Random sampling is done during training to avoid over-fitting. Adversarial PGD training is also conducted. Results are presented in TAB0. Our DQ method improves model's robustness in various settings, especially against adversarial attacks like R+FGSM. Adversarial training performs best overall, with PGD attack being the strongest. DQ also enhances black-box robustness without gradient masking, proving effective against both white-box and black-box attacks. Additionally, it can improve quantized model accuracy on clean images, serving as a beneficial substitute for normal quantization. DQ method improves model's robustness against adversarial attacks like R+FGSM and enhances black-box robustness. It also improves quantized model accuracy on clean images, serving as a beneficial substitute for normal quantization procedures. The method adds a regularization to shrink the dynamic range of activation, making optimization easier. Experiments with ResNet and CIFAR-10 show that DQ training outperforms vanilla quantization in accuracy. The Defensive Quantization (DQ) module enhances the security of quantized neural networks deployed on various platforms. It bridges efficiency and robustness in deep learning by defending against adversarial attacks while maintaining efficiency. Experimental results confirm the effectiveness of the new quantization method, allowing safe deployment of deep learning models on mobile devices. The white-box results show that Tanh-based quantization with PGD training improves accuracy but decreases black-box robustness. The black-box attack success rate is even lower than white-box, indicating a severe gradient masking problem. ReLU6 quantization does not show gradient masking but also does not improve robustness. Simple quantization leads to inferior robustness, and FGSM adversarial training is shown to be significant. FGSM adversarial training leads to gradient masking issues, while R+FGSM resolves this problem. Testing on CIFAR-10 dataset shows higher white-box robustness for FGSM, but lower black-box robustness compared to R+FGSM. Increasing \u03b2 in R+FGSM training improves adversarial robustness. In Section 5.1, the optimal \u03b2 for the experiment was found to be 0.002 for a 4-bit quantized Wide ResNet model. Adversarial accuracy initially increases with \u03b2, peaks at \u03b2 = 0.002, then decreases. Clean accuracy remains stable. The effect of noise amplification is suppressed with stronger regularization initially, but too strong regularization hinders training later on. The experiment used \u03b2 = 0.002 unless specified. Clean and adversarial samples from CIFAR-10 test set were visualized, comparing predictions of full precision model, vanilla 4-bit quantized model, and 4-bit Defensive Quantization model. The NVIDIA Turing Architecture now supports Tensor Cores. The VQ model shows worse robustness compared to FP and DQ models. DQ model outperforms FP model in defending attacks and has better confidence for true labels. Even when all models fail, DQ model has the lowest confidence for misclassified classes. Visualization of adversarial samples and predictions are provided for FP, VQ, and DQ models."
}