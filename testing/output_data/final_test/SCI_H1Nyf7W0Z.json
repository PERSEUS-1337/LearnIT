{
    "title": "H1Nyf7W0Z",
    "content": "Neural sequence generation is commonly approached using maximum-likelihood (ML) estimation or reinforcement learning (RL), each with its own shortcomings. To address these issues, an objective function for sequence generation using \u03b1-divergence is proposed, integrating the strengths of both ML and RL. Experimental results on machine translation tasks demonstrate the effectiveness of this approach. Optimizing the proposed objective function with increasing \u03b1 improves sequence generation performance in machine translation tasks compared to ML-based methods. Neural sequence models have been successfully applied in various machine learning tasks like caption generation, conversation, and speech recognition. Developing more effective learning algorithms is beneficial, with popular objective functions including maximum-likelihood (ML) and reinforcement learning (RL). However, optimizing the ML objective does not necessarily optimize the evaluation metric, highlighting limitations in training/testing discrepancy and sample inefficiency. Optimizing the proposed objective function with increasing \u03b1 improves sequence generation performance in machine translation tasks compared to ML-based methods. In machine translation, maximizing likelihood is different from optimizing the BLEU score, a popular metric. During training, ground-truth tokens are used, but during testing, predicted tokens are used. RL-based approaches do not suffer from training/testing discrepancy but face sample inefficiency. RL methods require pre-training via ML-based methods due to sampling distribution dependencies. The proposed \u03b1-DM objective function integrates ML and RL-based objective functions for neural sequence generation. It generalizes both functions as special cases and combines their gradients for optimization. By using importance sampling, it improves efficiency compared to on-policy RL sampling. Experimental results show that \u03b1-DM outperforms ML baseline and RAML methods in machine translation tasks. The proposed \u03b1-DM objective function, utilizing \u03b1-divergence, outperforms the ML baseline and RAML method in machine translation tasks. It is shown to be a generalization of both ML and RL-based objective functions, with performance improving as \u03b1 increases. In this section, ML-based and RL-based objective functions are introduced for learning neural sequence models. ML minimizes negative log-likelihood to train a neural sequence model, but does not directly optimize final performance measure, leading to training/testing discrepancy. The training/testing discrepancy in neural sequence models arises from objective score and sampling distribution discrepancies. Reinforcement learning in sequence generation tasks aims to minimize the negative total expected rewards for optimizing final performance measure. Reinforcement Learning (RL) is used to solve sequence prediction tasks by optimizing a reward-based objective function. RL addresses issues such as sample inefficiency and discrepancies in objective scores and sampling distributions. BID20 and BID3 optimize the objective function using policy gradient methods. In RL, a sequence prediction task is treated as selecting the next token based on an action trajectory, with the next token selection corresponding to the next action selection in RL. Reinforcement Learning (RL) may suffer from sample inefficiency, hindering the generation of high reward samples, especially in the early learning stage. This is due to the low predictive ability of the model distribution, resulting in sparse rewards and a vast action space. Previous studies have used pre-training with Machine Learning to address this issue. To prevent RL from being overly greedy and deterministic, some studies have used an entropy-regularized version of the policy gradient objective function. RAML replaces the sampling distribution of ML with a reward-based distribution to incorporate reward information into the objective function. Despite attempts to solve various problems simultaneously, a technical barrier exists due to a trade-off between sampling distribution discrepancy and sample inefficiency. The proposed method aims to control the trade-off of sampling distributions by combining them using \u03b1-divergence DA (p q). This method bridges ML-and RL-based objective functions by defining the \u03b1-DM objective function as the \u03b1-divergence between p \u03b8 and q (\u03c4). The \u03b1-DM objective function is a mixture of ML-and RL-based objective functions, with a gradient that can be used to train neural networks via \u03b1-divergence minimization. It converges to the gradient of entropy regularized RL or RAML by adjusting the \u03b1 parameter. See Appendix C for a summary of objective functions, gradients, and their connections. The \u03b1-DM objective function L (\u03b1,\u03c4 ) and the desired RL-based objective function L * (\u03c4 ) decrease linearly as \u03b1 increases to 1 with respect to sup-norm. This analysis motivates using a larger \u03b1 for the \u03b1-DM objective function. Proposition 1 and Proposition 2 provide conditions and results for \u03b1 \u2208 (0, 1) in relation to the probability distributions p and q. In this paper, an optimization strategy similar to RAML is employed, where target sentences are sampled from a data augmentation distribution q 0 (y|x) and the gradient is estimated using importance sampling. The parameter is updated using an importance sampling estimator, with samples from the proposal distribution q 0 (y|x) and importance weights proportional to p (\u03b1,\u03c4 ) \u03b8 (y i |x i ). The importance weight in \u03b1-DM is proportional to p(\u03b1,\u03c4)\u03b8(yi|xi). The difference between RAML and \u03b1-DM lies in this weight, which is normalized in each minibatch for consistent hyperparameters. This leads to a weighted IS estimator with lower variance. Normalizing q(\u03c4)(yi|xi) and p\u03b8(yi|xi) in each minibatch yields good results. Experimental evaluation on neural machine translation tasks shows the effectiveness of \u03b1-DM compared to ML and RAML, using BLEU scores on the IWSLT'14 German-English corpus. Training the same encoder-decoder model for each objective function allows for impact assessment. The model BID16 is used for each objective function with shared hyperparameters. Data augmentation is employed for RAML and \u03b1-DM, generating samples from a distribution. The encoder-decoder model is trained with specific architecture and learning rate optimization. Hyperparameter \u03c4 is crucial for maximizing BLEU performance. The impact of hyperparameter \u03b1 on neural sequence models trained with \u03b1-DM is investigated by varying \u03b1 from 0.0 to 0.9 and linearly annealing it during training. The models are evaluated using BLEU scores on the test dataset, with beam width set to 1 or 10. Results show improved performance compared to smaller \u03b1 values. The study explores the effect of hyperparameter \u03b1 on neural sequence models trained with \u03b1-DM. Results indicate that models perform better with \u03b1 around 0.5, outperforming smaller or larger \u03b1 values. Annealed versions of \u03b1-DM improve performance with wider \u03b1 range. \u03b1-DM with \u03b1 \u2208 {0.3, 0.4, 0.5} shows higher BLEU scores compared to ML and RAML baselines. Specifically, \u03b1-DM with \u03b1 = 0.5 without pre-training is comparable to on-policy RL-based methods. The study found that \u03b1-DM with \u03b1 around 0.5 performs better than smaller or larger values, outperforming ML and RAML baselines. Stochastic gradient descent with decaying learning rate was used, with mini-batch size of 128 and dropout probability of 0.3. Gradients are rescaled if norms exceed 5, and unknown tokens in predicted sentences are replaced with tokens with highest attention in source sentence. The study implemented models using a fork from the PyTorch 1 version of the OpenNMT toolkit. BLEU scores were calculated for the development and test sets. Augmented data was obtained similar to the RAML framework, with tokens replaced in the vocabulary using negative Hamming distance as reward. Hamming distance for each sentence is assumed to be less than [m \u00d7 0.25], with a sample uniformly selected from 0 to [m \u00d7 0.25]. The proposal distribution q 0 (y|x) used was different from RAML, assuming a simplified discrete distribution. The study implemented models using a fork from the PyTorch 1 version of the OpenNMT toolkit. BLEU scores were calculated for the development and test sets. The proposal distribution q 0 (y|x) used was different from RAML, assuming a simplified discrete distribution. The hyperparameter \u03c4 used in this experiment was different from that of RAML, with \u03c4 = 1.0 chosen to maximize the BLEU score. The proposed \u03b1-DM approach can be considered an off-policy approach with importance sampling in reward-based neural sequence model training. The proposed method aims to reduce gradient variance by using a critic network and replacing ground-truth tokens with generated tokens in the output sequence. It utilizes off-policy gradient methods with importance sampling to optimize the evaluation metric, offering computational efficiency and learning stability during training. The proposed \u03b1-DM method aims to improve computational efficiency and learning stability in RL by collecting samples before training, leading to a more stable learning process compared to on-policy approaches. Other off-policy RL-based methods like MRT, RANDOMER, and GNMT also compute rewards before training. The proposed approach combines ML and RL techniques to reduce gradient variance and optimize the evaluation metric. The proposed \u03b1-DM method aims to improve computational efficiency and learning stability in RL by collecting samples before training. It combines ML and RL techniques to reduce gradient variance and optimize the evaluation metric. Comparing this approach with GNMT's weighted mean objective function could be a future research direction. The study introduces a new \u03b1-divergence minimization objective function for neural sequence model training, combining ML and RL techniques. It outperforms ML baseline and RAML in machine translation tasks. The proposed approach addresses sample inefficiency in reinforcement learning and may have broader applications beyond sequence generation. The text introduces a new \u03b1-divergence minimization objective function for neural sequence model training, addressing sample inefficiency in reinforcement learning. The framework is general and independent from the task, potentially useful for combining different learning approaches. The gradient of \u03b1-DM can be obtained using a log-trick, with a proposition stating conditions for the divergence. Taylor's theorem introduces \u03b1-divergence minimization objective functions for neural sequence model training, including ML, RL, RAML, EnRL, and \u03b1-DM. These functions involve q(\u03c4) and connections between objectives are established using KL or \u03b1-divergences."
}