{
    "title": "B1xtFpVtvB",
    "content": "Agents trained with reinforcement learning often struggle to generalize to changes in the environment, leading to overfitting. This study explores how deep RL agents can overfit even when trained on multiple environments simultaneously in visual navigation tasks. Regularization method combining RL with supervised learning to encourage policy invariance to observation variations, improving generalization to unseen environments. Learning control policies from high-dimensional sensory input has been gaining traction with deep reinforcement learning (DRL), enabling simultaneous learning of perception and control modules. Evaluating learned policies in different environments is crucial for understanding generalization abilities, as variations in visual aspects, physical structure, and agent's goal can impact performance. Deep reinforcement learning (DRL) agents need to generalize their control policies to different environments with varying conditions, such as lighting, obstacles, and game levels. Overfitting to training environments is a common issue, leading to poor performance in testing environments. Supervised learning algorithms offer some generalization guarantees with proper regularization, but these guarantees may be weakened. In this paper, the authors explore generalization in visual navigation control policies learned through Deep Reinforcement Learning (DRL). They investigate how these policies adapt to changes in the environment and propose a training method that combines DRL with supervised learning for better generalization. The experiments are conducted using the VizDoom platform, allowing for the customization and generation of various environment variants. The text discusses how visual navigation for mobile robots combines vision and control, with classical approaches splitting the problem into sub-tasks like map construction, localization, planning, and path following. Deep learning approaches enable the use of non-linear classifiers for adaptation to different scenarios without manual tuning. In this work, reinforcement learning algorithms coupled with deep learning are used to navigate an agent towards a goal object using only visual observations. The problem is modeled as a partially observed Markov decision process (POMDP) with finite sets of states, actions, and observations. The mass function T represents the probability of the next state given the current state and action. The observation probability mass function calculates the probability of observing a specific outcome in a state. The initial state probability mass function determines the probability of the initial state. In Deep Reinforcement Learning (DRL), a policy \u03c0 \u03b8 is used to adjust parameters \u03b8 to maximize the discounted reward. In Deep Reinforcement Learning, a policy \u03c0 \u03b8 adjusts parameters \u03b8 to maximize the discounted reward. The proximal policy optimization (PPO) algorithm is used for this purpose, showing robust performance on various tasks. The goal is to learn from a finite training set and generalize well to unseen examples from a test set by sampling POMDPs from a distribution D. In Deep Reinforcement Learning, a policy \u03c0 \u03b8 adjusts parameters \u03b8 to maximize the discounted reward using the PPO algorithm. The goal is to learn from a finite training set and generalize well to unseen examples by sampling POMDPs from a distribution. The POMDPs may differ in observation, transition, and reward distributions, requiring a common policy \u03c0 \u03b8 across them with common state, action, and observation spaces. The policy generalizes well if it attains a high value for the expectation of the discounted reward over the full distribution. The discounted generalization reward aims to maximize the expectation of the discounted reward over the full distribution of POMDPs. This reward is equivalent to the discounted reward of a larger POMDP with a state space that may not be finite. Training in synthetic environments allows for the efficient simulation of large amounts of experience in a short time span, aiding in training reinforcement learning agents. However, there is often a discrepancy between synthetic and real-world dynamics and appearances. One popular method to bridge the reality gap in training reinforcement learning agents is domain randomization, which involves randomizing aspects of the training environment. This technique has shown success in transferring grasping policies from simulated environments to the real world. However, the resulting models are perception modules, not control policies. Previous work has focused on transferring perception modules from simulation to reality, but not controllers. In a study on generalization using a new environment called CoinRun, researchers found that RL agents tend to overfit even with large training sets. Unlike previous studies, they argue that injecting stochasticity into the system does not prevent memorization. Their work focuses on generalization in navigating under partial observability, unlike fully observable environments like CoinRun or grid-world. Domain adaptation is a key aspect of their research. Domain adaptation methods have been used for simulated to real transfer, allowing models trained on a source domain to generalize to a target domain. Bousmalis et al. (2017) trained a generative model to adapt synthetic images to appear real, successfully transferring a grasping policy from simulation to the real world. Generalization also involves transferring learned skills to solve different tasks, requiring the agent to maximize different reward functions. Schaul et al. (2015) introduced universal value functions containing the goal as part of the agent's state for this purpose. In the paper, universal value functions are introduced, where the reward is based on a state-action-goal tuple instead of a state-action pair. Universal value function approximators are used to estimate V \u03b8 (s, g) and generalize for unseen state-goal pairs in a grid-world setup. Deep reinforcement learning is utilized for training control policies, mapping pixels to motor control commands from raw camera images. DRL algorithms have been applied to various navigation tasks, including goal-conditioned and mapless navigation. The paper introduces universal value functions based on state-action-goal tuples. It proposes a regularization term to improve the robustness of learned policies to variations in observations. Domain randomization is explored as a technique to train policies that generalize well to variations and noise in observations. The paper discusses navigating an agent towards a goal object with random noise in observations. It examines domain randomization in RL training to improve policy generalization. The focus is on avoiding overfitting to training environments by training on multiple POMDPs. In reinforcement learning, training on different environments may not guarantee policies that generalize well to new environments. The training objective should include a term that encourages policy generalization to address this issue. This involves making the policy invariant to changes in observations through a regularization term. In reinforcement learning, a regularization penalty term is added to the objective to encourage policy generalization. This penalty term ensures policy invariance to changes in observations by using a transformation T on observations. The transformation maintains the semantic context of the state while introducing visual variations. The nature of T is further discussed in the experiments section. In reinforcement learning, a penalty term is added to encourage policy generalization by ensuring policy invariance to observation changes using a transformation T. The penalty in Equation 1 adds a constraint on the PPO objective, similar to trust region policy optimization, to ensure monotonic improvement of the average return. This method, called invariance regularization (IR), indicates the invariance of the learned policy to observation transformations. Two ways to solve the RL problem in Equation 1 are proposed: directly optimizing the full objective by adding the penalty to the original PPO loss. The second method involves splitting the training process into two stages: training RL first and then performing supervised learning to minimize d(\u03c0(o), \u03c0(T (o))). This method combines reinforcement learning with supervised learning and is detailed in Appendix A. The next section will discuss experiments using both methods, preceded by a study on the effectiveness of domain randomization in reducing overfitting in DRL agents. Two experiments are presented: one on training RL with domain randomization and another on using the invariance regularization (IR) method proposed in Section 4.2. The text chunk discusses the use of invariance regularization (IR) method with domain randomization to improve success rates in training an actor-critic style agent on VizDoom maps with unique textures. The network architecture includes convolutional and fully connected layers for policy and value estimation. The network architecture consists of convolutional and fully connected layers for policy and value estimation. ReLUs are used as non-linear operations in all layers. The PPO objective is optimized with a binary reward function and a discount factor of 0.99. Training environments are varied by changing textures on surfaces using textures provided by VizDoom. Agents are trained on subsets of different numbers of rooms from the generated environments. The experimental setup involves training agents on subsets of rooms with different visual inputs (RGB, RGB-D, Grayscale) and testing on unseen rooms. The number of training iterations is fixed, potentially affecting generalization performance. Depth plays a significant role in generalization by providing invariance to changes in visible spectrum. The experimental setup involves training agents with different visual inputs (RGB, RGB-D, Grayscale) on subsets of rooms and testing on unseen rooms. Depth plays a significant role in generalization, as shown in Table 1 where RGB-D agents outperform RGB agents. The results are averaged over 5 seeds, indicating the superior performance of agents with depth information. Despite achieving high rewards in training, agents trained on 100 and 500 environments show worse generalization compared to those trained on 10 and 50 environments, suggesting potential overfitting. While the 100 and 500 experiments outperform in max statistic, they exhibit higher variance in success rates across different seeds. This variability in results indicates a lack of empirical guarantee that RL agents will generalize effectively when exposed to multiple environments. The study found that domain randomization alone may not effectively adapt RL policies to variations in observations. Results showed limited progress in generalization, with success rates below 40% even after exposure to 50+ environments. Randomizing the environment during training did not improve generalization in RL policies, indicating that relying solely on the RL objective is insufficient for ensuring generalization. Previous success with domain randomization in supervised learning did not translate to RL policies. The success of domain randomization in previous works was reported using supervised learning, linking generalization abilities of machine learning algorithms to supervised learning setups. Adapting supervised learning techniques to regularize models trained with DRL is discussed, along with two methods proposed for using the IR penalty. The first method involves adding to the PPO objective, while the second method splits the objective into RL and supervised learning steps. The value of \u03bb used in all IR experiments is 1.0, and different transformations of observations were tested. In experiments testing transformation T of observations, KL divergence yielded the best results. Table 1 displays results of combining PPO with IR penalty. The split method showed stable success rates, outperforming vanilla PPO, especially with RGB/Grayscale inputs. Training with the full objective achieved the best results, surpassing vanilla PPO. Training on the full objective of the IR algorithm yielded superior results compared to vanilla PPO with domain randomization and the split version of the IR algorithm. The models trained on the full objective achieved high test success rates even with only 10 training environments, showing stable performance across different inputs and seeds. Adding more training environments did not significantly impact the average testing success rate, indicating that learning invariant features with the full objective does not require a large number of environments. Regularization in supervised learning has been shown to improve model performance on test sets. However, it is not commonly used in Deep Reinforcement Learning setups. Some methods achieve similar results to the full objective training, but lower success rates suggest randomness in learned policies. Regularization techniques such as dropout, batchnorm, and L2 are compared with a new method to address the generalization gap in Deep Reinforcement Learning. Experiments are conducted with different techniques on 50 environments, using RGB input only for a harder problem. The results show the average success rate over 5 seeds. The proposed method outperforms batchnorm, dropout, and L2 regularization techniques in improving success rates with more environments added. High entropy policies from dropout and L2 lead to better generalization by acting randomly in some instances. Success weighted shortest path length (SPL) is lower for random behaviors with high success probability, indicating robustness but not necessarily the shortest path to the goal. In the context of visual navigation agents trained with deep reinforcement learning algorithms, the study focuses on generalization capabilities. The research shows that RL agents tend to overfit even with large training sets. Domain randomization alone is insufficient for generalization, leading to the proposal of Invariance Regularization (IR) as a method to improve generalization success and stability across different seeds. In this study, the focus is on generalization capabilities of RL agents trained for visual navigation. The research highlights the importance of generalizing learned skills to different architectural designs of the environment. Future work includes exploring appropriate transformation functions for observations, such as adaptive forms learned through data augmentation or adversarial examples. The training process involves RL on original observations followed by a supervised learning objective on transformed observations. The training process involves training RL on one environment and then using the actions from the trained policy to fine-tune the model with supervised learning on textured environments. The model is trained in two stages without iterating between both. Training is done on a subset of rooms with various textures, and the resulting policies are tested on rooms with unseen textures. Multiple agents run in parallel during training to collect data quickly, allowing for variation in the training environment. Parallelization allows running each agent on a different training environment due to hardware limitations. Agents sample one environment from the training set and run on it for a set number of episodes before sampling another one. Success weighted by the time taken is measured using SPL, considering the success rate, shortest path length, and path taken by the agent."
}