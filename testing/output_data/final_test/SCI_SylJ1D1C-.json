{
    "title": "SylJ1D1C-",
    "content": "Partial differential equations (PDEs) are important in various disciplines like mathematics, physics, and computer science. With advancements in technology, data-driven discovery of hidden physical laws is now possible. A new feed-forward deep network called PDE-Net has been proposed to predict complex system dynamics and uncover underlying PDE models by learning differential operators. The PDE-Net is a deep network that learns differential operators through convolution kernels to approximate unknown nonlinear responses. It offers flexibility by learning both operators and responses, with properly constrained filters to identify governing PDE models. The network's design is based on the relation between differential operator orders and filter sum rule orders from wavelet theory. The PDE-Net, originating from wavelet theory, explores relations with networks like NIN and ResNet in computer vision. It has the potential to uncover hidden PDEs in observed dynamics and predict behavior in noisy environments. PDEs, like NavierStokes and Maxwell's equations, describe physical laws in various disciplines. With advancements in sensors, computational power, and data storage, vast amounts of data can now be collected for data-driven discovery of new physical laws. Can we learn a PDE model from data and make accurate predictions? Previous attempts have involved comparing numerical differentiations of experimental data with analytic derivatives of candidate functions. Recent advancements in data-driven discovery of physical laws involve using symbolic regression and evolutionary algorithms to determine nonlinear dynamical systems. Sparse regression techniques have also been utilized to construct a dictionary of simple functions and partial derivatives for accurate representation of data. Additionally, a framework has been developed to learn unknown parameters of nonlinear PDEs by introducing regularity between consecutive time steps using Gaussian processes. A new class of universal function approximators called physics informed neural networks has been introduced for discovering nonlinear PDEs parameterized by scalars. The recent advancements in data-driven discovery of physical laws involve using symbolic regression and sparse regression techniques to uncover hidden PDE models. However, symbolic regression is costly and does not scale well to large systems. The framework presented aims to predict complex system dynamics and uncover hidden PDE models with minimal prior knowledge. The framework aims to uncover hidden PDE models using deep learning techniques inspired by computer vision. Existing deep networks prioritize expressive power and prediction accuracy but lack transparency to reveal underlying PDE models. By combining deep learning and applied mathematics, a carefully designed network can learn the governing PDEs. In this paper, a deep feed-forward network called PDE-Net is designed to learn the form of the nonlinear response function F and make accurate predictions. Unlike previous work, PDE-Net requires minimal knowledge of the nonlinear response function and no knowledge of the differential operators involved. The PDE-Net is a deep feed-forward network designed to learn the nonlinear response function F using neural networks or other machine learning methods. It also learns discrete approximations of differential operators using convolution kernels. Prior knowledge of F can simplify training and improve results. Relations to existing networks like NIN and ResNet are discussed. Numerical experiments on linear and nonlinear PDEs are conducted in Sections 3 and 4. The PDE-Net is a deep feed-forward network that can uncover hidden equations in observed dynamics and predict behavior in noisy environments. It imposes constraints on learnable filters to identify governing PDE models while maintaining network power. This approach differs from existing deep convolutional networks by focusing on model identification rather than just prediction accuracy. The proposed approach in PDE-Net aims to uncover hidden equations in observed dynamics, predict future behavior accurately, and reveal the driving equations. Constraints on filters are motivated by earlier work on wavelet frame transforms and differential operators, which can be useful in network designs for machine learning tasks in computer vision. The PDE-Net aims to discover governing PDEs of observed data by designing a feed-forward network that predicts dynamical behavior and reveals the response function and differential operators involved. It combines automatic determination of differential operators and their approximations with approximating the nonlinear response function F. Discussions on the relation between convolutions and differentiations in a discrete setting are also included. The authors established connections between PDE-based and wavelet frame-based approaches for image restoration. The Haar wavelet frame transform on an image u approximates differential operators using high frequency coefficients. The authors discussed the relationship between convolutions and differentiations, focusing on the order of sum rules of filters and differential operators. The order of sum rules is linked to the order of vanishing moments in wavelet theory. The definition of the order of sum rules for a filter q is provided, specifying the conditions for total sum rules. The proposition links the orders of sum rules with orders of differential operators. An \u03b1th order differential operator can be approximated by the convolution of a filter with \u03b1 order of sum rules. A high order approximation of a differential operator can be achieved if the filter has an order of total sum rules with K > |\u03b1| + k, k 1. For example, the Haar wavelet frame filter bank's filter h 10 has sum rules of order (1, 0) and total sum rules of order 2\\{2}. The filters h10, h11, and q correspond to discretizations of differential operators with first and second order approximations. The concept of a moment matrix is introduced to constrain filters in the PDE-Net. The moment matrix is used to constrain filters in the PDE-Net, allowing for the approximation of differential operators at any given order by imposing constraints on the filter q. Constraints on the moment matrix guarantee the accuracy of the approximation, with frozen filters being uniquely determined when all entries of M(q) are constrained. In PDE-Net, filters are learned with constraints on moment matrices to approximate differential operators. Larger filters have higher representation capability but also increase memory and computation costs. Balancing this trade-off is crucial. Forward Euler is used for temporal discretization, impacting network architectures. In this paper, the focus is on forward Euler for temporal discretization in PDE-Net. The operators D 0 and D ij are convolution operators with filters q 0 and q ij. Introducing average operators improves network stability and enables capturing complex dynamics. The task of approximating F is treated as a multivariate regression problem, solvable by a neural network or classical machine learning methods. The paper focuses on using forward Euler for temporal discretization in PDE-Net. It introduces the concept of \u03b4t-blocks for approximating differential operators and nonlinear functions. Stacking multiple \u03b4t-blocks into a deep network, called PDE-Net, improves stability and enables long-term prediction. This network architecture can be adjusted based on prior knowledge of the response function F to simplify training and improve results. The PDE-Net is described as stacking \u03b4t-blocks and sharing parameters. Training with more blocks improves time stability. Sharing parameters reduces memory usage. Training involves pairs of data points for each solution path. The PDE-Net stacks \u03b4t-blocks and shares parameters to improve time stability and reduce memory usage. Training involves pairs of data points for each solution path, with learnable filters compared to frozen filters in the Frozen-PDE-Net. To enhance the PDE-Net's expressive power, multiple filters can be associated to approximate a differential operator. However, to maintain identifiability, only one filter is selected for correct approximation, while the rest modify local truncation errors. Using learnable filters instead of fixed numerical approximations in sparse regression methods increases flexibility and adaptability. The PDE-Net utilizes learnable filters to enhance flexibility and robustness in approximating unknown dynamics and longer time predictions. By approximating the response function from data and imposing constraints on moment matrices, the PDE-Net can identify differential operators and nonlinear response functions, providing transparency and potential to uncover hidden physical laws. This approach differs from existing methods and deep learning networks, with parameters divided into groups for differential operators and point-wise parameters. The PDE-Net uses filters to approximate differential operators and point-wise neural network parameters to approximate F. Hyper-parameters include the number and size of filters, number of layers, etc. Filters are initialized by freezing them to their corresponding differential operators. Layer-wise training is adopted for faster training speed. The PDE-Net utilizes filters to approximate differential operators and point-wise neural network parameters to approximate F. The network structure is similar to existing networks like NIN and ResNet, with shared parameters across layers. A warm-up step is used before training each \u03b4t-block to initialize parameters effectively. The PDE-Net uses mlpconv layers instead of ordinary convolution layers to improve feature extraction. Each \u03b4t-block has two paths for u and F, resembling the ResNet's residual block structure. Convection-diffusion equations are classical PDEs used in describing physical phenomena. Convection-diffusion equations describe the transfer of particles or energy in physical systems. These equations are widely used in various scientific and industrial applications. Anisotropy in physical properties leads to variable convection and diffusion coefficients. A 2-dimensional linear variable-coefficient convection problem is solved using a numerical scheme with high precision. The method involves spatial discretization and 4th order Runge-Kutta for temporal discretization with a time step size of \u03b4t = 0.01. Periodic boundary conditions are assumed, and noise is added to mimic real-world scenarios. The underlying PDE is known to be linear with an order no more than 4, and the response function takes a specific form. The PDE-Net is structured with convolution operators and 2D arrays approximating functions on \u2126. The PDE-Net utilizes 2D arrays to approximate functions on \u2126 using piecewise quadratic polynomial interpolation. Filters and coefficients are trainable parameters, with filters of size 5x5 or 7x7. LBFGS is used for optimization during training, with 28 data samples per batch per layer, up to 20 layers. Approximately 17k trainable parameters per \u03b4t-block are used, with a total of 560 data samples per batch. The PDE-Net is designed to approximate nonlinear evolution PDEs with up to 20 layers, requiring 560 data samples per batch. LBFGS outperforms SGD in training, and the network generalizes well. Numerical results show accurate prediction even with different initial data distributions. The section presents training results and evaluates prediction performance and PDE model identification. Hyper-parameter effects will also be investigated. The effects of hyper-parameters on the trained PDE-Net are investigated by analyzing the prediction ability and generalization of the network. Error analysis is conducted by comparing predicted dynamics with actual dynamics using high precision numerical schemes. Results show accurate predictions with 7x7 learnable filters, as depicted in error plots and predicted dynamics images. The PDE-Net's performance is analyzed with different hyper-parameters, showing that even with noisy data, it can make long-term predictions. Multiple \u03b4t-blocks improve stability and prediction length, with 7x7 filters outperforming 5x5 filters significantly. The network with 7x7 filters achieves reliable predictions 10 times longer than the one with 5x5 filters. The PDE-Net with multiple \u03b4t-blocks and 5x5 or 7x7 filters approximates coefficients for the linear problem. The coefficients learned by the PDE-Net closely match the true coefficients, with some oscillations due to noise in the training data. Larger filters do not improve coefficient learning, but multiple \u03b4t-blocks help with estimation. Having larger filters does not improve coefficient learning, but it helps in prolonging predictions of the PDE-Net. The learned coefficients by the PDE-Net with different filter sizes and \u03b4t-blocks closely match the true coefficients, with some errors shown in the results. The PDE-Net generalizes well and can predict dynamics accurately, despite some oscillations. The estimated dynamics in FIG6 show oscillations in the prediction, but still capture the main pattern. The PDE FORMULA20 is of second order, allowing for more accurate estimation of variable coefficients. Prediction errors are slightly higher due to fewer trainable parameters, but variance is smaller with accurate prior knowledge. Results are summarized in FIG7 and FIG8. Training the network without moment constraints emphasizes the importance of these constraints in PDE-Net filters. The Freed-PDE-Net, trained without moment constraints, outperforms the PDE-Net in prediction accuracy due to more trainable parameters. However, the correspondence between filters and differential operators is unknown, making it difficult to identify the underlying PDE. The numerical experiments demonstrate the PDE-Net's ability to accurately predict outcomes. The PDE-Net shows accurate prediction and identifies underlying PDE models in noisy environments. Deeper structures and larger filters enhance stability and prolong reliable predictions. Comparisons with Frozen-PDE-Net and Freed-PDE-Net highlight the importance of learnable yet constrained filters. Source/sink terms are crucial in modeling physical processes like particle transportation and energy transfer, especially in pollution distribution scenarios. The intensity of pollution sources is crucial for environmental control. PDE-Net accurately predicts and identifies underlying PDE models in noisy environments. Learned coefficients by the PDE-Net are compared for different orders of PDE. Data is generated using forward Euler for temporal discretization and central differencing for spatial discretization. The PDE-Net uses central differencing for spatial discretization on a 100 \u00d7 100 mesh, then restricted to a 50 \u00d7 50 mesh with zero boundary conditions. Gaussian noise is added to the sample sequences. The underlying PDE is a convection-diffusion equation of order no more than 2 with a nonlinear source depending on the variable u. The response function F and each \u03b4t-block of the PDE-Net are defined with convolution operators and 2D arrays approximating functions on \u2126 using piecewise quadratic polynomial interpolation. The trained PDE-Net uses piecewise quadratic polynomial interpolation for approximation on \u2126 and piecewise 4th order polynomial approximation over a regular grid of the interval [-30, 30]. The testing strategy is the same as in Section 3, with filter size of 7 \u00d7 7 and approximately 1.2k trainable parameters per \u03b4t-block. Numerical results show the PDE-Net's performance in predicting dynamical behavior and identifying the underlying PDE model, demonstrating its ability to generalize. The trained PDE-Net utilizes piecewise quadratic polynomial interpolation for approximation on \u2126 and piecewise 4th order polynomial approximation over a regular grid of the interval [-30, 30]. Comparisons between PDE-Net and Frozen-PDE-Net are shown in FIG0, highlighting the advantage of learning the filters. Results demonstrate the PDE-Net's strong performance in prediction and identifying the underlying PDE model. The PDE-Net, with 7 \u00d7 7 filters, accurately predicts long-time dynamics. Images of true and predicted dynamics are compared, showing accurate approximations near the center of the interval. The network, designed to discover hidden PDE models, demonstrates strong performance in predicting dynamical behavior. The PDE-Net, with two major components, approximates differential operations and the nonlinear response using deep neural networks. It can uncover hidden equations in dynamics and predict behavior accurately, even in noisy environments. Adjusting the network architecture based on prior knowledge can simplify training and improve results. In future work, the PDE-Net will be tested on real data sets to uncover hidden variables and develop stable numerical schemes for PDE models. The use of deep structure and larger filters can enhance stability and prolong reliable predictions."
}