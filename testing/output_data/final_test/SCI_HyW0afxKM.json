{
    "title": "HyW0afxKM",
    "content": "The proposed active learning algorithm, SGIM-PB, organizes its learning process to achieve complex tasks by learning sequences of motor policies. Learner can generalize over experience to continuously learn new outcomes actively guided by measures of progress. The focus is on learning a set of interrelated complex outcomes hierarchically organized. The new framework \"procedures\" enables autonomous discovery of combining learned skills to develop complex motor policies. It can actively choose outcomes and exploration strategies, including autonomous exploration or human-guided demonstrations. The architecture adapts policy complexity to tasks and enhances the agent's ability to learn complex tasks. This is relevant in the context of integrating robots into human environments. Service robots need to continuously learn new skills to operate in human environments. Tasks can be independent or complex, requiring a combination of learned skills. Drawing inspiration from developmental psychology, a new learning algorithm combines active motor skill learning, interactive learning, and strategical learning. This approach allows robots to adapt and learn a wide range of tasks even after deployment. The new learning algorithm combines active motor skill learning, interactive learning, and strategical learning to map parametrized outcomes to motor policies. It eliminates the need for manually designing reward functions for tasks and uses intrinsic motivation to drive learning through surprise functions. Competence progress measures help drive exploration, but efficiency decreases as the outcome space dimensionality increases. When the outcome space dimension increases, learning methods become less efficient due to the curse of dimensionality. To address this, heuristics like social guidance can drive robot exploration towards interesting and reachable spaces quickly. Combining intrinsically motivated learning and imitation through human demonstrations has shown to bootstrap exploration effectively. Interactive learning, where the robot actively seeks help from humans when needed, enables efficient learning from both exploration and demonstration. Imitation learning techniques involve providing information to the robot through various signals, actions, advice, or disambiguation. These approaches aim to include non-robotic experts in the learning process and allow the learner to choose what to learn or how to learn, known as strategic learning. The SAGG-RIAC algorithm focuses on self-generating goal outcomes to enhance the learning process. The SGIM-ACTS algorithm, introduced in BID12, enables a learner to actively choose both its strategy and target outcome. It organizes the learning process by selecting strategies and outcomes, utilizing autonomous exploration and imitation of human teachers. The algorithm has shown potential in learning hierarchically organized tasks on a high-dimensional robot. The learning of complex motor policies on a high-dimensional robot involves hierarchically organized tasks and the use of via-points to define these policies. Increasing the complexity of the policy allows for tackling more complex tasks, but may make simpler tasks more difficult. Allowing the learner to autonomously determine the complexity of the policy needed for a task would make the approach adaptive and suitable for a wider range of problems. Options are available for the learner to make these decisions. The approach aims to enable a robot learner to achieve a wide range of tasks by creating an unlimited number of complex policies. This new proposed learner can learn hierarchical sets of interrelated complex tasks and benefit from task hierarchy to reuse previously acquired skills for building more complex actions. The paper introduces a new algorithm, SGIM-PB, for a robot learner to achieve complex interrelated tasks by combining known policies according to their outcomes. This algorithm addresses the challenges of unlearnability of infinite task and policy spaces, and the curse of dimensionality of high-dimensionality policy spaces. The paper introduces the SGIM-PB algorithm for a robot learner to achieve complex tasks by combining policies based on outcomes. The algorithm addresses challenges of infinite task spaces and high-dimensionality policy spaces. The learner uses interactive learning, autonomous exploration, and task hierarchy to adapt policies to task complexity. The SGIM-PB algorithm enables a robot learner to predict outcomes and choose policies for complex tasks. It deals with infinite task spaces and high-dimensionality policy spaces by using interactive learning, autonomous exploration, and task hierarchy. The algorithm learns which policy to choose for specific outcomes and represents complex policies by concatenating primitive policies. The SGIM-PB algorithm allows a robot to learn complex tasks by exploring and exploiting a hierarchy of procedures. Procedures are built by chaining previously learned skills to create more complex policies. The algorithm updates procedures before execution to match the closest known subtasks based on the learner's current skill set. The SGIM-PB algorithm learns complex tasks by exploring and exploiting a hierarchy of procedures. It updates procedures before execution to match known subtasks based on the learner's skill set. The SAGG-RIAC algorithm uses local linear regression for goal-directed policy optimization. It involves building procedures to reproduce a goal outcome \u03c9 g through random exploration or local procedure optimization. Mimicry of one policy teacher strategy involves requesting a demonstration closest to the goal outcome \u03c9 g from the chosen teacher. The learner repeats the demonstrated policy from the teacher to reach the goal outcome \u03c9 g. It involves requesting a procedural demonstration from the teacher and refining it to reproduce the procedure. The learner uses Goal-Directed Optimization to optimize its input parameters and reach the best outcome \u03c9 g. The learner uses Goal-Directed Optimization to optimize input parameters and reach the goal outcome \u03c9 g by creating random inputs and storing policies and outcomes in episodic memory. It computes competence in reaching the goal outcome and updates its interest model to partition the outcome space into regions of high and low interest based on measures of interest. The learning agent uses Goal-Directed Optimization to partition its task space into regions based on measures of competence and interest. It selects strategies and goal outcomes based on empirical progress in each region of the outcome space. Sampling modes are used to choose outcomes and strategies stochastically. In Goal-Directed Optimization, the learning agent selects strategies and goal outcomes based on empirical progress in different regions of the outcome space. It uses modes to choose outcomes and strategies stochastically, with mode 3 focusing on generating a goal close to the outcome with the highest progress measure. The agent formalizes learning an inverse model between outcomes and policies, using procedures to learn task compositions and SGIM-PB as a proposed framework. SGIM-PB is a learning algorithm that leverages goal-babbling for autonomous exploration and social guidance to learn complex policies. It maps outcomes with policies and subgoal outcomes, with a general formalization and algorithmic architecture applicable to various problems. Experimental setup requirements include defining primitive policies, user outcomes of interest, and a performance measure for the robot. The study involves an experiment with a simulated robotic arm capable of learning an infinite number of tasks organized hierarchically. The robot can perform complex policies of unrestricted size in an environment contained within a cube. The learning agent is a planar robotic arm with 3 joints. The learning agent is a planar robotic arm with 3 joints, able to grab objects in an environment delimited by (x, y, z) coordinates. The robot can interact with the floor, a pen, and a joystick, with the ability to draw on the floor and control a video-game character's position on the screen. The experimental setup includes a robotic arm interacting with objects like a pen and joysticks to control a video-game character on the screen. The robot can only handle one object at a time and breaks if it touches a second object. The robot always starts from the same position before executing a task. The robot interacts with objects like a pen and joysticks, starting from the same position before executing tasks. Complex policies are recorded with outcomes, and motions of the robot's joints are encoded using Dynamic Movement Primitives. The forcing term in Dynamic Movement Primitives (DMP) is defined by weights and end positions, serving as the only parameters used by the robot. Primitive policies in DMP are parametrized by weights and joint parameters, with the ability to set positions on the vertical axis. Combining multiple primitive policies results in complex policies with concatenated parameters. The outcome subspaces the robot learns to reach are hierarchically organized and defined as different positions and scenarios at the end of a policy execution. These include the position of the end effector, the pen, the last drawn line on the floor, joysticks, and a video-game character. Procedural teachers were available to assist the SGIM-PB learner. Procedural teachers were available to assist the SGIM-PB learner by providing procedures for different outcome subspaces. Each teacher only gave procedures relevant to its own outcome space, with a cost of 5. The rules for providing procedures were specific to each teacher and outcome space. The SGIM-PB learner utilized procedural teachers for different outcome subspaces, each with specific rules and a cost of 5. The teachers provided demonstrations linearly distributed in their outcome space, totaling 27,600 points for evaluation. The evaluation involves computing the normalized Euclidean distance between benchmark outcomes and their nearest neighbor in the learner dataset. This process is repeated at predefined timestamps. The algorithm's efficiency is compared with 3 other algorithms: SAGG-RIAC, SGIM-ACTS, and IM-PB. The evaluation compared the efficiency of algorithms SGIM-PB, IM-PB, SAGG-RIAC, and SGIM-ACTS by computing the mean error in reproducing benchmarks with complex policies. SGIM-PB is an interactive learner driven by intrinsic motivation, with autonomous exploration strategies and mimicry of available teachers. The algorithm was run 5 times with 25,000 iterations each, using a \u03b3 value of 1.2 and specific probabilities for sampling modes. The global evaluation showed that algorithms capable of performing procedures had errors lower than their counterparts. The algorithms SGIM-PB and SGIM-ACTS show improved performance when using procedures, with SGIM-PB quickly reaching a lower error level than SAGG-RIAC after 500 iterations. The bootstrapping effect from mimicry teachers is observed in both algorithms. Individual outcome space evaluations also demonstrate the benefits of using procedures in learning. In the outcome space analysis, learners with demonstrations (SGIM-PB and SGIM-ACTS) outperform other algorithms, except for outcome space \u2126 5 where IM-PB is better due to more practice. SGIM-PB and SGIM-ACTS excel in outcome spaces \u2126 3 and \u2126 4, which require precise policies to prevent errors. SGIM-PB benefited from carefully crafted policies provided by policy teachers. The SGIM-PB learner benefited from carefully crafted policies provided by policy teachers and showed improvement in learning hierarchical tasks, especially in outcome space \u2126 5. The learner was able to request demonstrations from relevant teachers depending on the task at hand. The SGIM-PB learner adapts the complexity of its policies to the working task by selecting known policies that reach the closest outcomes in different outcome subspaces. The learner can request demonstrations from relevant teachers and showed improvement in learning hierarchical tasks. The SGIM-PB learner can adapt the complexity of its policies to different outcome subspaces, choosing longer policies for more complex tasks. It successfully discovers task hierarchy and uses complex motor policies to learn a wide range of tasks, selecting the most adapted teachers for the target outcome. The learner can limit the complexity of its policies instead of always increasing them. The SGIM-PB learner can adapt the complexity of its policies to different outcome subspaces, successfully discovering task hierarchy and using complex motor policies. It combines the ability to progress quickly in the beginning with the ability to progress further on highly hierarchical tasks. The aim is to enable a robot to learn sequences of actions of undetermined length to achieve various outcomes in a high-dimensional learning environment. In an infinite dimensionality space of sequences of actions, techniques like goal-babbling, social guidance, and strategic learning were used to develop the SGIM-PB algorithm. This algorithm allows the robot to learn complex tasks, adapt the length of action sequences, and discover task hierarchy. Procedures, social guidance, and intrinsic motivation play key roles in the robot's active learning process. The SGIM-PB algorithm enables the robot to learn complex tasks through goal-babbling, social guidance, and strategic learning. It aims to illustrate its effectiveness in a real-world application with a robotic platform. The algorithm can handle the curse of dimensionality in a larger procedure space and explore combinations of subtasks. Additionally, it can be extended to allow the robot learner to decide on how to execute a procedure. The current version proposes a \"refinement process\" for the robot learner to infer the best policy, which can be made more recursive by allowing the algorithm to select lower-level procedures as policy components."
}