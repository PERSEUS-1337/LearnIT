{
    "title": "BkfEzz-0-",
    "content": "Existing MARL communication methods rely on a trusted third party for reward distribution, making them unsuitable for peer-to-peer environments. This paper introduces Neuron as an Agent (NaaA) for reward distribution in MARL without a TTP, utilizing inter-agent reward distribution and auction theory. NaaA allows for representation trades in peer-to-peer settings, treating units in neural networks as agents. Numerical experiments validate the effectiveness of the NaaA framework in both single-agent and multi-agent environments. The NaaA framework optimizes performance in reinforcement learning by distributing rewards among agents without a trusted third party. Existing literature does not discuss reward distributions in this context. The Tragedy of the Commons can occur if rewards are limited, leading to reduced rewards for contributing agents. The proposed model involves agents redistributing rewards among themselves, determining optimal rewards through an auction game. The proposed method, Neuron as an Agent (NaaA), extends CommNet to distribute rewards in MARL without a trusted third party. Agents in NaaA maximize profit by redistributing costs and using auction theory to prevent costs from dropping too low. This approach ensures that agents autonomously evaluate the optimal reward distribution. The Neuron as an Agent (NaaA) method extends CommNet to distribute rewards in MARL without a trusted third party. NaaA enables representation trades in peer-to-peer environments and regards neural network units as agents. In an experiment using ViZDoom, results showed that the cameraman agent learned cooperative actions and outperformed CommNet in score. The Neuron as an Agent (NaaA) method extends CommNet to distribute rewards in MARL without a trusted third party. NaaA enables representation trades in peer-to-peer environments and regards neural network units as agents. Adaptive DropConnect (ADC) combines DropConnect with an adaptive algorithm, showing superior performance in classification and reinforcement learning tasks compared to DropConnect alone. The paper is organized with sections on problem setting, proposed method, related works, and experimental results in classification, single-agent RL, and MARL. The paper discusses maximizing rewards in MARL systems, with a focus on reward distribution among agents. In most MARL communication methods, a centralized agent determines the policy for reward distribution. For example, QUICR and COMA distribute rewards based on counterfactual reward differences. The proposed method, Neuron as an Agent (NaaA), extends CommNet BID23 to distribute rewards in MARL without a trusted third party. Agents receive rewards from the environment and distribute them to other agents as incentives for accurate information, following auction theory principles. Agents in Neuron as an Agent (NaaA) distribute rewards among themselves to incentivize accurate information, with a reduction factor \u03c1 to meet constraints. The internal environment provides rewards of \u2212\u03c1 instead of observations, while the external environment remains. Communication between agents is continuous and trainable through backpropagation, represented as a directed graph G = (V, E) where connections indicate information flow. The representation of agent vi at time t is denoted as xit \u2208 R. The set of agents connected to agent i is designated as Nouti = {j|(vi, vj) \u2208 E}. Assumptions added to the vi characteristics include selfishness in maximizing its own return, conservation of internal rewards, and the ability to select NOOP as an action. The reward signal xi is transferred to the agent, with a reduction factor \u03c1 subtracted from the reward of vj. The agent vi can select NOOP as an action, where the return is positive. The social welfare function G all is equivalent to the objective function G. The agent maximizes its cumulative discounted profit G it, which is approximated with the value function. In multi-agent systems, agents may choose NOOP action to maximize revenue and minimize costs, leading to a decrease in external rewards. This results in agents taking random actions with no information, causing a significant reduction in total external reward. The social dilemma in MARL arises when agents do not truthfully evaluate each other's value to maximize their own profit, leading to a decrease in external rewards. Auction theory is proposed to address this issue by making agents evaluate each other truthfully, borrowing ideas from digital goods auctions. Envy-free auctions are used to determine the true price of goods, with a focus on goods that can be copied without cost. In NaaA, equivalent goods have a single simultaneous price represented by \u03c1 jit. The envy-free auction process involves bidding, optimal price selection, allocation, and payment between a seller and buyers. Payment occurs if the bid exceeds the price, with allocation defined as g jit = H(b jit \u2212 q it ). Buyers then make payments based on the allocation. After allocation, buyers pay based on demand. The seller sends representation only to allocated buyers. Revenue is determined by demand and optimal price. Cost is the internal reward paid by one agent to others. The value function affects successful and unsuccessful purchasing cases. The value function in purchasing cases is approximated as a linear function. The optimization problem is presented using a state-action value function. The bidding price that maximizes returns is identified. Agents should only consider their counterfactual returns for long-term rewards. The bidding value in an envy-free auction is raised as agents consider long-term rewards. The Nash equilibrium is displayed in the auction. Q-learning is used to predict values, with the state parameterized using a vector. The algorithm can be used for MARL and network training. The application of NaaA to SGD is named Adaptive DropConnect (ADC), combining DropConnect and Adaptive DropOut. ADC is introduced as a potential NaaA application, involving inter-agent reward distribution with envy-free auction. The application of NaaA to SGD is named Adaptive DropConnect (ADC), combining DropConnect and Adaptive DropOut. ADC involves inter-agent reward distribution with envy-free auction. It computes allocation, bidding prices, asking prices, and updates parameters to maximize rewards in supervised optimization problems with multiple revisions. Agents update parameters to maximize rewards from a criterion calculator, using batch-likelihoods as rewards. Each agent, a classifier, adjusts weights to maximize rewards. A heuristic based on the absolute value of weights is used for counterfactual return. This algorithm is simple and can be applied to various deep learning problems. Existing MARL communication methods rely on a TTP to distribute rewards to agents, limiting their use in peer-to-peer environments. R/DIAL and CommNet focus on optimal communication among agents using Q-learning and backpropagation, respectively. QUICR-learning maximizes counterfactual rewards, while COMA maximizes rewards in an actor-critic setting. These methods all use a centralized environment for reward distribution, unlike NaaA. NaaA does not rely on a TTP for reward distribution, with each agent calculating its own reward. Trading agent competitions have been held in various contexts, yielding numerous trading algorithms. Existing auction-based methods for optimal price determination cannot be applied to the current situation due to lack of communication among agents. This paper introduces inter-agent reward distribution to multi-agent reinforcement learning (MARL) communications, focusing on auction theory and mechanism design. It discusses second-price auctions for goods in limited supply and extends digital goods auction methods to address collusion. The paper also explores controlling connections between units with Adaptive DropConnect (ADC). The paper introduces Adaptive DropConnect (ADC) as a regularization technique for controlling connections between units in neural networks. ADC uses a skew probability correlated to the absolute value of weights, making it closer to Adaptive DropOut BID2. Unlike previous methods using recurrent neural networks (RNN), ADC is RNN-free and simpler to implement, making it fast and widely applicable for machine learning tasks. The paper discusses using Adaptive DropConnect (ADC) as a regularization technique for neural networks, which is RNN-free and simpler to implement. It covers supervised learning tasks like image classification with datasets such as MNIST, CIFAR-10, and SVHN, as well as reinforcement learning tasks in single-agent environments like CartPole and multi-agent environments like ViZDoom. Three types of classification datasets were used: MNIST, CIFAR-10, and STL-10, each with the task of predicting image labels. The experiment involved predicting image labels using the STL-10 dataset, which had 1,300 images per class. Images were resized to 48x48 due to higher resolution. Two models, DropConnect and Adaptive DropConnect, were compared. The models consisted of convolutional and fully connected layers with dropout. Labels were predicted using log-softmaxed values. The Adaptive DropConnect model outperformed the baseline and DropConnect models in classification error rates across different datasets. The models were trained on MNIST, CIFAR-10, and STL-10 datasets for varying epochs. Results showed lower error rates for the Adaptive DropConnect model. Additionally, a single-agent reinforcement learning task was conducted on the CartPole task from OpenAI Gym with visual inputs. The proposed method improved standard RL in the CartPole task with visual inputs. A validation experiment in ViZDoom confirmed the effectiveness of the reward distribution method in a multi-agent setting. The experiment involved a defend the center scenario with a main player and a cameraman attacking enemies. In a defend the center scenario in ViZDoom, the main player could attack enemies with bullets while the cameraman could only scout. The main player had 8 possible actions, including attacking and turning, while the cameraman could only turn. The main player received a score of +1 for each successful attack and lost -1 for each death. Episodes ended when the main player died or after 525 steps. The main player and cameraman in ViZDoom enter a cooperative relationship, with the main player attacking enemies and the cameraman providing information. The main player receives a score for successful attacks and episodes end either when the main player dies or after 525 steps. The proposed method involves communication between the two agents, with the main player learning standard DQN and the cameraman seeking new information. The proposed method in ViZDoom involves the main player learning DQN with two perspectives: theirs and that of the cameraman. Communication is facilitated through a neural network, and rewards and communications are transmitted using this method. Training over 10 million steps resulted in the NaaA model outperforming other methods, with Adaptive DropConnect contributing to the improvement. The cameraman was able to observe enemy positions and the area behind the main player, providing valuable information during gameplay. The cameraman observed the area behind the main player to observe enemy attacks and take a better position. A heatmap visualization of revenue earned by the agent is shown in FIG3. The paper proposed a NaaA model for communication in MARL without a TTP, focusing on inter-agent reward distribution and auction theory. When an envy-free auction was introduced using auction theory, agents evaluated counterfactual returns of other agents. Experimental results showed NaaA outperformed baseline and CommNet-based methods. An Adaptive DropConnect algorithm optimized neural network topology with counterfactual return evaluation. Experiments on a single-agent platform demonstrated improved results. Future research may explore the connection between NaaA and neuroscience. Edeleman proposed neural Darwinism, where group selection occurs in the brain. Inter-agent rewards in this paper correspond to NTFs and could be used as a fitness function in genetic algorithms. The implementation of NaaA in blockchain BID24 is being considered, which could expand the application of deep reinforcement learning. Bitcoin BID18 may facilitate inter-agent reward distribution, and smart contracts BID4 could enable an auction mechanism. The NaaA reward design aims to promote global unity by allowing people to share their representations. The optimization problem in Eq:11 consists of two terms, with the second term depending on b, which is the focus of optimization. The optimal bidding price \u015d q t is determined through a specific equation. The equation is solved by finding the solution that satisfies certain conditions. Letting q = q jt and o = o ij,t+1 simplifies the equation."
}