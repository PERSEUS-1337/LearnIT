{
    "title": "Hke-JhA9Y7",
    "content": "The method proposed involves learning interpretable representations for regression tasks using networks of multi-type expression trees. Features are trained via gradient descent, with performance in a linear model used to adjust the rate of change among subcomponents. An archive of representations with accuracy-complexity trade-offs is maintained to aid generalization and interpretation. Stochastic optimization approaches are compared, showing that this method achieves the highest average test scores across various regression problems with significantly smaller representations. The performance of machine learning models relies heavily on the data representation used in training, with neural networks' representational capacity being a key factor. There is no consensus on the optimal network architecture design, leading to challenges as problems become more complex. One approach is tuning network architectures through hyperparameter optimization using grid or randomized search with cross-validation. In network architecture search, various methods like weight sharing, reinforcement learning, and population-based stochastic optimization are used to design neural networks. These methods involve evaluating candidate solutions and updating them iteratively to produce a desirable architecture, considering conflicting objectives like interpretability. Interpretability is a key concern in the scientific community when using ML models for predictions and insight into processes. Two main approaches are semantic, focusing on explaining model behavior, and syntactic, aiming for simple models like those built from first-principles. The goal is to find the simplest process description with generalizable predictions and disentangled factors of variation for easier interpretation. The text discusses the importance of interpretability in machine learning models and the use of evolutionary computation to create modular networks for better model interpretation. The goal is to assess the performance of different approaches in producing intelligible representations from neural network building blocks for regression. In Section 2, a new method called the feature engineering automation tool (FEAT) is introduced to optimize representations for neural networks. FEAT uses syntax trees to improve transparency and gradient descent for feedback. Various multi-objective variants are compared using evolutionary computation and non-evolutionary computation methods. The experiment in sections 4 and 5 benchmarks FEAT against state-of-the-art machine learning methods. The experiment benchmarks FEAT against state-of-the-art ML methods on 100 open-source regression problems. Feature engineering aims to find a new representation of input data for better model performance. The regression model associates inputs with real-valued outputs, and each predictor is scaled to zero mean, unit-variance. A fixed NN architecture is chosen for regression or classification tasks. FEAT is a method that optimizes a population of potential representations using evolutionary computation. It aims to find the true structure of the data process by minimizing a cost function through joint optimization. The approach involves tuning the representation structure to improve model performance. FEAT optimizes a population of potential representations using evolutionary computation. It involves fitting a linear model and selecting individuals with the lowest error on a validation set. The method uses syntax trees to represent the internal architecture. The proposed method uses syntax trees to represent the internal architecture of the network, with weights encoded in the edges of the graph. It includes activation functions like tanh, sigmoid, logit, and relu nodes, as well as arithmetic and boolean operators. The construction biases representations to be thinly connected, aiming to optimize potential representations using evolutionary computation. FEAT uses syntax trees to represent network architecture with weights in edges. It biases representations to be thinly connected for better legibility. Variations include mutation and crossover methods, with 6 operators like point mutation and sub-tree crossover. Operator probabilities impact algorithm performance. For the study, each operator is used with uniform probability. An ML model assesses representation fitness, suggesting changes for elements with small coefficients. The probability of mutation for each tree in a candidate representation is denoted by P Mi(n), using softmax-normalized probabilities based on coefficient magnitudes. The smaller the coefficient, the higher the mutation probability, controlled by parameter f. The study compared five algorithms for selection and survival in genetic algorithms, including lexicase selection, non-dominated sorting genetic algorithm, and a hybrid algorithm using both. The selection step chooses parents for generating offspring, resulting in a population of 2P representations. The survival step reduces the population back to size P to finish the generation. Weighting probabilities based on feedback control and node weights could be extended for differentiable nodes. The study compared five algorithms for selection and survival in genetic algorithms, including lexicase selection, NSGA2, simulated annealing, and random search. The hybrid algorithm using Lex for selection and NSGA2 for survival performed the best. Lex selects parents based on filtering the population using randomized orderings of training samples, scaling selection probability based on individual performance on difficult cases. Lex has shown strong performance in recent tests among SR methods. The survival step for Lex preserves offspring and the best individual in the population. The survival step for Lex in genetic algorithms preserves offspring and the best individual in the population using NSGA2's survival sub-routine, which applies preference for survival based on Pareto dominance relations. Individuals are ranked based on their Pareto front ranking, with lower ranked individuals chosen for survival. NSGA2 assigns crowding distance to break rank ties and select individuals for survival based on Pareto dominance. Three objectives are considered: reducing model error, minimizing complexity, and minimizing entanglement. The third objective is tested using metrics like correlation of transformation matrix and condition number. The first objective relates to mean squared loss, the second to representation complexity. The complexity of a representation is defined by assigning weights to operators, discouraging deep sub-expressions within complex nodes. Operator weights are subjective, aiming to reduce interpretational difficulty. The choice of operator weights is subjective for interpreting representations. Disentanglement refers to separating factors of variation in a process, with various ways to quantify it. In regression, a disentangled representation should have minimal features, each corresponding to a separate latent factor and orthogonal to each other. To promote disentanglement, minimizing collinearity between features in \u03c6 is crucial. Two measurements of collinearity were tested: average squared Pearson's correlation and condition number (CN) of the representation matrix \u03a6. Unlike correlation, CN considers the singular values of \u03a6. The text discusses the importance of minimizing collinearity in a disentangled representation in regression. Condition number (CN) is used to capture higher-order dependencies in the representation, and it is related to the sensitivity of the representation to perturbations in the training data. Neuroevolution methods like NEAT and Hyper-NEAT are mentioned as approaches to evolving neural network architectures without the parameter learning step. Neuroevolution strategies, primarily used in robotics, image classification, and reinforcement learning, do not prioritize interpretability. However, some methods, like the work of BID68, focus on optimizing neural networks using multi-objective evolutionary computation. Neuroevolution is part of the broader field of neural architecture search (NAS), which includes various approaches like parameter sharing, model-based optimization, reinforcement learning, and heuristic strategies. Additionally, FEAT is related to feature engineering approaches. FEAT is a feature engineering approach that leverages machine learning weights for functional building blocks and uses multiple type representations to learn continuous and rule-based features within a single representation. It incorporates elements of neural network learning to enhance its performance. FEAT incorporates neural network learning elements to enhance its representational capacity, using activation functions and edge-based encoding of weights. Unlike traditional symbolic regression methods, FEAT treats constants as leaves in expression trees and updates weights via Lasso instead of gradient descent. Non-stochastic methods like mixed integer non-linear programming have also been explored for small search spaces. The experiment aims to compare FEAT with state-of-the-art regression methods and characterize complexity. The study aims to optimize feedforward neural networks' hyperparameters, assess model complexity, and evaluate disentanglement objectives' impact on representations using regression datasets from OpenML. Various regression methods like MLP, RF, KernelRidge, ElasticNet, and XGBoost are compared, with hyperparameters tuned accordingly. The experiment includes standardized datasets from the Penn Machine Learning Benchmark repository and code for reproducibility. The study optimizes hyperparameters for feedforward neural networks using regression datasets. Hyperparameters are tuned with grid search and 10-fold cross validation. Model complexity is evaluated by counting nodes in the final model. The impact of disentanglement objectives on representations is assessed using different versions of the FEAT algorithm. The study evaluates the FEAT algorithm with disentanglement objectives for optimizing hyperparameters in feedforward neural networks. FEAT and XGBoost show the best predictive performance without significant differences, outperforming other methods. FEAT models are less complex than others, as shown by the number of nodes in the final solutions. FEAT's final models are less complex than XGBoost, RF, and MLP, with a similar runtime. FEAT and MLP produce correlated feature spaces, with Feat's representations containing more bivariate correlations. The results suggest explicitly minimizing collinearity. This paper introduces FEAT, a feature engineering archive tool that optimizes neural network architectures using syntax trees and model weights as feedback. FEAT achieves state-of-the-art performance on regression tasks with less complex representations compared to other methods, albeit with a slightly higher computational cost. The study introduces FEAT, a tool for optimizing neural network architectures using syntax trees and model weights as feedback. The improvement in performance comes with an additional computational cost limited to 60 minutes per training instance. Future work should focus on representation disentanglement and improving the model selection procedure. In future studies, the model selection procedure could benefit from incorporating disentanglement at various stages of the search process. This work was supported by NIH grants AI116794 and LM012601. Additional experiment information is provided in APPENDIX A.1, detailing hyperparameters for each method used in the experimental results. Wall-clock run times for each method are reported in FIG5, with Feat variants terminated at 200 generations or 60 minutes. The experiment tested five methods: NSGA2, Lex, LexNSGA2, Simulated annealing, and random search. Simulated annealing is a non-evolutionary technique based on the metallurgical process of annealing. Offspring compete with parents based on fitness, with the probability of offspring replacing its parent determined by a fitness function. The text discusses the use of a scheduling parameter, t, to control the rate of \"cooling\" in the search space. Random search is compared to selection and survival methods, with random search randomly sampling S using an initialization procedure. When FEAT is used without a complexity-aware survival method, a separate approach is taken. The text discusses the use of an archive to maintain a Pareto front according to minimum loss and complexity during optimization. The archive is tested on a validation set to select the final model, protecting against overfitting. Benchmarking on 88 datasets shows that LexNSGA2 achieves the best R2 value with small solutions compared to other methods. Runtime comparisons indicate similar performance among the methods. The runtime comparisons of different optimization methods show that NSGA2 is the fastest, while Random search is the slowest. Pairwise comparisons of methods are done using CV R2 rankings in TAB3. The runtime comparisons of optimization methods show that NSGA2 is the fastest, while Random search is the slowest. RF and XGB have different performance metrics."
}