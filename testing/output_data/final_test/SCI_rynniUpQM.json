{
    "title": "rynniUpQM",
    "content": "In this paper, a new approach called Mental Image DCGAN (MIDCGAN) is proposed, which uses deep convolutional generative adversarial networks to create a 'mental image' of input images as an ideal representation of the input data distribution. This mental image helps in recognizing entire classes of objects and improves single and zero shot recognition. Deep convolutional neural networks have greatly impacted machine learning and computer vision, but there is still a gap in learning similar to how humans do. Children build preferences for object viewpoints by interacting with them from various perspectives. The Mental Image DCGAN (MIDCGAN) approach aims to train a deep convolutional generative adversarial network by providing an ideal representation and samples of different input image variations. The Mental Image DCGAN (MIDCGAN) is trained to associate samples of input image variations back to a mental image using a GAN architecture. MIDCGAN bottleneck features are better for learning compared to features generated without a mental image. This approach provides a target mental image to be learned, improving the representation of category classes internally. The MIDCGAN approach uses a mental image as a target for learning and storing a representation of a category distribution. It creates a bottleneck feature vector that best represents the input distribution mapping to the mental image. The effectiveness of MIDCGAN is demonstrated on handwritten digits and instance-based object recognition. The MIDCGAN approach focuses on instance-based object recognition by providing a system with an iconic view of the object. It trains the system to imagine how the object would look from an ideal perspective. Evaluation includes quantitatively assessing the usefulness of bottleneck features, qualitatively generating mental images, and performing few-shot recognition on objects without learned mental images. Several architectures and improvements on training GANs were proposed after the introduction of the original generative adversarial networks (GAN). MIDCGAN is inspired by image-to-image mapping, image editing, and inpainting using GANs. GANs have been used for classification in unsupervised and semi-supervised manners. Rotation and pose invariance, as well as spatial transformer networks, are closely related topics. MIDCGAN learns invariance implicitly by mapping samples to canonical representation of the distribution, handling pose invariance learning and enabling single-shot recognition without constraints on object viewpoint. It achieves performance comparable to pre-trained models. DCGAN learns generative models through joint adversarial training of a generator and discriminator network, achieving single-shot recognition performance comparable to pre-trained models. It has been applied in various domains such as image generation, unsupervised learning, and image-to-image translation. MIDCGAN uses l2 loss and adversarial discriminative loss to train the generative network, mapping inputs to mental images. It differs from DCGAN by using an autoencoder encoder to derive input features into a bottleneck vector for learning. The architecture replaces the generator with a full autoencoder, producing a bottleneck feature vector for object representation. The MIDCGAN model uses an encoder to generate a feature vector, z n, from the input image, and a decoder to produce a sample from the generated distribution, p(x), representing the mental image of x. The encoder focuses on extracting meaningful features for classification tasks by emphasizing common features of a specific category, leading to implicit learning of useful features for classification without being distracted by irrelevant input details. The MIDCGAN architecture includes an encoder, decoder, and discriminator with different convolutional block structures. The discriminator can be a regular AlexNet style or have a feature extraction pipeline. MIDCGAN is trained using joint adversarial and generator l2 losses to produce sharper images closer to the target distribution. By adding the mental image as a target in the l2 loss component, the network is forced to select a specific mode of the target distribution, resulting in clearer images compared to a regular DCGAN. The l2 loss in MIDCGAN maps input samples to mental images, with the adversarial loss providing additional supervision. It forces the network to choose a specific mode of the target distribution. Both l1 and l2 losses were experimented with, showing no significant difference. The l2 loss is the squared difference between the target and generated mental images. The adversarial game between the discriminator and generator is a key component. The discriminator and generator in MIDCGAN play an adversarial game with a specific focus on the input and target distributions. Unlike regular GANs, MIDCGAN does not have a prior distribution for the bottleneck, and the encoder learns the distribution of z. This results in an adversarial loss component represented by Eq. 3, where the encoder E(x) maximizes an objective to match the actual data distribution p(x). The training of MIDCGAN involves updating the parameters of the networks using alternating SGD. The joint loss for the encoder-generator pair is a weighted sum of equations 1 and 3. Experimentation showed that the choice of \u03bb weights is dataset dependent. The convergence of MIDCGAN is quick due to the dual mode selection, allowing the discriminator to distinguish real target samples early on. The adversarial loss in MIDCGAN helps the discriminator distinguish real target samples early on. The bottleneck features z are important for input distribution representation and later classification. The encoder generates z for classification after training. In MIDCGAN, the encoder generates bottleneck features for classification using an l2 SVM. Four major experiments are presented, including evaluating MNIST and SVHN with stenciled targets as mental images, and object instance recognition using BigBIRD and University of Washington Kinects Objects Dataset. Learning in this manner outperforms features from a typical DCGAN architecture. In the experiment, the encoder, decoder, and separate discriminator were all ResNet architecture with a deeper and more expressive separate discriminator compared to the commonly used AlexNet discriminator. MIDCGAN with a separate discriminator outperforms other architectures in terms of performance with fewer training samples. In the experiment, MIDCGAN with a bottleneck size of 1024 and a separate discriminator of 0.53% error outperformed other architectures for the SVHN dataset. The model showed improvement from a bottleneck size of 256, achieving the best performance among all methods tested with a wide range of training samples. Object instance recognition focuses on identifying specific instances of objects, considering subtle differences between them. The Big Berkley Instance Recognition Dataset (BigBIRD) contains 125 instances of various objects, including household items like food boxes and water bottles. Many instances are similar in appearance, with objects shown from different viewing angles and rotations. MIDCGAN recognizes objects based on the first instance seen, with the ability to view objects from multiple angles. MIDCGAN can recognize objects from different poses by generating a mental image of the frontal view. The BigBIRD dataset consists of 125 object instances for a classification problem. A linear SVM is trained with 60,000 training images and tested with 7,500 images. The experiment uses a 125-way linear SVM with bottleneck features, fixing the bottleneck size at 4096 and using a separate discriminator. The loss function assigns weights to l2 and adversarial losses to capture object details broadly. The MIDCGAN features excel at object instance recognition, achieving 52.2% accuracy in single shot recognition compared to 14% for DCGAN. Even when trained with limited viewpoints, MIDCGAN can still recognize objects effectively. The challenge lies in generalizing these features to unseen objects, a problem known as transfer learning. The University of Washington Kinect Object Dataset (RGBD) contains 300 object instances grouped into 51 classes, with images of household objects not present in BigBIRD. Transfer results are shown in TAB3, following criteria from a previous study. The study used MIDCGAN for object recognition on the Kinects Objects Dataset and the Bigbird Dataset. MIDCGAN showed improved performance compared to DCGAN, recognizing objects correctly 52% of the time with 10 labeled images. The selection of the mental target image in MIDCGAN could be arbitrary, and in cases of uncertainty, the generated image did not resemble the actual object. MIDCGAN can determine confidence in object class prediction, used in robotics for object manipulation and recognition. Improvements include autonomous selection of mental images for semi-supervised training. The MIDCGAN architecture includes encoder, decoder, and discriminator networks with 3 major convolutional blocks. Prototype images can be selected systematically for training. The generated images results vary based on different architectures, training conditions, and datasets. The MIDCGAN architecture consists of encoder, decoder, and discriminator networks with 3 major convolutional blocks. The architecture includes simple and resnet designs, with the latter incorporating residual units for improved performance. The encoder features single identity convolutions for channel transformation, and a dense layer at the end converts features for classification. The bottleneck size is parameterized by nBn for classification suitability. The decoder in the MIDCGAN architecture has two dense layers that transform bottleneck features to a suitable size for spatial plane transformation. Three major convolutional blocks are used with upsampling instead of transposed convolution for consistency. The discriminator includes three types, with one being similar to the encoder but adding a binary discrimination layer. The discriminator in the MIDCGAN architecture includes three types: the encoder-like discriminator with a binary discrimination layer, the shared discriminator with shared weights except for the last layer, and the AlexNet discriminator for comparison with existing literature. All architectures use 4 x 4 convolutions, downsampling with a stride of 2 in the beginning convolution of each major block, and activation functions of tanh for the first and last convolutions, and LeakyReLU with a parameter of 0.2 for internal convolutions. The number of filters in the internal blocks is 32 for MNIST training and 64 for BigBird training. For MNIST training, 32 filters were used, while 64 filters were used for BigBird training. The datasets were split into training, validation, and testing sets. Adam optimizer was employed for training, with different learning rates and beta parameters for each dataset. The maximum epochs for training were set at 2500, but most networks converged much earlier. Advanced techniques like feature matching were not applied in this study. In this study, advanced techniques like feature matching, minibatch discrimination, historical averaging, and virtual batch normalization were not applied to improve convergence and performance of DCGAN and ResNet training. The focus was on basic techniques to stabilize training and find the Nash equilibrium point for adversarial training. The authors chose not to include more complicated heuristics suggested by BID21 to assess network performance fairly. The authors focused on basic techniques to stabilize training for DCGAN and ResNet, avoiding more complex heuristics to assess network performance fairly. Standard GAN training techniques were utilized, including one-sided label smoothing, batch normalization, and l2 regularization for ResNet units. Regularization was not applied to avoid confounding factors and maintain focus on learning from mental images. The study focused on stabilizing training for DCGAN and ResNet using basic techniques like one-sided label smoothing, batch normalization, and l2 regularization. The generator improved over epochs, learning to separate target modes. The bottleneck size and discriminator architecture influenced the descriptiveness of features. The best results were achieved with a bottleneck size of 1024 and a ResNet discriminator. In comparing performance with different training sample sizes and discriminators, the study found that a more descriptive ResNet discriminator outperformed a simpler AlexNet discriminator. Results showed that simpler discriminators produced simpler features that were easier to capture with fewer neurons. The study also demonstrated successful classification results with as few as 10 samples per class. The features learned through mental image generation are well suited for the task, as shown in the SVHN dataset. Additional generated images for qualitative comparisons of MIDCGAN with DCGAN are provided, showing the progression of image quality. Generated images for test and validation sets of MNIST, SVHN, and BigBird using MIDCGAN and DCGAN models are shown in different epochs, highlighting the progression of image quality. In the last epoch, MIDCGAN generated images for the BigBird test set, with every third column displaying the generated image."
}