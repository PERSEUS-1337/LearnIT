{
    "title": "r1lgTGL5DE",
    "content": "REINFORCE can be used to train models in structured prediction settings by drawing multiple samples per datapoint to learn with significantly less data. A REINFORCE estimator with baseline is derived based on sampling without replacement, improving training for sequence models predicting solutions to problems like the Travelling Salesman Problem. In structured prediction tasks like Neural Machine Translation and Image Captioning, using REINFORCE can optimize test-time performance by directly training with multiple samples per datapoint. This approach is more efficient for sequence models like predicting solutions for the Travelling Salesman Problem. In structured prediction tasks like Neural Machine Translation and Image Captioning, using REINFORCE can optimize test-time performance by directly training with multiple samples per datapoint. This approach is more efficient for sequence models like predicting solutions for the Travelling Salesman Problem. The paper proposes using sampling without replacement to improve the quality of the gradient estimate in sequence modelling tasks. This idea is inspired by previous work on Stochastic Beam Search for sampling sequences without replacement. The paper extends the idea of using REINFORCE for policy gradients in sequence models like the Travelling Salesman Problem. It shows that using multiple samples with REINFORCE is beneficial compared to single samples, both computationally and in terms of data-efficiency. Sampling without replacement results in slightly faster learning, and the REINFORCE estimator allows for estimating gradients of expectations. In this paper, the extension of using REINFORCE for policy gradients in sequence models is discussed. The approach involves estimating expectations using samples and reducing variance with a baseline. The paper considers a parametric distribution over discrete structures and introduces the Gumbel-Max trick for sampling from this distribution. The Gumbel-Max trick can be extended to the Gumbel-Top-k trick for drawing an ordered sample without replacement. This method involves taking the top k largest perturbed log-probabilities instead of just one. The Gumbel-Top-k trick is equivalent to Weighted Reservoir Sampling and results in a partial ranking according to the Plackett-Luce model. Stochastic Beam Search BID7 is a modification of beam search that replaces the standard top k operation by sampling without replacement. This method allows for estimating the expectation of a function using Monte Carlo sampling. Stochastic Beam Search BID7 is a modification of beam search that replaces the standard top k operation by sampling without replacement. Using MC sampling with replacement, equation 3 is estimated with k samples y 1 , ..., y k. When sampling without replacement, the Gumbel-Top-k trick is used to select the set S of k largest indices of G \u03c6i. Importance weights are included to correct for the effects of sampling without replacement. The (k + 1)-th largest Gumbel perturbed log-probability is estimated using an unbiased estimator. The estimator is based on threshold sampling, where a variably sized sample is defined based on a fixed threshold. Instead of fixing the threshold, the sample size k is fixed and an empirical threshold \u03ba is used. The estimator for the (k + 1)-th largest Gumbel perturbed log-probability is based on threshold sampling with a fixed sample size k and an empirical threshold \u03ba. Normalizing importance weights is preferred to reduce variance. Multiple samples per datapoint can provide counterfactual information and computational benefits. The estimator for the (k + 1)-th largest Gumbel perturbed log-probability involves threshold sampling with a fixed sample size k and an empirical threshold \u03ba. Normalizing importance weights is recommended for variance reduction. Sampling without replacement complicates defining a baseline, unlike with replacement. The problem involves predicting solutions for instances of the Travelling Salesman Problem (TSP) by finding the optimal order to visit locations. A policy is trained using a baseline and importance weights, with a focus on normalizing terms for variance reduction. The Attention Model by BID6 uses REINFORCE to minimize the expected length of a tour in the Travelling Salesman Problem. It considers instances as fully connected graphs processed by an encoder and decoded into a sequence of nodes to visit. Different variations of REINFORCE are compared, including with and without replacement using different numbers of samples per instance. The model by BID6 uses REINFORCE to minimize the expected length of a tour in the Travelling Salesman Problem with 20 nodes. Different estimators are compared, including single sample with batch baseline and single sample with greedy rollout baseline. The baseline for the tour length is obtained by greedily selecting the next location based on an earlier version of the model. Multiple sampling strategies with local baselines are compared, including with and without replacement methods. The study compares sampling strategies with local baselines for tour length optimization. Using Stochastic Beam Search, samples are drawn without replacement. Results show diminishing returns for larger sample sizes, with sampling without replacement performing on par or slightly better than using a greedy rollout baseline. The study compares sampling strategies for tour length optimization, showing that sampling without replacement performs as well as or slightly better than using a greedy rollout baseline. Estimators based on multiple samples with and without replacement are effective, requiring fewer instances and providing a computational benefit. Sampling without replacement is preferred over sampling with replacement, with results close to optimal. The study compared sampling strategies for tour length optimization, finding that sampling without replacement is as effective as using a greedy rollout baseline. Estimators based on multiple samples, with and without replacement, are efficient and require fewer instances. Sampling without replacement is preferred over sampling with replacement, yielding results comparable to optimal. The proposed estimators have potential to improve training efficiency in structured prediction settings like Neural Machine Translation or Image Captioning. The proof by BID7 introduces necessary notation for understanding the unbiasedness of the priority sampling estimator. It considers general keys and a probability distribution over a finite domain. Each element has a random key, and the estimator's unbiasedness is not influenced by the key's distribution. The distribution of keys does not affect the estimator's unbiasedness but determines the sampling scheme. Using Gumbel perturbed log-probabilities as keys is equivalent to the PPSWOR scheme. Each element in the sample provides an unbiased estimate of E[f(i)], leading to an overall unbiased estimator. The REINFORCE estimator based on multiple samples with the sample average as baseline is proven to be unbiased. Using the batch mean as baseline is equivalent to using the mean of the other elements in the batch. The distribution of keys does not affect the estimator's unbiasedness but determines the sampling scheme. The REINFORCE estimator based on multiple samples without replacement with baseline is proven to be unbiased by adapting and combining proofs. Definitions and shorthand for conditional probabilities are introduced to generalize the pairwise conditional inclusion event. The REINFORCE estimator with multiple samples without replacement and baseline is proven unbiased by adapting proofs. Generalizing pairwise conditional inclusion probabilities, the lemma is used to show the expectation with respect to keys defining random variables. The keys define random variables \u03ba and S = {i : h i > \u03ba}. Theorem 1 states an unbiased estimator B(S) for j\u2208S p \u03b8 (y j ) qj (\u03ba) f (y j )."
}