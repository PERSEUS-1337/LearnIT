{
    "title": "S1gAO0EYwB",
    "content": "We propose Elastic-InfoGAN, an unsupervised generative model that disentangles object identity from low-level aspects in class-imbalanced datasets. It addresses issues with InfoGAN's assumptions and demonstrates improved disentanglement in imbalanced data. By making the discovery of latent factors invariant to transformations, our approach shows better disentanglement and approximation of class imbalance in both artificial and real-world datasets. Generative models like Generative Adversarial Networks and Variational Autoencoders have shown promising results in generating realistic samples for complex data such as images. Advanced models aim to discover disentangled representations where different latent dimensions represent independent factors of variation in the data, like pose or identity in human faces. InfoGAN focuses on learning an unsupervised disentangled representation by maximizing mutual information. Elastic-InfoGAN augments InfoGAN to discover disentangled categorical representations from imbalanced data by making two simple modifications. Elastic-InfoGAN modifies InfoGAN by using the Gumbel-Softmax distribution to make class probabilities learnable parameters and enforce the same latent category for an image and its transformed version. Our model aims to discover partitioning that groups objects based on high-level factors like identity while being invariant to low-level factors like lighting, pose, and scale changes. Our modifications to InfoGAN improve disentanglement and categorical grouping of data, allowing for the discovery of imbalances. This work is the first to address unsupervised generative modeling of categorical disentangled representations in imbalanced data, showing superiority over Info-GAN. Our work demonstrates superiority over Info-GAN and other baselines in modeling real data distributions and discovering proportions of factor variations. Disentangled representation learning, including methods like InfoGAN and JointVAE, has shown promising results in uncovering meaningful latent factors in datasets like MNIST, CelebA, and SVHN. Our work proposes modifications to InfoGAN to improve its performance in discovering meaningful latent factors in imbalanced data, which is common in real-world datasets. Researchers have suggested re-sampling and class reweighting techniques to address the challenges posed by long-tailed data distributions. Elastic-InfoGAN utilizes Gumbel-Softmax distribution for oversampling rare classes and down-weighting dominant classes. It incorporates additional constraints to ensure close latent code distributions for transformed images and low entropy. The model addresses imbalanced data with unknown class distributions, enhancing InfoGAN's performance in discovering meaningful latent factors. Our model proposes an unsupervised generative method to disentangle latent categorical factors in imbalanced data, leveraging data augmentation for unsupervised image grouping. Unlike other methods, we target imbalanced data and perform generative modeling to maximize mutual information between features of an image and its transformed version. Our goal is to learn a generative model that can disentangle object category from other aspects and recover the true class imbalance distribution. We discuss extending InfoGAN to imbalanced data and how GAN framework can be used for learning disentangled representations. The aim is for generated samples to retain information about latent variables, allowing control over certain aspects of the generated image. InfoGAN aims to control properties like digit identity or rotation in generated images by maximizing mutual information between latent code c and generated samples G(z, c). This involves using a lower bound of mutual information to approximate P(c|x) via a neural network based auxiliary distribution Q(c|x) in the training objective. InfoGAN uses a discriminator network and entropy of the latent code distribution to give control over image variations. It employs a uniform categorical distribution for discrete variations but struggles with imbalanced datasets. Two augmentations are proposed to address this issue. The prior distribution in InfoGAN is learned using the Gumbel-Softmax distribution, allowing for differentiable sampling of latent variables. This enables identity-preserving transformation invariance and favors disentanglement of object identities. In InfoGAN, the Gumbel-Softmax distribution is used to learn the prior distribution for differentiable sampling of latent variables, promoting disentanglement of object identities. However, empirical results show that unsupervised grouping may focus on non-categorical attributes like rotation of the digit. In this work, the focus is on grouping unlabeled data based on class identity rather than non-categorical attributes like rotation. To capture object identity, modifications to InfoGAN are made to create identity-preserving transformations on real images. These transformations aim to make the model invariant to low-level factors such as rotation, thickness, and illumination. The study focuses on grouping unlabeled data by class identity using identity-preserving transformations on real images. These fixed transformations ensure that the object identity label remains the same, such as rotation for digits or horizontal flipping for faces. The transformation constraint loss function enforces peaky class distribution for proper inference about the latent object category. The study enforces peaky class distribution for proper inference about the latent object category using Gumbel-Softmax. Additional entropy loss ensures low entropy class distributions for real and fake images, with the objective of generating realistic images and associating latent variables with factors of variation in the data. The study demonstrates the advantage of Elastic-InfoGAN in discovering categorical disentanglement for imbalanced datasets like MNIST and YouTube-Faces. MNIST is a balanced dataset with 70k images, while YouTube-Faces is a real-world imbalanced video dataset with varying training samples for 40 face identity classes. The study focuses on the advantage of Elastic-InfoGAN in discovering categorical disentanglement for imbalanced datasets like MNIST and YouTube-Faces. MNIST has 70k images and is balanced, while YouTube-Faces is a real-world imbalanced video dataset with varying training samples for 40 face identity classes. The smallest/largest class in YouTube-Faces has 53/695 images, totaling 10,066 tightly-cropped face images. Results are reported over multiple runs for both datasets, with a focus on YouTube-Faces due to its representation of real-world data with challenging visual variations and class imbalance. The study compares different baselines for InfoGAN models, including Uniform InfoGAN, Ground-truth InfoGAN, and Gumbel-softmax. It emphasizes the importance of learnable priors for latent variables and applying transformation constraints to improve model performance on real-world data. The study evaluates various models for InfoGAN, including Gumbel-softmax with transformation constraints and entropy loss. Evaluation metrics focus on class-specific disentanglement and recovering ground-truth class distribution in imbalanced datasets. The study evaluates models for InfoGAN, focusing on class-specific disentanglement and recovering ground-truth class distribution in imbalanced datasets. It generates images for latent categorical codes, computes class histograms, and uses Normalized Mutual Information (NMI) to measure correlation between latent category assignments and classifier assignments for fake images. The NMI measures correlation between clusterings, ranging from 0 to 1. RMSE evaluates accuracy of approximating true class distribution in imbalanced datasets. A pre-trained classifier aligns learned and ground-truth distributions by classifying generated images and assigning variables to the most frequent class. Disentanglement quality is assessed using NMI and average entropy. Our approach evaluates disentanglement quality using NMI and average entropy. Compared to Uniform InfoGAN and JointVAE, our model shows significant improvements in NMI and ENT for MNIST and YouTube-Faces datasets. This improvement is attributed to our transformation constraint and the assumption of a uniform categorical prior by JointVAE. Enforcing the network to learn groupings that are invariant to identity-preserving transformations improves disentangled representation. Learning the prior using Gumbel-softmax leads to better categorical disentanglement than fixed uniform priors. Combining Gumbel-softmax with a transformation constraint works better than applying them individually, showing their complementarity. Using a fixed ground-truth prior does not yield the same benefits. Our full model, incorporating a transformation constraint and entropy loss, outperforms the Gumbel-Softmax baseline in approximating class imbalance. The RMSE between the learned prior distribution and ground-truth prior distribution is lowest with our model, indicating the effectiveness of our approach. Our approach, Elastic-InfoGAN, outperforms baseline methods in generating consistent images for each latent code, particularly in the MNIST and YouTube-Faces datasets. It also shows better disentanglement in generating faces of the same person more consistently. Additionally, Elastic-InfoGAN does not hinder the modeling of continuous factors in the imbalanced setting. In this work, a new unsupervised generative model called Elastic-InfoGAN is proposed to learn categorical disentanglement in imbalanced data. The model incorporates continuous latent codes along with existing categorical and noise vectors, demonstrating superior performance over alternative baselines. The results show that each continuous code captures specific factors, such as stroke width and digit rotation. The hope is that this research will inspire others to explore generative modeling of imbalanced data further. For MNIST, the original 28x28 images are used with a 10-dimensional categorical code for digit categories. For YouTube-Faces, faces are cropped using bounding box annotations and resized to 64x64, with a 40-dimensional categorical code for face identities. Different pre-trained architectures are used for MNIST and YouTube-Faces classification. Hyperparameters are set to balance loss terms, and skewed class probabilities can affect behavior. The architecture for YouTube Faces is based on StackGANv2, operating on cropped face images resized to 64x64. It uses a 100 dimensional noise vector and 40 dimensional samples from the Gumbel-Softmax distribution. The generator network takes noise and samples as input, while the discriminator and latent code prediction network share most layers. Skewed class probabilities can impact optimization. The architecture involves generating fake images using noise vector z and samples from the Gumbel-Softmax distribution. The process includes upsampling, convolutional layers, and batch normalization to increase spatial resolution. The fake images are created in stages, with each stage maintaining spatial resolution while transforming the feature representation. The architecture generates fake images in stages using convolutional layers and batch normalization. The discriminator networks consist of 4 convolutional layers with leaky ReLU, while the generator updates based on adversarial loss and other losses. The optimization process alternates between updating the generator, latent code predictor, and latent distribution parameters. For MNIST, baselines are trained for 200 epochs with a batch size of 64, while for YouTube-Faces, training continues until convergence with a batch size of 50. Class imbalances are specified for both datasets. The text discusses evaluating predicted class imbalance in a generative model. The optimization process involves updating the generator, latent code predictor, and latent distribution parameters. Baselines are trained for 200 epochs with a batch size of 64 for MNIST and until convergence with a batch size of 50 for YouTube-Faces, with specified class imbalances for both datasets. In Section 4.2 of the main paper, a metric is derived to evaluate the ability of a generative model to approximate class imbalance. The results show improved RMSE between the approximated and original imbalance distribution. However, there are flaws in the metric related to associating latent codes with ground-truth classes, leading to zero predicted class probabilities for some classes. This issue is more prominent in datasets with a larger number of classes. After deriving a metric to evaluate class imbalance approximation in generative models, it was found that some ground-truth classes had zero predicted probabilities due to multiple latent codes being associated with the same majority class. This strict metric may not be suitable for datasets with confusing categories, as it requires both matching raw probability values and correct class probabilities approximated by latent codes. The generative model must accurately associate latent codes with ground-truth classes to avoid imbalance issues. Evaluation methods like FID score may not capture this accurately. Metrics like min/max predicted priors can provide additional insights. Our method's min/max closely matches the ground-truth values, and the ordering of methods aligns with Table 2 using our RMSE metric. Better metrics are needed to evaluate class imbalance prediction accurately."
}