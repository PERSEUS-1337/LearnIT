{
    "title": "HylYtaVtwS",
    "content": "We investigate the robustness properties of image recognition models with an explicit episodic memory and shape bias at the ImageNet scale. The episodic memory improves model robustness against small-norm adversarial perturbations but not against natural perturbations. Features from a model trained to learn global, shape-based representations enhance robustness against natural perturbations and, when combined with episodic memory, also improve robustness against adversarial perturbations. Three important design choices for the episodic memory are addressed. The episodic memory in ImageNet-trained deep neural networks improves model robustness against small adversarial perturbations but not natural perturbations. To make the episodic memory more compact, reducing the number of memories by clustering them is preferable over reducing their dimensionality. Humans rely more on global shape information for visual recognition, while deep neural networks focus on local texture information. A key difference is that human visual recognition includes a strong episodic component, where familiarity or novelty of an object is automatically and involuntarily recognized. This aspect is lacking in DNNs, which struggle with recognizing specific instances of objects. In this paper, the authors address the deficiencies in standard deep vision models in recognizing familiarity/novelty of objects. They propose a minimal model with explicit episodic memory to make models more human-like and reduce sensitivity to small adversarial perturbations. However, the model does not address sensitivity to larger perturbations or heavy local texture reliance. The authors propose using features from DNNs to address local texture reliance issues in episodic memory, enhancing robustness against adversarial perturbations. Previous work by Zhao & Cho (2018) and Papernot & McDaniel (2018) also explored using episodic cache memory for improving adversarial robustness in image classifiers. The authors propose using DNN features to improve robustness against adversarial perturbations in episodic memory. Previous works did not address robustness to natural perturbations or investigate cache design choices. The authors propose using DNN features to enhance robustness against adversarial perturbations in episodic memory. Recent studies have shown that ImageNet-trained DNNs are more sensitive to natural distortions than human subjects. ImageNet-C benchmark is used to measure the robustness of neural networks against common perturbations and corruptions. The authors propose using DNN features to enhance robustness against adversarial perturbations in episodic memory. They show that explicit cache memory improves adversarial robustness in image recognition models at the ImageNet scale. Various design choices for the cache memory are investigated, such as retrieval method and cache size. The effects of design choices for cache memory, such as retrieval method, cache size, and key dimensionality, are explored. Caching alone does not enhance classifier robustness against natural perturbations. Utilizing global, shape-based features in the cache improves robustness against both natural and adversarial perturbations. Studies suggest humans may also be vulnerable to adversarial perturbations in limited experimental settings. In this study, pre-trained ResNet-50 models are used as feature extractors to build cache models with episodic memory storing low-dimensional embeddings for images. The cache models are similar to \"CacheOnly\" models and consist of keys and class labels. The keys are normalized to have unit l2 norm. When a new test image is presented, similarity between its key and all other keys in the cache is computed to obtain a probability distribution over labels. The distribution is obtained by averaging values stored in the cache weighted by similarity scores. The hyper-parameter \u03b8 acts as an inverse temperature parameter, with larger values producing sharper distributions. The cache model used in this study is called a continuous cache, where all items in the cache are considered and weighted by their similarity to the test item. The study compares two approaches for cache utilization: continuous cache weighted by similarity to test item and nearest neighbor search considering only most similar items. Different choices for the embedding layer were explored, including fc, avgpool, and layer4 bottleneck1 relu in ResNet-50 model implementation. Lower layers as embeddings resulted in significantly worse performance. The study explored different embedding layers in the ResNet-50 model, finding that lower layers led to worse performance. A global spatial average pooling operation was applied to layer4 bottleneck1 relu, resulting in different dimensional keys for fc and other layers. Additionally, a Shape-ResNet-50 model was considered for its more global, shape-based representations compared to a standard ResNet-50. The study compared a Shape-ResNet-50 model to a standard ImageNet-trained ResNet-50, showing that the former produces predictions more aligned with human judgments in texture vs. shape cue conflict experiments. Experiments were conducted on the ImageNet dataset, with a focus on robustness against adversarial and natural perturbations. The largest cache used in the experiments took up approximately 10.5GB of disk space. The study focused on adversarial perturbations with three threat models: white-box, gray-box, and black-box attacks. White-box attacks involve full knowledge of the model and cache items, while gray-box attacks have knowledge of the model but not the cache items. Gray-box attacks are considered more realistic as models used are publicly available but the database is private. The study conducted white-box attacks on the backbone model and tested adversarial examples on the cache model. Black-box attacks were also implemented by using a different model for the attacks. The ImageNet-trained ResNet-18 model was used to generate adversarial examples, and a gradient-based attack method called projected gradient descent was employed. The study conducted white-box and black-box attacks using a RandomStartProjectedGradientDescentAttack with various parameters. Targeted attacks were used to misclassify images, with unsuccessful attacks returning the original image. The study evaluated the robustness of image recognition models against natural perturbations using the ImageNet-C benchmark, which includes noise, blur, weather, and digital perturbations. The mCE metric was used to measure model performance on 3.75M images. The study assessed model robustness against perturbations in ImageNet-C using the mCE measure. mCE is calculated by averaging classification errors over severity levels and perturbation types. Lower mCE values indicate more robust classifiers. Adversarial accuracy was evaluated in gray-box, black-box, and white-box settings for cache models using different layers as embeddings. In the gray-box setting, lower layers showed more robustness with a reduction in clean accuracy, while the layer4 bottleneck1 relu layer achieved the highest gray-box accuracies. In the black-box setting, large perturbation adversarial examples for ResNet-18 were not effective against ResNet-50 or cache models, maintaining performance with a slight decrease in accuracy. In the white-box setting, a divergence in behavior was observed between fc and other layers, with the PGD attack being unsuccessful against the fc layer cache model. The study investigated the adversarial robustness of cache models, attributing easier white-box attacks to gradient obfuscation in the fc layer cache model. Gray-box adversarial examples were effective against the fc layer cache model, with similar results observed using Shape-ResNet-50 as the backbone. Different cache design choices were considered, including size, dimensionality, and retrieval method, impacting clean and adversarial accuracies. Dubey et al. (2019) also explored the adversarial robustness of cache models with large databases. The study focused on enhancing the adversarial robustness of cache models with very large databases by compacting cache memory and using a fast retrieval algorithm. Dubey et al. (2019) reduced key dimensionality and implemented a 50-nearest neighbor method without affecting accuracies. This suggests that continuous cache can be replaced with a 50-nn retrieval method safely. Reducing key dimensionality from 2048 to 256 using online PCA led to a drop in accuracies, while compressing cache size with mini-batch k-means resulted in a smaller decrease in accuracy. This indicates that cluster structure in keys plays a significant role in maintaining accuracy levels. To maintain accuracy levels in caching, it is preferable to reduce the number of items rather than reducing dimensionality. Caching improves robustness against small-norm perturbations but not against natural perturbations, as seen in ImageNet-C. In ImageNet-C, perturbations are visible to the eye and have an average normalized l \u221e -norm of \u2248 1 for all types, larger than the largest adversarial perturbation size of = 0.1. Robustness against such large perturbations may require learning more robust backbone features rather than test-time interventions like caching. Experiments using Shape-ResNet-50 as the backbone were conducted to investigate the effect of different features in the cache. Shape-ResNet-50 is shown to learn more global, shape-based representations compared to ImageNet-trained ResNet-50. Caching with Shape-ResNet-50 leads to similar performance, but significantly improves adversarial robustness in gray-box and black-box settings. However, it does not enhance robustness in the white-box setting. The combination of cache-based episodic memory and shape bias improves the robustness of image recognition models against natural and adversarial perturbations at the ImageNet scale. Caching alone enhances gray-box adversarial robustness, while a shape bias improves natural robustness. Together, they synergistically improve both, with shape-biased models displaying better adversarial robustness. Caching improves adversarial robustness by acting as a regularizer, reducing the Jacobian norm at test points. However, this improvement is only seen against small-norm perturbations, not larger ones like natural perturbations in ImageNet-C. Caching only enhances adversarial robustness in certain threat models, specifically in the gray-box setting. Zhao & Cho (2018) and Dubey et al. (2019) show evidence of improved robustness in black-box settings. Dubey et al. (2019) suggests that larger cache sizes lead to more robust models in gray-box and black-box attack scenarios. However, there is no substantial improvement in white-box settings at the ImageNet scale. White-box setting remains the most challenging for adversarial defense. In the white-box setting, achieving robustness may be more challenging than high generalization accuracy. Natural perturbations in ImageNet-C corrupt local information but preserve global information like shape, making models integrating information over long distances more robust. ShapeResNet-50 achieved this by removing local cues to class labels in training data. Architectural inductive biases can also lead to similar effects. Hendrycks & Dietterich (2019) demonstrated the benefits of feature aggregation. Feature aggregating architectures like ResNeXt are shown to be more robust to natural perturbations than ResNet, indicating better integration of local information into global representations. Insights into cache design choices suggest clustering keys over reducing dimensionality for larger datasets. Continuous cache retrieval method can be used for very large datasets. In future work, the method using the entire cache for predictions can be replaced with a k-nearest neighbor retrieval algorithm like Faiss without sacrificing accuracy. The choice of backbone architecture, such as Shape-ResNet-50 vs. ResNet-50, impacts the cache's effectiveness against natural and adversarial perturbations. Future research aims to explore robust features for few-shot recognition tasks and modeling neural and behavioral data from humans and monkeys."
}