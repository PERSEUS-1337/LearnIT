{
    "title": "Byx-FknVtS",
    "content": "Despite promising progress on unimodal data imputation, models for multimodal data imputation are lacking. The variational selective autoencoder (VSAE) proposed in this work can model joint distribution of observed/unobserved modalities and imputation mask, improving imputation for various tasks. Evaluation on synthetic datasets shows significant enhancement over existing models. Modern deep learning relies on large clean datasets, but partially-observed data can be valuable for learning and extracting information. A model that learns from partially-observed data can expand the application range of deep learning models, benefiting tasks like data imputation. Challenges remain in developing effective imputation models, with some focusing on fully-observed data and making strong assumptions about missing data mechanisms. Limited exploration exists for multimodal imputation, hindering the potential of such models in handling real-life raw data. The proposed Variational Selective Autoencoder (VSAE) aims to address the challenge of multimodal data imputation by modeling the joint distribution of data and mask, avoiding assumptions like MCAR. VSAE utilizes unimodal/multimodal proposal networks and is optimized efficiently with all components trained jointly. The Variational Selective Autoencoder (VSAE) is a novel framework for learning from partially-observed multimodal data. It can model the joint distribution of observed and unobserved modalities and the mask, allowing for data generation and imputation without strict assumptions on missing data mechanisms. Evaluation on various datasets shows improvement over existing imputation models. The Variational Selective Autoencoder (VSAE) is a framework for learning from partially-observed multimodal data by modeling the joint distribution of observed and unobserved modalities. It allows for data generation and imputation without strict assumptions on missing data mechanisms. The model encodes data to a latent space factorized with respect to modalities, selectively choosing between unimodal and multimodal encoders based on modality observation. The joint distribution of data and mask is modeled using the VAE formulation to derive the Evidence Lower Bound (ELBO) for log p(x, m). The ELBO for log p(x, m) with approximate posterior q(z|x, m) is derived following the VAE formulation. The proposal distribution for each modality is defined based on the assumption that the latent space of each modality is independent of the others. In the partially-observed setting, the objective function for training is defined by taking the expectation of the ELBO over the unavailable modality x u. The final objective is derived by approximating log p \u03b8 (x j |m, z) by sampling x j from the prior network and passing through the decoder. Even a single sample is sufficient for effective model learning. The prior network can serve as a self-supervision mechanism to identify the most likely samples. The model is evaluated on high-dimensional multi-modal and low-dimensional tabular data, compared with other latent variable models. Robustness is tested under various challenging missingness mechanisms, with results shown in Table 1 for data imputation. VSAE trained with partially-observed data outperforms other models on some datasets. This is attributed to the mask providing a natural dropout on the data space and the ability to ignore noisy or outlier data in low-dimensional datasets. Figure 2 shows VSAE's robustness to missing data ratios. VSAE is more robust to missing data ratios, as shown in Figure 2. Two bimodal datasets are synthesized using MNIST and SVHN datasets. VSAE performs better with lower variance compared to other baselines. The model is able to ignore unobserved noisy modalities and focus on observable useful data. VSAE can easily ignore unobserved noisy modalities and focus on observable useful data. It can learn mask distribution and improve performance by conditioning on the reconstructed mask in the data decoders. The mask vector informs the data decoder about missingness in the data space. The imputation process involves learning a generative distribution for unobserved missing data in all modalities. The binary mask vector indicates observed and unobserved modalities, with parameters estimated through maximizing marginal likelihood. Little and Rubin (1986) characterize the missingness mechanism. The missing data values are characterized by Little and Rubin (1986) in terms of independence relations. Imputation methods like MICE and MissForest work under MCAR or MAR assumptions. Recent deep learning models for imputation include autoencoders and GANs. Recent deep learning models for imputation include generative adversarial nets (GANs) and autoregressive models. The GAN-based imputation method GAIN assumes data is missing completely at random but does not scale well to high-dimensional multimodal data. Various VAE-based data imputation methods have been proposed, such as VAEAC which allows generation of missing data conditioned on observed data, but requires complete data during training. Other methods modify the VAE formulation to model the likelihood of observed data only, but require training a separate generative network for each dimension. Our method models the joint distribution of observed and unobserved data, including the missingness pattern, enabling both data generation and imputation without strict assumptions on the missingness mechanism. Previous works focus on conditional likelihood or joint distribution of modalities, but struggle with incomplete data or arbitrary conditioning. Our model, building on multimodal VAE approaches, addresses the limitations of existing methods by learning multimodal representations from partially observed training data and performing data imputation from any subset of modalities during testing. It leverages a shared latent space for all modalities and can handle missing data efficiently. The model utilizes factorized multimodal representations in the latent space resembling disentangled models. It employs a Variational Autoencoder (VAE) as a probabilistic generative model with an inference and generation network to encode and decode data. The model approximates the true intractable posterior with a proposal distribution and is trained until reconstructions match the training data. The inference module in the model uses an encoder to produce the sufficient statistics of the approximation posterior q\u03c6(z|x). The training criterion is to maximize the evidence lower bound (ELBO) with a diagonal normal distribution for the approximate posterior and prior. Conditional VAE approximates the conditional distribution p(x|y) by introducing a conditional input. The model architecture of CVAE includes neural networks for encoders and decoders, with different modalities represented by colors. The architecture integrates two auto-encoding structures and uses a selective proposal distribution to choose between unimodal and multimodal encoders. A common latent space is shared among all decoders. In the proposed model, selective factorized encoders are used to handle incomplete input data by combining unimodal and multimodal proposal distributions. The model can impute missing information by inferring latent variables from observed modalities and masks. The proposed model utilizes selective factorized encoders to handle incomplete input data by combining unimodal and multimodal proposal distributions. This allows the model to impute missing information by inferring latent variables from observed modalities and masks. The encoder outputs latent codes for all modalities, modeling proposal distributions as normal distributions with mean and variance parameters. Unimodal proposal distributions are determined by deterministic neural networks, while the multimodal proposal distribution is modeled by a neural network. The multimodal proposal distribution is modeled by a neural network with inputs x, o, and m. Reparameterization in standard VAE allows for end-to-end training. Variational latent codes are aggregated before decoding, using functions like concatenation, max/mean pooling, or matrix fusion. Decoders generate data and mask based on shared aggregated latent codes. Mask variable m is encoded for decoding. The mask variable is encoded into the latent space through the multimodal proposal network, shared by the mask and data decoders. The mask decoder reconstructs the binary mask vector using aggregated latent codes. The reparameterization trick allows for joint optimization of the objective on the training set without unobserved modalities. KL-divergence is calculated analytically for each factorized term. The learned model can be used for data imputation and generation. For imputation, observed modalities x o and mask m are fed through encoders to infer selective proposal distributions. Sampled latent codes are decoded to estimate unobserved modalities x u. For generation, latent codes are sampled from the prior and go through decoders to generate data and mask. Only modules after the aggregator are used without any inference modules. All layers are modeled by MLP without skip connections or resnet modules. Unimodal encoders infer unimodal proposal distribution from single modality data vector input. The encoders infer unimodal and multimodal proposal distributions from input data vectors. In the UCI repository experiment, unimodal encoders for numerical and categorical data are modeled using 3-layer 64-dim MLPs. Our method replaces unobserved modality vectors with noise, such as standard normal noise or zeros in the Bimodal experiment. Our model is implemented using PyTorch. In the UCI repository experiment, unimodal encoders for categorical data are modeled by 3-layer 64-dim MLPs with Batch Normalization and Leaky ReLU activations. Different dimensionalities are used for unimodal encoders in MNIST+MNIST and MNIST+SVHN bimodal experiments. The latent dimensions are set as 20-dim for UCI data and 256-dim for Bimodal experiments. Multimodal fusion models can be used for mapping observed data and masks to latent variables, but a similar architecture to unimodal encoders is used in this paper. The multimodal encoders in the experiment use full-dimensional input with noise or zeros for unobserved modalities. Vector concatenation is used as the aggregation method, and Adam optimizer is employed for all models with varying learning rates for different experiments. In bimodal experiments, the model is trained for 1000 epochs with a learning rate of 1e-4. Conditional log-likelihood is calculated by generating corresponding modalities from prior. The model is initially trained without calculating the log-likelihood, then fed partially-observed data to generate unobserved modalities. Evaluation is done on numerical datasets and MNIST+MNIST with different missing ratios. Table 6 shows imputation results on MNIST+SVHN dataset with missing ratios of 0.3, 0.5, and 0.7. The last two rows are trained with fully-observed data and evaluated based on combined errors of two modalities. Lower error rates indicate better performance."
}