{
    "title": "rJeWvcsPiQ",
    "content": "Deep neural networks, like DenseNet, have shown high performance in various tasks but suffer from high computational costs due to dense connections. To address this, a reinforcement learning framework is designed to efficiently prune DenseNet architectures while maintaining key advantages. This framework evaluates connection importance between block layers and introduces a reward-shaping trick to balance accuracy and computational costs. Our experiments show that DenseNet with Layer-wise Pruning (LWP) is more compact and efficient than existing alternatives. Previous studies have highlighted the high redundancy in DenseNet, making it challenging to apply neural architecture search methods efficiently. Therefore, a layer-wise pruning method is proposed to develop an adaptive strategy for searching an on-demand neural network structure for DenseNet, balancing computational budget and inference accuracy. Our proposed layer-wise pruning method for DenseNet, based on reinforcement learning, aims to prune weights and connections while maintaining accuracy on the validation dataset. The agent learns to output actions and receives rewards based on the network structure. The controller makes decisions for each layer, specifying connections within a range. This method enables effective pruning of neural networks. Submitted to NIPS 2018. The DenseNet connections form the agent's action space. Traversing this space is NP-hard. The output vector represents connection likelihoods. The network architecture is determined by sampling actions. The reward function encourages less computation. To maximize policy gradient training, advantage actor-critic (A2C) with state value function estimation is used. The training procedure includes curriculum learning, joint training, and training from scratch. Results on CIFAR show higher test error with more parameters. Layer-wise pruning is detailed in Algorithm 1. The training process includes curriculum learning, joint training, and training from scratch. Child networks are sampled from a distribution and the one with the highest reward is selected for further training. Algorithm 1 outlines the layer-wise pruning procedure. In this section, the proposed methods aim to learn more compact neural network architecture by analyzing the number of input channels in DenseNet layers and the connection dependency between convolution layers and their preceding layers. The input channel being 0 indicates a dropped layer, reducing the block layers. The connection dependency is illustrated in FIG3, where the values connecting the same target layer are almost equal, indicating the dependency on the source layer. The child network learned from vanilla DenseNet is compact and efficient, with small square connections in the same target layer having almost equal values, indicating a consistent dependency on preceding layers."
}