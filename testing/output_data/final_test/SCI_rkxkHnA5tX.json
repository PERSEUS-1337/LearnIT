{
    "title": "rkxkHnA5tX",
    "content": "State-of-the-art imitation learning algorithms assume the optimality of the demonstration set, which may hinder an agent's performance in acquiring skills due to noise and diversity in the demonstrations. In this paper, the focus is on learning from the most suitable demonstrations in a set, based on their ability to produce desirable outcomes for task goals. The framework, built on Model-Agnostic Meta-Learning, quickly assesses demonstration suitability and selects subsets for better skill acquisition. Videos of experiments can be found at: https://sites.google.com/view/deepdj. Learning from noisy demonstration sets is crucial in robot imitation learning, as unsuitable or irrelevant content can hinder the learning process. State-of-the-art imitation learning algorithms often assume optimality in demonstrations, making them vulnerable to potentially detrimental outcomes. To address this, a framework based on Model-Agnostic Meta-Learning quickly assesses demonstration suitability for better skill acquisition. In this paper, a generic framework is proposed to learn from noisy demonstration sets by evaluating the suitability of imitated skills judged by task specific heuristics. Unlike prior works, this framework does not require demonstrations to contain specific information, making it capable of coping with any forms of expert demonstrations. The proposed framework aims to selectively learn from suitable demonstrations by assessing learning outcomes after imitating each demonstration. Specific heuristics are predefined to evaluate the agent's performance in reaching task goals. The key challenges include efficiency and generalization ability in assessing outcomes quickly and generalizing to unseen demonstrations. The framework focuses on demonstration suitability for effective learning. The framework proposed aims to assess the suitability of demonstrations through meta-learning, allowing for quicker adaptation to new demonstrations. The adapted outcomes are evaluated using task heuristics to select suitable demonstrations for training agents. Two empirical approaches are demonstrated to utilize the suitability assessments. The framework fine-tunes meta-learned parameters by adjusting demonstrations based on suitability judgments, addressing imbalanced or multi-modal distribution issues. Regularization objective maximizes mutual information between demonstrations and induced behavioral differences to enhance adaptability. Tested on MuJuCo simulation sports environments, results show superiority over vanilla MAML and other baselines. The proposed method outperforms various baselines in learning better policies from noisy demonstration sets. Imitation learning involves acquiring skills from expert behaviors, formulated as adversarial training between demonstrations and imitated behaviors (GAIL). Extension works of GAIL augment the algorithm for imitating a diverse set of demonstrations. BID9 trains a deep q-network using expert demonstrations for one-shot imitation learning. The goal is to learn better skills from selected suitable demonstrations, even when the demonstrations are imperfect. Various experts violating optimality assumptions are discussed, with ways to cope with them. Tomasello (2016) shows infants imitate rationally, preferring demonstrations aiding skill development. Grollman & Billard (2011) use failure demos to prevent mimicking undesirable behaviors. BID14 treats limited demos as optimization problem. Recent works (BID5, BID17) filter bad actions using expert Q-values. However, this info may not apply to learning from raw video demos. The framework illustrated in Figure 1 involves meta-imitation training adaptive parameters on a demonstration set, followed by assessing imitation learning outcomes during the meta-testing phase. The goal is to create a general framework capable of handling various forms of demonstrations, even though the current work is focused on state-action pairs. This work is related to recent advancements in meta learning, aiming to develop fast adaptive parameters for learning from demonstrations. The model-agnostic framework (MAML) is used to train adaptive parameters for one-shot imitation learning during meta-testing. This extends MAML with regularization for evaluating a demonstration set to improve policy learning. The Markov Decision Process (MDP) is defined with state and action spaces, state transition probability, reward function, initial state distribution, reward discount factor, and horizon. In imitation learning, expert behaviors are demonstrated without a known reward function. The goal is for the agent to learn from these demonstrations to acquire desired skills. The problem setup involves a noisy demonstration set and multiple environment variants in a multi-goal learning task. Suitability assessment scores are needed to judge and select suitable demonstrations for different goals. The assessment scores are used to select suitable demonstrations for imitation learning. The adaptation process in meta-learning framework is utilized to learn a good policy from a noisy demonstration set. The inner imitation objective of MAML is regularized to alleviate insensitivity to unbalanced set distribution. In the meta-imitation-learning framework, the inner objective of MAML is regularized by maximizing mutual information between the demonstration and induced behavioral differences. The policy model is updated through gradient descent with learning rate \u03b1 to adapt to individual demonstrations, aiming for fast adaptation across multiple demonstrations. In the meta-imitation-learning framework, the inner objective of MAML is regularized by maximizing mutual information between the demonstration and induced behavioral differences. The policy model is updated through gradient descent with learning rate \u03b1. The imitation loss imitates the sampled demonstration vi, leading to the meta objective and meta-imitation-update with meta learning rate \u03b2. Two major imitation learning algorithms are adopted: generative adversarial imitation learning and behavioral cloning. Generative Adversarial Imitation Learning (GAIL) involves maintaining a discriminator Di for each demonstration vi in the GAIL setting, contributing to the inner update from \u03b8 to \u03b8i using the TRPO update rule with reward -log(Di(s, a)). In the meta-imitation-learning framework, the reward is denoted as R IL and the sparse task reward as R task. To optimize quick adaptation to any given demonstration, the R task term is dropped from the reward definition. A meta-discriminator D meta is trained along with adaptive policy parameters \u03b8 to adapt to unseen demonstrations. The discriminator parameters D meta,i are updated during the inner update for each demonstration vi in the GAIL setting. In meta-imitation-learning, the inner update follows the GAIL rule for discriminator. Behavioral cloning involves supervised training with a mean squared error loss. Regularization loss is added to balance demonstration distribution. Maximizing mutual information between induced behavioral differences and demonstrations is crucial. Posterior network Q is used for optimization. The variational lower bound is optimized using a siamese LSTM network to train the posterior network Q in meta-imitation-learning. Regularization term RMI is added to the reward function in GAIL to balance demonstration distribution. The regularization term is applied to the parameters before the imitation update in meta-imitation learning. An additional value network is used for the regularization reward. Policy parameters are updated using policy gradients from RMI, while other parts remain as standard supervised training. During meta-testing, suitable demonstrations are evaluated and selected. During meta-testing, suitable demonstrations are evaluated and selected based on task heuristics under environment variant. A scoring function K assesses the demonstration suitability, with the most suitable one(s) chosen for imitation. Two empirical approaches are used to select a subset of suitable demonstrations for learning. The approach for selecting a subset of suitable demonstrations involves using a weighted sum of gradients during meta-update. The weight for each demonstration is adjusted based on assessment scores, and a subset is chosen based on a predefined threshold. Pseudo code for this process is included in the appendix. The curr_chunk discusses methods for assessing and ranking demonstration suitability, evaluating different frameworks for learning from noisy demonstration sets in various environments. The term \"DSA\" is used for demonstration suitability assessments. Four experimental environments are used for evaluation, including Avg Fine-Tune, MAML, and MAML + MI frameworks. Four simulation sports environments are illustrated in FIG1. The curr_chunk discusses simulation sports environments designed to resemble real sports activities. Different datasets are tested for tasks like Free Throw and Penalty Save, with demonstrations from RL pre-trained agents. The goal is to assess demonstration suitability and the impact of learning agents on successful demonstrations. The curr_chunk discusses various tasks in simulation sports environments, including blocking penalty shots, handstand poses, and jump kicks. Different datasets with demonstrations from RL pre-trained agents are evaluated using metrics like Top-K combinations. The curr_chunk discusses combining top-K ranked demonstrations to create potentially better datasets for agents learning from imitation in simulation sports environments. Different subsets from top-1, top-2, top-N/2, and top-N are compared based on performances. In the noisy set, subsets from top-1, top-2, top-N/2, and top-N are selected for learning. MAML + MI outperforms other baselines in Free Throw, Penalty Save, and Handstand tasks by selecting the most suitable demonstrations from the set. MAML + MI outperforms other baselines by selecting the best subsets, achieving task goals efficiently. The top-1 demonstration is crucial, while top-2 can have a negative impact. Avg Fine-Tune combinations perform worse due to selecting detrimental demonstrations. In the MA environment, MAML + MI succeeds with the top-1 demonstration, while OSS algorithm shows promising results in generalization to unseen demonstrations. The learning reward curves show that learning from the selected subset by the OSS significantly improves performance in the MAML + MI case. Videos demonstrating how meta-trained parameters adapt to demonstrations are available on the project page. MAML + MI outperforms other baselines by adapting better to sampled demonstrations, especially in the free throw environment. The efficiency of MAML in adapting to sampled demonstrations is verified by setting a limited quota for meta-training and gradient updates. Training different agents to imitate demonstrations from a noisy set results in non-suitable choices, as shown in the reward curves. We propose a framework for learning a good policy through imitation learning from noisy demonstrations. The framework, based on MAML with mutual information maximized regularization, adapts to demonstrations for significant learning outcomes. It aims to discover suitable demonstrations from experts without relying on hand-crafted rules, with potential for future research in imitation learning from noisy sets. The framework proposed involves learning a policy through imitation learning from noisy demonstrations using MAML with mutual information maximized regularization. Task heuristics for different environments are described, such as minimum distances and body pose conditions. Policy networks are built for each environment using a 2-layer MLP, with a posterior network Q implemented in MAML + MI. The framework involves learning a policy through imitation learning from noisy demonstrations using MAML with mutual information maximized regularization. A siamese LSTM network is used for the posterior network Q, followed by fully connected networks to produce the 1-dimensional embedding. Meta batch size of 10 is used for training with up to 2,000 meta updates. The best MAML models with average return are chosen as adaptive parameters. The learning rate in the inner objective is set to \u03b1 = 1.0 with KL-divergence constraints set to 0.15. During meta-testing, the meta-learned parameters are adapted to each demonstration one at a time. The network architectures for policy networks and posterior networks Q are summarized in Table 3. The proposed method for training the fast adaptive set of parameters is outlined in Algorithm1, while Algorithm2 details the algorithm for meta-testing. Terminologies used in Sec. 3 and Sec. 4 are consistent throughout. The paper introduces MLP and Siamese-LSTM architectures for meta-learning. It also describes Algorithm3 for subset selection and Algorithm1 for Info Meta Imitation Learning. The algorithms involve initializing parameters and sampling demonstrations and environments for training. The paper introduces MLP and Siamese-LSTM architectures for meta-learning, along with Algorithm3 for subset selection and Algorithm1 for Info Meta Imitation Learning. The algorithms involve initializing parameters and sampling demonstrations and environments for training. In Fig.5, adaptation curves from each method in various environments are shown. Both top-1 and top-2 obtained by MAML + MI are suitable demonstration subsets. In Fig.6, demonstration subsets obtained by MAML + MI are analyzed. Demonstration 2, recorded from a taller agent, is not selected despite success in the task. The process involves initializing parameters, sampling trajectories, and meta-training. The parameters \u03b8 are updated using TRPO with gradient descent. The DSA score is computed for M random seeds to find the most suitable demonstration. Weighted meta-learning is applied to update \u03b8 for N iterations. The parameters \u03b8 are updated using TRPO with gradient descent. Weighted meta-learning is applied to update \u03b8 for N iterations, applying weights W for the demonstrations during meta-update. The framework can select suitable demonstrations from a large set, with MAML + MI eventually succeeding in achieving the task goal. Learning reward curves using behavioral cloning as the inner imitative update in Eq.1 on harder environments is also shown. In the handstand environment, MAML + MI outperforms other baselines, suggesting the top 7 demonstrations selected are more suitable. MAML also performs well, almost on par with MAML + MI. Behavioral cloning does not introduce much randomness, but cannot produce perfect results due to task complexity. For martial arts, both MAML + MI and MAML yield reasonable results, with MAML + MI achieving better overall performance. The Avg Fine-Tune method surprisingly works well with behavioral cloning due to supervised training steadiness. In the handstand environment, MAML + MI outperforms other baselines, suggesting the top 7 demonstrations selected are more suitable. MAML also performs well, almost on par with MAML + MI. Behavioral cloning does not introduce much randomness, but cannot produce perfect results due to task complexity. For martial arts, both MAML + MI and MAML yield reasonable results, with MAML + MI achieving better overall performance. The Avg Fine-Tune method surprisingly works well with behavioral cloning due to supervised training steadiness. In subset selection training, MAML + MI diverges consistently and produces more reasonable final results compared to training from scratch. Training from scratch is inefficient and prone to randomness in reinforcement learning scenarios."
}