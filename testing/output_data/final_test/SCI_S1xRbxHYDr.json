{
    "title": "S1xRbxHYDr",
    "content": "Deep neural networks (DNNs) are often overparametrized but can adjust complexity by developing compressible representations, as shown in recent studies. This paper presents new empirical evidence that increasing network width leads to the emergence of robustness and redundancy mechanisms. Experiments with various networks and datasets demonstrate that DNNs enhance either robustness, redundancy, or both at wider widths. Deep neural networks adjust their capacity by increasing robustness or redundancy at greater widths for various hyperparameters. Overparametrization allows DNNs to memorize datasets, even with randomized labels, and larger networks often generalize better than smaller ones. The capacity of deep neural networks is adjusted by increasing robustness or redundancy at greater widths for various hyperparameters. Overparametrization allows networks to memorize datasets, even with randomized labels, and larger networks often generalize better than smaller ones. Novak et al. (2018) demonstrated this for modern networks trained in ImageNet and CIFAR-10, raising questions about how overparametrized networks can perform well in structured tasks without overfitting. Previous studies have aimed to uncover why networks tend toward optima that generalize well, often by proving generalization bounds for simple models related to weight matrix norms or Rademacher complexity. Frankle & Carbin (2018) showed that crucial computations in certain networks were performed by sparse structures. Carbin (2018) found that large networks can perform as well as or better than smaller ones due to the presence of sparse subnetworks. They investigated the relationship between network overparametrization, robustness, and redundancy in deep neural networks. The study connected the compressibility of DNNs to their generalization ability, showing that increasing network size leads to higher degrees of robustness and redundancy. Increasing the size of deep neural networks can lead to higher degrees of robustness and redundancy. Large networks may adjust their complexity to the task at hand, as shown empirically. However, the mechanisms behind this phenomenon have not been empirically investigated yet. In this work, six capacity-constraining features are discussed to prevent overfitting in neural networks: redundant functional units, redundant blocked units, nonredundant blocked units, silent units, constant units, and semantically redundant units. The study focuses on measuring the robustness of large models by applying ablations to units in a DNN layer. Robustness is defined as the network's ability to output consistent labels when units are perturbed. Ablations are randomly applied to different proportions of units to create an ablation curve, with the area under the curve used as a metric for robustness. This measure helps distinguish features where robustness increases with the number of units from those where it remains unchanged. The study measures model redundancy through compressibility and similarity metrics in DNN layers. Compressibility quantifies the principal components needed to explain variance, while similarity measures correlated units. Features are distinguished based on increasing compressibility and stable similarity as the number of units (n) grows. The study analyzes model redundancy in DNN layers using compressibility and similarity metrics. It compares small (S) and large (L) deep networks trained on dataset D, showing scenarios where L is more robust and redundant than S. The study compares small (S) and large (L) deep networks trained on dataset D, showing scenarios where L is more robust and redundant than S. L has halved output weights with duplicates, making it less robust to ablation but more redundant. Another scenario is the addition of silent units, making activation vectors more compressible but not more similar. L can be more robust and equally or less redundant than S if they learn qualitatively different representations. The study compares small (S) and large (L) deep networks trained on dataset D, showing scenarios where L is more robust and redundant than S. L has halved output weights with duplicates, making it less robust to ablation but more redundant. Another scenario is the addition of silent units, making activation vectors more compressible but not more similar. L can be more robust and equally or less redundant than S if they learn qualitatively different representations. The central hypothesis is that L can learn more complex bag-of-features class representations, making it more robust without necessarily being more redundant. The study compares small (S) and large (L) deep networks trained on dataset D, showing scenarios where L is more robust and redundant than S. The central hypothesis is that L generalizes as well or better than S, and is more robust and/or redundant due to autoregularization effects. Various factors like batch normalization, data augmentation, dropout, and L2 weight decay are considered in network training and performance analysis. Multiple variants of tasks, networks, initializations, sizes, architectures, optimizers, and regularizers are tested to investigate redundancy and robustness in machine learning research. In experiments testing network variants with different levels of overparametrization, various models were used including MLPs for synthetic data, scaled-down AlexNet and ResNet56s for CIFAR-10, and ResNet18s and Inception-v3 for ImageNet. Testing accuracy results are shown in Figure 1. The testing accuracy of different network sizes on ImageNet and CIFAR-10 datasets was shown in Figure 1. Increasing model size led to improved performance, with the number of trainable parameters increasing exponentially. The experiments used ImageNet with 1 million training images and CIFAR-10 with a 50,000/5,000/10,000 train/validation/test split. Small-scale experiments with MLPs used synthetic data generated by teacher MLPs. MLPs with binary output identical in architecture to the 1/4x MLP models were trained and evaluated on datasets of 1,000 examples. Teacher networks output each label for 40-60% of random inputs. Networks were trained for a fixed number of iterations, except for CIFAR-10 AlexNets trained to the epoch of maximum validation accuracy. Trends in robustness and redundancy were invariant to training time after convergence in ResNet18s and other models. Initializations used random normal distributions for MLP and Inception-v3 networks, while AlexNet and ResNet models used Xavier/Glorot initialization. In the AlexNets, various initialization methods were experimented with, including Xavier/Glorot, LeCun, and He, along with uniform distributions. Different optimizers were used across models, such as RMSProp and momentum optimizer. Regularization techniques like data augmentation, dropout, and weight decay were also tested. Learning rates and batch sizes were varied, with joint adjustments affecting training time more than performance. The study experimented with varying learning rates and batch sizes in neural network models to analyze their impact on robustness and redundancy. Due to the complexity of the models and datasets, only a sampling of units was analyzed. Results were averaged across multiple samplings and model replicates, with error bars showing standard deviation. The study analyzed the impact of model size on robustness and redundancy in modern machine learning research. Results show that larger models exhibit increased robustness, with ResNet18 models becoming more robust as their size increases. However, redundancy varies between models, with ResNet18s showing more compressibility and similarity, while Inception-v3s lose compressibility. The discrepancy between robustness and redundancy trends in modern machine learning models, particularly Inception-v3s, suggests autoregularization by forming different representations at various sizes. Compressibility and similarity do not correlate well, especially in ResNet18s, possibly due to varying proportions of redundant units. Robustness and redundancy develop in networks trained on randomly-labeled data, raising questions about how models constrain their capacity for specific tasks. In ResNet56 models, fitting random labels increased redundancy but caused robustness to plateau. Robustness and redundancy do not determine generalization capability, as they are influenced by initialization variance. Testing different initialization levels on AlexNets showed that robustness increases with model size for low-variance Glorot initialization. In ResNet56 models, fitting random labels increased redundancy but caused robustness to plateau. Robustness and redundancy do not determine generalization capability, as they are influenced by initialization variance. Testing different initialization levels on AlexNets showed that robustness increases with model size for low-variance Glorot initialization. LeCun and He-initialized nets show different trends in redundancy and robustness, with He-initialized nets exhibiting a positive trend in redundancy but not robustness. AlexNets with different initializations show similar results, indicating that initialization distribution matters little compared to variance. In MLPs trained on uncorrelated data, altering initialization variance affects redundancy at larger model sizes. High variance does not increase redundancy or robustness in MLPs on low-dimensional data, suggesting a relationship with high dimensionality. The robustness-redundancy hypothesis applies to deep models but faces limitations with single-layer MLPs on high-dimensional, uncorrelated data. The data suggests that DNN models may operate with unique representations or nonredundant units at larger sizes. The relationship between a layer's depth and its robustness or redundancy is inconsistent. Analysis of trends in networks trained with and without explicit regularization shows changes in redundancy or robustness. In this study, the influence of optimizers on the redundancy and robustness of CIFAR-10 AlexNet models was examined. Different optimizers like SGD, momentum, and Adam were tested, showing varying effects on redundancy and robustness but not changing overall trends. Additionally, experiments varying learning rate and batch size in ResNet18, ResNet56, and AlexNet models trained on ImageNet and CIFAR-10 had little impact on outcomes. The analysis focused on activations, making the results contextually relevant to input data. Our results show that redundancy and robustness are crucial for networks to autoregularize when their size is increased. Different networks exhibit varying compressible features, suggesting that pruning unimportant units and compressing redundant units could be complementary for developing new compression algorithms. Redundancy is influenced by network initialization, while accuracy is not affected. Certain compression techniques could be improved by validating over multiple initializations to produce maximally redundant models. Redundancy and robustness may not predict generalization, as shown in related works by Zhou et al. (2018), Arora et al. (2014), and Morcos et al. (2018b). Maennel et al. (2018) shows that model networks in the overparametrized regime tend to develop weight vectors that align to a set. Model networks in the overparametrized regime align weight vectors to discrete directions determined by input data. This study quantitatively explores connections between overparametrization, robustness, and redundancy in various networks, revealing unique trends in robustness and compressibility. Larger model sizes lead to increased redundancy and robustness, influenced by initializations and displaying high variance. In this paper, the authors analyze the robustness and redundancy of deep neural networks, highlighting the impact of initializations and variance on network behavior. They discuss the similarities between deep networks and kernel machines, suggesting that networks with high-variance initializations can operate in a kernel-like regime. The study examines the relationship between robustness and redundancy in deep neural networks, showing that they increase with overparametrization. This sheds light on capacity-constraining features that support generalization and prevent overfitting, suggesting new research directions in learning theory and compression. The study explores the emergent properties of overparametrized neural networks using ResNet18s from He et al. (2016) for the ImageNet dataset. The networks consisted of initial convolution and batch norm layers, followed by 4 building block layers with 2 blocks each. Training was done on the ILSVRC 2012 dataset with SGD optimization and evaluation on 50,000 validation images. For Inception-v3s, training was done for 90 epochs on the ILSVRC 2012 dataset with batch normalization, data augmentation, and weight decay. Evaluation was performed on 50,000 validation images using the RMSProp optimizer. The network used for the CIFAR10 dataset was a scaled-down version of AlexNet with 5 layers, including convolutional and dense layers. Filters of 5x5 with stride 1 were applied in the convolutional layers, followed by max-pooling with a 3x3 kernel and stride 2. Local response normalization was applied after pooling. All activations were ReLU, and bias terms were included in each layer. The model was trained with an epsilon of 0.001 to prevent division by zero. The 1x sized model used 96 and 256 filters in convolutions, with 384 and 192 units in dense layers. Trained on 45,000 images with early stopping, the test set had 10,000 images. Optimized with SGD, weights decayed by 5% every epoch. ResNet56s from He et al. (2016) for CIFAR-10 had 3 building block layers with 9 blocks each. In the 1x sized model, convolutions used 16, 32, 64, and 128 filters. Trained on 45,000 images for 182 epochs with Xavier/Glorot initialization. Testing done on 10,000 images. Used SGD optimization with momentum 0.9, batch normalization, data augmentation, and weight decay. Simple multilayer perceptrons with 128 units in hidden layer. Trained for 50 epochs. The study trained networks with different sizes and regularization techniques on CIFAR-10 data. Similar neurons were identified based on Pearson correlation coefficients, with more similarities in certain layers. Regularization affected the amount but not the trends of robustness and redundancy in the networks. Regularization techniques like data augmentation, dropout, and weight decay positively impact generalization performance in AlexNets trained on CIFAR-10 data. While weight decay has a slight effect, data augmentation reduces robustness, compressibility, and similarity. Dropout and weight decay, on the other hand, have a significant positive effect on robustness and redundancy. These techniques do not change the general trends in robustness and redundancy but scale the curves up or down. Regularization techniques like data augmentation, dropout, and weight decay positively impact generalization performance in AlexNets trained on CIFAR-10 data. The learning rate and batch size factor have a slight effect on compressibility in AlexNets and ResNet56s. Varying a constant factor k from 1/4 to 4 as a multiplier for batch sizes and learning rates shows that compressibility tends to be slightly higher with higher k."
}