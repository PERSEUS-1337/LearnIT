{
    "title": "S18Su--CW",
    "content": "Thermometer encoding is a simple modification to neural network architectures that enhances robustness to adversarial examples. Experimental results on various datasets show increased accuracy on adversarial examples without compromising generalization. Notably, state-of-the-art accuracy against white-box attacks improved from 93.20% to 94.30% on MNIST and from 50.00% to 79.16% on CIFAR-10. Adversarial examples are inputs intentionally designed to cause machine learning models to produce incorrect outputs. They demonstrate that even models with superhuman performance fail on slightly modified inputs. There is currently no strong defense against adversarial examples, raising concerns in machine learning security. Adversarial examples can fool different models, making it possible for attackers to deceive a model without direct access. Attackers can enhance their success rate by observing a model's output and creating their own copy. Defending against adversarial examples through adversarial training was proposed but generating enough examples was challenging. Adversarial training methods like FGSM, BIM, and L-BFGS have been developed to reduce error rates on adversarial examples. However, models trained with one method tend to overfit and fail against other methods. BID9 introduced BIM as a compromise between speed and effectiveness, but even this method can still be exploited by attackers. In this work, the authors demonstrate that using thermometer code discretization and one-hot code discretization of real-valued inputs significantly improves a model's robustness to adversarial attacks. This advancement in the field aims to break the linear extrapolation behavior of machine learning models by preprocessing inputs with a highly nonlinear function while still maintaining successful performance on natural inputs. The recent success of the PixelRNN model has shown that one-hot discrete codes for color pixels are effective representations for input data. Images are encoded as a 3D tensor of integers in the range [0, 255], with each value representing an intensity value for a given color at a specific position. Input discretization is the process of normalizing these values for classification tasks. The approach involves mapping continuous-valued pixel inputs to binary vectors instead of low-bit versions of the original values. This allows different input values to activate different bits of the input vector, enabling the network to use different parameters for each input value. Discretization involves mapping continuous-valued inputs to binary vectors, enabling the network to use different parameters for each input value. Unlike quantization, discretization makes a meaningful change to the model and can defend against adversarial examples by changing which parameters are used. The authors provide evidence that various network architectures are vulnerable to adversarial examples due to the linear nature of their loss functions. They discuss the impact of perturbations on the model's output probabilities and highlight the susceptibility of models to adversarial attacks. The linear nature of neural networks makes them vulnerable to adversarial attacks, as even small perturbations can significantly impact model predictions. Using more non-linear activation functions, such as quadratic or RBF units, could potentially address this issue. The proposal suggests applying non-differentiable and non-linear transformation (discretization) to the input before passing it into the model to address the vulnerability of neural networks to adversarial attacks. This approach aims to avoid the difficulties in training highly non-linear activation functions while still introducing strong non-linearity. The comparison of regular inputs, quantized inputs, and discretized inputs on MNIST dataset under adversarial training is illustrated in FIG1. The proposal suggests applying non-linear transformations to input data to enhance neural network resistance to adversarial attacks. By discretizing input values and using learned weights to project them back to a single scalar, the model learns a highly non-linear function that effectively resists perturbations. Perturbations to pixel values have minimal impact on the network when starting from common values like 0 and 1. Two approaches to constructing discretized representations of input images are considered in this work. The proposal suggests applying non-linear transformations to input data to enhance neural network resistance to adversarial attacks by discretizing input values and using learned weights to project them back to a single scalar. One-hot encodings are simple to compute and understand but not well suited for representing categorical variables with an interpretation of ordering between them. The discretization function is defined pixel-wise for a pixel i \u2208 {1, . . . , n}. Thermometer encodings are proposed to discretize input images without losing relative distance information, preserving pairwise ordering information between pixels. This method enhances neural network resistance to adversarial attacks by making it difficult to attack with standard white-box algorithms. In this section, two novel iterative attacks are described to construct adversarial examples for networks trained on discretized inputs. Constructing white-box attacks on discretized inputs helps evaluate the model's robustness and compare against adversarial training techniques. The attacks assume inputs are discretized into thermometer encodings. The attacks described assume inputs are discretized into thermometer encodings. The first attack, Discrete Gradient Ascent (DGA), follows the gradient of the loss with respect to the input, constrained to be a discretized vector. The second attack, Logit-Space Projected Gradient Ascent (LS-PGA), allows intermediate iterates to be in the interior of the simplex, with the final image obtained by projecting back to the nearest vertex. LS-PGA is a generalization of DGA, where the final image is obtained by projecting back to the nearest vertex of the simplex. The DGA attack initializes by placing pixels into random buckets within \u03b5 of their true values and selects buckets likely to do the most harm at each step. Strengthening the attack involves re-running it multiple times and using the perturbation with the greatest loss. Pseudo-code for the DGA attack is provided in the appendix. To perform LS-PGA, discrete encodings are softened into continuous relaxations, and standard Projected Gradient Ascent (PGA) is applied. The distribution over embeddings is represented as a softmax over logits u, scaled with temperature T for improved attack. Logits are initialized randomly and adjusted at each step to ensure the model assigns probability only to relevant buckets. LS-PGA attack is used to assign probabilities to buckets close to the true value by adjusting logits. The attack can be strengthened by running it multiple times and selecting the perturbation with the highest loss. Experimental results comparing models trained with input discretization to adversarial defenses are provided. For MNIST experiments, a convolutional network is used, while Wide ResNet is used for CIFAR-10, CIFAR-100, and SVHN datasets. Wide ResNets have a depth of 30 for CIFAR-10 and CIFAR-100, and depth of 15 for SVHN with a width factor of k = 4. LS-PGA is found to be more powerful than DGA for attacks on discretized models. Attacks are described in terms of the maximum \u221e -norm of the attack, \u03b5. MNIST experiments used \u03b5 = 0.3, CIFAR experiments used \u03b5 = 0.031, and SVHN experiments used \u03b5 = 0.047 for iterative attacks. The effectiveness of iterated PGD/LS-PGA attacks on vanilla and discretized models for MNIST is shown in Figure 3. Adversarially-trained models in BID13 resulted in a slight loss in accuracy on clean examples but increased robustness towards adversarial examples. Experiments were conducted on models trained only on adversarial inputs. In additional experiments, the model's resistance to adversarial examples was tested on a mix of clean and adversarial inputs. Discretizing the inputs of the network significantly improved resistance to adversarial examples without sacrificing accuracy on clean examples. Thermometer encodings outperformed one-hot encodings in most settings. Results for MNIST and CIFAR-10 can be found in Tables 2, 3, 4, and 5, with additional results for CIFAR-100 and SVHN in the appendix. Figures 2 and 5 in the appendix show the test-set performance. In Figures 2 and 5 in the appendix, test-set accuracy for adversarially trained models on SVHN and CIFAR-10 datasets is plotted. Discretized models show quicker robustness against adversarial examples. Visualizations of non-discretized models exhibit a linear boundary between correct and incorrect classifications, while adversarially trained models show a parabolic shape. Loss for white-box attacks on various models is also examined. Loss for iterated white-box attacks on various models on MNIST data point has converged by step 40. Discretizing the input introduces extra parameters, with a negligible increase in parameter count for different datasets. The robustness likely comes from input discretization rather than model capacity. Hyperparameters used in experiments are described for CIFAR-10 and CIFAR-100 datasets. For CIFAR-10 and CIFAR-100 datasets, data augmentation includes zero-padding, random cropping, flipping, brightness and contrast adjustments. Different optimizers are used for MNIST, CIFAR-10, CIFAR-100, and SVHN datasets with specific learning rates and annealing schedules. A dropout of 0.3 is applied for CIFAR-10. The DGA and LS-PGA attacks are described in Algorithms 2 and 3, utilizing a getMask() sub-routine from Algorithm 1. Additional experiments were conducted using discretized models on MNIST. The main hyperparameters of Algorithm 3 are the step size \u03be and annealing rate \u03b4, with experiments showing similar accuracies for different values of \u03be and \u03b4. The experiments involved discretizing models on MNIST using different hyperparameters \u03be and \u03b4 for Algorithm 3. The performance was only slightly affected by changing \u03b4 from 1.0 to 1.2. Discretizing by using percentile information per color channel did not significantly impact robustness or accuracy. Additionally, training on a mix of clean and adversarial examples resulted in higher accuracy on clean examples but lower accuracy on white-box and black-box attacks compared to clean trained models. The experiments involved training models on a mix of clean and adversarial examples, resulting in higher accuracy on clean examples but decreased accuracy on white-box and black-box attacks compared to clean trained models. Additional experiments were performed on CIFAR-10, showing lower accuracy on adversarial examples but higher accuracy on clean examples with mixed training. Models with fewer levels of discretization had worse accuracy on clean data. Models with fewer levels of discretization had worse accuracy on clean examples but greater robustness to adversarial examples. Results are shown in Tables 12 and visualized in FIG3. Experimental results on CIFAR-100 are listed in TAB3. Adversarially trained models were trained on a mix of clean and adversarial examples. The effect of increasing levels of discretization for the MNIST and CIFAR-10 datasets is plotted in FIG3. In Figure 5, the convergence rate of clean and adversarially trained models on the CIFAR-10 dataset is plotted, showing that thermometer encoded inputs converge faster. Figure 6 shows the norm of the gradient during an attack on MNIST, with the gradient vanishing around 40 iterations. Linear interpolation between clean and adversarial examples is demonstrated in FIG5, highlighting differences in class probabilities between unquantized and discretized models. The discretized models are robust to values they were trained on but collapse beyond that threshold, unlike real-valued models. Thermometer-encoded models degrade faster than vanilla models beyond training epsilon, showing a weakness in practice. The nonlinearity that helps discretized models become robust also makes their behavior unpredictable. The problem of constraining the L-infinity norm of perturbed images to an epsilon ball around the original image aims to ensure human recognizability. However, this constraint excludes many potential attacks, including non-standard ones that can easily defeat state-of-the-art defenses. All current approaches are vulnerable to non-standard attacks, such as the \"larger epsilon\" attack, which is just one example. Fair comparisons reveal the breakability of existing defenses. In church-window plots for MNIST BID3, test-set images are used to craft plots showing adversarial attacks. Each plot includes a clean image at the center with perturbations in adversarial and orthogonal directions. The plots reveal the effectiveness of adversarially-trained models on MNIST. The church-window plots show adversarially-trained models on CIFAR-10, with almost-linear decision boundaries on non-discretized models. The x-axis represents the adversarial direction, the y-axis a random orthogonal direction, and the correct class is white. Each plot includes a training data point chosen randomly, with different orthogonal vectors for the y-axis. The \u03b5 bound for both axes is [-1.0, 1.0]."
}