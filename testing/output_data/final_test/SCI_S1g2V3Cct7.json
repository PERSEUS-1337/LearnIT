{
    "title": "S1g2V3Cct7",
    "content": "Continual learning involves learning new tasks while preserving old knowledge and ideally generalizing from past experience to learn new tasks faster. Neural networks trained by stochastic gradient descent often suffer from catastrophic forgetting, where they degrade on old tasks when trained on new tasks with different data distributions. This phenomenon hinders the accumulation of knowledge and skills in non-stationary data or sequences of new tasks. In the context of reinforcement learning, we explore this issue without explicitly indicating task boundaries, which is common in continuous learning scenarios. Various methods have been proposed to address catastrophic forgetting, but we focus on a straightforward approach. In addressing catastrophic forgetting, a solution using experience replay buffers for all past events with a mixture of on- and off-policy learning has been proposed. This strategy can reduce forgetting in both Atari and DMLab domains, matching the performance of methods requiring task identities. Randomly discarding data in constrained buffer storage allows for almost as good performance as an unbounded buffer. In real-world settings like robotics, gathering new experience can be difficult and expensive. Simultaneous training on multiple tasks may not be feasible, requiring agents to learn one task at a time. The sequence and boundaries between tasks are often unknown or continuously changing. In reinforcement learning, continual learning is crucial due to the changing environment and task sequences. The challenge lies in maintaining previously acquired abilities without catastrophic forgetting. An ideal system for continual learning should meet three key requirements. An ideal continual learning system should retain previously learned capacities without catastrophic forgetting. It should ensure that performance on previously encountered tasks remains as good as historically, while also allowing for rapid acquisition of new skills or knowledge. The challenge of stability-plasticity dilemma is addressed by Continual Learning with Experience And Replay (CLEAR), which allows for maintenance of performance on earlier tasks and fast adaptation to new tasks through a mixture of novel on-policy and off-policy replay experience. This approach also enforces behavioral cloning between the current policy and its past, resulting in a significant boost in performance and reduction in catastrophic forgetting. CLEAR addresses the stability-plasticity dilemma by enforcing behavioral cloning between the current policy and its past self, reducing catastrophic forgetting. Small replay buffers with uniform samples from past experiences are almost as effective as larger buffers. CLEAR outperforms state-of-the-art approaches without needing task information, offering a simple yet effective solution. In recent years, there has been renewed interest in overcoming catastrophic forgetting in reinforcement learning (RL) contexts and supervised learning from streaming data. Current strategies focus on protecting parameters inferred in one task while training on another, such as Elastic Weight Consolidation (EWC) and Progressive Networks approach. These approaches aim to mitigate catastrophic forgetting by constraining weights important for past tasks to change more slowly while learning new tasks. In recent years, strategies like Elastic Weight Consolidation (EWC) and Progressive Networks aim to mitigate catastrophic forgetting in reinforcement learning and supervised learning. Methods such as CLEAR, Zenke et al. (2017), BID10, and BID13 also address this issue by maintaining estimates of weight importance for past tasks. These methods assume known task identities or boundaries. Experience replay buffers are commonly used in reinforcement learning to rehearse old data for data-efficient learning on single tasks. Prioritized replay, learning from human demonstrations, and approximating replay buffers with generative models are some research directions in this area. Experience replay buffers have also been used to protect against catastrophic forgetting in toy tasks, with a focus on making buffers smaller. Mixing on-and off-policy updates in RL has been explored for speed and stability. In CLEAR, a mixture of replay data and fresh experience protects against catastrophic forgetting and allows for fast learning in reinforcement learning. The algorithm combines actor-critic training on new and replayed experiences, incorporating behavioral cloning to improve stability. The addition of behavioral cloning prevents network output on replayed tasks from degrading. In CLEAR, behavioral cloning is used to prevent network output drift on replayed tasks by penalizing KL divergence and L2 norm differences between historical and present policies and value functions. This is applied in a distributed training context using the Importance Weighted Actor-Learner Architecture. The network in the experiment is fed experiences by acting networks with asynchronously updated weights. Training follows V-Trace with defined target values and loss functions for value, policy gradient, and entropy. A mixture of novel and replay experiences is used, with additional policy and value cloning for replay experiences. Performance is not highly sensitive to the ratio of novel to replay experiences. Further implementation details are provided in Appendix A. The first experiment aimed to differentiate between interference and catastrophic forgetting in neural networks. Interference occurs when tasks are incompatible or mutually helpful, while catastrophic forgetting happens when a task's performance decreases due to another task overwriting it. The study compared training paradigms on three tasks within the DMLab set of environments. Training paradigms were compared on three tasks within the DMLab set of environments. Separate and simultaneous training showed minimal interference between tasks, with little difference observed. Simultaneous training slightly outperforms separate training due to commonalities in image processing and basic exploratory behaviors. Sequential training leads to catastrophic forgetting as performance on a task decays immediately when switching to another task. The efficacy of CLEAR for diminishing catastrophic forgetting is demonstrated in the experiment. CLEAR eliminates forgetting on all tasks while preserving overall training performance. When using CLEAR, there is little dropoff in performance when switching tasks, and the network picks up where it left off. Without behavioral cloning, the reduction in catastrophic forgetting is still present, but to a lesser extent. In the experiment, different ratios of new examples to replay examples during training were considered. Using 100% new examples leads to catastrophic forgetting, while 100% replay examples are resistant but with a slight decrease in performance. A 50-50 ratio is seen as a good tradeoff, reducing catastrophic forgetting significantly. The effectiveness of CLEAR does not degrade with more experiences and tasks in the buffer. Training with a 50-50 split of new and replay data shows that performance increases even with 100% replay. Off-policy learning can enhance performance from replay alone, showing the importance of both behavioral cloning and off-policy learning for the success of CLEAR. In testing replay buffer efficacy, small buffers with capacity for a limited number of experiences were compared to a large buffer. Results showed that even with a significantly smaller buffer, it is possible to reduce catastrophic forgetting. Decreasing the buffer size to 5 million led to a slight decrease in robustness, possibly due to over-fitting to limited examples. In an experiment testing replay buffer efficacy, it was found that relying on a replay buffer did not slow down learning of new tasks for CLEAR, which uses a mix of off-and on-policy learning. Performance on tasks was not affected by the amount of data stored in the buffer or the identities of preceding tasks. The study involved a cyclically repeating sequence of three DMLab tasks with a fourth task inserted as a \"probe\", showing that performance on the probe task was consistent regardless of when it was introduced in the training sequence. CLEAR greatly reduces catastrophic forgetting for all tasks, with consistent performance on the probe task regardless of when it is introduced in the training sequence. Comparison to state-of-the-art methods like Progress & Compress and Elastic Weight Consolidation is done using the same Atari tasks and network/hyperparameters as previous studies. In comparison to Progress & Compress and Elastic Weight Consolidation, CLEAR shows reduced catastrophic forgetting across tasks. It achieves comparable performance to P&C and outperforms EWC on tasks like krull, hero, and ms pacman. However, P&C performs better on tasks like beam rider and star gunner. The on-policy model experiences significant forgetting but quickly regains performance after re-exposure to tasks. The experiment shows that replay methods may be more suitable than storing large memory buffers in cases where memory storage is prohibitive. This contrasts with synapse-level consolidation and continual learning methods, with algorithms for continual learning potentially existing on a Pareto frontier. Protect inferred parameters like Progress & Compress may be better than replay methods. Leveraging task identities can reduce memory or computational demands and aid in rapid learning. CLEAR shows comparable performance to other methods, despite being simpler and not requiring knowledge of task boundaries. Adversarial training scenarios exist where fitting to old policy's action distribution could have negative effects on future learning. Moving forward, we anticipate algorithmic innovations that build on the ideas of preventing catastrophic forgetting. Weight-consolidation techniques like Progress & Compress could be combined with our approach for better performance. Off-policy approaches using Q-functions, such as Retrace, may also enhance our current V-Trace algorithm. Our exploration of CLEAR in continual learning scenarios aims to inspire further research with experimental protocols like probing tasks at different training sequence positions. CLEAR is a simple and powerful approach for preventing catastrophic forgetting in continual learning settings. It leverages on-policy learning on fresh experiences and off-policy learning with behavioral cloning on replay experience to adapt rapidly to new tasks while maintaining performance on past tasks. The method is scalable, practical, and takes advantage of memory and storage in modern computing facilities. It is considered a \"first line of defense\" against catastrophic forgetting in many RL contexts. In ICML 2017, a distributed setup for training was used with multiple actors and a single learner. The actors generate training examples on CPU and send them to the learner, who asynchronously propagates weight updates back to the actors. Each actor generates a training episode and inserts it into a buffer, using reservoir sampling if the buffer is full. The learner receives pairs of new and replayed episodes from the actors for training. The learner in a distributed training setup receives pairs of new and replayed episodes from actors, ensuring no actor contributes more than a single example to the batch. The network used for DMLab experiments is based on BID14, omitting the LSTM module for textual input tasks. For Atari, the network from Progress & Compress is used, along with all hyperparameters copied. The replay buffer stores all information needed for the V-Trace algorithm. The replay buffer for the V-Trace algorithm stores essential information such as input, output logits, value function, action, and reward. It is distributed among all actors equally, with each actor having a buffer capacity based on the total buffer size. The buffer is measured in environment frames and capped at half the number of frames on which the network is trained to prevent catastrophic forgetting. New unrolls are added when the buffer fills up. The replay buffer in the V-Trace algorithm prevents catastrophic forgetting by using reservoir sampling to store a random sample of all unrolls. Training was conducted using V-Trace with specific hyperparameters on DMLab/Atari tasks, including loss functions for policy-cloning and value-cloning. During training, L policy-gradient, L value, and L entropy were applied with specific weights. Evaluation of each network on all tasks was done using pools of testing actors that asynchronously updated their weights to match the learner. Testing actors ran on designated tasks regardless of the current task in use by training actors, with tasks changing after a specified number of learning episodes. The total number of episodes was monitored by the learner. In an experiment investigating the necessity of on-and off-policy learning in swiftly acquiring new tasks, a probe task with 100% replay experience was conducted. The task involved presenting a DMLab task at varying positions in a sequence of other tasks, using off-policy replay exclusively. The results were compared to previous experiments to determine the impact of different learning approaches. In a study on the importance of on-and off-policy learning for rapid task acquisition, CLEAR was used with 100% replay experience. Results showed that performance on the probe task deteriorated when it appeared later in the task sequence, highlighting the need for a mix of new experience and replay for efficient learning. The study used CLEAR with 100% replay experience to compare performance on tasks presented sequentially. Results showed that CLEAR achieved comparable performance to training on tasks separately or simultaneously, indicating minimal forgetting. The study demonstrated that using CLEAR with an equal balance of new and replay experience resulted in a good tradeoff between stability and plasticity, while 100% replay experience reduced forgetting but lowered overall performance. Additionally, reduced-size buffers still allowed CLEAR to achieve similar performance levels. CLEAR with reduced-size buffers maintains comparable performance to more complex methods like Progress & Compress and Elastic Weight Consolidation, without requiring information about task boundaries."
}