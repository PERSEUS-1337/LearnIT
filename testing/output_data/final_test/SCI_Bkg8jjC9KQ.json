{
    "title": "Bkg8jjC9KQ",
    "content": "Saddle-point problems related to generative adversarial networks (GANs) have gained attention in machine learning. Theoretical progress in efficient GAN training requires moving beyond convex-concave problems. Analyzing mirror descent (MD) in non-monotone problems with coherence, it is found that ordinary MD converges under certain conditions but may fail in other cases. Optimistic mirror descent (OMD) resolves this issue by converging in all scenarios. Optimistic mirror descent (OMD) converges in all coherent problems by taking an \u201cextra-gradient\u201d step, extending results for optimistic gradient descent (OGD) in bilinear problems. Stochastic analogues of these results are provided, validated through numerical experiments in various GAN models. The surge in deep learning breakthroughs has sparked interest in solving hard optimization problems, necessitating effective theoretical understanding for algorithmic improvements. Theoretical advances in optimization and machine learning communities have led to algorithmic improvements, such as avoiding saddle points in first-order methods. Efforts have been made to understand non-convex landscapes for efficient training, with properties like the \"strict saddle\" being leveraged in various objective functions. However, adversarial deep learning, especially in generative adversarial networks (GANs), remains less understood. Despite recent scrutiny, theoretical understanding of generative adversarial networks (GANs) lags behind \"single-agent\" deep learning. Techniques to enhance GAN stability include Wasserstein metrics, critic gradient penalties, feature matching, and minibatch discrimination. Previous work on adaptive dynamics in bilinear zero-sum games showed chaotic, recurrent behavior. Recent studies have revisited cycle-like behavior in bilinear games through the lens of GANs. Recent studies have revisited the behavior of continuous and discrete-time first order methods in bilinear games. Continuous-time descent methods in zero-sum games are Poincar\u00e9 recurrent, while discrete-time analogues show orbits spiraling slowly outwards. These recurrent systems have formal connections to Hamiltonian dynamics and do not behave in a gradient-like fashion. This critical failure of descent methods can be overcome through \"optimism\", interpreted as an \"extra-gradient\". Optimistic gradient descent (OGD) overcomes critical failures of descent methods in bilinear games by pushing the training process further along the gradient. This approach succeeds where vanilla gradient descent (GD) fails, specifically in unconstrained bilinear saddle-point problems. To improve training methodologies for GANs, it is beneficial to first establish enhancements in a more restricted setting before testing them in more challenging environments. By focusing on non-monotone problems related to variational inequalities, known as coherence, we aim to exploit the problem's geometry to overcome the limitations of ordinary descent methods. In coherent problems, Mirror Descent (MD) converges almost surely in strictly coherent problems but spirals outwards in null coherence. This limitation in MD for training convoluted, adversarial deep learning models is addressed by adding an extra-gradient step to look ahead and take an additional step along a \"future\" gradient. The extra-gradient technique, also known as optimistic mirror descent (OMD), is a popular method in the learning literature. It was first introduced as an additional step along a \"future\" gradient to address limitations in Mirror Descent for training deep learning models. This technique achieves optimal convergence rates in variational inequalities and has been effective in GAN training. Recent studies have explored variants of this method to improve performance. The extra-gradient algorithm, also known as optimistic mirror descent (OMD), has shown promising results in various problems, including GAN training. Recent studies have demonstrated its effectiveness in achieving optimal convergence rates in variational inequalities. Specifically, it has been proven to converge geometrically in strongly monotone, deterministic variational inequalities and ergodically in general stochastic variational inequalities. Additionally, the algorithm's output and analysis need to be adjusted to move closer to GAN training. The extra-gradient algorithm, also known as optimistic mirror descent (OMD), has been shown to generalize and extend previous results for OGD in pseudo-monotone problems. It settles an open question concerning the convergence of OGD in nonlinear problems and demonstrates convergence in stochastic saddle-point problems. These results highlight the performance gains of adding extra-gradient to state-of-the-art methods like Adam, validated on standard GAN models. The problem's value function in a zero-sum game between two players seeking to minimize loss and maximize reward is denoted by f. To solve the problem, incremental processes utilizing individual gradients of f are employed. Variational inequalities and coherence are key topics in saddle-point problems literature. Saddle-point problems focus on monotone cases like convex-concave functions. Solutions can be characterized by Stampacchia variational inequality or Minty form. The equivalence extends to bilinear and pseudo-monotone problems. Even non-pseudo-monotone examples can have unique solutions. The notion of coherence in saddle-point problems is introduced, where solutions must satisfy certain conditions. If the conditions are met strictly, it is called strictly coherent; if met as an equality, it is null-coherent. This concept plays a central role in the discussion, with antecedents in nonlinear programming. The concept of coherence in saddle-point problems is discussed, with antecedents in nonlinear programming. The term \"coherence\" is borrowed from Zhou et al. [2017b], who used \"variational coherence\" for a stronger variant. The set of solutions in a coherent problem does not need to be convex, and strict coherence is shown when the objective function is strictly convex-concave. Null-coherent problems include bilinear objectives with an interior solution. The concept of coherence in saddle-point problems is discussed, with antecedents in nonlinear programming. The method involves mirror descent adapted to a saddle-point context, utilizing a distance-generating function for generating new state variables. Various variants of the method exist, offering different approaches to solving the problem. The method involves mirror descent adapted to a saddle-point context using a continuous and K-strongly convex distance-generating function on X. The function generates a pseudo-distance known as the Bregman divergence, allowing for convergence verification of a sequence to a target point. The algorithm involves choosing an initialization, querying an oracle for gradient feedback, updating the state, and iterating until convergence. The Bregman divergence generates a pseudo-distance for convergence verification in mirror descent. The algorithm involves querying for gradient feedback, updating the state, and iterating until convergence, assuming Bregman reciprocity holds. The prox-mapping in mirror descent algorithm produces a feasible point by taking a step along a dual vector. Two examples include Euclidean projections and entropic regularization. The Gibbs-Shannon entropy function h(x) is widely used in DGF, being 1-strongly convex with respect to the L1 norm. The update rule x \u2190 P x (y) is known as the multiplicative weights algorithm and is essential for learning in games and adversarial bandits. The gradient input sequence \u011dn of mirror descent can be obtained from a first-order oracle, which may provide noisy gradient estimations. The gradient feedback sequence \u011dn is generated randomly from Xn and is not F n-measurable. Convergence analysis in convex-concave scenarios involves taking the ergodic average of the sequence Xn. Jensen's inequality and gradient monotonicity play a role in regret-based analysis. The main result for (MD) establishes a dichotomy between strict and null coherence, where in strictly coherent problems Xn converges to the solution set of (SP), while in null-coherent problems Xn drifts away and cycles without converging. The dichotomy between strict and null coherence in (MD) leads to convergence in strictly coherent problems but drifts away in null-coherent problems. Bilinear models and exact gradient input can cause non-convergence, especially in GANs. In GANs, mode collapse and cycling between different modes can occur due to vanishing step-size. Extra-gradient analysis addresses these issues by using an optimistic \"extra-gradient\" step to amortize the next prox step, inspired by previous work by BID27 and Nemirovski [2004]. The extra-gradient method, inspired by previous work by BID27 and Nemirovski [2004], generates an intermediate state and then samples to go back to the original state to create a new state. This leads to the optimistic mirror descent (OMD) algorithm for saddle-point problems. The extra-gradient method, based on previous work by BID27 and Nemirovski [2004], utilizes gradient feedback to update states. The algorithm includes a gradient reuse mechanism and convergence analysis shows O(1/n) rate in monotone problems. The extra-gradient method, utilizing gradient feedback, converges monotonically to a solution of the problem. The algorithm includes a gradient reuse mechanism and shows O(1/n) rate in monotone problems. The extra-gradient method converges monotonically to a solution of the problem, with a gradient reuse mechanism and O(1/n) rate in monotone problems. The convergence of Xn to a limit cycle is analyzed in detail, with conditions for convergence outlined in Theorem 4.3. The extra-gradient method guarantees monotonic decrease with controlled step-size policies, benefiting from mirror descent methods. Experimental validation was done on Gaussian mixture models with 16 Gaussians. The networks were trained using RMSprop and Adam, with results shown after specific iterations. The extra-gradient method outperforms standard optimization strategies in learning multi-modal distributions and reducing oscillatory behavior. Adam was the most promising training method in experiments with Gaussian mixture models. The extra-gradient method, particularly with Adam, showed improved performance in training Wasserstein-GAN on CelebA and CIFAR-10 datasets. Results based on inception score and Fr\u00e9chet distance metrics indicated higher scores and stability with the extra-gradient step. Visual samples generated at the end of training displayed accurate feature representation and low distortion. The extra-gradient method, especially with Adam, improved performance in training Wasserstein-GAN on CelebA and CIFAR-10 datasets. It provides noticeable gains in performance and stability, with accurate feature representation and low distortion in generated samples. The dichotomy between strict and null coherence justifies the effectiveness of the method, eliminating cycles and stabilizing it. This property allows for a local analysis with provable convergence guarantees in multi-modal settings and beyond zero-sum games. The strict convex-concave property of function f implies coherence in the optimization problem. The variational inequality and monotonicity of g(x) support this, leading to the satisfaction of the maximization variational inequality. The minimizer condition further confirms the solution of the optimization problem, establishing its coherence. The solution set of a coherent saddle-point problem is closed, as every solution satisfies the maximization variational inequality. This regularity result is needed for the convergence analysis. In this appendix, auxiliary results and estimates are provided for the convergence analysis of a coherent saddle-point problem. The Bregman divergence associated with a strongly convex distance-generating function is defined, along with the induced prox-mapping and convex conjugate. The presented results are essential for completeness in the literature. The convex conjugate h* is differentiable on Y, with its gradient satisfying specific identities. The mirror map Q: Y \u2192 X is generated by h, and for a distance-generating function on X, certain relationships hold. The update rule x \u2190 Px(y) is well-posed, and for interior points in X, certain inequalities are established based on the strong convexity of h. The text discusses the convexity of a function h, the relationship between \u03c6 and \u03c8, and basic bounds on the Bregman divergence. It also introduces the \"three-point identity\" lemma related to distance-generating functions. The text presents Proposition B.3, which establishes upper and lower bounds for a K-strongly convex distance-generating function h on X. The proof involves demonstrating various inequalities and applying Young's inequality. The proposition concludes that X n converges to p if D(p, X n ) approaches 0. The text discusses the possibility of X n converging to p even when lim inf n\u2192\u221e D(p, X n ) > 0, due to certain conditions not being met. This issue is addressed by the Bregman reciprocity condition, ensuring D(p, X n ) approaches 0 whenever X n \u2192 p. The text discusses the Bregman reciprocity condition to address convergence issues in X n approaching p. The analysis of the OMD algorithm requires relating prox steps along different directions, as shown in Proposition B.4 for a K-strongly convex distance-generating function on X. The text discusses the mirror descent algorithm and its convergence proof, with a focus on the gradient input sequence satisfying standard oracle assumptions. The algorithm is defined recursively with variable step sizes and gradient vectors. The convergence proof for the algorithm involves applying bounds and inequalities, ultimately leading to the proof of convergence. The convergence proof for the mirror descent algorithm under strict coherence relies on Proposition C.1 and Proposition C.2. Proposition C.1 shows that the Bregman divergence is an asymptotic constant of motion, leading to convergence to a saddle-point or a nonzero level set. This rules out chaotic or aperiodic behaviors in the algorithm. The convergence proof for the mirror descent algorithm relies on Proposition C.2, which shows that the solution set of the algorithm is closed and compact. This is essential for proving convergence to a saddle-point or a nonzero level set, ruling out chaotic or aperiodic behaviors. The solution set X* of the mirror descent algorithm is closed and compact. Assuming X* is not equal to X, if the sequence Xn has no limit points in X* with positive probability, there exists a compact set C in X such that C \u2229 X* = \u2205 and Xn \u2208 C for large n. By continuity of g and compactness of X* and C, there exists a > 0. Let Dn = D(p, Xn), then we have an estimate. The mirror descent algorithm's solution set X* is closed and compact. If X* is not equal to X and Xn has no limit points in X* with positive probability, there exists a compact set C in X such that C \u2229 X* = \u2205. By continuity of g and compactness of X* and C, there exists a > 0. Let Dn = D(p, Xn), then we have an estimate that converges to 0 with probability 1. The mirror descent algorithm's solution set X* is closed and compact. If X* is not equal to X and Xn has no limit points in X* with positive probability, there exists a compact set C in X such that C \u2229 X* = \u2205. By continuity of g and compactness of X* and C, there exists a > 0. Let Dn = D(p, Xn), then we have an estimate that converges to 0 with probability 1. The proof of Proposition C.1 shows that at least one of the limit points of Xn must lie in X* (a.s.). The fact that Xn is Fn-measurable implies that Rn is an Fn-adapted supermartingale. By Doob's convergence theorem, Rn converges to a finite random variable R\u221e. This implies that Dn converges to a random variable D(x*) with finite variance. It needs to be shown that g(Xn), Xn - x* \u2265 0 for large n, which can be proven by showing Xn lies in a neighborhood U of x* where a certain condition holds. The proof of Theorem 3.1 shows that Xn converges to x* with probability 1. However, Theorem 3.1(b) demonstrates the failure of (MD) to converge under null coherence. The Bregman divergence under (MD) satisfies the identity DISPLAYFORM13, with the proof of Corollary 3.2 being an immediate consequence of strict convex-concave problems satisfying strict coherence. Corollary 3.3 provides a more general result for two-player, zero-sum finite games defined by a matrix M. Proposition C.3 states that in finite zero-sum games with a unique interior equilibrium and exact gradient input, the Mirror Descent algorithm does not converge but cycles at a positive Bregman distance from the Nash equilibrium. This behavior occurs because the incremental step of the algorithm points away from the equilibrium, leading to orbiting around it as the best-case scenario. The Mirror Descent algorithm in finite zero-sum games with a unique interior equilibrium and exact gradient input does not converge but cycles at a positive Bregman distance from the Nash equilibrium. The algorithm takes small outward steps throughout its runtime, eventually converging to some limit cycle farther away from the equilibrium. The Mirror Descent algorithm in finite zero-sum games with a unique interior equilibrium and exact gradient input does not converge but cycles at a positive Bregman distance from the Nash equilibrium. The algorithm takes small outward steps throughout its runtime, eventually converging to some limit cycle farther away from the equilibrium. Theorem 4.1 states that if (SP) is coherent and g is L-Lipschitz continuous, and (OMD) is run with exact gradient input and a specific step-size sequence, then the sequence Xn converges monotonically to a solution x* of (SP). In the context of finite zero-sum games with a unique interior equilibrium and exact gradient input, the Mirror Descent algorithm does not converge but cycles at a positive Bregman distance from the Nash equilibrium. The algorithm takes small outward steps, eventually converging to a limit cycle farther from the equilibrium. In the presented results of image experiments using OMD training techniques, Inception and FID scores show that the extra-gradient add-on improves GAN training performance and stabilizes the model efficiently. Without the extra-gradient step, performance tends to drop after approximately 100k steps. Samples generated by Adam are also provided for comparison. In the CelebA and CIFAR-10 datasets, samples generated by Adam and optimistic Adam are compared after 100k steps. The generated samples from CelebA are more representative and faithful to the target data distribution. Network architectures and hyperparameters of the GANs used are provided for reproducibility, with a standard DCGAN architecture employed."
}