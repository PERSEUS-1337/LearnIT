{
    "title": "H1lFsREYPS",
    "content": "Researchers have proposed a model for generating synthetic question-and-answer data from large corpora like Wikipedia to train machine reading comprehension models. This technique outperforms general pre-training methods as the generated data closely resembles the downstream MRC data. However, generating high-quality synthetic data comparable to human-annotated datasets remains a challenge. To address this, a novel pre-training method called Answer-containing Sentence Generation (ASGen) is introduced, incorporating advanced techniques. The ASGen pre-training method involves dynamically determining K answers and pre-training the question generator on answer-containing sentence generation task. Experimental results show that this approach outperforms existing methods and improves the performance of MRC models on various datasets without architectural modifications. Researchers have proposed MRC models using high-quality human-annotated datasets like SQuAD-v1.1, SQuAD-v2.0, and KorQuAD, surpassing human performance. These datasets involve finding answers within paragraphs, requiring significant human annotation. To tackle the small annotated data size, a two-phase training method of transfer learning is often used. In the second phase, state-of-the-art MRC models utilize pre-training methods like ELMo and masked language models for significant performance improvements. However, pre-training on a different domain task may lead to performance degradation in downstream tasks. Generating synthetic data similar to downstream tasks is crucial to address this issue. Generating synthetic data similar to downstream tasks is crucial for pre-training models. Researchers have studied a model for generating synthetic MRC data from large corpora like Wikipedia, using a two-stage synthesis network. Golub et al. propose generating a fixed number (K) of answers conditioned on the paragraph, and question generation conditioned on the paragraph and the generated answer. Devlin et al. introduced a pre-training technique for the question generator by pretraining on the generation of next-sentence following the paragraph. ASGen is a novel method for synthetic data generation that addresses issues with existing methods by dynamically predicting answers and pre-training question generators on answer-containing sentences. This approach aims to improve question generation capability, as evaluated by comparing BLEU scores. Our method, ASGen, aims to enhance question generation capability by outperforming existing methods in BLEU score comparison. It involves a BERT-based generative model (BertGen) and answer-containing sentence generation pre-training. BertGen generates synthetic data from Wikipedia and is pre-trained on novel components to improve question generation. BertGen is a generator that encodes Wikipedia paragraphs using two separate networks: the answer generator and the question generator. The answer generator predicts answer candidates by applying a feed-forward layer on contextual embeddings. The question generator generates questions based on the predicted answers. The BertGen system utilizes a BERT encoder to process context and answer span information, followed by a Transformer decoder to generate questions. The model is pre-trained on generating answer-containing sentences by excluding the answer sentence from the context. Questions and answers are generated from a large corpus like Wikipedia, and the MRC model is trained on this data before fine-tuning on specific datasets like SQuAD. BERT is used as the default MRC model due to its high performance in various datasets. The MRC model is known for its state-of-the-art performance in various datasets. When creating question-answer pairs, humans tend to choose answer-like phrases first. Multiple answers can be selected from a context, especially if it is lengthy or contains keyphrases, nouns, dates, names, etc. Drawing a fixed number of answers from a context may result in low-quality answers if the number is too large. Predicting the number of answers in a context using regression with a fully connected unit. Calculating scores for start and end indices of answer spans based on encoder output and trainable vectors. The model predicts the number of answers and answer spans in a context using regression and cross-entropy loss. During inference, the top answer spans are selected based on score summation. Questions are generated for each answer span to cover different possible answers. The generation model can be pre-trained with an unsupervised task. The generation model can be pre-trained with an unsupervised task to improve performance. However, there are issues with this approach, such as the next-sentence generation task not being conditioned on the answer. To address these issues, the context is modified to exclude the sentence containing the previously generated answer, and the generator is pre-trained on generating this excluded answer-containing sentence. The context is modified to exclude the sentence containing the previously generated answer, and the generator is pre-trained on generating this excluded answer-containing sentence. This involves passing the generated answer to a sequence-to-sequence model for segmentation encoding and generating an answer-containing sentence embedding using a Transformer model. The sequence-to-sequence model, with the encoder initialized with BERT, is used to pre-train the question generation model by calculating the loss with cross-entropy. The dataset for answer-containing sentence generation tasks is built using paragraphs from English Wikipedia dump, and questions and answers are synthetically generated. Extensive filtering is applied to create synthetic MRC data for pre-training the downstream MRC model. Using the ASGen tool, 43M answer-paragraph pairs are generated from high-quality Wikipedia paragraphs for pre-training on answer-containing sentence generation. A smaller dataset (Small-Wiki) of 2.5M pairs and a test dataset (Test-Wiki) of 25K pairs are also used for evaluation. Additionally, one question is generated for each answer-paragraph pair in the Full-Wiki dataset, resulting in a total of 43M triples of paragraph, question, and answer for synthetic MRC data. Benchmark datasets like SQuAD-v1.1 are used for question generation tasks, with comparisons made to existing methods like UniLM. The training set is split for fair evaluation, and different test splits are used for evaluation. The study evaluates the effect of synthetic MRC data on fine-tuned MRC models using datasets like SQuAD-v1.1, SQuAD-v2.0, KorQuAD, and QUASAR-T. BERT is used for answer generation, and a comparison is made with a pre-training method from Devlin et al. The study evaluates the impact of synthetic MRC data on fine-tuned MRC models using datasets like SQuAD-v1.1, SQuAD-v2.0, KorQuAD, and QUASAR-T. Different pre-training methods are compared, including those from Devlin et al. (2019b) and Golub et al. (2017). The effectiveness of the methods is tested by training state-of-the-art models (BERT and BERT+CLKT) on generated data and fine-tuning on the respective train sets for each dataset. The 'BERT + CLKT' model, pre-trained for the Korean language, is used as a baseline to demonstrate the effectiveness of the method. An experiment shows the model's performance in generating answers in a given context, with the answer generator producing more appropriate answers than the fixed K approach. The answer generator outperforms the fixed K approach by reducing mean absolute error between ground-truth and prediction. ASGen shows superior performance in question generation compared to UniLM, with ASGen (Large) achieving higher BLEU-4 scores on both splits. The effectiveness of answer-containing sentence pre-training task (AS) is demonstrated by outperforming other pre-training tasks. In experiments, AS outperforms NS in question generation tasks, showing consistent improvement in performance without architecture changes. ASGen generates synthetic data for MRC models, demonstrating effectiveness before fine-tuning on downstream datasets. The MRC model pre-trained on synthetic data generated by ASGen shows significant improvements in F1 scores on SQuAD-v1.1, SQuAD-v2.0, and KorQuAD compared to baseline models. ASGen's synthetic data also outperforms 'BertGen+NS' in downstream data tasks. The effects of synthetic data size on MRC model performance are illustrated in Fig. 5, where ASGen consistently outperforms BertGen+NS with a fixed synthetic data size of 43M. ASGen consistently outperforms BertGen+NS in MRC tasks, especially with a small size of real data. The gap diminishes for larger data sizes but remains significant for the current SQuAD dataset. Increasing the size of generated data leads to better performance. Transfer learning experiments show improvements in downstream tasks using synthetic data generated by ASGen. After training BERT MRC model with ASGen, fine-tuning with QUASAR-T datasets improves F1 score. Comparison shows AS generates more specific questions than NS, as seen in Table 7 samples. The comparison between AS and NS in generating questions for Machine Reading Comprehension tasks shows that AS is more contextually rich and specific, using relevant words and subjects from the answer. AS includes more context-related questions compared to NS, which lacks conditioning on the answer. Popular datasets for MRC tasks include SQuAD-v1.1, SQuAD-v2.0, KorQuAD, and HotpotQA, but these datasets are relatively small. Question generation methods have been actively studied for various purposes, including data augmentation in question answering. Different models have been proposed to improve question generation quality, such as attention-based models, query-based generative models, and gated self-attention encoders. The proposed method (AS) aims to enhance question generation by pre-training models with an answer-containing sentence generation task. The quality of pre-training methods like Open-GPT, BERT, XLNet, and UniLM using a Transformer module has been popular in natural language processing. While similar to these approaches, our method focuses on generating answer-containing sentences to improve question generation. Synthetic data generation has shown that neural models produce better answers than using off-the-shelf tools for selecting named entities. The model proposed by Golub et al. (2017) separates answer generation and question generation, improving performance with advanced training methods. They dynamically choose top answer spans to generate diverse synthetic data for MRC, resulting in 43M synthetic training samples. The proposed method generates 43M synthetic training samples for the MRC model, achieving new state-of-the-art results on SQuAD question generation. It consistently improves performance on various datasets without architectural modifications. Additionally, low-quality stub articles and irrelevant pages are removed to extract usable text from Wikipedia articles. The text chunk discusses the process of cleaning Wikipedia text data by removing various entities and filtering paragraphs based on their character count. It mentions the creation of a final dataset containing 8.3M paragraphs from 2.4M articles. Additionally, it evaluates a question generation model on a separate data split called Test-Split3. The text chunk evaluates a question generation model on a separate data split called Test-Split3. It shows improvements in BLEU-4 score and discusses downstream MRC results for SQuAD v1.1, SQuAD v2.0, and KorQuAD. Multiple model checkpoints were selected for fine-tuning on final downstream data."
}