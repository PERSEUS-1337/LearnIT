{
    "title": "By-IifZRW",
    "content": "We propose a method to embed stochastic activation functions based on Gaussian processes in probabilistic neural networks. This allows for efficient implementation and training without the need for sampling. Variational Bayesian inference is applied to regularize and train the model, which can handle uncertain inputs and provide confidence estimates for predictions. The popularity of deep learning has led to new research on neural network fundamentals, focusing on optimal architecture through hyperparameter search and activation functions like ReLU. Leaky ReLU and parameterized ReLU were introduced for better results on image recognition tasks. The leaky ReLU was made adaptable by introducing a parameter for each neuron, allowing the activation function to adjust to training data. BID0 generalized this concept to piecewise linear activation functions with multiple points of slope change. BID18 added stochasticity by sampling the slope value from a uniform distribution. BID3 and BID9 replaced the negative part of ReLUs. BID3 and BID9 proposed replacing the negative part of ReLUs with a scaled exponential function, leading to automatic renormalization of inputs in neural networks. BID4 introduced nearly fully adaptable activation functions using a Fourier basis expansion. BID1 suggested learning activation functions alongside layer weights, using adaptive piecewise linear units. The research explores probabilistic activation functions in a neural network structure, developing Gaussian-process neurons with variational inference for computational efficiency. Slicing matrices along rows and columns is necessary to define the model. Gaussian Processes (GPs) are nonparametric models that provide flexible probabilistic approaches for function estimation. A Gaussian Process BID14 defines a distribution over a function f (x) \u223c GP(m(x), k(x, x )) where m is the mean function and k is the covariance function. For S inputs x \u2208 R S\u00d7N of N dimensions, the corresponding function values f follow a multivariate normal distribution with mean vector m and covariance matrix K. The zero mean function m(x) = 0 and the squared exponential (SE) covariance function with scalar inputs are used in this work. The text discusses Gaussian Processes (GPs) and their use in function estimation. It explains the covariance function used in GPs, which determines the similarity of function values for nearby inputs. The text also introduces the non-parametric model of Gaussian Process neurons (GPNs) for efficient training and inference. The text introduces Gaussian Process neurons (GPNs) with scalar inputs and stacked layers for a multi-layer feed-forward network. It discusses the parametric representation of GPN using virtual observation inducing points and targets. Marginalizing latent variables for predictions is challenging due to intractable analytic integration, leading to temporary parameterization of activation functions for tractable training objectives. The text discusses the use of virtual observations in Gaussian Process neurons (GPNs) for parametric representation and training objectives in multi-layer feed-forward networks. By introducing inducing points and targets, the activation function shape is determined by these points, allowing for factorization of the conditional approximation. The text discusses factorizing the conditional approximation over samples in a GPN feed-forward network. Marginalizing eq. (4) over F l n results in a prior distribution for three layers. The posterior is approximated using variational inference and the central limit theorem on activations. The distribution for X l is conditionally normal given the previous layer values, but the marginals P(X l n ) will not be normally distributed due to non-linear input from the previous layer. By placing a GP prior on the virtual observation targets U l, the marginal distribution of the response can be verified. The marginal distribution of the response in a GPN feed-forward network can be verified by applying variational inference to approximate the posterior given training data. Activation functions learned from the data are incorporated via virtual observation targets, requiring an adaptable variational posterior. The posterior in a GPN feed-forward network needs to be adaptable for storing information. A normal distribution factorized over GPN units with free mean and covariance is chosen for the approximative posterior of U l. The inducing targets of a GPN can be correlated, but the covariance matrix can be constrained to be diagonal to reduce variational parameters. The overall approximating posterior is estimated by substituting distributions into an equation, resulting in \u0141 = \u2212\u0141 reg + \u0141 pred. The term \u0141 reg represents the sum of KL-divergences between the GP prior on virtual observation targets and their approximative posterior Q(U l), aiming to keep the approximative posterior close to the prior. The activation of each GPN will converge to a normal distribution due to the central limit theorem. The weights in GPNs are assumed to have sufficiently random distributions, similar to standard feed forward neural networks. The marginal distributions of GPNs can be written as a graphical model corresponding to the approximate posterior. The weights in GPNs are assumed to have random distributions, leading to normal distribution convergence. The graphical model of the approximate posterior allows exact moment calculations and analytical propagation between layers. Conditional distributions are evaluated for GP regression, recovering standard distributions. Virtual observations behave as if they were not present, aiding in equation evaluation. DISPLAYFORM10 allows for the evaluation of eq. FORMULA2 by adding Gaussian noise to P(X l | F l). The distribution Q(A L) is obtained iteratively by calculating the marginals Q(A l) for l \u2208 {1, . . . , L}. Mean and covariance of the marginal response are evaluated, with the marginal mean calculated using the squared exponential kernel as a normal PDF. The marginal covariances are obtained using the law of total expectation. The calculation of moments of F l is concluded, and the activation distribution propagation from one layer to the next is explained. The marginal distribution of A l+1 is determined iteratively using equations (33), (35), (37), (39), and (40) over the layers l. Computational power can be saved by propagating only the variances, assuming \u03a3 F l s is diagonal. \u0141 pred is identified as the expected log-probability of observations under the marginal distribution Q(F L). The variational lower bound calculation is completed, and the objective function is determined as a deterministic function of the parameters. Training involves maximizing the objective with respect to variational and model parameters using gradient-descent algorithms. Automatic differentiation is assumed for obtaining necessary derivatives. Activation functions are represented using variational parameters. The activation functions in a GPN network can be represented using 2R variational parameters per GPN, where R is the number of inducing points. By sharing the same activation function within groups of neurons or across whole layers, the number of parameters can be reduced. Precomputing kernel matrices and their inverses for fixed inducing points helps in reducing computational complexity. The number of parameters and computational complexity in GPN networks depend only on R, making them suitable for training on datasets of unlimited size. A non-parametric model based on Gaussian Processes for learning activation functions in a neural network was presented. Variational methods were used for efficient Bayesian inference, providing confidence estimations in predictions. The model, validated in experiments, showed less overfitting compared to traditional networks despite having more parameters."
}