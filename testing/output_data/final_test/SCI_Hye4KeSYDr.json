{
    "title": "Hye4KeSYDr",
    "content": "In this paper, a new evaluation criteria called robustness analysis is introduced to measure the importance of a set of features in a machine learning model. This criteria measures the minimum tolerance of adversarial perturbation to establish the link between features and predictions. By analyzing the tolerance level for adversarial attacks, a set of features that support a prediction robustly can be identified. This methodology has been applied to various prediction tasks across different domains, showing that the explanations derived capture significant feature sets effectively. Recent machine learning research has led to the rapid adoption of various models in real-world applications, raising concerns about credibility, fairness, and interpretability. Researchers have explored different notions of model interpretability, including trustability, fairness, and improving model performance by understanding weak points. Most interpretability tasks focus on extracting relevant features for predictions, known as feature-based explanation, which measures the fidelity of the explanation to the model. The fidelity-based attribution evaluation varies depending on design, including completeness, sensitivity-n, infidelity, and causal local explanation metric. The concept of smallest sufficient region (SSR) and smallest destroying region (SDR) considers the ranking of feature attribution scores rather than the actual score itself. SDR-based evaluations measure the function value changes when the most salient features are removed. The attribution evaluations focus on how function value changes when high-valued salient features are removed. Arbitrary reference values can introduce bias in the attribution map, favoring bright pixels and omitting important dark objects. An alternative method is to remove pixels using sampling from a predefined distribution or generative model. In this paper, the authors propose a method to reduce bias in input perturbation by focusing on important features for predictions rather than perturbing all features. Unlike previous approaches, they consider the minimum norm of perturbation in arbitrary directions to change the model's sensitivity. The authors introduce a new evaluation criteria to test the importance of a set of features by computing the minimum adversarial perturbation that can alter the model's decision. This approach involves formulating the model explanation as a two-player min-max game between an explanator and an adversarial attacker. The framework introduces a two-player min-max game between an explanator and an adversarial attacker to find important features that maximize minimum perturbation. New evaluation criteria for feature-based explanations based on robustness analysis are defined, along with efficient algorithms to generate explanations. Experiments show the proposed method can identify important features missed by previous methods in computer vision and NLP models. Our method introduces a new approach to extract features for classification problems, aiming to identify important features that were previously overlooked. The goal is to generate a compact set of relevant features that contrast the current prediction to a target class. The evaluation of such explanations is crucial to determine their quality and justification. The evaluation of explanations for feature extraction in classification problems is essential to determine their quality and justification. Recent studies use an axiomatic approach to define relevant features and assess them based on their impact on prediction. However, the practice of approximating feature removal may introduce bias in evaluation. To address this, two new criteria are proposed to improve the assessment process. The text proposes two new criteria for evaluating the importance of features in classification problems. These criteria are based on assumptions about the impact of perturbations on different sets of features. The evaluation framework focuses on adversarial robustness when certain features are fixed. The adversarial perturbation norm on a set of features S, named Robustness-S, is defined as the perturbation value on features in S being constrained to 0. An explanation partitions input features into relevant set Sr and irrelevant set Sr. Higher robustness on Sr comes from a larger coverage of pertinent features, making an explanation better. Conversely, including important features in Sr leads to smaller robustness on Sr. These criteria evaluate feature importance in classification problems based on perturbation impact. Robustness-Sr measures the minimum adversarial distortion when important features are anchored and perturbation is only allowed in low-weight regions. The size of the relevant set Sr can be varied to evaluate Robustness-Sr at different points, allowing for the calculation of the area under curve (AUC) to determine the average Robustness-Sr. Robustness-Sr measures the minimum distortion distance when important features are perturbable, with the rest anchored. AUC of Robustness-Sr is the average when varying the size of the relevant set. Evaluation criteria are sensitive to the cardinality of the set. To evaluate a feature attribution method, sort features by weight and evaluate Robustness-Sr for top-K features. The text discusses evaluating Robustness-Sr and comparing it to existing measurements for feature attribution ranking. It introduces the concept of untargeted and targeted adversarial robustness, highlighting the importance of key features in classification. The proposed criteria aim to address why an example is classified a certain way and provide interesting results in experiments. The proposed criteria for evaluating Robustness-Sr focus on measuring the importance of features by considering perturbations in feature values rather than feature removal. This approach allows for a broader view of prediction behavior and captures a wider range of important features. The text discusses methods for evaluating the robustness of neural networks, including adversarial attacks and neural network verification methods. Adversarial attacks aim to find a feasible solution that leads to an upper bound of distortion distance, while verification methods provide a lower bound to ensure model predictions remain stable within a certain perturbation range. These methods have different efficiency and time consumption characteristics. The proposed framework can be combined with methods to compute robustness, such as attacks and verifications. The evaluation focuses on maximizing/minimizing robustness under a cardinality constraint. The proposed framework combines methods to compute robustness, focusing on maximizing/minimizing robustness under a cardinality constraint. To address optimization problems for Robustness-S r, a greedy-styled algorithm is proposed to iteratively add the most promising feature into the set S r until it reaches the size constraint K. The proposed framework introduces a Greedy method to iteratively add features to set S r until it reaches size K. To efficiently consider the correlation between features, a smoothed regression approach is proposed to solve optimization problems for Robustness-S r. The proposed method suggests learning a mapping from binary space to approximate the function g, capturing joint relationships between features and smoothing out noises. This approach allows for efficient consideration of feature correlations in optimizing Robustness-S r. The Reg-Greedy approach samples subsets of one-hot encoded vectors to capture feature interactions and correlations in linear regression, smoothing out noise encountered by greedy methods. This method considers feature correlations in optimizing Robustness-S r efficiently. The Reg-Greedy approach proposes a method to efficiently capture feature interactions and correlations in linear regression by using regression with a special kernel. Sampling from the distribution of {0, 1} d can lead to exponential complexity, so the approach incrementally adds indices to S t r using regression weights to decide which index to add. This method aims to optimize Robustness-S r by considering feature correlations. The Reg-Greedy approach combines regression in a greedy procedure to narrow down the sampling space and focus on feature interactions. It aims to optimize Robustness-S r by gradually adding indices based on regression weights. The method has a sample complexity of O(d) and considers the \u2113 2 norm for all experiments. In the evaluation of different algorithms for optimizing Robustness-S r, various approaches such as Reg-Greedy, One-Step Reg, and Greedy were considered alongside baseline methods like vanilla gradient and integrated gradient. Perturbation-based approaches like leave-one-out and SHAP were also included in the comparison. The results were presented in quantitative evaluations and rank correlations between explanations with respect to original and randomized models. The study compared different explanation methods for optimizing Robustness-S r and Robustness-S r on image datasets like MNIST and ImageNet, as well as a text dataset YahooAnswers. Results showed that the pure greedy method performed poorly compared to regression-based methods, possibly due to ignoring feature correlations and introducing noise. The proposed regression-greedy method consistently outperforms others on optimization criteria. Vanilla gradient performs competitively as a baseline method. Visualizing explanations will be done using Reg-Greedy with Robustness-S r criterion. Commonly adopted measurements are Insertion and Deletion. The proposed regression-greedy method, Reg-Greedy, outperforms others on optimization criteria. Comparison with other explanations is done using Deletion and Insertion criteria. These criteria measure the impact of top-relevant features on predicted class probabilities. In experiments, the proposed method consistently outperforms other explanations on Deletion and Insertion criteria, showing a good influence on prediction. AUCs are reported in Figure 12 and Table 3. A sanity check is conducted to ensure the explanation reflects the model behavior. The proposed explanation reflects model behavior. A sanity check was conducted by randomly re-initializing model parameters. Reg-Greedy showed lower rank correlation, indicating sensitivity to parameter changes. Visualized explanations on MNIST showed differences in highlighting white pixels among explanation methods. Reg-Greedy focuses on highlighting both crucial positive and pertinent negative pixels around digits, providing additional insights not captured by baseline explanations. It shows better performance on the Robustness-S r score, indicating sensitivity to parameter changes. In discussing robustness measurement, examples are provided in Figure 4 to explain why an input digit is classified as one class and not another. The explanation is based on a targeted perturbation distance towards a specific class, showing how the generated explanation varies depending on the target class. The highlighted pixels in the explanation are crucial for distinguishing between different classes. The explanation highlights how changing certain pixels can alter the classification of a digit. It shows a unique feature of the explanation method compared to other existing techniques. The explanation method's ability to capture pertinent negative features depends on input range. LRP may fail to highlight negative pixels if input intensity is normalized between 0 and 1. The explanation method in ReLU network differs from Oramas et al. (2019) as it does not convey specific target class information. Users need to infer the target class from pertinent negative features. Qualitatively, the ReLU method provides more natural explanations compared to Oramas et al. (2019). Our method provides visual explanations on ImageNet, highlighting key features for different predicted classes such as \"fish\", \"bird\", \"dog\", and \"sea lion\". When changing the targeted class to 7, our explanation highlights crucial evidence that distinguishes between 2 and 7, suggesting our method is more effective for targeted explanation tasks. Our method provides visual explanations on ImageNet, highlighting key features for different predicted classes such as \"fish\", \"bird\", \"dog\", and \"sea lion\". Comparing explanations quantitatively on ImageNet, Reg-Greedy outperforms existing explanations. Visualization results in Figure 6 demonstrate that our method provides more compact explanations focusing on the actual object being classified. Additionally, our explanation method is applied to text classification models, selecting embedding vectors with the largest reward at each iteration. The Greedy algorithm is used to select the choice with the largest reward, especially in the context of an LSTM network classifying sentences into different categories. Experiments show that the Greedy method generates explanations that align closely with human intuition, particularly in predicting the label \"sport\". The method proposes an objective measurement of feature-based explanations by measuring the \"minimum adversarial perturbation\". Objective Measurements for Explanations Evaluation has been a challenging task due to the lack of ground truth. While human intuitions can be used to assess explanations, they may favor user-friendly explanations that do not necessarily reflect the model's behavior. Recent literature has proposed objective measurements, also known as functionally-grounded evaluations, to assess explanation fidelity. These measurements include completeness, sensitivity-n, local accuracy, and infidelity, aiming to ensure the sum of attribution values align with the prediction difference of the original input and baseline. The second family of explanation evaluation methods focus on identifying the most important features for a prediction by removing or preserving them. Various approaches have been proposed, such as evaluating feature attribution scores, optimizing explanations through an optimization problem, and training an auxiliary model to learn the explanation generation process. These methods aim to ensure that the change in prediction function value reflects the presence or absence of specific features. In evaluating model fidelity and feature removal, approximations are often used to estimate the effect of removing certain features. This can involve setting feature values to zero, the mean, blurred, random, or using generative models to represent feature absence. Adversarial robustness is a key aspect in machine learning model evaluation. Various algorithms have been developed to find adversarial examples, but they are mainly focused on neural networks. For other models like tree-based or nearest neighbor classifiers, decision-based attacks are used to find adversarial examples. The proposed framework can be applied to decision-based attacks in classifiers. It can also be used for neural network verification, but this process is slow and provides loose lower bounds. The work is related to counterfactual explanations and methods that enforce semantic structure for perturbations. In this paper, a new evaluation criteria called robustness analysis is introduced to establish the link between a set of features and a prediction. A new explanation method is developed to optimize this criterion by finding important feature sets. Experimental results show that the proposed explanations capture significant features across various domains. The proposed explanation method optimizes the robustness analysis criterion by identifying important feature sets across multiple domains. Comparisons between the methods are shown in Figure 8, highlighting untargeted and targeted Robustness-S r criteria."
}