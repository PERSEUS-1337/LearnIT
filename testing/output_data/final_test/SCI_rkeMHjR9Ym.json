{
    "title": "rkeMHjR9Ym",
    "content": "We study discrete time dynamical systems in recurrent neural networks, utilizing stochastic gradient descent to learn weight matrices from input data. The SGD estimate linearly converges to the ground truth weights with near-optimal sample size. Results apply to increasing activations with bounded derivatives. Numerical experiments confirm fast convergence on ReLU and leaky ReLU. Sequential data in various fields like natural language processing, time series prediction, and control design often rely on algorithms rooted in dynamical systems theory. Linear models such as Kalman filter and PID controller have been successful in control theory since the 1960s. More recently, nonlinear models like recurrent neural networks (RNN) have been used for complex tasks like machine translation and speech recognition. Unlike feedforward neural networks, RNNs are dynamical systems that use their internal state to process inputs. RNNs are dynamical systems that process inputs using their internal state. This work aims to explain the inner workings of RNNs, focusing on the RNN state equation with nonlinear activation function \u03c6, state weight matrix A, and input weight matrix B. The equation h t+1 = \u03c6(Ah t + Bu t ) governs the dynamic behavior of RNNs and distinguishes them from feedforward networks. The weight matrices A and B determine the dynamics and are learned using stochastic gradient descent (SGD). Contributions include exploring the efficiency of SGD for learning these weight matrices from input/state pairs. The study focuses on the convergence of stochastic gradient descent (SGD) in RNNs with various activation functions, showing linear convergence to ground truth weight matrices for stable systems. The required sample size is O(n + p), extended to unstable systems with samples from multiple RNN trajectories. Results apply to increasing activation functions like leaky ReLU and Gaussian input data, with numerical experiments supporting faster convergence as activation slope increases. The paper characterizes the statistical properties of the state vector and presents a novel SGD convergence result with nonlinear activations. It contributes to the foundational understanding of RNN training via SGD and is related to optimization and statistics literature on linear dynamical systems and neural networks. The work identifies weight matrices in linear system identification and is connected to the optimal control problem. Recent literature explores data-dependent bounds and characterizes non-asymptotic learning performance. Studies by Recht et al. delve into optimal control problems, while Hardt et al. demonstrate gradient descent's ability to learn single-input-single-output (SISO) LDS with polynomial guarantees. Sample complexity bounds for learning LDS are provided by BID7 and Faradonbeh et al. BID12 and BID10 focus on identifying sparse systems, and BID6 shows stable RNNs can be approximated by feed-forward networks. The notation includes spectral norm, minimum singular value, and 1-Lipschitz activation functions. Proper scaling of parameters can transform systems with Lipschitz activation into 1-Lipschitz activation systems. The dynamical system described involves an activation function \u03c6(\u00b7) and weight matrices A, B. The state equation corresponds to a recurrent neural network with hidden state h t and input u t. The output equation for Elman networks is characterized by output activation \u03c6 y and weights C, D. Our goal is to learn unknown weights A and B efficiently using a constant step size in a regression problem formulated with RNN trajectories. The approach involves utilizing SGD on the loss function to learn the ground truth parameter C. The analysis focuses on learning the ground truth parameter C using SGD with a constant learning rate. Algorithm 1 estimates \u0108 = \u0398 after SGD iterations, from which A and B are decoded. The choice of activation function affects the state equation analysis, with a focus on Lipschitz and increasing activation functions.\u03b2-increasing activation functions are defined, excluding ReLU but including Leaky ReLU as a generalization. Leaky ReLU is a \u03b2-increasing function obtained by blending an increasing and 1-Lipschitz activation with the linear activation. The state-vector covariance \u03a3[h t ] is well-conditioned for SGD under proper assumptions. The system is stable if A < 1, leading to exponential convergence of the state vector h t to 0. This stability ensures convergence of the (B t ) t\u22650 sequence. The condition number of the covariance plays a critical role in the analysis, bounded by \u03c1. The main result guarantees non-asymptotic convergence for SGD in stable systems (A < 1), with insights into sample complexity and convergence rate. Theorem 3.3 requires N(n + p)/\u03b2^4 samples for learning. Theorem 3.3 guarantees non-asymptotic convergence for SGD in stable systems with insights into sample complexity and convergence rate. It requires O(n(n + p) log 1 \u03b5 ) iterations to reach \u03b5-neighborhood of the ground truth, which can be accelerated for odd activation functions. Theorem 3.4 discusses faster learning for odd activation functions in SGD iterations. With specific choices of parameters, the convergence rate is guaranteed with high probability. The SGD error decays as a function of \u03b2, with larger values leading to faster convergence, as demonstrated in numerical experiments. The proof strategy combines ideas from statistics and optimization to show the well-behaved nature of input data. The input data is proven to be well-behaved with a well-conditioned covariance. The random input data provides sufficient excitation for the output state. Analyzing temporal dependencies between samples is challenging, but the dependency between samples decays exponentially fast. Subsampling the original trajectory allows for obtaining nearly independent data. The input data is well-behaved with a well-conditioned covariance, providing sufficient excitation for the output state. Analyzing temporal dependencies between samples is challenging, but decays exponentially fast. Subsampling the trajectory allows for obtaining nearly independent data. A logarithmically small T can generate large subtrajectories of size N/T. Additional perturbation arguments in Appendix D establish the well-behavedness of the data matrix. Theorem 4.1 proves fast convergence for \u03b2-increasing activations and well-behaved datasets. The convergence result shows the linear convergence of SGD for \u03b2-increasing functions by connecting the strong convexity parameter of the loss function. The strong convexity parameter of the loss function with \u03b2-increasing activations is connected to the loss function with linear activations. The linear loss is defined, and a convergence result is established. The SGD convergence theorem is derived for \u03b2-increasing functions with specific constraints on input samples and learning rate. The theorem provides a convergence rate for SGD with \u03b2-increasing activations, extending results on linear regression. Previous work has explored similar results for isotonic regression, robust regression, and leaky ReLU activations under specific assumptions. Our work establishes a deterministic linear convergence guarantee for SGD. Our work establishes a deterministic linear convergence guarantee for SGD with \u03b2-increasing activations, extending results on linear regression. Extensions to proximal gradient methods for high-dimensional nonlinear problems are left for future work. The main results in Section 3 address the conditions under which Theorem 4.1 is applicable to data obtained from RNN state equations. Desirable characteristics of the state vector are provided to enable statistical results. The state vector satisfies certain conditions, ensuring well-concentration and small expectation. The theorem provides statistical guarantees for learning the RNN state equation based on these assumptions. SGD iterations on the loss function satisfy certain conditions with high probability, isolating the optimization problem from the statistical properties of the state vector. The optimization problem is isolated from the statistical properties of the state vector. Tighter bounds on achievable parameters lead to improved performance for SGD. Theorems 3.3 and 3.4 are corollaries of Theorem 4.2 with specific parameter settings. Learning from a single RNN trajectory is discussed for stable systems, where earlier states' impact diminishes over time. Multiple nearly-independent trajectories are considered for unstable systems to address the amplification of older states' impact. The text discusses utilizing independent trajectories for learning weights A and B using the SGD Algorithm 1. The samples collected are now independent of each other, simplifying the analysis. An upper bound on the condition number of the state-vector covariance, denoted as \u03c1, is crucial for the analysis. This term involves B T0 instead of B \u221e, as B \u221e = \u221e when A > 1. The text discusses the importance of the condition number \u03c1 for single-output systems, particularly in unstable systems. It involves independent trajectories, sample size requirements, and running SGD over specific equations to achieve desired results. In experiments on ReLU and Leaky ReLU activations, the state dimension is n = 50 and input dimension is p = 100. The ground truth matrix A is a scaled random unitary matrix, while B has i.i.d. N(0,1) entries. The scaling \u00b5 is determined from empirical covariance matrices using Algorithm 2. The learning rate used is \u03b7 = 1/100 in all experiments. In experiments with ReLU and Leaky ReLU activations, the ground truth state matrix A is evaluated using two performance measures. The normalized error and normalized loss are calculated by running Algorithm 1 for 50000 SGD iterations. Results are plotted as a function of \u03c4, with N = 500. Leaky ReLU errors are plotted in FIG4 with varying slopes, showing linear convergence with increasing \u03b2. The rate of convergence significantly improves for less stable systems driven by A with a larger spectral norm. ReLU converges for small A, but SGD gets stuck before reaching the ground truth when A = 0.8. The training loss stagnates for more unstable systems, and the overall gradient continues to decay, indicating that SGD converges before reaching a global minima. As A becomes more stable, the rate of convergence improves, and a linear rate is visible. Increasing the sample size to N = 2500 for ReLU experiments shows that SGD converges for this more overdetermined problem. The study increased the sample size to N = 2500, finding that SGD converges even for A = 0.9 in the more overdetermined problem. Results show that ReLU activation has a well-behaved population landscape of loss, but requires more data compared to Leaky ReLU to find global minima. Experiments confirmed that SGD quickly finds optimal weight matrices for the state equation, with improved convergence rate as activation slope \u03b2 increases. The research demonstrated that SGD can efficiently learn nonlinear dynamical systems characterized by weight matrices and activation functions, with optimal sample complexity. Our study on system identification demonstrated efficient learning with optimal sample complexity and computational performance, focusing on activations like Leaky ReLU. Empirical evidence showed Leaky ReLU's faster convergence and lower sample requirements compared to ReLU. Unanswered questions include improving bounds on state-vector covariance, exploring performance bounds for ReLU, and addressing the complexity of hidden states in RNNs. Learning the (A, B, C, D) system for linear dynamical systems is challenging, and adding nonlinear activations complicates the task. Theoretical guarantees were obtained using normally distributed input and least-squares regression. The study aims to provide insights for classification tasks like natural language processing. Theorem 4.1 proof involves distinct scalars and bounds on expectations. The section characterizes the properties of the state vector ht when the input sequence is normally distributed, providing bounds on expectations and distinct scalars. The Orlicz norms define scalar random variables and provide a framework for subgaussianity. The state equation shows that ht+1 is a Lipschitz function of the input sequence. Fixing all vectors except u\u03c4, ht+1 is a Lipschitz function of u\u03c4. The Orlicz norms define scalar random variables and provide a framework for subgaussianity. Fixing all vectors except u\u03c4, ht+1 is a Lipschitz function of u\u03c4 for 0 \u2264 \u03c4 \u2264 t. The function ht+1(u\u03c4) is A t\u2212\u03c4 B Lipschitz. There exist absolute constants c > 0 such that ht satisfies certain bounds. Observing that a0 = qt, at = qt, the telescopic sum is written. Focusing on the terms f(ai+1) \u2212 f(ai), the only difference is the ui, \u00fbi terms. Applying the Cauchy-Schwarz inequality to bound the sum, it is shown that ht is a Bt Lipschitz function of qt. When ut \u223c N(0, Ip), qt is distributed as N(0, Itp). Using Gaussian concentration of Lipschitz functions, \u03b1v satisfies certain conditions. The relation ht+1 \u2264 Aht + But is established. The recursion for ht is established, showing a symmetric distribution around 0. The state-vector lower bound theorem is then discussed for the nonlinear state equation. The state-vector lower bound theorem states that for a nonlinear state equation with a \u03b2-increasing function \u03c6, the state vector obeys certain conditions. The proof involves writing ht as the sum of two independent vectors, one with independent entries. By considering a multivariate Gaussian vector g, it can be shown that But \u223c g1 + g2 where g1, g2 are independent and g1 has i.i.d. entries. This result is crucial for establishing a lower bound. Theorem B.6 focuses on refining the lower bound for multiple-input-single-output (MISO) systems. It states that for any t \u2265 1, the state vector obeys certain conditions. Lemma B.7 provides a vector lower bound for a \u03b2-increasing function \u03c6 applied to a random vector x with i.i.d. entries. Applying the law of total covariance simplifies the problem by using a lower bound based on the independence of x and y. The covariance \u03a3x[\u03c6(x + y)] is a diagonal matrix with its lowest eigenvalue representing the minimum variance. Lemma B.9 provides a lower bound for var[\u03c6(xi + yi)] \u2265 \u03b2^2 var[X]. This lower bound holds even after taking the expectation over y, concluding the proof. Lemma B.8 (Law of total covariance) states that for random vectors x and y with finite covariance, certain conditions apply. The text discusses the application of the law of total expectation to derive a covariance bound for stable systems with temporal dependence. It also introduces a scalar lower bound lemma for a \u03b2-increasing function and provides a proof for it. The analysis highlights the exponential decay of past states' impact in stable systems. The text discusses the exponential decay of past states' impact in stable systems. It introduces the truncation of the state vector and shows that the impact of truncation can be small for stable systems. The text discusses the construction of sub-trajectories in stable systems, showing how a single trajectory can be split into multiple nearly independent trajectories based on a defined sampling rate and offset. The text discusses the construction of sub-trajectories in stable systems, showing how a single trajectory can be split into multiple nearly independent trajectories based on a defined sampling rate and offset. The truncated sub-trajectory states are independent of each other, and if the input is randomly generated, the truncated states are fairly close to the actual states. The text discusses constructing sub-trajectories in stable systems by splitting a single trajectory into independent segments based on a sampling rate and offset. The truncated sub-trajectory states are independent and close to the actual states when the input is randomly generated. The section provides bounds on the condition number of data matrices obtained from the RNN trajectory. The text addresses constructing independent sub-trajectories in stable systems by splitting a single trajectory based on a sampling rate and offset. It provides bounds on the condition number of data matrices obtained from the RNN trajectory. The text discusses bounding the impact of perturbation E using Theorem F.1 and Corollary F.2 on Q. It also introduces assumptions on scaling \u00b5 and matrix X, providing bounds on the 2-norm of each row of X and X^T X. The text discusses bounding the impact of perturbation E using Theorem F.1 and Corollary F.2 on Q. It also introduces assumptions on scaling \u00b5 and matrix X, providing bounds on the 2-norm of each row of X and X^T X. In the following paragraphs, the bound for individual matrices X (\u03c4) is shown using Lemmas D.1 and A.1, with considerations for sub-trajectory matrices and specific events with probabilities. The text discusses bounding the impact of perturbation E using Theorem F.1 and Corollary F.2 on Q. It also introduces assumptions on scaling \u00b5 and matrix X, providing bounds on the 2-norm of each row of X and X^T X. In the following paragraphs, the bound for individual matrices X (\u03c4) is shown using Lemmas D.1 and A.1, with considerations for sub-trajectory matrices and specific events with probabilities. Let us call this Event 2. We will show that our choice of L ensures right hand side is small enough and guarantees the desired claim. Both Event 1 and Event 2 hold with probability 1\u22124 exp(\u2212c1N0/\u03c1 2 )\u22122N\u03c4 exp(\u2212100n), implying (D.5) holds with Q = X (\u03c4). Union bounding this over 1 \u2264 \u03c4 \u2264 L, (D.5) uniformly holds with Q = X (\u03c4) and all rows of X are 2-bounded with probability 1 \u2212 4N exp(\u2212100n) \u2212 8L exp(\u2212c1N0/\u03c1 2). Applying Lemma A.1 on (X (\u03c4)) L \u03c4=1, we conclude with the bound (D.4) on the merged matrix X. The text discusses the convergence of stochastic gradient descent (SGD) by combining Theorem D.3 with a deterministic SGD convergence result. It shows that the convergence of individual rows of matrix C implies the convergence of the overall matrix \u0398\u03c4. SGD updates each row of C separately, leading to the convergence result. The text discusses the convergence of stochastic gradient descent (SGD) by combining Theorem D.3 with a deterministic SGD convergence result. Convergence of individual rows of matrix C implies convergence of the overall matrix \u0398\u03c4. SGD updates each row of C separately, leading to the convergence result. The proof of Theorem 3.3 involves applying Lemmas B.3 and 3.2 to establish convergence for a particular row of C. The proof of Theorem 3.3 involves applying Lemmas B.3 and 3.2 to establish convergence for a particular row of C. Theorem E.1 provides a parametrized result on unstable systems based on certain assumptions. Theorem E.1 provides a parametrized result on unstable systems under specific assumptions. It involves running SGD with certain parameters and guarantees that all iterates satisfy a certain condition with high probability. Theorem F.1 bounds the empirical covariance of matrices with independent subgaussian rows, guaranteeing certain conditions with high probability. Theorem F.1 bounds the empirical covariance of matrices with independent subgaussian rows. Centering subexponential variables around zero introduces a factor of 2 when bounding subexponential norm. The tail bound for the sum of independent zero-mean subexponential random variables is given."
}