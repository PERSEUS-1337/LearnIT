{
    "title": "Hkn7CBaTW",
    "content": "Explanation methods like DeConvNet, Guided BackProp, and LRP were created to understand deep neural networks, but they do not provide accurate explanations for linear models. Despite this, they are commonly used on complex multi-layer networks. This raises concerns as linear models are simpler neural networks. The proposed generalization based on linear models introduces two new explanation techniques (PatternNet and PatternAttribution) that are theoretically sound for linear models and offer improved explanations for deep networks. Deep learning has had a significant impact on various applications, with recent neural network classifiers excelling at detecting relevant signals in input data like images. These classifiers use multiple layers with millions of parameters to filter out distractions and produce a condensed output indicating the presence of a specific signal, such as a cat in an image. While deep neural networks are powerful, they are often seen as 'black-box' models. Various techniques have been proposed to better understand how these classifiers make decisions and operate. Various techniques, such as saliency maps, DeConvNet, and Guided BackProp, aim to explain classifier decisions by visualizing how relevant signals are encoded in input data. The visualization should highlight aspects of the input image that led to the classifier's decision, such as detecting a cat. These methods operate under the assumption that the output signal can be propagated back through the classifier to provide an explanation. Explanation methods like Layer-wise Relevance Propagation (LRP) and Deep Taylor Decomposition (DTD) visualize pixelwise contributions in color channels. These methods are applied to deep learning models like ImageNet, but lack theoretical analysis and empirical evaluations. Gradient-based methods like DeConvNet and Guided BackProp fail to handle individual linear layers correctly. In this work, gradient-based methods fail to recover the signal even for a single-layer linear model, indicating their limitations in explaining deep neural networks. The authors propose two novel explanation methods, PatternNet and PatternAttribution, to improve explanations in real-world deep neural networks visually. The authors propose two novel explanation methods, PatternNet and PatternAttribution, to improve explanations in real-world deep neural networks visually and quantitatively. They aim to provide qualitatively and measurably improved explanations, crucial for reliable explanation techniques in non-intuitive domains like health and sciences. The notation and scope include definitions for scalars, vectors, and covariance, with a focus on analyzing neural networks excluding the final soft-max output layer. The networks considered have linear neurons with optional ReLU, max-pooling, or soft-max layers for analytical treatment. The curr_chunk discusses the analysis of linear neurons and nonlinearities independently in neural networks. It highlights that the weight vector of linear models does not explain the signal it detects, as its primary objective is canceling the distractor. Various explanation methods for deep neural networks are also mentioned, such as saliency map, DCN, GBP, LRP, and DTD. Biases are considered constant neurons for clarity in the analysis. The curr_chunk discusses analyzing explanation methods for deep neural networks using a linear model and data sampled from a linear generative model. It focuses on how the signal and distractor components are encoded in the input data and the shortcomings of current explanation approaches. The example involves training a linear regression model to extract the signal from the data, where the distractor obfuscates the signal, making the detection task more challenging. The model needs to filter out the distractor to optimally extract the signal. The weight vector in the linear model acts as a filter to extract the signal by filtering out the distractor. The optimal weight vector is orthogonal to the distractor, adjusting when the distractor's direction changes. Changes in the signal direction can be compensated for by adjusting the weight vector's sign and magnitude. The direction of the weight vector is mainly influenced by the distractor in a linear model. In linear models, the weight vector filters out distractors to extract the signal. The direction of the weight vector is influenced by the distractor, which can be compensated for by adjusting its sign and magnitude. Gaussian noise can be mitigated by averaging over measurements, and adding noise corresponds to L2 regularization. Deep neural networks are affected by both noise and the signal direction in determining the weight vector. In deep neural networks, gradient-based methods struggle to distinguish signal from distractor in linear models, leading to sub-optimal explanations in deeper networks. Improved layer-wise explanation techniques are developed to provide better explanations for deep neural networks. Terminology includes the filter w for optimal output extraction, pattern a s for output variation direction, signal s = a s y, and distractor d for non-informative data components. Explanation methods for individual classifier decisions are discussed in this section, focusing on function, signal, and attribution visualizations. These methods provide different information about the network and complement each other. Gradients and saliency maps are used to explain the function in input space, approximating how the model extracts output from input. Saliency maps estimate the influence of moving in a specific direction in input space on the output. The saliency map in deep neural networks helps to analyze the weights and extract the signal direction. Different methods like DeConvNet, Guided BackProp, and PatternNet aim to map network activations back to input pixel space to show the original input pattern causing a specific activation. The signal detected by the neural network represents the component of the data that triggers network activations. Attempts were made to visualize deep neural network signals using DeConvNet and Guided BackProp. These methods treat rectifiers differently, with DeConvNet leaving them out from the forward pass and adding ReLUs after each deconvolution, while Guided BackProp uses ReLUs from the forward pass. The back-projections for the network's linear components approximate the features that activate higher layer neurons, but do not guarantee a reconstruction in input space. For linear models, these visualizations reduce to the gradient showing the filter w, not the pattern a s or the signal s. PatternNet is proposed as a new approach to estimate the correct direction for visualizing deep neural network signals. Unlike DeConvNet and Guided BackProp, PatternNet aims to improve upon their visualizations by considering the direction of the filter w and the signal s. Additionally, the concept of attribution is introduced to analyze how signal dimensions contribute to the output through the layers. This approach provides a more optimal attribution by element-wise multiplying the signal with the weight vector. BID0 and BID11 introduced layer-wise relevance propagation (LRP) and deep Taylor decomposition (DTD) to decompose pixel-wise contributions in neural networks. DTD decomposes neuron activations based on input contributions using a first-order Taylor expansion. Relevances are redistributed towards inputs in the backward pass, similar to how a ReLU unit stops gradient propagation. The deep Taylor decomposition (DTD) involves choosing a root point x0 to estimate the distractor and extract the signal from the data. PatternAttribution is an extension of DTD that learns how to set the root point from data. It attributes output values to input dimensions to show the relevance of individual components to the output, similar to LRP. Visualizing the function is straightforward, but visualizing the signal and attribution is more challenging, requiring a good estimate of the signal and distractor. The text discusses the challenge of accurately estimating the signal and distractor in neural networks. A quality measure for neuron-wise signal estimators is proposed to evaluate existing approaches and derive optimal signal estimators. These estimators are used in techniques like PatternNet and PatternAttribution to explain the signal and attribution. The input data consists of signal and distractor components, with the signal contributing to the output. Estimating the signal is a complex problem, and linear estimators are used in this context. The text discusses the challenge of accurately estimating the signal and distractor in neural networks. A quality measure for neuron-wise signal estimators is proposed to evaluate existing approaches and derive optimal signal estimators. Linear estimators are used to address the ill-posed problem of estimating the signal. The introduced quality measure evaluates how much information about the signal can be reconstructed from the residuals using a linear projection. The optimal signal estimators remove most of the information in the residuals, yielding large quality measure values. The text discusses signal estimation in neural networks, focusing on finding the optimal signal estimator. The identity estimator assumes all data is signal, leading to the z-rule in the deep Taylor framework. For ReLU and max-pooling networks, the z-rule simplifies to element-wise multiplication of input and saliency map, treating the entire network input as signal. The text discusses signal estimation in neural networks, focusing on finding the optimal signal estimator. It implies that distractors are included in the attribution, and the contributions from distractors are cancelled out during the forward pass but not in the backward pass. The noisy nature of visualizations based on the z-rule is caused by distractor contributions. The implicit assumption made by DeConvNet and Guided BackProp is that the detected signal varies in the direction of the weight vector. In the deep Taylor decomposition framework, this corresponds to the w 2 -rule, resulting in a specific signal estimator. The text discusses finding the optimal signal estimator in neural networks. It suggests learning the signal estimator S from data by optimizing a criterion. The estimator should have zero correlation for all possible v to be optimal. Two possible solutions to this problem will be presented. The contribution from the bias neuron is considered 0 when optimizing the estimator. The linear estimator assumes a linear dependency between input signals and output, yielding a signal estimator. This approach works well for convolutional layers but leaves correlation in dense layers with ReLUs. To improve, a two-component estimator is proposed. The text discusses moving beyond the linear signal estimator by understanding how the rectifier influences training. It emphasizes the need to distinguish between positive and negative regimes in order to approximate the signal accurately. The proposal of a two-component signal estimator is introduced, with expressions derived for patterns in each regime. The text proposes PatternNet and PatternAttribution as a two-component signal estimator to optimize signal estimation in both positive and negative regimes. It involves a layer-wise back-projection of the estimated signal to input space, using neuron-wise nonlinear signal estimators in each layer. PatternAttribution is a signal estimator that improves upon existing frameworks like LRP and DTD by providing clearer heat maps through neuron-wise contributions to the classification score. It replaces weights with informative directions during the backward pass, similar to backpropagation. The method focuses on evaluating explanations for image classification using Theano BID2 and BID4. It utilizes the ImageNet dataset with a pre-trained VGG-16 model, rescaled and cropped images to 224x224 pixels. Signal estimators are trained on the first half of the dataset, and a vector is optimized on the second half for generalization testing. Results are based on the official validation set of 50000 samples. The signal estimators are trained on the first half of the dataset and a vector is optimized on the second half for generalization testing. The linear and two component signal estimators are obtained by solving closed form solutions. The quality of a signal estimator is assessed with a computationally prohibitive equation, which is optimized using stochastic mini-batch gradient descent. Individual explanations are computationally cheap after learning to explain. Our method produces explanations quickly by implementing a back-propagation pass with a modified weight vector. It is applicable to image models and a generalization of neuroimaging theory. The correlation measure shows that the filter-based estimator removes some information in the first layer, but the gradient performs similarly to a random estimator in higher layers. The optimized estimators in neural networks do not correspond to the detected stimulus, implying invalid assumptions made by DeConvNet and Guided BackProp. S a and S a+\u2212 perform comparably in convolutional layers, with S a+\u2212 being best in dense layers. Image degradation is measured directly for individual neurons and indirectly for the whole network by corrupting patches based on attribution ordering. In an experiment, image patches are analyzed by computing attribution values and sorting them based on heat map values. Patches are replaced with their mean per color channel to assess the impact on classifier output. Different estimators are evaluated, with the two component model showing the fastest decrease in performance. The baseline, where patches are randomly ordered, performs the worst. The two component model S a+\u2212 leads to the fastest decrease in confidence in the original prediction by a large margin. Its excellent quantitative performance is also backed up by visualizations. Comparing signal estimators on input images, the S w estimator captures some structure, while the optimized estimator S a struggles with color information. The two component model S a+\u2212 produces a crisp heat map of the attribution. Visualizations for six randomly selected images from ImageNet show PatternNet's ability to recover patterns. PatternNet is able to recover a signal close to the original without additional rectifiers, unlike DeConvNet and Guided BackProp. The optimization of the pattern captures important directions in input space, producing crisp visualizations. Comparison with other methods like LRP and DTD shows PatternNet's superiority in capturing the true signal in the data. Additionally, the method is a generalization of previous work by BID5. Our proposed approach, a generalization of previous work by BID5, can optimally solve a toy example in section 2 for deep neural networks. It shares the idea of learning to explain a model properly with Zintgraf et al. and BID3. The approach is just as expensive as a single back-propagation step after training, allowing for real-time application. Understanding and explaining nonlinear methods in machine learning is a significant challenge, with theoretical contributions being scarce. The model gradient direction does not always provide an accurate estimate for the signal in the data. The model gradient reflects the relation between signal direction and noise contributions, challenging popular explanation approaches for neural networks. A proposed objective function improves signal visualizations and decomposition methods, leading to a better understanding of deep neural networks. This project has received funding from the European Union's Horizon 2020 research and innovation programme. The research project received funding from various sources including the European Union's Horizon 2020 program and the BMBF for the Berlin Big Data Center. The authors acknowledge contributions from several individuals and discuss visualization algorithms for ReLu networks, highlighting similarities and differences in approaches. The back-projection through a max-pooling layer is only through the active path in the forward pass. The Predictive-Differences analysis visualizations were created using opensource code with default parameter settings for VGG. The research project received funding from various sources including the European Union's Horizon 2020 program and the BMBF for the Berlin Big Data Center. The authors acknowledge contributions from several individuals and discuss visualization algorithms for ReLu networks, highlighting similarities and differences in approaches. Comparison between the proposed methods PatternNet and PatternAttribution to the Prediction-Differences approach by BID27."
}