{
    "title": "B1EGg7ZCb",
    "content": "Autonomous vehicles in city transportation are increasingly common. Companies are focusing on teaching these vehicles smart city fleet coordination. Simulation-based modeling and hand-coded rules currently dictate decision-making, but the potential for complex intelligent behavior through Reinforcement Learning is being explored. The Deep Q-Learning (DQN) model is being adapted to a multi-agent setting to teach agents to fulfill customer demand in a partially observable environment. Transfer learning is utilized to teach agents to balance multiple objectives, such as navigating to a charging station when energy levels are low. Evaluations show successful implementation of the solution. The solution presented successfully teaches agents cooperation policies in a multi-agent system for autonomous fleet control. The goal is to train agents/cars optimal relocation strategies to maximize fleet efficiency and meet customer trip demand, moving away from traditional hand-coded rules towards a more efficient approach. Recent works in Reinforcement Learning, particularly Deep Reinforcement Learning, have shown effectiveness in addressing complex problems with dynamic environments. Deep Q Learning (DQN) by DeepMind has demonstrated success in learning human-level performance in Atari 2600 games. This study aims to build upon Multi-Agent Deep Reinforcement Learning (MA-DRL) to develop adaptive decision-making for a multi-agent system of cars and fleet. In this study, the focus is on Multi-Agent Deep Reinforcement Learning (MA-DRL) applied to a system of cars and fleet coordination in a city environment. The approach involves representing the environment as an image-like state with specific layers of information. The study also explores teaching agents in a partially observable environment, utilizing Transfer Learning for teaching coordination strategies and optimizing utility. Results show success in teaching agents to coordinate with other cars, prioritize tasks like charging, and ensure their own survival while serving customers. Deep Reinforcement Learning is gaining attention for solving complex problems, especially in multiagent systems. Researchers like Tampuu, Ardi and Foerster, Jakob are exploring cooperation and competition among agents. However, scalability remains a challenge, with Palmer, Gregory addressing the issue of outdated memory in multi-agent systems. The novel approach introduced by Egorov and Maxim uses convolutional neural networks and stochastic policies to solve the multi-agent system problem of experience replay memory becoming outdated. The solution is aimed at large scale autonomous fleet control. The Reinforcement Learning model is trained to create an environment representing city dynamics for Uber/Lyft ride sharing. Agents travel to customers on the map to fulfill demand in this environment. The agent, a car in this case, has actions of right, left, up, down, and stay with an energy level that decreases over time. Agents cannot leave the defined map or collide with obstacles. Customers have a goal location and drop-off location, with a wait time before being removed from the system. Customers are generated from a distribution of demand data based on real trip data. Obstacles are locations agents cannot travel to, while charging stations are locations to refill energy levels. Open road spaces are where agents and customers can exist. Activities include advancing agents, resolving pickups or collisions, decrementing energy levels and wait times, removing customers that have waited too long or have been picked up. After generating customers from demand data, obstacles and charging stations are defined. Agents move, pick up customers, and manage energy levels. Rewards are assigned for different actions, such as moving efficiently and charging. Incentives are balanced to encourage optimal behavior without detracting from customer pickups. The environment for the Deep Reinforcement Learning model is represented as a vector with image-like structure, consisting of 5 layers encoding different information. These layers include Self Layer for agent's location and Other Agents Layer for locations of all other agents. The environment for the Deep Reinforcement Learning model consists of 5 layers encoding different information: Self Layer for agent's location, Other Agents Layer for locations of other agents, Customer Layer for customer locations and wait times, Obstacles Layer for obstacles and charging stations, and Extra Agent Information for energy and priority of the agent. The curr_chunk discusses the implementation of Deep Q Learning (DQN) in a partially observable multi-agent system. The goal is to learn an optimal decision-making policy to maximize future rewards using Q-Learning, a model-free learning algorithm. The Q-Function in Q-Learning is recursively calculated with transitions of (s,a,r,s), representing the state, action, reward, and next state. It is essentially the Bellman equation, calculating the current reward plus the discounted future reward of the next time step. This can be implemented as a table of states by actions and recursively solved if the environment is explored enough. Recent success in Deep Q Learning has been achieved by using neural networks to represent the Q-Function in complex environments with large state representations, like Atari games. Convolutional Neural Networks (CNNs) are used to interpret image data and estimate the Q(a,s) value, allowing for a more efficient and computationally feasible approach compared to traditional look-up tables. Using Convolutional Neural Networks (CNNs) to interpret image-like state representations is crucial for understanding spatial relationships within the environment. The geospatial location of agents in relation to others and customers is also important. CNNs allow for scaling environments to large sizes without a significant increase in processing time. The deep Q-Learning algorithm's convergence is facilitated by Experience Replay, e-greedy exploration, and target networks, which store transitions for training and enable random actions initially. The biggest challenge with multi-agent systems is optimizing one's control policy while understanding the intentions of other agents. Image-like state representations are modified to account for other agents in the system, with each agent having a layer representing themselves. Target networks, a separate network frozen in time, stabilize Q-values convergence during training. The final state s represents the image-like state after all agents have moved, with conflict resolutions needed before reaching the final state. Agents move sequentially based on priority, controlled by a single network for efficient fleet management. Incentivization and penalties are used to encourage efficient interactions among agents. The penalty for missing a customer incentivizes agents to act efficiently and learn a divide and conquer technique. The replay memory is updated with transitions from multiple agents by increasing its size. Partial observability sets this work apart from related works. The final addition to our methods involves making agents partially observable to limit the size of state image-like representations, preventing exponential growth in training time as the map size increases. This approach also helps with the missed customer penalty and improves efficiency in large environments with numerous customers appearing and disappearing. The final method discussed is Transfer Learning to teach agents multiple objectives in the AV fleet control problem. This approach aims to address the issue of agents becoming convoluted in recognizing the importance of picking up customers due to penalties for missed customers. By limiting the state representation size and making agents partially observable, efficiency is improved in large environments with numerous customers appearing and disappearing. However, a drawback is that agents may not generalize well to unseen maps with different geographies. Transfer learning was used to teach agents in the AV fleet control problem multiple objectives. Initially, agents were trained with an infinite amount of energy, then in the next iteration, each agent was given a random energy level to balance the main objective of fulfilling customer demand with the secondary objective of charging its battery when needed. This approach helped prevent agents from only going to charging locations or picking up customers until they ran out of energy. In the second part of the Results/Tests, experimental results were provided to test the effectiveness of agent communication and transfer learning for teaching agents to charge. Baseline policies were compared with Dijkstra's algorithm for shortest distance to customers. The goal was to demonstrate the power of Deep Reinforcement Learning (DRL) compared to rules-based approaches. The experiments aim to showcase the superiority of Deep Reinforcement Learning (DRL) over a rules-based Dijkstra's approach in learning coordination policies for maximizing rewards. The environment consists of a 7x7 map with 2 agents placed randomly, tasked with picking up customers on opposite sides of the map. Results show the model's ability to perform a divide and conquer strategy, outperforming the baseline model in fulfilling customer requests over time. The experiments demonstrate the superiority of Deep Reinforcement Learning (DRL) over a rules-based approach in teaching agents multiple objectives through transfer learning. The model agents efficiently perform a divide and conquer strategy, outperforming the baseline in fulfilling customer requests on a 7x7 map. The experiments showed the success of Deep Reinforcement Learning (DRL) in teaching agents to balance multiple objectives through transfer learning on a 10x10 bridge map with 4 agents. The model was trained to pick up customers with infinite energy initially, then fine-tuned with a random amount of energy and penalty for losing all energy. Epsilon for e-greedy exploration was set to .5 and linearly decayed to .01 over 3000 iterations, resulting in the agent learning to keep itself alive while picking up customers. The RL model was successful in learning how to balance objectives effectively by comparing it to two baseline agents - a conservative agent and a more aggressive agent. Setting hard thresholds for charging may not always be optimal as it depends on the scenario. The conservative baseline model successfully goes and charges itself when needed. The RL model effectively balances objectives, outperforming conservative and aggressive baseline models. Despite occasional deaths, the model significantly improves net reward by balancing customer fulfillment and deaths. Deep Reinforcement Learning is a powerful tool for solving complex problems, as seen in Deep Mind's success in teaching agents to defeat world champions in games like Go. Multi-Agent Reinforcement Learning offers potential for exploring agent communication. In a ride-sharing environment, a DRL solution was scaled up to maintain real-life dynamics. Agents were taught effective cooperation strategies and multiple objectives using a multi-agent deep reinforcement solution. Transfer learning was utilized to adapt decision policies for multiple objectives."
}