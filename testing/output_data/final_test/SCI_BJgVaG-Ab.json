{
    "title": "BJgVaG-Ab",
    "content": "The text discusses the challenges of deep reinforcement learning in control systems and introduces a framework that combines formal methods with hierarchical reinforcement learning. This framework allows for the specification of tasks with logical expressions, learns hierarchical policies, and can construct new skills from existing ones without additional learning. The proposed methods are evaluated in a grid world simulation and on a Baxter robot. Learning has gained attention for its success in games, robotics manipulation, and autonomous driving. However, training a policy to master a skill requires extensive interactions with the environment. Most learned policies are specific to one skill and not easily transferable. Skill composition involves creating new skills from existing ones without additional learning. This concept has been used in stochastic optimal control to develop optimal control laws based on Markov decision processes. Studies have shown that adding Q-functions can lead to approximately optimal policies in simulated manipulation tasks. Hierarchical reinforcement learning allows for transfer among tasks by obtaining task-invariant low-level policies. By re-training the meta-policy that schedules over the low-level policies, different skills can be obtained with fewer samples. Authors of BID7 have successfully applied this idea in learning locomotor controllers, while authors of BID18 have used a deep hierarchical architecture for multi-task learning with natural language instructions. Temporal logic, commonly used in software and digital circuit verification, has been utilized by BID1 and BID2 for expressing complex behaviors and causal relationships. BID19 and BID13 have used temporal logic to synthesize provably correct control policies. In this work, the focus is on hierarchical skill acquisition and zero-shot skill composition using temporal logic as the task specification language. The authors propose a technique to synthesize new skills without further interaction with the environment, leveraging the transformation between temporal logic formulas and finite state automata. Their main contributions include constructing deterministic meta-controllers directly from task specifications. In this work, the authors propose a technique to construct deterministic meta-controllers from task specifications using finite state automata. They use intrinsic rewards derived from the automaton's input alphabets to aid RL agents in learning complex behaviors efficiently. In this framework, deterministic meta-controllers are constructed from task specifications using finite state automata. Intrinsic rewards are derived from the automaton's input alphabets to aid RL agents in learning complex behaviors efficiently. The hierarchical policy allows for skill composition through FSA manipulation, resulting in a transparent compositional outcome. This method enables learning of hierarchical policies with any non-hierarchical RL algorithm without constraints on policy representation or problem class. The options framework BID23 is briefly introduced, along with the definition of a Markov Decision Process. An MDP is a tuple defined by state space S, action space A, transition function p, and reward function R. The goal is to find a policy that maximizes expected return over a fixed time horizon T. The options framework introduces temporal abstractions over the action space, with options defined as a tuple including a set of states I, an options policy \u03c0o, and a parameter \u03b2. The options framework introduces temporal abstractions over the action space, with options defined by a policy \u03c0o and a termination probability \u03b2. Tasks are specified with Truncated Linear Temporal Logic (TLTL), excluding the Always operator. The text discusses the translation of scTLTL formulas into finite state automata (FSA) using robustness degree to measure satisfaction levels. An FSA is defined with automaton states, input alphabet, transitions, and initial state. Transition in automata states at MDP state s can be calculated. The translation from scTLTL formulas to FSA can be done automatically. An example illustrates the FSA resulting from a formula \u00acb U a, with three automaton states. The input alphabet is defined, and the FSA transitions according to specific equations. The specification is satisfied when reaching the final state qf. In scTLTL, policies are defined to satisfy specific formulas. Problem 1 involves finding a policy that meets a given scTLTL specification. Problem 2 focuses on composing policies for multiple scTLTL formulas. Problem 2 involves composing policies for multiple scTLTL formulas by constructing a policy that satisfies the conjunction of given specifications. This is useful for breaking complex tasks into manageable components and combining policies to satisfy the original task. The approach involves constructing a product MDP from the given MDP and FSA for control synthesis using RL algorithms. Using product automaton for control synthesis has been extended to continuous state-action spaces for robotics systems. A policy switching scheme is proposed for Problem 2 to satisfy compositional task specifications, utilizing FSA characteristics for decision making. Problem 1 can be addressed with any episode-based RL algorithm, but sparse feedback is an issue. To tackle this and pave the way for automata-guided HRL, an FSA augmented MDP is introduced for scTLTL formulas. The FSA augmented MDP is defined by formula \u03c6 as M \u03c6 = S , A, p(\u00b7|\u00b7, \u00b7), R(\u00b7, \u00b7) where S \u2286 S \u00d7 Q \u03c6 , A is the same as the original MDP. The goal is to find the optimal policy that maximizes the expected sum of discounted return over the horizon T, using a policy \u03c0 : S \u2192 A. The FSA augmented MDP can be constructed with any standard MDP and a scTLTL formula, and solved with any algorithm. MDP can be constructed with any standard MDP and a scTLTL formula, and solved with any off-the-shelf RL algorithm. By learning the flat policy \u03c0, we avoid the need to learn multiple options policies separately. The optimal options policy for any option o q can be extracted from the optimal policy \u03c0. The reward function encourages transitioning to the next state, but may also promote entering trap states. To address this, imposing a constraint can help. One way to address the issue of trap states is by imposing terminal rewards on both q f and trap. Terminal rewards R q f = 2 and R trap = -2 are assigned. The learning routine using FSA augmented MDP is described in Appendix D, utilizing memory replay popular in off-policy RL methods. On-policy methods can also be used. A solution for Problem 2 involves constructing the FSA of \u03c6 from \u03c6 1 and \u03c6 2, using \u03c6 to synthesize the policy for the combined skill. Transition probability p \u03c6 is defined for q = (q 1 , q 2 ) \u2208 Q \u03c6 and q = (q 1 , q 2 ) \u2208 Q \u03c6. The product automaton A \u03c61 and A \u03c62 result in A \u03c6 where trap states occur if b is true before a, leading to a violation of the specification. State pairs with a trap state component are aggregated into one trap state in the product automaton. Equation (10) defines the predicates guarding the outgoing edges of q, q 1, and q 2 as \u03a8 q, \u03a8 q1, and \u03a8 q2 respectively. Equation (10) defines predicates for outgoing edges of states q, q1, and q2 in the product automaton. The transition at q in A \u03c6 exists only if corresponding transitions at q1 and q2 exist in A \u03c61 and A \u03c62. A switching policy based on robustness comparison is proposed for satisfying \u03c6 = \u03c61 \u2227 \u03c62. In a grid world navigation example, a robot moves in a 1-dimensional space with defined regions to visit. The robot follows a switching policy to satisfy a specific task specification. In a grid world navigation example, a robot moves in a 1-dimensional space with defined regions to visit. The robot follows a switching policy to satisfy a specific task specification. The robot needs to visit regions a and b at least once. Q-Learning BID28 is used to learn a deterministic optimal policy. The resultant optimal policy is illustrated in FIG1, where each automaton state serves a specific purpose. The robot must start at automata state q0 and follow the policy to reach both regions a and b. The robot in a grid world navigation example follows a switching policy to visit regions a and b. Q-Learning BID28 is used to learn an optimal policy, depicted in FIG1. To satisfy an added requirement \u03c62 = \u00acb U a, policy \u03c0 \u03c62 is learned instead of learning from scratch. The resultant optimal policy is shown in FIG4, where the robot aims to reach a while avoiding b. The robot follows a switching policy to visit regions a and b in a grid world navigation example. Q-Learning is used to learn optimal policies \u03c0 \u03c61 and \u03c0 \u03c62. These policies are combined to create policy \u03c0 \u03c61\u2227\u03c62, guiding the robot to reach a and then go to b. The robustness of both policies is the same from s = \u22125 to s = 0, but disagreements emerge as s becomes larger. In a grid world navigation example, Q-Learning is used to learn optimal policies \u03c0 \u03c61 and \u03c0 \u03c62 for a robot following a switching policy. Disagreements arise as s becomes larger, affecting the robustness of their conjunction. More complex tasks requiring temporal reasoning are evaluated on a simulated Baxter robot in a specific environment with square and circular regions. In a grid world navigation example, Q-Learning is used to learn optimal policies for a robot following a switching policy. Tasks involve square and circular regions, with objects like boxes and an interactive ball. Tasks are specified by scTLTL formulas, and policy optimization is done using proximal policy search BID20. The policy is a Gaussian distribution parameterized by a neural network with 2 hidden layers. State and action spaces vary across tasks and comparison cases. The action spaces vary across tasks and comparison cases, with different rewards structures designed for evaluation. Results are shown in FIG3, with robustness as the comparison metric. Reported values are averaged over 60 episodes. The FSA augmented MDP and terminal robustness reward performed similarly in convergence rate, with the FSA MDP showing lower variance in final performance. The FSA MDP achieved the highest success rate in robot deployment tests compared to the robustness reward. The policy switching technique for skill composition was evaluated by learning four simple policies and constructing combined policies. The resulting trajectories were compared with those learned from scratch using terminal robustness and heuristic rewards. The plots show that as task complexity increases, using robustness and heuristic rewards fails to satisfy specifications. In this paper, the FSA augmented MDP enables effective learning of hierarchical policies for tasks specified by scTLTL. Automata guided skill composition combines existing skills to create new skills without additional learning. Robotic simulations show that these methods enable simple policies to perform complex tasks, with limitations including discontinuity at the switching point. The FSA augmented MDP allows for hierarchical policy learning based on scTLTL tasks. Automata guided skill composition combines skills for new tasks without additional learning. Robotic simulations demonstrate the effectiveness of these methods in enabling simple policies to accomplish complex tasks, despite limitations at the switching point. The low level controller can handle objects with minor uncertainty, while the learning agent deals with it. The action space for tasks \u03c6 6 and \u03c6 7 is three-dimensional, with the first two dimensions representing a target position (x, y) and the third dimension controlling which object is placed at that position."
}