{
    "title": "SyxIWpVYvr",
    "content": "Likelihood-based generative models are effective for detecting out-of-distribution (OOD) inputs that could impact machine learning systems. However, the influence of input complexity on these models' likelihoods can hinder their performance in detecting certain types of inputs. This paper proposes an efficient OOD score based on input complexity, which outperforms existing detection methods across various datasets and model sizes. Assessing the novelty or difference of input data from training data is crucial for model reliability. Detecting out-of-distribution (OOD) inputs is crucial for machine learning systems, especially deep neural network classifiers. Various approaches have been proposed for OOD detection, including learning a density model to approximate the true distribution. This is essential for safe and reliable model operation, even in the absence of labeled data sets. Learning a density model M approximates the true distribution of training inputs. Good approximations result in low likelihood for out-of-distribution inputs. Recent generative models like PixelCNN++ and Glow can accurately approximate the density of complex data like audio and images. These models perform well in learning the density and can yield similar results across different datasets. Generative models trained on CIFAR10 show higher likelihoods for SVHN than for CIFAR10 itself. This behavior is not consistent across datasets, with some correctly producing lower likelihoods. Explanations for this phenomenon have been suggested, but a full understanding remains elusive. This paper sheds light on this behavior, showing that likelihoods from generative models are biased towards the complexity of the inputs. Complex images tend to have lower likelihoods, while simple images yield the highest likelihoods. In the second part of the paper, the authors propose using complexity estimates to detect out-of-distribution (OOD) inputs. They introduce an OOD score based on likelihood ratio tests, which can effectively detect OOD inputs using generative models. Experimental results show that this approach performs comparably or better than existing methods, using various datasets, generative models, and complexity estimates. The log-likelihood of an input x given a model M is evaluated in bits per dimension. Different data sets are compared based on their log-likelihood distributions, with Constant-color images having the highest log-likelihood followed by Omniglot, MNIST, and FashionMNIST. The log-likelihood of different data sets varies, with Constant-color images having the highest log-likelihood, followed by Omniglot, MNIST, and FashionMNIST. Conversely, Noise, TrafficSign, and TinyImageNet have lower log-likelihoods due to their more complex and colorful images. This trend suggests that log-likelihoods increase as images become simpler and contain less information. Further experiments confirm this observation by progressively simplifying input images. The experiment involved computing likelihoods of progressively simpler inputs by average-pooling noise images with different factors. The results showed a progressive increase in log-likelihood as image complexity decreased, indicating that image complexity plays a significant role in generative models' likelihoods. Further study on quantifying complexity is needed to understand this phenomenon better. To further study the observed phenomenon, we can use Kolmogorov complexity as a measure of input complexity. By calculating the normalized size of compressed inputs, we can estimate their complexity. Different compressors like PNG, JPEG2000, or FLIF can be used for images, while other lossless compressors are available for audio or text data types. Generative models' likelihoods show a negative correlation with complexity estimates, indicating that likelihood-based measures are influenced by input image complexity. Strong correlations suggest that likelihood values can be replaced with complexity estimates for similar results in detecting out-of-distribution inputs. In terms of detecting out-of-distribution (OOD) inputs, complexity estimates can perform similarly to likelihoods from generative models. By compensating for complexity when testing for OOD inputs, we can calculate an OOD score by subtracting the negative log-likelihoods from the complexity estimate. This score, denoted as S, can be interpreted as a likelihood-ratio test statistic, following Bayesian model comparison principles. A universal model, such as a compressor M0, can be used to adjust for all possible inputs without bias towards specific data types. In Bayesian model comparison, the goal is to compare the posterior probabilities of different models based on data X. Generative models aim to approximate the marginal likelihood for data x \u2208 X by integrating out all model parameters. Choosing between models is simplified to a likelihood ratio, especially with uniform priors. The ratio S in Bayesian model comparison reflects the Occam's razor principle by comparing the predictive performance of a learned model M to a universal lossless compressor M0. Inputs not present in M's training data are better predicted by M0, leading to high S values, while in-distribution samples are better predicted by M, resulting in lower S values. In OOD detection, strategies include using scores S(z) for ranking problematic instances, interpreting S(z) as Bayes factor for decision-making, setting a threshold based on the distribution of scores, and optimizing a threshold with ground truth OOD data. In OOD detection, strategies involve using scores S(z) for ranking problematic instances, interpreting S(z) as Bayes factor for decision-making, setting a threshold based on the distribution of scores, and optimizing a threshold with ground truth OOD data. A fourth strategy involves optimizing a threshold value for S(z) if ground truth OOD data Y is available, to target a desired percentage of false positives or negatives. The choice of strategy depends on the application's characteristics. This work keeps the evaluation generic and does not adopt a specific thresholding strategy, allowing for comparison with existing literature using the AUROC measure. Our method for OOD detection does not require additional training or specific background models for different types of data. Previous studies suggest that typicality affects the performance of likelihood-based generative models in detecting OOD inputs. The works of Nalisnick et al. (2019b) propose a Monte Carlo method for typicality testing, limited to batches of similar inputs. H\u00f8st-Madsen et al. (2019) combine typicality and minimum description length for novelty detection, focusing on bit sequences. Their approach relies on strong parametric assumptions, making generalization difficult. Various methods for OOD detection under a classification framework have been proposed by different researchers. The method of Hendrycks et al. (2019) extends to non-labeled data using generative models for OOD detection. They train a generative model on a dataset, compute scores, and evaluate using AUROC. The performance summary of S over log-likelihoods alone shows problematic results for various datasets, with AUROC values below 0.1. Likelihoods for SVHN, Constant, Omniglot, MNIST, and FashionMNIST datasets are consistently higher than CIFAR10. AUROCs for other datasets are above 0.5 but none exceed 0.67, except for Noise dataset. AUROC values for OOD detection with CIFAR10 test partition are close to 0.5. The AUROCs obtained with S show improved results for less complex datasets like MNIST or SVHN, with values above 0.7 and approaching 0.9 or 1. S clearly enhances OOD detection compared to likelihoods alone, even reversing low AUROC results for simpler datasets. The choice of training set, compressor/generative model, and model size also impact performance. In terms of model size and data sets, larger generative models and better compressors improve OOD detection performance. FashionMNIST is easier for OOD detection than CIFAR10, likely due to the generative model's ability to approximate data density. Better compressors also yield slightly improved AUROCs. Using larger models and better compressors leads to a more reliable system and higher AUROC values. The experiments show that better models and compressors improve system reliability and AUROC values. Further analysis is needed to confirm this hypothesis. Comparing S to previous approaches, it is competitive with both classifier and generative-based methods, achieving the best scores with FashionMNIST. When compared to existing approaches, S shows competitive performance with both classifier and generative-based methods, achieving high scores with FashionMNIST. It outperforms most approaches in AUROC values, with only two instances where it is surpassed by the ensemble-based approach WAIC. S is efficient due to its use of a single generative model and compression library, making it a parameter-free alternative that is easy to use. The text discusses the use of generative models' likelihoods for detecting out-of-distribution (OOD) data. Input complexity plays a significant role in these likelihoods, affecting the results of OOD detection. Different approaches and test results are presented, showing the effectiveness of S, a parameter-free measure, in comparison to existing methods. The text introduces a parameter-free measure, S, for detecting out-of-distribution data using generative models' likelihoods. The measure shows comparable or better results than existing methods across various datasets, model sizes, and compression algorithms. It requires no hyper-parameters besides defining a generative model and compression algorithm, making it easy to apply in practical scenarios. The text discusses the creation of synthetic image data sets, including Noise and Constant images, to aid in understanding the problem. It also mentions the selection of aligned versions of data sets with variations like CelebA and FaceScrub. Additionally, it highlights the importance of noting class overlaps between data sets like CIFAR10, TinyImageNet, and CIFAR100. The data sets used for training generative models are split into train, validation, and test sets following specific rules. Input sizes are standardized to 32x32 3-channel images. Two different generative models are utilized: an autoregressive model and an invertible model. Two generative models are used: PixelCNN++ as an autoregressive model and Glow as an invertible model. Glow is implemented with 3 blocks of 32 flows, while PixelCNN++ has 5 residual blocks per stage. Both models are trained using the Adam optimizer with an initial learning rate of 10^-4. The training of both Glow and PixelCNN++ models for image generation on CIFAR10 dataset involved using the Adam optimizer with an initial learning rate of 10^-4. The learning rate was reduced by a factor of 1/5 if validation loss did not decrease for 5 consecutive epochs, and by a factor of 1/100 to finish training. The batch size was set to 50, and the final model weights were chosen based on the best validation loss. The likelihoods obtained in validation matched those reported in the literature. PyTorch version 1.2.0 was used for training on a single NVIDIA GeForce GTX 1080Ti GPU, taking several hours. Three options for lossless compression of input images were explored. Three lossless compression formats are considered for input images: PNG, JPEG2000, and FLIF. PNG is a well-known format compressed using OpenCV 3, while JPEG2000 offers modern features like progressive decoding. FLIF claims to generate files 53% smaller than JPEG2000 and is the most modern algorithm. The complexity estimate L(x) is computed by compressing the input x using these algorithms. The complexity estimate L(x) is computed by compressing the input x with different compressors. An improved version of L, denoted as L i, forces S to work with the best compressor for each x. FLIF was found to be the best compressor in most cases. Additional results include reporting average log-likelihood M in Table 4 and global Pearson's in Table 5. In Table 6, AUROC values are reported based on log-likelihoods M, complexity estimates L, a two-tail test T, and the proposed score S. Table 7 shows AUROC values using the score S across different Glow model sizes with a PNG compressor. Table 8 displays AUROC values across various data sets, models, and compressors. The study compared AUROC values using the score S with different Glow model sizes and a PNG compressor, showing similar results for FashionMNIST and other compressors."
}