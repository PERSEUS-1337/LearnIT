{
    "title": "B1epooR5FX",
    "content": "Predicted Variables aim to integrate machine learning into programming languages, creating a new type of variable that determines its value using ML. They improve performance on algorithmic problems like binary search and QuickSort compared to traditional heuristics. Predicted Variables (PVars) offer the advantage of seamless integration of machine learning into existing systems and algorithms without the need to replace domain knowledge. Our PVars implementation relies on standard RL methods and uses a heuristic function as an initial function to learn faster. PVars quickly adopt the behavior of the initial function and improve performance without substantial setbacks, making them suitable for safe deployment in critical applications. Machine Learning has seen numerous successes in various areas over the past decade, but it adds complexity to software systems due to a fundamental impedance mismatch between traditional system building and ML approaches. Predicted Variables (PVars) hybridize ML with programming by determining values using ML when evaluated. Developers can use PVars like any other variable, combining them with heuristics, domain-specific knowledge, and problem constraints. Predicted Variables (PVars) allow developers to integrate machine learning tightly into algorithms, making it easier to use ML in software development by avoiding traditional steps like collecting training data, defining a training loss, and continuously updating the model. This inversion of control compared to traditional ML systems enables ML to improve the performance of classical algorithms in domains that have not traditionally used ML. In this paper, ML is shown to enhance \"classical\" algorithms using PVars based on deep RL. Different ML methods like supervised learning or active learning can also be applied. The framework described is applicable to various programming languages, with the central concept being a predicted variable or function. The initial function can serve as the current heuristic for a PVar. The hybrid approach introduced in this paper utilizes predicted variables to improve algorithmic solutions such as binary search, QuickSort, and caches. By replacing and enriching commonly used heuristics with predicted variables, better solutions are achieved while maintaining safety and minimizing regret. The main contribution lies in the general applicability of the framework, with a focus on self-contained setups and reproducibility. The paper introduces the PVar API for integrating machine learning into software development, showcasing its versatility in various domains. It emphasizes the effectiveness of ML models over run time and highlights the potential for PVars in user modeling and content recommendations. The main contributions include leveraging standard RL methods through the PVars interface. The paper introduces the PVar API for integrating machine learning into software development, showcasing its versatility in various domains. It emphasizes the effectiveness of ML models over run time and highlights the potential for PVars in user modeling and content recommendations. The main contributions include leveraging standard RL methods through the PVars interface. The paper also proposes an approach to learn using the initial function, demonstrating its feasibility on 3 standard algorithmic problems. It describes how PVars can be used in software development and how heuristics are replaced to guide training and avoid unstable behavior. The implementation and application of PVars to algorithmic applications are discussed, along with experiments showing their intuitive approach in bringing ML closer to software development. Related work is also described, emphasizing the simple API of PVars for developers to provide context, predict values, and provide feedback on predictions. The PVar API allows developers to have full control over data input, feedback, and model inference. Developers can create PVars with specified output type, shape, range, and initial function. PVars can be used like normal variables, determining their value through inference in the ML model. This approach replaces heuristics and provides a more stable training process. Developers can use a PVar instead of a heuristic or constant, which can be a stochastic variable that determines its value based on context observations. Feedback on predictions is provided to improve performance, aiming to maximize reward values over time. The API allows for integrating PVars easily into existing applications with little overhead. Model hyperparameters can be specified through additional configuration. The PVar's interface is determined by its definition, and an initial function can be passed to it. This function can serve as a heuristic that the PVar replaces. The initial function passed to the PVar serves as a heuristic for guiding its learning process, similar to imitation learning. It helps improve the PVar's performance by allowing it to explore solutions not easily reachable from a random starting point. This exploration generates initial trajectories for learning, even though it may be biased. The PVar relies on a policy selection strategy to balance between learning a good policy and the safety of the initial function. It can switch between exploiting the learned policy, exploring alternative values, and using the initial function at the action or episode level. The initial function acts as a safety net, allowing the PVar to fallback if the learned policy misbehaves. PVars can be used in different algorithmic problems, making it easy for developers to leverage machine learning with minimal code. Experimental results show that using PVars improves algorithm performance. The PVar relies on a policy selection strategy to balance between learning a good policy and the safety of the initial function. It can switch between exploiting the learned policy, exploring alternative values, and using the initial function at the action or episode level. Experimental results show that using PVars improves algorithm performance by measuring cumulative regret over training episodes to evaluate the impact. Regret measures how much worse a method performs compared to another, while cumulative regret captures whether a method is better than another over all previous decisions. The goal is to have regret never very high and cumulative regret become permanently negative as early as possible for better performance. The PVar allows developers to evaluate the model without a training and evaluation mode distinction. Overfitting is not a concern due to online learning. Regret numbers may show performance regressions from exploration noise. Computational costs of inference are not considered in the feasibility study. PVars are useful for problems with high computational costs or slow hardware. The implementation is a small library exposing the PVar interface to client applications. The implementation is a small library exposing the PVar interface to client applications. PVars assemble observations, actions, and feedback into episode logs for training models asynchronously. Recent progress in RL enables PVars to be applied to various use cases beyond RL methods. Models are built on DDQN for categorical outputs and TD3 for continuous outputs, with hyperparameters summarized in the appendix. The implementation is a small library exposing the PVar interface to client applications, assembling observations, actions, and feedback into episode logs for training models asynchronously. Hyperparameters used in experiments are summarized in the appendix, with a focus on tuning hyperparameters rather than defining new problem-specific heuristics. The policy selection strategy gradually increases the use of the learned policy based on received rewards, demonstrating effectiveness in FIG3. Reproducibility issues faced in RL technology are also acknowledged. The experiments show that reproducibility issues exist in the implementation, with some runs exhibiting desired behavior while others show baseline performance. Despite not having a solution, the community's developments may apply. The appendix provides performance results for re-running experiments. Binary search is used to find target values in sorted arrays efficiently. Binary search has a worst-case runtime complexity of log2(N) steps. Knowing more about the data distribution can reduce expected runtime. PVars can speed up binary search by estimating the position of x. The PVar observes values at both ends of the search interval and outputs a relative position for the next read index. To enhance the learning signal for the model, developers can incorporate problem-specific knowledge into the reward function or how the PVar is utilized. One approach is to reward reducing the search space size in binary search, which speeds up the search and can be implemented by shaping the reward as search range reduction. This allows for attributing feedback to current predictions and simplifying the problem from RL to contextual bandit. Alternatively, changing how predictions are used can help the PVar learn faster and avoid predicting very poor values. The text discusses using heuristics in algorithms like binary search to predict split positions, reducing the risk of selecting bad values. By combining different heuristics and using a value to mix between them, the approach aims to improve learning. However, there is a limitation in finding the optimal strategy outside of certain intervals. Testing is done in a controlled environment to evaluate the effectiveness of these approaches. In a test environment, different variants of binary search using a PVar are compared to a vanilla binary search baseline. Results show that directly predicting relative position with a simple reward initially performs poorly but improves over time. Using an initial function reduces initial regret, and using a shaped reward allows the PVar to learn behavior effectively. In a test environment, different variants of binary search using a PVar are compared to a vanilla binary search baseline. Results show that directly predicting relative position with a simple reward initially performs poorly but improves over time. Using an initial function reduces initial regret, and using a shaped reward allows the PVar to learn behavior effectively. QuickSort BID12 sorts an array in-place by partitioning it into two sets (smaller/larger than the pivot) recursively until the array is fully sorted. The optimal choice for a pivot is the median of the range, which splits it into two parts of equal size. To improve QuickSort using a PVar, the aim is to tune the pivot selection heuristic. To improve QuickSort, a PVar is used to tune the pivot selection heuristic by sampling elements from the array and selecting the median as the pivot. The feedback signal for recursion considers the impact on computational cost. The cost includes computing the median of samples and partitioning the array. This approach reduces the problem to a contextual bandit problem, similar to a shaped reward in binary search. Evaluation is done in a test environment. The PVar is used to improve QuickSort by sampling elements and selecting the median as the pivot. Experimental results show that the learned method outperforms baseline heuristics within 100 episodes. The PVar learns to select more samples at larger array sizes, similar to the adaptive baseline but without manual heuristic engineering. The PVar method outperforms baseline heuristics without manual engineering. It adapts to changing environments and prefers 13 over 15 samples at large array sizes due to limited training examples. Caches use a cache replacement policy (CRP) like LRU to improve performance using machine learning. The PVar method proposes two approaches to improve cache performance using machine learning. The first approach involves a PVar predicting which element to evict or choosing not to evict at all. The second approach enhances LRU by predicting an offset to the last access timestamp, determining which items to keep in the cache longer or evict sooner. The PVar method offers two approaches to optimize cache performance using machine learning. One approach involves predicting element eviction based on negative scores, while the other enhances LRU by predicting offsets to last access timestamps. The PVar receives feedback on cache hits or misses, with an additional reward for evictions in the discrete approach. Observations include access history, memory contents, and evicted elements. Keys can be observed as categorical inputs or historical frequency features, with an embedding layer shared between actor and critic networks for key input handling. The PVar method offers two approaches to optimize cache performance using machine learning. One approach involves predicting element eviction based on negative scores, while the other enhances LRU by predicting offsets to last access timestamps. The experiment involves three combinations of options for cache prediction: discrete caches observing keys, continuous caches observing keys, and continuous caches observing frequencies. Evaluation is done using a cache with size 10 and integer keys from 1 to 100, with synthetic access patterns sampled from a power law distribution. Results are compared with a standard LRU cache and an oracle cache for performance analysis. Hit ratio without exploration is examined to understand potential model performance, while cumulative regret is reported under exploration noise. The continuous variant of cache prediction quickly outperforms the LRU baseline, with negative cumulative regret after a few hundred episodes. Probabilistic programming BID5 introduces interfaces to simplify developer complexity when working with statistical models and conditioning variable values on run-time observations. In contrast to PVars, the introduced interfaces are specialized on working with distributions and graphical models. BID22 propose a programming interface for approximate computation, not explicitly targeting machine learning models. PVars aim to integrate with standard algorithms and systems, improving heuristics, optimizing database systems, or replacing constants. Another similar approach is Spiral BID2, but it is more specialized. The Spiral BID2 approach, while similar to PVars, is more limited in scope as it focuses on predicting boolean values only. Various papers apply machine learning to algorithmic problems, such as Neural Turing Machines BID7 for program execution modeling and BID1 for combinatorial optimization. These approaches replace existing methods with ML systems, demonstrating the inversion of control. Additionally, imitation learning BID13 involves replicating expert behavior, but most RL problems lack a good initial function. Imitation learning involves replicating expert behavior with limited training data. Using previously trained agents can kickstart learning of new models. Some applications leverage external data sources like YouTube videos for training speed. ML models have shown improvement in cache performance. Performance improvement in cache using ML models like BID27, BID20, BID8, BID21, and BID6. Algorithm selection and search algorithms have been enhanced using RL and genetic algorithms respectively. A new concept called predicted variables (PVars) has been introduced to simplify machine learning integration into existing code, giving developers full control over ML model usage and training. PVars bridge the gap between traditional software development and machine learning, allowing developers to focus on refining algorithms and metrics. Predicted variables (PVars) simplify machine learning integration by using variables in programming to determine values through machine learning. PVars observe context and feedback for predictions, improving performance in algorithmic problems. The feasibility of PVars is demonstrated in three problems, showing advantages and disadvantages of using reinforcement learning. The focus is on building a framework for easy ML use and performance enhancement. Predicted variables (PVars) aim to simplify machine learning integration by improving performance in various domains. The current implementation is based on standard RL methods, with potential for other ML methods like supervised learning. PVars open up new opportunities for ML use in unexplored areas, with plans to become a standard feature in programming languages like C++29, Python 4, and Java 12. The table shows when different variants of using a PVar in binary search reach break-even, indicating the number of episodes needed for cumulative regret to become permanently negative. Reward shaping and smart prediction usage improve performance, but even simple methods show improvements. No model outperforms interpolation search on a uniform distribution. The PVars framework allows reliance on the initial function or algorithmic formulation to limit costs of a poorly performing learned policy. The PVars framework in software control allows developers to limit costs of poorly performing policies by relying on safe initial functions or algorithmic formulations. Experiments show reproducibility with cumulative regret averaging -1.59 @5K (-2.20 @50K) and reaching break-even in 85% of cases after 1965 episodes on average. The experiment showed reproducibility with cumulative regret averaging -913 @1K (-1064 @10K) and reaching break-even in 94% of cases after 368 episodes on average. The experiment in section 4.4 compared the performance of a learned cache policy with predicted variables to the LRU heuristic using cumulative regret after 20000 episodes. Results showed that in 26% of runs, the cumulative regret was strictly negative, while in 60% of runs, the learned cache policy performed at least as well as the LRU heuristic. However, in 14% of runs, the learned policy performed worse. Hyperparameters for the experiments are provided in table 5 for reproducibility."
}