{
    "title": "ByOfBggRZ",
    "content": "Interpreting neural networks is a crucial task in machine learning. A novel framework for detecting statistical interactions in feedforward multilayer neural networks by interpreting learned weights is developed. The method achieves better interaction detection performance without searching an exponential solution space. Interactions between input features are created by nonlinear activation functions, and interacting paths are encoded in weight matrices. Experimental results on synthetic and real-world datasets demonstrate the method's performance and the importance of discovered interactions. Interpreting neural networks is crucial for understanding their predictive power. Complex machine learning models can learn unintended patterns, posing risks in critical decision-making domains like healthcare and finance. Approaches to interpreting neural networks include explaining feature importance, developing attention-based models, and providing visualizations. Neural networks are capable of modeling complex statistical interactions between features for automatic feature learning. Interpreting these interactions is crucial for scientific discoveries and hypothesis validation. Physicists and doctors may benefit from understanding joint factors and interactions in predictive models. In this paper, a framework called Neural Interaction Detection (NID) is proposed to detect statistical interactions in risk prediction models. The approach is efficient by approximating hidden unit importance and using a special form of generalized additive model to determine top-K true interactions. Theoretical justifications are provided on why interactions between features are created at hidden units. The paper introduces Neural Interaction Detection (NID) to detect statistical interactions in risk prediction models using a specialized form of generalized additive model. Experimental results show NID's effectiveness compared to other methods. The paper also reviews related work, defines notations, quantifies interactions in neural networks, presents the interaction detection framework, and demonstrates its utility on real-world datasets. The history of statistical interaction detection is discussed, dating back to the introduction of two-way ANOVA in the 1920s. Interaction detection methods include conducting individual tests for each combination of features or pre-specifying all interaction forms of interest and using lasso to select important ones. Approaches like ANOVA and Additive Groves fall into the former category, with two-way ANOVA being a standard method for pairwise interaction detection. Threeway ANOVA analyzes interactions between three variables, but higher-order ANOVAs are rarely done due to computational complexity. Additive Groves also conducts individual tests for interactions. Additive Groves is a method that detects interactions in a nonlinear manner, comparing regression trees with and without the interaction of interest. Lasso-based methods are popular for quick selection of interactions in additive models, but require specifying all interaction terms. Our approach to interaction detection is fast and capable of detecting interactions of variable order without limiting their functional forms, unlike lasso-based methods. It avoids high false positive rates and is interpretable, unlike traditional neural networks. Recent advances have been made in interpreting neural networks, both in their traditional feedforward form and as deep architectures. The benefits of neural network depth have been justified theoretically, with new research focusing on interpreting feature importance. Deep architectures have seen breakthroughs, particularly with the use of attention mechanisms in convolutional and recurrent architectures. Various methods like feature map visualization, de-convolution, and saliency maps have been crucial for understanding how convolutional networks represent images. Long short-term memory networks have also been studied for their multiplicative interactions. Recent research has focused on interpreting neural networks, particularly in understanding multiplicative interactions in long short-term memory networks (LSTMs). A new approach extracts non-additive interactions between variables from the weights of a neural network, showcasing generalized relationships across a sequence. This method differs from previous works in interpretability by emphasizing the importance of interactions determined by both incoming weights and outgoing paths within the network. In a feedforward neural network with L hidden layers, input features are treated as the 0-th layer. Each layer has a number of hidden units, weight matrices, bias vectors, and an activation function. Interactions are subsets of input features, and vectors can be restricted to specific dimensions. The feedforward neural network with L hidden layers has hidden units h() and output y with input x \u2208 Rp. A directed acyclic graph G = (V, E) is constructed based on non-zero weights, representing input features and hidden units. Statistical interactions involve joint influences of variables on an output, such as pairwise and higher-order interactions. In feedforward neural networks, statistical interactions between features are created at hidden units with nonlinear activation functions. A framework is proposed to identify and quantify interactions at a hidden unit for efficient detection, which are then combined across hidden units. Interacting features must follow strongly weighted connections to a common hidden unit before reaching the final output. In feedforward neural networks, interactions between features are created at hidden units with nonlinear activation functions. A framework is proposed to detect and quantify these interactions, which are then combined across hidden units. Interacting features follow strongly weighted connections to a common hidden unit before reaching the final output. The presence of a common descendant in the directed graph indicates interaction among input features. In feedforward neural networks, interactions between features are detected and quantified at hidden units with nonlinear activation functions. The framework focuses on characterizing the relative importance of interactions based on nonzero weights in the weight vector. The search complexity is limited to interactions at the first hidden layer for efficient interaction detection and high performance. The framework focuses on characterizing interaction strength in feedforward neural networks by averaging relevant feature weights to represent interactions. Different averaging functions from the generalized mean family are considered to define interaction strength, ensuring properties like zero evaluation when an interaction doesn't exist and non-decreasing strength with increasing feature weights. The framework characterizes interaction strength in feedforward neural networks by averaging feature weights. Interaction strength does not decrease with increasing feature weights, and is less sensitive to changes in large feature weights. The definition of interaction strength at individual hidden units is incomplete without considering their outgoing paths. The influence of hidden units in feedforward neural networks is quantified by considering their outgoing paths. Inspired by Garson's algorithm, the influence is computed through cumulative matrix multiplications of weight matrices. A definition of hidden unit influence is proposed, which upper bounds the gradient magnitude of the hidden unit with its activation function. This definition computes Lipschitz constants for the corresponding units. The text discusses the computation of Lipschitz constants for neural network units, particularly focusing on the importance of input gradients in determining variable importance. The interaction strength of potential interactions in hidden units is also explored, with a scheme for aggregating strengths across units to compare interactions of different orders. Our feature interaction detection algorithm NID can extract interactions of all orders without individually testing for each of them. The methodology involves training a feedforward network with regularization, interpreting learned weights to rank interaction candidates, and determining a cutoff for the top-K interactions. Data contains statistical interactions and main effects, with two architectures studied: MLP and MLP-M. MLP is a standard multilayer perceptron, while MLP-M includes additional univariate networks to discourage spurious interactions. The feature interaction detection algorithm NID can extract interactions of all orders by training a feedforward network with regularization. The methodology involves ranking interaction candidates based on learned weights and determining a cutoff for the top-K interactions. The approach includes sparsity regularization on the MLP portions of the architectures to suppress unimportant interacting paths and push the modeling of main effects towards univariate networks. The greedy algorithm designed ranks interaction candidates by considering top-ranked interactions of every order, significantly reducing the search space while still considering all orders. The algorithm traverses the input weight matrix across hidden units, selecting top-ranked interaction candidates based on their strengths. By summing the strengths of top-ranked interactions, variable-order candidates are ranked relative to each other. The averaging function is set to min(\u00b7) for improved performance in experimental evaluation. The greedy algorithm in Algorithm 1 improves the ranking of higher-order interactions by considering top-ranked interactions of every order. The algorithm selects interaction candidates based on their strengths and uses an averaging function set to min(\u00b7) for better performance. Theorem 4 justifies this improvement by showing that higher-order interactions have a better chance of ranking above false positives. Theorem 4 justifies the improvement in the ranking of higher-order interactions by showing that a d-way interaction will outperform its d \u2212 1 subsets in rankings as long as there is no sudden drop in weight. This improvement extends to cases where d = |S \u2229 R| > 1. Algorithm 1 assumes a sufficient number of first-layer hidden units to predict the true top-K interactions using a Generalized Additive Model (GAM) with interactions. The text discusses a model called MLP-Cutoff where main effects and interactions are captured by small feedforward networks. The model gradually adds top-ranked interactions to a Generalized Additive Model (GAM) until performance plateaus on a validation set. Pairwise interaction detection is a standard problem in the literature, and all pairs of interactions are ranked. In this section, experiments on simulated and real-world datasets are discussed to study the performance of the approach on interaction detection using algorithms like factorization machines and hierarchical lasso. The proposed NID framework relies on selecting an averaging function from the generalized mean family, such as maximum, root mean square, arithmetic mean, geometric mean, harmonic mean, and minimum, to determine interaction strengths between features. In experiments, a test suite of 10 synthetic functions was used to train MLP and MLP-M models. The proposed greedy ranking algorithm was used to obtain interaction rankings, focusing on correct interactions ranked before any false positives. The highest number of true top interactions was recovered using the minimum averaging function, which was chosen for all experiments. Neural Network Configuration involved training feedforward networks of MLP and MLP-M architectures with specific hidden layer sizes and ReLU activation. The objective functions were meansquared error for regression and cross-entropy for classification tasks. L1 constants were tuned in the range of 5e-6 to 5e-4 for MLP and MLP-M on synthetic test suite and real-world datasets. The interaction detection framework was tested on simulated and real-world datasets using various parameters such as L1 and L2 constants. Synthetic test functions with different interactions were used for simulated experiments. Real-world datasets included regression datasets. Random train/valid/test splits were used for synthetic experiments. The study tested an interaction detection framework on real-world datasets including regression datasets with different prediction tasks such as California housing prices, bike sharing rental counts, Higgs Boson classification, and letter recognition. The study compared the performance of NID, a framework for pairwise interaction detection, to three baseline methods: Two-Way ANOVA, Hierarchical lasso, and RuleFit. Additive Groves was also used as a reference method for interaction detection. In Section 4, the framework NID is utilized for pairwise interaction detection using datasets generated by synthetic functions F1-F10. AUC scores of interaction strength from baseline methods and NID for MLP and MLP-M are compared, with MLP-M showing comparable or better performance except for F6 due to modeling spurious main effects. In Section 4, NID is used for pairwise interaction detection with synthetic functions F1-F10. MLP-M performed worse on F6 due to modeling spurious main effects in the {8, 9, 10} interaction. NID generally outperformed AG in AUC scores, especially on F5, F6, and F8. NID-MLP-M showed comparable or better performance than ANOVA, HierLasso, and RuleFit. In Figure 4, heat maps of synthetic functions display pairwise interaction strengths interpreted from MLP-M. F6 and F7 show erroneous interaction strengths, with NID proposing weak interactions for {8, 9, 10} in F6. Real-world datasets also show interaction strengths, such as a high-strength interaction between longitude and latitude in the cal housing dataset. The outcome variable, California housing price, strongly depends on geographical location. High-strength interactions are observed in heat maps of various datasets. For example, the letter dataset requires distinguishing letters A-M from N-Z using pixel display features. A neural network learns highly interacting functions to make this distinction. A greedy interaction ranking algorithm is used for higher-order interaction detection without an exponential search. Visualizations on synthetic and real-world datasets show the predictive capability of detected interactions. The text discusses the predictive capability of detected interactions in closing the performance gap between MLP-Cutoff and MLP-M. Experiments comparing NID and AG with added noise are also mentioned, along with significant improvements in runtime. Visualizations of pairwise interaction strengths proposed by the NID framework on MLP-M for datasets generated by functions F1-F10 are shown in heat maps. Higher-order interaction detection on synthetic and real-world datasets is visualized in Figures 6 and 7. The text discusses higher-order interaction detection on synthetic and real-world datasets. The interaction rankings generated by NID for MLP-M are shown on the x-axes, with blue bars indicating validation performance of MLP-Cutoff as interactions are added. A cutoff heuristic is used to stop adding interactions once MLP-Cutoff's validation performance matches or exceeds MLP-M's. True interactions are found before the cutoff point, with challenges mainly in F6 and F7. The text discusses detecting interactions mainly associated with F6 and F7, showing improvements in performance on various datasets. MLP-Cutoff reaches a cutoff point with a small number of superset interactions due to redundancy. Interaction detection is evaluated based on predictive performance on real-world data. The study analyzed interactions' predictive performance by comparing MLP-Cutoff with and without interactions. Results show relative performance improvements on real-world and synthetic datasets. Interaction rankings were generated by the NID framework applied to MLP-M, with redundant subset interactions identified. The NID framework on MLP-M shows that a small number of high-order interactions are predictive. A metric called toprank recall is used to assess ranking quality by considering correctly ranked interactions before any false positives. Subset interactions are ignored, focusing only on superset interactions as true interactions. The NID framework focuses on high-order interactions in MLP-M, using top-rank recall to evaluate ranking quality. Results show NID achieves similar recall as AG but with significantly faster runtime. Higher-order interaction detection may struggle with interlinked variables in functions. The NID framework focuses on detecting interactions in neural networks, achieving high recall with faster runtime compared to AG. However, it may struggle with distinguishing interlinked pairwise interactions from higher-order interactions. Spurious interactions and missed detections can occur due to feature correlations. The NID framework detects interactions in neural networks by interpreting learned weights of a feedforward neural network. It aims to accurately detect various types of interactions without searching through an exponential solution space. Future work includes detecting feature interactions in intermediate hidden layers and interpreting weights in other deep neural architectures. Proposition 2 states that in a feedforward neural network, interactions at common hidden units can be represented by a directed acyclic graph based on non-zero weights. The value of any hidden unit is a function of parent hidden units, and there exists a vertex in the graph where the ancestors are a subset of the input features. In a feedforward neural network, interactions at common hidden units are represented by a directed acyclic graph based on non-zero weights. The value of any hidden unit is a function of its ancestors at the input layer. The final output of the network is a weighted summation over the hidden units at the L-th layer. A common descendant will create an interaction among input features in most cases. The existence of counterexamples in neural networks is seen when early hidden layers capture interactions that are negated in later layers. For example, two interactions may cancel each other out in subsequent layers. While legitimate, such counterexamples are unlikely due to random fluctuations. A finer analysis on interaction strength can be done on a bivariate ReLU function, quantifying the interaction between variables with cross-term coefficients. The text discusses the coefficient of interaction in neural networks, specifically focusing on the strength of interactions between variables in different layers. It mentions the use of a bivariate ReLU function to quantify these interactions and highlights the importance of modeling the interaction strength accurately. The text also touches on the differentiability of functions and the partial derivative of the final output with respect to hidden units. The text discusses improving the ranking of higher-order interactions in neural networks. It introduces a theorem and proof regarding the ranking of interactions proposed by Algorithm 1. The approach is evaluated in a large p setting. In a large p setting, the approach is evaluated with pairwise interactions using a synthetic function. The dataset consists of n samples and p features, with weights for pairwise interactions and main effects. The experiment sets p = 1000, n = 1e4, and K = 5, with normally distributed values for X and noise. The training is done with MLP-M using the same hyperparameters as before. In a large p setting, the approach is evaluated with pairwise interactions using a synthetic function. The dataset consists of n samples and p features, with weights for pairwise interactions and main effects. The experiment sets p = 1000, n = 1e4, and K = 5, with normally distributed values for X and noise. The training is done with MLP-M using the same hyperparameters as before. We train MLP-M with a larger main network architecture of five hidden layers, extract interactions using the NID framework, and compare interaction detection performance with different regularizations on MLP-M networks. In this work, group lasso regularization is studied for neural networks, focusing on input and hidden unit groups. The authors define group lasso and sparse group lasso for variable selection. Networks with these regularizations applied to the input weight matrix also had L1 regularization on other weights. Large dataset sizes were used in experiments to tune the regularizers. In experiments, large dataset sizes of 1e5 were used to tune regularizers by gradually increasing their strengths. The neural network hyperparameters remained the same as discussed earlier. Average pairwise interaction strength AUC over 10 trials of each function in a synthetic test suite was reported in Table 4 for different regularizers. The evaluation was conducted on synthetic datasets with binary class labels. In evaluation, the method is compared against logistic regression methods for interaction detection on synthetic datasets with binary class labels. Modifications to hyperparameters are made based on validation performance, with the main network having specific hidden unit sizes and L1 regularization set to 5e\u22124. FHIM detects predictive pairwise interactions in logistic regression models for binary class labels. FHIM detects pairwise interactions predictive of binary class labels using data from Equation 5 in BID30. Shooter, developed by Min et al., identifies multiplicative interactions in logistic regression models by relaxing hierarchical hereditary assumptions. Shooter evaluates interactions per level of order, comparing NID and Shooter under relaxed assumptions. Interaction rankings are determined by thresholding strengths. Interactions up to degree 5 are considered due to degradation in MLP-M's performance beyond that. Sparsity factor set at 5%. In the experiment, Shooter and NID were compared under relaxed assumptions, with Shooter evaluating interactions up to degree 5. NID showed fair precision but low recall, with interactions identified by NID being subsets or supersets of each other. Shooter's limitation is assuming multiplicative interactions. In the synthetic function F6 TAB3, the {8, 9, 10} interaction can be approximated as main effects for each variable x8, x9, and x10. The MLP-M was trained on data generated from synthetic function F6 (TAB3) and visualized the interaction between longitude and latitude. The MLP-M models the {8,9,10} interaction as spurious main effects with parabolas scaled by a constant. The visualization of the interaction between longitude and latitude for predicting housing prices in California, extracted from the MLP-Cutoff 5 model trained on the cal housing dataset, shows a special joint information requirement for generating the visualization. This highly interacting nature between longitude and latitude confirms the importance of this interaction in the NID experiments."
}