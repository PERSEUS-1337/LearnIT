{
    "title": "H1BHbmWCZ",
    "content": "In this paper, the authors present three directions of visual development using supervised and semi-supervised techniques. The first direction involves implementing semi-supervised object detection and recognition using Soft Attention and Generative Adversarial Networks (GANs). The second and third directions focus on supervised networks learning spatial locality and quantity concepts using Convolutional Neural Networks (CNNs). These approaches are based on Experiential Robot Learning and aim to pave the way for autonomous development of robotic visual modules. The study raises questions about whether a robot can learn from its environment, appreciate the concept of plurality of objects, and have attention mechanisms to focus on specific objects in its visual field. The authors explore the development of autonomous child robots that learn in-the-wild, focusing on object location understanding and communication. They present weakly-supervised and semi-supervised models to equip these robots with learning capabilities, avoiding complex algorithms and hand-engineered solutions. The advent of Deep Learning has revolutionized Perception systems, with Deep Neural Networks being used for various applications. Techniques often rely on supervised methods like Stochastic Gradient Descent algorithm. Developing systems that leverage annotated data subsets and unannotated real-time data is crucial. Experiential Robot Learning (ERL) in BID1 aims to enable autonomous agents to improve their skills and effectiveness by learning in real-world environments. This method emphasizes self-improvement and scalability, allowing robots to learn autonomously without the need for constant human intervention. In previous work BID1, a cognitive bootstrap was used to implement Experiential Robot Learning (ERL) by repurposing fine-tuning for CNNs. This allowed the robot to expand its visually-recognizable object vocabulary with human supervision. While supervision is useful, it can limit self-improvement and scalability. Accessible supervision, like voice recognition for labeling, was demonstrated in the previous work. The paper discusses the implementation of robot visual modules with ERL, focusing on voice recognition for labeling objects. The experiments' results and implementation details are presented, with computations done on a desktop computer using TensorFlow and Keras API. Future optimization aims to deploy models and computations locally on the robotic agent. The module aims to provide a self-guiding mechanism for object detection using Soft Attention principles and Generative Adversarial Networks architecture. It reframes the problem into a regression task, utilizing a CNN trained on ImageNet as the Discriminator. Initial attempts at Hard Attention regression on Bounding Box coordinates were unsuccessful due to differentiability requirements. Physical robotic implementation is not within the scope of the paper. The model is separated into three stages for object detection. The first stage involves standard feature extraction, where activation maps from a pre-trained CNN on ImageNet mask out some features in the image. The second stage reconstructs and upscales features to form masks using Deconvolution layers. The input image is passed through these stages to detect objects. The third stage of the object detection model involves using a standard CNN with softmax output to generate a 'score' based on the confidence in the soft attention mechanism. The model utilizes activation maps produced by a pre-trained VGG16 network to mask out features in the image, ensuring that important objects are not entirely masked out. In object detection models, the balance between spatial correlation and abstraction is crucial. As we move down the CNN layers, features become more abstract and less correlated with input dimensions. It is important to use a large input to recover information lost during pooling operations. This stage defines the model's performance for the task at hand. In object detection models, the balance between spatial correlation and abstraction is crucial. As we move down the CNN layers, features become more abstract and less correlated with input dimensions. It is important to use a large input to recover information lost during pooling operations. This stage defines the model's performance for the task at hand. In the next stage, activation maps from Stage One are used to rescale and reconstruct feature maps through Deconvolution. This process helps in applying a soft attention mechanism by using adjusted maps as masks to contrast objects from their background. The third stage involves a standard CNN pre-trained on ImageNet, acting as a discriminator in the model. This approach differs from the previous Hard Attention path, which involved regressing proposed Bounding Boxes by cropping images and scoring them through another CNN. Despite the unsuccessful results, the Hard Attention path is mentioned for inspiration within the community. The third stage involves a CNN pre-trained on ImageNet, acting as a static and non-trainable discriminator in the model. It outputs a 1000-wide matrix representing ImageNet classes and evaluates the soft attention mechanism by regressing on the confidence score with a static target of '1'. The model uses a CNN pre-trained on ImageNet to regress on a confidence score with a static target of '1'. It aims to judge an object's location qualitatively in its field of view by repurposing CNN classifiers with 9 custom labels. The Location CNN was designed to encode the spatial location of object activations in specific areas of the image. It uses specialized large filters as 'pass gates' to understand and tackle the concept of multiplicity in object instances. The network architecture and parameters are provided, requiring minimal tuning as shown in the Results and Discussion section. The ability to visually recognize multiplicity or plurality is crucial for a child robot to learn counting. An autoencoder paradigm was adopted to understand how a CNN handles plurality, starting with a modified VGG16 architecture for classification. Despite low validation accuracy, the goal was to see if CNNs can grasp the concept of plurality inherently. The study focused on understanding how a CNN handles the concept of plurality using an autoencoder architecture. A dataset of over 800 images featuring different numbers of objects was compiled. The goal was to determine if the CNN was struggling with mapping instances to correct labels or if information on object plurality was being lost. In Developmental Robotics, BID13 served as inspiration for research efforts, focusing on grounding numbers in child robots. Object Detection in Computer Vision, like BID11 and BID8, relies on annotated data for training, hindering future improvements without additional annotations. In the field of unsupervised object detection, various approaches have been explored. For instance, BID14 and BID16 have utilized different techniques such as regression and co-localization to detect objects without the need for annotated data. Additionally, in the realm of counting using CNNs, Transfer Learning has been employed to address challenges in this area. BID12 uses Transfer Learning for counting and object localization, tested on MNIST and a synthetic Pedestrian dataset. BID3 employs an Inception-like architecture to detect object counts. BID7 utilizes MESA metric to estimate distance between predicted and target counts. Results are shown on in-house datasets due to limited resources for fast prototyping, not aiming to advance state-of-the-art in Computer Vision. In experiments using a custom dataset of less than 2000 images, the performance of the system was not affected by the number of images. The number of epochs needed to reduce loss varied based on different configurations of hyper-parameters in Deconvolution layers, such as kernel size and number of filters. Adding Dropout or BatchNormalization layers and following Deconv layers with Convolution layers can further refine constructed masks. In experiments with a custom dataset of less than 2000 images, system performance was not affected by image quantity. The number of epochs to reduce loss varied based on Deconvolution layer configurations. Adding Dropout or BatchNormalization layers can refine masks further. The system used a batch size of 60, Categorical Crossentropy loss, and Adadelta optimizer, converging in less than 50 epochs to 0.96 and 0.9 accuracy on training and validation sets. The dataset had over 1500 images across 9 classes, with images of singular objects against plain backgrounds. Training performance of the network is promising, with validation accuracy around 0.9. The network was trained with Binary Crossentropy loss and Adadelta optimizer over 80 epochs. Despite slow learning, the network eventually saturates at a certain loss. The results show the network understands multiplicity, producing multiple shapes for corresponding objects. The ResNet50 CNN successfully encodes information about the number of objects in an image, as shown by producing spoon-like shapes for 5 spoons. This ability is crucial for repurposing CNNs as counting networks. The network also reconstructs single objects accurately, indicating its capability to maintain object information. Further engineering could reduce loss, but the results confirm CNNs encode object quantity information. The experiments showed insights on object detection reformulated as a regression problem using Soft Attention. The network successfully isolated objects but needs further investigation for dynamics. Object Location network achieved high accuracy in classification, indicating CNNs can provide object location labels. Limitations include isolating only one object and the need for image reiteration. The experiments demonstrated the potential of CNNs in providing object location labels, with a focus on tackling cluttered backgrounds. Additionally, the study highlighted the ability of CNNs like ResNet50 to perceive and encode object plurality. The research aims to address challenges in weakly and semi-supervised computer vision, emphasizing the importance of reframing object counting as a classification problem. The focus is not on achieving state-of-the-art accuracy but on exploring solutions for an open-ended child robot's needs. The potential for self-improvement in child robots under weakly or unsupervised conditions is explored by reframing object location detection as a classification problem. This allows robots to learn from human feedback and consolidate knowledge over time to improve their abilities."
}