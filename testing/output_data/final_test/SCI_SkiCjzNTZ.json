{
    "title": "SkiCjzNTZ",
    "content": "We propose a framework using field theory to explain the performance of deep neural networks. Correlations between weights in the same layer are described by symmetries, and breaking these symmetries helps networks generalize better. Through a two parameter field theory, networks can break symmetries on their own during training, known as spontaneous symmetry breaking. This process allows networks to generalize without user input. In the context of residual networks, we show that remaining symmetries are broken based on empirical results, with similarities to quantum field theory. Deep neural networks, such as AlexNet, VGG, Inception, and ResNet, have been successful in image recognition tasks. Understanding their inner workings remains challenging, with discoveries like the training process stopping at an information bottleneck. A framework using field theory explains network performance, with correlations between weights in layers resembling quantum field theory. This framework can explain phenomena like training on random labels with zero error and shattered gradients. Recent work has shown that deep neural networks can regularize themselves and train on randomly labeled data with zero error. Gradients in deep networks behave as white noise, and a Taylor series expansion can explain ensemble behavior in ResNets. This expansion generates a symmetry-breaking layer that improves generalization. ResNets achieve symmetry breaking through communication between layers. Recent work has shown that ResNets can break symmetry through communication between layers. A quantum field theory can describe how errors propagate through each layer in the network, leading to a phase transition towards the end of training. This theory explains the remarkable performance of deep networks compared to classical models. The theory describes how deep networks break symmetry layer by layer in the decoupling limit. It focuses on spontaneous symmetry breaking, where weights with zero Hessian eigenvalues exist at the loss minimum. This can explain various experimental results, and the layer decoupling limit in ResNets approximates loss functions using symmetry invariant quantities. The paper discusses how deep neural networks achieve unprecedented power through spontaneous symmetry breaking of affine symmetries. It provides background on deep neural networks and field theory, shows that remnant symmetries can exist in neural networks, and presents experimental results confirming the theory. The framework is introduced using a field theory based on Lagrangian mechanics, with a focus on the layers of neurons in a deep neural network. The paper explores how deep neural networks achieve power through symmetry breaking. It discusses preserving symmetry in neural networks with nonlinear operators and transformation matrices. If a symmetry group exists, the output is covariant. The paper discusses how deep neural networks achieve power through symmetry breaking and preserving symmetry with nonlinear operators and transformation matrices. The loss function in statistical learning is minimized using mutual information, training error, or Kullback-Leibler divergence in the continuum limit of samples and layers. The loss functional is defined to transition into Lagrangian mechanics and field theory in the continuum limit of samples and layers. The loss rate per layer is bounded from below, and minimizing it guarantees the minimization of the total loss, requiring invariance under symmetry transformations. The loss functional for a deep neural network is defined to be invariant under symmetry transformations. If Q1(t) and Q2(t) do not belong in the same symmetry group, the equality may not hold. The Lagrangian dynamics describe the feature map flow at each layer, with the minimizer of the loss rate determining the trajectory for the representation flow. The Lagrangian density is defined as L = T - V, where T is the kinetic energy and V is the potential energy. The potential energy V is an invariant under symmetry transformations. A series expansion of V around the minimum is set up, with each term being an invariant. The Taylor series expansion about the minimum w i = 0 of the potential is performed, where the Hessian matrix H i j is involved. The O(D) symmetry enforces weight Hessian eigenvalues to be H i i = m 2 /2 for some constant m 2. This symmetry can be generalized to the quartic term where V is even around the minimum, giving ii ii = \u03bb/4 for some constant \u03bb. The power series expansion is a good approximation in the decoupling limit, relevant for Residual Networks. The kinetic term T is expanded in power series of derivatives, with coefficients fixed by the Hamiltonian kinetic energy. The Lagrangian density for a scalar field in field theory is described, with higher order terms in derivatives being negligible. The effect of the learning rate is accounted for using results from thermal field theory, linking the temperature to the learning rate. Spontaneous symmetry breaking in a deep neural network is discussed, with a scalar field potential invariant under O(D) transformations. A phase transition occurs at a critical learning rate value where the loss minimum changes. The loss minimum in a deep neural network occurs at a critical learning rate value \u03b7 = \u03b7 c, leading to spontaneous symmetry breaking and phase transition. This phenomenon generates long-range correlations between representations and desired outputs, with implications from field theory discussed in Appendix C. The loss rate bifurcates at \u03b7 = \u03b7 c, reducing the orthogonal group O(D) to a reflection symmetry O(1) = {1, -1}. When the learning rate decreases below a critical point, new minima are generated causing instability at point A. The weight transitions through point B to reach the new minimum at point C. A cyclical learning rate can outperform a monotonic decreasing rate due to the invariant loss rate from spontaneous symmetry breaking. Spontaneous symmetry breaking splits weights into two sets, with directions \u03c0 and \u03c3 in weight space. This has been experimentally shown in FIG0 by BID2. The text discusses the experimental demonstration of symmetry breaking in neural networks, where weights split into two sets with directions \u03c0 and \u03c3 in weight space. The kinetic term T is not invariant under transformation Q(t), requiring the use of covariant derivatives for invariance. The focus is on spontaneous symmetry breaking in neural networks, showing that learning by deep neural networks involves breaking symmetries in the weights. Deep neural networks break symmetries in weights, with non-linear layers preserving symmetries across layers. Weight pairs in adjacent layers, not within the same layer, are approximately invariant under leftover symmetries. Experimental results show spontaneous symmetry breaking in deep neural networks. Theorem 1 states that deep feedforward networks learn by breaking symmetries through non-linearities. ReLU reduces the symmetry of an Aff(D) invariant to a subgroup Aff(D), where D < D. ReLU preserves some continuous symmetries. If a group G commutes with a nonlinear operator R, then R preserves the symmetry G. Remnant Symmetry occurs when a group G commutes with a non-linear operator at a layer. Invariant loss functions require predicted output to be covariant with input. Neural network weights in adjacent layers form an approximate invariant under remnant symmetry. The pair of weights in adjacent layers can be considered an invariant for the loss rate. In neural networks, weights in adjacent layers form an approximate invariant under remnant symmetry. This symmetry is continuous and can generate strong correlations between inputs and outputs. The Goldstone Theorem states that for every spontaneously broken continuous symmetry, there exists a weight with a zero eigenvalue. The remnant symmetry is considered an orthogonal group O(D) with W being a D \u00d7 D matrix. In deep neural networks, the weights exhibit an approximate invariant under remnant symmetry, specifically an orthogonal group O(D). By choosing a subset \u0393 of the weight matrix W that is invariant under Aff(D), we can write down the Lagrangian for a deep feedforward network. The Goldstone theorem predicts that weight deviations split into two sets (\u03c3, \u03c0) with different behaviors due to spontaneous symmetry breaking. This analysis applies to piecewise-linear activation functions like ReLU. The weights in deep neural networks exhibit approximate invariance under remnant symmetry, specifically an orthogonal group O(D). Weight deviations split into two sets (\u03c3, \u03c0) due to spontaneous symmetry breaking, with weights \u03c0 having zero eigenvalues and a spectrum dominated by small frequencies. The neural network undergoes a phase transition out of the information bottleneck via spontaneous symmetry breaking, leading to weights \u03c0 with zero Hessian eigenvalues. The loss minimization in deep neural networks leads to highly correlated feature maps across layers, indicating independence from input. Gradient variance explosion occurs at the end of training, connected to spontaneous symmetry breaking. Neural networks show resilience to overfitting. Neural networks exhibit resilience to overfitting due to their ability to resist sampling noise in weights. Zero eigenvalue weights after symmetry breaking provide robustness to the model, known as implicit regularization. This study reveals deep neural networks undergo spontaneous symmetry breaking, shedding light on unexplained phenomena in deep learning. Theoretical model explains experimental results with two parameters due to decoupling limit. Phase transition near end of training improves network performance drastically. Model can qualitatively describe phase transition behaviors in networks where decoupling limit may not apply, suggesting nearby layer interactions are responsible. The proposed field theory and symmetry breaking perspective explain experimental findings in network training. Decreasing learning rate leads to phase transition and new minima development. Cyclical learning rate accelerates reaching new minimum. Stochasticity in gradient descent improves generalization by moving weights towards local minimum. Stochasticity in gradient descent helps generalize by moving weights towards local minimum. PReLU's BID4 breaks symmetry, leading to better generalization. Results from Shwartz-Ziv & Tishby (2017) attribute spontaneous symmetry breaking. Deep neural networks can train on random labels with low training loss. Identity mapping outperforms other skip connections due to small output from residual units. The small output from residual units leads to easier spontaneous symmetry breaking. Identity skip connections break additional symmetry, causing covariance issues under symmetry transformations. The shattered gradient problem is reduced in ResNet due to the decoupling limit. Non-residual networks have larger weight eigenvalues, resulting in higher oscillation frequency in the correlation function. In recurrent neural networks, multiplicative gating BID16 combines input x and hidden state h through element-wise product, outperforming addition method by breaking output covariance. Field theory results are stated without proof, using Lagrangian mechanics for fields w(x, t) and Euler-Lagrange equation for equations of motion. Lagrangian consists of kinetic term T and potential term V, with action S defined as the integral of Lagrangian density. The potential for a scalar field allowing spontaneous symmetry breaking has a specific form. In the decoupling limit, the equation of motion for the field is the Klein-Gordon Equation. The field can be treated as a random variable with a probability distribution that peaks at the solution of the Klein-Gordon equation. The correlation function between different points in the field can be defined using integrals over all paths. In the decoupling limit, the correlation function of the fields across two points in space and time is related to the Green's function of the Klein-Gordon equation. Small eigenvalues are generated by spontaneous symmetry breaking, leading to specific weights in the Lagrangian. In the decoupling limit, the Lagrangian can be written as L = T \u2212 V, where the fields \u03c0 and \u03c3 decouple from each other. The \u03c3 fields satisfy the Klein-Gordon Equation, while the \u03c0 fields satisfy the wave equation. The correlation functions of the weights across sample space and layers can be expressed in terms of the fields. The correlation functions of the weights across sample space and layers, P \u03c3 = \u03c3(z , t )\u03c3(z, t) and P \u03c0 = \u03c0(z , t )\u03c0(z, t) are the Green's functions of the respective equations of motion. Fourier transforming the correlation functions give P \u03c3,\u03c0 (t, k) = i 2\u03c9 0 exp \u2212 i\u03c9 0 t, where \u03c9 0 = |k| 2 + |m 2 \u03c3,\u03c0 |. The Goldstone Theorem states that for every continuous symmetry that is spontaneously broken, a weight \u03c0 with zero Hessian eigenvalue is generated at zero temperature (learning rate \u03b7). Quantum mechanics studies errors and approximates deep neural networks with non-linear operators. A scalar quantum field theory can describe a neural network layer by layer. A dynamical model is needed to describe the weights layer by layer."
}