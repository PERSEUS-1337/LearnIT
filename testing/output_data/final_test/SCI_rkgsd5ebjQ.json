{
    "title": "rkgsd5ebjQ",
    "content": "In this paper, a novel semantic-aware neural abstractive summarization model is proposed to address the limitations of existing neural systems. The model focuses on generating high-quality summaries by interpreting semantic content. It outperforms the popular pointer-generator summarizer in identifying off-topic information and produces more informative, faithful, and less redundant summaries according to human evaluation. Automatic text summarization shows promise in tackling information overload. The existing summarization systems are mainly extractive, leading to redundancy and incoherence. Abstractive summarization methods are needed to address this issue. Neural frameworks have shown promise in abstractive summarization, but they struggle with acquiring semantic interpretation of the input text, which is crucial for summarization. Neural summarization systems struggle with semantic understanding, leading to unfaithful generations. Current models can be easily misled by off-topic information, resulting in fabricated or nonsensical summaries. A novel adversarial evaluation metric is designed to measure the robustness of each summarizer against irrelevant information. The 32nd Conference on Neural Information Processing Systems (NIPS 2018) in Montr\u00e9al, Canada featured discussions on summarization systems' struggle with semantic understanding. Canada's Minister of Defense resigned following a scandal involving the beating death of a Somali teen by Canadian peacekeeping troops in 1993. Defense Minister David Collenette denied his resignation was related to the Somalia scandal. The 32nd Conference on Neural Information Processing Systems (NIPS 2018) in Montr\u00e9al, Canada discussed the challenges faced by summarization systems in understanding semantics. Following a scandal involving the beating death of a Somali teen by Canadian peacekeeping troops in 1993, Canada's Defense Minister David Collenette resigned. The proposed solution is a novel semantic-aware abstractive summarization model. The proposed solution discussed at the 32nd Conference on Neural Information Processing Systems (NIPS 2018) in Montr\u00e9al, Canada is a novel semantic-aware abstractive summarization model. The model generates summary-worthy semantic structures from input articles and constructs a fluent summary using an encoder-decoder architecture with dual attention mechanism for summary decoding. A segment-based reranking strategy is used to produce diverse hypotheses and reduce redundancy. Performance evaluation against adversarial samples shows improvements over existing models. Our model outperforms the seq2seq attentional model and pointer-generator model in generating on-topic summaries with higher ROUGE and METEOR scores. It uses fewer extractive fragments, reduces redundancy, and produces more informative and faithful summaries according to human evaluation. Additionally, a pointer-generator summarizer is proposed to discourage the generation of fabricated content in neural abstractive models. The proposed model improves neural summarization by generating salient semantic structures through multi-task learning and a reranking-based decoder. It combines content selection and surface realization in a single end-to-end trained neural network, leveraging multi-task learning for natural language processing tasks. The proposed model enhances neural summarization by incorporating semantic parsing and dual attention mechanisms in a multi-task learning framework. The model generates semantic roles before constructing the summary, improving informativeness and faithfulness. The model utilizes a bidirectional LSTM encoder and attention mechanism to generate semantic roles and produce a summary with a decoder. The loss function is based on negative log likelihood, and the encoder's hidden state is a concatenation of forward and backward LSTMs. The model proposes two decoder architectures - separate decoder and shared decoder - to handle semantic information. In the separate decoder model, each decoder is implemented as its own single-layer LSTM, focusing more on its respective task. In the shared decoder model, one single-layer LSTM is used to produce important semantic structures followed by the summary, reducing parameters and increasing exposure to semantic information. The model uses multi-head attention over the input to acquire different language features, which is beneficial for summarization. The shared decoder model allows different heads to learn semantic and summary aspects. The target semantic output is semantic role labeling (SRL) to identify predicate-argument structures. The model utilizes multi-head attention over the input to capture various language features, aiding in summarization. It employs a shared decoder model for learning semantic and summary aspects, with the target output being semantic role labeling (SRL) to identify predicate-argument structures. The training data is created by parsing articles in the style of PropBank BID14 using the DeepSRL parser from BID10, selecting up to five SRL structures with the most overlap with the reference summary. A dual attention mechanism is used to incorporate semantic information during summary decoding, without relying on external resources. Our dual attention mechanism incorporates semantic information during summary decoding, focusing on specific content portions for information representation. The segment-reranking beam search decoder promotes beam diversity and semantic coverage, with a de-duplication strategy to eliminate redundant predicates. The proposed summary decoder enhances beam search by leveraging generated semantic information to reduce redundancy and improve content coverage through reranking based on repetition and semantic coverage. The proposed summary decoder enhances beam search by leveraging generated semantic information to reduce redundancy and improve content coverage through reranking based on repetition and semantic coverage. The algorithm penalizes longer sentences more heavily than shorter ones and ranks hypotheses based on conditional likelihood and unigram novelty. Beam diversity is enforced through methods like beam expansion to ensure hypothesis diversity during non-reranking timesteps. Beam Expansion and Selection. The algorithm enhances beam search by ranking only the top K extensions from each beam, ensuring diversity by carrying over unique hypotheses. Hypothesis diversity is further reinforced through likelihood and dissimilarity selection methods. The dissimilarity metric used is token-level Levenshtein edit distance. In experiments, B = 12, K = 6, N = 6. In experiments, B = 12, K = 6, N = 6. We use two large datasets of news articles with human-written summaries: CNN/Daily Mail corpus (CNN/DM) and New York Times corpus (NYT). CNN/DM dataset has 287,226 training pairs, 13,368 validation pairs, and 11,490 test pairs. NYT dataset has 280,146 training pairs, and 15,564 pairs each for validation and test. A vocabulary of 50k words is shared by input and output in all experiments, with model parameters and learning rate adopted from prior work. In experiments, model parameters and learning rate are adopted from prior work. Models are trained in stages with increasing token lengths. Unknown tokens are replaced during decoding. Baselines include TEXTRANK and LEAD-2. Abstractive comparison models are SEQ2SEQ and POINT-GEN. Results for model variants are reported. Automatic evaluation metrics are used for F1 scores. Automatic evaluation metrics such as F1 scores of ROUGE-1, 2, and L BID17, and METEOR scores are reported. Extractiveness and redundancy of summaries are measured using density and a new redundancy metric. The study introduces a new adversarial evaluation scheme to test a model's ability to discern irrelevant information in summaries. This involves purposely mixing off-topic sentences into test articles to see if the summarization system can ignore unrelated sentences. 5,000 articles from the CNN/DM test set with a \"news\" tag are randomly selected, with one to four sentences from articles with a \"sports\" tag inserted into each article. The study introduces a new adversarial evaluation scheme to test a model's ability to discern irrelevant information in summaries by inserting off-topic sentences into test articles. For NYT, 15,000 articles with government related tags had up to four sentences from articles outside the domain inserted, ensuring discourse chains are not broken. The study evaluates models' ability to handle irrelevant information by inserting off-topic sentences into test articles. Results show that the models with shared decoder and multi-head attention outperform seq2seq and pointer-generator in ROUGE and METEOR scores. Our shared decoder model consistently outperforms seq2seq and pointer-generator in ROUGE-L and METEOR scores, even with the addition of irrelevant sentences. The semantic decoder helps capture important information, leading to higher quality summaries. On CNN/DM dataset, our models significantly outperform seq2seq across all metrics, with the shared decoder model showing better ROUGE scores than the pointer-generator. Our system summaries, by all model variations, reuse fewer and shorter phrases from the input than the pointer-generator model, indicating a stronger ability to rephrase. Human Evaluation: A pilot study was conducted on 60 samples from the NYT test set, where summaries by different models were ranked by human judges. The study evaluated model summaries based on non-redundancy, fluency, faithfulness, and informativeness. The model outperformed seq2seq in all metrics, with higher rankings in informativeness and fluency compared to the reference summaries. The concise style of the model's summaries was preferred by readers over the informal style of the reference summaries. The study compared human rankings of model summaries on non-redundancy, fluency, faithfulness, and informativeness. Our model showed statistically significant improvement over seq2seq in all metrics. The use of Semantic Roles in summaries indicated adequacy of our semantic decoder, with a high percentage of generated predicates and SRL structures being reused. The study demonstrated that using gold-standard semantic roles in summaries improved ROUGE scores, suggesting potential for enhancing the semantic decoder. Experiments with a coverage mechanism did not show significant differences, while a proposed reranker addressed issues more explicitly without additional training time. The summarization model can also be trained with alternative semantic information. The study showed that using gold-standard semantic roles in summaries improved ROUGE scores. Future work will explore other forms of semantic representation and novel model architectures for semantic-aware neural abstractive summarization. A neural abstractive summarization model with a dual attention mechanism and reranking-based decoder outperformed popular models in handling irrelevant information. Experiments on news corpora demonstrated more informative and less redundant summaries, confirmed by human evaluation."
}