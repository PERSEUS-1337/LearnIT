{
    "title": "Hk4fpoA5Km",
    "content": "The Adversarial Imitation Learning framework has issues with implicit bias in reward functions and high sample complexity. To address this, a new algorithm called Discriminator-Actor-Critic is proposed, reducing interaction sample complexity by 10x and allowing for unbiased reward function application to various problems. The Adversarial Imitation Learning (AIL) algorithms like GAIL and AIRL learn to imitate expert actions using expert demonstrations, a discriminator, and reinforcement learning. They outperform supervised Behavioral Cloning with fewer expert demonstrations, addressing distributional drift. This work addresses issues with AIL algorithms, such as the need for numerous policy interactions for convergence and improper handling of terminal states. The framework combines Discriminator-Actor-Critic imitation learning with a method to explicitly learn rewards for absorbing states. GAIL requires minimal expert transitions to learn robust rewards, while policy frame transitions from the environment can be significantly higher. In this work, the sample complexity of policy frame transitions can be reduced by incorporating an off-policy RL algorithm and an off-policy discriminator. Specific design choices for AIL algorithms and MDPs can significantly impact agent performance in environments with absorbing states. The text discusses the importance of incorporating prior knowledge of the environment reward when choosing a suitable reward function for GAIL and AIRL. It introduces a new algorithm called Discriminator-Actor-Critic (DAC) to improve policy performance in environments with absorbing states by handling absorbing state transitions. DAC algorithm addresses bias from incorrect handling of absorbing states in GAIL and AIRL variants, achieving state-of-the-art AIL performance. It provides solutions for terminal state handling in RL benchmarks, accelerates learning from demonstrations, and shows robustness to noisy expert demonstrations. Imitation learning has been extensively studied under Behavioral Cloning (BC) and Inverse Reinforcement Learning (IRL) frameworks. Maximum Entropy IRL introduces an iterative-sampling based estimator for recovering non-linear reward functions in high-dimensional spaces. Adversarial IRL explores theoretical and practical aspects, connecting IRL with cost learning in GANs. In practical scenarios, the focus is often on recovering the expert's policy rather than the reward. In imitation learning, the focus is on recovering the expert's policy rather than the reward function. Generative Adversarial Imitation Learning (GAIL) framework bypasses the need for the expert's reward function by matching occupancies. Recent work has improved stability and robustness, connecting to model-based imitation learning. Combining expert demonstrations with reinforcement learning for initializing policies has shown success in learning complex behaviors from sparse reward signals. There is a growing interest in combining extrinsic sparse reward signals with imitation learning for guided exploration. Off-policy learning from demonstration accelerates reinforcement learning by structured exploration. Biases associated with specific MDP benchmarks and handling time limits in RL are discussed. The problem with biases associated with episode terminations is severe for AIL algorithms. In Section 4.1, RL benchmarks may not adequately consider absorbing states. The work is related to AIL algorithms BID19, BID10, and BID42. BID42 relies only on observations, unlike BID19 which assumes transition tuples. Other papers have introduced sample-efficient imitation learning algorithms, such as SAM combining DDPG with GAIL, and algorithms based on off-policy reinforcement learning like BID33 and BID36. The work considers problems defined by a Markov Decision Process (MDP). The Markov Decision Process (MDP) is formalized by a tuple: (S, A, p(s), p(s |s, a), r(s, a, s ), \u03b3). It defines state and action spaces, initial state distribution, environment dynamics, reward function, and return discount factor. Absorbing states can be defined for tasks with finite length episodes, where the agent enters a state with zero reward and transitions to itself. This notation simplifies the definition of returns. In reinforcement learning, the goal is to learn a policy that maximizes expected returns. Absorbing states, depicted in an episode of MDP, transition to themselves with zero reward. A common assumption in imitation learning is assigning zero reward to absorbing states. Standard benchmark MDPs often omit absorbing states, biasing reward learning. The proposed DAC algorithm assigns potentially non-zero rewards to absorbing states, addressing this bias. In reinforcement learning, it is crucial to properly handle absorbing states for algorithms where rewards are learned. Time-dependent termination in MDP benchmarks can make tasks non-Markovian, affecting returns. Solutions include using a time-dependent value function or bootstrapping after the terminal state. This approach is derived for infinite horizon problems and does not treat states after time-dependent termination as absorbing states. In reinforcement learning, handling absorbing states is crucial for algorithms where rewards are learned. After time-dependent termination, states are treated as absorbing, and bootstrapping is performed for terminal states. To learn a robust reward function, the GAIL framework is used, which trains a binary classifier to distinguish between expert and trained policy transitions. The policy is rewarded for confusing the discriminator, maximizing performance via on-policy RL. The GAIL framework provides a reward for confusing the discriminator, maximizing performance through on-policy RL optimization. It matches expert and trained policy occupancy measures by maximizing a defined reward with a regularization term based on GAN loss functions. GAIL can be integrated with any on-policy RL algorithm but is adapted for off-policy training in this work. In Section 4.3, modifications are discussed to adapt the algorithm to off-policy training. Specific instances of biased rewards in AIL algorithms due to terminal states are elaborated on. An approach for unbiasing rewards is presented in Section 4.2, followed by the derivation of an off-policy formulation named Discriminator-Actor-Critic (DAC). Examples of bias in AIL algorithms assigning zero rewards to absorbing states are also discussed. In off-policy training, algorithms like GM-MIL BID21 and OptionGAN BID16 use reward functions to assign rewards to intermediate states in MuJoCo locomotion tasks. Absorbing states are ignored in rollouts, leading to difficulties in learning proper rewards and performing bootstrapping after terminal states. To address this, a survival bonus in the form of per-step positive reward can be added to encourage agents to survive longer in the environment. The reward function r(s, a) = \u2212 log(1 \u2212 D(s, a)) has been effective in certain environments. The reward function r(s, a) = log(D(s, a)) is used for tasks with per step penalties, while r(s, a) = \u2212 log(1 \u2212 D(s, a)) is effective for environments requiring a survival bonus but may lead to sub-optimal solutions in tasks where speed is crucial. The proposed method suggests learning rewards for absorbing states in order to recover different reward functions without adjusting the form of reward estimator. This approach aims to address issues related to strong priors and the need to craft different reward functions for each new task. The proposed method suggests learning rewards for absorbing states to address issues related to strong priors and crafting different reward functions for each new task. This allows algorithms to estimate returns for final transitions and optimize policies accordingly. After terminating an episode, transitions to an absorbing state are added, enabling algorithms to properly estimate values for terminal states. The proposed method involves learning rewards for absorbing states to address issues related to strong priors and crafting different reward functions for each new task. This allows algorithms to estimate returns for final transitions and optimize policies accordingly. The GAIL discriminator can distinguish whether reaching an absorbing state is a desirable behavior from the expert's perspective and assign rewards accordingly. To address the sample inefficiency of GAIL, an off-policy RL algorithm is used for training the GAIL discriminator. Equation 2 matches occupancy measures between the expert and replay buffer distributions. Importance sampling is used to recover on-policy expectations. The algorithm works well without importance weights. GAIL discriminator defines rewards for training a policy with TD3. TD3 balances sample complexity and implementation simplicity, making it suitable for practical applications. The DAC algorithm was implemented using TensorFlow Eager and evaluated on popular benchmarks for continuous control. New robotic continuous control tasks were defined in PyBullet, along with a Virtual Reality system for capturing human examples. Multi-task environments were considered challenging for adversarial imitation learning. The DAC algorithm used adversarial imitation learning with critic and policy networks having a 2-layer MLP architecture. Gradient clipping was applied to the actor network. The discriminator had a 2-layer MLP architecture with 100 hidden units. Adam optimizer was used with a decaying learning rate. Regularization in the form of gradient penalties was used to improve stability. Regularization in the form of gradient penalties is used to prevent over-fitting in the discriminator of the DAC algorithm. This technique was initially introduced as an alternative to weight clipping for Wasserstein GANs and has been shown to improve stability in JS-based GANs as well. Expert trajectories are sub-sampled to make the imitation learning task more challenging, and evaluation is performed using 10 different random seeds to compute average episode rewards. The reward is normalized to show zero as random rewards and one as expert rewards. The DAC algorithm uses regularization to prevent over-fitting in the discriminator. Evaluation is done with 10 random seeds, normalizing rewards from zero to one. DAC outperforms GAIL on MuJoCo tasks, showing higher sample efficiency and comparable rewards with fewer environment steps. GAIL uses a reward function with biases encoded in it for training on a specific environment. Results show bias effects on performance, with the agent achieving high rewards without expert demonstrations. The choice of reward function can provide strong prior knowledge for the RL algorithm to recover the expert policy. The method evaluated on two environments in PyBullet with per-step penalty involves a Kuka IIWA arm and 3 blocks on a virtual table. The agent must reach or push blocks using Cartesian displacement action and a compact observation-space. Sparse rewards indicate successful task completion in imitation learning. The DAC algorithm quickly learns to imitate expert human demonstrations in imitation learning experiments using a VR setup. Alternative reward functions require proper handling of absorbing states to avoid early termination. In this work, the authors address sample inefficiency and reward biases in the GAIL framework by proposing a mechanism to learn rewards for absorbing states. This approach aims to improve sample efficiency and mitigate the need for hand-crafted reward functions in tasks. The authors propose an off-policy RL algorithm to improve sample efficiency in imitation learning. Their algorithm achieves state-of-the-art performance on standard RL benchmarks and requires fewer samples than previous GAIL work. The agent in the Kuka-Reach tasks must bring the robot gripper to one of three blocks, while in the Kuka-PushNext tasks, the agent must push one block next to another."
}