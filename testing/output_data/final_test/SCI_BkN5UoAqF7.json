{
    "title": "BkN5UoAqF7",
    "content": "The goal of imitation learning (IL) is to enable a learner to imitate expert behavior from expert demonstrations. Generative adversarial imitation learning (GAIL) has shown progress on IL for complex tasks but requires many environment interactions during training. Reducing the number of interactions could make IL algorithms more applicable to real-world problems. The paper proposes a model-free IL algorithm for continuous control, making three key changes to existing AIL methods. It adopts the Off-PAC algorithm to optimize the learner policy, estimates state-action value using off-policy samples without learning reward functions, and represents the stochastic policy function with bounded outputs. Experimental results show competitive performance with GAIL while reducing environment interactions. Imitation learning (IL) is used when designing rewards for RL algorithms is unclear, as agents can behave poorly. IL enables learners to imitate expert behavior without a reward signal, making it applicable to real-world problems where designing rewards is challenging. Model-free IL for continuous control focuses on representing state-action pairs in continuous spaces. Behavioral cloning (BC) is a simple IL method that learns expert policies in a supervised manner without environment interactions. Behavioral cloning (BC) is a supervised IL method that learns expert policies without environment interactions. However, BC can fail to imitate expert behavior when only a limited number of demonstrations are available due to compounding errors. Inverse Reinforcement Learning (IRL) is another approach that overcomes the compounding error problem. Generative adversarial imitation learning (GAIL) has achieved state-of-the-art performance on continuous control tasks. The adversarial IL (AIL) framework, popular for IL, is more sample efficient than BC but has high sample complexity in terms of environment interaction. AIL methods require a large number of state-action pairs obtained through learner-environment interaction, hindering real-world applications due to training time and potential damage to environments and the learner. Existing AIL methods may have policies that damage the environments and the learner during training, increasing the possibility of damage with more interactions. Real-world applications require algorithms that reduce interactions while maintaining imitation capability. Sample complexity in terms of environment interactions is caused by on-policy RL methods and the use of three optimization processes in AIL methods. Increasing the number of related parameterized functions can lead to unstable or slower training progress, necessitating more interactions. The proposed IL algorithm for continuous control aims to improve sample complexity by adopting an off-policy actor-critic algorithm and Gaussian policy. This approach addresses the issue of slow training progress and the need for more interactions in existing AIL methods. The proposed off-policy IL algorithm aims to optimize the learner policy by estimating state-action values without learning reward functions and using a bounded stochastic policy function. This approach reduces training time and interactions while enabling the learner to imitate expert behavior effectively. The off-policy IL algorithm reduces environment interactions by requiring fewer interactions to imitate expert behavior. Omitting reward learning and bounding action values make training stable and faster in a Markov Decision Process (MDP) framework. The performance measure in a Markov Decision Process (MDP) is defined as the expectation of the discounted return when the agent follows a policy. The state-action value function for the agent following a policy is also discussed. The state-action value function for the agent following a policy is defined as DISPLAYFORM0 |T , \u03c0 , and Q \u03c0,\u03bd denotes its approximator parameterized by \u03bd. RL aims to find an optimal policy maximizing performance. IRL seeks a reward function where expert returns are greater than non-experts. AIL methods adopt max-margin IRL for their objective. The objective of AIL is to train the learner policy using Off-PAC algorithms without learning reward functions. The learner policy is updated by taking the gradient of the state-action value, with proposed formulas for the gradient provided. The algorithm represents the stochastic policy function to ensure bounded outputs. In this section, a new IRL objective is introduced to learn the reward function and value function approximator. The parameterized reward function is defined as R \u03c9 (s, a) = log r \u03c9 (s, a), with r \u03c9 (s, a) representing the probability of expert action execution. This allows for the definition of a Bernoulli distribution to distinguish expert policy from other policies. The reward function R \u03c9 (s, a) = log r \u03c9 (s, a) distinguishes expert actions with a Bernoulli distribution. The return is defined as a log likelihood with p \u03c9 (\u03c0 E |s t , a t ), ensuring a finite return with discount factor \u03b3 in standard RL. The IRL objective assigns r \u03c9 = 1 for expert state-action pairs and r \u03c9 = 0 for non-expert pairs, potentially leading to issues with non-expert policy returns. Our proposed approach aims to address the issue of non-expert policy returns by introducing uncertainty in the evaluation of state-action pairs. Unlike existing methods, our objective assigns equal probability to expert and non-expert actions, explicitly incorporating uncertainty in the return calculation. The proposed approach introduces uncertainty in evaluating state-action pairs to address non-expert policy returns. The objective assigns equal probability to expert and non-expert actions, incorporating uncertainty in return calculation. The optimal solution satisfies the assumption of IRL, with the discounted return represented as a log likelihood. A value function approximator Q \u03c0 \u03b8 following the learner policy \u03c0 \u03b8 can be formed as a log probability. The Bellman equation for the learner policy can be rewritten using additional Bernoulli distributions. The loss function can be bounded by the log likelihood ratio between the two distributions, with learning the approximator matching them. The Jensen-Shannon divergence is used to measure the difference between two probability distributions in optimizing Q \u03c0 \u03b8 ,\u03bd. The optimal reward function R \u03c9 * (s, a) is obtained to rewrite the Bellman equation. The objective is to assign specific rewards based on states and actions, leading to obtaining r \u03c9 * through the Bellman equation. The Bellman equation is optimized using the Jensen-Shannon divergence to measure the difference between probability distributions. The aim of Imitation Learning (IL) is to imitate expert behavior by obtaining a generative model over actions conditioned on states. This is similar to the goal of conditional generative adversarial networks. The aim of Imitation Learning is to imitate expert behavior by obtaining a generative model over actions conditioned on states, similar to conditional generative adversarial networks. The off-policy actor-critic imitation learning algorithm uses a conditional generator to represent the stochastic learner policy. The buffer B \u03b2 stores triplets (s t , a t , s t+1 ) in a FIFO manner. The approximator Q \u03c0 \u03b8 ,\u03bd (s t , a t ) = log q \u03c0 \u03b8 ,\u03bd (s t , a t ) takes values in (\u2212\u221e, 0]. The gradient (6) uses the Jacobian of Q \u03c0 \u03b8 ,\u03bd to update the learner policy. Target value function approximator parameters are updated to track \u03bd. Q \u03c0 \u03b8 ,\u03bd and \u03c0 \u03b8 are updated at the end of each episode. The connection between GANs and IL has been highlighted, showing IRL as a dual problem of RL. The GAIL algorithm is a popular choice for imitation learning, with some extensions proposed. However, these extensions have not addressed reducing training interactions. Our algorithm, detailed in section 5, has comparable imitation capability to GAIL while improving sample complexity. Hester & Osband proposed an off-policy algorithm using expert demonstrations, addressing problems with both demonstrations and hand-crafted rewards. Our algorithm focuses on problems where only expert demonstrations are given, without the ability for the learner to query the expert during training. We aim to answer three questions through experiments: Can our algorithm enable the learner to imitate expert behavior? Is our algorithm more sample efficient than Behavior Cloning (BC)? Is our algorithm more efficient than Generative Adversarial Imitation Learning (GAIL) in terms of training time? We conduct experiments on five physics-based control tasks simulated with MuJoCo physics simulator. In the experiments, the algorithm is compared with BC, GAIL, and GAIL initialized by BC. The agent is trained using TRPO and rewards from OpenAI Gym, then used as an expert for IL algorithms. Two setups are used to study sample efficiency: sparse sampling randomly selects 100 triplets from each trajectory, while dense sampling uses all triplets. Our algorithm is evaluated for sample efficiency in dense and sparse sampling setups using full triplets in trajectories. Performance is measured by cumulative reward earned in experiments on various tasks. Compared to GAIL, our algorithm shows mixed results across tasks, being competitive in some and worse in others. Overall, it is deemed competitive with GAIL in terms of performance. Our algorithm is competitive with GAIL in terms of performance, successfully imitating expert behavior in dense sampling but struggling in sparse sampling with smaller datasets. It outperforms BC across tasks, showing better sample efficiency in expert demonstration. Training efficiency is also demonstrated in Figure 3, where our algorithm trains the learner more efficiently than GAIL in terms of training time. Our algorithm demonstrates improved sample efficiency compared to GAIL, as shown in ablation experiments on the Antv1 task. Variants like Ours+OnP require significantly more interactions, suggesting that off-policy learning enhances sample efficiency. Additionally, Ours+IRL(D) and Ours+IRL(E) show that omitting reward learning can impact performance. The study compared different variants of the algorithm, showing that omitting reward learning and using Gaussian policy or deterministic policy with fixed input noises can affect training stability and speed. The algorithm achieved competitive performance with GAIL while reducing environment interactions significantly. The algorithm uses two neural networks, PN and QN, with different hidden units and non-linearities. PN has 100 hidden units with hyperbolic tangent nonlinearity, while QN has 500 hidden units with sigmoid nonlinearity. All layers use leaky rectified nonlinearity and Xavier initialization. PN takes concatenated vector representations of state s and noise z as input, with z generated from a normal distribution. QN takes a different input. The algorithm uses two neural networks, PN and QN, with different hidden units and non-linearities. QN is trained using RMSProp with specific parameters and learning rates. The target QN is updated using a linear decay rate. Various settings such as minibatch size, replay buffer size, and discount factor are specified. Publicly available code is used for implementation. The experiments are conducted on a PC with specific hardware specifications."
}