{
    "title": "HyeuP2EtDB",
    "content": "Humans can learn task-agnostic priors from interactive experience and utilize them for novel tasks without finetuning. The Scoring-Aggregating-Planning (SAP) framework learns task-agnostic semantics and dynamics priors from interactions, plans on unseen tasks in zero-shot condition, and incorporates a neural score function and dynamics model for planning. This approach achieves similar goals as previous works with off-policy imperfect data, resulting in a generalizable policy for unseen tasks. The proposed framework results in a generalizable policy for unseen tasks, outperforming baseline methods in various applications. While deep RL methods excel in specific tasks, they struggle to generalize without re-training. Humans can adapt quickly to new tasks by leveraging learned priors from past experiences. In this paper, the goal is to teach machine agents to utilize learned priors to generalize to new tasks without the need for fine-tuning. The idea is inspired by how humans can quickly adapt to new tasks by leveraging past experiences and knowledge. The paper aims to enable machine agents to generalize to new tasks using learned priors. It proposes a method where trajectories are composed of state-action pairs with object interactions, and terminal rewards are approximated by aggregating scores. A convolutional neural network parameterizes action-local region pairs, and a neural dynamics model is used to enable agents to utilize the scores. The agent focuses on learning action-state preference scores for local regions and a dynamics model. The agent learns action-state scores for object interactions and a dynamics model to predict future states. It applies priors to new tasks and utilizes planning algorithms. The sparse terminal reward setting is adopted to simplify task evaluation. The method differs from Inverse Reinforcement Learning in three ways. The proposed method differs from Inverse Reinforcement Learning in three key ways: learning a scoring function of a local state and action, using a dynamics model for policy generation, and eliminating the need for expensive expert demonstrations by adding a sparse reward instead. This approach reduces data collection costs and improves model robustness. The proposed method introduces a scoring function for planning and object discovery, demonstrating its effectiveness in various tasks such as a grid-world example, \"Super Mario Bros,\" and a robotics environment. The method learns task-agnostic priors, incorporates a self-supervised dynamics model, and outperforms baselines while discovering objects in an unsupervised manner. In this paper, objects emerge unsupervised from the method, useful for visual tasks. Each environment is formulated as a Markov Decision Process (MDP) with state space (S), action space (A), transition probability (p), reward (r), and discount factor (\u03b3). Reinforcement learning aims to maximize future rewards by learning a conditional distribution over actions given a state. Model-based methods learn a dynamics model M(\u00b7|s, a) to predict the next state from current state and action. Model Predictive Control (MPC) algorithm approximates the optimal action by looking ahead for a horizon of H steps and selecting an action sequence that maximizes future rewards. The environment is assumed to be deterministic, and MPC can use the ground truth environment dynamics. To facilitate learning priors and generalization, the problem is formulated for an intelligent agent presented with exploratory trajectories. The agent is presented with exploratory trajectories in a training environment. Each trajectory is evaluated with a terminal reward when a task is given. At test time, the agent must perform the task using zero extra interaction with a new but related environment. The test environment may have different dynamics but the same possible actions. The agent is evaluated based on the terminal reward per trajectory. The proposed formulation for evaluating an agent on task T involves giving a single terminal reward per trajectory based on previous experiences. This reward is used solely for evaluation and cannot be utilized during the task. The focus is on locomotion tasks with object interactions, and the Scoring-Aggregating-Planning framework is proposed to solve this problem. The framework involves learning a per step neural scoring function to score a sub-region surrounding the agent, aggregating local region scores along the trajectory to approximate the terminal sparse reward, and learning a dynamics model to approximate the true transition. The proposed framework involves learning a per step neural scoring function to score a sub-region surrounding the agent, aggregating local region scores along the trajectory to approximate the terminal sparse reward, and learning a dynamics model to approximate the true transition. Model Predictive Control algorithm is then used to derive the final policy on a new environment for the task. The framework involves learning a per step neural scoring function to score a sub-region around the agent and aggregating local region scores to approximate the terminal sparse reward. This is done by dividing the local region into sub-regions and using a scoring network to produce a scoring table. The scores are aggregated into a single score J to learn the scoring network. The framework involves learning a per step neural scoring function to score a sub-region around the agent and aggregating local region scores to approximate the terminal sparse reward. This is achieved by using planning algorithms to find optimal actions based on the learned scoring function and dynamics model. The dynamics model is learned through a neural network that predicts the next state based on the current state and action. The framework involves using a neural scoring function and dynamics model to optimize actions for a task in a given environment. The Model Predictive Control algorithm is used to find the best trajectory by minimizing a specific objective function. The process includes selecting action sequences, estimating future states, and choosing the best sequence based on aggregated scores. This approach is related to Inverse Reinforcement Learning. Inverse Reinforcement Learning (IRL) aims to learn a reward function from expert demonstrations, while recent work has extended this to visual states. Unlike IRL, our method learns from exploratory data not related to specific tasks. Some works address violations of IRL assumptions, such as inaccurate state perception or incomplete dynamics models. Recent work has focused on reward shaping and learning reward functions automatically, with some efforts requiring manual design. This approach differs from Inverse Reinforcement Learning (IRL) which learns from expert demonstrations and addresses violations of its assumptions. Recent work has focused on automatic reward shaping methods for RL, including abstracting state space and building graphs on discretized states. However, these methods do not apply to high-dimensional inputs like images. RUDDER utilizes LSTM to decompose rewards, while our SAP framework can handle high-dimensional tasks with sparse rewards. Strategies like curiosity-driven exploration and count-based exploration address the sparse reward problem efficiently. In goal-conditioned tasks, Hindsight Experience Replay can be used to learn from experiences with different goals. Auxiliary tasks can help learn meaningful intermediate representations. This problem is approached by learning a scoring function for each timestep based on the single terminal reward. In the planning part of our SAP framework, a dynamics model is trained using various methods such as Gaussian Process, time-varying linear models, mixture of gaussian models, and neural networks. This approach has been widely studied in robotics and applied to high dimensional spaces. Model-based reinforcement learning (RL) has been extensively studied in high dimensional spaces like robotic applications and Atari games. However, previous work has not explored combining it with learning dense task-agnostic scores from sparse signals. Recent research emphasizes the importance of priors in tasks like playing video games and utilizing visual priors in domains such as robotics for generalization. While some methods explicitly handle object-level learning, our approach focuses on meaningful scores without explicitly modeling objects. Our method focuses on learning meaningful scores in the SAP framework, allowing for generalization to new tasks without finetuning. Comparisons with other methods are made in different environments, including a didactic gridworld task, \"Super Mario Bros,\" and a robotics blocked reacher task. The framework's ability to learn meaningful scores and induce correct policies is investigated. In a didactic task called Hidden Reward Gridworld, our method aims to recover per step scores by learning the value of objects in a grid environment. The task involves exploring trajectories to collect as much value as possible from objects with unknown point values. The experiment uses an 8x8 grid with 16 different types of objects worth varying points. To add challenge, noisy features are generated for each object instead of revealing their identities. Our method aims to recover per step scores by learning the value of objects in a grid environment using a two-layer neural network. The neural network fits the object value based on features with minimal error, even in the presence of feature noise. Two behavior cloning agents are trained to imitate exploratory data and SAP behavior on the training environment. In a grid world environment, two behavior cloning agents, BC-random and BC-SAP, are trained to imitate exploratory data and SAP behavior. BC-random has inferior performance as it clones exploration behavior, while BC-SAP performs well in the training environment but worse in a new environment. SAP is shown to learn accurate scoring functions in the presence of object feature noises and generalize better than alternative algorithms. The algorithm is also evaluated in the Super Mario Bros environment, where dense scores for each step are not clearly defined. The environment features high-dimensional visual observations with a 84x84 size 4-frame stacked gray-scale observation. The goal is for the agent to survive and move towards the right as far as possible, with the environment providing a delayed terminal sparse reward based on the agent's progress. The agent's local region is divided into eight 12x12 pixel sub-regions, each scored by a CNN to output a final score. The agent's local region is divided into eight 12x12 pixel sub-regions, each scored by a CNN to output a final score. A dynamics model is learned by training another CNN to predict the future location of the agent recursively. Video predictive models are avoided due to the blurry effect when predicting long term future. Our method involves using learned scores and dynamics model with a standard MPC algorithm to predict long-term future actions. We compare our approach with Exploration Data, Behavioral Cloning (BC), and DARLA, which focuses on transferring latent state representations between training and testing environments. The proposed SAP model achieves transferability from training to testing environments by obtaining a disentangled representation of generative factors. It outperforms baselines significantly and addresses limitations of Behavioral Cloning (BC) by using a behavioral cloning agent with predefined naive human priors. The SAP model outperforms DARLA and NHP methods in training and generalization tasks by learning disentangled representations in Mario games. It demonstrates better understanding of the world and stronger performance without finetuning, showing good generalization abilities. The SAP model shows improved performance with a perfect dynamics model, outperforming NHP. Ablative experiments with a groundtruth model demonstrate performance boosts for both NHP and SAP on original and novel tasks. The SAP model outperforms NHP with a perfect dynamics model, showing improved performance on original and novel tasks. The absence of a \"done\" signal affects NHP performance, indicating heavy reliance on this signal. Further ablation studies are suggested, including planning horizon and visual representations. Visualized greedy actions from learned scores in a new environment are also presented. In a new environment, actions are induced by greedily maximizing one-step scores, which may differ from the original policy. The actions are visually different in World 5 Stage 1, showing reasonable behavior like avoiding obstacles and monsters. However, some elements like \"Piranha Plants\" are not recognized due to prior learning from a different stage. Additional visualizations of action maps and prior scores for local sub-regions are available in the appendix. The SAP method learns meaningful scores for different objects in unsupervised manner and produces good actions in new environments. In a 3-D robotics environment, the BlockedReacher-v0 task involves moving a robot hand past obstacles to reach a specific point quickly. The SAP framework is used to move a robot hand to a point on y = 1.0 quickly in different obstacle configurations. The environment is divided into voxels and sub-regions for scoring using a neural network. The SAP framework utilizes a 3D convolutional neural net for dynamics modeling and MPC planning with a horizon of 8 steps. Compared to the NHP method, SAP requires fewer steps due to its ability to avoid obstacles efficiently. The learned dynamics model shows high accuracy in this domain. The SAP framework, utilizing a 3D convolutional neural net for dynamics modeling and MPC planning with an 8-step horizon, demonstrates high accuracy in robotics environments. It outperforms baseline methods on both training and unseen testing tasks, showcasing the ability to learn generalizable priors without finetuning. The SAP framework, utilizing a 3D convolutional neural net for dynamics modeling and MPC planning with an 8-step horizon, demonstrates high accuracy in robotics environments. It outperforms baseline methods on both training and unseen testing tasks, showcasing the ability to learn generalizable priors without finetuning. In different application domains, there is a welcoming avenue for future work to improve each component of the framework, especially for complex tasks with a much more complicated action space. Extracting relational priors from existing human priors in video game playing for real-world applications in an SAP framework is still an open question. Our method excels in evaluating interactive samples in the real world, providing task-specific scores or human evaluations. In theory, our method can be extended to cases with binary sparse rewards by choosing appropriate aggregators. The environment consists of a gridworld with features represented by noised vectors. Actions include moving in four directions, and the agent is ignored if it tries to move outside the grid. Training involves a two-layer neural network with specific hidden units. In the environment, a two-layer neural network is trained with specific hidden units to approximate grid scores. Hyperparameters include using an adam optimizer with a learning rate of 1e-3, reduced to 1e-4 after 30000 iterations. The batchsize is 128, and a planning horizon of 4 is used. The Super Mario environment is wrapped with additional wrappers for action space and a CNN is trained for the score function. The score function is trained using a CNN with 2 conv layers and 1 fully connected layer. Each conv layer uses a 3x3 filter with stride 2 and \"same padding\", with 8 and 16 output channels. The fully connected layer has 128 units. Relu activation is used for intermediate layers, except the last layer which has two output heads for location shift and \"done\" prediction. Training uses an adam optimizer with learning rate 3e-4, batchsize of 256 for score function and 64 for dynamics model. Planning horizon is 10, discount factor is 0.95, and 128 environments are used in MPC. Each data point in training is a tuple of down sampled trajectory and calculated values. During scoring function training, data points consist of down sampled trajectories and calculated scores. Trajectories are down sampled by taking data from every two steps, with half ending in a \"done\" event. For those ending in \"done\", the score is the distance Mario traveled. For others, the score is the distance traveled plus a mean future score, which is the average extra distance traveled by longer trajectories. Pure behavior cloning gets stuck early on, so actions are selected by sampling from output logits. During scoring function training, data points consist of down sampled trajectories and calculated scores. Trajectories are down sampled by taking data from every two steps, with half ending in a \"done\" event. For those ending in \"done\", the score is the distance Mario traveled. For others, the score is the distance traveled plus a mean future score, which is the average extra distance traveled by longer trajectories. Pure behavior cloning gets stuck early on, so actions are selected by sampling from output logits. We train a policy with curiosity as rewards, early stopping training after 5e7 steps, and adding epsilon-greedy noise when sampling demonstrations. Ablative experiments evaluate the effect of planning horizon in a MPC method, showing our method outperforms baselines consistently with different planning steps. In experiments evaluating the proposed visual representation, local sub-regions contribute to training and zero-shot generalization performance. Even without subregions, the SAP model outperforms the second strongest baseline due to scoring and planning steps empowering the agent to learn and generalize. Model dissection involves ablating the scoring-aggregating and planning components, with comparisons to NHP method and Greedy method. In Table 5, it is shown that all components of SAP are critical to its performance. Comparisons are made with privileged BC and a reinforcement learning agent trained with curiosity-driven reward and sparse reward. The privileged BC baseline uses near-optimal trajectories for training, while the reinforcement learning agent is limited to 10M training steps. In Table 6, both baselines show a significant drop in generalization compared to SAP. Additional visualizations in Figures 8 and 9 demonstrate the agent's ability to navigate obstacles and avoid monsters. The scores of state-action pairs align with human intuition, showcasing the agent's performance in different scenarios. The agent's performance in different scenarios is showcased through visualizations in Figures 8 and 9. The score is significantly lower when the agent's position is to the left of \"koopa\" compared to other positions, consistent with human priors. In the Blocked Reach environment, a 7-DoF robotics arm is manipulated for a specific task using a 200 \u00d7 200 \u00d7 200 voxel cube. The action space is discretized into two choices for each dimension, resulting in a total of 8 actions. Four configurations are designed for evaluating different methods. In the Blocked Reach environment, a 7-DoF robotics arm is manipulated using a 200 \u00d7 200 \u00d7 200 voxel cube with 8 actions. Four configurations are designed for evaluation, each with three objects as obstacles of varying heights. The score function utilizes a fully-connected neural network with 128 units and Relu activation. The dynamics model involves training a 3-D convolution neural network with specific parameters for encoding local voxels and actions. The FC layer connects to flattened features after convolution. Action is encoded with a one-hot vector connected to a 64-unit FC layer. The last three \u03b4 positions are also encoded with a 64-unit FC layer. The encoded features are concatenated and pass through a 128-unit hidden FC layer to output predicted position change. Training uses adam optimizer with learning rate 3e-4, \u03b2 1 = 0.9, \u03b2 2 = 0.999. Batchsize is 128 for score function training and 64 for dynamics model. Planning horizon is set to 8. Naive human prior baseline in blocked robot environment is an 8-step MPC. Behavioral cloning baselines are omitted due to previous results."
}