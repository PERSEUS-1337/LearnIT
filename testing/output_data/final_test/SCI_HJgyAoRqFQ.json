{
    "title": "HJgyAoRqFQ",
    "content": "Recurrent neural networks (RNNs) face challenges in training due to input noise amplification and weight inaccuracies. A method for denoising the hidden state during training improves generalization by incorporating attractor dynamics to clean up representations in each sequence step. The state-denoised recurrent neural network (SDRNN) outperforms generic RNNs on various tasks by performing multiple internal processing steps for each external sequence step. Incorporating attractor dynamics in deep learning can improve noise robustness and transfer learning. While traditional methods focus on handling external noise, internal noise within deep-net architectures poses a greater challenge. Attractor dynamics should be utilized not only in recurrent networks but also in deep feedforward nets to enhance performance and address internal noise issues. In this article, a suppression method is proposed to improve the generalization performance of deep nets, focusing on recurrent nets. The approach draws inspiration from the human brain's robust information processing system, utilizing categorization and interpretation processes to suppress irrelevant features that may interfere with subsequent processing. Categorization and interpretation processes suppress noise and increase behavior reliability. Language, like categorization, treats variations as identical for effective communication. Color terms in native language influence perceptual tasks. Suppressing variability aids in information processing systems for accurate message interpretation. The sender should limit messages to a canonical form for successful interpretation by the receiver. Noise suppression methods in deep-learning models mimic language's role in communication. A recurrent neural network architecture, called an attractor net, is proposed for denoising internal states in deep networks. Experiments show benefits of denoising in data-limited situations. Attractor nets are discrete-time nonlinear dynamical systems that converge to a limit cycle or fixed point under certain conditions. They have a history dating back to the work of Hopfield in the 1980s, where dynamics perform local energy minimization based on an energy function. Attractor nets converge to a fixed point or limit cycle under specific conditions. Koiran's framework, based on synchronous updates and continuous-valued neurons, shows that with symmetric weights and nonnegative self-connections, the network asymptotically converges to stability. Attractor nets converge to stability by mapping cues to well-formed outputs, such as in content-addressable memory applications. Training procedures sculpt the energy landscape to make each input an attractor, allowing for noise-corrupted inputs to be cleaned up and reconstructed. The attractor dynamics operate in the same representational space as the input and output. Attractor nets use a higher dimensional latent space to increase representational capacity. The input is projected to an n-dimensional attractor state, where n > m for flexibility. The attractor dynamics operate with an initialization of a 0 = 0 and use tanh function. The asymptotic attractor state is mapped to the output using an output activation function. Attractor nets use m degrees of freedom for visible features and n - m for latent features. Short-circuit connections between input and output prevent vanishing gradients in deep networks, allowing for easy gradient propagation. If m = n, the network can simply copy x to y. In simulations, an alternative architecture allows for copying x to y by using unbounded input and a bounding nonlinearity on the output. This formulation is elegant for tanh neuron layers. Supervised training involves noisy input states and corresponding target outputs. The aim is to sculpt attractor basins related to noise corruption levels. The denoising loss is normalized to indicate denoising success or failure. Attractor networks were trained with varying parameters, including the number of target attractor states, noise corruption levels, and the number of units in the attractor net. The network is run to convergence using a criterion to ensure stability. Convergence typically occurs in under 5 steps with \u03b4 = .01. The percentage of noise variance removed by the attractor dynamics is shown in FIG1. Different attractor net sizes (n) affect storage capacity and overfitting potential. The smallest and largest nets (n = 50, 200) have lower storage capacity than intermediate nets (n = 100, 150) as noise suppression drops with increasing attractors. Hidden attractor space should be about twice as large as input/output space. Training noise level should match testing noise level for optimal noise suppression. Optimal parameters are \u03c3 = 0.250 and n = 100 for network capacity. After demonstrating attractor dynamics for denoising vector representation, the focus shifts to denoising hidden states of RNNs for sequence processing. RNNs are sensitive to noise due to feedback loops amplifying noise. An unrolled RNN architecture with an integrated attractor net is shown in Figure 3, aiming to denoise the hidden state for improved generalization performance. The architecture described in Figure 3 integrates an attractor net to denoise the hidden state of RNNs for sequence processing. The attractor net parameters are trained to minimize L denoise, while the RNN parameters are trained to minimize a supervised task loss, L task. This model, referred to as a state denoised recurrent neural net (SDRNN), is trained on both L denoise and L task objectives. The SDRNN training procedure involves taking stochastic gradient steps on L task for all RNN and attractor weights, recomputing hidden states for each training sequence, denoising the hidden state with the attractor net, and repeating the denoising step if necessary. The model is tested on a streamed parity task with 10 binary inputs to determine the target output based on the presence of an odd number of 1s in the input sequence. The architecture includes m = 10 hidden units, n = 20 attractor units, and uses tanh and GRU hidden units. The attractor net was trained with \u03c3 = .5 and ran for 15 iterations. Models were trained on 256 binary sequences and tested on 768 held-out sequences and noisy versions of the training sequences. Performance of RNN, RNN+A, and SDRNN was evaluated on the held-out sequences. The SDRNN outperforms both the RNN and RNN+A on held-out sequences, showing better generalization to novel sequences and noise resistance. Performance improves with GRU hidden units, but denoising the hidden state has a larger impact. GRUs and LSTMs are considered critical innovations in deep learning. The restricted state space in deep learning is a critical innovation for better generalization. Varying \u03c3 in training attractors affects performance, with an intermediate \u03c3 being optimal. Weight decay to forget attractors learned did not show systematic improvement. Introducing weight decay with an L2 regularizer did not show systematic improvement in training attractors. Denoising the internal state during training helped the model overcome input noise and generalize better. The model was tested on a majority task with binary sequences as input. For the majority task with binary sequences, different models were tested with fixed-length sequences. The models had specific parameters and were compared based on their performance on test sets. The SDRNN outperformed the RNN and RNN+A models for all sequence lengths on both test sets. The SDRNN outperforms the RNN for short novel sequences and long noisy sequences. Both architectures reach a ceiling for short noisy sequences. The lack of a difference is seen for novel long sequences in the Reber grammar simulation. The task involves discriminating between strings that can and cannot be generated by the finite-state grammar using specific parameters and models. The SDRNN outperforms the RNN in various tasks, including the Reber grammar simulation and the symmetry task. The Reber grammar task involves discriminating between strings generated by a finite-state grammar. The symmetry task requires retaining the first half of a sequence to compare against the second half. The SDRNN outperforms the RNN in tasks like the Reber grammar simulation and symmetry task. For the Reber grammar task, symbols are generated and strings are formed with fillers. Negative cases are created by exchanging symbols or substituting them. Training and test sets have equal positive and negative examples. The SDRNN is trained on 5000 examples and tested on 2000, with varying slot filler lengths. The SDRNN shows a 70% reduction in test error compared to RNN or RNN+A in natural language processing tasks like part-of-speech tagging. Data from the NLTK library is used with GloVe embeddings for word representation and 147 POS tags as outputs. Models are trained and tested on sentences with early stopping and validation sets. The comparison between SDRNN and RNN is done on four replications of each simulation. The SDRNN, compared to RNN, shows impressive performance advantages on small training sets. It incorporates attractor networks for forward and backward GRUs, with 50 GRUs in each direction and 100 attractor units in each network. The SDRNN is related to the fast associative memory model and outperforms RNN in sequence-processing tasks. The SDRNN is a sequence-processing recurrent net with a distinct multistage subnetwork modulating its hidden state through associative weights trained via Hebbian learning. The goal is rapid learning over a single sequence by evoking memory of earlier states to integrate with the current state. This model operates in the hidden activation space, unlike a denoising goal-focused architecture. The SDRNN is a sequence-processing recurrent net with a distinct multistage subnetwork modulating its hidden state through associative weights trained via Hebbian learning. In contrast to a more computationally powerful architecture, the SDRNN operates in a hidden-hidden state space. Like Graves's model, SDRNN varies in computation time needed for internal updates based on attractor dynamics settling into a well-formed state. Research with a cognitive science focus has proposed a model that efficiently learns new concepts and control policies by operating in a linguistically constrained representational space obtained through pretraining on a language task. The consciousness prior of BID3 suggests operating in a simplified representational space for language. Bengio theorizes that a restricted representation in the brain may aid rapid learning. BID20 proposed an auxiliary loss function for RNN training to improve sequence processing performance. Noise robustness is crucial for neural networks. The development of a recurrent net architecture with an explicit objective of attaining noise robustness in neural networks, regardless of the source of noise. Attractor dynamics are used to ensure well-formed representations in various tasks, including deep feedforward and recurrent nets. Convolutional attractor networks are being explored for image transformations like denoising and superresolution. Transformations like denoising, superresolution, and coloring in BID19 require satisfying constraints expressed in a Lyapunov function and approximated by deep feedforward nets. Transfer learning, similar to SDRNN, leverages representations from one domain to another. The attractor net is initialized with weights from a normal distribution, and a fixed initial learning rate is used with the ADAM optimizer. Mean squared error is used for all tasks in synthetic simulations. For synthetic simulations tasks like parity, majority, Reber, and symmetry, training stops when performance asymptotes on the training set. Testing uses weights with highest accuracy, not lowest loss. Mean squared error is used for all tasks. In Parity, learning rate is .008 and training continues until 100% accuracy or 5000 epochs. The training set is 100% or until 5000 epochs are reached, with complete batch training to avoid noise. Hidden units are tanh, and entropy calculation is based on discretizing hidden units. Attractor weights are trained on attractor loss in one simulation, and on both losses in following simulations. Networks are trained for 2500 epochs or until perfect classification accuracy. Attractor net is run for 5 steps with 10 hidden units and \u03c3 = 0.25. Balance between positive and negative examples is expected in both training and test sets. The networks are trained for 2500 epochs or until perfect classification accuracy is achieved on the training set. Strings longer than 20 characters were filtered out, and shorter strings were left padded with the begin symbol, B. The architecture included 20 hidden units and 40 attractor units, with the attractor net running for 5 steps. The introduction of L denoise was postponed until 100 epochs had passed. A learning rate of .002 was used for both losses for f = 10 and .003 for f = 1. The data set had 472 unique POS tags, with only the 147 most popular tags preserved and the rest grouped into a catch-all category, totaling 148 tags. The study utilized 148 tags, with a catch-all category containing 0.21% of words. An independent attractor network was used for each RNN direction, with specific parameters such as m = 50 GRU hidden units, n = 100 attractor units, and \u03c3 = 0.5. Dropout of p = 0.2 was applied before projecting the output into the final POS class probability layer. Averaged categorical cross-entropy was used as the optimization objective for RNN models, with attractor nets running for 15 iterations."
}