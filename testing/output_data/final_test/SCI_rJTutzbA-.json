{
    "title": "rJTutzbA-",
    "content": "Momentum based stochastic gradient methods like heavy ball (HB) and Nesterov's accelerated gradient descent (NAG) are commonly used for training deep networks and supervised learning models, showing improvements over stochastic gradient descent (SGD). Fast gradient methods are proven to be better than gradient descent only in the deterministic case with exact gradients. However, this work challenges the belief that these methods outperform SGD in all cases, showing that there are problem instances where they cannot despite optimal parameter settings. The practical performance gains of heavy ball (HB) and Nesterov's accelerated gradient descent (NAG) are likely due to minibatching, as negative problem instances are not carefully constructed pathological cases. The Accelerated Stochastic Gradient Descent (ASGD) algorithm significantly outperforms HB, NAG, and SGD on the same problem instances. ASGD, based on Nesterov's Acceleration, is a simple stochastic algorithm with performance gains proven through empirical results. The code for implementing ASGD can be found at https://github.com/rahulkidambi/AccSGD. First order optimization methods, like gradient descent, are crucial for large scale optimization problems such as training deep neural networks. However, for smooth convex functions, gradient descent is suboptimal, leading to the development of fast gradient/momentum based methods like the heavy ball method and Nesterov's accelerated gradient descent. Stochastic Gradient Descent (SGD) is also commonly used for training deep neural networks on large datasets. Stochastic Gradient Descent (SGD) BID10 samples a random subset of training data to compute gradient estimates for optimization. The theoretical advantages of fast gradient methods (Nesterov, 1983) coupled with cheap stochastic gradient estimates led to the influential work demonstrating empirical advantages of SGD with momentum. Momentum methods are widely adopted for training deep neural nets, with gradient descent often referring to momentum methods. In practice, momentum methods use stochastic gradients instead of exact gradients. It is unclear if momentum methods offer any improvement over SGD in the stochastic first order oracle model. Jain et al. (2017) studied a variant of Nesterov's accelerated gradient updates BID2 for stochastic linear regression, showing that their method, referred to as Accelerated Stochastic Gradient Method (ASGD), improves upon SGD in information-theoretically admissible regimes. It is still unknown if other methods like NAG and HB can achieve similar performance gains. The study compares the performance of HB and NAG to SGD, showing that HB does not outperform SGD even when theoretically admissible. Empirical evidence also suggests that NAG does not surpass SGD in performance. The improved performance of HB and NAG is attributed to mini-batching, making it challenging for them to outperform SGD with small batch sizes. ASGD offers noticeable improvements over HB and NAG, achieving 5-7% better test error even with batch sizes like 128. The paper proves that HB is not optimal in the SFO model and ASGD outperforms both HB and SGD. Experiments show that HB's suboptimality in the SFO model extends beyond linear regression problems. The suboptimality of HB in the SFO model is widespread, as seen empirically with NAG as well. Momentum methods excel due to mini-batching, reducing variance in stochastic gradients. Algorithm 1 outlines HB with a SFO, showing improvements over SGD by a factor of \u221a\u03ba. The main result of this section establishes that HB cannot provide a similar improvement over SGD as ASGD. Regardless of the choice of parameters, HB's performance does not surpass SGD by more than a constant. ASGD can achieve a better approximation to the optimal solution in fewer iterations compared to HB. The text discusses the empirical observation that NAG achieves similar rates as HB and SGD for the same problem instance. It is conjectured that a lower bound for NAG can be established using a proof technique similar to that of HB. Additionally, an intuitive version of ASGD is presented, with inputs including short step \u03b4, long step parameter \u03ba, and statistical advantage parameter \u03be. The algorithm maintains two iterates: descent iterate w t and a running average w t. The running average is a weighted average of the previous average and a long gradient step from the descent iterate, while the descent iterate is updated as a convex combination of short gradient step from the descent iterate and the running average. The idea is that the algorithm can make progress on different directions at a similar pace by taking a long step as well as a short step and an appropriate average of both of them. Algorithm 3 and ASGD as proposed in Jain et al. (2017) explore performance of SGD, HB, NAG, and ASGD for linear regression problems with different distributions. The study compares the performance of SGD, HB, NAG, and ASGD for linear regression with different distributions. The algorithm converges based on error criteria and grid search is used to find optimal learning rate and momentum parameters for NAG and HB. The study compares the convergence performance of SGD, HB, NAG, and ASGD for linear regression. Optimal parameters for NAG and HB are chosen based on minimal error. Experimental results on training deep autoencoders for the mnist dataset are presented following the setup of Hinton & Salakhutdinov (2006). The study evaluates the convergence performance of optimization algorithms (SGD, HB, NAG, ASGD) for linear regression. Deep autoencoders are trained on the mnist dataset using a specific network architecture and initialization scheme. Training is conducted with two minibatch sizes and a grid search over learning rate, momentum, and long step is performed for each optimization algorithm. The study evaluated the convergence performance of optimization algorithms (SGD, HB, NAG, ASGD) for linear regression. A grid search was conducted over learning rate, momentum, and long step parameters for each algorithm. Results were compared to those in published literature, showing differences in update counts and batch sizes. Minibatch sizes also impacted the loss decay rates of HB and NAG compared to SGD. The study compared the convergence performance of optimization algorithms (SGD, HB, NAG, ASGD) for linear regression. While HB and NAG decay the loss faster than SGD for a minibatch size of 8, this advantage is not seen for a minibatch size of 1. ASGD performs slightly better than NAG for batch size 8 in training error, but decays the error faster for a batch size of 1. Experimental results were presented for training deep residual networks with pre-activation blocks for image classification in cifar-10. The experiment introduces learning rate decay based on a validation set scheme, where the learning rate is decreased after every 3 epochs if the validation error does not improve. Training error plots are shown for different batch sizes, with a comparison of HB, NAG, and SGD algorithms. Grid search is performed for each algorithm with varying minibatch sizes. The final error achieved by SGD, HB, and NAG are very close for both batch sizes after a grid search over all parameters. NAG shows superior convergence compared to SGD and HB for batch size 128, but this advantage disappears for batch size 8. The next experiment compares ASGD with HB and NAG, showing that ASGD has similar/favorable final error compared to other methods. The focus is now on understanding if ASGD has a superior convergence speed, considering the differing learning rates and decay iterations used by each algorithm. The results show that ASGD decays error faster than HB and NAG across different batch sizes. ASGD outperforms HB when fully optimized, indicating superior convergence. This suggests that ASGD has a faster error decay rate compared to other methods. The primary method in the family of first order oracle methods is Gradient Descent (GD), which is suboptimal for smooth convex optimization. Momentum methods like the Heavy Ball method and Nesterov's Accelerated gradient descent address this issue. Stochastic first order methods, such as Stochastic Gradient Descent (SGD), have been highly effective beyond convex objectives. Accelerating SGD is challenging due to the instability of fast gradient methods in handling noise. The instability of fast gradient methods in dealing with noise has been evidenced by negative results. While HB and NAG are effective with exact first order oracles, their performance with inexact gradients is not well understood. Efforts spanning decades in various communities have tried to understand HB's performance with noisy gradients. Polyak (1987) concluded that improvements offered by HB with inexact gradients diminish unless strong assumptions on inexactness are made. Improved non-asymptotic rates are suggested by stochastic HB. Stochastic HB offers improved non-asymptotic rates but at the cost of worse asymptotic behavior. Loizou & Richt\u00e1rik's method requires stochastic gradients close to exact gradients for matching rates of standard HB. Faster methods for finite-sums like SDCA, SAG, SVRG, and SAGA provide linear convergence rates for strongly convex finite-sums. SAGA (Defazio et al., 2014) offers linear convergence rates for strongly convex finite-sums, surpassing SGD's sub-linear rates. Accelerated variants have been developed to improve these methods. However, they require storing the entire training set in memory and multiple passes over it for progress. Simple streaming methods like SGD, ASGD, HB, and NAG are used to mitigate the memory and computation requirements for deep learning problems. Distinctions between offline and online stochastic methods are discussed in Frostig et al. (2015b). Momentum-based methods like BID16 have become popular for training deep networks, outperforming standard SGD on practical problems. Methods like Adagrad, RMSProp, and Adam offer advantages orthogonal to fast gradient methods. Exploring augmentation with ASGD is suggested over HB or NAG based acceleration schemes. Entropy-SGD, proposed by Chaudhari et al., introduces an altered objective with a local strong convexity term. In this paper, the authors propose Entropy-SGD, an altered objective with a local strong convexity term to improve generalization. They mention using SGD in their procedure and employ the HB/NAG method for better performance. Path normalized SGD BID3 is another variant that could benefit from HB/NAG or ASGD methods. The performance gain of HB over SGD in a stochastic setting is attributed to mini-batching rather than the algorithm's ability to accelerate with stochastic gradients. The authors introduce Entropy-SGD with a local strong convexity term to enhance generalization, utilizing SGD and HB/NAG methods. ASGD shows improvement over SGD in various problem instances like training a resnet on cifar-10 and an autoencoder on mnist. The study motivates exploring ASGD in NLP domains and developing momentum tuning schemes. The text discusses the covariance matrix of \u03b8 and introduces Proposition 3, which states that either B (1) or B (2) has eigenvalues larger than 1. Lemmas 4 and 5 provide conditions for the eigenvalues of B (1) and B (2) respectively. The analysis involves computing the characteristic polynomial of B to obtain bounds on its roots, denoted by x and t. Lemma 6 provides the characteristic polynomial of matrix B. Corollary 7 substitutes a value in the characteristic equation. Lemma 4 shows that the characteristic polynomial approaches infinity as z approaches infinity. By setting certain conditions, it is shown that there exists a solution. Lemma 6 provides the characteristic polynomial of matrix B, while Lemma 9 discusses the momentum parameter \u03b1 in the context of convergence in optimization algorithms. Corollary 11 states that for large momentum values, stochastic heavy ball behaves like stochastic gradient descent. By substituting values in equations and bounding terms, it is shown that lower-order terms become negative due to a large constant. The expression evaluates to \u2264 0 given an upperbound on the value of c, with terms of O(1/\u03ba 4 ) and O(1/\u03ba 5 ). Lemma 13 ensures that the HB method produces a non-zero result regardless of the starting iterate. Lemma 13 states that the HB method generates a non-zero component along the largest eigen direction of B for any starting iterate. The proof involves examining the subspace spanned by covariance for each dimension separately, decaying at a rate of \u03bb max (B). The HB method generates a non-zero component along the largest eigen direction of B for any starting iterate, with the subspace spanned by covariance for each dimension decaying at a rate of \u03bb max (B). In the analysis, it is shown that dropping the superscript j that represents the dimension does not affect the generality, leading to the formation of a matrix D containing two identical rows representing an eigenvector of B with an eigenvalue \u03b1. The analysis shows that the matrix D, formed with two identical rows representing an eigenvector of B with eigenvalue \u03b1, spans a three-dimensional subspace when eigenvalues of B have different magnitudes. This implies that D contains a component along the largest eigendirection of B, rounding up the proof. To determine if matrix D spans a three-dimensional subspace, consider matrix R with det(R) = 0 implying three non-zero eigenvalues. Define q \u03b3 = (t \u2212 \u03b3) 2 + (c \u2212 1)x 2, express R accordingly. Show determinant cannot be zero by analyzing convergent regime and proving \u03b4\u03c3 is much larger than allowed by HB updates. Prove c+(c\u22122)\u03b1 holds for any admissible value of \u03b1 to ensure determinant does not vanish in divergent regime. The proof of Lemma 5 shows that regardless of the chosen stepsize and momentum, matrix B (j) has an eigenvalue with magnitude at least 1 - 500 \u03ba for some j \u2208 {1, 2}. This is established by combining Lemmas 9 and 12. The ASGD updates from Jain et al. (2017) are then detailed, starting with two iterates a 0 and d 0, and proceeding through time steps t = 0, 1, ... T - 1. The ASGD updates from Jain et al. (2017) involve starting with two iterates a 0 and d 0, and implementing updates at time steps t = 0, 1, ... T - 1. The step sizes are specified as \u03b2 1 = c 2 3 / \u221a \u03ba \u03ba, \u03b1 1 = c 3 /(c 3 + \u03b2), \u03b3 1 = \u03b2/(c 3 \u03bb min ) and \u03b4 1 = 1/R 2. The update equations involve a convex combination of the current running average and a short gradient step. In this paper, the algorithm parameters are set with c 3 = 0.7. The experimental setup and results for linear regression are presented. The expected error of algorithms like SGD, HB, NAG, and ASGD can be computed by tracking covariance matrices. The rate of decay of each algorithm is indicated by the largest magnitude of eigenvalues of corresponding matrices. The algorithm parameters are set with c3 = 0.7. The rate of decay of each algorithm is indicated by the largest magnitude of eigenvalues of corresponding matrices. The range of parameters explored for the results include varying the condition number \u03ba from {2^4, 2^5, ..., 2^28} for all optimization methods. Different sampling and parameter settings were used for NAG, HB, SGD, and ASGD to achieve convergence within the algorithm's range. The algorithm parameters were set with c3 = 0.7, and the range of parameters explored included varying the condition number \u03ba for all optimization methods. The chosen step parameters were 3\u03ba/2 for the Gaussian case and 2\u03ba/3 for the Discrete case. The rate of decay was computed using equation 1, and the results were presented in table 3 and Figure 7. Grid search was performed to select parameters that give the smallest \u03bb max, showing similar patterns to actual runs of SGD, HB, and NAG. The grid search was performed to select parameters for optimization methods, showing similar patterns to actual runs of SGD, HB, and NAG. Learning rates varied with batch sizes, and parameter ranges were explored extensively. The grid search was conducted to select parameters for optimization methods, with varying learning rates and momentum values for different algorithms. Weight decay of 0.0005 was used in all experiments on cifar-10 dataset. The grid search was conducted to select parameters for optimization methods, with varying learning rates and momentum values for different algorithms. Parameters were chosen based on running for 40 epochs and picking the grid search parameter that yields the smallest validation error. If the validation error does not decay by at least 1% every three passes over the data, the learning rate is cut by a constant factor. The grid search was conducted to select parameters for optimization methods, with varying learning rates and momentum values for different algorithms. Parameters were chosen based on running for 120 epochs and picking the grid search parameter that yields the smallest validation error. If the validation error does not decay by at least 0.2% every four passes over the data, the learning rate is adjusted by a constant factor. The grid search was conducted to select parameters for optimization methods, with varying learning rates and momentum values for different algorithms. Parameters were chosen based on running for 120 epochs and picking the grid search parameter that yields the smallest validation error. If the validation error does not decay by at least 0.2% every four passes over the data, the learning rate is adjusted by a constant factor. The learning rates and decay factors for SGD, NAG/HB, and ASGD were specified for comparison across algorithms. When comparing SGD with NAG or HB, parameters like learning rate and momentum are adjusted. Plots show training function values for different algorithms and batch sizes. NAG shows improvements over SGD and HB initially for batch size 128 but not for batch size 8. Towards the end of training, NAG rapidly decreases the training function value for both batch sizes. The reason for this is unclear, but it indicates overfitting to the data. ASGD is also compared with momentum methods in terms of training error plots. Comparison of ASGD with momentum methods: Training error plots for ASGD are compared to HB and NAG in Figures 9 and 10. Learning rate and decay schedule of ASGD are constrained to match HB and NAG, which were determined through grid search."
}