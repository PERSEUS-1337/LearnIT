{
    "title": "SkmiegW0b",
    "content": "We study building models that disentangle independent factors of variation using weakly labeled data. An autoencoder model is introduced and trained with constraints on image pairs and triplets. The existence of reference ambiguity in disentangling tasks with weakly labeled data is formally proven. The proposed model successfully transfers attributes in various datasets but also encounters cases of reference ambiguity. One way to simplify classification and regression tasks is by building an intermediate feature representation that separates attributes better than the input data. This disentangles factors of variation, allowing classifiers and regressors to focus on discriminating the attributes of interest. This approach can improve performance in tasks like image synthesis and attribute transfer between images. Supervised learning can be utilized when labeling is available. When labeling is possible, supervised learning can be used to solve tasks involving attributes between images. However, some attributes, like style, may not be easily quantifiable. Weak labeling, where only the changed attribute is known but not by how much, can be used without manual annotation. A model using weak labels can define similarities between feature subsets from two input images. Adversarial training of autoencoders can help disentangle attributes when only weak labels are available, improving upon previous methods. Our discriminator does not rely on class labels but uses image pairs as inputs, reducing the number of parameters. We address the shortcut problem where information is encoded in one part of the feature, ignoring the rest. Our method solves this issue and addresses the reference ambiguity inherent in disentangling tasks with weak labels. Experimental results show that this ambiguity is rare and only occurs in complex data. Autoencoders in BID1, BID7, and BID0 reconstruct input data using internal image representations. Variational autoencoders in Kingma & Welling (2014) use a generative model with latent variables. BID8 autoencoders are trained with transformed image input pairs, allowing the network to learn object presence and location. Autoencoders encourage latent representations to retain input information. Generative Adversarial Nets (GAN) like BIGAN and CoGAN learn to create realistic images through adversarial training with neural networks. BIGAN focuses on feature representation, while CoGAN learns joint distribution of multi-domain images without needing correspondences. InfoGan is also mentioned in the context of enforcing constraints on random variables. In recent methods, neural networks are used for disentangling features with varying levels of supervision. For example, in BID19, pose invariant features are learned using both identity and pose labels. BID22 utilizes autoencoders to generate different viewpoints of objects by separating object category and viewpoint factors. The encoder output in BID4 is split into class label and nuisance factors, with a penalty term in the objective function. Hierarchical Boltzmann Machines and Variational Fair Autoencoders are used for disentangling features in neural networks. Autoencoders can also be utilized for visual analogy, while GAN is used for disentangling intrinsic image factors without ground truth labeling. The work most related to this approach involves an autoencoder restoring an image by swapping parts of the internal image representation. The current method improves on previous approaches by using adversarial training with image pairs instead of triplets, eliminating the need for expensive labels like viewpoint alignment. Unlike other methods, this approach trains a single discriminator for all labels, avoiding parameter growth with label numbers. The focus is on designing and training two models to map data samples to partitioned feature subvectors. The approach improves on previous methods by using adversarial training with image pairs instead of triplets, eliminating the need for expensive labels like viewpoint alignment. It focuses on designing and training two models to map data samples to partitioned feature subvectors, with an encoder and decoder model. The encoder extracts features related to specific factors of variation, while the decoder maps these features back to images for advanced editing capabilities. The data model assumes observed data x is generated through an unknown invertible process f dependent on factors v and c. In training, pairs of images x1 and x2 with varying v but common c are used. The data model assumes x is generated by an unknown process f with factors v and c. Image pairs x1 and x2 with varying v but common c are used in training. Factors v and c are encoded as Nv and Nc by the encoder Enc. The goal is to find the inverse function f^-1 to separate and recover v and c from data x. The goal is to find N v and N c that satisfy feature disentangling properties for all v, c. The decoder maps features to images in an autoencoder setup. The ideal decoder should have data disentangling properties for image synthesis. An adversarial term is introduced for training disentangling without conditioning on the common factor. In the training procedure, an adversarial term is used to achieve disentangling without conditioning on the common factor. Two main challenges, the shortcut problem and reference ambiguity, are addressed. The objective function includes an autoencoder loss and an adversarial loss, implemented using neural networks. Sampling of independent factors is used in the terms, with images formed as pairs or triplets with shared or independent common factors. In this term, images x1 and x2 with common factor c1 are used in an autoencoder setup. The encoder reconstructs x1 and x2 using subvectors Nv(x1) and Nc(x2). An adversarial training approach is introduced with a generator (encoder-decoder pair) and discriminator (neural network) to distinguish between real and fake image pairs. The generator aims to fool the discriminator by creating x3\u22951 that resembles x2. The common factor is c1, and the varying factor is independent of v1. The decoder must utilize Nc(x1) as x3 does not contain information about c1. The objective function is the Composite loss, optimizing the weighted sum of two losses with \u03bb regulating their importance. The shortcut problem arises when the decoder ignores Nc, leading to incorrect factor transfer. Reducing the dimensionality of Nv can address this issue by preventing a complete representation of input images. The shortcut problem in factor transfer can be addressed by using adversarial training. When the global optimum of the composite loss is reached, the c factor is transferred to x3\u22951. The labeling of images based on shared c factors allows for feature disentangling in factor transfer. This is achieved by imposing Nc(x1) = Nc(x2) for pairs [x1, x2] \u223c px1,x2|c, leading to the definition of a function C(c) = Nc(x1) that only depends on c. The function C(c) only depends on c, ensuring that images with the same v but different c result in different features. A bijective function R c = C \u22121 satisfies property (2) for c, but other disentangling properties may not be guaranteed. A function g reproduces the data distribution by generating samples y 1 = g(v 1 , c) and y 2 = g(v 2 , c) with the same distribution as the data. Reference ambiguity occurs when a decoder reproduces the data without satisfying disentangling properties. If p v assigns the same probability value to different instances of v, encoders can reproduce the data distribution without satisfying disentangling. The encoder-decoder pair defines a data distribution identical to the training set, ensuring feature disentanglement. The feature disentanglement property is not satisfied in the data distribution identical to the training set. The data disentanglement property is also not met. This implies that all factors of variation cannot be provably disentangled from weakly labeled data, even with access to all data and known distributions. The function T(v, c) mirrors the mapping of v as the car type changes, with T(v, c) \u223c U[\u2212\u03c0, \u03c0]. An encoder reverses the azimuth ordering of cars, making it impossible to transfer viewpoints between systems. Convolutional neural networks are used for all models, with optimization of the composite loss written as DISPLAYFORM0. Regularization is added to the adversarial loss to ensure a minimum value for each logarithm. The neural network architecture used in the study includes an encoder and decoder based on DCGAN with modifications, and a simplified version of the VGG network for the discriminator. Both encoder and decoder utilize convolutional layers, activation functions, and normalization layers like batch normalization or instance normalization. Instance normalization (IN) in the study differs from batch normalization (BN) by computing mean and standard deviation across the spatial domain of the input, not along the batch dimension. IN consistently improves performance in tests on various datasets like MNIST, Sprites, and ShapeNet. Ablation studies on ShapeNet cars focused on feature dimensionality and the presence of the adversarial term (L AE + L GAN ). Reference ambiguity was not a significant issue in practice for most datasets, only observed in more complex data like ShapeNet chairs. The encoder (Enc), decoder (Dec), and discriminator (Dsc) are trained with input triplets using ShapeNet cars dataset. Approximately 3K car types were used for training and 300 for testing, with 80K images rendered from 24 viewpoints. Objects were normalized to fit in a 100 \u00d7 100 pixel bounding box in a 128 \u00d7 128 pixel image. t-SNE embeddings of N v features for different models are visualized in FIG2. The study compares different models with varying feature sizes for 2D data. The performance is evaluated using mean average precision for viewpoint prediction. Different normalization choices are also compared for nearest neighbor classification, with batch and instance normalization showing similar performance. In viewpoint classification, batch and instance normalization perform equally well, while no normalization is slightly worse. Instance normalization is better for car type classification. The MNIST dataset contains 60K images of handwritten digits for training and 10K for testing. The Sprites dataset has 672 animated character images for training, testing, and validation. The study focuses on changes in sprites' appearance, including body shape, gender, hair, armor, and weapons. The system is trained using image pairs of the same sprite without pose labels. The L AE + L GAN model is used with specific dimensions for ShapeNet chairs, resulting in 160K images in the dataset. Results on attribute transfer are compared with ShapeNet cars, showing reference ambiguity for chairs due to their higher complexity. In this paper, the challenges of disentangling factors of variation in images are studied, focusing on the shortcut problem and reference ambiguity. A novel training method using image triplets with autoencoders is introduced to address these challenges. The shortcut problem is controlled through adversarial training, allowing for the use of large feature dimensions. It is shown that reference ambiguity is inherent in disentangling tasks with weak labels. Training and transfer of factors of variation are demonstrated to be complex tasks. The learning algorithm may not guarantee training and transfer of factors of variation, but our model shows good generalization capabilities on various datasets."
}