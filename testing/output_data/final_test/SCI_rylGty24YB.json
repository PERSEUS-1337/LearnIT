{
    "title": "rylGty24YB",
    "content": "In a study comparing candidate models, an amortized variational inference framework called Any Parameter Encoder (APE) was introduced to efficiently estimate posteriors across models of the same architecture. APE extends the encoder neural network to take both data feature and model parameter vectors as input, reducing posterior inference to a single forward pass. Experimental results showed that APE yielded comparable posteriors to more expensive methods in less time, especially when the encoder architecture was designed in a model-aware fashion. Variable models like topic models, embedding models, and dynamical systems models are used for extracting insights from large datasets. Model checking and comparison are crucial steps in this process. Within-model comparison involves determining the best parameter vector to explain a dataset, typically by maximizing the data's marginal likelihood. This likelihood computation involves marginalizing over a hidden variable for latent variable models. The curr_chunk discusses the challenges of computing likelihood in models with hidden variables and introduces variational inference as a solution. It presents new VI tools for within-model comparisons in large datasets, emphasizing the importance of model insights and uncertainties. The need for within-model comparison is highlighted in practical modeling tasks, with a focus on topic modeling applications. In topic modeling applications, domain experts may suggest alternative parameters to improve interpretability. Algorithms propose data-driven transformations to escape local optima in latent variable models. Estimating approximate posteriors for new candidate parameters is essential but costly for large datasets. Our contribution is the Any Parameter Encoder (APE), which amortizes posterior inference across models and data. Inspired by scaling a single model to large datasets, APE uses an encoder neural network to generalize across models by feeding model parameters and data features as input. APE is applicable to models with continuous hidden variables and aims for fast yet accurate estimation of each example. The Any Parameter Encoder (APE) amortizes posterior inference across models and data by using an encoder neural network to generalize across models. It focuses on fast yet accurate estimation of local posteriors for a range of model parameters in topic models, such as the Logistic Normal topic model. The document discusses using variational inference to approximate the true posterior in a document-specific mixture of topics model. The process involves optimizing a simpler density to minimize KL divergence and solving an evidence lower bound optimization problem for each model parameter. This approach amortizes posterior inference across models and data, focusing on fast and accurate estimation of local posteriors in topic models. The document discusses using variational inference to approximate the true posterior in a document-specific mixture of topics model. The process involves optimizing a simpler density to minimize KL divergence and solving an evidence lower bound optimization problem for each model parameter. This approach amortizes posterior inference across models and data, focusing on fast and accurate estimation of local posteriors in topic models. The method involves iterative updates of gradient ascent for each example and using an encoder neural network to speed up inference. The goal is to enable rapid estimation of posteriors for many model parameters. Model Parameters. The Any Parameter Encoder enables rapid estimation of posteriors for various parameters by using a neural network to transform inputs into variational parameters. Unlike the Standard Encoder, it can generalize to many parameters. The design for the encoder architecture in topic models involves careful selection of a neural network architecture to produce accurate approximate posteriors. The Any Parameter Encoder allows for quick estimation of posteriors using a neural network. The architecture for the encoder in topic models is crucial for accurate approximations of posteriors. The proposed model-aware encoder architecture aims to improve training effectiveness by capturing structure in the generative model. The proposed model-aware encoder architecture aims to improve training effectiveness in topic models by using a non-linear function of \u03b8Txn. This specialized design has a smaller input dimension K compared to the naive approach, providing desirable inductive bias for mean and covariance estimates. Further work is needed to develop similar strategies for general latent variable models. Training the encoder requires a set of parameter vectors {\u03b8m} of interest. The proposed Any Parameter Encoder (APE) maximizes ELBO across multiple models using stochastic gradient ascent with the reparameterization trick. APE is compared to other inference methods on topic modeling tasks, using Logistic Normal for q. APE includes both naive and model-aware encoder architectures with MLPs of 2 layers and 100 units per layer. Three baselines are considered in Pyro and PyTorch. In Pyro and PyTorch, three baselines are compared for topic modeling tasks: Variational Inference (VI), Standard encoder VAEs, and Pyro's off-the-shelf implementation of Hamiltonian Monte Carlo with the No U-Turn Sampler (NUTS). Synthetic data experiments are conducted on a V = 100 vocabulary dataset with K = 20 true topics. The study compares different methods for topic modeling tasks using synthetic data. A warm-started run initialized via the Any-Parameter Encoder (APE) outperformed a randomly-initialized run in terms of speed and quality. APE results were closer to Variational Inference (VI) with less error. The dataset consisted of 500 documents with 50,000 possible model parameters sampled from a Dirichlet prior. The APE was trained on 25 million document-\u03b8 pairs and evaluated on unseen pairs from the same generative process, specifically focusing on product reviews. The study analyzed consumer product reviews using a model with 6,343 text documents, V = 3000 vocabulary terms, and K = 30 topics. Results showed that the model-aware input layer outperformed the naive encoder in terms of likelihood. The Any Parameter Encoder achieved quality close to non-amortized VI and NUTS with a significant speedup. It also provided a useful warm start initialization to VI for model comparison. The Any Parameter Encoder (APE) showed agreement with Variational Inference (VI) in model comparison, outperforming the Standard Encoder. APE produced posterior approximations nearly as good as VI but over 100 times faster. Future work includes simultaneous training of parameters and encoders, and handling Bayesian nonparametric models. The text discusses Variational Inference (VI) and encoder methods for optimization. VI involves maximizing an objective function using gradient ascent to learn variational parameters. Different learning rate parameters were tested, with the best one reported in Table 1. The Standard Encoder is compared to VI, with the Any Parameter Encoder (APE) showing faster performance. Future work includes training parameters and encoders simultaneously. The text discusses the use of a standard encoder with a temperature parameter for topic models, which speeds up training by tuning the posterior peakiness. The encoder has two hidden layers with 100 units each, chosen through hyperparameter sweeps. The model has a total of 24,721 trainable parameters on synthetic data and 316,781 on real data. For Hamiltonian Monte Carlo with the No U-Turn Sampler (NUTS), a step size of 1 is used and adapted during warmup. The method's lower posterior predictive log likelihood compared to VI is due to wider posteriors. Pyro implementation is slow, so NUTS sampler is warm-started using VI for rapid mixing. Different models are generated from a Dirichlet prior. Training is done in random batches with 500 documents and 50,000 topics. Topics represent \"toy bars\" inspired by Griffiths and Steyvers (2004). For training both APE and the Standard encoder on synthetic and real data, Adam with an exponential decay learning schedule is used. The initial learning rate is 0.01 with a decay rate of .8 every 50,000 steps. The hyperparameters were chosen based on a learning rate finder and training is done for 2 epochs with a batch size of 100. The standard VAE encoder is trained on a single model with parameters drawn from a symmetric Dirichlet prior with \u03b1 = 0.1. The encoder weights are updated using stochastic backpropagation while holding the decoder weights fixed. The encoder weights are updated using stochastic backpropagation for VAEs, including APE, where topics are part of the input. The encoder can take a model parameter vector for rapid inference across many models."
}