{
    "title": "Syl6tjAqKX",
    "content": "The prefrontal cortex (PFC) is a key part of the brain responsible for behavior. A novel modular architecture of neural networks with a Behavioral Module (BM) and end-to-end training strategy is proposed, inspired by PFC functionality. This approach efficiently learns behaviors and preferences, useful for user modeling and recommendation tasks. In experiments with video games, the method separates main task objectives and behaviors between different BMs, shows network extendability, and efficient transfer of newly learned BMs to unseen tasks. In this work, factors affecting task performance are grouped into Strategy and Behaviour categories. Deep Networks have focused on learning Strategy components for achieving defined goals, while Behaviour factors are not directly linked to goals. Examples include sentiment status and individual decision-making preferences. Incorporating Behavior Component in Deep Networks is crucial for mimicking human behavior in systems like dialog agents or social robots. Previous research has focused on Strategy components, but Behavior has been overlooked. Drawing inspiration from brain structure and visual cortex, this work aims to model Behavior alongside Strategy for more robust Human-Computer Interaction systems. Incorporating Behavior Component in Deep Networks is crucial for mimicking human behavior in systems like dialog agents or social robots. This work introduces a modular architecture with a Behavioural Module (BM) inspired by the prefrontal cortex (PFC) connectivity. The model training strategy is based on Reinforcement Learning to mimic human behavior formation process and unique personality development. The text discusses the incorporation of a Behavioral Module inspired by the prefrontal cortex in deep networks to mimic human behavior formation and unique personality development. It introduces positive rewards to encourage specific actions and behavioral patterns in trained agents, validated in Atari 2600 games. In the challenging domain of classic Atari 2600 games, an AI algorithm learns game playing by understanding input space, objectives, and solutions. The agent's behavior is represented by preferences over different sets of actions, with human satisfaction and correctness of output as rewards. Human behavior effects can be observed in various similar situations, making it easier to identify patterns in similar domains and problems. In the study of transferring behavior modules (BMs) across tasks, a novel modular architecture and learning method are proposed. The approach allows for zero-shot transfer of learned modules to new tasks, demonstrating effectiveness in video games domain. Recent research has focused on task separation in deep learning, with studies on the effects of hyper-parameters and modular networks using neuroevolution algorithms. This approach has shown improved performance compared to monolithic architectures. In 2016, BID4 proposed a coevolutionary algorithm for domain transfer problem to avoid training from scratch. It independently learns a pool of networks. In 2017, BID8 introduced PathNet for task-transfer in Atari2600 games, with a fixed size architecture and a genetic algorithm for learning active paths. FeUdal Networks, proposed by BID29, also uses a Modular design with Manager and Worker modules for Reinforcement Learning with sub-goals. FeUdal networks utilize Manager and Worker modules for learning abstract goals and primitive actions in environments with long-term credit assignment and sparse reward signals. BID0 introduced the Neural Module Network for Visual Question Answering, consisting of separate modules for different tasks. A similar dynamic architecture was applied to a robot manipulator task by BID7, achieving good performance on a zero-shot learning task. The Modular Neural Network was also used in Reinforcement Learning in a robotics environment. The Modular Neural Network was applied in a robotics environment for a Reinforcement Learning task. Each module was responsible for a separate sub-task, but could only be combined in a sequential manner. Unlike previous works, this approach focused on learning a behavior module for representing user sentiment states or preferences without affecting the main goals. This leads to high adaptability, expandability, removability of modules, and potential for transferring learned representations to unseen tasks. The modular network introduces a Behavior component into Deep Networks, separating behavior and main task functionalities. It consists of a Main Network for the main task, a replaceable Behavior Module for encoding agent behavior, and a Discriminator for transferring learned behaviors. The architecture includes Convolutional layers for sensory cortex, Fully-Connected layers for motor cortex, and Behavior Module for behavior representation. The deep Q-Network (DQN) with target network and memory replay is used to solve the main task. The network structure consists of convolutional and fully-connected layers with ReLU activation functions. The output represents expected future rewards for actions, following the Bellman equation. The separation of behavioral component from the main task functionality is achieved by designing a network with a replaceable and removable behavior module. The Behavioral Module (BM) in the network is designed to have no significant impact on the main task performance. It consists of two fc layers with ReLU and linear activations. The BM input is the output from the last convolutional layer of the main network, and its output is fed to the first fc layer of the main network. The network architecture follows PFC connectivity pathways. The forward pass equations involve network input, layer outputs, activation functions, and convolution operations. The Main Network includes layers from l1 to l5. The Behavioral Module (BM) in the network consists of two fc layers with ReLU and linear activations. It ensures minimal impact on the main task performance. The Main Network contains layers from l1 to l5. The introduction of BM does not require an additional loss function and the loss is directly incorporated into the main network loss (Lm). Rewards for desired behaviors of the agent are introduced, similar to the PE effect on human behavior formation process. Each preferred action played is rewarded with an extra action reward. The network modularization allows learned BMs to be transferred to different main networks without re-training. This facilitates knowledge sharing among models in various implementations and tasks. Two transfer approaches, fine-tuning and adversarial transfer, are considered in this work. In the adversarial transfer approach, a discriminator network enforces convolutional layers to learn features similar to the source task. The discriminator network has fully-connected layers with Relu and Softmax functions to classify output as from the source or target task. The weights update is formulated with parameters for the discriminator and convolutional layers. In this section, experiments focus on separating agent behavior from the main task and transferring learned behaviors across tasks. The proposed network's flexibility is demonstrated, including a zero-shot setting for behavior module transfer without additional training. Evaluation is done on Atari 2600 games due to their controlled environment and wide range of tasks available. The proposed network in the study is evaluated on four Atari 2600 games - Pong, Space Invaders, Demon Attack, and Breakout. Data pre-processing involves converting game frames to gray-scale images and resizing them to 84x84 pixels. Training the network involves using the standard Q-learning algorithm and evaluating the effect of zero-behavior during training. The proposed network in the study is evaluated on four Atari 2600 games using the standard Q-learning algorithm. The training involved a combined reward system and evaluation metrics focused on game play performance. The Average Main Score Ratio (AMSR) was used to compare the performance of the proposed modular network with the Vanilla DQN model. The proposed modular network's performance is evaluated using the Average Main Score Ratio (AMSR) metric, which compares it to the Vanilla DQN model. The Behavior Distance (BD) metric measures the network's ability to model desired behavior by calculating the Bhattacharyya distance between action distributions. The experiment demonstrates that behavior functionality can be learned independently from the main task through two-stage training. During the two-stage training process, the main network was initially trained with five behaviors in Stage 1, followed by the training of the remaining behaviors in Stage 2. The impact of action reward magnitude on training was studied, showing that increasing the reward above a certain value led to a decrease in the main task's performance. This highlights the importance of selecting appropriate reward magnitudes for optimal learning outcomes. During Stage 2, setting the action reward value to 0.5r was observed to impact the main reward and behavior pattern. The complexity of the BMs was also studied by varying the number of layers. Additionally, the effects of dropout, BM0, and different learning rates on the Behavior module were examined. Results showed that using 2 fully-connected layers improved the main task score, but adding more layers did not enhance performance. The final model trained with 2-layer BMs, 0.5 dropout layers, higher BM learning rate, BM0, and action reward r showed high main task scores. Removing BMs did not degrade performance, and the action reward magnitude directly impacted agent behavior. The effect of action reward magnitude correlates with agent preferences, aligning with human behavior formation. The model allows separation of strategy and behavior functionalities, with successful network expansion through fine-tuning and adversarial transfer approaches for zero-shot performance. The model allows separation of strategy and behavior functionalities, with successful network expansion through fine-tuning and adversarial transfer approaches for zero-shot performance. When tested on unseen Stage 2 BMs, even a simple zero-shot transfer of learned BMs resulted in good performance. Zero-shot transfer of separately learned BMs to unseen tasks showed slightly worse performance compared to separately trained models. The Modular Network architecture with Behavior Module allows for successful separation of Strategy and Behavior functionalities, enabling independent learning of new Behavior Modules. The Adversarial 0-shot transfer approach shows high potential for transferring learned BMs to unseen tasks without degrading performance. This property allows the model to work in a general setting, aligning with human behavior. In future work, the study will be extended to other domains like style transfer, chat bots, and recommendation systems. The focus will also be on improving module transfer quality and exploring key parameters in the Behavior Separation task."
}