{
    "title": "HJx7l309Fm",
    "content": "Reinforcement learning in multi-agent scenarios is crucial for real-world applications, presenting challenges beyond single-agent settings. An actor-critic algorithm is introduced for training decentralized policies in multi-agent environments, utilizing centrally computed critics with a shared attention mechanism. This mechanism selects relevant information for each agent at every timestep, enabling more effective and scalable learning in complex scenarios compared to recent approaches. The approach is versatile, applicable to cooperative and adversarial settings with individualized rewards, without assumptions about agent action spaces. This advancement in reinforcement learning has shown promising progress in various domains, such as Atari games, Go, and continuous environments. In multi-agent reinforcement learning, agents must learn dynamics of environment and other agents. Various approaches have been developed, including training agents independently to maximize individual rewards. This violates the assumption of stationary and Markovian environments in reinforcement learning. Recent work in multi-agent reinforcement learning attempts to combine the strengths of different approaches. A critic is centrally learned with information from all agents, while the actors receive. This addresses the challenges of dynamic and nonstationary environments caused by changing policies of other agents. Our approach extends prior works by centrally learning a critic with an attention mechanism, allowing agents to know which other agents to pay attention to in real-world environments. The proposed approach in multi-agent reinforcement learning includes an attention mechanism that dynamically selects which agents to attend to at each time point, improving performance in complex interactions. This approach extends prior works by centrally learning a critic with an attention mechanism, allowing agents to know which other agents to pay attention to in real-world environments. The approach in multi-agent reinforcement learning includes an attention mechanism that improves performance in complex interactions. It works well in various environments and tasks, surpassing prior work focused only on cooperative settings. The paper discusses related work, describes the approach, presents experimental studies, and plans to release the code after the reviewing period. Recent advancements in multi-agent reinforcement learning have seen the application of deep learning methods to MARL settings, allowing for learning in high-dimensional state spaces. However, challenges such as nonstationarity of the environment, lack of coordination in cooperative settings, and credit assignment with global rewards have been encountered. Some approaches have proposed actor-critic frameworks with centralized training and decentralized execution, while others have utilized attention in a fully centralized multi-agent setting. The actor-critic method proposed by BID20 addresses challenges in multi-agent learning by using separate centralized critics for each agent, reducing non-stationarity in environments. They introduce a centralized critic for cooperative settings with shared rewards, incorporating a \"counterfactual baseline\" for calculating the advantage function. The proposed approach focuses on marginalizing a single agent's actions while keeping others fixed, allowing for complex multi-agent credit assignment. Attention models have been successful in selecting contextual information in various fields like computer vision, natural language processing, and reinforcement learning. Another attention-based actor-critic algorithm for MARL has been proposed, which centralizes policies while keeping critics decentralized. This approach complements the previous method and combining both could improve performance in cases where centralized policies are preferred. Our proposed approach for MARL is more flexible and scalable than previous methods, allowing for training policies in environments with any reward setup and different action spaces for each agent. It utilizes a variance-reducing baseline and centralized critics that attend to relevant information for each agent. Starting with the necessary notation and basic building blocks, we describe our ideas within the framework of Markov Games BID19, a multi-agent extension of Markov Decision Processes. The text discusses multi-agent reinforcement learning (MARL) where each agent has a state transition function, reward function, and learns a policy to maximize expected returns. Policy gradient techniques are used to estimate the gradient of an agent's expected rewards. Actor-critic methods aim to reduce variance in policy gradient estimators by using a function approximation of expected returns. This involves learning a function to estimate discounted returns through temporal-difference learning. Recent approaches in maximum entropy reinforcement learning focus on learning a soft value function to prevent convergence to non-optimal deterministic policies. The policy gradient is modified to include an entropy term with a state-dependent baseline. The loss function for temporal-difference learning of the value function is revised with a new target. In the multi-agent setting, the approach involves learning critics for each agent by selectively considering other agents' actions to address credit assignment. In the multi-agent setting, the approach involves learning critics for each agent by selectively considering other agents' actions to address credit assignment. The attention mechanism functions in a manner similar to a differentiable key-value memory model. Each agent queries the other agents for information and incorporates it into the estimate of its value function. This paradigm doesn't assume temporal or spatial locality of inputs, unlike other approaches in natural language processing and computer vision fields. In the multi-agent setting, agents learn critics by considering other agents' actions for credit assignment. The attention mechanism compares embeddings using a query-key system and multiple attention heads, each contributing to agent i. The contributions from all heads are concatenated into a single vector. In the multi-agent setting, agents learn critics by considering other agents' actions for credit assignment. Each head can focus on a different mixture of agents, with shared weights for selectors, keys, and values. Critics are updated together to minimize a joint regression loss function in a multi-task regression problem. This method can be extended to include additional information beyond local observations and actions. In the multi-agent setting, critics are updated together to minimize a joint regression loss function. The action-value estimate for each agent considers observations and actions from all agents. Individual policies are updated using a gradient that includes a multi-agent baseline for calculating the advantage function. Unlike in the MADDPG algorithm, all actions are sampled from current policies to calculate the gradient estimate for each agent. In the multi-agent setting, an advantage function using a baseline can help solve the credit assignment problem by comparing specific actions to average actions for the agent. This function is more general and flexible, accommodating different action spaces and dynamic attention mechanisms. In the multi-agent setting, an advantage function with a baseline helps solve the credit assignment problem by comparing actions to average actions. This approach accommodates different action spaces and dynamic attention mechanisms, simplifying calculations for discrete policies and continuous policies. In the multi-agent setting, an advantage function with a baseline helps solve the credit assignment problem by comparing actions to average actions. This approach accommodates different action spaces and dynamic attention mechanisms, simplifying calculations for discrete policies and continuous policies. In the case of continuous policies, estimating the expectation in Equation 9 can be done by sampling actions from the policy and averaging their Q-values, but this requires multiple passes through the network. Two environments are constructed to test the capabilities of the MAAC approach and baselines, focusing on scalability as the number of agents grows. In a multi-agent setting, the approach of centralizing critics by concatenating all agents' observations and actions may not scale well. A cooperative environment, Cooperative Treasure Collection, with shared rewards was implemented to vary the number of agents. Experimental results validate this claim. Additionally, a Rover-Tower task environment was created to evaluate methods' ability to attend to relevant information, which can dynamically change during an episode. These environments were implemented in the multi-agent particle environment framework. The framework introduced by BID23 and extended by BID20 is useful for creating environments with complex agent interactions. Discrete action spaces are used to simplify the control problem, with agents able to move in different directions. In the Cooperative Treasure Collection environment, there are 8 agents - 6 treasure hunters and 2 treasure banks, each corresponding to a different color of treasure. The hunters collect treasure that randomly respawns upon collection. In the Cooperative Treasure Collection environment, agents collect treasures of different colors and deposit them in corresponding banks. Hunters receive rewards for collecting treasure and all agents receive rewards for depositing. The environment also includes a Rover-Tower scenario with 8 agents, where rovers and towers are paired and negatively rewarded based on distance to goals. In a Rover-Tower scenario, rovers are paired with towers and negatively rewarded based on distance to goals. Communication is limited to discrete signals from towers to rovers, unlike centralized policy approaches. Comparisons are made with MADDPG and COMA for centralized training of decentralized policies. The study compares MADDPG, BID20, and COMA for centralized training of decentralized policies, along with a single-agent RL approach, DDPG. Gumbel-Softmax reparametrization is used for learning in discrete action spaces. Soft actor critic is used for optimization, and variations like MADDPG+SAC and COMA+SAC are implemented. An ablated version with uniform attention is also considered. The study compares different centralized training methods for decentralized policies, including MADDPG, BID20, and COMA, along with a single-agent RL approach, DDPG. Various algorithms are compared using the same number of parameters and training seeds. Results show that MAAC is competitive with other methods, with uniform attention performing well in Cooperative Treasure Collection but not in Rover-Tower. MADDPG and MADDPG+SAC perform well in Rover-Tower but not in CTC, while COMA variants do not fare well in either environment. DDGP is considered a weaker baseline. In Cooperative Treasure Collection (CTC), rewards are shared across agents, allowing MAAC (uniform) and DDPG to perform well. However, in Rover-Tower, where rewards are tied to another agent's observations, dynamic attention is beneficial, explaining why MAAC (uniform) performs poorly in this environment. COMA uses a centralized network for predicting Q-values for all agents, which may perform best in environments with global rewards and similar action spaces. However, in environments with agents having differing roles and non-global rewards like Rover-Tower, COMA does not fare well. MADDPG is a strong method but may have low performance in environments with large observation spaces for all agents. Our approach MAAC outperforms MADDPG+SAC in scalability as the number of agents increases. MADDPG-like critics use information non-selectively, while our approach can learn to pay more attention to specific agents through the attention mechanism. Future research will focus on improving scalability by sharing policies among agents and performing attention on sub-groups. The Rover-Tower task involves many agents, but each agent only receives information about its paired agent. The proposed algorithm focuses on training decentralized policies in multi-agent settings by using attention to select relevant information for estimating critics. The approach aims to handle scenarios where agents interact with a large group of agents, modeling real-life situations where agents are organized in clusters or sub-societies. The algorithm shows promise in complex environments with multiple agents and different reward configurations. The algorithm focuses on training decentralized policies in multi-agent settings using attention to estimate critics. It shows promise in complex environments with multiple agents and different reward configurations. The training procedure involves parallel rollouts, updating critics and policies every 100 steps. The training procedure involves updating Q-function and policy objectives using minibatches from the replay buffer. Gradient descent is performed using Adam optimizer with a learning rate of 0.001. Parameters of the target critic and policies are updated towards the learned critic's parameters. A discount factor of 0.99 is used, and all networks are updated in parallel using a GPU. Target critics stabilize experience replay for off-policy reinforcement learning with neural network function approximators. The training procedure involves updating Q-function and policy objectives using minibatches from the replay buffer with a discount factor of 0.99. Networks use a hidden dimension of 128 and Leaky Rectified Linear Units. Soft Actor-Critic uses a temperature setting of 0.2. Attention critics typically have 4 attention heads. To compare to DDPG and MADDPG in environments with discrete action spaces, a modification is made to enable policies that output discrete communication messages. Using policies that produce differentiable samples through a Gumbel-Softmax distribution allows for training policies with the gradient of expected returns without the log derivative trick. The entropy of attention weights for each agent and attention head is examined to understand how attention evolves during training. Lower entropy indicates focus on specific agents, with each agent using a different combination of attention heads in Rover-Tower. The use of attention heads in Rover-Tower varies based on individualized rewards, with all agents using them similarly in Treasure Collection where rewards are shared. Visualizing attention weights shows rovers strongly attend to paired towers, with different combinations of heads used by each agent. The rover in the Treasure Collection Environment learns to attend to the tower it is paired with without explicit supervision. It can dynamically change its attention without affecting performance. A state-value function is added for continuous action spaces, and the model is tested in the Cooperative Navigation environment, showing results compared to MADDPG in TAB3. This task does not require attention as all agents are relevant to each other. Our approach in the Rover-Tower environment matches but does not surpass MADDPG performance. Attention weights do not harm performance in simple cases, and our method handles continuous action spaces well."
}