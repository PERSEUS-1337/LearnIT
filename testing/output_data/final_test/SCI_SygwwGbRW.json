{
    "title": "SygwwGbRW",
    "content": "We introduce a new memory architecture called semi-parametric topological memory (SPTM) for navigation in unseen environments. SPTM consists of a graph with nodes representing locations and a deep network for retrieving nodes based on observations. The graph stores connectivity, not metric information. SPTM is used in a navigation system where an agent can build a topological map of a maze in 5 minutes and navigate towards goals with high success rates. The SPTM agent outperforms the best baseline by three times in goal-directed navigation. Deep learning (DL) is utilized for efficient navigation in complex 3D environments, with approaches categorized into reactive, memory-based, and map-based strategies. Animals use specialized navigation strategies like landmark, route-based, and map-based navigation, rather than relying on metric representations. The paper proposes a semi-parametric topological memory (SPTM) architecture for navigation, inspired by landmark-based navigation in animals. SPTM consists of a non-parametric memory graph and a parametric deep network for retrieving nodes based on observations. The graph contains no metric relations, only connectivity information, and the agent builds it by appending observations and adding shortcut connections while exploring the environment. The agent builds a graph by appending observations and adding shortcut connections based on visual similarities. Network R retrieves nodes from the graph to localize the agent. A locomotion network L complements the memory for movement between nodes. The system is trained in a self-supervised manner and evaluated on goal-directed maze navigation in 3D environments. The agent is initialized in a new maze with a goal location, using only images for navigation. The proposed system for goal-directed maze navigation outperforms baseline approaches significantly. With 5 minutes of maze footage, the system builds an internal representation of the environment and achieves a higher success rate in navigation than the best-performing baseline by a factor of three. The concept of a cognitive map supporting navigation has been extensively studied in psychology. More information and implementation of the method can be found at https://sites.google.com/view/SPTM. The debate over the existence of cognitive maps in animals and humans continues, with some researchers suggesting metric maps while others argue for landmark-based navigation. Robotics typically use metric maps for navigation, while vision-based SLAM methods are relevant for high-quality mapping. Modern deep learning methods enable end-to-end learning of sensorimotor control, predicting control signals from high-dimensional sensory observations like images. Different approaches to navigation using deep learning vary in learning methods and memory representation. Reactive methods lack explicit memory and struggle in complex environments, while systems with LSTM or episodic memory can store information about the environment but may not efficiently navigate in new environments. The curr_chunk discusses the use of addressable memory for navigation in new environments, contrasting with LSTM-based systems. It highlights the generalization capabilities of the proposed memory structure and its applicability to large continuous state spaces. The approach differs from DL systems using map-like representations and instead focuses on building a 2D multi-scale metric map for efficient navigation. Our method utilizes a topological map for navigation in a continuous space without relying on external camera poses or ego-motion information. While metric maps dominate robotics, topological maps have a history in navigation. Landmark-based navigation systems and SLAM systems inspired by the hippocampus have been explored in research. In the context of deep learning, an agent interacts with an environment in discrete time steps by receiving observations and taking actions. The environment is a maze in a three-dimensional world, and the agent builds an internal representation during exploration before navigating towards goal locations. The agent in a maze environment uses a semi-parametric topological memory (SPTM) to store internal representations and navigate towards goal locations. SPTM consists of a memory graph and a deep network for retrieval, acting as a planning module based on current and goal observations. The planning module generates a waypoint observation for short-range navigation towards the goal. The retrieval network estimates similarity between observations and is trained in a self-supervised manner. The retrieval network is trained to predict the similarity between pairs of observations based on temporal proximity. Training data is generated from trajectories of a random agent exploring the environment, with pairs of observations labeled as close or distant based on time steps. A siamese architecture is used for the network. The network R uses a siamese architecture with a deep convolutional encoder based on ResNet-18 to generate a 512-dimensional embedding vector. The vectors are processed by a fully-connected network with a 2-way softmax. The network is trained with cross-entropy loss. A similarity threshold is used for creating shortcut connections in the graph, with two types of edges for spatial adjacency and loop closure. Visual shortcuts are only added if |i\u2212j| > \u2206T to avoid trivial edges, and sequences are matched to enhance the robustness of visual shortcuts. The process of finding waypoints involves localization, planning, and waypoint selection using SPTM. Localization is done by retrieving the agent's location based on nearest neighbors in the memory. The siamese architecture allows for efficient nearest neighbor queries. The process involves localization based on nearest neighbors in memory, with a modified approach for temporally consistent self-localization. This method improves performance and reduces search time. In the planning step, the shortest path on the graph between two nodes is found using Dijkstra's algorithm. In the planning step, Dijkstra's algorithm is used to find the shortest path between two nodes. A waypoint is selected along the path based on a robust adaptive strategy, considering reachability and a fixed similarity threshold. The network is trained to navigate towards target observations near the agent. The network L is trained to navigate towards target observations in the vicinity of the agent by mapping a pair of current and goal observations into action probabilities. It can produce actions deterministically or stochastically. The network is trained in a self-supervised manner based on trajectories of a randomly acting agent, generating training samples from these trajectories. The network is trained in a self-supervised manner based on trajectories of a randomly acting agent to learn the conditional action distribution P(a|o t, o t+k). Training data for k > 1 is noisy but provides a useful training signal for short-range navigation. The SPTM-based agent navigates towards a goal in a three-dimensional maze, producing a waypoint observation given the current agent's observation and the goal observation. The SPTM-based agent produces a waypoint observation by combining the current agent's observation and the goal observation. The locomotion network is then used to navigate towards the waypoint using inputs from the retrieval network and observations of the environment. The networks are based on ResNet-18, a high-capacity architecture enabled by self-supervised training. Training data is generated online by a random agent in the environment, maintaining a replay buffer of recent samples. The agent in the training environment maintains a replay buffer of recent samples and updates using the Adam optimizer with a learning rate of 0.0001. The networks R and L are trained for 1 and 2 million mini-batch iterations, respectively. Goal localization is performed once at the beginning of a navigation episode, and shortest paths to the goal are computed once at the start of navigation. Nearest-neighbor queries for agent self-localization are the remaining computationally expensive operations. The siamese architecture of the retrieval network allows precomputation of embedding vectors for observations in memory, reducing evaluation to a small fully-connected network during navigation. Experiments in a simulated 3D environment based on Doom BID23 show an SPTM agent navigating towards a goal in a maze. The proposed method is evaluated for goal-directed navigation in new environments, using different mazes for training, validation, and testing with randomized layouts and textures. The study created 400 versions of a single labyrinth layout with randomized goal placements and textures. They also developed 3 mazes for validation and 7 mazes for testing. Each maze has 4 goal locations marked by special objects. Validation mazes were used to tune parameters, while fixed parameters were used for testing mazes. The agent is given an exploration sequence of approximately 5 minutes in-simulation time when presented with a new maze. The study involved generating 400 maze layouts with randomized goals for testing. Each maze had 4 goal locations, and the agent attempted goal-directed navigation trials. Trials were considered successful if completed within 5,000 simulation steps or 2.4 minutes. Hyperparameters for the SPTM agent were set based on validation set evaluation. The method performs well for a range of hyperparameter values and is robust to temporal subsampling of the walkthrough sequence. The SPTM graph is built by subsampling the sequence by a factor of 4. The threshold for creating shortcuts in the graph is set as a percentile of pairwise distances, with 2000 shortcuts created. Visual shortcuts have a minimum distance of 5 and a smoothing window size of 10. Waypoint selection thresholds are set at 0.7 for local and 0.95 for reach, with minimum and maximum waypoint distances of 1 and 7. The proposed method is compared to state-of-the-art deep-learning-based navigation baselines in a realistic setting. The comparison excludes works that rely on ground-truth information such as depth maps or ego-motion. The SPTM agent's performance is evaluated based on the percentage of successfully completed navigation trials in 5,000 steps. The first baseline is a goal-agnostic agent trained on collecting invisible beacons in a maze using the A3C algorithm. The agent receives a reward for collecting a beacon and is trained for 5,000 simulation steps. The second baseline is a feedforward network trained on goal-directed navigation with input of current observation and goal image. The third baseline approach involves a network with LSTM memory for goal-directed navigation. The agent lacks memory and cannot utilize the exploration phase effectively. The network architecture is similar to the first baseline, but input includes the 4 most recent frames and the goal image. Training involves navigating the environment for 10,000 steps in exploration. The agent navigates the environment for 10,000 steps in exploration mode, then spends 5,000 steps in goal-directed navigation mode with a goal image. Memory cells are not reset between stages to allow the agent to learn and efficiently navigate. Results are shown in Table 1 and Figure 5 for completed navigation trials within 5,000 steps. The proposed SPTM agent outperforms baselines in all mazes, with an average success rate three times higher than the best-performing baseline. The agent reaches the goal faster than baselines, indicating the inefficiency of standard LSTM memory for storing long sequences. The duration of the walkthrough footage exceeds the capabilities of standard recurrent networks. The SPTM agent outperforms baselines in all mazes, with a higher success rate and faster goal achievement. Existing methods struggle with generalization to unseen environments, with the best-performing baseline being goal-agnostic. Generalization performance is challenging due to spurious correlations and the inefficiency of goal-directed baselines. The SPTM agent outperforms goal-directed baselines in maze environments by automatically creating shortcut connections, significantly reducing the average path length to the goal. The SPTM agent improves navigation in maze environments by creating shortcut connections, leading to shorter paths to the goal. Ablation study shows that removing vision-based shortcuts results in a significant decline in performance. The SPTM agent enhances navigation in maze environments by creating shortcut connections for shorter paths to the goal. Additional experiments include performance in validation environments, robustness to hyperparameter settings, and experiments with automated exploration. The proposed memory architecture consists of a topological graph and a deep network for efficient goal-reaching in unseen environments after minimal training. The current system aims to improve navigation in unseen environments by suggesting future work such as enhancing network performance, incorporating ego-motion information, addressing memory size issues, and exploring end-to-end training possibilities. The system aims to enhance navigation in new environments by integrating SPTM into an end-to-end trainable system. The retrieval and locomotion networks are based on ResNet-18 and process 160x120 pixel images. The networks use a ResNet implementation and have specific architectures for processing observations. The ResNet-18 encoder is followed by a fully-connected layer with 7 outputs corresponding to different actions. Training is implemented in Keras and Tensorflow, generating data online and using a replay buffer of size 10,000. Mini-batch iterations are performed using the Adam optimizer with a learning rate of 0.0001. The baselines are updated using the Adam optimizer BID24 with specific parameters. The architectures used are BID33 and BID31, consisting of convolutional and fully-connected layers. Grayscale frames are found to perform better than RGB frames. The baselines are trained for 80 million action steps, equivalent to 320 million simulation steps. The validation mazes layouts are shown in FIG0, with success rate plots on each maze in FIG1. Performance of SPTM agent with different hyperparameters is in Table S1. Robustness was tested by varying texture distribution and exploration sequence properties. The method was tested with various textures in a training maze, while using walkthrough sequences generated autonomously by baseline agents trained with reinforcement learning in simple mazes. The results are reported in TAB5. The method was tested with various textures in a training maze, using autonomously generated walkthrough sequences. The results in TAB5 show that despite some decrease in performance with automatically generated trajectories, the method still outperforms baselines significantly. Experiments substituting the retrieval and locomotion networks showed the importance of these networks in the method's performance. The experiment involved downsampling images, converting to grayscale, and computing cosine distances between them. Two variants were tested: with local contrast normalization and without. The per-pixel comparison baseline performed poorly, with visual shortcuts being catastrophically wrong. The experiment involved downsampling images, converting to grayscale, and computing cosine distances between them. Two variants were tested: with local contrast normalization and without. Visual shortcuts made with this technique were catastrophically wrong. Modifications to the method were necessary to use actions from the exploration sequence, including no shortcut connections in the graph, introducing opposites for actions, and incorporating random actions to prevent the agent from getting stuck. Taking 10% of random actions led to good results. The method involves localizing the goal and agent, moving along an exploration graph-line, and taking actions based on localized observations. Despite using recorded actions, the method performs worse due to lack of shortcut connections and inability to correct deviations from the exploration trajectory. The environment allows the agent to slide along walls at angles less than 90 degrees, preventing complete failure when deviating from the exploration path. Successful navigation trials can be viewed at the provided link. Hyperparameter effects and additional ablation studies are reported in tables for evaluation."
}