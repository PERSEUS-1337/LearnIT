{
    "title": "Sy3XxCx0Z",
    "content": "Modeling informal inference in natural language is challenging, but with large annotated data, training complex models like neural networks for natural language inference (NLI) has become feasible. This paper aims to enrich neural NLI models with external knowledge to improve performance on the Stanford Natural Language Inference (SNLI) dataset. Natural language inference (NLI) involves determining if a hypothesis can be inferred from a premise. Recent advances in NLI include larger annotated datasets like SNLI and MultiNLI, enabling the training of more complex inference models. Neural network models have shown state-of-the-art performance on these datasets. In this paper, the focus is on exploring whether external knowledge can enhance the performance of NLI models. The proposed neural network-based models aim to leverage external knowledge to improve inference accuracy, especially in cases where the machine struggles to distinguish relationships between concepts from the provided training data. In this paper, the focus is on exploring whether external knowledge can enhance the performance of NLI models. The proposed neural network-based models aim to leverage external knowledge to improve inference accuracy, especially in cases where the machine struggles to distinguish relationships between concepts from the provided training data. The models demonstrate that utilizing external knowledge in neural network models outperforms previously reported best models, particularly in tasks like NLI that can benefit from external knowledge. The study explores the use of external knowledge to enhance NLI models, showing that it improves inference accuracy, especially with limited training data. External semantic knowledge benefits local inference, co-attention, and aggregation. Previous work on NLI used small datasets and conventional methods, demonstrating the usefulness of external knowledge like WordNet and FrameNet. Recent large-scale datasets have enabled training more complex neural networks, falling into sentence encoding-based and inter-sentence attention-based approaches. The study discusses the use of external knowledge to enhance NLI models, improving inference accuracy with limited training data. Different approaches include sentence encoding-based models and inter-sentence attention-based models. Sentence encoding-based models use neural networks to encode sentences, while inter-sentence attention-based models focus on the local interaction between two sentences. In the context of enhancing NLI models, inter-sentence attention-based models utilize attention mechanisms to gather local inference information for classification. External knowledge is integrated with neural networks for the first time in NLI tasks, showing effectiveness in various NLP tasks. In this paper, a neural network with external knowledge improves accuracy on the SNLI benchmark to 88.6%. Using relation features from knowledge graphs like WordNet enhances natural language inference models. The use of WordNet to measure semantic relatedness in word pairs, including synonymy, antonymy, hypernymy, etc., is crucial for natural language inference models. Adding a new feature, same hypernym, significantly improves results. The relation features from WordNet are integrated into a neural inference model to capture external semantic knowledge. Key statistics of these features are reported in TAB1. The natural inference models presented include input encoding, knowledge enriched co-attention, local inference collection, and inference composition. External knowledge enhances co-attention calculation, local inference collection, and inference composition. Neural inference networks predict relationships between sentences by analyzing word sequences and predicting logic relationships. Embedding vectors are used to represent words, and a label is predicted to indicate the relationship between the sentences. The model includes input encoding, knowledge-enriched co-attention, local inference collection, and inference composition. The vocabulary size and E can be initialized with pre-trained word embeddings. The sentences are fed into encoders to obtain context-dependent hidden states. Bidirectional LSTMs are used as encoders, running forward and backward on the sequence. Hidden states are concatenated to represent each time step and its context. The calculation of hidden states for unidirectional LSTMs involves element-wise multiplication of vectors. Parameters are to be learned, and D is the dimension of the model. The LSTM utilizes gating functions for input vectors to generate hidden states. Soft-alignment of word pairs is achieved through a co-attention mechanism based on relation features. The co-attention calculation involves a non-linear or linear function, with a hyper-parameter tuned on the development set. Word pairs with semantic relationships are identified through various features. The LSTM uses gating functions to create hidden states from input vectors. Soft-alignment of word pairs is achieved through a co-attention mechanism, identifying semantic relationships based on various features. The co-attention matrix determines local relevance between premise and hypothesis, using normalized attention weight matrices. Matching tricks are used to calculate local inference relationships. The inference model introduces knowledge-enriched inference composition using BiLSTMs to determine the overall inference relationship between premise and hypothesis. BiLSTMs in the composition layer judge local inference relationships and distinguish crucial vectors for sentence-level inference. The output hidden vectors are converted to a fixed-length vector for classification. The inference model utilizes BiLSTMs for knowledge-enriched inference composition, converting hidden vectors to a fixed-length vector using pooling operations for classification. Weighted pooling based on external knowledge is proposed, along with mean and max pooling. The final prediction heavily relies on word pairs from external knowledge. The model includes a feed-forward neural network, concatenates pooling vectors, and uses a multilayer perceptron classifier with tanh activation and softmax output layer. Training is end-to-end. The model is trained end-to-end using cross-entropy loss and the SNLI dataset focuses on entailment, contradiction, and neutral relationships. WordNet 3.0 is used for semantic relation features, and lemmatization is done using Stanford CoreNLP. The code is released for replicability, and models are selected based on the development set. The models are selected based on the development set. Training details include 300D hidden states for LSTMs, word embeddings initialized with GloVe 840B, Adam optimization with a learning rate of 0.0004, mini-batch size of 32, dropout with a keep rate of 0.5, and various models achieving accuracy scores ranging from 80.6 to 86.3. The models achieved accuracy scores ranging from 80.6 to 89.1 on the SNLI dataset. Different neural networks were used as encoders, with the gated-Att BiLSTM BID9 model achieving an accuracy of 85.5%. The models in the first group use sentence-encoding based approaches, with BID5 employing LSTMs as encoders and achieving an accuracy of 80.6%. Many related works follow this framework, with performances listed in TAB2. BiLSTM BID9 achieves 85.5% accuracy, surpassing state-of-the-art sentence-encoding models. BID34 introduces a matching-LSTM for locally-aligned words, achieving 86.1% accuracy. ESIM BID8 was the previous state-of-the-art with 88.0% accuracy, while KIM, enriched with external knowledge, achieves 88.6% accuracy. The ensemble model also shows promising results. The ensemble model, averaging probability distributions from ten individual KIMs, achieves a high accuracy of 89.1%. Different ratios of training data were sampled to compare the importance of external knowledge. Adding external knowledge in various aspects improves accuracy, with the baseline ESIM performing poorly with limited training data. When utilizing external knowledge in different components (\"A\", \"I\", \"C\"), the accuracy of the model improves significantly. The most important role is played by collecting local inference information (\"I\"). Combining all three components results in the best accuracy of 72.6%. External knowledge helps most when there is limited training data, especially in co-attention and composition. The optimal \u03bb values for different training data sizes are 20 for 0.8%, 2 for 4%, 1 for 20%, and 0.2 for 100%. Using more external knowledge leads to better accuracies, especially with limited training data. The KIM model achieves state-of-the-art accuracy in natural language inference by incorporating external knowledge. The KIM model, equipped with external knowledge, achieves state-of-the-art accuracy on the SNLI dataset by incorporating co-attention, local inference, and composing inference components. This approach of infusing neural networks with external knowledge may benefit tasks beyond NLI, such as question answering and machine translation."
}