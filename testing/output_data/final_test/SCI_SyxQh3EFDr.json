{
    "title": "SyxQh3EFDr",
    "content": "Hierarchical label structures are common in machine learning tasks, with explicit or latent hierarchies. Current methods often use cross-entropy loss, assuming independence among class labels. A new training method, Hierarchical Complement Objective Training (HCOT), maximizes the probability of the ground truth class while neutralizing other class probabilities hierarchically. HCOT outperforms state-of-the-art models in image classification and semantic segmentation tasks on CIFAR100, Imagenet, and PASCAL-context datasets. In machine learning tasks, hierarchical label structures are common. HCOT outperforms state-of-the-art models in CIFAR100, Imagenet, and PASCAL-context datasets. It can be applied to tasks with latent label hierarchies, such as image classification with hierarchical categories or semantic segmentation. In deep learning, cross-entropy loss is commonly used for training models assuming independent prediction classes. However, in tasks with hierarchical label structures, this assumption hinders the utilization of the hierarchy. Challenges arise when dealing with tasks that exhibit latent label hierarchies, like semantic segmentation. Techniques are being developed to leverage hierarchical information in such cases. In this paper, new techniques are proposed to leverage label hierarchy information by introducing new training objectives. The approach penalizes incorrect classes at different granularity levels, allowing for better utilization of the label hierarchy during training. The concept of Complement Objective Training (COT) is introduced to incorporate hierarchy information effectively. Complement Objective Training (COT) incorporates label hierarchy information by maximizing the correct class probability and neutralizing incorrect classes. This training paradigm widens gaps between classes to better utilize the label hierarchy during training. Hierarchical Complement Objective Training (HCOT) introduces a novel complement objective called \"Hierarchical Complement Entropy\" to improve class probabilities by maximizing the predicted probability of the ground truth, neutralizing incorrect classes in the same coarse-level category, and penalizing others on different branches in the label hierarchy. Compared to cross-entropy and COT, HCOT leads to confident predictions for the ground-truth class. HCOT introduces Hierarchical Complement Entropy to improve class probabilities by maximizing the predicted probability of the ground truth and penalizing irrelevant classes. Experimental results show better performance compared to cross-entropy and COT, especially for coarse-level classes. HCOT introduces Hierarchical Complement Entropy to enhance model performance in predicting coarse-level classes and tasks with latent label hierarchy. It is the first paradigm to train deep neural models using a label hierarchy, leading to significant performance improvements. Explicit label hierarchy is common in many tasks, with prior works focusing on exploiting hierarchical structures for better performance. HCOT introduces Hierarchical Complement Entropy to enhance model performance by utilizing label hierarchy. Unlike HD-CNN and Blockout, which modify model architectures, HCOT focuses on designing a training objective for deep neural networks to capture hierarchical structures among class labels. The goal of semantic segmentation is to assign a semantic label to each pixel in an image. EncNet utilizes the semantic context of scenes to consider global information during training. The proposed Hierarchical Complement Objective Training (HCOT) introduces a new training paradigm for leveraging information in a label hierarchy. It defines a novel training objective, Hierarchical Complement Entropy (HCE), as the complement objective for HCOT. Complement Objective Training (COT) involves training a neural model with both a primary objective (e.g., cross-entropy) and a complement objective to maximize the predicted probability of the ground-truth class. The complement objective, such as complement entropy, is designed to neutralize predicted probabilities of incorrect classes, making the model more confident about the ground-truth class. It involves using the Shannon entropy function over the probability distribution of incorrect classes, defined by the softmax function. The proposed Hierarchical Complement Entropy (HCE) regulates probability masses in a hierarchical fashion, similar to complement entropy, but considers the generalization gap between predicted and true data distributions. It groups sibling classes belonging to the same parental class of the ground-truth class. The proposed Hierarchical Complement Entropy (HCE) regulates probability masses in a hierarchical fashion by considering the relationship between sibling classes within the same parental class of the ground-truth class. It imposes probability regulation based on the hierarchical structure of the labels, ensuring gaps between inner and outer hierarchies are addressed. The proposed Hierarchical Complement Entropy (HCE) enforces hierarchical structure in training by regulating probability masses between sibling classes. The loss function includes cross entropy and complement objective terms, optimized through Direct or Alternative optimization methods. HCOT is evaluated in image classification experiments. HCOT is evaluated on image classification using CIFAR-100 and ImageNet-2012 datasets. CIFAR-100 consists of 60k colored natural images divided into 100 classes, further grouped into 20 coarse classes with label hierarchy. Data augmentation techniques are applied during training, while testing images remain at 32x32 pixels. Experimental Setup for CIFAR-100 involves training models with specific settings like SGD optimizer, weight decay, and learning rate adjustments. WideResNet training follows similar settings with learning rate adjustments at specific epochs. No dropout is applied to any baseline models. Alternating training is used with primary and complement objectives. The method shows improvements in results. Our method, Hierarchical Complement Entropy (HCOT), outperforms state-of-the-art models like ResNet and SE-ResNet on CIFAR-100, reducing error rates significantly. SE-ResNet incorporates Squeeze-and-Excitation blocks to capture inter-dependencies between convolutional layers. Results in Table 1 show error rates for different models, with HCOT consistently performing better. Additionally, HCOT can be combined with techniques like Mixup and Cutout for further improvements. HCOT, a method that outperforms ResNet and SE-ResNet on CIFAR-100, can be combined with techniques like Mixup and Cutout to improve model performance. Experiments show that models trained with HCOT consistently outperform the baseline and models trained with COT. HCOT significantly improves performance on coarse-level labels, while also enhancing fine-level labels. This improvement is attributed to modeling label hierarchies, which is not considered in the baseline or COT. The HCOT method improves model performance on CIFAR-100 by modeling label hierarchies, leading to better generalization and performance compared to baseline models like SE-PreAct ResNet-18. The visualization of logits shows distinct clusters with clear boundaries, indicating better generalization. The ImageNet-2012 dataset is used for studying hierarchical labeling structures. Experimental Setup: The experimental setup includes random crops and horizontal flips during training, with 224 \u00d7 224 center crops for testing. The model follows a specific setup with 256 minibatch size, 90 training epochs, and a learning rate decay strategy. Results show significant improvements in error rates with 52 coarse categories in HCOT compared to COT and baseline models. HCOT showed improvements in error rates compared to COT and baseline models, especially with 52 coarse categories. Performance peaked at Nc = 52, but degraded at extremes (Nc = 1 or 1000). Semantic segmentation tasks contain latent information about label hierarchy. Hierarchically-Constrained Object Tracking (HCOT) utilizes latent label hierarchy information for more accurate semantic segmentation on the Pascal-Context dataset. The dataset includes dense semantic labels for training and testing on 4,998 and 5,105 images respectively, with 60 semantic labels created for segmentation. Experimental Setup: The study utilized EncNet as the baseline and incorporated Joint Pyramid Upsampling (JPU) to achieve state-of-the-art results in semantic segmentation. The model was trained for 80 epochs with SGD, using a batch size of 16 and a polynomial learning rate scheduling. Images were cropped to 480 \u00d7 480 for training. The study utilized EncNet as the baseline model for semantic segmentation, incorporating Joint Pyramid Upsampling (JPU) for improved results. Training was done for 80 epochs with SGD and a polynomial learning rate scheduling. Evaluation metrics included pixel accuracy (PixAcc) and mean Intersection of Union (mIoU). Results showed that HCOT outperformed the baseline model in terms of segmentation quality. HCOT outperforms baseline models like cross-entropy and EncNet+JPU in semantic segmentation by retaining latent label hierarchies, resulting in clearer segmentation without irrelevant semantics. Visualizations in Figure 3 demonstrate the superior performance of HCOT compared to cross-entropy on the PASCALContext dataset. Hierarchical Complement Objective Training (HCOT) is proposed to leverage label hierarchy in training for image classification and semantic segmentation tasks. HCOT neutralizes incorrect class probabilities at different levels of granularity, resulting in less fragmented segments with fewer noises. Experimental results show that models trained with HCOT outperform state-of-the-art methods. Future work includes extending HCOT to Natural Language Processing tasks with hierarchical information."
}