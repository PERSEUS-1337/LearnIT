{
    "title": "rkgbwsAcYm",
    "content": "Transfer learning through fine-tuning a pre-trained neural network with a large dataset like ImageNet can speed up training, but accuracy is often limited by the new task's small dataset size. To address this, regularization methods like SPAR have been explored. A new framework called DELTA focuses on preserving the outer layer outputs of the target network instead of constraining weights. DELTA aligns the outer layer outputs by selecting and constraining feature maps using attention learned in a supervised manner. The proposed method DELTA outperforms state-of-the-art algorithms like L2 and L2-SP in accuracy for new tasks. Deep learning practitioners often face over-fitting with limited training instances, but weight fine-tuning can help obtain high-quality models. Fine-tuning involves training a network with a large source dataset like ImageNet and then fine-tuning the weights with data from the target domain, serving as a form of transfer learning in deep learning. Weight fine-tuning is a transfer learning approach in deep learning where pretrained weights from a source dataset provide better initialization for the target task. In fine-tuning, lower convolution layer weights are fixed while upper layers are re-trained. Regularization methods like L2-SP aim to prevent over-fitting by incorporating the Euclid distance between target weights and the starting point (source network weights) in the loss function. L2-SP minimizes empirical loss in deep learning by reducing weight distance between networks, outperforming standard weight decay. However, this regularization method may not always yield optimal results for transfer learning due to potential catastrophic memory loss or suboptimal model constraints. Further research is needed to enhance current methods in deep transfer learning. Weight regularization for CNN is motivated by the idea that layers with similar weights should produce similar outputs. However, regulating model parameters directly may be excessive due to the complex structures of deep neural networks. The focus should be on regularizing the behavior of outer layer outputs rather than model parameters. This can improve generalization by aligning the behaviors of the outer layers of the target network with a pre-trained network. In Convolutional Neural Networks, the outer layer refers to a convolution layer. In this paper, a novel regularization approach DELTA is proposed to improve generalization in Convolutional Neural Networks. DELTA focuses on selecting discriminative features from outer layer outputs through a supervised attention mechanism. The goal is to address the challenge of measuring similarity between feature maps without understanding their semantics or representations. DELTA is a regularization approach that enhances generalization in Convolutional Neural Networks by re-weighting feature maps with a supervised attention mechanism. It characterizes the distance between networks using outer layer outputs and incorporates it as a regularization term in the loss function. The key insight is \"unactivated channel re-usage\" which identifies transferable channels and preserves them through regularization, while reusing untransferable channels. Extensive experiments show its effectiveness in deep transfer learning. DELTA is a regularization approach that outperformed existing deep transfer learning algorithms in weight similarity. It showed higher accuracy on various image classification datasets. The paper is organized into sections discussing related works, introducing the feature map based regularization method, presenting experimental results, and concluding the study. Transfer learning aims to transfer knowledge from a source task to a target task. Transfer learning involves transferring knowledge from a source task to a target task using deep neural networks. Different label spaces between tasks can lead to issues with feature transferability. Various studies have analyzed factors affecting deep transfer learning performance, such as the reuse of parameters and quantifying feature transferability. In transfer learning, Huh et al. analyzed features from an ImageNet pre-trained network for various computer vision tasks. Recent studies have proposed methods to improve transfer learning, including filter subset selection, sparse transfer, and parameter transfer. A key study in deep transfer learning is BID10, which explored regularization schemes to accelerate learning without overfitting. This paper's contributions include a focus on L2-norm regularization to enhance deep transfer learning performance. The key contributions of this paper include DELTA, which constrains the L2-norm of the difference between source and target network behaviors. It also incorporates a supervised attention mechanism in the regularization term. This work is related to knowledge distillation for model compression, focusing on teacher-student network training. Our work focuses on transferring knowledge between different tasks using a supervised transfer learning mechanism. We extend the concept of knowledge distillation to regularize network outputs and incorporate target task labels. Additionally, we use a supervised attention mechanism to regulate feature maps based on filter importance. This approach is related to continual learning and attention mechanisms for CNN models. Our methodology involves continual learning and attention mechanisms for CNN models. Deep convolutional networks have a large number of parameters that can lead to over-fitting. Regularization helps constrain parameters to reduce this risk. The optimization objective with regularization is to obtain a specific form. Regularization with a tuning parameter balances the trade-off between empirical loss and regularization loss in obtaining a specific form. Transfer learning can be used with a pre-trained network to accelerate training of a target network, but accuracy may be limited. Novel regularization techniques can further improve transfer learning. Regularization techniques can enhance transfer learning by constraining the differences between target and source networks, such as using the geometric distance between their parameters as regularization terms. This approach aims to improve the transfer learning process by minimizing divergence between the networks. In transfer learning, existing approaches use regularization to constrain parameter divergence between networks, but they do not consider network behavior with new data or leverage supervision from labeled data. Instead of bounding weight differences, this research focuses on regulating network behaviors to align layers between source and target networks. Behaviors are defined as layer outputs, which are semantically rich and discriminative. The DELTA method introduces a new regularizer to measure the distance between behaviors of target and source networks. This regularizer helps in reducing the transfer learning problem to a learning problem by characterizing the differences over the training dataset. It can regulate behavioral variances based on labeled samples in the dataset. DELTA introduces a regularizer to measure behavior differences between target and source networks, reducing transfer learning to a learning problem. It incorporates a parameter-based proximal term to accelerate optimization, balancing two terms with tuning parameters \u03b1 and \u03b2. The regularizer constrains the L2-norm of private parameters in \u03c9 to improve inner layer parameter consistency. DELTA adopts the SPAR strategy for optimization using w* as initialization, enhancing generalizability. The DELTA method introduces a regularizer to measure behavior differences between networks, improving generalizability and optimization. It considers the distance between outer layer outputs of the networks, characterizing them based on input and parameters using feature maps. The regularizer defines the behavioral difference between the feature maps using Euclidean distance. The DELTA method introduces a regularizer to measure behavior differences between networks by comparing feature maps generated by the networks. It calculates the Euclidean distance between the feature maps and aggregates the distances using non-negative weights. The aim is to pay more attention to the differences in behavior between the networks. The supervised attention method proposes weights for feature maps based on potential performance loss when removing features from the network. Convolution filters are defined with parameters in a four-dimensional tensor shape, containing multiple filters in a convolutional layer. The supervised attention method proposes weights for feature maps based on potential performance loss when removing features from the network. It evaluates the weight of a filter by measuring the performance reduction when the filter is disabled in the network. Removing filters with greater discrimination capacity leads to higher performance loss, so these channels are constrained more strictly. DELTA sets the weight of a channel based on the difference in empirical losses of the network with and without the channel.softmax is used to normalize the weights to ensure non-negativity. The supervised attention mechanism assigns weights to feature maps based on their discrimination power, with higher weights given to more discriminative filters. A baseline algorithm, L2-FE, is used to evaluate the performance of these weights on target datasets without modifying the outer layer parameters of the network. The DELTA method was evaluated on target datasets including Caltech 256, Stanford Dogs 120, and MIT Indoors 67 for different tasks. Caltech 256 dataset contains 256 object categories with varying numbers of training examples used for validation. The Stanford Dogs dataset has 120 breeds of dogs with 100 examples per category for fine-grained image categorization. MIT Indoors 67 has 67 indoor scene categories with 80 training images each. CUB-200-2011 has 11,788 images of 200 bird species with classification labels used for training. Food-101 is a dataset with 101 food categories and 101,000 images for fine-grained image categorization. The dataset is challenging due to noise in training images. The method was implemented using ResNet-101 and Inception-V3 as base networks, following a similar procedure to BID10. The last layer of the base network is replaced with random initialization before fine-tuning with the target dataset. For ResNet-101 and Inception-V3 models, input images are resized and normalized before data augmentation. A batch size of 64 is used with SGD optimization. The learning rate decreases during training, and five-fold cross-validation is employed to find optimal hyperparameters. DELTA is compared to baseline algorithms under the same conditions, with experiments repeated five times. In experiments comparing DELTA to baseline algorithms, DELTA shows faster convergence and smoother learning curves than L2-SP regularization. Results in TAB0 show DELTA outperforming L2-SP on some datasets. In experiments, fine-tuning with L2 normalization does not significantly outperform using the pre-trained model as a feature extractor. However, L2-SP outperforms naive methods without SPAR. The proposed attention mechanism shows greater benefits. Data augmentation is used to improve image classification, with a focus on maintaining the original aspect ratio and applying 10-crop testing. Experimental results in TAB1 show improved classification accuracy with additional data augmentation for L2, L2-SP, and DELTA methods. The DELTA method showed improved classification accuracy compared to L2 and L2-SP methods. An experiment was conducted to analyze how parameters of convolution filters change after fine-tuning using ResNet-101 on the Stanford Dogs 120 dataset. Filters were grouped into stages and the Euclidean distance between parameter vectors before and after fine-tuning was calculated, showing a significant difference. After fine-tuning using ResNet-101 on the Stanford Dogs 120 dataset, the DELTA method showed improved classification accuracy compared to L2 and L2-SP methods. A sharp difference in distance distributions was observed, with attention allowing for \"unactivated\" convolution filters to be reused for better image classification. The effect of \"unactivated channel re-usage\" was noted, with some filters being driven far away from the initial value. By attributing attention to the original image, high-contributing pixels in activated feature maps were identified. After fine-tuning ResNet-101 on Stanford Dogs 120 dataset, DELTA method improved classification accuracy over L2 and L2-SP methods. Attention allowed for reusing \"unactivated\" filters for better classification. Regularization of DELTA with attention showed improved concentration in activation maps. DELTA with attention showed improved concentration in activation maps, highlighting important regions around the head of animals. Statistical results on part locations of CUB-200-2011 datasets supported qualitative cases, with key points annotated for discriminating bird categories. DELTA outperformed other regularization methods in normalized activations on these key points. In this paper, a regularization technique called DELTA is studied, which transfers behaviors and semantics from a source network to a target one by constraining the difference between feature maps. DELTA focuses on discriminate features for bird recognition and uses attention models obtained through supervised learning. The optimization for regularization is accelerated using SPAR. Experimental results on real-world datasets show that DELTA outperforms other methods in activation concentration and normalized activations on key points. DELTA, a regularization technique, surpasses state-of-the-art transfer learning methods in experiments based on convolutional neural networks."
}