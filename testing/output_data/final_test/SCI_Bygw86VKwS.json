{
    "title": "Bygw86VKwS",
    "content": "There is a recent surge in interest in safe and robust techniques in reinforcement learning (RL). The paper proposes a novel approach to fault-tolerance in reinforcement learning (RL) to address systemic failures and safety thresholds. It introduces a controller that learns policies to cope with adversarial attacks and system failures, demonstrating fault-tolerant control. The solution is represented by a variant of SGs, with an optimal controller behavior characterized. A value function approximation algorithm is introduced for convergence in unknown environments. Reinforcement learning (RL) enables adaptive agents to learn in real-world settings, such as factory industrial applications, traffic light control, robotics, and autonomous vehicles. These systems are prone to failures like actuator faults and sensor failures. Preprogrammed behaviors may not always be effective in handling such unexpected events. Reinforcement learning (RL) in real-world settings can be inadequate for ensuring safe task execution. Deploying RL agents introduces risks of catastrophic outcomes in unseen conditions. Focus has been on producing RL algorithms with safety guarantees, robust control, and risk minimization. Methods like H \u221e control, coherent risk, and conditional value at risk are being explored. In the context of ensuring safe task execution in real-world settings with reinforcement learning (RL), methods like H \u221e control, coherent risk, and conditional value at risk are being explored to introduce safety guarantees, robust control, and risk minimization. Recent focus has been on training RL agents to cope with random failures, abrupt system subcomponent failures, and states where failures occur. The objective is to produce RL policies that act cautiously and account for uncertainty and pessimistic views of the world. In this paper, a method is introduced to learn optimal policies in response to random and adversarial attacks that cause system failures. An adversary is introduced to determine a stopping criterion that leads to the worst outcomes for the controller. By using a game-theoretic construction, a policy robust against adversarial attacks can be learned by an adaptive agent through reinforcement learning. This approach generates experiences for the agent to learn a best-response policy against such scenarios in a two-player stochastic game. The paper introduces a novel two-player stochastic game where a controller learns to modify system dynamics to maximize payoff while an adversary tries to stop the system to increase costs. This framework aims to find optimal policies robust against stoppages during critical times. The main contribution is a systematic treatment of robust control under worst-case failures, including a formal analysis of the game between the controller and stopper, with results centered around a minimax proof establishing the game's value. The paper discusses a novel two-player stochastic game focusing on fault-tolerance and optimal stopping problems. It introduces a minimax proof approach different from standard game theory methods and characterizes the adversary optimal stopping rule. The framework addresses optimal policies under worst-case transitions and establishes the game's value through theoretical results. The paper introduces a novel two-player stochastic game focusing on fault-tolerance and optimal stopping problems. It proves the existence of a unique fixed point solution for the game's Bellman operator and characterizes the optimal stopping time. An equivalence between the game of control and stopping and worst-case OSPs is established, showing that the fixed point solution of the game solves the OSP. An approximate dynamic programming method is developed to compute optimal controls in settings where system dynamics and reward functions are unknown. The method allows the agent to observe realized rewards by interacting with the environment. Our method introduces an adversary simulating high-cost stoppages to induce an FT trained policy, inspired by a two-player optimal stopping game framework. The game involves one stopper and one controller, requiring a novel construction using open sets for the minimax result. This study combines control and optimal stopping in a unique way. The study of optimal control combining control and stopping is limited to a few studies. Games of control and stopping have been analyzed in continuous-time, requiring computing analytic solutions to non-linear partial differential equations. Current iterative methods in unknown environments are restricted to risk-neutral settings, introducing a notion of risk adds considerable difficulty. The paper introduces a framework for solving optimal stopping problems with worst-case transitions in unknown environments, incorporating a notion of risk. It provides a formal description of the problem, introduces a game framework, and presents theoretical analysis and an approximate dynamic programming approach for computing optimal controls. The main problem addressed is FT RL, focusing on finding a control policy for abrupt system stoppages and failures at worst states. A fictitious adversary determines when to stop the process, leading to an FT best-response control. The agent's actions influence the system's state sequence. The agent's actions influence the system's state sequence, with rewards dependent on the state and chosen action. The agent's policy is a map from states to actions, drawn from a compact set. The problem has a finite horizon, but may stop randomly before reaching it. If stopped, the agent incurs a cost and the process terminates. The agent's performance function is determined by the transition function. The FT control problem involves a reward function, a bequest function, and a discount factor. The controller must address abrupt system failures and stoppages, especially in scenarios like adversarial attacks or high-cost state stoppages. This is relevant in real-world situations such as engine failures in autonomous vehicles and network power failures. In autonomous vehicles, network power failures and digital network attacks are key concerns. The controller aims to avoid system states leading to high likelihood of subcomponent failure, such as avoiding tasks increasing the risk of component failure or breakages in robotics. To create robust control, a stopping rule is needed to halt the system at states with highest costs, inducing a response that is resilient against systemic faults. This requires a formalism combining an optimal stopping rule and reinforcement learning. The problem considered involves finding a stopping rule to minimize J and an optimal policy to maximize J in a setting where players have limited knowledge of the system. The controller aims to manipulate state visitations to maximize J, while an adversarial stopper chooses a stopping rule to minimize J. This scenario is modeled as a Stackelberg game between the controller and stopper. The paper addresses OSPs under worst-case transitions, aiming to find a stopping rule under adverse expectations. An example is given with actuator failure in RL applications, where an adaptive learner must adjust actions based on operability of actuators. This introduces risk and the need for the agent to operate with a subset of actuators. The agent's actions are restricted to a subset of its actuators due to potential failures, leading to the need for a stopping rule and control policy to maximize overall payoff. The controller learns to execute a policy that is robust against actuator failure by maximizing overall payoff through a stopping action. The interdependence between players' actions is addressed using an SG framework in a stochastic system. The problem involves a filtration over a sample space with a time horizon of K rounds. A stopping time is a random variable that determines when a stopping criterion has been met. An SG is a Markov decision process where two players manipulate system transitions over rounds to receive rewards or costs. In a two-player zero-sum stochastic game (SG), players receive rewards or costs based on their joint actions in each round. The game is represented by a 6-tuple S, A i\u2208{1,2}, P, R, \u03b3 where players aim to maximize their expected cumulative return. In a two-player zero-sum stochastic game, players aim to maximize their expected cumulative return by using strategies that depend only on the current state and round. An equilibrium exists in Markov strategies even when the opponent can draw from non-Markovian strategies. The players' actions are drawn from the same set in usual cases, but we consider a game where player II can choose from a different set of actions. In a two-player zero-sum stochastic game, players aim to maximize their expected cumulative return using strategies that depend on the current state and round. Player II can choose a strategy to stop the process at specific times from a set of stopping times. The value of the game, denoted by J, represents the minimum payoff each player can guarantee under the equilibrium strategy. If J exists, it constitutes a subgame perfect equilibrium where neither player can improve their payoff by changing their strategy. The central task is to establish an equilibrium in a two-player zero-sum stochastic game by proving the existence of a value J. This value represents the maximum payoff the controller can expect in an environment subject to adversarial attacks. Introducing a stopping criterion requires an alternative analysis to determine an equilibrium with Markov strategies and characterize the stopping criterion. The analysis focuses on introducing a stopping criterion in stochastic games, altering standard methods. An approximate dynamic programming method is developed to compute the value function through simulation. A simulation-based scheme is proposed for unknown transition models and reward functions. An equivalence between robust OSPs and games of control and stopping is constructed. The results expand the theory of risk in reinforcement learning to address catastrophic system states in games of control and stopping. In this paper, the theory of SGs is developed to cover games of control and stopping in unknown environments. An equivalence between robust OSPs and games of control and stopping is proven, with a focus on the Bellman operator as a contraction mapping. The operator T is introduced to break down the game into sub minimax problems, crucial for establishing a value iterative method. The value iterative method for computing game values involves player strategies in two-player games. Markov controls focus on the current state and game duration, limiting optimal player performance. Theorem 1 establishes the existence of the game value in Markov controls by commuting max and min operators. Theorem 1 establishes the existence of an equilibrium pair (\u03c4 ,\u03c0) \u2208 V \u00d7 \u03a0, where \u03c0 is the controller's optimal Markov policy against adversarial attacks. The computation of the game value, denoted by J, is the subject of the next section. Best-response strategies are defined to characterize the equilibrium, and the game value can be computed by repeatedly applying T, converging to a fixed point property. The game has a fixed point property where a unique function J \u2208 L 2 exists. The existence of a fixed point of T is established in Theorem 2, coinciding with the game value. An iterative scheme for computing J is developed in Sec. 4. An SPE defines a strategic configuration where both players play their best response strategies. In the context of the FT RL problem, an SPE describes optimal responses against stoppages that incur high costs. Proposition 1 states that (\u03c4 ,\u03c0) \u2208 V \u00d7 \u03a0 consists of best response strategies. The existence and characterization of the optimal stopping time for player II is established in Theorem 3, which induces an FT control by the controller. The equivalence between the robust OSP and the SG of control and stopping is shown in Theorem 4, highlighting the relationship between different methods. The method presented in the curr_chunk offers a simulation-based value-iterative scheme to solve the robust OSP problem. It converges to the value of the game, allowing for extraction of optimal controls in environments with unknown transition models and reward functions. The iterative method tunes weights of basis functions to approximate the value function through simulated system trajectories. The algorithm uses basis functions to approximate the value function through simulated system trajectories. It converges to an approximation of J and provides a bound for the approximation error based on the basis choice. The function Q is approximated by the algorithm, which serves as a generalised Q-learning algorithm for zero-sum controller stopper games. The algorithm approximates the function Q, which in turn approximates the value J. It generates weights that converge to a vector r, and \u03a6r approximates Q. A bound is provided for the game outcome using controls from the algorithm. The main results include the convergence of weights r n to r, and approximation bounds using the projection \u03a0. The error bound in J approximation depends on the projection quality, enabling the solution of the FT RL problem through behavior simulation. The paper addresses fault-tolerance in reinforcement learning by simulating environment behavior and using an update rule to approximate the value function. It establishes an equilibrium value in a discrete-time SG and proposes an approximate dynamic programming algorithm for solving the game through simulation. The paper discusses fault-tolerance in reinforcement learning through simulation and an update rule to approximate the value function in a discrete-time SG. It introduces assumptions and definitions for the analysis, including ergodicity and Markovian transition dynamics. The main results are proven using preliminary lemmata and definitions related to contraction operators and residuals. The paper introduces assumptions and definitions for fault-tolerance in reinforcement learning, including ergodicity and Markovian transition dynamics. It discusses the use of contraction operators and residuals to prove main results related to the value function approximation in a discrete-time SG. The paper includes lemmata and propositions to support the analysis. The paper introduces assumptions and definitions for fault-tolerance in reinforcement learning, including ergodicity and Markovian transition dynamics. It discusses the use of contraction operators and residuals to prove main results related to the value function approximation in a discrete-time SG. Lemmata and propositions are included to support the analysis. Lemma A.7 states that if J 2 are fixed points w.r.t T 1 and T 2 respectively, then there exist constants c 1 and c 2 such that T satisfies a constant shift property. The proof of Lemma A.1 involves inequalities and properties of the max operator. Lemmata A.2, A.3, and A.4 are given without proof but can be easily verified. The proof of Proposition A.1 involves applying Lemmata A.2 and A.3. Lemma A.5 is proven using the triangle inequality, while Lemma A.6 follows directly from Lemma A.5. Lemma A.7's proof shows that for any s in S, if J \u2264 J then... The proof involves showing that for any s in S, if J \u2264 J, then certain inequalities hold. By applying various observations and properties, the desired results are obtained. After taking expectations and using Jensen's inequality, the value of the game with horizon n can be obtained from n iterations of dynamic recursion. The analysis incorporates important departures to accommodate the actions of two antagonistic players. The proof of Theorem 5 involves constructing \u03c4 sensibly and showing certain inequalities hold for any s in S when J \u2264 J. The proof involves showing that the quantity Q is a fixed point of F, allowing for the approximation of the value. Additionally, it is proven that the operator \u03a0F is a contraction on Q. The result is established using the orthogonality of the projection and the Pythagorean theorem. Jensen's inequality, stationarity, and the non-expansive property of P are used to derive the final result. The proof involves showing that the quantity Q is a fixed point of F, allowing for the approximation of the value. By using the triangle inequality and fixed point properties, a bound on the term Q \u2212Q is placed. The result is then obtained by substituting the result of step 4. The quantity s k can be described in terms of an inner product, enabling the use of classic arguments for approximate dynamic programming. Theorem 6 follows directly from Theorem 2 with a minor adjustment in substituting the max operator with min."
}