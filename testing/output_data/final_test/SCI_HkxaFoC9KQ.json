{
    "title": "HkxaFoC9KQ",
    "content": "Our approach enhances deep reinforcement learning agents with relational reasoning, improving performance, learning efficiency, and interpretability. By encoding images as vectors and using message-passing, our agent excelled in StarCraft II mini-games and navigation tasks, surpassing human grandmaster-level in some games. It demonstrated superior generalization and reflected important problem structures in its learned representations. The main contribution of this work is introducing techniques for representing and reasoning about states in model-free deep reinforcement learning agents via relational inductive biases. Experiments show advantages in efficiency, generalization, and interpretability, scaling up to meet challenging test environments in artificial intelligence. Recent deep RL systems have shown remarkable performance but suffer from low sample efficiency and poor transfer beyond training environments. Various structured approaches to RL have been explored to address these limitations. Various structured approaches to RL have attempted to incorporate entity-based and symbolic representations, but struggle with learning rich representations and are limited to simple tasks. Recent approaches have explored using relational inductive biases in deep learning, such as \"graph networks,\" to strike a balance between flexibility and efficiency. These methods explicitly represent entities and their relations using sets and graphs, performing relational reasoning through learned message-passing and attention mechanisms. Our approach introduces relational inductive biases for entity-and relation-centric state representations and iterated relational reasoning into a deep RL agent. It does not rely on prior knowledge of the problem structure and is agnostic to specific relations. Our architecture utilizes a convolutional front-end for processing raw visual input data and a self-attention mechanism for relational reasoning. The deep RL agent is based on an off-policy advantage actor-critic method, achieving state-of-the-art performance on challenging tasks, including surpassing grandmaster level on four StarCraft II mini-games. The architecture utilizes a convolutional front-end for processing visual input and a self-attention mechanism for relational reasoning. The agent surpasses grandmaster level on four mini-games and introduces a new task called \"Box-World\" to stress planning and reasoning. The agent efficiently reaches higher performance levels and generalizes to solve more complex problems than trained on. The intermediate representations in relational computations are interpretable, indicating a rich understanding of problem structure. The agent uses a deep RL algorithm based on a distributed advantage actor-critic method. The agent's architecture includes a convolutional front-end for visual input and a self-attention mechanism for relational reasoning. It achieves grandmaster level on mini-games and introduces the \"Box-World\" task for planning and reasoning. The agent uses a deep RL algorithm with an actor-critic method to optimize policy and value estimation. The agent's architecture also includes a multi-head dot-product attention mechanism for processing entities and updating state values. The agent's architecture transforms image input into entity vectors for non-local computation, preserving spatial information by concatenating spatial coordinates onto feature vectors. The architecture transforms image input into entity vectors for non-local computation by computing pairwise interactions between entities using self-attention. The implementation is based on BID23's multi-head dotproduct attention (MHDPA), which projects entities into matrices of query, key, and value vectors. The similarities between query and keys are computed by a dot-product, normalized into attention weights, and used to compute pairwise interaction terms. This is efficiently computed using matrix multiplications and passed through a multilayer perceptron (MLP) with a final stage applied. The implementation involves a non-linear transformation to compute complex relationships between entities. A one-step relational update process, termed a \"block\", can be iteratively applied using shared or unshared parameters to compute higher order interactions among entities. The stack of multiple relational blocks is referred to as the relational module, with the final output used to compute \u03c0 and B. The output matrix is reduced to a vector by max-pooling, passed through a small MLP, and split into a vector of \u03c0's. The curr_chunk describes a navigation environment called Box-World 1, designed for abstract relational reasoning and planning. It consists of a 12x12 pixel room with keys and boxes, where an agent can move in four directions to pick up keys. The agent's actions are determined by a (c + 1)-dimensional vector split into a c-dimensional vector of logits for actions and a baseline value estimate, B. The logits are normalized for probability parameters in a multinomial distribution. The Box-World 1 environment involves navigating a 12x12 pixel room to collect keys and unlock boxes. Keys match box locks by color, allowing access to box contents. The goal is to acquire a gem represented by a white pixel. Keys obtained can unlock other boxes. The Box-World 1 environment involves navigating a 12x12 pixel room to collect keys and unlock boxes. Keys match box locks by color, allowing access to box contents. In each level, there is a unique sequence of boxes that need to be opened in order to reach the gem. The difficulty of the level is determined by the number of boxes in the path to the goal, the number of distractor branches, and the length of the distractor branches. The task is computationally difficult because keys can only be used once, requiring the agent to reason about whether a box is along a distractor branch or the solution path. Keys and boxes appear in random locations in the room, demanding the capacity to reason about them based on their abstract properties. The agent in the Box-World environment follows a reinforcement learning setup with a relational module. Training involved levels with solution lengths of 1 to 4, with distractor branches randomly sampled from 0 to 4. Two task variants were used: one with distractor branches of length 1 and another with length 3. Training details can be found in the Appendix. In the Box-World environment, agents with a relational module achieved close to optimal performance in tasks with distractor branches of length 1 and 3. The number of relational blocks required varied based on the length of distractor branches, with more blocks enabling higher-order relational computations. Baseline agents relying on convolutional and fully-connected layers performed significantly worse in comparison. Similar results were observed with alternative RL algorithms. In Box-World, agents using Q-learning with prioritized experience replay took longer to train. Backward branching in the level generation graph allowed for simpler strategies, while forward branching required more complex planning. Baseline agents performed better in backward branching scenarios. The analysis revealed that weak relational reasoning capacity led to poor performance in forward branching scenes. Attention weights showed entities focusing on relevant objects along the solution path, such as keys unlocking locks. The attention analysis showed that keys attend to the locks they can unlock, locks attend to the keys that can unlock them, and all objects focus on the avatar's location. This suggests the importance of object-avatar relations in measuring relative position and supporting navigation. The model was tested on levels requiring longer sequences of boxes and new key-lock combinations without further training. The function used to compute weights should be able to generalize to unseen key-lock combinations. The attention analysis revealed the importance of object-avatar relations in navigation. The relational module showed superior performance in solving complex and new levels in Box-World, achieving high success rates in zero-shot transfer tasks. This highlights the module's ability to handle previously unseen problems effectively. StarCraft II is a challenging video game for RL agents due to its multiagent nature, large state and action space, and delayed consequences of actions. Agents were trained on 7 mini-games in SC2LE to simulate the mechanics of the full game. The 7 mini-games in SC2LE simulate the mechanics of the full game, testing agents with different strategies and skills like precision unit control, focus-firing, and army splitting for optimal performance. In StarCraft II (SC2), a strong control baseline was constructed to test against a relational agent. The model capacity was increased by using 2 residual convolutional blocks and a 2D-ConvLSTM to account for partial observability and memory of past actions in the SC2 environment. This was crucial as SC2 actions are carried out over multiple timesteps, requiring the agent to remember previous actions to avoid repetition. The relational module in the control agent was replaced by residual convolutional blocks. The agent's output includes non-spatial arguments (Args) and spatial arguments (Args x,y) as modifiers of actions. The relational-agent achieved high scores in StarCraft II mini-games, particularly excelling in the Defeat Zerglings and Banelings task. The relational agent with iterative blocks of attention was crucial for the strategy of splitting the army and kiting enemy units in the Defeat Zerglings and Banelings task. Using the full action set provided by SC2LE, the agent achieved high scores across all mini-games, outperforming previous models. The agent augmented with a relational module achieved state-of-the-art results in six mini-games and surpassed the human grandmaster in four of them. The relational agent with iterative blocks of attention improved performance in various mini-games compared to the control agent. Factors contributing to this improvement include a better RL algorithm, robust architecture, hyperparameter tuning, and longer training. The relational agent showed significant gains in Collect Mineral Shards, Defeat Roaches, and Defeat Zerglings and Banelings tasks. In Box-World, understanding relational structure leads to better generalization. Agents trained on Collect Mineral Shards were tested on modified levels with more marines. The agents had no prior exposure to these experiments. Increasing the number of marines only affects unit deployment strategy. The number of marines available should only affect unit deployment strategy, not model performance. Medium-sized networks show interesting generalization capabilities, with the best seeds of the relational agent achieving better scores. However, there is high variability in results, especially with larger models. Qualitative analysis shows distinct behaviors between the best performing control and relational agents. Further research is needed to confirm generalization capabilities. Further research is needed to confirm the generalization capabilities of a relational agent in complex domains like StarCraft II. By incorporating structured perception and relational reasoning into deep RL architectures, agents can learn interpretable representations and outperform baseline agents in terms of sample complexity and overall performance. The learned representations enable better generalization, as evidenced by behavioral analyses and attention weight analysis. The model's internal computations were interpretable and congruent with task-relevant relations. Future directions include scaling the approach to larger input spaces, exploring distinct attentional mechanisms, filtering out unimportant relations, and perceiving complex scenes via structured formats like scene graphs. Additionally, exploring richer graph network implementations and learned approaches for inducing compositional programs are suggested. The approach explores learned methods for inducing compositional programs and reasoning about structured data, drawing inspiration from symbolic AI. It may interface well with hierarchical RL, planning, and structured behavior representation, blurring the line between model-free agents and those with abstract planning capabilities. The inductive biases for entity and relation-centric representations aim to make general environment knowledge available for decision-making. The agent learns to exploit relational architectural prior in a principled hybrid of statistical learning and structured approaches. Box-world levels are procedurally generated with a random graph defining the path to the goal and distractor branches. 20 keys and locks are positioned randomly in the room for the agent to navigate. The Box-World levels are procedurally generated with 20 keys and locks. The agent receives rewards for collecting gems and opening boxes, with different outcomes leading to level termination. The generation process ensures a large variety of levels, allowing for different training-test splits. Alternative RL algorithms showed similar results, with relational agents outperforming baseline agents across different algorithms. The RL algorithm proposed by BID10 was used with A3C or distributed DQN on 10x10 pixel maps with up to 3 boxes and distractor branches. Distributed A2C agents with off-policy corrections were employed, consisting of 100 actors and a single learner. Model updates were done on GPU with mini-batches of 32 trajectories. Training used RMSprop optimizer with tuned learning rates. The input module had two convolutional layers followed by ReLU activation. The input module had two convolutional layers with a stride of 1 and ReLU activation. The output was tagged with spatial position channels and passed to a relational module with attention heads. The output was aggregated and passed through fully connected layers to produce policy logits and baseline function. The baseline agent had a similar architecture but used residual-convolutional blocks instead of the relational module. The StarCraft II agents were trained with a module containing 3 to 6 residual-convolutional blocks, each consisting of two convolutional layers with 3x3 kernels and 26 output channels. Training was done with Adam optimizer for 10 billion steps using batches of 32 trajectories unrolled for 80 steps. Input-preprocessing involved presenting agents with 4 sources of information: minimap, screen, player, and previous-action, with numerical features re-scaled using a logarithmic transformation. The StarCraft II agents were trained with a module containing residual-convolutional blocks for input-preprocessing. Numerical features were re-scaled with a logarithmic transformation, and categorical features were embedded into a 10-dimensional space. Spatially encoded inputs were processed through convolutional layers and residual blocks to form spatial and non-spatial inputs for the model. The model utilizes a 2-layer MLP and Conv2DLSTM for memory and relational processing. The output is processed through MHDPA blocks and split into relational-spatial and relational-nonspatial pathways. The final output is a combination of inputs 2D and relational-nonspatial. The model combines inputs 2D and relational-nonspatial features to produce policy logits and baselines values. Actions are sampled based on policy logits and embedded into a 16-dimensional vector. Spatial arguments are obtained by deconvolving relational-spatial features. The control agent architecture utilizes 1 \u00d7 1 \u00d7 1 convolution layers for spatial arguments (x, y) and Conv2DLSTM layers for 2D outputs. A 12-layer deep residual model is employed for relational-spatial outputs, while a separate pathway involves a 2-layer MLP for relational-nonspatial outputs. The architecture is similar to the relational agent."
}