{
    "title": "r1laNeBYPB",
    "content": "Graph Neural Networks (GNNs) are deep models that operate on data represented as graphs. An efficient memory layer for GNNs has been introduced to learn graph representation and pooling. Two new networks, MemGNN and GMN, based on this memory layer can learn hierarchical graph representations. Experimental results show state-of-the-art performance in graph classification and regression benchmarks, with learned representations corresponding to chemical features in molecule data. Graph Neural Networks (GNNs) operate on data represented as graphs like social networks, knowledge graphs, molecules, point clouds, and robots. Unlike regular inputs with spatial locality, GNN inputs are variable-size graphs with permutation-invariant nodes and interactions. GNNs like GGNN, MPNN, GCN, and GAT learn node embeddings through an iterative process of transferring, transforming, and aggregating node embeddings from topological neighbors. Each iteration expands the receptive field by one hop, influencing nodes within k hops after k iterations. GNNs improve representation learning compared to other methods like random walks, matrix factorization, kernel methods, and probabilistic graphical models. However, these models lack hierarchical representation learning. Recent advancements like DiffPool, TopKPool, and SAGPool enable models to learn hierarchical graph representation by combining GNN and pooling layers to cluster nodes in a meaningful way for the task. In this paper, a memory layer is introduced for joint graph representation learning and graph coarsening. It consists of a multi-head array of memory keys and a convolution operator to aggregate soft cluster assignments. The memory layer does not require connectivity information and relies on global information, making it more efficient and improving performance. Two networks based on this layer are also introduced: Memory-based Graph. In this paper, a memory layer is introduced for joint graph representation learning and graph coarsening. Two networks based on this layer are also introduced: Memory-based Graph Neural Network (MemGNN) and Graph Memory Network (GMN). Memory Augmented Neural Networks (MANNs) utilize external memory with differentiable read-write operators to enhance reinforcement learning, meta learning, few-shot learning, and multi-hop reasoning. Unlike RNNs, MANNs store and retrieve longer term memories with fewer parameters. Memory in Memory Augmented Neural Networks (MANNs) allows for storing and retrieving long-term memories with fewer parameters. Different memory implementations include key-value memory, array-structured memory, and Sparse Access Memory (SAM). Graph Neural Networks (GNNs) use message passing for learning node embeddings over graphs, with models like GraphSAGE and GAT using different techniques for aggregation. GCN models extend convolution to arbitrary topology, with Spectral GCNs using spectral filters over graph Laplacian for defining features. Graph Neural Networks (GNNs) utilize memory layers with feed-forward networks for learning node embeddings. Graph pooling can be done globally or hierarchically, with different aggregation techniques such as sum, max, Set2Set, and SortPool. Non-parametric methods like clique pooling, kNN pooling, and Graclus rely on topological information for efficiency. Graph Neural Networks (GNNs) utilize memory layers with feed-forward networks for learning node embeddings. Different graph pooling techniques like DiffPool, Mincut pool, TopKPool, and SAGPool use various methods to compute node embeddings and cluster assignments efficiently. The memory layer in a graph neural network acts as a parametric function that coarsens input nodes and transforms their features. It consists of memory keys and a convolutional layer, with attention matrices aggregated using a shared input query. The addressing scheme involves computing similarity between memory keys and a given query. The addressing scheme in the proposed architecture involves computing similarity between memory keys and a given query, with the attention weight defined as a function of the distance between them. The memory layer coarsens input queries and transforms them into a new query space, treating the input queries as node embeddings and the keys as cluster centroids. The proposed architecture involves using a clustering-friendly distribution to measure similarity between keys and a query. The memory keys are modeled as a multi-head array to increase model capacity, producing a tensor of cluster assignments. The heads are aggregated into a single assignment matrix using a convolution operator. The proposed architecture utilizes a [1 \u00d7 1] convolution to aggregate information across heads, reducing them to a single matrix. A memory read generates a value matrix representing coarsened node embeddings, which is then projected using a single layer neural network with a LeakyReLU activation function. The proposed architecture uses coarsened embeddings to represent output queries, allowing a memory layer to learn node embeddings and coarsen the graph. Two architectures, GMN and MemGNN, differ in how the query network is implemented. GMN uses a feed-forward network, while MemGNN implements the query network differently. The GMN model treats graphs as a set of permutation-invariant nodes without message passing, using memory layers to generate initial query embeddings. The MemGNN model encodes topological information into initial query embeddings using a query network, unlike the GMN architecture. It relies on an iterative process of passing messages and aggregating them to compute the initial query. The MemGNN model introduces an edge-based GAT (e-GAT) as an extension to the original GAT model. e-GAT learns attention weights from both neighbor nodes and input edge attributes, crucial for data with edge information like molecule datasets. The attention score in an e-GAT layer is computed using trainable node and edge weights, along with a single-layer feed-forward network. The model is trained using supervised classification and unsupervised clustering loss functions. The MemGNN model utilizes a supervised classification loss (L ent) and an unsupervised clustering loss to train the model. The unsupervised loss encourages the model to learn clustering-friendly embeddings in the latent space by using a Kullback-Leibler (KL) divergence loss between soft assignments and an auxiliary distribution. The total loss is defined based on the number of samples. The distribution is defined by a total loss formula with a scalar weight. Model parameters are initialized randomly and optimized using stochastic gradient descent. Gradients of L ent are back-propagated batch-wise to stabilize training. The proposed method is evaluated on nine graph benchmarks, including classification and regression datasets commonly used in graph kernel and GNN literature. The datasets mentioned in the literature include ESOL for water solubility, Lipophilicity for octanol/water distribution, Bace for binding results of inhibitors, DD for enzyme structure classification, Enzymes for enzyme functional class prediction, Proteins for protein function prediction, and COLLAB for researcher field prediction. Refer to Appendix A.2 and A.1 for more details on the datasets and implementation. The performance of models on DD, Enzymes, Proteins, and COLLAB datasets was evaluated using 10-fold cross-validation. Results showed that the proposed models significantly improved accuracy on DD, Enzymes, and Proteins datasets. MemGNN outperformed GMN on COLLAB, while GMN achieved better results on Enzymes. MemGNN outperforms GMN on COLLAB, while GMN achieves better results on Enzymes, Proteins, and DD datasets. On COLLAB, our models are outperformed by diffpool-det and WL Optimal Assignment. The former is a GNN with deterministic clustering algorithm, and the latter is a graph kernel method. ESOL and Lipophilicity datasets are evaluated using RMSE for regression benchmarks. MemGNN model is trained and compared to baseline models including GCN, MPNN, and DAG based models. Our MemGNN model achieves state-of-the-art results on ESOL and Lipophilicity benchmarks, as well as on the Bace, Reddit-Binary, and Tox21 datasets. The proposed e-GAT model is investigated, and the MemGNN model is trained using both GAT and e-GAT models as the query network. Edge attributes are used as benchmarks, with node and edge feature dimensions set to 16 and 4, respectively. The node and edge feature dimensions were set to 16 and 4, respectively. The e-GAT model outperformed the standard GAT model on the ESOL dataset. Different topological features were evaluated for the GMN model, with the adjacency matrix yielding the best performance. Down-sampling methods for dense datasets were also investigated. In dense datasets like COLLAB, two down-sampling methods were tested to improve memory and computation. The first method randomly selects 10% of edges, while the second method keeps the top 10% based on RWR scores. Training the MemGNN model on COLLAB resulted in 73.9% and 73.1% accuracy for random and RWR-based sampling, showing random sampling slightly outperforms. The number of keys in clusters is not directly related to the number of nodes in input graphs, with smaller graphs potentially having more meaningful clusters. In the ESOL dataset, molecules have an average of 13.3 nodes, while ENZYMES have an average of 32.69 nodes. Performance is best with 10 keys for ENZYMES and 64 keys for ESOL. Increasing memory heads improves performance, with 8, 64, and 160 keys resulting in RMSE of 0.56, 0.52, and 0.54 respectively. Memory keys represent cluster centroids, enhancing model performance by capturing meaningful structures in the graph. The network has learned chemical features essential for determining molecule solubility, visualized clusters of meaningful chemical substructures like carbon chains and Hydroxyl groups. Memory keys were initialized using K-Means algorithm but did not show significant improvement over randomly selected keys. Our study introduced efficient memory layers and deep models for hierarchical graph representation learning, achieving state-of-the-art results on nine graph classification and regression tasks. The learned representations captured chemical features of molecules without using message passing. Node attributes combined with topological embeddings and memory layers yielded notable results, with binary adjacency matrices being sufficient for topological embeddings. The memory layer effectively processed node embeddings for clustering and aggregation. Our study introduced efficient memory layers and deep models for hierarchical graph representation learning, achieving state-of-the-art results on nine graph classification and regression tasks. The learned representations captured chemical features of molecules without using message passing. Node attributes combined with topological embeddings and memory layers yielded notable results, with binary adjacency matrices being sufficient for topological embeddings. The memory layer effectively processed node embeddings for clustering and aggregation. In section 4.2, it was found that kernel methods or deep models with deterministic clustering outperformed our models on the COLLAB dataset, especially in graphs with dense communities like cliques. However, our MemGNN model performed better than the GMN model on the DD dataset, indicating the need for message passing in datasets relying more on local information. This suggests that for data with strong local interactions, message passing is crucial for performance improvement. Future directions include introducing a new model based on these findings. Future Directions: Introducing a model based on MemGNN and GMN architectures for node classification by attending to node embeddings and centroids of clusters from different hierarchy layers. Investigating representation learning capabilities of proposed models in self-supervised setting. Implementation details include the use of PyTorch, Adam optimizer, batch-normalization, skip-connections, LeakyRelu activation functions, and dropout for regularization. Hyper-parameters were determined through random search strategy. The best hyper-parameters for graph classification and regression tasks are shown in Table 3. Dataset statistics are presented in Table 4, including results on BACE, Tox21, and Reddit-Binary datasets. Evaluation is based on AUC-ROC measure, with MemGNN model performance compared on datasets with initial edge attributes. The MemGNN model achieves state-of-the-art results by a margin of 4.0% AUC-ROC on the BACE benchmark and is competitive with the GCN model on the Tox21 dataset. The Reddit-Binary dataset is used for predicting the type of community in online discussion threads, with the GMN model achieving state-of-the-art accuracy. The introduced GMN model achieves state-of-the-art accuracy by 0.44% margin. e-GAT outperforms GAT on RMSE and R2 score for ESOL dataset. Table 6 shows mean validation accuracy for various methods, with GMN achieving 86.39% and MemGNN achieving 85.55%. The GMN model achieves state-of-the-art accuracy with 86.39%, while MemGNN achieves 85.55%. The relevance score of nodes in the graph is calculated using a linear system, determining nodes with higher relevance scores for message passing in MemGNN or as topological embeddings in GMN. The restart probability defines how far the agent can walk from the source node. The inverse of the adjacency matrix for big graphs is computationally expensive, but there are methods to make the estimation more efficient. Clusters learned by a MeMGNN for ESOL and LIPO datasets show recognition of chemical groups like OH, CCl3, COOH, CO, and benzene rings, which greatly impact the solubility of molecules."
}