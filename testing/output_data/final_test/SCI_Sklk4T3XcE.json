{
    "title": "Sklk4T3XcE",
    "content": "Reinforcement learning and evolutionary algorithms are used to create complex control solutions, but explaining their \"black box\" nature can be challenging. This paper addresses explainability of blackbox control algorithms through six techniques tested on a simulated rover navigating obstacles using an evolved multi-layer perception. Explanation of machine learning algorithms is a challenging field of research, with most techniques focusing on supervised learning. This paper explores explainability techniques designed for supervised learning and applies them to reward-based machine learning like reinforcement learning and evolutionary algorithms. Machine learning techniques, such as reinforcement learning and evolutionary algorithms, can be used to automatically learn high-performance control systems for complex problems in the context of autonomy. Training involves adjusting weights of a neural network-based control policy using inputs from sensors to command control actions. The goal is to produce a high-performance non-linear control policy through a series of trials in a simulator. Machine learning algorithms are often considered \"blackbox\" as their internal workings are unclear. Trust in machine learning can be difficult due to issues with coverage and generalizability. Coverage refers to the algorithm's performance in tested scenarios, while generalizability depends on the data set used. Users may overlook gaps in data sets, leading to potential performance issues in unseen scenarios. The blackbox nature of machine learning algorithms can lead to issues with coverage and generalizability. Poor choices of parameters can result in poor generalization, and improving explainability can help build trust in their deployment. Explanation of machine learning algorithms is crucial for trust in deployment. Understanding control algorithms helps identify gaps and flaws for generalization. However, defining meaningful explanations remains a challenge in the field. Control strategies evolve over time, making evaluation complex. In the context of developing control strategies for machine learning algorithms, various techniques were experimented with to provide explanations. These techniques included Bayesian rule lists, function analysis, integrated gradients, grammar-based decision trees, sensitivity analysis with temporal modeling using LSTMs, and explanation templates. These explanations aim to assist in detecting pitfalls and errors in developing such systems. The paper discusses the use of different explainability algorithms for control data, including local and global explanations. It also introduces an obstacle avoidance problem, a neural network controller, and the Monte Carlo algorithm. The need for explainability is highlighted, and six explainability algorithms are applied to the example problem. In this study, six explainability algorithms are applied to a test problem involving a rover navigating towards a goal while avoiding obstacles on a 2D plane. The neural network controlling the rover's movements is trained using an evolutionary algorithm. The rover's performance is evaluated after 70 time steps, with sensors providing feedback at each step. The rover's performance is evaluated through sensors providing feedback at each time step. The world is divided into four quadrants with two sensors per quadrant, one for obstacles and one for the goal. The sensors return inverse square distances from obstacles to the rover and from the rover to the goal. The sensor space is divided into four regions for input-output mapping, with a trade-off between region granularity and input space dimensionality. Each sensor observes objects in its quadrant based on distance, contributing to an eight-dimensional sensor vector for the rover's state space. This state is used to compute x, y movement in the next time step. The rover's x, y movement is determined by a Multi Layer Perceptron (MLP) with sigmoid activation function. The MLP output is normalized and scaled to the maximum distance the rover can move in one time step. We use a Monte Carlo algorithm to determine the weights for the neural network controller. The rover's movement is controlled by a Multi Layer Perceptron (MLP) with sigmoid activation. A Monte Carlo algorithm is used to determine weights for the neural network controller. The weights are evaluated in a simulation for 70 time steps, with 1000 runs in the first phase and 200 runs in the second phase. The best performing rover's weights are saved as the final solution. The Monte Carlo algorithm is used to create a neural network controller for the rover. The controller's performance is evaluated by observing its path and improvement during training. Random neural networks perform poorly, but some excel. Performance improves slightly in Phase 2 of training. The neural network controller's performance improves slightly in Phase 2 of training, but the analysis does not reveal how it actually operates or if there are hidden failure modes to be aware of. Machine learning algorithms, especially neural networks, can have subtle failures not apparent in basic testing. Inspecting the interconnected weights of a neural network is usually not helpful in determining its correct operation. An example of such a failure occurred when the range of weight values was accidentally limited during training, leading to the network's inability to fully approximate non-linear functions. The training revealed a performance issue in the control policy due to an error, which was discovered during a unit test on approximating a sine wave. To gain further insights into the neural network's operation, various explainability methods were applied, including Bayesian rule lists, function analysis, integrated gradients, decision trees, sensitivity analysis with LSTMs, and explanation templates. Integrated gradients was chosen for its local interpretability. Explanations in control policies focus on individual actions rather than entire sequences. Bayesian Rule Lists consist of if-then rules based on controller inputs, not the neural network itself. Temporal aspects make explanations complex, with only grammar-based decision trees and LSTMs reasoning over time. Integrated gradients provide local explanations, while other methods offer more global insights into the control policy. The controller data is mapped to discrete rules for the obstacle sensors, converting values into categorical labels. Outputs are converted to binary values for x and y coordinates. Goal sensor values are ignored, focusing on obstacle sensors. This mapping allows for conversion of sensor and control data into labels. The controller data is mapped to discrete rules for obstacle sensors, converting values into categorical labels. A BRL extraction was performed using a control run of 70 time steps, generating rules for going up, down, left, and right. The up rule showed probabilities based on obstacles. However, the neural network's behavior of heading towards obstacles raised safety concerns. The BRL exposed potential hazards but was difficult to use and lacked insight into the controller's full behavior. The BRL analysis of the controller's behavior was limited and did not provide a comprehensive understanding. Next, the neural network controller's input/output functions were examined, revealing mappings for obstacle and goal sensors. The goal sensor controller behaved as expected, moving towards the goal, while the obstacle sensor controllers showed some variability. The obstacle sensor controllers for the rover exhibit non-intuitive behavior, with the X controller moving the rover right when close to obstacles in the up direction, but switching to the left when very close. The Y controller accelerates towards obstacles when they are close, confirming the Bayesian Rule List explanation rules. Sensitivity analysis of the neural network inputs to outputs can reveal the most important factors influencing the controller's decisions, with gradient analysis being a basic method for this purpose. The gradient analysis in neural networks using backpropagation was tested by calculating the gradient of obstacle sensors with respect to controller output. Results showed limitations in explainability, as the closest sensor had the smallest gradient due to saturation. Larger changes need to be considered for better understanding. Integrated Gradients attempts to address the limitations of local gradients by considering larger changes in sensor values. By calculating gradients from the rover position to a baseline value, important changes along this path are recorded. Results show improved sensor values, with the up sensor now having the second largest value. Another approach involves modeling the neural network's global properties using an explanation template called BID2. The explanation template BID2 uses a simple control algorithm with free parameters determined by analyzing neural network behavior. Linear regression on rover data revealed parameters for moving towards goals and avoiding obstacles. The study explores the rover's tendency to avoid obstacles by moving right when approaching them. Using grammar-based decision trees (GBDTs), an interpretable model is learned to gain insight into system behavior. GBDT offers good representational ability and interpretability, allowing for modeling of different types of data using appropriate grammars. The study uses GBDTs to model system behavior, with one approach focusing on neural network policy input-output behavior and another on time series data. Training data is constructed using input-output pairs from the neural network policy, with output angles rounded to the nearest 45 degrees for GBDT compatibility. The GBDT model, trained using genetic programming, achieved 85.5% accuracy by creating rules to distinguish between three policy outputs. The primary actions required for successful navigation are up, up left, and up right, as the goal is located above the start point. The agent's behavior in navigating obstacles can be observed in the output. The GBDT model achieved 85.5% accuracy by creating rules for navigation. The agent's behavior in navigating obstacles is observed as it moves towards the goal. The neural network policy may be overfitted to specific obstacle arrangements, raising questions about the explanatory power of the discovered rules. The GBDT model achieved 85.5% accuracy by creating rules for navigation, capturing temporal properties of the controller-environment system. A training dataset with multivariate sequences was used to distinguish between optimal and suboptimal controller paths. A grammar based on simple temporal logic was specified to gain insight into the learned rules. The GBDT model, achieving 99.9% accuracy, identified three key temporal properties to distinguish between optimal and suboptimal sequences. These properties must be simultaneously satisfied. The GBDT model identified key properties for optimal sequences, including specific thresholds for action values and obstacle sensor readings. These findings correlated strong right and strong up actions with a weak right obstacle sensor to the optimal policy. This approach combines temporal modeling with attributions to highlight salient sequential inputs. An LSTM classifier is trained to distinguish between optimal and suboptimal input sequences, and integrated gradients are used to evaluate the importance of each input. Attributions provide local explanations for specific examples, showing important features and time steps. FIG1 illustrates a repeating pattern in features 7 and 8 in examples classified as optimal. The highlighted values in the data show a repeating pattern in features 7 and 8, indicating that sequences produced by the optimal controller reach the goal much sooner than 70 time steps. The controller has learned to cycle near the goal to collect maximum reward, as stopping completely is difficult to learn. This behavior involves moving downwards when the goal is near and located below. The optimal controller exhibits cycling behavior near the goal, moving downwards when the goal is below and jumping up and rightward when the goal is detected by a different sensor. While temporal modeling helps identify patterns, it requires human analysis to understand relevance. Explanation algorithms used in supervised learning may not directly apply to reward-based control learning due to the time-extended nature of control policies. Control policies in reward-based learning have a time-extended property, making explanations from supervised learning inadequate as they focus on immediate benefits. Grammar-based decision trees and temporal modeling attempt to address this issue but struggle to summarize the complex operations needed to optimize for future time steps. While explanation algorithms can expose major flaws in neural network controllers, they may not reveal more subtle issues. Explanation algorithms were applied to a neural network controller trained using a Monte Carlo algorithm for obstacle avoidance. These algorithms aim to explain the controller's behavior by analyzing its inputs and outputs, revealing potential hazards in its decision-making process. The explanation algorithms applied to a neural network controller trained for obstacle avoidance revealed a potential hazard in its decision-making process. However, gaining deep insights beyond this flaw was challenging."
}