{
    "title": "Syxp2bgKoX",
    "content": "The EnergyNet is a CNN designed for energy-aware dynamic routing, reducing run time energy cost without sacrificing accuracy by learning a dynamic routing policy to skip certain layers. Empirical results show up to 40% and 65% energy cost reduction on CIFAR10 and Tiny ImageNet testing sets, respectively, while maintaining accuracy. The EnergyNet, a CNN designed for energy-aware dynamic routing, reduces run time energy cost without sacrificing accuracy by learning a dynamic routing policy to skip certain layers. It achieves up to 40% and 65% energy cost reduction on CIFAR10 and Tiny ImageNet testing sets, respectively. Energy awareness can also serve as training regularization, improving prediction accuracy by up to 0.7% higher top-1 testing accuracy on CIFAR-10 and 1.0% higher top-5 testing accuracy on Tiny ImageNet when saving energy. High-performance Convolutional Neural Networks (CNNs) have high energy consumption due to their large size and numerous parameters. Various methods have been proposed to reduce energy costs in CNNs, focusing on model size and computations. However, simply reducing model size and operations may not always lower energy costs. One approach involves pruning layers based on energy cost, while another formulates CNN training as an optimization problem with an energy budget constraint. These methods show promise in creating more energy-efficient CNNs. The paper introduces EnergyNet, a new CNN model that combines energy cost with dynamic routing for adaptive energy-efficient inference. EnergyNet is a gated CNN architecture that uses conditional computing to route input data through the network, reducing energy costs by generating dynamic routing strategies for different input images. EnergyNet is a gated CNN architecture that uses conditional computing to route input data through the network, reducing energy costs by generating dynamic routing strategies for different input images. By sharing parameters between RNN gates, EnergyNet achieves a minimal energy cost of 0.04% compared to the base CNN model. It utilizes a gating network to decide whether to skip certain layers, optimizing a weighted combination of accuracy and energy loss. Empirical results show EnergyNet can reduce energy costs by up to 40% and 65% during inference on CIFAR10 and Tiny ImageNet testing sets. EnergyNet implements a dynamic routing algorithm using gating networks to reduce energy costs by up to 40% and 65% on CIFAR10 and Tiny ImageNet testing sets, respectively, while maintaining high accuracy. The model achieves higher testing accuracy compared to the baseline when saving energy, acting as a regularization method to prevent overfitting. EnergyNet uses gating networks to decide whether to skip blocks based on the output of a gating network G. A new energy-aware loss function is adopted for learning the skipping policy. The gating network consists of a global average pooling and a linear projection to reduce features to a 10-dimensional vector, followed by an LSTM network to generate a binary scalar. EnergyNet utilizes a single layer of dimension 10 for generating a binary scalar, with RNNGate design incurring minimal overhead compared to its feed-forward counterpart. To reduce energy costs, all RNNGates share the same weights. Dynamic routing in EnergyNet is learned by minimizing an energy cost along with accuracy loss, with a weighting coefficient \u03b1 and parameters W and G for the base model and gating network. Prediction loss L(W, G) and energy cost E(W, G) are calculated to optimize the CNN model. The CNN model associated with W and G calculates the energy cost of layers by considering the energy costs of memory hierarchies and MAC operations. A hierarchical memory architecture is commonly used in CNN accelerators to minimize memory access and data movement costs. The design includes three memory hierarchies - main memory, cache memory, and local register files, and a simulation tool called \"SCALE-Sim\" is used to calculate memory accesses and MAC operations. EnergyNet saves more energy than baseline ResNet on CIFAR10 and Tiny ImageNet without compromising prediction accuracy. EnergyNet-38 and EnergyNet-50 outperform ResNet-38 and ResNet-50 in energy savings and testing accuracy. The training process involves setting a small weighting coefficient alpha. The training process involves a three-step strategy with a small weighting coefficient alpha to stabilize training and improve performance. EnergyNet achieves energy savings by reducing energy cost while maintaining or enhancing accuracy. EnergyNet achieves significant energy savings without compromising prediction accuracy, outperforming the original ResNet model. Through 20 trials, EnergyNet-38 maintains a prediction accuracy of 92.47%-92.58% with energy savings of 39.55%-40.52%. The model can overcome overfitting and reduce energy costs by 4\u00d7 compared to ResNet-38 and ResNet-50 baselines. EnergyNet can reduce energy cost by about 4\u00d7 compared to ResNet-38 and ResNet-50, with testing accuracy losses of 3% and 4% respectively."
}