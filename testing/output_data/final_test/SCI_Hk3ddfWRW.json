{
    "title": "Hk3ddfWRW",
    "content": "Recent advances in learning from demonstrations (LfD) with deep neural networks have enabled the learning of complex robot skills involving high dimensional perception like raw image inputs. In this paper, a new LfD approach is introduced for learning multiple modes of behavior from visual data using a stochastic deep neural network (SNN). The SNN represents the underlying intention in demonstrations as a stochastic activation, and an efficient algorithm is presented for training SNNs. Additionally, an architecture with a stochastic attention module is proposed for learning with vision inputs. The method is demonstrated on real robot visual object reaching tasks, showing improved performance compared to traditional imitation learning techniques. Imitation learning (LfD) simplifies robotic control by directly learning from expert demonstrations, especially useful for modern robotic applications with high-dimensional sensory inputs. LfD has been successful in teaching complex robot skills like locomotion, driving, flying, and manipulation. Deep representation learning advancements have further enhanced LfD capabilities. Video results can be viewed at https://vimeo.com/240212286/fd401241b9. Advances in deep representation learning have improved Learning from Demonstrations (LfD) methods for high-dimensional perception tasks like mapping images to controls. These advancements enable learning of generalizable skills, beneficial for industrial challenges like pick and place tasks. However, a challenge in LfD is learning different modes of the same task, such as picking up objects with varying intentions. Standard approaches using deep neural networks are not suitable for learning multimodal behaviors in LfD with image inputs. In LfD, NNs struggle with learning multimodal behaviors due to their deterministic mapping, resulting in an 'averaging' of different modes in the data. Adding labels for each mode is a common but impractical solution, as it requires more data recording and pre-specification of intentions. This approach places additional burden on the client and is conceptually flawed. In this work, a novel approach for Learning from Demonstrations (LfD) with multi-modal demonstrations is proposed. The method is based on a stochastic neural network model that represents latent intentions as random activations. A learning algorithm for training stochastic networks is introduced, along with a network architecture suitable for LfD with raw image inputs. The method can reproduce behavior with multiple intentions in real-robot tasks and outperforms deterministic counterparts in scenarios with multiple intentions. The focus is on direct imitation learning through behavioral cloning. The text discusses different approaches to Learning from Demonstrations (LfD), including behavioral cloning and inverse reinforcement learning (IRL). Behavioral cloning does not require a model of task dynamics or additional queries, while IRL seeks a reward model that explains demonstrated behavior. Model-free IRL approaches can learn complex behavior policies from high-dimensional data but require additional policy rollouts. Multi-task IRL learns from unlabeled demonstrations with varying intentions. BID6 and BID2 propose Bayesian and EM approaches for inferring agent intentions in a dynamic environment. Recent works have extended GAIL to high-dimensional multi-modal demonstrations. BID28 introduced a method for learning multi-modal policies from raw visual inputs using mixture density networks. The first LfD approach that can handle multiple modes in demonstrations without requiring additional robot rollouts or mode labeling. Uses a stochastic neural network model related to VAEs and GANs. In the context of multi-modal video prediction, our proposed training algorithm is similar to the K-best loss introduced by BID8 for stochastic neural networks. Our contribution includes a formal mathematical treatment of this method, the introduction of optimistic sampling to enhance performance, and demonstrating its significance in robotic imitation learning. We present preliminary concepts and problem formulation for imitation learning with multi-modal behaviors in expert demonstrations. Given a dataset of N trajectories, where tasks are recorded as pairs of observations and actions, Imitation Learning from Demonstrations (LfD) aims to learn a policy parametrized by feature weights to reliably perform tasks. The policy is learned by solving a maximum-likelihood objective, which is the empirical average of the conditional log likelihood. The supervised learning problem involves estimating the expected conditional log likelihood using a consistent estimator. Stochastic Neural Networks use multilayer perceptrons as general purpose function approximators for nonlinear regression in feedforward neural networks. The parameters are learned by maximizing the objective function. Stochastic Neural Networks introduce stochastic hidden variables to model multimodal conditional distributions, unlike deterministic neural networks. Sigmoid belief nets were an early attempt at this but had intractable inference. Stochastic feedforward neural networks (SNNs) add a stochastic latent variable to the deterministic features for modeling multimodal distributions. In imitation learning, mentor demonstrations involve multiple behaviors or intentions, modeled using a stochastic latent variable z. The conditional distribution is decomposed, and learning parameters \u03b8 involves a generalized EM algorithm with importance sampling and error back-propagation. The mentor demonstrations may include various strategies for tasks, such as picking up objects from a scene. In imitation learning, mentor demonstrations involve multiple intentions modeled using a stochastic latent variable. The goal is to learn a policy that accurately mimics the mentor's intentions. A sampling-based algorithm is proposed for learning the parameters, with a focus on vision-based inputs and attention over image features. The demonstrator's intention is modeled using a random vector with a probability distribution. In imitation learning, mentor demonstrations involve modeling multiple intentions with a stochastic latent variable. The conditional data likelihood is obtained by marginalizing out the random variable of intention. SNNs can be seen as directed graphical models where the generative process combines an observation with a latent intention to generate an action. The action probability is modeled given an intention, with the assumption that the demonstrator policy is memory-less. In imitation learning, mentor demonstrations involve modeling multiple intentions with a stochastic latent variable. The action probability is modeled as log P (u|z, x; \u03b8) \u221d \u2212d(f (x, z; \u03b8), u), where f is a deterministic NN that takes input x and intention z. The output action distribution can be multi-modal due to the random vector z input to a nonlinear NN computation. A Monte Carlo sampling algorithm is described for learning the parameters of the SNN model. For each given parameter \u03b8, sequence of observations x 1:T, and sequence of actions u 1:T, the reward function r(z; x 1:T, u 1:T, \u03b8) associates an intention with the data likelihood. A Monte Carlo approximation of the likelihood is used for computing \u03b8 by maximizing log r(z i; x 1:T, u 1:T, \u03b8) with gradient-based optimization. This approach provides a lower bound of the data log likelihood, corresponding to a maximum-likelihood approach. The gradient estimator is unbiased and consistent for E z\u223cP [\u2207 \u03b8 log r(z; x 1:T, u 1:T)]. In practice, the gradient estimator is unbiased and consistent for E z\u223cP [\u2207 \u03b8 log r(z; x 1:T , u 1:T , \u03b8)], but suffers from high variance. A new sampling strategy focuses on intentions with the highest reward to reduce variance in training SNNs. The proposed approximation has lower variance compared to a naive Monte Carlo approach. The proposed approach focuses on intentions with the highest reward to reduce variance in training SNNs. It defines an intention-driven probability distribution that samples from the most correct intentions. The expected reward induced by this distribution is equivalent to the conditional likelihood function of the most correct intentions. This metric, known as expected shortfall, is used to evaluate risky tail distributions in financial risk literature. The approach focuses on maximizing expected reward by sampling from a limited support distribution Q\u03b1 using Monte Carlo techniques. This reduces variance compared to estimating the original likelihood. Sampling from Q\u03b1 is done using empirical quantile estimation, with N\u03b1 samples corresponding to the \u03b1\u2212quantile. The approach involves maximizing expected reward by sampling from a limited support distribution Q\u03b1 using Monte Carlo techniques. This is achieved by estimating the empirical quantile with N\u03b1 samples corresponding to the \u03b1\u2212quantile, providing a consistent estimator of E z\u223cQ\u03b1 [r(z; x 1:T , u 1:T , \u03b8)] with order O(N \u22121/2 ). For the special case of N \u03b1 = 1, a simple min operation can be used to update the parameters \u03b8 by choosing the sampled z with the lowest error. Optimistic Sampling: The approach involves intention-driven sampling (IDS) to estimate the gradient. At test time, a set of recent high-reward z values is stored and sampled uniformly for prediction, improving performance significantly. Optimistic sampling, specifically intention-driven sampling (IDS), is used to estimate the gradient over training data, improving prediction performance significantly. The variance of the gradient estimator is analyzed, with a trade-off between bias and variance by adjusting the parameter \u03b1. Algorithm 1 outlines the IDS process for updating \u03b8 and sampling from Q\u03b1. The IDS algorithm uses optimistic sampling to estimate gradients over training data, improving prediction performance significantly. It adjusts the parameter \u03b1 to balance bias and variance in the gradient estimator. The algorithm reshapes the sampling distribution to lower the variance of the gradient estimate, integrating naturally with NN architecture. In practice, the IDS algorithm integrates optimistically with optimistic sampling for better performance. Introducing Intention-SNN (I-SNN), an architecture for LfD domains, it uses attention over image features. The I-SNN architecture consists of a CNN feature extractor followed by a spatial softmax layer to map features onto the image. The I-SNN architecture uses a spatial softmax layer to map features onto the image and applies stochastic soft attention over the spatial softmax features using a MLP. The attention weight vector is obtained through a softmax function and multiplied with the spatial softmax output to obtain attention-modulated feature activations. The control network in the I-SNN architecture uses a standard MLP to map features and current pose to control decisions. The architecture allows for selecting relevant features by giving them higher weights in the attention. The effectiveness of the approach is demonstrated in a reaching task with the IRB-120 robot, comparing it to a deterministic NN approach for LfD. Training an I-SNN using IDS is compared with training it using the SNN algorithm by Tang & Salakhutdinov (2013). Deterministic NN policies were evaluated with the same structure as I-SNN, except for the stochastic intention module. Parameters for SNN training included a loss function represented as log P (u|h, x; \u03b8) \u221d \u2212 f (x, h; \u03b8) \u2212 u 1, using an L1 regression loss. Monte Carlo samples were chosen as N = 5. In future work, spatial information may be added to features or the attention module for cases with identical appearance. Despite efforts, a conditional VAE was unsuccessful due to the complexity of mapping images to latent variables. The approach used in the study did not require a recognition module, which proved effective. The intention variable dimension chosen for experiments was z \u2208 R 5. The study utilized a 6-DOF IRB-120 robot with a 3-dimensional Cartesian vector control to navigate the end-effector to reach objects in the scene. Data collection included RGB images and end-effector Cartesian poses. Task configurations were defined by specific scene arrangements and initial end-effector poses. The study involved collecting demonstrations from 468 different task configurations, totaling 1404 trajectories, where humans used a 3DConnexion space-mouse to navigate the end-effector to reach objects. Training compared IDS and the SNN algorithm for the I-SNN architecture. During training, weights of convolutional layers were pre-trained in the Feature Extraction module using a deterministic NN model. Adam BID16 was used for optimization with default parameters. Models were evaluated on 10 task configurations with 20 trials each, totaling 200 trials. Optimistic sampling was used for IDS algorithm, while SNN performed better with uniform sampling. The deterministic NN model succeeded in reaching objects in only 3 task configurations out of 10. The IDS algorithm outperformed the SNN algorithm in mode learning ability, reaching all three objects in all tasks. The deterministic NN could only reach one object, while I-SNN trained by IDS reached all three objects, demonstrating strong mode learning capability. The IDS algorithm demonstrated superior performance over the SNN algorithm in mode learning ability, focusing on the best samples and consistently attending to the same object throughout execution. This approach, combined with optimistic sampling, led to better results in learning from demonstrations with multiple modes. The IDS algorithm outperformed the SNN algorithm in mode learning ability, focusing on best samples and consistently attending to the same object. Optimistic sampling led to better results in learning from demonstrations with multiple modes. The IDS algorithm showed better performance in training loss, while the SNN algorithm had better average log-likelihood loss. The algorithm presented is efficient for training neural networks with vision-based inputs, outperforming standard approaches in learning different modes. Future work includes exploring more complex manipulation tasks and investigating attention mechanisms for better generalization. The algorithm efficiently trains neural networks with vision-based inputs, surpassing standard methods in learning various modes. It is shown that maximizing the Kullback Liebler divergence between two distributions leads to local maximization of the likelihood problem. The closed form solution for maximizing the posterior distribution is derived using Bayes theorem. The algorithm efficiently trains neural networks with vision-based inputs, surpassing standard methods in learning various modes. Tang & Salakhutdinov (2013) present a generalized EM algorithm to train a SNN, using an approximate posterior distribution in the E-step and updating \u03b8 parameters in the M-step with the gradient vector. The algorithm efficiently trains neural networks with vision-based inputs, surpassing standard methods in learning various modes. In the M-step, the \u03b8 parameters are updated with the gradient vector based on the empirical expected log likelihood. The gradient estimator is consistent and has a bias of O(N \u22121/2 ) under certain regular assumptions. The bias of the gradient estimator is O(N \u22121/2 ) under regular assumptions, with the variance given by DISPLAYFORM5. The analysis in this section focuses on the variance of the gradient estimate of CVaR(R(z; \u03b8)), using shorthand notation R(z; \u03b8) for the log reward function. The update formula for the CVaR gradient estimate is considered for a given cut-off level \u03b1 and sample size n. The correlation in the quantile is based on the order statistics of the reward. The quantile is defined based on order statistics of the reward. Under certain conditions, the order statistic is independent of the reward. The variance of the gradient estimator can be expressed using the results from BID15. The variance of the intention-driven sampling gradient estimate can be expressed using the quantile. In contrast to recurrent neural networks, the I-SNN uniformly commits to the same mode at the task level throughout task execution. Visualization of the stochastic Intention network shows trajectories generated by the I-SNN trained by the IDS algorithm. In this section, simulation results comparing an IDS approach with a CVAE based approach are presented. The task involves predicting the location of a target in an image with randomly positioned targets of different colors. The goal is to simplify image processing and make a single-step decision without trajectory considerations. The IDS algorithm used the I-SNN architecture with a single conv layer to predict the 2-dimensional target position in an image with randomly positioned targets. The CVAE also utilized the I-SNN for the generation network and an MLP for the recognition network to predict the target position. The IDS algorithm utilized an MLP for the conditional prior network and added a term to the training loss. The latent variable z was chosen to be a standard Gaussian for fair comparison with the CVAE method. Results were evaluated based on the shortest distance from the prediction to the target positions on a test set. Various CVAE parameter settings were explored, with sensitivity to minibatch sizes and z dimension. The CVAE was sensitive to parameters like MLP architectures, learning rates, and the dimension of z. It worked well for N = 2 and N = 3 targets but not for N = 5. In contrast, the IDS approach performed robustly for all values of N and converged much faster than the CVAE."
}