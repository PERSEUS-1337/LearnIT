{
    "title": "rJl97IIt_E",
    "content": "Gaussian processes face scaling and shape constraint issues. Deep Random Splines, generated by a neural network, offer flexibility with shape constraints. They model point process data for neuroscience, using a variational autoencoder for inference. The novel recurrent encoder architecture can handle multiple point processes as input, addressing the limitations of Gaussian Processes (GPs) in terms of scaling and shape constraints. GPs are useful for modeling random functions but struggle with memory and runtime issues, as well as shape constraints. Splines are another tool for function modeling, especially when there are no shape constraints. When shape constraints are present, basis function expansion may not apply to splines, leading to a constrained optimization problem. Bayesian inference is necessary when modeling a random function with splines, requiring distributions on spline parameters. Previous methods for Bayesian inference without shape constraints rely on basis function expansion and simple parameter distributions. Deep Random Splines (DRS) are introduced as an alternative to Gaussian Processes (GPs) for modeling random functions, combining the complexity of deep generative models with the ability to enforce shape constraints of splines. DRS use a deep probabilistic model where standard Gaussian noise is transformed through a neural network to obtain spline parameters, allowing for modeling nonnegative intensity functions of Poisson processes. The paper introduces Deep Random Splines (DRS) as a novel approach for modeling random functions, specifically nonnegative intensity functions of Poisson processes. The method utilizes a parametrization of nonnegative splines through an intersection of convex sets and employs the method of alternating projections to ensure nonnegativity. Additionally, a variational autoencoder with a unique encoder architecture is proposed for scalable inference with continuous point processes as input. The model outperforms traditional alternatives in both simulated and real data scenarios. The paper explains Deep Random Splines (DRS) for modeling random functions, focusing on nonnegative intensity functions of Poisson processes. It introduces a parametrization of nonnegative splines and discusses how constraints can be enforced. The model is compared against other alternatives in simulated and real spiking activity datasets, showing superior performance. The paper also discusses functions on intervals and defines splines of degree d and smoothness s. The paper introduces Deep Random Splines (DRS) for modeling random functions, focusing on nonnegative intensity functions of Poisson processes. It discusses a parametrization of nonnegative splines and how constraints can be enforced. The model is compared against other alternatives in simulated and real spiking activity datasets, showing superior performance. The splines are defined by a set of parameters in each interval, with a specific parametrization to enforce constraints like nonnegativity. The paper introduces Deep Random Splines (DRS) for modeling random functions, focusing on nonnegative intensity functions of Poisson processes. A neural network transforms Z into \u03a8, with DRS given by g f \u03b8 (Z). Constraints like nonnegativity, monotonicity, and convexity/concavity can be enforced using a parametrization of nonnegative splines. This parametrization decomposes into convex sets easily characterized by parameters, allowing for better modeling. The paper discusses Deep Random Splines (DRS) for modeling nonnegative intensity functions of Poisson processes. A polynomial of degree d can be nonnegative in an interval if it can be written in a specific form using symmetric positive semidefinite matrices. Piecewise polynomials can also be nonnegative if they satisfy certain conditions with parameters (Q). The paper introduces constraints on parameters to ensure nonnegative splines, defining sets C1, Cj, and C0 with specific conditions. These constraints guarantee continuity, matching derivatives, and symmetry for nonnegative piecewise polynomials. The parametrization allows for nonnegative splines on a specified domain. The paper discusses the use of a neural network to map to nonnegative splines by utilizing a surjective function onto the parameter set \u03a8. The method of alternating projections allows for the analytical computation of this function, overcoming the issue of the semidefinite constraint on matrices. The method of alternating projections allows for the analytical computation of a function to map to nonnegative splines, overcoming the semidefinite constraint on matrices. Projection onto closed, convex sets can be done efficiently, making it useful for obtaining points in the intersection. The method of alternating projections allows for the analytical computation of a function to map to nonnegative splines efficiently. By letting h be the first M iterations, f \u03b8 maps to \u03a8, and \u2207 \u03b8 f \u03b8 (z) can be computed. Dykstra's algorithm can be used to find such an h function. The method of alternating projections efficiently computes a function to map to nonnegative splines. It converges to the projection of BID8 BID4 BID27 and is faster than other methods using reverse mode automatic differentiation packages. This method does not require additional hyperparameters like learning rate and has a linear convergence rate independent of the starting point. The method of alternating projections efficiently computes nonnegative splines without the need for additional hyperparameters like learning rate. It ensures convergence to the projection of BID8 BID4 BID27 with a linear convergence rate independent of the starting point. Monotonicity, convexity, or concavity can be enforced by parametrizing the derivative of the spline. This section begins with a review of inhomogeneous Poisson processes parametrized by an intensity function. The random set S follows a Poisson process with intensity g. The log likelihood of S can be evaluated exactly when g is a spline, making fitting a DRS more tractable compared to models using GPs. Splines vary smoothly, reflecting the assumption that the expected number of events changes smoothly over time. The DRS-VAE model, based on splines to model intensity functions, is proposed for analyzing microelectrode array data where N neurons are measured over time for R repetitions. Each X r,n represents a spike train, and the hidden state Z r is low-dimensional. The latent state Z r for each trial in the DRS-VAE model is a shared low-dimensional representation of the spike trains X r,n. There is a suggestion to simplify the parametrization of nonnegative splines by using a different function \u03c4 to enforce nonnegativity constraints, but squared splines were found to perform poorly. Variational autoencoder is a technique for inference in a model with hidden variables. It estimates model parameters and approximates the posterior distribution by using a parametrized distribution. The goal is to find values that approximate the true posterior. Bayesian inference involves maximizing the ELBO over (\u03b8, \u03c6) to approximate the true posterior. The ELBO is maximized over \u03c6 to minimize the KL divergence between the approximate and true posterior, and over \u03b8 to maximize a lower bound on the log likelihood. Differentiation with respect to \u03c6 is straightforward using the reparametrization trick, allowing for the use of stochastic gradient methods for inference. In order to perform inference with point process data, a recurrent architecture is used for \u00b5 \u03c6 and \u03c3 \u03c6, requiring N separate LSTMs, one per point process. The final states of each LSTM are concatenated and transformed to map to the hidden space R m. Regular LSTMs were found to be faster than bidirectional LSTMs with similar performance. The architecture for performing inference with point process data involves using a recurrent model with separate LSTMs for each point process. The ELBO is approximated at each stochastic gradient step using a model that combines equations. Another model, BID10, utilizes a hidden Markov model transformed through a neural network to obtain event counts on time bins. Their model has a higher-dimensional hidden state compared to the one used in the previous model. The model discussed in the previous paragraphs utilizes separate LSTMs for each point process in inference with point process data. The current chunk discusses extending the model to multi-dimensional point processes by changing the structure of the Gaussian distribution and using piecewise constant, nonnegative functions. It also mentions the ease of extending the model to more complicated point processes and simulating data with different types of trials. The study involved sampling 1200 trials, with 1000 trials used for training and the rest for testing. The model was compared against PfLDS and GPFA models, which discretize time into bins and have a latent variable per trial. PfLDS uses Gaussian linear dynamics and a Poisson distribution, while GPFA uses a GP distribution and a Gaussian distribution. The comparison was made against DRS-VAE. The study compared methods PfLDS and GPFA against DRS-VAE for analyzing point process data. PfLDS uses Gaussian linear dynamics and Poisson distribution, while GPFA uses GP distribution. Parameters included 11 knots, d=3, s=2, mini-batch size of 2, and L=2. LSTM states had 100 units, andf is a feed-forward neural network with ReLU activations and 3 hidden layers of 100 units each. 102 iterations of alternating projections were applied. Time was discretized into 13 bins for PfLDS and GPFA. The study compared PfLDS and GPFA methods with DRS-VAE for analyzing point process data. Parameters included 11 knots, d=3, s=2, mini-batch size of 2, and L=2. LSTM states had 100 units, and a feed-forward neural network with ReLU activations and 3 hidden layers of 100 units each. The latent dimension m in the model was set to 2, and the overall latent dimension for an entire trial was 2B = 26. The left panel of FIG3 shows the posterior means of hidden variables in the model for each of the 200 test trials, forming separate clusters for different trial types. The right panel of FIG3 displays events for a specific point process on a trial, with true intensity, model posterior samples, and PfLDS samples. Our method shows a smoother function closer to truth than PfLDS. Performance comparison in TAB0 shows our model outperforms PfLDS in per-trial ELBO on test data. Our model has a larger ELBO than PfLDS, indicating a better log likelihood. While log likelihoods are not directly comparable due to different distributions, we can convert PfLDS to make a quantitative comparison. By comparing L2 distances between posterior intensity samples and actual intensity function, our method outperforms GPFA. Our method outperforms GPFA in recovering intensity functions closer to the truth in a statistically significant way. The dataset consists of measurements of 20 neurons for 3590 trials on the interval [-100, 300) ms of a primate, with a training set of 3000 trials and a test set with the rest. We split the data into a training set with 3000 trials and a test set with the remaining trials. Parameters were set for the LSTM and feed-forward network architecture. Time was divided into 20 bins for comparison against PfLDS. Ground truth comparison was not possible due to lack of access. Our model outperformed PfLDS in generating low-dimensional representations of trials, as shown by the spike train visualization and higher ELBO values on test data. The posterior intensities from our model appeared smoother and more plausible compared to PfLDS, even with PfLDS having access to 20 times more hidden dimensions. Additionally, our model had a higher percentage of correctly predicted test trial types when using 15-nearest neighbors on the posterior means of train data. Our model also outperforms PfLDS in generating low-dimensional representations of trials and shows a higher percentage of correctly predicted test trial types using 15-nearest neighbors on the posterior means of train data. Additionally, the model was applied to a dataset from the Churchland lab at Columbia University, consisting of 1300 train and 188 test trials. The study includes 1300 train and 188 test trials where a primate's neural activity is measured while pedaling. Different trial types are based on pedal direction and speed. The model uses hyperparameters with 26 knots, 28 bins, and a hidden dimension of 10. Comparison with PfLDS shows higher ELBO for PfLDS but better latent representations in the study. Accuracy of predicting test trial types is higher with the study model. In this paper, Deep Random Splines are introduced as an alternative to Gaussian processes for modeling random functions. The model allows for shape constraints on the random functions and is tractable due to key modeling choices and results from spline and optimization literatures. The paper introduces Deep Random Splines (DRS) as an alternative to Gaussian processes for modeling random functions with shape constraints. A variational autoencoder utilizing DRS accurately models neural activity. Future work includes applying DRS-VAE to multi-dimensional point processes and exploring more complex point processes like Hawkes processes. Deep Random Splines (DRS) can be applied in various settings beyond the scope of the paper, as they are useful in any scenario involving a random function. Nonnegative splines of even degree can be characterized using a symmetric positive semidefinite matrix, allowing for the representation of piecewise polynomials with specific constraints. A piecewise polynomial of degree d with knots t 0 , . . . , t I defined as p (i) (t) for t \u2208 [t i\u22121 , t i ) for i = 1, . . . , I is nonnegative if it can be written as: DISPLAYFORM1 for i = 1, . . . , I. Projections onto the space of smooth splines can be achieved through alternating projections onto different sets C j. The details on how to project onto C 1 , C 2, and C 3 for odd-degree splines are covered, with the possibility of extending the method to higher degree splines and even degree splines. Given (Q DISPLAYFORM0 , defining a piecewise polynomial, projections can be derived using (k +1)\u00d7(k +1) matrices. Projections onto the space of smooth splines can be achieved through alternating projections onto different sets C j. Computing the projection onto C 1 involves solving a quadratic optimization problem with linear constraints. The Lagrangian and KKT conditions are used to verify the solution. Similarly, the projection onto C 2 involves solving another optimization problem with constraints on the second derivatives of the piecewise function at the knots. The quadratic optimization problem with linear constraints is solved by writing the Lagrangian and solving the KKT conditions. Successful trials are included, spikes are limited to a specific time window, and the GLM is trained using group lasso with groups defined by neurons. The GLM is trained using group lasso with groups defined by neurons, where a penalty is added to ensure coefficients hit zero simultaneously. Neurons are removed based on a regularization parameter, keeping only 20 useful neurons. PfLDS does not require removing neurons and still outperforms with minimal performance increase. Successful trials are kept, and the total number of neurons is reduced to 20 using group lasso. The PfLDS model is trained with all neurons and shows only a slight improvement, similar to the results seen with reaching data. Each trial is extended to match the length of the longest trial, with no added spikes."
}