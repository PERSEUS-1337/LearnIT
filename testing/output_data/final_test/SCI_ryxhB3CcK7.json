{
    "title": "ryxhB3CcK7",
    "content": "We propose a new class of probabilistic neural-symbolic models for visual question answering (VQA) that provide interpretable explanations of their decision making in the form of programs. Our approach learns a rich latent space to propagate program annotations from known to novel questions. By formalizing module networks as structured latent variable models, our model generates more interpretable programs and achieves better accuracy on VQA in the low-data regime. In recent years, deep representation learning has advanced artificial intelligence tasks like image recognition, machine translation, visual question answering, visual dialog, and reinforcement learning. While deep neural networks excel in these tasks, aspects of human cognition like compositional generalization and reasoning are challenging to model with current deep learning approaches. Symbolic approaches offer strong compositional and reasoning capabilities that are difficult to achieve with neural networks. Neuro-symbolic processing aims to combine learning and reasoning capabilities for AI systems tackling higher-level tasks involving reasoning. Symbolic instructions are easier to specify and more interpretable than neural network parameters. For high-level tasks, it is sensible to specify \"what\" to do using symbols and learn \"how\" to do the task using representation learning techniques. For example, in visual question answering, one can ask the model to reason about the answer by specifying a series of operations to be executed as a \"program\". The network can then learn how to execute such a program from data using deep representation learning. This paper explores neuro-symbolic models for visual question answering, aiming to retain interpretability while specifying minimal teaching examples. Neural module networks generate programs in prefix notation to construct a network that operates on images and attention maps to answer questions. The approach involves treating programs as latent variables in a generative model of questions and answers given images. The model class for visual question answering involves treating programs as stochastic latent variables to capture uncertainty and improve interpretability. It assumes independence between questions and answers conditioned on the program, using neural module networks to predict answers based on images and programs. Our model, referred to as variational neural module networks (V-NMN), incorporates a probabilistic latent variable neuro-symbolic model. Training involves stages of grounding questions into programs, learning execution, and optimizing the full variational objective. This approach successfully learns a probabilistic neural-symbolic model for visual question answering. The V-NMN model is a probabilistic neural-symbolic model for visual question answering. It can answer compositional questions about shapes in an image and provide interpretable explanations of its actions. The model outperforms a state-of-the-art neural-symbolic model on the SHAPES BID2 dataset, even with limited training examples. The proposed approach for visual question answering outperforms a state-of-the-art neural-symbolic model on the SHAPES BID2 dataset. The model involves planning and executing modular operations to answer questions, with the goal of providing faithful explanations on unseen questions. The model factorizes p(x, z, a|i) = p(x|z)p(a|i, z)p(z) to learn a more meaningful latent representation for questions by modeling uncertainity. The prior p(z) is learnt by training a neural sequence model on valid strings simulated from known syntax, capturing the notion that many differently-worded questions are reducible to identical underlying semantics. The proposed model captures the independence of questions from answers for an image given programs. To use the model for VQA, two tasks need to be addressed - learning model parameters and performing inference for a specific query. The training objective is the log-marginal-conditional-likelihood, which involves an intractable summation over latent programs. A variational approximation is used for inference, with an amortized inference neural network and a NMN model run on the image. The model proposed involves using a LSTM BID19 model for NMN and a recurrent network for mapping programs to questions. The objective includes maximizing approximate maximum likelihood training for distribution parameters. For data points with annotations, the likelihood of the program given the question is added to the objective. The overall objective combines these components. The goal is to learn the association of questions with programs and propagate them to other questions using a scaling factor \u03b1. At test time, questions for images are answered by computing p(a|i, x) = z p(a|i, z)p(z|x) using an approximation p q (a|i, x) \u2248 E zi\u223cq(z|x) p(a|i, z i ). Challenges include the optimization problem with a sequence of discrete tokens for the latent variable z and the conditional log-likelihood parameterized by a dynamically assembled NMN based on sample programs. The curr_chunk discusses the challenges of learning meaningful latent variable models in neural modules due to inconsistencies in program grounding and the difficulty of optimizing NMN parameters. The proposed training regime and adjustments to the objective aim to make optimization more tractable and enable training NMNs with reduced question-aligned program complexity. The proposed training regime aims to make optimization more tractable for neural modules by breaking ELBO training into three stages: question coding, module training, and joint training. The goal is to learn a code for questions in the latent space of programs, enabling training with reduced question-aligned program complexity. The coding process propagates supervised groundings to unsupervised questions, providing interpretable explanations of the model's actions. Recent theory suggests setting \u03b2 < 1 to avoid learning degenerate latent representations. The negative log-likelihood and KL divergence terms bound the mutual information between data and latent variables. Setting \u03b2 < 1 can lead to different models utilizing the latent variable in distinct ways to achieve desired behavior. The objective J(x; \u03c6, \u03b8) is optimized using the score function estimator with a moving average baseline. The KL term encourages high-likelihood sequences while penalizing syntactically invalid programs. In addition to penalizing syntactically invalid programs, the model applies REINFORCE on the entire sequence and computes rewards using the full monte carlo gradient. The proposed question coding formulation significantly improves results compared to the semi-supervised approach in BID26. The proposed question coding formulation improves performance compared to BID26 by optimizing the joint elbo with respect to neural module parameters. Beam search with a beam size of 1 is used during training to balance bias and variance tradeoff. The proposed joint training approach involves tuning question code and neural module network parameters together with scaling factors and a semi-supervised learning term. The gamma factor controls the likelihood scaling for answers relative to questions. The full training procedure is optimized using the monte carlo gradient. The full training procedure, outlined in Algorithm 1, discusses neuro-symbolic models and their connection to related work. It distinguishes itself by focusing on the integration of symbolic reasoning and neural networks for efficient learning. Our work focuses on interpretability in the low-sample regime, unlike other approaches that aim to get neural networks to perform symbolic manipulations or build hybrid models. We propose a model that learns how to execute programs by jointly modeling the program's output. This differs from other models that generate programs but do not capture the execution process. Our work focuses on interpretability in the low-sample regime by simplifying the mapping of image data and modeling a stochastic latent space. Our approach aims to preserve the semantics of task specification and execute programs on raw pixels, differentiating from prior work on visual question answering using neural module networks. Our work aims to simplify task specification semantics and execute programs on raw pixels for interpretability in the low-sample regime. We construct a latent variable model embedding previous work, focusing on preserving human-given lexicon. Other approaches prioritize interpretable question answering with minimal supervision. Exciting progress has been made in developing lower-variance estimators for discrete generative models. Our work focuses on simplifying task specification semantics and executing programs on raw pixels for interpretability in the low-sample regime. We introduce a latent variable model that incorporates previous work and emphasizes preserving human-given lexicon. Different from other approaches that prioritize interpretable question answering with minimal supervision, we aim to develop lower-variance estimators for discrete generative models. Variational Autoencoder Models. Our joint model assumes a discrete structured latent variable, does semi-supervised training, and is targeted towards neural module networks and question answering. In program induction, one learns to write programs given input-output pairs and optionally language or vision. The key difference between program induction and our problem is that program induction assumes grounded tokens/instructions, while we learn the execution engine and grounding from scratch. Our problem involves inducing programs conditioned on specific inputs with fewer samples than typical machine learning tasks. The SHAPES dataset BID2 contains 244 unique binary questions and 15,616 images. It is used for visual question answering with neural module networks and tests compositional generalization. The dataset has train, val, and test splits with no repeated questions, ensuring generalization across images. The dataset SHAPES BID2 tests for compositional generalization and reasoning in visual question answering. Questions range from length 4 to 11, with corresponding programs of length 4 to 6. Training involves limiting question-aligned programs to 5-20% of unique questions, ensuring unseen programs for 80% of questions during training. The question vocabulary is 14, and the program vocabulary is 12. Training the program prior involves using a set of 1848 unique programs simulated from syntax. Training the program prior involves using a set of 1848 unique programs simulated from syntax. Evaluating the effect of training the prior with varying-sized subsets of the 212 programs present in the training set shows that sequence-level policy-gradient based training can be highly variable. A stage-wise training regime is implemented for all models to reduce the impact of stochasticity. The module networks are trained jointly with the program predictor under different settings of \u03b3. Mean and variance accuracies on test are reported from this training regime over four random sets of supervisory programs. Evaluation metrics include question-coding reconstruction accuracy and program prediction accuracy on the training set. Module training and joint training focus on visual question answering accuracy by matching ground truth answers. The program z is treated as a variable length sequence of tokens with an end of sequence token. The program z is represented as a variable length sequence of tokens with an end of sequence token. The prior p(z) is learned through maximum likelihood training on simulated samples. Different approaches, such as using a generative model directly from validity annotations, are considered. The log p(a|z, i) term is parameterized by an NMN dynamically assembled from the program string. Sequence to sequence models based on LSTM cells are used for the generative model p(x|z) and inference (q(z|x)) terms. The NMN programs use syntax similar to previous work but with different neural modules. Objectives from a baseline approach are adapted for clear comparison. The V-NMN approach consistently improves performance in data-sparse regimes compared to the baseline LSTM + image model. Both methods show improvement with greater program supervision, but V-NMN outperforms NMN BID26 by achieving test accuracies over 20-35% points higher for >5% program supervision. However, both methods perform poorly on the test set with only 5% program supervision. Our V-NMN approach significantly improves initial program prediction accuracy compared to the NMN approach. Question reconstruction accuracy is closely linked to overall performance, showing the importance of supervision in guiding the mapping of structured latent variables. The limited supervision negatively affects NMN program prediction, with the 5% model resorting to simple structures. Mistakes made by the V-NMN model are also seen when reconstructing the question. High fidelity is observed between the learnt question space, answer space, and latent space. The impact of regularizing the program posterior to be close to the prior is explored. Without the KL term, the model focuses solely on reconstruction and fails to learn compositionality in the latent z space, impacting supervised grounding and program prediction accuracy. Beam search is used instead of sampling for module training to optimize Equation (5), with empirical studies supporting this design choice. In low-supervision settings, using sampling from the sequence model q(z|x) instead of beam search leads to a drop in module training performance. With more supervision, the choice of beam search or sampling does not matter as the confidence of the model increases with the improvement in correctness. In low-supervision settings, using sampling from the sequence model q(z|x) instead of beam search leads to a drop in module training performance. Better program representations for the problem can be found for \u03b2 < 1. VQA accuracy drops from 96.90% at \u03b2 = 0.1 to 78.49% at \u03b2 = 1. Question reconstruction drops from 97.43% at \u03b2 = 0.1 to 43.40% at \u03b2 = 1, showing auto-decoding behavior. N2NMN approach BID21 evaluates question-attention based module networks in the fully unsupervised setting, achieving 96.19% on TEST. However, the programs become non-compositional, making the modules less interpretable. The paper presents a novel probabilistic neural symbolic model for interpretable visual question answering, offering explanations of the model's actions on unseen questions with minimal annotated symbolic traces. The model utilizes programs as stochastic latent variables, allowing for effective sharing of statistics across questions and enabling annotations for known questions to propagate to unknown ones. This approach outperforms previous work on visual question answering, specifically neural module networks, on a dataset of compositional questions about shapes. The curr_chunk discusses the modeling choices and hyperparameters used in the sequence to sequence models and image CNN for visual question answering. The models are based on LSTM cells with specific configurations, and the images are processed by a convolutional neural network with specific filter sizes. The neural network uses 10x10 filters in the first layer with a stride of 10 and 1x1 convolution in the second layer with a stride of 1. The channel widths for both layers are 64 dimensional. A moving average baseline is used to reduce variance in rewards during training. The q(z|x) network is updated with a decay rate D for the baseline. Total derivatives are considered when estimating the gradient for the variational lower bound to account for the reward's dependence on variational parameters. The neural network uses 10x10 filters in the first layer with a stride of 10 and 1x1 convolution in the second layer with a stride of 1. A moving average baseline is used to reduce variance in rewards during training. The reward is estimated using q(z|x) and optimized with the ADAM optimizer. A set of samples is generated for training the program prior, followed by filtering based on constraints. The default prior p(z) is from programs. The default prior p(z) is from programs simulated from known syntax, but training the prior on unaligned, ground truth programs improves question coding performance. With 5% supervision, using the syntactic prior increases question reconstruction from 38.39% to 54.86% and program prediction accuracy from 56.23% to 65.45%. However, with more supervision, regularizing with the syntactic prior slightly decreases performance. Regularizing with the syntactic prior slightly hurts performance marginally, treating program supervision as a supervised learning problem in this setting."
}