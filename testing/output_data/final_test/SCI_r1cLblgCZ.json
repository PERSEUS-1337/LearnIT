{
    "title": "r1cLblgCZ",
    "content": "The recurrent auto-encoder model can summarize sequential data by encoding it into a fixed-length vector and then reconstructing it through the decoder. The fixed-length vector represents selected dimensions of time series features. A rolling fixed window approach is used to generate samples, showing the change of features over time as a smooth trajectory. Visualizations and unsupervised clustering techniques are applied to analyze the fixed-length vectors further. The proposed method can be used in large-scale industrial processes for sensor signal analysis, reflecting operating states. Research has focused on generalizing DTW to multidimensional levels, with studies on IoT, wearable sensors, and gesture recognition. In the neural network field, a recurrent auto-encoder model based on LSTM neurons has been proposed for learning video data representations. The reconstructed outputs of video clips show similarity based on qualitative examination. D' Avino et al. (2017) used an LSTM-based recurrent auto-encoder model for video data, learning intrinsic representations. High reconstruction error can indicate video forgery. Audio clips can be treated as sequential data, with BID2 representing variable-length audio data as fixed-length vectors using a recurrent auto-encoder model. Similar audio segments have nearby vector representations. Other works focus on time series data, such as BID12's recurrent auto-encoder model for fixed-length representation of univariate data. The auto-encoder model aims to provide fixed-length representation for time series data and was trained on labelled datasets to become a generic feature extractor. Dimensionality reduction via t-SNE shows that ground truth classification can be observed in the extracted features. Another study presented a time series compression algorithm using RNN encoder-decoder structure and an auto-encoder for higher compression ratio. A separate research used an auto-encoder model with database metrics to identify anomalies by setting a threshold on the reconstruction error. The recurrent auto-encoder model utilizes LSTM neurons with hyperbolic tangent activation in all recurrent layers to understand complex and time-dependent relationships in signal analysis. The RNN encoder reads an input sequence and summarizes information into a fixed-length vector, while the decoder reconstructs the original sequence. The role of the recurrent encoder is to project the input sequence into a fixed-length hidden context vector. Additionally, gated recurrent unit (GRU) neurons can be used for further improvement. The encoder in the recurrent auto-encoder model summarizes input information into a fixed-length vector using LSTM neurons. Regularization techniques like dropout can be applied to prevent overfitting, with dropout recommended for non-recurrent connections. The decoder reconstructs the original sequence from the context vector. The recurrent auto-encoder model uses an RNN structure to decode information and output a sequence of vectors. In a large-scale industrial system, only a subset of sensors is included in the output dimensions for partial reconstruction. The context vector summarizes the input sequence for end-to-end training. The relaxed auto-encoder allows the encoder to capture key variables relevant to the output dimensions. Multiple context vectors can be generated from different models using subsets of sensors, reflecting different system states. This partial reconstruction has practical significance for industrial applications. The relaxed auto-encoder captures key variables for output dimensions. Context vectors from different models reflect various system states, useful for industrial applications. Samples can be drawn consecutively from the dataset for diagnostic measurements in an industrial context. This method allows for generating samples from an unbounded time series, crucial for time-critical applications like sensor data analysis. The RNN encoder compresses sequential data into fixed-length vectors, leading to highly correlated consecutive context vectors. These vectors form smooth trajectories in space, with similar activation indicating similar underlying states. Dimensionality reduction techniques like PCA can visualize these context vectors in lower dimensions. The RNN encoder compresses sequential data into fixed-length vectors, which can be visualized using techniques like PCA. Unsupervised clustering algorithms can then be applied to assign context vectors to clusters, followed by supervised classification algorithms to learn the relationships between them. The trained classifier can be used for cluster assignment in validation sets and online settings, indicating changes in underlying states. Training samples are drawn using a windowing approach with fixed sequence length. The large-scale industrial system has 158 sensors, leading to a fixed sequence length of 36 for the recurrent auto-encoder model. The dataset was scaled into z-scores for gradient-based training. The model consists of three layers in both the RNN encoder and decoder, each with 400 neurons. Adam optimizer with a 0.4 dropout rate was used for training. The RNN model used Adam optimizer with a 0.4 dropout rate for training. The output dimensionality was set at K = 6, comprising key pressure sensors. Three scenarios were tested with different dimensionality settings. Training and validation MSEs were visualized in figure 2. The RNN model used Adam optimizer with a 0.4 dropout rate for training and had an output dimensionality of K = 6. Three scenarios were tested with different dimensionality settings, resulting in varying training and validation MSEs. The LSTM neurons with hyperbolic tangent activation struggled to compress-decompress high dimensional time series data in the first model with complete dimensionality (P = 158; K = 158). However, the model with partial reconstruction (P = 158; K = 6) showed significantly lower MSEs as it captured all relevant information via the RNN encoder. The recurrent auto-encoder model, available via the RNN encoder, captures relevant information such as lead variables. Selected samples in the validation set show similarity between original labels and reconstructed outputs. The model captures mean level shifts and temporal variations of output dimensions. Context vectors can be extracted for inspection, with successive vectors showing similar activation. Correlation matrix of context vectors can be visualized on a heatmap. The context vector c is a multi-dimensional real vector R. The model uses a context vector c in a multi-dimensional real vector space. Dimensionality reduction through PCA allows for efficient embedding in lower dimensions. A SVM classifier with RBF kernel is used for cluster assignment. Results are visualized in a two-dimensional space using PCA. The context vectors in a multi-dimensional space are visualized using PCA. Different clusters are identified with the K-means algorithm, and SVM decision boundaries are shown in the charts. The output dimensions are displayed on a shared time axis, with training and validation sets separated. The context vectors separate into two distinct neighborhoods, capturing shifts in mean values. Increasing the number of clusters captures more subtleties, with six clusters showing oscillations between close points. In the six clusters scenario, context vectors oscillate between close clusters, indicating generalizability to unseen data. A repeated experiment with different configuration (K = 158; P = 2) confirms robust representations. Output dimensionality K is changed to illustrate effects of partial reconstruction. Context vectors form a smooth trajectory. The context vectors form smooth trajectories in low-dimensional and high-dimensional spaces, allowing for the identification of distinct neighborhoods through unsupervised clustering algorithms like K-means. These clusters can be manually labeled to differentiate operating states, triggering alarms when vectors move beyond predefined boundaries. The study focuses on using context vectors to identify clusters in unlabelled time series data for diagnostics and maintenance in industrial systems. By relaxing the dimensionality of the output sequence, the recurrent auto-encoder can perform partial reconstruction, allowing users to define specific sets of sensors for analysis. This approach generates actionable insights and aids in diagnosing the industrial system. The proposed method utilizes a recurrent auto-encoder model for multidimensional time series clustering in industrial sensor data. It can effectively summarize unlabelled and unbounded time series data, offering insights for operating state recognition in multi-sensor processes. The proposed method uses a recurrent auto-encoder model for clustering multidimensional time series data from industrial sensors. It does not include categorical sensor measurements but focuses on real-valued measurements. The technical method described in the paper is the subject of a British patent application. The rotary components are driven by an industrial RB-211 jet turbine on a single shaft through a gearbox, with natural gas passing through low pressure and high pressure stages before reaching the desired pressure level. The suction scrubber removes condensate from the gas before it is fed through centrifugal compressors. The hot compressed gas is discharged from the compressor and its temperature is lowered via intercoolers."
}