{
    "title": "rJe2syrtvS",
    "content": "The text discusses the challenges of reinforcement learning in real-world scenarios and proposes a robotic system that can autonomously improve with real-world data. It addresses issues like lack of episodic resets, state estimation, and hand-engineered rewards, offering scalable solutions. The system's efficacy is demonstrated in dexterous robotic manipulation tasks in simulation and the real world, with an analysis of associated challenges. Reinforcement learning has the potential to enable autonomous systems like robots to acquire skills independently. Reinforcement learning can enable real-world autonomous systems, like robots, to acquire and improve skills continuously. However, challenges arise due to the mismatch between typical reinforcement learning assumptions and real-world constraints. Successful robotic learning experiments often require instrumentation to define reward functions and reset the environment between trials. Reinforcement learning algorithms face challenges in real-world deployment for robotic tasks due to unrealistic assumptions like access to low-dimensional state, known reward functions, and episodic resets. Continuous learning in real-world environments requires lifting these constraints to match real-world conditions. Significant human engineering is needed to implement assumptions for real-world reinforcement learning, limiting robots' ability to gather experience in various environments. Deep function approximators require large amounts of real-world data for effective generalization, but robots struggle to collect this autonomously. To address these challenges, robotic systems should be able to learn from raw sensory inputs and assign rewards. Robotic systems need to learn from raw sensory inputs and assign rewards autonomously to improve their ability to gather real-world experience. A learning system with these capabilities can enhance robotic agents' performance without extensive human intervention. This proposed system aims to continuously improve robots by leveraging their own experiences, reducing the need for manual resets and human engineering efforts. The integration of these capabilities presents a challenging learning problem that requires a detailed empirical analysis in both simulation and real-world settings. The proposed robotic learning system aims to enable robots to learn autonomously from raw sensory inputs, assign rewards, and operate without manual resets. Through empirical analysis in simulation and real-world settings, the system outperforms prior work in dexterous manipulation tasks. The combination of components and specific design decisions are novel and crucial for meeting the system's requirements. In the standard reinforcement learning paradigm, the goal is to learn a policy to maximize rewards in a Markov decision process. Real-world robotic learning requires continuous learning without instrumentation for state estimates, rewards, or resets. Real-world robotic learning necessitates obtaining information from the robot's own sensors for learning policies and rewards, without access to episodic resets. Building a robotic system for real-world learning requires the robot to learn from its own sensory observations, such as raw camera images and proprioceptive inputs, without the need for complex state estimation systems or extensive instrumentation. This presents unique challenges in combining components for a complete learning system in robotics. The proposed instrumentation-free system, R3L, allows learning from raw sensory inputs without explicit supervision or system instrumentation. This eliminates the need for resets, rewards, and state estimation typically required in traditional robotic applications of RL. The system leverages interaction with the environment to learn without providing object poses directly, requiring the learning system to extract such information. While many RL frameworks can support learning from raw sensory inputs in theory, practical considerations must be taken into account. In our work, we adopt the off-policy actor-critic reinforcement learning framework using the SAC algorithm, which can learn directly from visual inputs. However, SAC typically requires rewards and episodic resets, but when these assumptions are removed, it poses challenges for learning. Vision-based RL algorithms like SAC face challenges when reward functions are not hand-defined, requiring new techniques to address. In real-world environments, robots must extract reward signals from sensor readings, leading to manual and tedious processes to engineer computer vision systems or task-specific heuristics for obtaining rewards. To address challenges in real-world robotic learning, a system is needed that can assign rewards to itself with minimal human intervention. One approach is to have users specify desired behavior through example images, allowing the algorithm to assign rewards based on goal accomplishment. This scalable solution minimizes human engineering efforts and utilizes a data-driven reward specification framework called VICE. The VICE algorithm, introduced by Fu et al. (2018), learns rewards in a task-agnostic way by using success examples in the form of images to train a discriminator. This discriminator helps guide the reinforcement learning agent towards success. While previously considered for learning tasks from raw sensory observations, using VICE presents unique challenges when learning without episodic resets. Details of the algorithm can be found in Fu et al. (2018) and Singh et al. (2019). In real-world settings, building continuously learning RL systems without episodic resets is challenging. While algorithms like SAC theoretically don't need episodic learning, practical implementations often rely on explicit resets, leading to difficulties in solving complex tasks. RL algorithms struggle when applied to nonepisodic settings, hindering their scalability and autonomy in real-world learning. In real-world settings, building continuously learning RL systems without episodic resets is challenging. Simply applying actor-critic methods to the reset-free setting doesn't learn intended behaviors easily. Vision-based RL with actor-critic algorithms, vision-based goal classifier for rewards, and reset-free learning are fundamental components needed for a real-world robotic learning system. However, combining these components proves to be quite challenging in learning effective policies. Insights into these challenges are provided in Section 3, leading to proposed changes in Section 4 for effective and autonomous learning in the real world without human intervention. Figure 3 shows the object repositioning task. The system design outlined in Section 2 provides a complete system for real-world reinforcement learning without instrumentation. However, when applied to robotic problems, this design is found to be largely ineffective. Results from a simulated robotic manipulation task involving repositioning objects with a three-fingered robotic hand are presented. The goal is to reposition the object to a target pose from any initial pose in the arena. When the system is instantiated with vision-based SAC, rewards from goal images using VICE, and run without episodic resets, the algorithm fails to make progress. Experiments were set up to investigate the difficulty of the problem by combining varying observation types, reward structures, and the ability to reset. Training time rewards were compared under different combinations to achieve average training performance. In comparing training with true rewards vs. classifier-based rewards, with vs. without external resets, and from state vs. from vision on the object re-positioning task, it was found that learning without resets is more challenging, especially when combined with visual inputs. The policy performs better with low-dimensional state compared to image observations, indicating that combining reset-free learning with visual inputs significantly increases the difficulty of the task. When evaluating test-time performance of learned policies under reset-free conditions, it was found that learning from low-dimensional state achieved decent training rewards but poor test-time performance. This is attributed to limited data diversity due to the agent being stuck at the goal, affecting the efficacy of learned policies. Prior work suggests considering learning reset-controllers to address this challenge. In response to challenges with learning reset-controllers, the proposal suggests solutions for instrumentation-free reinforcement learning. Two key improvements are randomized perturbation controllers and unsupervised representation learning, essential for uninstrumented real-world training. These components, when incorporated into the system, enable learning in uninstrumented environments, as demonstrated in the results. Standard actor-critic algorithms are ineffective for reset-free RL, even with full state information. The policies trained with full state information struggle to learn the repositioning task effectively. Prior work has attempted to address this by using reset controllers, but they only succeed from a narrow range of initial states. In contrast, this proposal takes a different approach to learning in a reset-free setting. In a reset-free setting, the challenge lies in the narrow distribution of states visited by the policy, hindering learning. To address this, random perturbation controllers are used to increase state exploration. The policy alternates between standard and perturbation controllers to improve learning efficiency. The policy is trained with VICE-based rewards for desired goals, while the perturbation controller is trained with intrinsic motivation to explore under-explored states. The perturbation controller uses random network distillation for training, allowing policies to learn more effectively. This approach enables learning from various starting states, although learning from visual observations remains challenging. Learning from visual observations remains a challenge, but experiments show that learning without resets from low-dimensional state is easier. To address this, a variational autoencoder (VAE) is used to convert the vision-based learning problem into a state-based one, sharing the latent-variable representation across actor and critic networks. Incorporating unsupervised learning into reinforcement learning becomes critical in the reset-free setting, as shown in experiments. The experiments in Section 3 show that learning without resets from visual observations is challenging. To address this, a perturbation controller and joint training with unsupervised learning are used in the R3L system for real-world reinforcement learning. The method combines soft-actor critic with VICE for visual observations and introduces auxiliary reconstruction objectives for unsupervised representation learning. The primary contribution of this work is to propose a paradigm for continual instrumentation-free real-world robotic learning through reinforcement learning purely in the real world. This approach aims to avoid domain shift and extensive simulation efforts seen in prior works, focusing on training policies in the real world directly. Several algorithms have demonstrated the feasibility of real-world reinforcement learning, emphasizing the importance of training in diverse states for task accomplishment. The proposed approach aims to perform reinforcement learning in real-world environments with minimal human instrumentation, focusing on learning from raw visual inputs. This presents unique challenges not addressed in previous works, where hand-designed reward schemes and reset mechanisms were common. The difficulty of learning from raw visual inputs has been a hurdle for policy gradient algorithms, but advancements in modified objectives and more efficient algorithms have made progress in simulated domains. Reinforcement learning on raw visual input is challenging in nonepisodic, reset-free scenarios. Reward function design is crucial but difficult in real-world settings. Prior works used additional sensors, demonstrations, or interactive supervision for reward evaluation. This work leverages Fu et al.'s algorithm to assign rewards based on a goal classifier's likelihood, aiming for minimal human intervention in real-world environments. In contrast to prior works that require manual resets and access to ground truth reward functions, we propose an algorithm for automated reinforcement learning in real-world settings. Our method enables fully automated reinforcement learning in real-world settings, outperforming an ablation method using a reset controller. Unlike prior approaches focusing on robustness to perturbations or effective exploration, our goal is to learn without resets. This work is related to developmental robotics and lifelong learning, but our focus is on enabling RL systems to learn in the real world without instrumentation or interruption. Our work focuses on enabling RL systems to learn in real-world settings without instrumentation or interruption, with a particular emphasis on continual learning and sensory-motor development. The experimental evaluation assesses the R3L system's ability to learn complex robotic manipulation tasks under realistic conditions, without hand-specified rewards or resets. The study evaluates the R3L system's capability to perform dexterous manipulation tasks with a three-fingered robotic hand in simulated and real-world environments. Tasks include valve rotation, bead manipulation, and object repositioning without explicit resets or hand-specified rewards. The study evaluates the R3L system's capability to perform dexterous manipulation tasks with a three-fingered robotic hand in simulated and real-world environments. Tasks include valve rotation, bead manipulation, and object repositioning without explicit resets or hand-specified rewards. The tasks involve reaching specific goal configurations such as moving abacus beads, rotating valves, and repositioning objects. The evaluation compares the proposed system implementation with various baselines and ablations, all operating under the same assumptions. Additional details and videos can be found at the provided link. The study evaluates the R3L system's performance in dexterous manipulation tasks with a robotic hand. R3L outperforms other methods in reaching goal configurations without explicit resets or hand-specified rewards. The comparison includes SAC, VICE, Reset Controller + VAE, VICE + VAE, and R3L w/o VAE. R3L achieves the best performance across tasks, solving them effectively. Different prior methods and ablations fail for different reasons: methods without the reconstruction objective struggle at parsing the high-dimensional input and are unable to solve the harder task; methods without the perturbation controller are ineffective at learning how to reach the goal from novel initialization positions for the more challenging object repositioning tasks. An explicit reset controller learns to solve the easier tasks due to encouraging exploration of the state space. Performance in free object repositioning experiments varied across 3 choices of reset states, indicating high variance in evaluation performance. Our method does not require task-specific knowledge for choosing reset states, using random perturbations instead. This allows for robust, instrumentation-free training with fast convergence. R3L aims to enable uninstrumented training in the real world, showing generalization to real-world robotic systems without instrumentation. In the instrumentation-free setting, the robot learns behavior through interaction with goal images. Experiments are conducted using the DClaw robotic hand with an RGB camera as the only sensory input. Evaluations are done on saved policies after training, with success criteria for valve rotation and bead manipulation. Comparison shows our method outperforming a baseline in real-world scenarios. The design and instantiation of R3L, a system for real-world reinforcement learning, is presented. The system must learn from raw sensory observations, easily specified reward functions, and without episodic resets. Simple and scalable fixes are proposed to address unexpected learning challenges through unsupervised representation learning. The curr_chunk discusses the effectiveness of introducing unsupervised representation learning and a randomized perturbation controller in addressing challenges in training robots without instrumentation. This approach allows robots to learn autonomously in real-world environments, enabling them to collect large amounts of experience and acquire diverse behavioral repertoires. However, there are still challenges such as sample complexity, optimization difficulties, and safe operation. The curr_chunk discusses addressing challenges in real-world robotic learning, including sample complexity, optimization difficulties, safe operation, communication latency, and noise. The approach involves initializing networks, training a reward classifier, and collecting exploration data. The VICE classifier is trained with binary labels and rewards defined as logits, with experiments using 200 goal images normalized for each task. The approach involves training a standard beta-VAE to maximize the evidence lower bound, using random states in the observation space for training data. The system is evaluated across three simulated tasks: bead manipulation, valve rotation, and free object repositioning. The bead manipulation task requires positioning two beads on each end of an abacus rod from any initial configuration. The task involves manipulating beads on an abacus rod by sliding or splitting them, with the true reward being the mean goal distance of all four beads. Evaluation is done from 8 initial configurations, with the final reward averaged across evaluation rollouts. Another task involves turning a valve to a given orientation, with the true reward being the orientation distance. Evaluation is also done from 8 initial configurations. The task involves manipulating a three-pronged object with a claw, using an RGB camera for images and executing actions at 10Hz. Training is done asynchronously with a limit of two gradient steps per transition. Evaluations are conducted post-training, with success defined as the valve being within 15 degrees of the goal. Each policy is evaluated over 8 rollouts. The rod is 22cm long with beads measuring 3.5cm in diameter. Evaluations were done post-training with 8 specified configurations. Success was defined as beads within 2cm of goal positions. Policy achieved over 80% success after 20 hours. The evaluation rollouts on the valve rotation task using our method achieved high success rates after 5 hours of training. The results were saved at regular intervals and evaluated post-training. Each row represents a different policy, and each column an evaluation rollout from a different initial configuration. The goal is highlighted in yellow. This research is currently under review as a conference paper at ICLR 2020. The evaluation rollouts on the valve rotation task using the VICE single goal baseline showed inconsistent results, except when the initial configuration matched the goal configuration."
}