{
    "title": "SygxYoC5FX",
    "content": "Representation learning techniques on graphs have shown significant effects in downstream machine learning tasks. GraphSAGE, a framework proposed by Hamilton and Ying, is efficient for inductively learning representations for unobservable graph structures. However, it lacks selective neighbor sampling and memory of trained nodes. To address these issues, an unsupervised method is presented that samples neighborhood information attended by co-occurring structures and optimizes a trainable global bias for each node in the graph. Experimental results demonstrate that this approach outperforms state-of-the-art methods for representation learning on graphs. Representation learning on graphs is crucial for various real-world applications like social network analysis, molecule screening, knowledge base reasoning, and biological protein-protein networks. Learning low-dimensional vector embeddings of nodes in large graphs has proven effective for prediction and analysis tasks. Node embedding involves exploring high-dimensional information about a node's neighborhood with a dense vector embedding, which can be used for tasks like node classification and link prediction. While previous approaches can learn embeddings on graphs, they struggle to generalize to new nodes added to evolving networks like social circles on platforms such as Facebook. GraphSAGE is an approach proposed to generate node embeddings for new nodes in evolving networks like social circles on platforms such as Facebook. However, it randomly samples neighbors, making it difficult to explore the most useful ones. To address this issue, a supervised approach inspired by GAT is suggested to assign different importance to relevant neighbors and ignore irrelevant ones. Additionally, GraphSAGE focuses on training parameters but overlooks preserving the memory of trained nodes, leading to a waste of resources. To address the issue of randomly sampling neighbors in GraphSAGE, a bi-attention architecture is introduced for selective neighbor sampling in unsupervised learning scenarios. This architecture assigns different weights to neighbor nodes to ensure more relevant ones are selected, improving representation learning. The bi-attention architecture BID16 is stacked on representations aggregated from both sides in a positive node pair to learn the most relevant representations for each pair. A fixed-size uniform sampling method efficiently generates node embeddings in batches. Additionally, an additive global embedding bias is applied to each node's aggregated embedding to combine transductive and inductive approaches, providing a memorable global identification of each node in training sets. The BIGSAGE approach utilizes bi-attention architecture and global bias to learn node embeddings for large and evolving network data in an unsupervised and inductive manner. It focuses on exploring relevant neighbors and preserving learned knowledge of nodes. Various unsupervised learning approaches like DeepWalk and node2vec have been proposed for node embedding learning based on random-walks. GraphSAGE, inspired by previous methods like LINE and SDNE, aims to preserve first and second-order proximity in network embeddings. Unlike other approaches, GraphSAGE also considers node attributes and labels for embedding learning. It requires rich node attributes for sampling and aggregating embeddings that capture local neighborhood structural information. In response to challenges in large networks, inductive methods like Bojchevski & G\u00fcnnemann's work have shown impressive performance in graph embedding learning. Inductive learning in graph embedding focuses on learning from a single node and its local neighborhood, rather than the entire graph. Attention mechanisms, inspired by neuroscience, are used in deep learning for various tasks by focusing on relevant input information. This paper is inspired by attention mechanisms to construct a new approach. The paper introduces a hierarchical bi-attended sampling and global-biased aggregating framework (BIGSAGE) inspired by attention mechanisms for graph embedding. It includes algorithms for training and embedding generation, details on bi-attention architecture, and combining global bias within the framework. The framework operates on an undirected network with nodes connected by edges and an attribute matrix. Global embedding bias matrix and hierarchical layer number are defined for the embedding output. The hierarchical layer number K is defined for the embedding output, represented by h_k, and the final output embedding z. The unsupervised learning process involves applying a graph-based loss function similar to GraphSAGE, using random walk and negative sampling. The training of the BIGSAGE framework involves initializing parameters, calculating graph-based loss, and updating model parameters with SGD. BIGSAGE generates embeddings by rerunning random walks on a full graph, using a bi-attention mechanism to capture relevant information from positive nodes. The final embeddings are obtained by averaging the generated embeddings of each node, which are then used in downstream machine learning tasks. The bi-attention architecture in BIGSAGE involves running random walks on a graph to generate positive node pairs and calculating similarity matrices to find the most relevant neighborhood match. Attention is applied to the final encoded embeddings using a softmax function, resulting in an aggregation process illustrated in FIG1. The aggregation process with bi-attention architecture in BIGSAGE involves adding a trainable bias to the encoder's final output for each node in the training set. By training a global bias, the framework can learn aggregator function parameters for inductive learning and preserve embedding information for known nodes. Applying global bias vectors to all layers improves representation expressivity and training efficiency. The aggregation process in BIGSAGE involves adding a global bias to improve representation expressivity and training efficiency. It allows for direct updates to lower layers, impacting the loss function instantly. BIGSAGE is evaluated against strong baselines on three benchmark tasks. In our comparison experiments, we evaluate BIGSAGE against GraphSAGE using different aggregators for graph representation learning in an unsupervised and inductive setting. The bi-attention architecture and global bias effects are also studied separately. The comparison experiments evaluate BIGSAGE against GraphSAGE using different aggregators for graph representation learning in an unsupervised and inductive setting. Graph2Gauss and SPINE are also compared to show trade-offs in sampling granularity control and embedding effectiveness. Our approach utilizes a bi-attention layer with a sampling time of T = 10 and embeddings dimensionality set to 256. Implemented in Tensorflow, it is trained with the Adam optimizer at a learning rate of 0.0001. Comparison results are reported in TAB0. The task involves predicting the community of a post on Reddit using word2vec embeddings of post contents. Training is done on the first 20 days of data. The dataset contains 232,965 nodes(posts) with an average degree of 492. Our model shows slightly poorer performance compared to GraphSAGE due to differences in sampled neighborhoods. BIGSAGE outperforms GraphSAGE in mean and pool aggregator. Another evaluation on Pubmed dataset with 19717 nodes and 44324 edges shows better prediction results in all three. Our model outperforms GraphSAGE by 14% on the PPI data using the Mean-aggregator, with better prediction results in all three aggregators. The dataset consists of 24 graphs representing different human tissues, with 56944 nodes and 818716 edges. Each node has 50 features and 121 labels from gene ontology. The Mean-aggregator outperforms other methods in the study on PPI data. Different variants of BIGSAGE were tested, showing advancements over GraphSAGE but less accuracy than the original BIGSAGE. The use of bi-attention layer and global bias in different configurations were analyzed, highlighting the impact of these components on the model's efficiency. BIGSAGE demonstrates high efficiency with global bias across all layers. The model emphasizes the importance of memory for known nodes stored in global embedding biases. Comparison with Graph2Gauss and GraphSAGE on Pubmed and PPI datasets shows the superiority of BIGSAGE in evolving network data and generalizing across different graphs. The proposed approach preserves local proximity and learns global identities for seen nodes while generalizing to unseen nodes or networks. The BIGSAGE model utilizes a bi-attention architecture and hierarchical aggregating layers to capture relevant representations of co-occurring nodes. It combines inductive and transductive approaches by incorporating trainable global embedding bias in all layers. Experimental results show BIGSAGE outperforms state-of-the-art baselines on unsupervised and inductive tasks."
}