{
    "title": "Bkgz_krKPB",
    "content": "Large-scale pre-trained language model BERT has been successful in various language tasks, but its application in text generation is still a challenge. A new approach called Conditional Masked Language Modeling (C-MLM) is proposed to fine-tune BERT for text generation. The fine-tuned BERT serves as extra supervision to enhance Seq2Seq models for better text generation. This method outperforms Transformer baselines in experiments. The proposed approach outperforms Transformer on text generation tasks like machine translation and text summarization. It achieves state-of-the-art results on IWSLT German-English and English-Vietnamese MT datasets. Pre-trained language models like ELMo, GPT, and BERT have become essential for NLP tasks. BERT, pre-trained with deep bidirectional Transformer, has revolutionized language understanding tasks. However, applying BERT to language generation remains a challenge. Applying BERT to language generation is a challenge despite its success in language understanding tasks. The use of BERT for text generation is relatively unexplored, with potential benefits from its bidirectional contextual knowledge. However, the non-auto-regressive nature of BERT's Masked Language Modeling objective hinders its direct application to auto-regressive text generation. In this paper, a novel approach is proposed to distill knowledge from BERT for text generation tasks. A Conditional Masked Language Modeling (C-MLM) task is introduced to fine-tune BERT on a target dataset. The fine-tuned BERT is used as a teacher model to generate word probability logits for training samples, allowing a text generation model to learn from these outputs for improved performance. BERT's ability to look into the future acts as a regularization method for text generation models, capturing long-term dependencies for global coherence and improved performance. Our approach to leveraging BERT for text generation is modular and compatible with any model size or architecture. The main contributions include introducing sequence-level knowledge and conducting comprehensive evaluations on various text generation tasks. The proposed approach significantly outperforms strong Transformer baselines on text generation tasks like machine translation, text summarization, and image captioning. It achieves new state-of-the-art on IWSLT14 German-English and IWSLT15 English-Vietnamese datasets. Pre-trained language models like CoVe, ELMo, GPT, and ULMFit have been used for NLP tasks before BERT was introduced. In this study, the focus is on applying BERT to text generation, which is a relatively unexplored area. Previous work has shown promising results with BERT in tasks like cross-lingual natural language inference and unsupervised neural machine translation. Additionally, BERT has been formulated as a Markov Random Field LM for improved diversity in unsupervised text generation. In this study, the focus is on utilizing BERT for text generation, a novel approach that diverges from previous work. Zhang et al. (2019a) and Song et al. (2019) introduced innovative methods for unsupervised text generation, while Ghazvininejad et al. (2019) and Yang et al. (2019) explored different techniques for translation and NMT. The unique aspect of our approach is the use of BERT as a regularization tool in MLE training, enhancing prediction accuracy by incorporating future information. Our work also aligns with models that aim to balance left-to-right generative models with a right-to-left counterpart. Our approach diverges from previous work by utilizing BERT as a regularization tool in text generation. We focus on using pre-trained language models like BERT to enhance prediction accuracy in MLE training. This method aligns with models balancing left-to-right generative models with a right-to-left counterpart. Our approach diverges from previous work by utilizing BERT as a regularization tool in text generation. We focus on leveraging the knowledge learned from BERT for text generation in a generic sequence-to-sequence setting. This involves distilling the knowledge in BERT for text generation, rather than just focusing on model compression. Seq2Seq models learn parameters to estimate conditional likelihood P(Y|X) using attention-based RNN or Transformer. This framework is state-of-the-art for text generation tasks, with conditional probabilities modeled as classifications over the word vocabulary. During training, the model generates tokens based on partial sentences from the training data. A bidirectional model may offer advantages in this context. Bidirectional models like BERT can provide more context for text generation tasks compared to standard Seq2Seq models. To address the limitation of Seq2Seq models, a new conditional language model (C-MLM) is proposed to fine-tune BERT for better text generation. BERT is a deep bidirectional Transformer trained via Masked Language Modeling (MLM), where 15% of tokens are randomly masked during training. The trained BERT model aims to estimate the joint probability of masked tokens in sequence pairs X and Y. A conditional-MLM variant allows fine-tuning of BERT for text generation tasks like machine translation. This approach combines MLM pre-training with Seq2Seq learning to leverage bidirectional language models. The C-MLM objective trains the network to model joint probability, similar to conditional LM but allows predicting based on context. It differs from MASS pre-training and focuses on leveraging BERT for text generation. Knowledge distillation is used to transfer knowledge to a Seq2Seq model for text generation. The knowledge distilled from fine-tuned BERT into a Seq2Seq model for text generation utilizes probability distribution of masked words from both backward and forward contexts. This soft target approach enhances conventional generation models by providing sequence-level global guidance. In a knowledge distillation setting, BERT acts as a teacher while Seq2Seq model acts as a student. The Seq2Seq model is trained with a compound objective function that includes soft target estimation from fine-tuned BERT and hard-assigned labels. The approach has minimal requirements on the model architecture, with the aim of matching word probability distributions between the teacher (BERT) and the student (Seq2Seq). The Seq2Seq model can be trained with a minimal architecture requirement to estimate word-level probabilities. The additional loss term encourages future planning for generation. Experiments were conducted on machine translation tasks using small-scale and medium-scale datasets. For machine translation tasks, experiments were conducted on small-scale and medium-scale datasets such as IWSLT15 En-Vi and WMT14 English-German. Different preprocessing steps were followed for each dataset, and BLEU scores were reported for evaluation. Abstractive summarization experiments were done on the Gigaword dataset, with adjustments made to address distribution mismatch between validation and test data. The study conducted experiments on machine translation tasks using small-scale and medium-scale datasets like IWSLT15 En-Vi and WMT14 English-German. They reported BLEU scores for evaluation. For abstractive summarization, experiments were performed on the Gigaword dataset with adjustments made to address distribution mismatch between validation and test data. The implementation was based on the PyTorch version of OpenNMT seq2seq toolkit, using a 6-layer Transformer model with specific hyperparameters. BERT models were utilized for machine translation and summarization tasks. In the fine-tuning process of BERT, the pre-trained byte-pair-encoding is used for tokenization. The learning rate schedule follows a specific formula, and Adam optimizer is used for parameter updates. In the distillation stage, BERT's prediction logits are pre-computed using top-K distillation with a set value of K. Temperature tuning is also done for softmax applied at the teacher's logits. We use beam search with beam size 4 and length penalty during inference. Hyperparameters are tuned on the development set. Experimental results on machine translation task show significant improvement over the strong Transformer baseline. Our method improves over the Transformer baseline by 1.54 BLEU points for IWSLT German-English translation, achieving new state of the art. It outperforms ConvS2S+MRT and Lightweight and Dynamic Convolution models. For IWSLT English-Vietnamese translation, our model outperforms Seq2Seq-OT. Our method is model-agnostic and outperforms Seq2Seq-OT. Different from original KD, we do not apply the same temperature on the student due to high T of Seq2Seq resulting in worse performance. Parameter counts exclude word embedding and final linear projection. BERT-base has 86M trainable parameters. Our method improves over the well-tuned Transformer baseline for English-German translation. Our method outperforms the well-tuned Transformer baseline and the state-of-the-art Dynamic Convolution model on abstractive summarization tasks. The performance on the test set is lower due to corrupted examples, suggesting a different distribution from the validation set. Our results on the dev/test-dev sets are considered more reliable. Our best model, despite lacking a summarization-specific design, performs comparably to state-of-the-art models on all metrics. Factors contributing to this include additional parameters of BERT, extra data for pretraining, and its bidirectional nature. An ablation study is conducted to better understand the key contributions of our method. In an ablation study, two extra teachers, BERT sm and BERT l2r, were finetuned for C-MLM. Results show BERT sm still works well, while BERT l2r slightly hurts performance due to noisy learning targets. Bidirectional knowledge is deemed more important than extra parameters, with pre-trained weights useful for stable C-MLM training. The proposed approach shows higher BLEU scores on longer translation pairs for different datasets. The method performs better on longer output sentences, resulting in overall BLEU improvement. Translation examples on the IWSLT German-English dataset demonstrate the baseline Transformer's limitations in recovering certain words. The proposed approach utilizes pre-trained language models to improve text generation without explicit parameter sharing or auxiliary tasks. It shows higher BLEU scores on longer translation pairs and performs better on longer output sentences. The method addresses the baseline Transformer's limitations in recovering certain words, leading to more coherent and accurate text generation. Our Conditional MLM mechanism leverages unsupervised language models pre-trained on a large corpus and adapts to supervised sequence-to-sequence tasks. It improves text generation tasks like machine translation and abstractive summarization, achieving new state-of-the-art results on some translation tasks. Future work includes extending Conditional MLM to multimodal inputs like image captioning. Experiments were conducted on single GPU of NVIDIA Titan RTX or V100, with 4 V100s used for WMT En-De training. The hyper-parameters for fine-tuning the C-MLM model in different tasks involve training steps, warmup steps, learning rate, batch size, dropout rate, and specific parameters like \u03b1 and T. The experiments were conducted on GPUs like NVIDIA Titan RTX or V100. The hyper-parameters for fine-tuning the C-MLM model in different tasks include training steps, warmup steps, learning rate, batch size, dropout rate, \u03b1, and T. Experiments were conducted on GPUs like NVIDIA Titan RTX or V100. For example, in WMT En-De, the baseline model trains for 30k steps with a batch size of 384k tokens, while the proposed model continues training for 50k steps with a batch size of 64k tokens. In Gigaword, the baseline model trains for 50k steps with a batch size of 40k tokens, while the proposed model trains for 70k steps with a batch size of 36k tokens. The proposed model trains for 70k steps with 4k warmup steps and a batch size of 36k tokens. The learning rate is set to 2, \u03b1 = 0.1, and T = 10. Seq2Seq model uses dropout of 0.1. Examples of Gigaword summarization and En-DE generation are provided. Our model outperforms the baseline Transformer in generating coherent sentences. The Gigaword summarization dataset showcases qualitative examples where our model outperforms the baseline Transformer in generating coherent summaries. Please refer to Section 4.5 in the main paper for a detailed explanation."
}