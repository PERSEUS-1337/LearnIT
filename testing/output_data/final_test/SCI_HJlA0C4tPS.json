{
    "title": "HJlA0C4tPS",
    "content": "We present a deep generative model for unsupervised text style transfer that unifies previously proposed techniques. The model learns to transform sequences from one domain to another in an unsupervised fashion using a recurrent language model and an encoder-decoder. Amortized variational inference is used to compute a practical surrogate for the intractable marginal data likelihood. Connections are drawn between the variational objective and other unsupervised style transfer and machine translation techniques. Our probabilistic approach unifies unsupervised style transfer and machine translation techniques, demonstrating effectiveness across various tasks such as sentiment transfer, formality transfer, and author imitation. It outperforms state-of-the-art non-generative baselines and matches the current state-of-the-art in unsupervised machine translation. Unsupervised sequence transduction methods are gaining attention due to the scarcity of parallel corpora in natural language processing applications like machine translation, summarization, and dialogue response generation. Style transfer tasks, which modify text style rather than content, are particularly challenging to obtain parallel data for. In the context of unsupervised sequence transduction methods, style transfer tasks focus on modifying text style rather than content. Various tasks include formality transfer, author imitation, word decipherment, sentiment transfer, and language translation. Recent work in unsupervised text style transfer explores non-generative or non-probabilistic modeling approaches, such as adversarial discriminators and designing unsupervised training objectives for improved performance. Probabilistic models offer explicit assumptions about data and allow for reasoning during system design, reducing the need for heuristic approaches in unsupervised sequence transduction. Classical models like HMM or semi-HMM enforce strong constraints, while newer methods like backtranslation loss show state-of-the-art performance in tasks like unsupervised machine translation and style transfer. Powerful variational approximation techniques have enabled training probabilistic models without strong independence assumptions, leading to advancements in unsupervised style transfer. A new approach defines a generative probabilistic model treating non-parallel corpora in two domains as partially observed parallel corpora, with few independence assumptions and an intractable true posterior. Amortized variational inference is used for efficient computation. In utilizing amortized variational inference, a natural unsupervised objective emerges from the modeling approach, outperforming manually defined objectives in text style transfer tasks. The model also achieves state-of-the-art results in unsupervised machine translation. The model proposed focuses on style transfer tasks, such as formality transfer and sentiment transfer, using non-parallel corpora. It utilizes unsupervised learning to transform text from one domain to another while preserving content. The proposed model focuses on style transfer tasks using non-parallel corpora, treating them as partially observed parallel corpora. It introduces latent variables to complete the parallel corpus and aims to infer these variables conditioned on the observed non-parallel corpora. The model is referred to as a deep latent sequence model and includes unsupervised learning and inference techniques. The proposed deep latent sequence model focuses on style transfer tasks using non-parallel corpora by introducing latent variables to complete the parallel corpus. Unsupervised learning and inference techniques are used to model the joint likelihood of the complete data, allowing for text transduction with latent variables marginalized out during learning and inferred at test time. The joint likelihood of the data is approximated during training, with parameter sharing introduced through variational inference. An encoder-decoder architecture is used for transduction tasks, with recurrent language models as prior distributions for each domain. The model uses recurrent language models as prior distributions for each domain, avoiding strong independence assumptions. Exact inference is challenging due to the model's flexibility, addressed using amortized variational inference to derive a surrogate objective. Amortized variational inference is used to derive a surrogate objective for learning, the evidence lower bound (ELBO), which introduces inference network distributions to approximate true posteriors. The KL terms in the ELBO objective act as regularizers to bias induced sentences towards desired domains. Amortized variational techniques have been commonly applied to continuous latent variables, such as in the variational autoencoder (VAE). In the context of amortized variational inference, the approach is used for inference over discrete sequences, with the goal of transforming one domain to another. The inference network and generative distribution share parameters to represent two directions of transfer efficiently. In the context of amortized variational inference, the approach aims to transform one domain to another efficiently by sharing parameters between the inference network and generative distribution. Parameter tying is used to couple the learning problems for both domains and enable joint learning from the full data. Additionally, parameter sharing between the encoder and decoder is introduced for both directions of transduction, with a domain embedding specifying the transfer direction. Ablation analysis highlights the importance of parameter sharing for achieving good performance. In the context of amortized variational inference, parameter tying is used to couple learning problems between the inference network and generative distribution. Gradients are challenging to backpropagate through discrete samples, leading to the use of Gumbel-softmax and REINFORCE as stochastic gradient estimators. However, approximating gradients of the reconstruction loss remains difficult, with a stop-gradient baseline outperforming other estimators. As a result, gradients for the inference network from the reconstruction term are simply not computed. During training, the inference network gradients are obtained from the reconstruction term, with greedy sampling of latent sequences. Good initialization is crucial for successful optimization in unsupervised learning, as the encoder-decoder structure struggles with generating realistic sentences initially. A self-reconstruction loss is applied in the early epochs to address this issue. During training, a self-reconstruction loss L rec is applied in the initial epochs to improve sentence generation. The probabilistic formulation connects with recent advances in unsupervised text transduction methods, such as back translation loss for unsupervised machine translation and style transfer systems. In a supervised setting, back translation is viewed as a data augmentation technique and is naturally incorporated into the ELBO objective as the reconstruction loss term. Previous work has integrated pretrained language models into neural semi-supervised or unsupervised objectives, using them as rewards or adversaries. The ELBO objective connects with these LM regularizers by expanding the KL loss term. The objective includes an additional \"regularizer\", the negative entropy term. In experiments, the negative entropy term, \u2212H q, is used as a regularizer to prevent a peaked transduction distribution. This regularization helps avoid bad local optima and improves performance in unsupervised sequence transduction tasks. The model is tested on various style transfer tasks and general unsupervised machine translation benchmarks. The curr_chunk discusses machine translation models, including unsupervised machine translation (UNMT) and the effect of the negative entropy term in the KL loss term. It also mentions word decipherment and model configurations. The curr_chunk discusses word substitution in a cipher dictionary for enciphered corpus, evaluating difficulty levels with 100% word encipherment. It also mentions selecting the best model based on unsupervised reconstruction loss and evaluating with BLEU score on a test set. Sentiment transfer task involves paraphrasing a sentence with a different sentiment while maintaining original content. The curr_chunk discusses evaluating sentiment transfer difficulty using automatic metrics like classification accuracy, self-BLEU, and perplexity. It uses a convolutional classifier for accuracy assessment and an LSTM language model for perplexity calculation. The Yelp reviews dataset is utilized for testing, containing negative and positive sentences. The curr_chunk discusses formality transfer using the GYAFC dataset, which contains formal and informal sentences from the Entertainment and Music domain. The dataset includes parallel data for evaluation on sentiment transfer models. The curr_chunk discusses author imitation using a dataset of Shakespeare's plays translated into modern English. The model used is re-implemented for fair comparison with the original paper. The setting follows the formality transfer task on a parallel corpus. The curr_chunk discusses testing a method on related language translation tasks, including Bosnian and Serbian pairs, and evaluating unsupervised machine translation on the WMT'16 German English task. The BLEU-1 score is reported for the related language translation task, while comparisons are made with state-of-the-art models for unsupervised machine translation. The curr_chunk compares the performance of UNMT with other methods in unsupervised text style transfer tasks. Results show that UNMT outperforms prior methods, indicating the importance of flexible and powerful architectures. The improvements of UNMT highlight the importance of flexible architectures. The model outperforms UNMT in style transfer tasks, showing effectiveness and consistency. Additionally, the addition of a language model alone can be beneficial, but our method consistently outperforms this baseline. The PPL of system outputs under pretrained domain LMs is also evaluated. After evaluating system outputs under pretrained domain LMs, it was found that BT+NLL achieved extremely low PPL for sentiment and formality transfer tasks. The output contained repeated and overly simple phrases, resulting in low perplexity and high accuracy. In contrast, our system achieved competitive PPL, highest accuracy, and higher BLEU score than the UNMT baseline. Additionally, an experiment on word substitution decipherment task showed the importance of parameter sharing between two directions. Our method removed parameter sharing between transduction distributions, optimizing two encoder-decoder models. Despite achieving a low BLEU score, it showed a smaller gain over UNMT for tasks with divergent domains. The discrimination regularization effect of LM priors may be less important when domains are very different. Our method outperforms UNMT in preserving the content of the source sentence, especially in generating nouns. The F1 measure comparison shows our system's advantage across all word categories. In experiments, greedy decoding from the inference network is used to approximate the expectation for ELBO, aiming to reduce gradient estimator variance during training. A comparison between greedy and sample-based gradient approximations on sentiment transfer task shows sample-based training underperforms on both ELBO and task evaluations. Stopping gradients from propagating to the inference network from the reconstruction loss helps stabilize the training process. In this section, different methods for propagating gradients in sentiment transfer tasks are compared: Stop Gradient, Gumbel Softmax, and REINFORCE. These methods vary in how they handle the propagation of gradients from the reconstruction loss to the inference network. The stop-gradient trick produces superior ELBO over Gumbel Softmax and REINFORCE in sentiment transfer tasks, showing better optimization of the likelihood objective under a probabilistic formulation. This is attributed to reduced variance and a better balance of bias and variance overall. In sentiment transfer tasks, the stop-gradient trick outperforms Gumbel Softmax and REINFORCE by achieving a better balance of bias and variance. The attentional encoder-decoder architecture for UNMT, BT+NLL, and our method includes word embeddings of size 128, 1 layer LSTM with hidden size of 512 for both encoder and decoder, dropout with a rate of 0.3, max pooling over encoder hidden states, and a noise function for UNMT baseline in its denoising autoencoder loss. In hyperparameter tuning, varying pooling window size, decaying patience hyperparameter, and weight \u03bb for NLL or KL term is crucial. BT+NLL method tends to generate short sentences in sentiment transfer tasks. The baseline BT+NLL tends to generate overly simple and repetitive sentences in sentiment transfer and formality transfer tasks. Examples are shown in Table 7."
}