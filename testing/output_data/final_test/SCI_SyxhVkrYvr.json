{
    "title": "SyxhVkrYvr",
    "content": "Neural networks in Natural Language Processing are prone to being over-sensitive to small input changes and under-sensitive to deletions of large text fractions. This paper focuses on addressing under-sensitivity in natural language inference by preventing models from becoming overly confident when words are deleted. A novel technique using interval bound propagation (IBP) is developed to verify this specification for models based on the decomposable attention mechanism. Different training methods and metrics are compared to tackle under-sensitivity, with experiments on SNLI and MNLI datasets showing promising results. In experiments on SNLI and MNLI datasets, IBP training significantly improves verified accuracy. Adversarial samples reveal neural networks' sensitivity to text transformations, such as character flips and paraphrases. Deleting input text can increase model confidence, leading to under-sensitivity in predictions. Neural models can make accurate predictions on NLP tasks without understanding the text, relying on spurious cues in the data. This leads to strong performance on biased data but fails on samples without these cues. Identifying reduced inputs is challenging due to the vast space of possible text deletions. Previous methods like beam search or bandits are not guaranteed to find optimal solutions. In this work, the under-sensitivity issue in neural models is addressed by designing an undersensitivity specification that prevents models from becoming more confident when input words are deleted. Interval bound propagation (IBP) is used to efficiently cover the reduction space and verify the undersensitivity specification. IBP can be applied at test time to check for undersensitivity in model inputs. In this study, the focus is on verifying the behavior of the decomposable attention model (DAM) in natural language inference (NLI) tasks. The research compares different training methods to address under-sensitivity, including standard training, data augmentation, and adversarial techniques. The study aims to ensure models adhere to the undersensitivity specification using interval bound propagation (IBP) efficiently. The study focuses on verifying the behavior of the decomposable attention model in natural language inference tasks. It compares different training methods to address under-sensitivity and evaluates their effectiveness. The main contributions include formalizing the problem of verifying under-sensitivity, verifying the model using Interval Bound Propagation, and analyzing the efficacy of evaluation and training methods for developing robust models. The development of neural architectures for natural language processing tasks has been driven by the analysis of large-scale datasets. Research on NLP adversarial examples explores various types of attacks, including word and character-level perturbations, back-translation systems, syntactic and lexical transformations, and the introduction of task-specific adversarial attacks. Formal verification in NLP involves providing provable guarantees that models adhere to specified criteria. Various studies have introduced task-specific adversarial attacks in different NLP domains, such as Reading Comprehension, Machine Translation, NLI, and Fact Checking. Additionally, research has explored penalizing logical inconsistencies in NLI predictions, using background-knowledge guided adversaries, addressing under-sensitivity in dialogue settings, and demonstrating the link between excessive prediction invariance and model vulnerability in computer vision. Formal verification in NLP involves ensuring models meet specified criteria. Methods include exhaustive enumeration using Mixed-Integer Programming or Satisfiability Modulo Theory, and incomplete methods using convex relaxation. Complete methods are costly but conclusive, while incomplete methods are conservative and scalable. Verification methods in NLP aim to prove the truth of specifications, with recent work focusing on \u221e norm-bounded perturbations for image classification. This paper extends Interval Bound Propagation (IBP) to NLI, addressing the challenge of verifying models with discrete inputs. Recent studies have verified CNN and LSTM models against over-sensitivity adversaries in NLP, as well as output length specifications in machine translation models. Neural networks can exhibit under-sensitivity behavior, showing excessive prediction invariance to word deletions. Verification of large models remains a challenging task, with neural networks capable of fitting data in unexpected ways. In NLI, it is possible to delete a significant portion of premise words while maintaining or increasing prediction confidence. The specification addresses model output probabilities when parts of the input text are deleted by introducing a perturbation space X containing all possible reductions. It aims to verify if there exists a reduced input with higher probability for the prediction than the original input. The process of determining how prediction probabilities change when words are deleted is complex and prone to inconsistencies. The process of determining how prediction probabilities change when words are deleted is complex and prone to inconsistencies. It is important to be cautious and not too restrictive in the specification design, ensuring that whatever is specified is desirable. A conservative choice is to specify that prediction probabilities should not increase under arbitrary input deletion. Other specifications, such as decreasing certainty as more input is deleted, are worth considering. However, verifying even a conservative under-sensitivity specification is challenging for most inputs in the DAM model. Different approaches can be used to establish if the specification is satisfied, such as exhaustively evaluating all possible reductions with unlimited computational capacity or statistically sampling. Interval Bound Propagation (IBP) verification provides a formal guarantee by establishing outer bounds for X in (x nom) and resulting bounds on output probabilities. The Decomposable Attention Model (DAM) architecture includes word embeddings, attention, and feed-forward networks. IBP verification is used to efficiently assert whether an input satisfies Specification (1) in the DAM model. The DAM architecture efficiently asserts if an input satisfies Specification (1) by comparing embedded word sequences using a vector-valued function. The model outputs entailment labels for premise and hypothesis pairs. The DAM architecture compares embedded word sequences using a vector-valued function to verify input-output using Interval Bound Propagation. It computes attention masks to aggregate word vectors from two sequences and generates a single vector representation for each sequence. A logit vector is computed for each class using a feed-forward network. IBP is a verification method that tracks input-output relationships through a neural network using interval arithmetic. It bounds the activation of each layer with axis-aligned bounding boxes and evaluates the worst-case violation of the specification at the last layer. IBP is a verification method that bounds the activation of each layer in a neural network using interval arithmetic. It can be used for training and testing, but the output bounds may over-approximate the true image space, leading to false negatives. Keeping the bounds tight is crucial to minimize errors. IBP has been applied to MLPs and convolutional networks, and this work extends it to a model with an attention component. The text discusses verifying a specification for the DAM model by comparing upper probability bounds with predicted class probabilities. It also explores model behavior when removing single words at fixed positions and extending this to arbitrary multi-token deletions. The need for bounds on attention normalization is highlighted, with a focus on per-token upper and lower bounds. The text discusses verifying a specification for the DAM model by comparing upper probability bounds with predicted class probabilities. It also explores model behavior when removing single words at fixed positions and extending this to arbitrary multi-token deletions. The need for bounds on attention normalization is highlighted, with a focus on per-token upper and lower bounds. The model variables behave when an individual token at a fixed position is removed from one of the sequences, with a symmetric architecture allowing for the same derivation for the other input sequence. The text discusses the effect of word deletion on attention weights in the DAM model. When deleting a token at position r, attention weights P(B) are rescaled to account for missing normalization mass. The model computes convex combinations \u0100 and B, with unchanged elements in \u0100 but the r-th column removed, and unchanged dimensionality in B. When deleting a token at position r in the DAM model, attention weights P(B) are rescaled. The model computes convex combinations \u0100 and B, with unchanged elements in \u0100 but the r-th column removed. The dimensionality in B remains unchanged. Single-word deletions can be computed exactly without approximation, establishing upper and lower bounds for each element. These matrices are then fed into dense feed-forward layers G and H in the DAM architecture. After computing convex combinations \u0100 and B for single-word deletions in the DAM model, bounds on output logits are obtained by propagating these bounds through dense feed-forward layers G and H. Simplifications allow for direct bounds on the output vector v 2. The behavior of intermediate representations under deletions of multiple words is similar to individual word deletions. The DAM model establishes bounds for multi-word deletions by reusing single-word deletion bounds. These bounds are then input into a feed-forward network for IBP. Experiments on NLI datasets SNLI and MNLI show under-sensitivity for both premise and hypothesis reductions. The study evaluates the performance of the DAM model on NLI datasets SNLI and MNLI, showing under-sensitivity for both premise and hypothesis reductions. Various metrics such as Accuracy, Verified Accuracy, and Beam Search Heuristic are used to assess the model's performance. The study evaluates the DAM model's under-sensitivity on NLI datasets using metrics like Accuracy and Beam Search Heuristic. Training methods include Standard Training, Data Augmentation, and Adversarial Training to address under-sensitivity by deleting words systematically within the perturbation space. The study evaluates the DAM model's under-sensitivity on NLI datasets using metrics like Accuracy and Beam Search Heuristic. It compares random adversarial search with beam search for perturbations and picks the strongest violation. Altered samples are recomputed throughout training to mitigate under-sensitivity. IBP verification provides upper bounds on prediction probability for reduced inputs. The training methods involve penalizing the model with an auxiliary hinge loss based on upper bounds on prediction probability. Additional training loss is added besides standard log-likelihood, with experiments using various scales for the contribution. Training details include a learning rate of 0.001, Adam optimizer, batch size 128, and early stopping based on verified accuracy. Continuous phasing in of the perturbation space is found useful for verified training. Continuous phasing in of the perturbation space is crucial for verified training, gradually increasing the volume and radius. Experimentation with different phasein intervals and perturbation strategies did not improve verifiability results. IBP shows promising results for data verification, with a notable gap in accuracy compared to standard methods. IBP verification is effective when adding the IBP-verifiability objective during training, verifying a percentage of samples on SNLI and MNLI. However, tuning for verifiability can decrease test accuracy compared to standard methods. Computational efficiency of IBP verification is discussed in Table 2. The computational efficiency of IBP verification is highlighted in Table 2, showing a small overhead compared to standard forward passes. Training methods differ in adhering to under-sensitivity specifications, with standard training showing lower adherence. The study compares different training methods for adherence to under-sensitivity specifications. Data augmentation and random adversarial training show minimal improvements on SNLI and MNLI datasets. Beam search adversarial training improves SNLI verification rates but not for MNLI. Entropy regularization enhances verified accuracy. IBP-Training objective significantly boosts verification rates. The study evaluates various training methods for adherence to under-sensitivity specifications. IBP-Training objective notably enhances verification rates, especially for short sequences. Adversarial training shows closer verification rates to Beam Search Heuristic for short sequences, indicating potential for high verifiability but loose IBP bounds. The study highlights the challenges of under-sensitivity in model verification. Adversarial accuracy may not guarantee finding all violations, leading to false confidence. IBP verification offers guarantees but can have false negatives. Low verification rates are common in datasets with high sample complexity. Addressing under-sensitivity remains a significant challenge. The study discusses the challenges of under-sensitivity in model verification, emphasizing the limitations of current verification methods. It proposes using Interval Bound Propagation to verify the attention-based DAM model as a step towards verifying larger architectures like BERT. However, hurdles include BERT's network depth leading to looser bounds and its tokenization requiring special consideration for perturbations. The study investigates under-sensitivity to input text deletions in NLI and suggests formally verifying model behavior through IBP. The study explores under-sensitivity in model verification, focusing on the Decomposable Attention Model. Various training methods were compared for their effectiveness in addressing under-sensitivity, with IBP-training showing promise in improving verified accuracy."
}