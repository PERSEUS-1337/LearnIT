{
    "title": "BkxA5lBFvH",
    "content": "We study safe adaptation in real-world reinforcement learning scenarios, where models trained on past experiences must perform tasks in new situations without catastrophic failure. Prior experience can help learn models for cautious adaptation, leading to risk-averse domain adaptation (RADA) that trains probabilistic model-based RL agents in source domains to capture uncertainty. In a population of source domains, models capture uncertainty for cautious adaptation. A maximin policy accelerates domain adaptation in a safety-critical driving environment. Comparisons are made with meta-reinforcement learning approaches. An experienced human adapts to a new car by driving mindfully and observing its responses. Recent works have addressed the challenge of safe adaptation in autonomous systems, such as self-driving cars learning to drive in new environments. Training machine learning models without considering safe behavior under uncertainty can lead to real-world missteps and damage. Recent works have proposed methods for safe exploration during reinforcement learning to help agents avoid risky actions. One approach is to transfer notions of uncertainty and risk from prior experience in related domains, like simulated environments, to make learning safer. This idea is motivated by how a planetary rover could use its experience on Earth to perform cautiously on a new planet. RADA is a model-based reinforcement learning approach that pretrains a dynamics model on various training domains with unknown dynamics to estimate uncertainty. It selects cautious actions in new environments based on a maximin notion of risk-aversion, similar to a human driver navigating unknown terrain. The cautious phase of exploration in RADA involves feeding back collected information to finetune the model for new domains, leading to more confident predictions and lower risks over time. This approach enables fast and safe adaptation in only a few episodes, emphasizing worst-case outcomes in out-of-domain settings. In out-of-domain settings, safety in reinforcement learning is crucial for managing risks and reducing negative outcomes. Conditional Value at Risk (CVaR) is a commonly used measure in portfolio optimization to optimize the lower \u03b1-quartile of reward distribution. Meta-learning approaches like RL 2 may not effectively learn safety across learning episodes. In reinforcement learning, safety is addressed by adopting measures like CVaR to create robust policies for shifting from source to target domains. Rajeswaran et al. propose learning robust policies by sampling from an ensemble of models' \u03b1-quartile. This leads to more conservative policies that are robust to domain shifts. Model-based reinforcement learning offers an unsupervised learning signal that can be exploited at test time for quick adaptation to unforeseen circumstances. Prior work has shown that real robots can adapt to disturbances through meta-learning. This approach complements training for fast adaptation to disturbances in the environment, although it does not consider safety measures. In model-based reinforcement learning, policies can be regularized using uncertainty to encourage cautious behavior in unfamiliar environments. This approach aims to keep state trajectories within the training domain. Similar to collision prediction models favoring 'safe' collisions, the policy becomes less conservative as it adapts to new environments and reduces uncertainty. Domain randomization techniques are also utilized for adaptation to new environments. RADA pretrains on varied dynamics environments for safe adaptation, following a cautious action policy different from pretraining. Built upon PETS framework for model-based reinforcement learning, focusing on uncertainty and cautious behavior in unfamiliar environments. The PETS framework for reinforcement learning includes a probabilistic dynamics model that trains an ensemble of models, an action selection scheme using model-predictive control, and reward computation through autoregressive propagation. The PETS framework for reinforcement learning involves a dynamics model predicting state distributions and rewards for action sequences. Monte Carlo sampling and particle propagation are used to generate possible states after executing actions. The action sequence with the highest predicted reward is selected for execution. The Risk-Averse Domain Adaptation (RADA) approach focuses on safe learning by adapting to new environments with unknown dynamics. It involves training on various domains and transferring knowledge to new domains with different values of the domain ID variable z. For example, in the context of learning to drive cars, each car represents a unique domain ID. RADA, a solution building upon the PETS framework, addresses domain adaptation by capturing epistemic uncertainty in dynamics models for risk-averse settings. It utilizes probabilistic models and model-based RL agents to adapt to new environments without rewards or supervision. RADA, a solution based on the PETS framework, focuses on domain adaptation by incorporating epistemic uncertainty in dynamics models for risk-averse scenarios. It involves a pretraining phase with a probabilistic ensemble of models to capture uncertainty due to unknown factors, followed by an adaptation/finetuning phase for safe and efficient adaptation to new environments. During the pretraining phase, a probabilistic ensemble of models captures uncertainty in dynamics due to unknown factors. This uncertainty informs cautious exploration during adaptation in the target environment by using a maximin notion of cautious behavior. The action selection and reward computation scheme are adapted with a \"generalized action score\" controlled by a \"caution parameter\" \u03b3 \u2208 [0, 100]. During the pretraining phase, a probabilistic ensemble of models captures uncertainty in dynamics. The \"generalized action score\" is controlled by a \"caution parameter\" \u03b3 \u2208 [0, 100], which measures the mean score of the bottom 100 \u2212 \u03b3 percentile of predicted outcomes from the PETS model. A \"\u03b3-cautious action policy\" is defined based on this parameter. During pretraining, a probabilistic ensemble dynamics model is initialized, and a \"\u03b3-cautious action policy\" is defined based on the caution parameter \u03b3. This policy selects actions using the generalized action score R \u03b3 to perform safe adaptation in unknown environments, prioritizing caution over maximizing expected rewards. During pretraining, a probabilistic ensemble dynamics model is initialized with a \"\u03b3-cautious action policy\" for safe adaptation in unknown environments. The model is trained on domain D and evolves through evolutionary search stages to refine action sequences. The ensemble model is finetuned over time in the target environment using the \u03b3-cautious policy, improving its dynamics model without the need for a manually specified reward function. During adaptation, the model is kept close to the original model by maintaining a replay buffer of pretraining data. Model updates are computed on this dataset, leading to improved predictions over time. Eventually, the model converges to deterministic predictions, making the \u03b3-cautious action policy identical to the standard policy. The adaptation procedure in Algorithm 1 combines cautious action selection and model finetuning. Evaluating various ablations of RADA in a driving environment, we assess the importance of pretraining on multiple domains, replaying pretraining for stabilizing finetuning, and using a \u03b3-cautious action policy (\u03b3 > 0) during adaptation. RADA incorporates all three techniques and utilizes a PETS ensemble of 5 fully connected models with 4 layers each. The hypothesis is that a cautious approach not only aids in quicker adaptation by avoiding catastrophic failure but also helps resolve uncertainty about the environment during the adaptation process. Our study evaluates the RADA algorithm in a driving environment, comparing different variations such as RADA\\{caution,DR} and RADA\\{caution,replay}. We also compare RADA against PETS trained directly in the target environment and robust adversarial reinforcement learning (RARL). Additionally, three meta-learning approaches were implemented as baselines. In a driving environment based on Duckietown, various meta-learning approaches were tested but experienced training failures. The task involves making a right turn around a corner to reach a fixed goal tile. The environment is designed for sim-to-real transfer, with tiles fixed at a size of 0.585. If the car attempts to drive off the road tiles, it is held in place to prevent accidents. The task involves making efficient right turns around corners in a driving environment based on Duckietown. The agent must avoid hitting corners to receive rewards, with direct control over steering angle and velocity. The car width ranges from 0.050 to 0.099 during pretraining episodes. The car width ranges from 0.050 to 0.099 and is sampled uniformly before each training episode. Evaluation includes adaptation to different car widths: 0.075 (in-distribution) and 0.1, 0.125, 0.15, 0.175, 0.20 (out-of-distribution). Performance metrics measure return and collisions in the target environment. Safety is assessed by the cumulative number of collisions during adaptation. Results show that different methods vary in their ability to avoid catastrophic failures and ensure safe adaptation. RADA variants undergo pretraining for 32 episodes, while RARL requires 1000 episodes. Performance is evaluated based on average maximum reward and total collisions, with all methods performing worse as the car width deviates from the training range. RADA maintains high performance up to car width 0.2, with fewer collisions during adaptation compared to other methods. Cautious action selection is crucial for its success, while domain randomization and pretraining replay impact training speed and stability. Training directly on the target domain results in more collisions and does not yield the best performance. Pretraining on training car widths sets up a curriculum for training, making it easier for the agent to learn how to make the right turn. RARL policies are robust but perform worse than RADA, yielding lower rewards and more collisions. Results show adaptation speed in target environments based on average maximum reward and boundary collisions. Evaluation is done at various in-domain and out-of-domain car widths. RADA outperforms other approaches in adapting to different car widths, showing the least overall collisions except for car width 0.1. Multi-domain pretraining is valuable for slower adaptation. RADA with caution and replay leads to unstable training. The probabilistic dynamics model in RADA captures epistemic uncertainty. The dynamics model captures epistemic uncertainty related to unknown car width in Duckietown. Figure 5 visualizes how well the model predicts car trajectories for different car widths. The model predicts car trajectories with varying uncertainty based on different car widths, shown in Fig 5. Ground truth trajectories align closely with predicted trajectories, illustrating the model's ability to capture epistemic uncertainty. In some cases, aggressive actions lead to erratic behavior, causing the car to swerve off course. The dynamics model successfully learns to represent epistemic uncertainty in car trajectories, with bimodal distribution capturing different behaviors. RADA is a new approach for safe adaptation of RL agents in new environments, transferring knowledge and using risk-aversion during action selection. In a driving environment, RADA shows fast, safe adaptation to drive cars around corners. In preliminary experiments, RADA shows feasibility in adapting to drive cars around corners with a fixed number of pretraining environments. Results indicate success when the number of environments is limited. Results indicate that RADA is feasible with a finite number of pretraining environments. Trajectory predictions by the fully pretrained dynamics model improve over pretraining time, starting as random and becoming more accurate. During the stages of RADA pretraining, the model gradually learns to model epistemic uncertainty related to unknown car width."
}