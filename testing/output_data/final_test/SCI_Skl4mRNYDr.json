{
    "title": "Skl4mRNYDr",
    "content": "Imitative Models combine the benefits of Imitation Learning (IL) and goal-directed planning by using probabilistic predictive models to achieve specified goals. This method outperforms IL approaches and planning-based algorithms in a dynamic simulated autonomous driving task. Our approach to autonomous driving is efficiently learned from expert demonstrations and robust to poorly-specified goals. Unlike traditional imitation learning, we aim for flexibility in achieving general goals without explicit goal labels. Model-based reinforcement learning methods, on the other hand, do not require expert demonstrations and can adapt to new tasks specified through reward functions. Model-based reinforcement learning (MBRL) can adapt to new tasks specified through reward functions. The key drawback is that these models learn dynamics of possible behavior rather than desirable behavior, making designing reward functions challenging. Our goal is to devise an algorithm combining MBRL's flexibility with the advantages of imitation learning (IL) to achieve new tasks at test-time. Our method combines the advantages of MBRL and IL by leveraging MBRL's flexibility to adapt to new tasks at test-time and IL's ability to learn desirable behavior from offline data. We train a model to predict expert trajectories using a density function, then use probabilistic inference to generate plans for new tasks. This approach allows us to achieve complex goals without prior expert demonstrations, as demonstrated in a dynamic simulated autonomous driving task. Our method, deep imitative models, uses demonstrations to learn a probability density function q of future behavior for autonomous driving tasks. It generates expert-like plans with minimal reward engineering and offers flexibility to adapt to new tasks. Videos and more information can be found at https://sites.google.com/view/imitative-models. Our method, deep imitative models, generates expert-like behaviors with minimal reward engineering, flexibly incorporates new tasks, is robust to goal specification noise, and outperforms prior CARLA IL methods. We formalize assumptions and notation for modeling Partially-Observed Markov Decision Processes (POMDPs). The agent's state is denoted as s t \u2208 R D at time t, with \u03c6 representing all observations. We construct an \"Imitative Model\" q(S 1:T |\u03c6) to learn expert behavior from expert trajectories. The text discusses modeling expert behavior trajectories using a distribution of expert behavior. By training a model to forecast expert trajectories, the scene-conditioned expert dynamics can be captured. The goal is to direct the agent towards specific goals by defining tasks with goal variables. The probability of a plan conditioned on the goal is modeled using a posterior, with examples given after deriving a maximum a posteriori. Our approach involves generating expert-like plans to achieve abstract goals through gradient-based optimization. Different goal likelihoods guide the planning process, producing interpretable multi-step plans with scores for expertness and goal achievement probability. This method integrates mid-level and high-level controllers to determine how to reach desired goals. The approach integrates mid-level and high-level controllers by using demonstrations from both at train-time, with only the high-level controller available at test-time. The high-level controller's action specifies a subgoal for the mid-level controller, which learns a density model of future trajectories. The model imitates mid-level behaviors and is connected to an A* path-planning algorithm for goal likelihoods. This approach resembles offline IL for learning the midlevel controller and trajectory optimization for inference with an Imitative Model. Constraint-based planning to goal sets involves applying a Dirac-delta distribution on final states to constrain the agent's actions without introducing hyperparameters. By using a Final-State Indicator likelihood with a set of waypoints, this method outperformed previous dynamic-world CARLA approaches. Different goal sets can provide high-level task information to the agent, offering various ways to guide its actions effectively. CARLA methods involve providing line-segments or regions as goal sets to guide the agent's actions. The methods work well when the goal set contains \"expert-like\" final positions. Unconstrained planning allows for a goal likelihood with full support to encourage goals without constraints on the final state of the trajectory. The (D) Gaussian Final-State likelihood encourages the plan to arrive at a final state, either a single desired goal or a sequence of states. It can also specify desired end velocity or acceleration. This approach is useful when some final states are not reachable with an expert-like plan. The model allows imitation prior to be robust to poorly specified goals by incorporating cost-based knowledge at test-time. This can be combined with other goal-seeking objectives. Instantiating q(S|\u03c6) in CARLA is described, and designing general goal likelihoods can be seen as a form of reward engineering. Designing goal likelihoods is easier than designing reward functions because prior q(s|\u03c6) already models desirable behavior, making it unnecessary for manual tuning. This contrasts with model-free RL, which relies solely on reward design, and model-based RL, which heavily relies on reward design for goal-driven behavior. Goal likelihoods are easy to design when goals provide sufficient information, assuming at least one goal in the set is reachable within the model's time-horizon. In an autonomous driving application, the agent's state at time t is modeled as s t \u2208 R D. In an autonomous driving application, the agent's state at time t is represented as s t \u2208 R D with D = 2, indicating the agent's location on the ground plane. The agent utilizes environment perception \u03c6 \u2190 {s \u2212\u03c4 :0 , \u03c7, \u03bb}, where \u03c7 is a high-dimensional observation of the scene, and \u03bb is a low-dimensional traffic light signal. The featurized LIDAR observation \u03c7 = R 200\u00d7200\u00d72 provides a 2-bin histogram of points above and at ground level in a 0.5m 2 cell at a specific position. The model requirements include a deep imitative model that forecasts future expert behavior and the ability to compute q(s|\u03c6)\u2200s \u2208 R T \u00d7D for gradient-based optimization. The ability to compute \u2207 s q(s|\u03c6) enables gradient-based optimization for planning. R2P2, a generative autoregressive flow, is extended to instantiate the deep imitative model q(S|\u03c6). Designing q(s|\u03c6) is not the primary focus of this work, and other trajectory density estimation techniques could be used. Our method trains an imitative model from expert examples and repurposes it as an imitative planner for autonomous driving. A route planner provides waypoints to the imitative planner, which computes expert-like paths to goals. The best plan is chosen based on the planning objective and provided to a low-level PID-controller for steering and throttle actions. The trajectory distribution is complex and multimodal, utilizing neural networks for computation. The text discusses the development of a new model for autonomous driving that includes traffic light state information to improve decision-making. It utilizes neural networks for computation and planning, with three layers of spatial abstraction for route planning, path planning, and feedback control. The model is based on imitative models and aims to study autonomous driving scenarios. The text discusses a new model for autonomous driving that incorporates traffic light state information to enhance decision-making. It uses neural networks for computation and planning, with three layers of spatial abstraction for route planning, path planning, and feedback control. The model is based on imitative models and aims to study autonomous driving scenarios. The route planner outputs waypoints based on GPS navigation, but does not consider environmental factors like other vehicles. A goal likelihood is formed from the route and passed to the planner, which generates a state-space plan. The plan is then fed to a PID controller for steering, throttle, and braking. Pseudocode of the driving and inference algorithms are provided. The text discusses previous work on offline imitation learning in the CARLA simulator, focusing on Behavior Cloning algorithms. These methods perform well despite theoretical shortcomings. They use a high-level routing algorithm similar to the new approach, involving an A* planner generating waypoints. However, they use a Waypoint Classifier to classify waypoints into directives, such as Turn left, Turn right, Follow Lane, and Go Straight. The original motivation for these controls was to enable human direction of the robot. In autonomous driving scenarios, utilizing detailed spatial information from waypoints is crucial. Our approach, CIL-States (CILS), differs from prior methods by using identical inputs and PID controllers. It offers more flexibility post-training, can learn without goal labels, and generates interpretable trajectories. This contrasts with MBRL, which can plan with a predictive model but only represents possible dynamics. MBRL can plan with a predictive model but only represents possible dynamics. Crafting a proper reward function for expert-like behavior can be challenging and time-consuming. Our MBRL approach for CARLA uses a reachability tree over a dynamic obstacle-based reward function. It combines planning and imitation learning to capture a distribution over possible trajectories and plan trajectories at test-time. Our approach aims to generate expert-like plans with offline learning and minimal reward engineering, evaluated using the CARLA driving simulator. We strive to achieve state-of-the-art performance on CARLA by equating expert-like behavior with high performance. Our approach aims to achieve state-of-the-art CARLA performance using commonly available resources like waypoint-based routes, LIDAR, and traffic-light observations. We advocate for the use of such resources in other approaches. Additionally, we investigate the flexibility of our approach to new tasks by applying different goal likelihoods. Our approach investigates the flexibility to new tasks by applying various goal likelihoods and testing robustness to error in provided goals. Training is done on a dataset of 25 hours of driving in Town01, with test episodes starting in different positions in static or dynamic worlds. Goal sets are constructed from CARLA's waypointer output for Final-State Indicator. Our approach investigates flexibility in new tasks by applying different goal likelihoods and testing robustness to errors in provided goals. The goal sets are constructed from CARLA's waypointer output for Final-State Indicator, representing increasing levels of coarseness in direction. Three metrics are used to evaluate performance: success rate in reaching the destination without collisions, red-light violations, and time spent driving in the wrong lane or off-road. Results show comparisons with CILS, MBRL, and prior work in the CARLA simulator. Our method outperforms CILS, MBRL, and prior work in all settings, including static and dynamic worlds, training and test conditions. Goal Indicator methods perform well without hyperparameters. Using the light state to define different goal sets improves performance. The planner prefers closer goals when obstructed or at a red light, and farther goals when unobstructed or at a green light. Our method leverages expert behavior to reproduce it in new situations, affirming questions (1) and (2). It shows flexibility to different goal likelihoods and noise-robustness when the path planner is degraded. A \"decoy waypoints\" experiment tests the model's ability to stay within demonstrated behavior. In a \"decoy waypoints\" experiment, our method showed surprising robustness to distractions, executing dozens of planning rounds without catastrophic failure. Additionally, our model was tested under systemic bias by navigating with waypoints on the wrong side of the road. Our method demonstrated effectiveness in navigating with waypoints on the correct side of the road, even when increasing the hyperparameter value. Results showed a 48% success rate on Town01 Dynamic, indicating robustness to errors in goal-specification. Additionally, a pothole avoidance experiment tested the model's flexibility to test-time objectives. Our method incorporated a cost map to simulate potholes near waypoints, resulting in better avoidance of collisions with the environment. It drove closer to the centerline and occasionally entered the opposite lane to avoid potholes. The model demonstrated flexibility to obstacles not seen during training and internalized obstacle avoidance by staying on the road. The proposed \"Imitative Models\" combine the benefits of IL and MBRL, serving as probabilistic predictive models for planning. Imitative Models combine the benefits of IL and MBRL by enabling interpretable expert-like trajectories to achieve new goals. They can incorporate new goals and plan to them at test-time, circumventing the need for difficult reward-engineering and costly data collection. The model outperformed six IL approaches and an MBRL approach in a dynamic simulated autonomous driving task, showing robustness to poorly specified goals. This method is applicable in settings with expert demonstrations and demands flexibility to new situations. Future work could investigate methods to handle observation noise and out-of-distribution observations for robust real systems. Additionally, extending the approach to reason about all agents in the environment for a closed-loop plan could facilitate more general planning. Pseudocode for receding-horizon control, planning in the latent space of the trajectory, and details on speed-based throttle and position-based steering PID controllers are provided in the algorithms. Our implementation involves performing gradient descent on the objective in terms of z 1:T. We optimized N = 120 different z initializations in batch and returned the highest-scoring value. The planning procedure usually converges quickly with M = 10 optimization steps. We now derive an approach to optimize the main objective with set constraints. We derive an approach to optimize the main objective with set constraints by exploiting properties of the model and constraints to enable approximate optimization. Shorthand notation is used for brevity. We define a useful delta function for goal likelihood with set constraints. The corresponding maximum a posteriori optimization problem is derived by exploiting the properties of the model. By exploiting the properties of the model and constraints, closed-form solutions can be derived for optimizing the main objective with set constraints. This enables the application of gradient descent to solve the constrained optimization problem efficiently. Multiple point goals can define optional end points for planning, forming a goal-set as a line segment connecting two points. The solution for line-segment goals involves finding the point along the line that maximizes the likelihood subject to a specific constraint. The text discusses optimizing objectives with set constraints using closed-form solutions. It introduces the concept of multiple line-segments forming paths to follow, with examples shown. Additionally, it mentions using polygon regions to specify areas for the agent to go to, offering a closed-form solution and simple specification. The text discusses optimizing objectives with set constraints using closed-form solutions. It introduces the concept of line-segments forming paths to follow, with examples shown. Solving equations with a polygon has two cases: depending on whether \u00b5 T is inside or outside the polygon. The waypointer uses the CARLA planner's route to generate waypoints, without interpolating in constrained-based planning. In relaxed goal likelihoods, the route is interpolated every 2 meters, using the first 20 waypoints. The text discusses optimizing objectives with set constraints using closed-form solutions. It introduces the concept of line-segments forming paths to follow, with examples shown. The waypointer uses the CARLA planner's route to generate waypoints, interpolating every 2 meters and using the first 20 waypoints. A \"smart\" waypointer removes nearby waypoints closer than 5 meters from the vehicle when a green light is observed, and far waypoints beyond 5 meters when a red light is detected. Performance differences between methods with and without the smart waypointer are minimal, with the smart waypointer improving the vehicle's ability to stop for red lights. The text discusses optimizing objectives with set constraints using closed-form solutions. It introduces goal sets for Final-State Indicators, Line-Segment Final-State Indicators, and Polygon Final-State Region Indicators. Waypoints are fed into the Final-State Indicator for constrained optimization, including the vehicle's current position to allow it to stop. Gradient-descent optimization is used to move the plan closer to the nearest goal. The same procedure is applied to generate goal sets for Line Segment indicators, and ordered waypoints form a polygon for Final-State Region Indicators. The text discusses creating a polygon goal set surrounding waypoints for constrained optimization. The goal set includes Line-Segment and Gaussian Final-State Mixture indicators. The task is to drive to the furthest road location from the initial position, which is more challenging than previous methods. Visualizations of different goal likelihoods are provided. The architecture of q(S|\u03c6) is shown in Table 5. Before training q(S|\u03c6), a dataset was collected using CARLA's expert in Town01, consisting of over 900 episodes with 100 vehicles. The dataset includes 60,701 train, 7586 validation, and 7567 test scenes with past and future position information. Additionally, 100 episodes were collected in Town02. The conditional imitation learning baseline predicts setpoints for the PID-controller using scene observations and traffic light information. Effective for stable control on straightaways, but struggles with corners. A model-based reinforcement learning baseline learns a dynamics model from expert data for comparison. The model predicts vehicle actions based on expert data using an MLP with two hidden layers. It plans a reachability tree through free-space to the waypoint while avoiding obstacles, choosing the lowest-cost path near the goal. Steering angles are normalized and throttle is constant at 0.5 for planning over 20 time steps. The planned trajectory efficiently reaches the waypoint by expanding state nodes with available actions and avoiding obstacles. LIDAR images are converted into obstacle maps by expanding obstacles by the car's radius. A discrete temporal dimension is added to the search space for future time steps, with static obstacles remaining static and LIDAR points colliding with a vehicle replaced using a constant velocity model. Failure modes are attributed to inaccuracies in constant velocity prediction and the model's perception limitations. The model's failure modes include inaccuracies in constant velocity prediction and the inability to perceive lanes in LIDAR. Decoy waypoints experiment perturbs waypoints with a standard deviation of 8 meters, leading to confusion at intersections. Visualizations are provided, and the model's explicit likelihoods can be used to test plan reliability for real-world safety-critical applications. The experiment evaluated the model's ability to recognize safe and unsafe plans for real-world safety-critical applications like autonomous driving. Results showed 97.5% recall and 90.2% precision in detecting bad plans, indicating the effectiveness of imitative models in estimating plan reliability. A threshold on the planning criterion was determined for calibration. Intelligent calibration using information retrieval statistics on offline validation yielded good performance in classifying plans to different types of waypoints. The model showed a strong preference for valid waypoints, achieving 97.5% recall and 90.2% precision in classifying unreliable plans. The presence of observation noise and uncertain/out-of-distribution observations is a practical issue to consider. Addressing noise and uncertain/out-of-distribution observations in autonomous vehicles is crucial. Bayesian filtering can help estimate obstacle locations accurately, but high-dimensional filtering is challenging. Approximate Bayesian deep learning techniques like RNNs can be beneficial. Placing a prior over neural network weights can provide confidence in density estimation, especially for unfamiliar scenes generating uncertainty. This uncertainty can prompt cautious reactions. One way to address uncertain and out-of-distribution observations in autonomous vehicles is through an ensembling approach to detect when inputs are out of distribution. The model assumes access to a noisy traffic-light state, which was tested by simulating noise with a 20% probability. The study evaluated the effect of noise in the traffic-light state on CARLA's Dynamic Navigation task. Flipping the light state detector showed success most of the time but tended to violate red lights more often. The model's behavior near intersections was observed to be \"jerky\", alternating between stopping and non-stopping plans. It was suggested that adding noise in the training data could make the model more robust. The study evaluated the impact of noise on CARLA's Dynamic Navigation task. Noise in the light state degraded performance, but the method still operated successfully. Potholes were simulated by inserting them near waypoints in the environment. Root cost pixels were inserted in the cost map and blurred for visualization. The study involved tuning hyperparameters for unconstrained likelihoods using a binary-search procedure. Different values were set based on whether the prior or posterior dominated, with tighter or looser covariances as a result. This process was repeated for different experiments, with no hyperparameters needed for Constrained-Goal Likelihoods."
}