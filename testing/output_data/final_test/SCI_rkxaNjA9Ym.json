{
    "title": "rkxaNjA9Ym",
    "content": "The high computational and parameter complexity of neural networks hinders their training and deployment on energy and storage-constrained systems. Various techniques, including fixed-point implementation, have been proposed to reduce network complexity. However, a systematic approach for designing full fixed-point training and inference of deep neural networks is still lacking. A precision assignment methodology is described for neural network training, where all network parameters are assigned close to minimal precision. This methodology enables tracking the convergence behavior of full precision training, leading to a systematic way of determining suitable precision for fixed-point training. The text discusses the high computational and parameter complexity of deep neural networks, which leads to high energy consumption. A precision assignment methodology is proposed for training neural networks with minimal precision, reducing complexity. The methodology is validated empirically on various datasets, showing a reduction in complexity compared to other fixed-point designs. The high energy consumption of deep neural networks is due to their training via the stochastic gradient descent algorithm, which is slow and inhibits deployment on energy-constrained platforms. To address this, reduced-precision representations like quantized floating-point and fixed-point have been used in training and inference. Various approaches, such as fully binarized neural networks and 16-bit fixed-point tensors, have been employed to accelerate training and reduce complexity. The previous paragraph discusses the use of reduced-precision representations in deep neural network training to address high energy consumption. The current paragraph highlights the challenges in realizing true fixed-point DNN training, emphasizing the need for precision in computations to ensure accuracy and convergence. Various factors contribute to this gap, including the interdependence of variable precision and the dynamic range knowledge required for proper quantization. Our work proposes a systematic methodology to determine per-layer precision requirements for a fixed-point network, addressing challenges such as quantization noise, precision trade-offs, dynamic range, and stability. This approach aims to achieve statistical similarity with full precision training, building on the assumption of a fully-trained baseline floating-point network. Our methodology demonstrates the feasibility of quantizing all variables, including gradients, with unknown dynamic ranges. It also highlights how precision requirements vary across layers in neural networks, with techniques like weight binarization and gradient ternarization aiding in hyper-precision reduction. Our methodology showcases the effectiveness of fixed-point DNN training by achieving high fidelity to the baseline with minimal loss of accuracy. Precision assignment reduces training costs significantly compared to related works, demonstrating up to 6\u00d7, 8\u00d7, and 4\u00d7 reductions in representational, computational, and communication costs, respectively. The DNN quantization setup is detailed in Appendix A, defining constraints for fixed-point arithmetic in adaptive filters and signal processing systems. It includes signed and unsigned fixed-point scalars with precision and binary representations, determined by the quantization step size. An additive model for quantization is assumed. The quantization noise and bias in fixed-point arithmetic are defined by the relative quantization bias \u03b7 A and reflected quantization noise variance. The clipping rate \u03b2 T of a tensor T is also discussed using metrics inspired by previous studies. The metrics introduced by Sakr et al. (2017) and used by Wu et al. (2018a) are algorithmic in nature, making them easily reproducible. They include Representational Cost for weights (C W ) and activations (C A ), which measure the number of bits needed to represent weights, weight gradients, activations, and activation gradients. The goal is to find a minimal precision configuration for a FX network to minimize the mismatch probability between predicted labels. The quantization criteria aim to bound the mismatch probability between predicted labels and FL network labels by equalizing feedforward quantization noise, properly clipping gradients, limiting noise bias in weight updates, restricting activation gradient noise, and setting weight accumulator precision to avoid premature convergence. The quantization criteria aim to bound the mismatch probability by equalizing feedforward quantization noise, clipping gradients, limiting noise bias in weight updates, and setting weight accumulator precision. Further details and motivations are provided in the appendix. The claim ensures the satisfiability of quantization criteria for precision requirements, with closed form expressions provided. The validity of the claim is proven in Appendix C. The weight and activation gradients quantization step sizes are upper bounded based on the largest singular value of the square-Jacobian of G. The accumulator PDR and step size are determined by the smallest learning rate used during training. Practical considerations include feedforward precisions, quantization noise gains computation, and setting values for precision using backward signals. Estimates of second order statistics are also discussed. Estimates of second order statistics for gradient tensors are required for computing the Jacobian during training of large neural networks. The Jacobians are spatially averaged to reduce size, inspired by Batch Normalization. Numerical simulations are conducted to validate the approach. Numerical simulations were conducted to validate the precision configuration C o for probed Jacobians, using deep learning benchmarking datasets CIFAR-10, CIFAR-100, and SVHN. Various neural networks were trained on a Pascal P100 NVIDIA GPU, including CIFAR-10 ConvNet, SVHN ConvNet, CIFAR-10 ResNet, and CIFAR-100 ResNet with different architectures and configurations. The precision configuration C o for different neural networks is depicted in FIG4, showing dependencies on network type. Weight precision decreases with depth in ConvNets, consistent with earlier layers being more destructive. Weight perturbations in earlier layers are most destructive, while the precisions of activation gradients and internal weight accumulators increase with depth. The precisions of weight gradients and activations remain relatively constant across layers, with ResNets showing uniform precision. The gap between weight accumulators and other precisions is less pronounced in ResNets compared to ConvNets, suggesting information is spread equally due to shortcut connections. The analysis validates the choice of precision assignment C o by comparing it with perturbed configurations. Results show that increasing precision above C o leads to diminishing returns, while reducing precision below C o results in noticeable accuracy degradation. This supports the contention that C o is close-to-minimal. Additional experimental results in Appendix D further support this conclusion. The experimental results in Appendix D support the near minimality of precision assignment C o. The accuracy is most sensitive to the precision of weights and activation gradients. The study aims to quantify the reduction in training cost and accuracy resulting from the proposed method compared to other methods, using the same network architecture and training procedure. Different training methods are compared, including baseline FL training, FX training using C o, binarized network (BN) training, and the impact of quantizing specific tensors on accuracy. The study compares different training methods, including fixed-point training with stochastic quantization, ternarized gradients, and floating-point computations with ternarized weight gradients. The precision configuration is inherited from C o, and costs are computed assuming two bits for weight gradients. The comparison results are presented in TAB3. The comparison in TAB3 shows a significant complexity reduction when using fixed-point training compared to floating-point. This reduction is observed across different networks with minimal increase in test error. Binarization in fixed-point training eliminates multiplications, leading to lower computational costs. The accuracy of the CIFAR-100 network is even better with fixed-point training compared to the baseline. Binarization in fixed-point training reduces computational costs by eliminating multiplications. ResNets show less accuracy drop compared to ConvNets with binarization. SQ has similar costs to FX but with larger CW due to full precision accumulators. TG compresses weight gradients but does not reduce complexity. It has the lowest communication cost but slightly worse accuracy than other schemes. Various works have addressed the issue of reduced precision in deep learning. Reducing complexity in deep learning involves addressing precision assignment in the inference path of DNNs. Research efforts have focused on quantization and pruning techniques to improve efficiency, with some approaches incurring training overhead. Binary weighted neural networks have been explored as a subset of fixed-point training to reduce computational costs. In binary weighted neural networks, training with pre-determined precisions in the inference path was explored to reduce complexity. Stochastic quantization was employed to counter quantization bias accumulation in weight updates, with tensors quantized to 16-b FX. Our work circumvents the overhead of implementing stochastic quantization, distinguishing it from other approaches like DoReFa-Net. Our work introduces a systematic methodology for determining precision requirements for FX-only training of deep neural networks, avoiding the need for re-scaling and re-computation in floating-point format. Unlike prior works, we address the problem of predicting precision requirements and propose a new number format called Flexpoint for training neural networks with minimal precision requirements. In this study, precision requirements for FX-only training of deep neural networks were analyzed using quantization criteria. FX training was compared to FL baseline, showing complexity reduction gains in training costs. The precision assignment methodology was found to be nearly minimal, but initial baseline runs in floating-point can be costly. Future work can focus on predicting precision configurations based on data statistics and network architecture. Future work can utilize the analysis in this paper to improve network complexity reduction methods such as weight pruning, parameter sharing, and clustering. The quantization process described involves feedforward computation at each layer with activation and weight tensors quantized to specific formats. The quantization process involves activation and weight tensors at each layer being quantized to a fixed-point format with precision. Back-propagation of activation and weight gradients, along with internal weight accumulator updates, are also part of the process. The goal is to reduce network complexity through methods like weight pruning and parameter sharing. Criterion 1 (EFQN) ensures equal contribution of feedforward quantization noise sources to the budget. Applying this criterion avoids trial-and-error adjustments, addressing issues with overly quantized tensors. Clipping gradients is necessary for FX numbers due to their arbitrary dynamic range, with a preference for a small PDR to reduce quantization noise. This parallels signal processing theory's understanding of quantization effects. Criterion 2 (GC) addresses the trade-off between quantization noise and overflow errors in signal processing theory. Back-propagation training must avoid bias accumulation to prevent corruption of weight updates. FX quantization can induce bias when gradients are not uniformly distributed. Criterion 3 (RQB) uses \u03b7 to address this bias accumulation based on quantization step size. Criterion 4 (BQN) extends Criterion 1 (EFQN) for the back-propagation phase by setting the quantization step of activation gradients to minimize noise. Criterion 5 (AS) links feedforward and gradient precisions by adjusting weight accumulators based on weight quantization thresholds. This ensures that weight updates are not corrupted by quantization noise. The validity of Claim 1 is supported by five lemmas addressing quantization criteria. Lemma 1 states EFQN criterion is met with specific precisions and rounding operations. The quantization noise gains are crucial for satisfying EFQN. The quantization gains are determined by linearly expanding those used in a previous study. A second order upper bound is used as a surrogate for a certain expression. By defining the quantization step size, we can equate terms to a reference precision, ensuring integer values for precisions. Rounding operations are then applied to logarithm terms to complete the process. The GC criterion holds for \u03b2 0 = 5% if weight and activation gradients PDRs are lower bounded. Weight gradients follow a Gaussian distribution due to linearity of derivatives and Central Limit Theorem. Gradient mean oscillates around zero, leading to symmetric Gaussian distribution. The GC criterion holds for activation gradients with a larger PDR than weight gradients due to the sparsity of activation gradients caused by activation functions like ReLU. Increasing the PDR is recommended when using regularizers that sparsify gradients. The RQB criterion holds for \u03b7 0 = 1% if the weight gradient is provided. The RQB criterion holds for \u03b7 0 = 1% if the weight gradient quantization step size is upper bounded. The Central Limit Theorem (CLT) is invoked in the proofs of Lemmas 2 & 3 to approximate gradients averaged over mini-batches. The CLT is an asymptotic result that may be imprecise for a finite number of samples in neural network training. It is important to quantify the preciseness of the CLT approximation, especially with mini-batch sizes in the range of hundreds or thousands. The Berry-Essen Theorem provides an upper bound on the deviation of the cumulative distribution of the true average from the approximated Gaussian random variable via the CLT. To estimate the data-dependent quantity \u03c1\u03c33, a forward-backward pass was performed for all training samples in four networks. The maximum ratio found for all gradient tensors was 2.8097, with a mini-batch size of 256 used in all experiments. The CLT approximation in Lemmas 2 & 3 is valid with a worst case Kolmogorov-Smirnov distance of KS < 2.8097. Experiments in the main paper focus on 1-b perturbations to the precision configuration matrix. Further investigation into the minimality of C o and its sensitivity to precision perturbation per tenor type is shown in FIG3, including random fractional precision perturbations. The study investigates the impact of precision perturbations on test error, showing that negative perturbations have a more significant effect than positive ones. It also explores the sensitivity of different tensor types to precision reduction. The study explores the impact of precision perturbations on test error and the sensitivity of different tensor types to precision reduction. The most sensitive tensor types are weights and activation gradients, while the least sensitive are activations and weight gradients. This finding suggests a duality between forward propagation of activations and backpropagation of derivatives in terms of numerical precision. The methodology involves setting feedforward precisions and computing them accordingly. The study explores the impact of precision perturbations on test error and the sensitivity of different tensor types to precision reduction. The methodology involves setting feedforward precisions and computing them accordingly. E (min) = 94.7 and the feedforward precisions should be set according to specific guidelines. The spatial variance of gradient tensors is estimated using moving window averages, with a running variance estimate updated at each iteration. The study examines the impact of precision perturbations on test error and the sensitivity of different tensor types to precision reduction. The running variance estimate of each gradient tensor is recorded every epoch, and the PDRs of the gradient tensors are computed. The feedforward precisions are adjusted based on the findings, with specific guidelines followed."
}