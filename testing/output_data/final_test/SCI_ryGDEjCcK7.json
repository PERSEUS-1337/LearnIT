{
    "title": "ryGDEjCcK7",
    "content": "We introduce a new normalization technique that speeds up convergence by transforming layer weights instead of layer outputs. This method maintains a balance between positive and negative weights in the layer output. Validated on various benchmarks like CIFAR-10/100, SVHN, and ILSVRC 2012 ImageNet, normalizing layers in neural networks have played a significant role in the deep learning revolution. Batch normalization (BatchNorm) is a widely adopted technique in image classification tasks for its numerous benefits. Equilibrium Normalization (EquiNorm) is introduced as a new normalization technique that works in weight space and uses batch statistics. It results in rapid convergence but may lead to overfitting. When combined with additional regularization, EquiNorm can outperform BatchNorm. Several alternatives to batch normalization have been proposed, including batch renormalization BID3. Batch normalization has been extended as batch renormalization BID3 to handle smaller batch sizes. Layer/Instance Normalization involves computing statistics independently for each instance, avoiding averaging across mini-batches. Group Normalization averages statistics over small groups of channels, showing superior performance compared to layer and instance normalization. These techniques avoid dependence on batch statistics, allowing for potentially smaller batches to be used. Weight normalization introduces additional stability in neural network training by constraining the norm of weights for each output channel/neuron to be one. This is done through an explicit division operation in the forward pass, known as weight normalization. To match BatchNorm's generalization performance on tasks like CIFAR-10, weight normalization needs to be used together with partial BatchNorm. Local Response Normalization, a precursor to batch norm, was widely used before its introduction in architectures like AlexNet. EquiNorm modifies weights of a convolution before application, assuming positive inputs, stride one, cyclic padding, and non-zero weights. It aims to justify the method by making specific assumptions about the convolution. Equilibrium normalization balances positive and negative weights in a convolutional layer by modifying the weights through scalar quantities s and b. This transformation is fully differentiated during the backwards pass to ensure correct gradients. An additional affine transformation is included after the convolution to maintain expressivity. Equilibrium normalization balances positive and negative weights in a convolutional layer by modifying the weights through scalar quantities. Constraints are imposed on the mean of the output and the magnitude of positive weight elements to ensure balance. The constant r is set to achieve an average contribution of 1 per output element. Equilibrium normalization in a convolutional layer balances positive and negative weights by imposing constraints on the mean output and magnitude of positive weight elements. When using multiple input channels, weights only multiply inputs from a single channel, requiring per-channel sums. Similarly, when using multiple output channels, the procedure is applied to each channel's weights separately. Intermediate values are computed as matrix shapes, and a running estimate of data statistics is used at test time. Equilibrium normalization in a convolutional layer balances positive and negative weights by imposing constraints on the mean output and magnitude of positive weight elements. The statistics computed during training time involve a 3 \u00d7 3 kernel against a 3 \u00d7 3 image with padding 1, resulting in a magnitude of 9.0. An approximate computation method using only one pass is proposed to calculate the sum of positive elements. This approach controls covariate shift, similar to batch normalization, by maintaining the statistics of the outputs of a layer. Equilibrium normalization in a convolutional layer balances positive and negative weights by imposing constraints on the mean output and magnitude of positive weight elements. The approach proposed controls the shift in layer outputs by normalizing with different norms, such as L1, instead of precisely controlling mean and variance like batch normalization. This L1 control is motivated by Young's convolution inequality and can be effective when there is a single input and output channel. EquiNorm assumes positive input, normalizes input sum to L1 norm, and weights to 2w+. Young's convolution inequality applied with p=q=r=1 or p=2, q=1, r=2 for L2 norm. Different from WN, BID11 method normalizes weights by L2 norm and centers outputs with additional mean-only output. EquiNorm normalizes input sum to L1 norm and weights to 2w+. BID11 method normalizes weights by L2 norm and centers outputs with additional mean-only output batchnorm. Removing pre-normalization is recommended as it is unnecessary when normalization happens immediately inside the first convolution. When using Equilibrium normalization with strided convolution, the balance between positive and negative weights may be slightly disrupted. Most deep learning frameworks use zero-padded convolutions instead of cyclic padding, which can affect edge pixels but has a dampening effect. The normalization factor does not include a parameter like BatchNorm, as it only becomes zero if every weight for every input channel is zero. In practice, it is beneficial to initialize weights in a balanced fashion to avoid all positive or all negative channel weights. This is achieved by modifying the default initialization to resample any kernel-weight's signs. Comparisons were made with BatchNorm and GroupNorm, showing superiority over Layer/Instance Normalization. WeightNorm was also compared but had issues converging with deep architectures. The CIFAR-10 dataset is a standard benchmark for image classification tasks due to its complexity and tractable training times. Data augmentation techniques like random flips and crops are used to prevent overfitting. However, deep architectures like ResNet-152 may struggle to converge reliably. Initial experiments with a non-bottleneck wide ResNet network showed faster convergence with EquiNorm compared to BatchNorm, but also higher overfitting. This was attributed to batch statistics having less noise with EquiNorm. Larger step sizes with BatchNorm did not achieve comparable fast convergence. EquiNorm showed higher test accuracy (95.8%) compared to BatchNorm (94.7%) by using fewer instances for batch statistics or introducing activation noise with manifold mixup. Applying EquiNorm and BatchNorm to a larger ResNet-152 architecture also improved test accuracy. EquiNorm showed faster convergence compared to BatchNorm in training with 30 epochs using a modified version of their published code and hyper-parameters. The test set performance was similar at the final epoch, with EquiNorm achieving similar results on the CIFAR-100 dataset. EquiNorm achieved a 94.0% median test accuracy compared to BatchNorm's 93.3% on the SVHN+EXTRA dataset BID10, which is larger than CIFAR-10/100. Training was done across 2 GPUs with larger mini-batches to maintain batch statistics noise. EquiNorm showed similar generalization performance as BatchNorm, while GroupNorm appeared inferior due to default group size. The study compared EquiNorm, BatchNorm, and GroupNorm on the ImageNet classification task with ResNet50 architecture. Results showed a generalization gap between EquiNorm and BatchNorm/GroupNorm. Additional regularization did not eliminate this gap. Results were aggregated over multiple runs to account for variability. The study compared EquiNorm, BatchNorm, and GroupNorm on ImageNet classification with ResNet50. Results showed a generalization gap between EquiNorm and BatchNorm/GroupNorm, not eliminated by additional regularization. Median and inter-quartile statistics were used to represent performance variability, as two-standard-deviation bars can show inaccurate values for small samples."
}