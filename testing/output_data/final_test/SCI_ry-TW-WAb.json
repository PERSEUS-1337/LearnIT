{
    "title": "ry-TW-WAb",
    "content": "In this paper, a neural network preparation method for pruning and few-bit quantization is presented as a variational inference problem. A quantizing prior leading to a sparse posterior distribution over weights is introduced, along with a differentiable Kullback-Leibler divergence approximation. Training with Variational Network Quantization allows for replacing weights with quantization values without significant loss of task accuracy. Results are demonstrated for ternary quantization on LeNet-5 (MNIST) and DenseNet (CIFAR-10), showing reduction of redundancy in neural network parameters. The curr_chunk discusses methods for reducing redundancy in neural network models, including pruning and quantization techniques. Pruning involves removing irrelevant units, while quantization reduces the bit-precision of weights. These methods aim to maintain task accuracy while improving hardware efficiency. Reducing redundancy in neural network models involves methods like pruning and quantization, which aim to improve hardware efficiency while maintaining task accuracy. Quantization methods range from fixed bit-width computation to aggressive binarization of weights and activations. Few-bit quantization is often done through k-means clustering of trained weights, with ternary networks allowing for simultaneous pruning and few-bit quantization. Bayesian methods for network compression also play a role in identifying redundancies through a posterior distribution over network weights. Variational Network Quantization (VNQ) is a Bayesian network compression method that combines pruning and few-bit quantization of weights. It introduces a multi-modal quantizing prior to penalize weights of low variance unless they are close to one of the target values for quantization. This method aims to reduce parameter redundancy in neural network models. After training, the method yields a Bayesian neural network with a multi-modal posterior over weights, allowing for pruning and quantization. Posterior uncertainties are useful for network analysis and obtaining uncertainty estimates over predictions. The method results in a deterministic feed-forward neural network with heavily quantized weights, applicable to pre-trained networks or training from scratch. Target values for quantization can be fixed manually or learned during training, demonstrated on LeNet-5 (MNIST) and DenseNet for ternary quantization. Our method demonstrates ternary quantization on LeNet-5 (MNIST) and DenseNet (CIFAR-10), achieving a validation accuracy increase from 99.2% to 99.3% after 195 epochs. We initialize means from a pre-trained deterministic network and variances with log \u03c3 2 = \u22128. Post-training, weights with small absolute values or large variances are pruned, and remaining weights are quantized without loss in accuracy. This method extends recent work using a Bayesian objective for neural network pruning. The method discussed in this section involves using a Bayesian objective for neural network pruning. It focuses on learning dropout noise levels per weight and pruning weights with large dropout noise, utilizing Variational Dropout for this purpose. This approach aligns compression objectives with Bayesian inference, optimizing the evidence lower bound via stochastic gradient ascent and reparameterization tricks. Bayesian inference penalizes complex models, leading to automatic regularization effects. Sparse Variational Dropout prevents overfitting by maximizing model evidence. Complex models are penalized for having many parameter settings with poor likelihood, known as \"Bayesian Occam's Razor.\" This principle extends to variational Bayesian inference. The variational Bayesian objective in inference maximizes the evidence lower bound (ELBO) to minimize total message length, balancing data description and model complexity. MDL favors stochastic models for better compressibility. The use of stochastic models is favored for better compressibility in variational Bayesian inference, which models a distribution over parameters instead of a point estimate in Bayesian neural networks. This approach connects to rate-distortion theory and empirical Bayes by learning the prior from data. In Bayesian neural networks, the posterior distribution over parameters is approximated using methods like MCMC, stochastic gradient Langevin dynamics, variational approximations, Bayes by Backprop, and Probabilistic Backpropagation. The true posterior is intractable, so the approximation is optimized by minimizing the KL divergence through maximizing the evidence lower bound (ELBO). In Bayesian neural networks, the posterior distribution over parameters is approximated using methods like MCMC, stochastic gradient Langevin dynamics, variational approximations, Bayes by Backprop, and Probabilistic Backpropagation. The approximation is optimized by minimizing the KL divergence through maximizing the evidence lower bound (ELBO) using the Reparameterization Trick and Local Reparameterization Trick to reduce variance in the stochastic gradient estimator. The prior and posterior distributions are chosen to allow for analytical computation of the KL divergence term. In Bayesian neural networks, the posterior distribution over parameters is approximated using methods like MCMC, stochastic gradient Langevin dynamics, variational approximations, Bayes by Backprop, and Probabilistic Backpropagation. Dropout is a method for regularization of neural networks where activations are stochastically dropped during training. Gaussian dropout training is mathematically equivalent to maximizing the ELBO under a prior distribution. In Bayesian neural networks, the posterior distribution over parameters is approximated using methods like MCMC, stochastic gradient Langevin dynamics, variational approximations, Bayes by Backprop, and Probabilistic Backpropagation. Dropout is a method for regularization of neural networks where activations are stochastically dropped during training. Gaussian dropout training is equivalent to maximizing the ELBO under a prior distribution. The scale invariant log-uniform prior is used to learn individual dropout rates per weight, known as \"Variational Dropout\". The KL term in the ELBO is approximated in the original publication. Learning dropout rates is crucial for network compression as high dropout rates can lead to pruning without loss in accuracy. The original variational dropout paper limited values to \u03b1 \u2264 1, which is not suitable for pruning. An improved approximation is proposed with parameters k1 = 0.63576, k2 = 1.87320, and k3 = 1.48695, accurate across the full range of log \u03b1. Additionally, an additive approach is suggested for better results. The authors propose an additive noise reparameterization to reduce variance in the gradient during Sparse VD training. Pruning is done by thresholding weights based on log \u03b1 values, leading to effective removal of weights with large variance or close to zero mean. Visualization of the pruning threshold is shown in Fig. 1. Sparse VD training can be performed from random initialization or with pre-trained networks by initializing the means \u03b8 ij accordingly. Group-sparsity constraints allow for pruning of whole neurons or convolutional filters. Variational Dropout introduces a \"high dropout rate constraint\" via the implicit prior distribution over weights, inducing sparsity into the posterior. Scale-mixtures of normals are used to achieve this sparsity. The spike-and-slab prior, log uniform prior, and normal-Jeffreys prior are discussed in relation to Dropout training and Bayesian Compression. The log-uniform prior is seen as a continuous relaxation of the spike-and-slab prior, with implications for neural network preparation for post-training quantization. The text discusses using a multi-modal quantizing prior in a neural network for post-training quantization, treating it as a variational inference problem. The goal is to achieve soft quantization by learning a posterior distribution that minimizes accuracy loss. The algorithm involves maximizing the ELBO under a mean-field approximation of the posterior, with a quantizing prior driving weights towards target values. This approach is a continuous relaxation of a \"multi-spike-and-slab\" prior, with multiple spikes at specific locations. The text discusses using a \"multi-spike-and-slab\" prior in neural network quantization, where weights are driven towards target values for quantization. Pruning can be achieved by setting certain targets to zero and using a threshold for weight variance. This approach allows for significant weight reduction without loss in accuracy. In neural network quantization, a multi-spike-and-slab prior is used to drive weights towards target values for quantization. Pruning is done by setting targets to zero and using a threshold for weight variance, allowing for significant weight reduction without accuracy loss. The log uniform prior is interpreted as a marginal over the scale-hyperparameter, with a hyper-prior over locations and a mixture of weighted delta distributions. Marginalizing over locations yields the quantizing prior. In neural network quantization, weights are pruned based on a threshold for weight variance, leading to significant reduction without accuracy loss. A post-training quantization scheme assigns weights the quantized value with the highest likelihood under the approximate posterior, minimizing the squared distance between the mean and quantized values. Pruning can be increased by setting weights exceeding the threshold to zero before quantization assignment. The KL divergence from the prior to the mean-field posterior is considered under the quantizing prior. The KL divergence from the prior to the mean-field posterior is approximated using a differentiable approximation to keep computational effort low during training. The approximation is based on a mixture of shifted versions of F KL,LU to handle multi-modal quantizing priors. The KL divergence from the prior to the mean-field posterior is approximated using a mixture of shifted versions of F KL,LU with \u03b8-dependent Gaussian windowing functions. The approximation has a maximum absolute deviation of 1.07 nats from the ground-truth and can be reused for any symmetric ternary codebook. Detailed quantitative evaluation is provided in FIG4 in the Appendix. The KL divergence from a prior based on the codebook c a to the posterior q \u03c6 (w) is approximated by F KL (\u03b8/s, \u03c3/s, c r ). This allows learning the quantization level a during training. In experiments, weights are pruned via thresholding before being quantized to minimize the squared distance to the quantization values c k. Training stability is improved by using warm-up BID43, ramping up the KL divergence term factor \u03b2 from 0 to 1. To improve stability during training, clipping is applied to ensure specific ranges for log \u03c3 2 ij and \u03b8 ij. Gradient-stopping is used to prevent weights from getting stuck at boundaries, leading to better accuracies. Learning codebook values involves weighting functions to form the final approximation F KL. Our method involves using a lower learning rate for adjusting the codebook to prevent codebook values from collapsing during early stages of training. We demonstrate our approach with LeNet-5 on the MNIST dataset, pre-processing images by subtracting the mean and dividing by the standard deviation. Training the pre-trained network involves running 5 epochs with Glorot initialization and Adam optimizer, achieving a validation accuracy of 99.2%. Means are initialized with pre-trained weights and variances with log \u03c3 2 = \u22128. The warm-up factor \u03b2 increases linearly from 0 to 1 over the first 15 epochs. VNQ training lasts 195 epochs with a batch-size of 128, decreasing the learning rate from 0.001 to 0 linearly. Results are presented in Table 1 and a weight distribution visualization in Fig. 1. VNQ training effectively prepares a network for pruning and quantization with minimal accuracy loss, eliminating the need for fine-tuning. Our method achieves lower pruning rates compared to methods that do not consider few-bit quantization in their objective. Few-bit quantization limits network capacity, suggesting the need to prune fewer weights. Our pruning rates align with other papers on ternary quantization, with sparsity levels between 30% and 50%. The text discusses validation errors, non-pruned weights percentage, and bit-precision per parameter in the context of different training methods such as VNQ, pruning, and quantization. It also mentions the challenges in comparing pruning, quantizing, and ternarizing methods due to various factors influencing compression rates. The text discusses the training procedure for a DenseNet BID23 on CIFAR-10, including batch size, KL term weight, learning rate schedule, and training duration. After pre-training a DenseNet with a validation accuracy of 93.19%, the codebook parameter for non-zero values is initialized with the maximum absolute value over pre-trained weights per layer. Results in Table 2 show lower sparsity levels for DenseNet compared to LeNet, possibly due to DenseNet's optimized architecture. The first and last layers are most sensitive to pruning and quantization, but the complete network can be pruned and quantized with minimal additional loss after sufficient training. Our DenseNet model (L = 76, k = 12) consists of an initial convolutional layer, three dense blocks, and a final classification layer. Transition layers are placed in-between dense blocks. The network can be pruned and quantized with minimal loss in accuracy. After training the DenseNet model, pruning and quantization were applied to reduce the percentage of non-pruned weights and bit-precision per weight. Results showed that quantizing weights with low variance in the first layer led to a drop in accuracy, especially without gradient stopping. Pruning and quantizing the first layer without gradient stopping resulted in a more significant decrease in accuracy. Our method extends Sparse VD for network pruning by using a quantizing prior, leading to a multi-modal posterior suitable for fewbit quantization and pruning. Bayesian Compression and Structured Bayesian Pruning extend Sparse VD to prune whole neurons or filters via groupsparsity constraints. Bayesian Compression determines bit-precision per layer via posterior variance but does not enforce clustering of weights during training. Extending our method to include group-constraints for pruning is a potential future direction. Our method extends Sparse VD for network pruning by using a quantizing prior, leading to a multi-modal posterior suitable for fewbit quantization and pruning. Another Bayesian method for simultaneous network quantization and pruning is soft weight-sharing (SWS), which uses a Gaussian mixture model prior. SWS acts like a probabilistic version of k-means clustering with the advantage of automatic collapse of unnecessary mixture components. BID10 learn dropout rates by using a continuous relaxation of dropout's discrete masks via the concrete distribution. Our method utilizes a quantizing prior to extend Sparse VD for network pruning, resulting in a multi-modal posterior suitable for few-bit quantization. Trained Ternary Quantization BID53 achieves impressive accuracy but has low sparsity rates. The KL divergence approximation in our method may benefit from improvement for tighter weight distribution. There is room for improvement in the KL divergence approximation for tighter weight distribution after quantization and pruning. Differentiable approximations like neural networks or polynomial regression could be explored to enhance the current approximation method. In our experiments, the MC approximation achieves comparable accuracy with higher pruning rates compared to the functional KL approximation. However, with DenseNet on CIFAR-10, the MC approximation validation accuracy drops significantly after pruning and quantization. Our pruning rates are lower compared to similar methods, possibly due to heavily quantized networks having lower capacity than full-precision networks. Heavily quantized networks have lower capacity than full-precision networks, leading to lower pruning rates. Increasing the number of neurons with binary weights per layer can prevent drops in accuracy. Future work could explore the trade-off between low bit-precision and sparsity rates by testing more quantization levels. The Sparse VD method is used with the Local Reparameterization Trick and Additive Noise Reparametrization to optimize the stochastic gradient variational lower bound. We optimize posterior means and log-variances (\u03b8, log \u03c3 2 ) and the codebook level a using Variational Network Quantization for fully connected and convolutional layers. The KL divergence under the quantizing prior is analytically intractable, so an approximation is presented for the KL divergence under a log uniform prior. The quantizing prior is a composition of shifted log uniform priors, and an approximation is constructed for it. The original approximation is used to calculate a KL divergence approximation from a shifted log-uniform prior to a Gaussian posterior. The KL divergence is dominated by the mixture prior component at the quantization level. The approximation can be shifted to the quantization level for certain values of posterior variances and means. The KL approximation is constructed by mixing shifted versions of the original non-shifted approximation using Gaussian window functions for weighting. The quality of the KL approximation is evaluated by comparing it to a ground-truth Monte Carlo approximation. Results of this comparison are shown in a figure. The quality of the functional KL approximation is assessed by comparing it to a naive Monte Carlo approximation. The ground-truth Monte Carlo approximation uses many more samples, which is computationally expensive. The maximum absolute error between the functional KL approximation and the ground-truth is 1.07 nats. The functional KL approximation is compared to a naive Monte Carlo approximation, with results showing differences in validation error on MNIST and CIFAR-10 datasets. The naive MC approximation leads to higher error rates after pruning and quantizing the network on CIFAR-10. The validation error is compared before and after pruning and quantization, showing differences in weight distribution with the naive MC approximation for the KL divergence. The effects of missing local reparameterization and single-sample MC approximation lead to noisier gradients. After pruning and quantization, validation accuracy drops from 79.25% to 22.29% in the MC approximation for KL divergence without local reparameterization. Each panel displays a layer, starting from input to final layer."
}