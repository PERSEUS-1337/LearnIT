{
    "title": "BklMjsRqY7",
    "content": "Efforts to reduce numerical precision in deep learning training focus on quantizing weights and activations while using wide high-precision accumulators for partial sums. The lack of a framework to analyze accumulation precision requirements leads to conservative design choices, limiting the complexity reduction of multiply-accumulate units. A statistical approach is presented to analyze the impact of reduced accumulation precision on training, showing that a poor choice results in loss of information and reduced variance in partial sums. Equations are derived to relate variance to accumulation length and minimum bit requirements. This analysis is applied to three benchmark networks, including CIFAR-10. Our analysis on accumulation precision requirements for deep learning networks, including CIFAR-10 ResNet 32, ImageNet ResNet 18, and ImageNet AlexNet, shows that networks converge successfully to single precision floating-point baseline when using our proposed equations. Reducing accumulation precision further degrades network quality, highlighting the importance of our tight bounds. This analysis enables precise customization of computation hardware for optimal area and power efficiency in deep learning applications. Reduced precision deep learning techniques significantly reduce computational complexity, offering scalable hardware efficiency compared to other methods like pruning. Parameter complexity scales linearly while multiplication hardware complexity scales quadratically with precision bit-width, leading to potential 30x-1000x complexity reduction in comparison to single precision floating-point hardware. Reduced precision deep learning techniques offer scalable hardware efficiency by reducing computational complexity. Our work enables convergence in reduced precision accumulation, resulting in 1.5\u00d7 \u223c 2.2\u00d7 area reduction. Most efforts in reduced precision deep learning focus on quantizing representations and assume wide accumulators, leading to training instability and accuracy degradation. The hardware complexity in reduced precision floating-point numbers is dominated by the accumulator bit-width, limiting the benefits of reduced precision computations. Accumulation precision requirements in deep learning training are much lower than 32-b, enabling further complexity reduction of FPUs by 1.5 \u223c 2.2\u00d7. This new angle on reduced precision deep learning training forms the basis of our paper. Our work focuses on establishing theoretical foundations for estimating accumulation bit precision requirements in deep learning. Previous works in deep learning and high performance computing communities align with our goal. While early works on reduced precision deep learning mainly consider fixed-point arithmetic, challenges arise when quantizing signals involving the backpropagation algorithm due to a weak handle on scalar dynamic range. Hardware solutions like Flexpoint, a hybrid of fixed-point and floating-point, have been explored to address these challenges. In deep learning, various schemes like WAGE BID19 and MPT BID14 have focused on representation precision using different quantization methods. BID17 quantized all representations to 8-b floating-point and found that accumulation in 16-b with algorithmic contrivance enabled convergence. The issue of numerical errors in floating-point accumulation has been studied in high performance computing, with BID15 being among the first to estimate the effects statistically. BID15 were pioneers in estimating the effects of floating-point accumulation statistically. They derived mean square error estimates for accumulation using quantization noise analysis. Later works focused on worst-case accumulation error estimates, with BID9 providing upper bounds through round-off error analysis. BID0 extended this analysis to different summing algorithms, including chunk-based summations, evaluating various chunking approaches and their benefits. These analyses, while informative, are often general and not application-specific. Our contribution introduces the variance retention ratio (VRR) for deep learning accumulation precision analysis. We derive a formula for VRR to determine accumulation bit-width for computation hardware. Experimental verification is done on three benchmarking networks. The binary representation of a number includes a signed bit, exponent bits, and mantissa bits. Floating-point operations, like the dot product in deep learning, require multiplication and addition. Ideal floating-point operations need bit growth to avoid information loss. In a typical MAC operation, the output should have increased precision based on the operands' bit-precision and exponent difference. In deep learning, precision of computations is crucial to avoid accumulation errors like \"swamping.\" Variance engineering in weight initialization helps prevent vanishing or exploding gradients, ensuring fine convergence of DNNs. Output statistics of dot products are studied based on accumulated terms' independence. The accumulation variance in deep learning is crucial for avoiding swamping errors. Variance engineering in weight initialization helps prevent convergence issues in neural networks. The violation of key assumptions can lead to poor convergence behavior, as seen in the evaluation of accumulation variance in ResNet 18. In deep learning, accumulation variance is important to prevent swamping errors. An abnormality in reduced precision gradient computation is observed, with variance directly linked to accumulation length. The study focuses on trade-offs between accumulation variance, length, and mantissa precision, differing from previous works on reduced precision deep learning. Our work focuses on associating second order statistics to mantissa precision in reduced precision deep learning. We aim to compute the correct n th partial sum of accumulation of n terms, treating reduced precision floating-point arithmetic as unbiased approximate computing. The variance of the partial sum is crucial, with statistically independent, zero-mean terms. Our work focuses on the variance retention ratio in reduced precision deep learning, where the variance of the partial sum is crucial. The formula for the VRR is a function of n, m_p, and m_acc, ensuring quality of computation under reduced precision. The VRR curve shows a \"knee\" with respect to n, indicating a break point where certain mantissa precision is no longer suitable. The text discusses the concept of swamping in reduced precision deep learning and defines full and partial swamping. It presents a formula for the variance retention ratio when only full swamping is considered. The VRR is a function of n, m_p, and m_acc, showing a \"knee\" in the curve indicating when certain mantissa precision is no longer suitable. The proof in Appendix A shows that with a large m acc, V RR approaches 1, while with small m acc and n \u2192 \u221e, V RR approaches 0 due to the exponential decrease in the Q-function term. Limited precision leads to incorrect results with a large accumulation length. V RR can provide a sharp decision boundary for accumulation precision. The VRR formula considers full and partial swamping effects, with VRR dependent on m_acc, m_p, and n. Extreme behavior is observed with large m_acc leading to VRR approaching 1, while small m_acc and n approaching infinity result in VRR approaching 0 due to exponential decay. The VRR formula explains the stability improvements of accumulation using chunking, breaking the accumulation into smaller chunks to obtain more accurate results. This technique greatly improves the stability of sums by reducing the impact of exponential decay, especially with limited accumulation precision. The VRR formula explains the stability improvements of accumulation using chunking, breaking the accumulation into smaller chunks to obtain more accurate results. It is common to encounter sparse operands in deep learning dot products, where the effective accumulation length is often less than described by the network topology. When an accumulation is known to have sparse inputs with a known non-zero ratio (NZR), a better estimate of the VRR can be obtained. When dealing with sparse inputs in accumulation, estimating the VRR with known NZR is crucial. This estimation can be less conservative and help in determining the mantissa precision assignment. The VRR shows a breakdown region as accumulation length varies, which can be observed through the normalized exponential variance lost. In the context of sparse inputs in accumulation, estimating the VRR with known NZR is important for determining mantissa precision assignment. The normalized exponential variance lost is plotted for different values of m acc, considering normal and chunk-based accumulation. A cut-off value of v(n) < 50 is chosen based on accumulation length and precision. Chunk size is a hyperparameter in chunk-based accumulation, with an optimal size minimizing accumulation error. Based on the analysis of accumulation error and chunk size, it was found that chunking reduces error significantly as long as the chunk size is optimal. The VRR curve for different accumulation setups shows that chunking raises the VRR close to unity, with a flat curve indicating that the specific chunk size is not critical as long as it is moderate. This prevents large accumulations within and between chunks. In upcoming chunking experiments, a chunk size of 64 will be used to predict mantissa precisions required for training ResNet 32 on CIFAR-10, ResNet 18, and AlexNet on ImageNet. Benchmarks were chosen based on popularity and topology. Configuration includes 6-b exponents in accumulations, quantized intermediate tensors, and 16-bit precision for final layer. Loss scaling technique is used to limit underflows, with a scaling factor of 1000 for all models. CUDA code of GEMM is modified for rounding of partial sums. The CUDA code of the GEMM function is modified to enable rounding of partial sums and quantization of dot product inputs. Predicted precisions for different networks and layers are listed in TAB1 for normal and chunk-based accumulation with a chunk size of 64. The accumulation precision for CIFAR-10 ResNet 32 is generally lower than that of ImageNet networks due to shorter dot products. Precision requirements for convolutional layers in ImageNet networks vary, with GRAD accumulation depending on the feature map. The precision requirements for different networks and layers are listed in TAB1 for normal and chunk-based accumulation with a chunk size of 64. The benefits of chunking range from 1 to 6 bits, with AlexNet requiring less precision than ResNet 18 due to higher sparsity of operands. The converged test error is close to the baseline with no more than 0.5% degradation, but significantly increases when precision is further reduced. The benefits of chunking in reducing precision requirements are highlighted in the experiments conducted on ResNet 18. The predicted precision assignment ensures close fidelity to the baseline, with convergence curves plotted in FIG4 to investigate the validity and conservatism of the analysis. In FIG4, convergence curves are plotted to analyze the impact of accumulation precision on training. Experiments with precision perturbation show that when PP = 0, the converged accuracy remains within 0.5% of the baseline, validating the analysis. The experiments show that when PP < 0, a noticeable accuracy degradation occurs, especially for ImageNet ResNet 18. Higher perturbations lead to worse degradation. ImageNet AlexNet is more robust to perturbation compared to ResNets. The effects of PP are more pronounced for chunk-based accumulation, as shown in FIG4 (d). The effects of precision perturbation (PP) are more pronounced for chunk-based accumulation, with a specific precision perturbation corresponding to a relatively higher change. Comparing FIG3 and 5 (b) shows that chunk-based accumulation is more sensitive to precision perturbation. The analysis predicts the minimum precision required for convergence in deep learning training, proving the method's accuracy in determining the precision needed for benchmark networks. The analysis accurately determines the minimum precision needed for convergence in deep learning training, especially for recurrent architectures like LSTMs. It is a valuable tool for hardware designers implementing reduced-precision FPUs, addressing a critical missing link for low-precision floating-point hardware in DNN training. The stability of sums under reduced-precision floating-point accumulation is a challenging problem that this analysis tackles effectively. The analysis addresses the challenging problem of stability in reduced-precision floating-point accumulation for deep learning training, specifically for recurrent architectures like LSTMs. It determines the minimum precision required for convergence and provides tight results, although based on certain assumptions for mathematical tractability. Assumption 2 states that computation in reduced precision floating-point arithmetic is unbiased. Assumption 3 focuses on the monotonic accumulation in iterations leading to a full swamping event, excluding catastrophic events. Assumption 5 states that once a full swamping event occurs, the computation of partial sum accumulation is halted. It is unlikely for the computation to recover from swamping, and any partial recovery has negligible effects on the final result. Assumptions 3, 4, and 5 aid in mathematical tractability in proving Lemma 1. Assumption 6 assumes that the bits of the mantissa representation of partial sums and product terms are equally likely to be zero or one, which is standard in quantization noise analysis and useful in proving Theorem 1. During swamping, the VRR is computed by finding V ar(s n ) swamping = E swamping s 2 n using the Law of Total Expectation. A reduced set of events\u00c2 is considered to represent accumulation occurrences, with the first full swamping event at iteration i = 2. The first occurrence of full swamping at iteration i is determined by simplifying conditions and considering accumulated sums from previous iterations. Assumptions allow for a monotonic accumulation leading to full swamping. The condition for full swamping at iteration i is based on specific inequalities and the result of accumulation at iteration i is equal to the accumulation at iteration n. The event set for analysis consists of mutually exclusive events where full swamping occurs for the first time at iteration i. By the Central Limit Theorem, the accumulation at iteration i follows a normal distribution. Additionally, the event where no full swamping occurs has a probability of qn. The formula for the VRR is derived under certain conditions, with a normalization constant needed. Full swamping leads to computation stopping, with probability P(Ai) for this event. Partial swamping causes additional variance loss, with m stages before event Ai can occur. Monotonic accumulation is considered for each stage. The text discusses partial swamping stages before a full swamping event, where incoming product terms are truncated at each stage, leading to fractional variance loss. The total fractional variance lost is computed before the occurrence of the event. The variance is updated based on certain conditions to ensure positivity. The text discusses events where variance is updated to ensure positivity, considering scenarios of partial swamping before full swamping. Boundary conditions are accounted for, leading to fractional variance loss calculations. The text discusses updating variance to ensure positivity, considering partial and full swamping scenarios. Boundary conditions are considered, leading to fractional variance loss calculations. The law of total expectation is used to compute the VRR formula, concluding the proof. Variance of intermediate results and final result is computed based on mantissa precision. The text discusses updating variance to ensure positivity, considering partial and full swamping scenarios. Boundary conditions are considered, leading to fractional variance loss calculations. The law of total expectation is used to compute the VRR formula, concluding the proof. The intra-chunk accumulation uses m acc mantissa bits, limiting mantissa growth. The variance of the computed result when chunking is used can be calculated using VRR with chunking formula."
}