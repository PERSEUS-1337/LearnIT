{
    "title": "Hk9Xc_lR-",
    "content": "Generative adversarial training involves minimizing moment matching loss using a set of discriminator functions, typically neural networks. The discriminator set needs to be large enough to identify the true distribution and small enough to avoid memorizing samples. This paper proves that a discriminative discriminator set is guaranteed when its linear span is dense in bounded continuous functions. Generalization bounds are developed between the learned and true distribution, showing that generalization is ensured with a small discriminator set, regardless of the generator's size. Generative adversarial networks (GANs) minimize moment matching loss using discriminator functions. Different GAN variants use various sets of discriminators: Wasserstain GAN (W-GAN) uses Lip 1 functions, MMD-GAN uses a unit ball in RKHS, and Energy-based GANs use a different set. The analysis sheds light on GANs' practical performance. Energy-based GANs utilize a set of continuous functions bounded between 0 and M for some constant M > 0. The evaluation metric, KL divergence, requires the log density ratios of generators and true distributions to be within the linear span of the discriminator set. This condition partially explains the counter-intuitive behavior in testing likelihood in flow GANs. The analysis extends to neural f-divergences, showing similar results on discrimination and generalization properties. In this paper, a neural f-divergence is discriminative if its discriminators' linear span without the output activation function is dense in the bounded continuous function space. Various norms are defined for continuous functions on subset X of R^d. The set of continuous functions on X is denoted by C(X), and the Banach space of bounded continuous functions is denoted by C_b(X). Borel probability measures on X are denoted by P_B(X). Weak convergence is denoted by \u03bd_n \u03bd. Base measures are denoted by \u03c4. The weak convergence, denoted by \u03bd_n \u03bd, involves the density of \u00b5 \u2208 P_B(X) as \u03c1 \u00b5 = d\u00b5 d\u03c4. The integral probability metric is minimized in many GAN variants. Minimizing towards zero matches moments E \u00b5 [f ] = E \u03bd [f ] for all discriminators f \u2208 F. The discriminator set is typically restricted to parametric function classes like F nn = {f \u03b8 : \u03b8 \u2208 \u0398}. Neural distances, denoted as d Fnn (\u00b5, \u03bd), are the object function optimized by W-GAN due to their practical optimizability and ability to leverage neural networks' representation power. Studying neural distances directly, rather than the Wasserstein metric, is crucial for understanding GANs' practical performance. When the parameter function set F nn is sufficiently large, moment matching on F nn implies equality between distributions \u00b5 and \u03bd, making F nn a universal approximator even with small sets like single-neuron neural networks. The discriminative property of a set of functions F on a metric space X is defined by moment matching, where equality between distributions \u00b5 and \u03bd is implied by matching moments on F. The linear span of F needs to be large enough to approximate all indicator test functions well. Theorem 2.2 states that for a function set F \u2282 C b (X), the discriminative property holds if the span of F is dense in the space of bounded continuous functions C b (X) under the uniform norm. The discriminative property of a set of functions F on a metric space X is defined by moment matching, where equality between distributions \u00b5 and \u03bd is implied by matching moments on F. The linear span of F needs to be large enough to approximate all indicator test functions well. Theorem 2.2 states that for a function set F \u2282 C b (X), the discriminative property holds if the span of F is dense in the space of bounded continuous functions C b (X) under the uniform norm. Functions in C b (X) are characterized using weak convergence, and neural f-divergence analysis faces challenges due to moment matching being only a sufficient condition for minimizing it. The linear span of discriminators without output activation function, denoted as F0, is a necessary condition for minimizing neural f-divergence. A neural f-divergence is discriminative if the linear span of its discriminators is dense in the bounded Lipschitz function space BL(X). The bounded Lipschitz (BL) distance metrizes weak convergence, where requiring cl(spanF) \u2287 Cb(X) is a weaker condition than cl(F) \u2287 Cb(X). This condition is satisfied by function sets smaller than those used in practice, including neural networks with only a single neuron. Neural networks with a single neuron are universal approximators in Cb(X) according to classical theories. The activation function \u03c3 must be non-polynomial for spanF nn to be dense in C(X). Bounded parameter sets like \u0398 can be used for efficient search in practice, especially for non-decreasing homogeneous activation functions like ReLU. Bach (2017) proves that for compact set X \u2282 Rd, ReLU activation functions with bounded parameter sets like \u0398 = {\u03b8 \u2208 Rd+1 : \u03b8 \u2264 1} are sufficient to discriminate between Borel probability measures. This is not the case for activation functions like tanh or sigmoid due to an approximation gap. Using a bounded parameter set \u0398 ensures a bounded Lipschitz norm for neural networks, leading to an upper bound on the neural distance by Wasserstein distance. The Lipschitz constraint in W-GAN enforces \u03b8 \u221e \u2264 \u03b4 to bound the neural distance by Wasserstein distance. However, it does not necessarily make the function set F discriminative. The Lipschitz constraint plays a role in stabilizing training and ensuring generalization bounds. In practice, weak convergence is often used to establish discriminative properties when d F (\u00b5, \u03bd n ) \u2192 0 for a sequence of \u03bd n. If spanF is dense in C b (X), then lim n\u2192\u221e d F (\u00b5, \u03bd n ) = 0 implies \u03bd n weakly converges to \u00b5. Theorem 10 of BID28 discusses similar results for adversarial divergences, but lacks the specific weak convergence result for neural distances. Our result does not rely on the compactness assumption of X. When X is compact, Wasserstein distance and the BL distance are equivalent and both metrize weak convergence. This is applicable to neural networks with ReLU activation function and bounded parameter set \u0398. Neural distances, such as d Fnn, are topologically equivalent to Wasserstein and BL distances as they metrize weak convergence. However, they are weaker than BL distance due to smaller F. Theorem 2.2 characterizes conditions for a neural distance to be discriminative, showing even single neuron networks can achieve this. The benefit of using larger and deeper networks remains unclear, highlighting the need to understand the discriminative power of neural distances. The text discusses the discriminative power of neural distances and the relationship with BL distance. It explores how to control stronger distances using neural distances and develops bounds for this purpose. The discussion focuses on using neural distances to control stronger distances like BL distance and KL divergence. It involves translating generalization bounds in d F (\u00b5, \u03bd) to BL distance and KL divergence, addressing the difference of moments | E \u00b5 g \u2212 E \u03bd g| for functions outside of F, and controlling functions in spanF using a norm ||g|| F ,1. The neural distance d F (\u00b5, \u03bd) can control the discrepancy E \u00b5 g \u2212 E \u03bd g for functions outside of spanF but inside cl(spanF) through an error decay function. This function ensures that g is approximated by F with error decay function (r) if inf r\u22650 (r) = 0. Efforts are needed to derive the error decay function for specific F and g. Proposition 6 of Bach (2017) derives the decay rate for approximating bounded Lipschitz functions with rectified neurons, showing a bound between BL distance and neural distance. The result indicates that the bound weakens as the dimension increases, as non-parametric sets are approximated by parametric ones. Likelihood and KL divergence are common in statistical learning, but recent advances in deep unsupervised learning question the use of likelihood as the right objective for training and evaluation. Recent studies have shown a counter-intuitive phenomenon in GANs where both testing and training likelihood decrease as the GAN loss is minimized. The neural distances in GANs may be too weak to control KL divergence properly. A proposition states that if the density ratio behaves nicely in a certain space, then KL divergence can be bounded. Regularization techniques help GAN training by shrinking the discriminator set, which is necessary because practical data distributions often lack densities. This is crucial as GANs can only optimize the empirical loss based on a sample of the true model, requiring generalization bounds to control the exact loss. Generalization bounds are needed to control the loss of GANs when minimizing the empirical version. The discriminator set F must be small enough to be generalizable yet large enough to be discriminative. Rademacher complexity is used to establish these bounds for GANs. The generalization bounds for GANs depend on the Rademacher complexity of the discriminator set F and are independent of the generator set G. Other standard metrics like BL distance and KL divergence are also discussed. The problem is reduced to bounding the discrepancy between the true model \u00b5 and its empirical version \u03bc m, which can be achieved using concentration bounds from statistical learning theory. The generalization bounds for GANs are determined by the Rademacher complexity of the discriminator set F, independent of the generator set G. Theorems relate the generalization error to the Rademacher complexity, showing that a smaller discriminator set leads to better generalizability. The choice of discriminator set F in GANs should balance generalizability and discriminative power. Parametric neural discriminators, such as neural networks with a single ReLU unit, strike a good balance by providing a small generalization error with a probability of at least 1-\u03b4. The generalization bounds are determined by the Rademacher complexity of F, showing that a smaller discriminator set leads to better generalizability. With parametric neural discriminators, the generalization error can be minimized with a probability of at least 1-\u03b4. The Rademacher complexity of the discriminator set determines the generalization bounds, emphasizing the importance of a smaller set for better generalizability. This result can be applied to neural discriminators, particularly in the context of GANs, where the learning bounds can be discussed with choices of non-parametric discriminators. The Rademacher complexity of bounded sets in a RKHS is utilized to provide learning bounds for MMD-based GANs, specifically for Wasserstein distance. The text discusses the advantages of using parametric neural discriminators in MMD-based GANs, presenting results for Wasserstein distance and total variance distance. It also provides a bound for generalization error with a probability of at least 1-\u03b4, comparing it to previous results and emphasizing the direct interest in bounding a specific quantity. The text discusses bounding generalization error using Eqn. (10) and adapting Rademacher complexity. It emphasizes the importance of using a larger generator set G with a larger discriminator set F for meaningful bounds on KL divergence. The text discusses the importance of compatibility between the generator set G and discriminator set F for bounding generalization error. It highlights the trade-off in choosing a small generator set with well-behaved density functions and a large enough discriminator set to include all density ratios within a certain radius. Corollary 3.5 provides a bound for testing likelihood based on the error decay function. However, the condition for this bound is strong, requiring positive densities for both true distribution \u00b5 and generators \u03bd, with a well-behaved log-density ratio. In practical computer vision applications, \u00b5 and \u03bd often concentrate on local regions with peaky densities or no valid densities, leading to a large or infinite compatibility coefficient \u039b F ,G and a loose bound on testing likelihood. Theoretical literature on GANs focuses on discrimination and generalization properties, with a surge of research interest in the field. Capacity assumptions about the discriminator set F are crucial for justifying the discriminative power of GANs. The neural distance may not provide a strong enough bound for KL divergence, leading to uncorrelated negative testing likelihood during GAN training. In GANs, capacity assumptions about the discriminator set F are essential for proving discriminative power. The MMD-based GANs avoid parametrization of discriminators by utilizing the optimal discriminator in the non-parametric RKHS space, satisfying capacity assumptions and justifying their discriminative power easily. The BID28 paper defines adversarial divergences in GANs and shows that using a restricted discriminator family has a moment-matching effect. They also discuss convergence in objective functions and provide conditions for strict adversarial divergence. Our work fills a gap by providing a necessary and sufficient condition for strict adversarial divergence. Additionally, we define generalization error in FORMULA0 and limit the number of samples from the generated distribution to a polynomial amount for efficient training. The BID28 paper discusses adversarial divergences in GANs and convergence in objective functions. They provide conditions for strict adversarial divergence and limit the number of samples for efficient training. Other research studies dynamics of GAN training and optimal transportation in GANs. The BID20 paper introduces a new formulation of GANs and VAEs, unifying popular methods for training deep generative models. It also discusses the discrimination and generalization properties of GANs with parameterized discriminator classes like neural networks, providing generalization bounds in different evaluation metrics. Our bounds show that generalization in GANs is guaranteed with a small discriminator set, regardless of generator size. GAN methods often strike a balance between discrimination and generalization. The generalization bound in KL divergence explains testing likelihood behaviors in GAN training. Future exploration includes methods to compute neural distance/divergence, a challenging minimax problem. The text discusses the need for stable training methods and theoretical analysis for minimax problems in GANs. It mentions the importance of the discriminator set in generalization bounds and suggests incorporating structural information from the generator set for sharper bounds. The analysis is proposed to be extended to conditional GANs, with a focus on neural discriminators and Rademacher complexity. The text discusses the spectral normalized complexity of neural networks and the empirical Rademacher complexity for data matrices. It provides a generalization bound for the neural discriminator set in GANs. The text provides a generalization bound for the neural discriminator set in GANs, incorporating spectral normalized complexity and optimization error. It eliminates the need for a large number of parameters and can be applied to spectral normalized GANs, explaining their empirical success. The text discusses the learning bounds of GANs with different non-parametric discriminator sets, emphasizing the benefits of using parametric neural discriminators. It focuses on the bounded Lipschitz distance and Wasserstein distance when X is compact, providing tight bounds for the uniform distribution on X. The bound requires at least m = exp(\u2126(d)) samples for accurate estimation. The text discusses the challenges of generalization in training with the Total Variation (TV) distance, highlighting that even with an infinite number of samples, generalization cannot be guaranteed. It emphasizes that training with TV distance does not generalize effectively, even when the training loss on empirical samples is minimal. Training with TV distance may not guarantee effective generalization, even with an infinite number of samples. However, using a stronger metric could lead to faster convergence in a weaker metric. The practical difficulty of calculating and optimizing TV and Wasserstein distances makes neural distance more favorable. The f-GAN family minimizes f-divergence for GANs, including the original GAN. The \u03c6-divergence, also known as BID33, extends the analysis of f-GANs by interpreting it as a form of penalized moment matching. It differs from IPM and is nonnegative and discriminative under certain conditions.\u03c6-divergence is defined as DISPLAYFORM0 for distributions \u00b5 and \u03bd on X, with a convex function \u03c6 satisfying \u03c6(1) = 0. The \u03c6-divergence is nonnegative and discriminative, with different choices of \u03c6 recovering popular divergences. Introducing another convex function \u03c8(t) helps develop intuition and provides a variational representation for moment matching. The convex conjugate \u03c8 * is introduced to provide a variational representation for moment matching. The term in the equation serves as a complexity penalty on the function f, ensuring finiteness. The convexity of \u03a8 \u03bd,\u03c8 * [f] is discussed, along with the strict convexity of \u03c8 * when \u03c8 is strictly convex. The convex conjugate \u03c8 * is used for moment matching, with a complexity penalty on function f to ensure finiteness. Practical f-GANs optimize over a parametric set of neural networks, leading to a neural \u03c6-divergence. This divergence can differ from the F-related IPM and may not always be non-negative. However, under certain conditions, it can still exhibit non-negativity and discriminative properties. The property of moment matching on features defined by the last linear layer of discriminators is a key step to establish discriminative power in neural divergence. Moment matching on F is a sufficient condition for zero neural \u03c6-divergence, under certain conditions on F. The function set F includes a constant function and has a specific form with positive numbers associated with each function. The output activation function of a deep neural network, specified by DISPLAYFORM7, plays a crucial role in ensuring the output respects the input domain of the convex function \u03c8*. This condition is commonly satisfied by f-GANs in practice, with the activation function \u03c3 ensuring this property. The proof of Theorem B.1 involves differentiability assumptions and the fact that b0 is a differentiable minimum point of \u03c8*. The proof of Theorem B.1 involves differentiability assumptions and the fact that b0 is a differentiable minimum point of \u03c8*. Our results on neural \u03c6-divergence can be extended to the framework of BID28, focusing on \u03c6-divergence due to its practical importance. Theorems in BID28 are related but have different conditions compared to ours, which are clear and satisfied by all \u03c6-divergence listed in BID33. If there exists C > 0 such that F \u2282 {f \u2208 C(X) : f Lip \u2264 C}, we have DISPLAYFORM4. The proof involves differentiability assumptions and the fact that b0 is a differentiable minimum point of \u03c8*. Results on neural \u03c6-divergence can be extended to BID28, focusing on practical importance. Theorems in BID28 have different conditions compared to ours. If there exists C > 0 such that F \u2282 {f \u2208 C(X) : f Lip \u2264 C}, we have DISPLAYFORM4. The first half is a direct application of Theorem B.1 and Theorem 10 in BID28. For the second half, we have DISPLAYFORM5 where we use Theorem B.1 i) in the first inequality and the Lipschitz condition of F in the second inequality. Similar to the case of neural distance, we can establish generalization bounds for neural \u03c6-divergence. Theorem B.3 states assumptions and empirical distribution with m samples from \u00b5. With probability at least 1 \u2212 2\u03b4, we have DISPLAYFORM1 where R (\u00b5) m (F) is the Rademacher complexity of F. The only difference between Theorem 3.1 and Theorem B.3 is the failure probability change. The difference between Theorem 3.1 and Theorem B.3 is the change in failure probability from \u03b4 to 2\u03b4. The proof of Theorem B.3 involves Rademacher complexity and generalization bounds for different choices of F in the neural divergence setting. In the neural divergence setting, under certain conditions, a parametric function class F is assumed to be L-Lipschitz continuous. With probability at least 1 - 2\u03b4, a specific display formula is satisfied. The proof of Theorem 2.2 involves standard techniques and references a lemma for completeness. The necessary part involves properties of discriminative sets in a space, leading to the existence of a bounded linear functional. In the context of neural divergence, a parametric function class F is assumed to be L-Lipschitz continuous. The proof involves properties of discriminative sets in a space, leading to the existence of a bounded linear functional. The representation theorem for compact metric spaces states the existence of a signed, regular Borel measure. The Hahn decomposition of the measure leads to a contradiction, proving Corollary 2.4. Theorem 2.5 shows that a function in C b (X) can be approximated by F with error decay function. The function g in C b (X) can be approximated by F with error decay function (r), where r is a non-increasing function. As r approaches infinity, the error approaches zero, leading to weak convergence. If F \u2286 BL C (X) for some C > 0, the bounded Lipschitz distance metrizes weak convergence. Taking the infimum over all possible values yields the desired result. The proof of Proposition 2.7 involves taking the infimum over all possible values. Corollary 2.8, based on Proposition 5 of BID4, shows that for any bounded Lipschitz function g satisfying certain conditions, a specific result is obtained. Equation (10) is derived using standard methods and an optimality condition. Theorem 3.1 is proven by showing the relationship between certain functions and their values. In this section, the analysis of the consistency of GAN objective and likelihood objective is tested on two toy datasets - a 2D Gaussian dataset and a 2D 8-Gaussian mixture dataset. The ground-truth distribution is a 2D Gaussian with specific parameters. The generator function used for the Gaussian distribution is defined, and the generative model is trained accordingly. The generative model is trained using WGAN with weight clipping, utilizing neural networks and quadratic polynomial discriminators. The quadratic polynomial discriminators yield higher testing log likelihood and better performance compared to neural networks, as guaranteed by Corollary 3.5. The training loss converges to zero, and the testing log likelihood increases monotonically during training. In a simple Gaussian example, maximizing the likelihood (MLE) on the training dataset converges faster than WGAN. By carefully choosing the discriminator set, the testing log likelihood can be optimized simultaneously with the GAN objective. The ground truth distribution is a 2D Gaussian mixture with 8 Gaussians equally distributed on a circle with radius \u221a2. The training dataset consists of 10,000 samples, with 256 samples shown in one batch. The generator assumes 8 Gaussian components with equal weights. Training parameters include scaling and biasing parameters for each component. The model is trained using WGAN with clipping and an MLP with 4 hidden layers. Results show that the generator's samples closely resemble real samples, but there is inconsistency between GAN loss and log likelihood. The training process shows that the generator's density becomes more singular over time, leading to erratic behavior in log likelihood. Despite the increasing negative GAN loss, the log likelihood oscillates frequently, with over half of the time being -\u221e. This inconsistency between GAN loss and likelihood is attributed to neural discriminators not approximating singular density ratios well. Training the model to maximize likelihood on the dataset results in getting stuck in a local minimum. The training process reveals that maximizing likelihood can lead to getting stuck in a local minimum due to the presence of multiple local minima in the negative log-likelihood loss. FlowGAN attempts to address this issue by combining WGAN loss and log likelihood, but also faces challenges with local minima."
}