{
    "title": "HkxLiJSKwB",
    "content": "The proposed Graph-based Motion Planning Networks (GrMPN) framework allows learning and planning on general irregular graphs, overcoming limitations of existing planning architectures. GrMPN is invariant to graph permutation, enhancing generalization and data-efficiency capabilities. The proposed GrMPN method demonstrates strong generalization and data-efficiency in various domains such as 2D mazes, path planning on irregular graphs, and motion planning for robot configurations. Reinforcement learning (RL) involves sequential decision making by an agent in an environment, often formulated as a Markov decision process (MDP). Model-based RL, like GrMPN, requires an environment model estimation but is computationally expensive yet data-efficient. Deep reinforcement learning (DRL) has seen success in various applications, ranging from games to robotics and chemical synthesis. DRL methods can be model-based or model-free, with the former being more data-efficient but computationally expensive. Transfer learning across tasks is challenging but desired, with recent work suggesting the use of a differentiable planning module in a policy network. A differentiable planning module called Value Iteration Networks (VIN) is proposed in a policy network architecture for efficient policy optimization and transfer learning. VIN utilizes convolutional neural networks and max-pooling for value function updates, laying the foundation for various other differentiable planning network architectures. Recent works have utilized graph neural networks to address limitations in learning with regular environment structures, such as transition functions forming 2D lattice structures. Generalized VIN and other approaches have applied graph neural networks for planning in environments with irregular structures, enabling processing of irregular data structures like graphs. GVIN stands out for emulating the value iteration algorithm on arbitrary graphs, with a differentiable policy network architecture similar to VIN. In this paper, different formulations for value iteration networks on irregular graphs are proposed based on graph neural network models. These models can learn optimal policies on general graphs without prior knowledge of transition and reward functions. They are invariant to graph isomorphism, allowing for generalization to graphs of different sizes and structures, enabling zero-shot learning for planning. The Bellman equations are represented as message passing in these models. The paper proposes using message passing neural networks (MPNN) to emulate the value iteration algorithm on graphs. Two general formulations of graph-based value iteration networks are introduced, based on Graph Networks (GN) and Graph Attention Networks (GAT). The contributions include the development of a MPNN-based path planning network (GrMPN) for learning to plan on general graphs, and GrMPN is invariant to graph isomorphism, enabling transfer planning on different graph structures. GrMPN is a message passing neural network for planning on graphs, invariant to graph isomorphism. It outperforms existing approaches in data-efficiency, performance, and scalability. Background on Markov decision process (MDP), value iteration algorithm, value iteration networks (VIN), and graph neural networks (GNN) is provided. Value iteration (VI) is a dynamic programming algorithm that maximizes the value function V(s) for all states in a Markov decision process (MDP) by iteratively updating it using the Bellman backup operator. The optimal policy \u03c0* can be computed based on the converged value function V*. Q-value functions are also defined, and there is a relationship between V and Q functions. The reward function R(s, a) plays a crucial role in goal-oriented tasks. Value Iteration Networks (VIN) is a differentiable planning framework that can perform transfer-planning for goal-oriented tasks with different goal states and learn the shared underlying MDP between tasks. It uses a policy network trained through imitation learning to approximate reward and transition functions from an unknown MDP. Value Iteration Networks (VIN) embeds value iteration as convolutions and max-pooling over feature channels to find optimal paths in maze-like environments. It uses trainable parameters for reward and transition embeddings, inspiring other differentiable planning algorithms. Value Iteration Networks (VIN) have inspired various differentiable planning algorithms, such as QMDP-Net, Memory Augmented Control Network (MACN), and Predictron framework. Gated path planning networks (GPPN) extend VIN by using LSTM for recursive updates, showing success in path planning tasks on grid-based navigation with fully or partially observable states. Regular lattices are required for these algorithms to exploit their underlying state space. Recent efforts have been made to extend Value Iteration Networks (VIN) to differentiable planning on general graphs. Niu et al. propose a graph-based model-based deep reinforcement learning framework called Generalized Value Iteration (GVIN), which can learn an underlying MDP and optimal planning policy through imitation or reinforcement learning. GVIN applies recursive graph convolution and max-pooling operators to emulate the value iteration algorithm on general graphs. GVIN is a graph-based model that extends Value Iteration Networks (VIN) to differentiable planning on general graphs. It uses specially designed convolution kernels to transfer planning to unseen graphs of arbitrary size. The reward graph signal is denoted as R = f R (G, v * ), and the value functions V (v) on graph nodes can be computed recursively. However, GVIN's directional and spatial kernels are not sufficient to capture invariance to graph isomorphism, leading to poor performance in domains with complex geometric structures. Graph neural networks (GNN) have gained attention for processing data on irregular domains like graphs or sets. GNN computes encoded features for each node based on graph structure, node, and edge features. After computing these features, an additional function is used to compute the output at each node. Graph convolution networks (GCN) extend traditional CNNs to handle convolution operations on graphs through spectral methods. GCN is based on fundamental convolution operations on spectral domains but is limited to graphs of the same size. Later introduced graph convolutional networks on the spatial domain can learn on graphs of arbitrary sizes but are limited to subsets of node neighbors or random walks of k-hop neighborhoods. Graph convolution networks (GCN) are limited to graphs of the same size, while graph convolutional networks on the spatial domain can learn on graphs of arbitrary sizes but are limited to subsets of node neighbors or random walks of k-hop neighborhoods. Graph attention network (GAT) and Message passing neural network (MPNN) offer more efficient and scalable solutions for large graphs by modifying the convolution operation and using message passing to compute graph embedding. Message Passing Neural Network (MPNN) is a unified framework for graph convolution and other existing graph neural networks. It involves two phases: message passing and readout, where the message passing operation at a node is based on its state and messages received from neighbor nodes. MPNN is designed similarly to Gated Graph Neural Networks (GG-NN) but uses GRU for the recursive message operation. Various frameworks have been converted to MPNN variants, including Graph Convolution Network, Gated Graph Neural Networks, Interaction Network, Molecular Graph Convolutions, and Deep Tensor Neural Networks. The paper discusses the application of GRU in implementing recursive message operations in MPNN. It introduces different message functions and updates based on received information from nodes and edges. The Graph Network (GN) framework is also introduced, which encompasses various graph neural networks like MPNN, GNN, GCN, and GAT as special cases. The updates of Graph Network (GN) consist of three update functions and aggregation functions based on message passing mechanism. These aggregation functions are critical for graph representation learning and must be invariant to the order of nodes. The three update functions are defined for node, edge, and global features using aggregated information. The Graph Network (GN) uses aggregation functions for element-wise operations and update functions with neural networks. It differs from MPNN by incorporating edge features and global graph nodes. The proposed GrMPN is a graph-based motion planning network trained on motion planning problems to generate optimal plans for test problems. The GrMPN framework proposes a general graph-based value iteration approach for learning optimal policies on new graphs. It focuses on imitation learning and utilizes message passing principles. Additionally, a feature-extract function is designed to learn a reward function based on the graph and goal node. GrMPN is a graph-based value iteration approach for learning optimal policies on new graphs. It involves recurrent application of graph operations at nodes and edges using aggregation and update functions. The algorithm does not represent the global node and uses one aggregation function and two update functions. The message aggregation is similar to GPPN, and the implementation is based on Graph Networks (GN). GrMPN is a graph-based value iteration approach that utilizes Graph Networks (GN) for learning optimal policies on new graphs. The algorithm, GrMPN-GN, can be easily rewritten as an application of graph neural networks like MPNN, GGNN, and GAT. Multi-heads are used to represent channels for Q value functions, with attention coefficients computed for each action. This formulation is denoted as GrMPN-GAT, connecting GrMPN-GN and GrMPN-GAT through reformulation. GrMPN-GAT is a GN module with attentions where edge features contain attention coefficients. The algorithm is similar to GrMPN-GN. Evaluation on 2D mazes, irregular graphs, and motion planning shows comparable performance to VIN. GrMPN frameworks, such as GrMPN-GAT and GrMPN-GN, are data-efficient and generalize well to unseen graphs. They handle long-range planning effectively by generalizing across nodes and graphs. GrMPN-GAT, with attended updates for value functions, outperforms GrMPN-GN. The experiments follow a standard encode-decode graph network design. In the experiments, a two-layered MLP encoder with ReLu activation and 16 hidden units is used for graph components. For larger lattices, hidden units are increased to 32. An additional node encoding is used for motion planning problems. The RMSProp algorithm with a learning rate of 0.001 is employed. The number of message passing steps varies across experiments, with different values for 2D mazes and irregular graphs. In experiments, a two-layered MLP encoder with ReLu activation and 16 hidden units is used for graph components. For larger lattices, hidden units are increased to 32. An additional node encoding is used for motion planning problems. The RMSProp algorithm with a learning rate of 0.001 is employed. The number of message passing steps varies across experiments. In the current chunk, 20 2D mazes are used with k = 20 in irregular graph experiments, except for the last motion planning task on a roadmap of 500 configurations where k = 40. Different metrics are used including %Accuracy, %Success, and path difference. Training focuses on imitation learning with a cross-entropy loss for supervised learning. In this experiment, evaluations are conducted on 2D mazes with a regular graph structure comparing GrMPN against VIN, GVIN, and GPPN. The environment settings are similar to VIN, GVIN, and GPPN, with graphs generated using the same script. GPPN requires a complete optimal policy, giving it a slight advantage. Training and testing are done on graphs of sizes 12 \u00d7 12, 16 \u00d7 16, 20 \u00d7 20, with varying numbers of generated graphs for training. Testing data consists of 1000 graphs of corresponding sizes. The results in Table 1 show that GrMPN-GAT and GrMPN-GN outperform other baselines and are more data-efficient. GPPN performs slightly better with a large amount of data but requires optimal policies. GVIN, relying on specially designed convolution kernels, does not perform as well as GrMPN-GAT and GrMPN-GN, which are known to be invariant to graph isomorphism. GrMPN-GAT and GrMPN-GN show significant data-efficiency on large 20 \u00d7 20 domains, learning well even with limited data. Performance of GrMPN-GAT on larger domains is better than on smaller domains due to the increased number of nodes involved in training. The experiment involves creating synthetic graphs with random coordinates in 2D space, varying parameters to generate Dense, Sparse, and Treelike graphs. Tree-like graphs are created using Networkx's geographical threshold graph function with low connectedness probability between nodes, presenting unique challenges not considered in previous experiments. In GVIN, challenges arise with tree-like graphs due to fewer nodes for training and issues with long-range planning. 10000 graphs are generated with varying node numbers. Training and testing data split is 6/7 and 1/7 respectively. Results are compared on Dense, Sparse, and Tree-like graphs. GrMPN methods perform similarly to GVIN on Dense graphs but slightly better in terms of Accuracy and Distance difference. On Sparse graphs, GrMPN methods outperform GVIN. GrMPN-GAT and GrMPN-GN show fast updates across nodes on Sparse graphs. PRM is widely used in robotic motion planning for navigation, ensuring collision-free paths and meeting robot system constraints. GrMPN is evaluated on 2D navigation and manipulation tasks to enhance PRM. In this section, GrMPN aims to improve PRM by incorporating transfer planning for a simulated 7-DoF Baxter robot arm. Different encoding layers are proposed for node features, such as GN, GCN, or MLP. Data generation involves using a standard PRM algorithm to create 2000 roadmaps for training in mobile robot navigation. The architecture of the implementation results in a hierarchical graph-based planning method. The study involves generating roadmaps for training with 200 robot configurations represented as graph nodes. Two test sets of 1000 roadmaps with different obstacle settings are created. The Dijkstra algorithm is used to find optimal trajectories for random start and goal states. The 7-DoF Baxter robot arm is simulated with various environment settings and start/goal configurations. PRM and Dijkstra algorithms are used to find optimal trajectories from start nodes to the goal. The study evaluates the performance of GrMPN methods in 2D navigation with a mobile robot, showing superior results compared to GVIN. The generalization ability of GrMPN methods is tested using a trained model on a created roadmap test set. The distance difference (Diff) measures the cost variance between predicted and optimal paths planned by Dijkstra. GVIN's performance on large testing graphs is not reported due to degradation. The trained model using Tree-like graph data demonstrates good generalization on unseen graphs generated by PRM in different environments. The study evaluates the performance of GrMPN methods in 2D navigation with a mobile robot, showing superior results compared to GVIN. GrMPN can generalize well on unseen graphs and larger graphs with 500 nodes, suggesting zero-shot learning capability for planning. Results show high accuracy and success rates in motion planning tasks with a simulated 7-DoF Baxter arm. GrMPN is based on graph neural networks and outperforms GVIN in scalability and performance on large graphs. The results in Figs. 2 and 3 demonstrate the ablation of GrMPN-GAT and GrMPN-GN on varying graph processing steps, showcasing the value function maps post-training. GrMPN-GAT exhibits superior generalization across nodes compared to GrMPN-GN, particularly beneficial for long-range planning. Table 6 summarizes information on generated graphs, while Fig. 4 illustrates GVIN's limitation in spreading updates to distant nodes. GrMPN-GAT shows slightly better generalization across nodes than GrMPN-GN, with fewer un-updated nodes. The results show that GrMPN-GAT outperforms GrMPN-GN due to having fewer un-updated nodes. VIN struggles with sparse graphs and long-range planning, indicating weak generalization. Data generation involves creating graph maps with robot configurations using a PRM algorithm. Test sets consist of roadmaps with varying numbers of configurations and obstacle settings. Each graph is solved using the Dijkstra algorithm to find an optimal trajectory. The comparison between GVIN, GrMPN-GAT, and GrMPN-GN shows that GrMPN-GAT performs better in updating value functions for nodes far from the goal. The color map in Figure 8 indicates that GrMPN-GAT has wider value propagation, leading to better generalization for long-range planning and across task graphs."
}