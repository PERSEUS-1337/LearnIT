{
    "title": "HJxhWa4KDr",
    "content": "In this paper, a novel random forest kernel is proposed to improve the performance of MMD GAN. The random-forest kernel utilizes probabilistic routing and can be integrated with CNN frameworks. It offers advantages such as ensemble learning from GAN discriminator outputs, characteristic properties for MMD structure, and flexibility in capturing distribution differences. This approach combines the strengths of CNN, kernel methods, and ensemble learning. Our random-forest kernel based MMD GAN shows promising results on CIFAR-10, CelebA, and LSUN bedroom datasets. We also provide theoretical analysis to support our findings. GANs are widely used in various tasks like image-to-image translation, 3D reconstruction, video prediction, and text-to-image generation, but their training can be challenging due to instability. The instability of GAN training can be attributed to the distance used in the discriminator to measure the difference between generated and target distributions. Jensen-Shannon divergence may fail when distributions have zero intersection, leading to infinite KL divergences and hindering gradient updates for the generator. Integral Probability Metrics (IPM) based GANs have been proposed to better measure distribution differences. IPM based GANs use different distance metrics in the discriminator to measure distribution differences. A new random-forest kernel is proposed to enhance the performance of MMD GAN discriminator by leveraging decision forests' non-linear discriminating power. This kernel incorporates stochastic and differentiable decision trees for back-propagation training. The random-forest kernel enhances the MMD GAN discriminator by using decision forests for non-linear discrimination. Each dimension of the GAN discriminator outputs is randomly connected to a soft decision forest node, with leaf values representing the probability of sample x i falling into a certain leaf node. The random forest kernel formula includes parameters for the GAN discriminator and the random forest, combining random forest and deep neural networks for improved performance. In Kontschieder et al. (2015), a differentiable decision tree model and deep convolutional networks are combined for classification tasks. Shen et al. (2017) and Shen et al. (2018) extend this idea to label distribution learning and regression. Zuo & Drummond (2017), Zuo et al. (2018), and Avraham et al. (2019) introduce deep decision forests, aggregating results by multiplication. Zuo et al. (2018) combine random forest and GAN by introducing forest structure in GAN discriminator. Avraham et al. (2019) use forest structure as a non-linear mapping function in regularization. Breiman explores the relationship between random forest and kernel methods. Breiman (2000) initiates the literature on the relationship between random forest and kernel methods, showing that a random tree partition is equivalent to a kernel on the true margin. Shen & Vogelstein (2018) prove the characteristic of random forest kernel. Theoretical analysis by Davies & Ghahramani (2014), Arlot & Genuer (2014), and Scornet (2016) is also discussed. However, the decision functions in these forest kernels are non-differentiable hard margins, limiting their direct use in backpropagation. MMD GAN combines random forest with deep neural networks through a random-forest kernel, a novel approach in kernel MMD GAN. MMD GAN with a random-forest kernel outperforms other kernels in terms of accuracy and training stability, as shown through theoretical analysis and numerical experiments. The random-forest kernel used in MMD GAN is characteristic and provides unbiased gradient estimators during training. The study compares the random-forest kernel with Gaussian RBF, rational quadratic, and bounded RBF kernels, demonstrating superior performance. The paper introduces MMD GAN and deep random forest embedded in a CNN. It proposes random-forest kernels and MMD GAN with random-forest kernels. Training techniques and theoretical results are discussed. Experimental setups compare the proposed random-forest kernel with others. The generative model maps noise distribution to data space, while the discriminative model distinguishes generated from real data distribution. The paper discusses MMD GAN and introduces a random-forest kernel for MMD GAN. It also reviews a stochastic and differentiable variant of random forest embedded in a CNN. The squared MMD is used to distinguish between generated and real data distributions. The proposed random-forest kernel is applied in MMD GAN, showcasing its advantages. The random-forest kernel is applied in MMD GAN, illustrating its advantages and training technique. It consists of T random trees, each with internal nodes Nt and leaf nodes Lt. The routing function \u00b5t(x; \u03b8tF) determines the probability of x falling into a leaf node, based on the tree structure. The random-forest kernel in MMD GAN utilizes T random trees with internal nodes Nt and leaf nodes Lt. The decision function p t j (x; \u03b8 t F) is derived using a convolutional neural network for constructing decision functions. The parameter \u03b8 N trained from a CNN network is denoted as h(\u00b7; \u03b8 N) for d-dimensional output. The connection function C represents the link between internal nodes and CNN outputs. Each internal node is randomly assigned to an element of the CNN output. Leaf nodes in each tree have unique paths with lengths determined by the number of left son nodes. The random-forest kernel is proposed for MMD GAN, utilizing ensemble learning benefits. The output of the MMD GAN discriminator can be seen as feature inputs to the model. The random-forest kernel in MMD GAN benefits from ensemble learning. It combines CNN, kernel method, and ensemble learning for discriminative power. The random-forest kernel has theoretical advantages, being characteristic and providing unbiased gradients for MMD GAN. The authors propose a penalty to prevent trees in the random-forest kernel from getting stuck on plateaus by encouraging balanced splits at internal nodes. This penalty penalizes the difference between desired and actual average probabilities of falling into two subtrees. The authors introduce a regularizer in the training of random-forest kernel to promote balanced splits at internal nodes, enhancing non-linear discrimination power. The regularizer formulation considers real and generated samples, calculating specific kernel functions. The final regularizer R is the sum of three parts, incorporating different sets of samples. The authors present theoretical results on the unbiasedness of Random-Forest Kernel MMD GAN training. They evaluate the random-forest kernel in the context of MMD GAN and compare it with the Gaussian kernel. The random-forest kernel is compared with Gaussian and rational quadratic kernels in terms of symmetry and complexity. It is noted that the asymmetry and complexity of the random-forest kernel may aid in discriminating between distributions in MMD GAN training. Different architectures are used for different datasets, with batch normalization being utilized in all experiments. In all experiments, batch normalization is used in the generator and spectral normalization is used in the discriminator. The discriminator's kernel details are in Appendix A.1. The discriminator output layer dimensions are set to 70 for random-forest kernel and 16 for other kernels. The initial learning rate is 10^-4, decreasing by 0.8 at iterations 30000, 60000, 90000, and 120000. Adam optimizer with momentum parameters \u03b21 = 0.5 and \u03b22 = 0.999 is used. Batch size is 64, models trained for 150000 iterations on CIFAR-10, CelebA, and LSUN bedroom datasets. The study evaluates the performance of different kernels and loss functions in GANs using metrics like Inception Score, Fr\u00e9chet Inception Distance, and Kernel Inception Distance. Results show comparisons between random-forest kernel, mix-rbf kernel, mix-rq kernel, and rbf-b kernel in various settings. The study compares the performance of different kernels and loss functions in GANs using metrics like Inception Score and Kernel Inception Distance. Results show that the random-forest kernel outperforms mix-rbf and mix-rq kernels in CIFAR-10 and LSUN bedroom datasets. Additionally, the random-forest kernel shows comparable or better performance than the rbf-b kernel in terms of the repulsive loss. Visualizations of model-generated pictures with various kernels and loss functions are provided in the appendix. The random-forest kernel formulation includes 10 trees with a depth of 3, totaling 70 internal nodes. The study compares different kernels and loss functions in GANs using metrics like Inception Score and Kernel Inception Distance. The random-forest kernel formulation includes 10 trees with a depth of 3, totaling 70 internal nodes. Parameters are trainable, with fixed values for experimental simplification. Comparison with bounded RBF kernel is done in the MMD GAN setting. Visualizations of model-generated pictures with various kernels and loss functions are provided. In a 2-dimensional visualization experiment, 25000 points are randomly generated from a uniform distribution with an input dimension of 70. The random-forest kernel output values are calculated, and the points are transformed to 2-dimensional using t-SNE. The local structure of the random-forest kernel is visualized using filled contours. Different architectures are used for experiments on CIFAR-10, LSUN bedroom, and CelebA datasets. The network architecture used in image generation on CIFAR-10 and LSUN bedroom datasets includes a discriminator and a 10-layer ResNet generator. The ResNet generator consists of a linear layer and 4 residual blocks. The shape parameters for CIFAR-10 are h = w = 4, and for LSUN bedroom, h = w = 8. The output dimension of the discriminator varies depending on the kernel used. Section B discusses the main propositions used to prove Theorem 2, including representing neural networks as computation graphs and proving the Lipschitz property. In Section B.3, the Lipschitz property of discriminators is proven, along with the non-differentiability of the network for certain parameters. The measure of bad parameter sets is shown to be zero. Section C provides explicit proofs of main propositions and Theorem 2. The use of deconvolution nets for spatial upsampling and batch normalization for stabilizing learning are discussed. Relu functions are utilized as non-linear components in both generator and discriminator networks, avoiding spatial pooling. In feed-forward networks like CNN and FC, non-linear components like Relu functions are used, avoiding spatial pooling. The network is formulated as a computation graph with layers and nodes, where each node corresponds to a function that receives input and outputs a vector. The network can be defined recursively based on the construction of the graph. The functions in the network can be linear or non-linear, with non-linear functions like ReLU and max-pooling having no learnable weights. The network is defined recursively based on the graph structure, with nodes representing functions that output vectors. The neural network's output neurons are denoted by \u03b8, with activation functions considered piecewise analytic. This means the functions can be partitioned into pieces, satisfying conditions for being piecewise analytic. The neural network's output neurons have activation functions that are piecewise analytic, allowing for partitioning into pieces that satisfy conditions for being piecewise analytic. The Lipschitz property of the discriminative networks is investigated in this section. In the analysis of discriminators' parameters, the focus is on the ball B r (\u03b8 D ) with center \u03b8 D and radius r. The \"bad sets\" where the network is not differentiable are intentionally ignored, and it is shown that their measure is zero. Proposition 5 states that the set of critical parameters has measure zero for any distribution P X. To prove this, Lemma 6 and Lemma 7 are introduced. Lemma 6 describes the growth and Lipschitz properties of neural networks, including convolutional and fully connected networks. It proves these properties by induction on the nodes of the network, showing that functions like sigmoid and ReLU are Lipschitz continuous. The sigmoid and ReLU functions are shown to be Lipschitz continuous in neural networks. The random forest's growth conditions and Lipschitz property are also discussed. The random forest model is analyzed for Lipschitz properties, showing that the nodes in the forest have specific assignments. The Lipschitz continuity of the random forest is demonstrated through probability functions and product forms. The Lipschitz property of the random forest model is demonstrated through probability functions and product forms. The composition of the neural network and the random forest is shown to satisfy growth conditions and Lipschitz properties. The growth conditions and Lipschitz property of the random forest model are proven using continuous functions and linear kernel properties. The concatenation of functions leads to the assertion being proved. The linear kernel is shown to be continuously differentiable with specific growth conditions. The proof of Proposition 5 involves introducing Lemmas 9 and 11, describing paths in the computational graph, and defining backward trajectories. The growth conditions and Lipschitz property of the random forest model are established using continuous functions and linear kernel properties. The linear kernel is shown to be continuously differentiable with specific growth conditions. Lemma 9 introduces backward trajectories in the network graph, showing the existence of a real analytic function for nodes in the graph. The proof of Lemma 9 involves induction on the nodes of the network, demonstrating the analytic properties of the functions involved. The proof of Lemma 9 involves demonstrating the existence of a real analytic function for nodes in the network graph, showing properties of the functions involved through induction on the nodes. The proof of Lemma 9 involves demonstrating the existence of a real analytic function for nodes in the network graph, showing properties of the functions involved through induction on the nodes. Then, the continuity of the network and g p0 imply that S p0 is a closed set. For \u03b7 small enough, there holds B \u03b7 (\u03b8 N ) \u2286 p\u2208A S p, contradicting the assumption \u03b8 N \u2208 \u2202S \u2202i. Consequently, there exists a j \u2208 {1, . . . , J i } satisfying equation 20. Lemma 10 states that a real analytic function F is either zero or has measure zero. Lemma 11 involves induction to show that for certain boundaries, there exists a pair that satisfies specific conditions. The proof involves contradiction and the existence of trajectories within the set boundaries. Lemma 10 states that a real analytic function F is either zero or has measure zero. Lemma 11 involves induction to show that for certain boundaries, there exists a pair that satisfies specific conditions. Moreover, since \u03b8 N \u2208 \u2202S \u2202i, there exists an index p \u2208 \u2202i such that for s = (p, q), we have \u03b8 N \u2208 M s. By the assumption \u00b5(\u2202S \u00ac\u03c0(i)) = 0, we conclude that \u00b5(\u2202S \u00aci) = 0. The random forest \u00b5(T)(\u00b7) can be considered as the composition of affine transformations and sigmoid functions, making it continuously differentiable with respect to \u03b8 F. The differentiability of the neural network h \u03b8 N (x) with respect to \u03b8 N is investigated. Lemma 10 states that a real analytic function F is either zero or has measure zero. Lemma 11 involves induction to show that for certain boundaries, there exists a pair that satisfies specific conditions. The proof of the inclusion \u0398 x \u2286 \u2202S P is done by contradiction. The network is differentiable at \u03b8 N,0, contradicting the fact that \u03b8 N,0 \u2208 \u0398 x. Therefore, we have \u0398 x \u2286 \u2202S P and hence \u00b5(\u0398 x ) = 0. The sets S 1 and S 2 are measurable, with S 1 \u2282 S 2, leading to \u03bd(S 1 ) \u2264 \u03bd(S 2). Fubini's theorem is implied by Lemma. The proof involves showing that if \u03bd(S 1 ) = 0, then \u00b5(\u0398 P X ) = 0. Lemma 13 states conditions for a function to be differentiable at a point. Theorem 2 is proven using augmented networks. Consider the augmented network h(\u03b8D,\u03c8)(X,Z) with inputs from PX\u2297PZ. By Proposition 5, the network is differentiable at \u03bb0 for almost all \u03b8D and all \u03c8. A sequence (\u03bbn) converging to \u03bb0 implies a \u03b4>0 such that \u03bbn\u2212\u03bb0<\u03b4 for all n. Proposition 4 shows the existence of a regular function c(u) with EPXc(X)<\u221e such that |K(h\u03bbn(u))\u2212K(h\u03bb0(u))|\u2264c(u)\u03bbn\u2212\u03bb0. This leads to |\u2202\u03bbK(h\u03bb0(u))|\u2264c(u) for almost all u. The sequence g_n(x) converges to 0 and is bounded by 2c(u). By the dominated convergence theorem, g_n(x) converges to 0. The networks h(2) and h(3) also exhibit similar results. MMD 2 u is unbiased according to Lemma 6 in Gretton et al. Generated samples on CIFAR-10, CelebA, and LSUN bedroom datasets are shown in Figures 4, 5, and 6."
}