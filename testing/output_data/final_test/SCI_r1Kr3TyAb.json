{
    "title": "r1Kr3TyAb",
    "content": "In this work, a mathematical analysis is conducted on the Batch normalization (BN) effect on gradient backpropagation in residual network training. The analysis shows that BN and residual branches work together to confine gradient variance, avoiding the gradient vanishing/explosion problem. It is also discussed that shallower yet wider residual networks have stronger learning performance than deeper yet thinner ones. Batch normalization (BN) is introduced to reduce internal covariate shift in deep neural networks. It normalizes each input element at each layer to improve training efficiency. Batch normalization (BN) normalizes each dimension of the input to a layer before providing it as new input. A mathematical analysis on gradient propagation in batch normalized networks is conducted to address the lack of discussion on the backpropagated gradient effect. Techniques like normalized initialization and intermediate normalization layers enable deep networks with multiple layers to converge using SGD in backpropagation. The concept of residual networks, introduced to address the degradation in accuracy of conventional CNNs as network layers increase, involves residual blocks fitting a residual mapping. The highway network, inspired by LSTM, includes additional gates in shortcut branches. A mathematical model is proposed to analyze the effect of batch normalization on gradient propagation in training residual networks, showing their superior performance. Wide residual networks were introduced to address the issue of gradient flow in residual networks, where some blocks may not learn useful representations. This can result in only a few blocks contributing significantly while others have minimal impact on the network's learning process. The paper discusses the issue of dormant residual blocks in deep neural networks, which hinder gradient propagation and network convergence. It proposes a mathematical model to identify these blocks and suggests proper initialization as a solution to the vanishing/exploding gradient problem. Proper initialization of neural networks is crucial for faster convergence to a good local minimum. Different methods have been proposed, including random weight initialization with sigmoid activation in its linear region and adopting a properly scaled uniform distribution. Recent approaches consider ReLU/PReLU activation and focus on avoiding exponential reduction or magnification of input and gradient magnitudes. Weight vectors are initialized with zero mean and a certain variance value to achieve this objective. The study focused on deriving the variance of activations and gradients at each layer to determine the initial variance of weight vectors. By comparing the variance of backpropagated gradients, they analyzed the Batch Normalization effect. In the simplest case of a layer with Batch Normalization, input and output values were denoted as x and x, with z representing the standard normal variate of x. The gradient backpropagation involved input gradient values to the Batch Normalization layer as \u2206x. In gradient backpropagation, the batch of input gradient values to the BN layer is \u2206x while the batch of output gradient values from the BN layer is \u2206x. By manipulating the formulas, we can derive E(\u2206x i ) = 0 and V ar( DISPLAYFORM1 3.2 CASCADED BN/RELU/CONV LAYER. This section examines a more complex case with three operations in cascade: batch normalization, ReLU activation, and convolution. The input to the Lth Layer is y L\u22121 and the output is y L. The relationships between the operations are defined by equations. In a cascaded BN/ReLU/Conv layer, the mean and variance of the output are derived from the input. The BN sub-layer output is \u03b3i zi + \u03b2i, with zi as the standard normal variate of the input. The ReLU sub-layer effect is analyzed assuming a small parameter a, leading to a nearly uniform distribution of zi. The influence of the Conv sub-layer is considered under the assumption of independent elements in WfL with a mean of 0. In a cascaded BN/ReLU/Conv layer, the mean and variance of the output are derived from the input. The BN sub-layer output is \u03b3i zi + \u03b2i, with zi as the standard normal variate of the input. The ReLU sub-layer effect is analyzed assuming a small parameter a, leading to a nearly uniform distribution of zi. The influence of the Conv sub-layer is considered under the assumption of independent elements in WfL with a mean of 0. All elements in WfL and yL\u22121 are mutually independent with the same distribution of mean 0. Gradient propagation from the Lth layer to the (L \u2212 1)th layer is focused on, with the gradient passing through the BN sub-layer of the Lth layer having a mean of 0. Elements in WbL and \u2206yL are also assumed to be mutually independent with a mean of 0. In a cascaded BN/ReLU/Conv layer, the output mean and variance are derived from the input. The gradient propagation through ReLU assumes independence of elements. Gradients passing through the BN sub-layer are analyzed for independence. The final result is obtained by considering the terms for checking gradient explosion or vanishing. The Batch Normalization (BN) simplifies weight initialization by helping maintain gradients across the network, stabilizing optimization. The last product term in the equation is crucial for checking gradient explosion or vanishing. The BN ensures propagated gradients are maintained throughout the network, especially when weight initialization is consistent. Residual blocks in a network have multiple scales with a fixed number of blocks, each with the same number of filters. The input passes through BN, ReLU, and CONV sub-layers in the first block to shape it, while in subsequent blocks, it only goes through the shortcut branch. The convolution branch in all blocks consists of BN, ReLU, and CONV sub-layers. The convolution branch in the network consists of sequences of BN, ReLU, and CONV sub-layers. The constants c2 and a are defined for simplifying computations. Each residual block in a scale receives inputs and outputs of different sizes. In the network, each residual block in a scale receives inputs and outputs of different sizes. The variance of the gradient in the backward propagation is computed step by step, showing the relationship to the gradient vanishing and explosion problem. The gradient variance across a batch is used as a measure for the stability of gradient backpropagation. The number of filters in each convolution layer increases by a factor of k in each scale. The variance of weights is assumed to be approximately equal across layers. The gradient variance across a batch is used as a measure for the stability of gradient backpropagation in a Resnet-15 model with 15 residual blocks and 3 scales on the CIFAR-10 dataset. The variance of weights is assumed to be approximately equal across layers, and the change in gradient variance between residual blocks is minimal, especially with high L values. The behavior observed in the gradient variance plots aligns with the analysis, showing a gradual increase from right to left due to backpropagation effects. The gradient variance across scales in a ResNet model shows a gradual increase in slope, with variance inversely proportional to distance between residual blocks. A dip in variance occurs when moving between scales, attributed to the BN sub-layer in the shortcut branch. Experiments support the theory, including the concept of \"effective depth\" for gradient paths in ResNet. The gradient updates in training ResNet models mainly come from paths of 5-17 modules in length, showing that residual networks are an ensemble of sub-networks. Some residual blocks remain dormant during gradient backpropagation, leading to a consistent gradient variance. This supports the concept of effective depth and implies non-varying gradient values in the weights of residual blocks. In ResNet models, gradient updates mainly come from paths of varying lengths, leading to consistent gradient variance. Networks with lower depth show sharper gradient changes between residual blocks, utilizing weights more effectively for better learning. Three Resnet models are compared based on block numbers and filter sizes, trained on CIFAR-10 dataset. Training accuracy comparisons are shown in figures for Resnet-9 vs. Resnet-99 and Resnet-9 vs. Resnet-33. Resnet-9 reaches higher accuracy faster than Resnet-99 and Resnet-33, with a smaller gap between Resnet-9 and Resnet-33. Test set accuracy shows Resnet-9 performing the best and Resnet-99 the worst. Resnet with 9 blocks achieves 94.4% accuracy. Gradient variance remains consistent across epochs for Resnet-99, Resnet-33, and Resnet-9. The gradient variance remains consistent across Resnet-99, Resnet-33, and Resnet-9. Resnet-99 has slowly varying gradients at the end of a scale, while Resnet-9 shows sharper changes. Shallower but wider Resnets demonstrate stronger learning performance. Mathematical analysis explores the impact of Batch Normalization on gradient stability in residual network training. The Saak transform, introduced by BID9, offers a new perspective on deep learning without the need for data labels or backpropagation in training filter weights. Comparing multi-stage Saak transforms with residual networks could provide insights into their performance.ReLU is applied to the output of a BN layer to enhance learning capabilities. The ReLU operation is applied to the output of a BN layer to calculate variance and mean. The shifted Gaussian variate due to the BN operation is discussed, showing the step-by-step procedure in deriving the results. The ReLU operation is applied to the output of a BN layer to calculate variance and mean. The results show that E(ReLU (\u03b3z + \u03b2)) = \u03b3E(y) and E((ReLU (\u03b3z + \u03b2)) = \u03b3^2E(y^2). The ReLU operation is applied to the output of a BN layer to calculate variance and mean. Function F is defined as DISPLAYFORM0 where \u2206y denotes input gradient in gradient backpropagation and y denotes the input activation during forward pass to the ReLU layer. The probability that the input in forward pass to the ReLU layer is greater than 0 is DISPLAYFORM1. Using eq 5 and the assumption that the input standard normal variate in forward pass and the input gradient in gradient pass are independent, we have DISPLAYFORM2. The ReLU operation is applied to the output of a BN layer to calculate variance and mean. Function F is defined as DISPLAYFORM0 where \u2206y denotes input gradient in gradient backpropagation and y denotes the input activation during forward pass to the ReLU layer. The probability that the input in forward pass to the ReLU layer is greater than 0 is DISPLAYFORM1. Using eq 5 and the assumption that the input standard normal variate in forward pass and the input gradient in gradient pass are independent, we have DISPLAYFORM2. For the last scale, the field is k times smaller, and based on Eq. (10), we get DISPLAYFORM1. The gradient passes through two BN-ReLU-Conv Layers in the convolution branch of block L = N > 1, with the same receptive field between the two layers. Using the output of the previous residual block in place of the denominator, we assume independence between \u2206 L and \u2206 L. For Block L>1 without BN-ReLU-Conv Layer in shortcut branch, V ar(\u2206 L,i ) = V ar(\u2206y L,i ). All models had 15 residual blocks, with Model 1 reaching higher accuracy faster than the other two models. Model 1 achieved a final accuracy of 92.5%, Model 2 90.6%, and Model 3 9.09%. Model 3, without BN, shows poor performance with only 9.09% accuracy. Model 1 performs the best with 92.5% accuracy, followed closely by Model 2 at 90.6%. The gradient variance for Model 2 remains stable, while Model 3 experiences gradient explosion from the start. Batch normalization (BN) is crucial for stabilizing training of residual networks, preventing gradient vanishing and explosion. Model 3 struggled without BN, leading to undefined loss function and limited learning. Removing BN hindered training from the start, highlighting the importance of batch normalization in optimization."
}