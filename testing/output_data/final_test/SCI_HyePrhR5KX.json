{
    "title": "HyePrhR5KX",
    "content": "Representation Learning over graph structured data has gained attention for its wide applicability. Most progress has been in static graphs, but joint learning of dynamic and static aspects is still early. DyRep is a new framework for dynamic graphs, using a deep temporal point process model to capture evolving network information. DyRep is a deep temporal point process model that captures evolving network information with a temporal-attentive representation network. It outperforms state-of-the-art baselines for dynamic link prediction and time prediction tasks, demonstrating its capability to generalize over unseen nodes. Recent advances in deep learning have helped address the challenge of encoding high-dimensional and non-Euclidean graph information. There is a growing need for principled approaches to advance graph embedding techniques for dynamic graphs with complex temporal properties. Graph embedding techniques for dynamic graphs focus on modeling two distinct dynamic processes: Topological Evolution and Node Interactions. Most real-world graphs exhibit these processes evolving at different time scales, highlighting the need to model interleaved dependencies between them. The next step for advancing dynamic graph models is to understand dependencies between evolving processes. Communication events involve nodes interacting, while evolving representations aim to capture evolving graph information over time. Existing techniques include a discrete-time approach that observes dynamic graphs as static snapshots, but they may lose fine-grained temporal information. The continuous-time approach aims to model evolution at a finer time granularity to address challenges in capturing fine-grained temporal dynamics and selecting appropriate aggregation granularity. Existing approaches have shown effectiveness in specific settings but struggle to model highly nonlinear evolution of structural properties coupled with complex temporal dynamics. It remains an open problem to effectively learn informative representations for such complex systems. Our goal is to learn node representations by modeling a latent mediation process that bridges the Association Process and Communication Process in dynamic graphs. This process drives complex temporal dynamics and leads to the nonlinear evolution of node representations. The text discusses the evolution of node representations in dynamic graphs through a latent mediation process that bridges association and communication processes. This phenomenon, termed evolution through mediation, illustrates how changes in a node's neighborhood impact its representation and social interactions. The proposed framework, DyRep, aims to model this interleaved evolution effectively. The framework DyRep captures continuous-time temporal dynamics in dynamic graphs by updating node representations based on association and communication events. It utilizes a deep temporal point process and a Temporal Attention Mechanism to model the structural and temporal components effectively. Our framework DyRep captures continuous-time temporal dynamics in dynamic graphs by updating node representations based on association and communication events. An efficient unsupervised training procedure is designed for end-to-end training. Significant improvement over state-of-the-art baselines is demonstrated on real-world dynamic graphs for dynamic link prediction and time prediction tasks. Extensive qualitative analysis through embedding visualization and ablation studies confirms the effectiveness of our framework. Dynamic network embedding techniques aim to learn functions for computing node representations that can be applied to new nodes. Various methods, such as matrix factorization, CNN-based approaches, and deep recurrent models, have been used for dynamic network embedding. Existing literature focuses on temporal modeling of dynamic networks for link prediction tasks, but lacks a focus on representation learning. Previous models have not considered time at a finer level or captured both topological evolution and interactions simultaneously. Stochastic point processes are random processes involving discrete events in time, represented as a counting process N(t). Temporal point processes are characterized by the conditional intensity function \u03bb(t), which models the rate of events happening given previous events. This is crucial for understanding dynamic network embedding techniques. The tiny window DISPLAYFORM0 represents the history of events until time t, with k indicating the type of event. Persistent edges in the graph are only formed through topological events, while interaction events do not contribute to them. The events are ordered by time in window DISPLAYFORM1, with k representing the evolution rate associated with topological and interaction events. Node representation in a dynamic graph setting involves evolving d-dimensional representations of nodes over time, denoted as z v (t). The framework observes the graph evolution as a stream of events, with new nodes being incorporated naturally. The method is inductive, focusing on learning functions to compute node representations rather than node-specific embeddings. DyRep is a unified architecture designed to model evolving information over graphs by incorporating a two-time scale temporal point process model and an inductive representation network to learn node representations. The framework supports growth of networks by modeling addition of nodes and structural edges, with future work planned for deletion. The DyRep framework models evolving information on graphs by incorporating a two-time scale temporal point process model. It aims to learn node representations that encode local and global effects of communication and association events, driving the dynamics of observed events. The DyRep framework utilizes a continuous-time deep model of temporal point processes to model communication and association events on graphs. It learns node representations that capture the effects of these events, with a focus on compatibility between nodes at the most recent timepoint before an event occurs. The function g k (t) is learned through a representation network and parameterizes the intensity function of the point process model in the DyRep framework. An attention mechanism is used to understand how the past influences the future, and an outer function f k must meet criteria for positivity and capturing different time scales. A modified softplus function with a dynamics parameter \u03c8 k is employed to account for these time-scale dependencies. In a deep recurrent architecture, the intensity function is parameterized and node representations are learned to capture the effect of observed events. Self-propagation and exogenous drive principles govern node evolution in the embedded space. The node's features are updated during time intervals between global events. Nodes involved in events create pathways for information propagation. Node u acts as a bridge between neighborhoods u and v, aggregating information for v. Node u acts as a bridge between neighborhoods u and v, aggregating information for v using an aggregate function parametrized by u's communication and association history with its neighbors. The embeddings for both nodes involved in an event are updated using a recurrent architecture, evolving z v based on the previous representation of node v. Initial representation of a node v can be initialized using input node features from the dataset or a random vector. Eq. 4 represents a neural network based functional form for this process. The Localized Embedding Propagation principle captures rich structural properties based on neighborhood structure, supporting unseen nodes and various node and edge types. Attention mechanisms are used to capture information from each neighbor in a weighted fashion, focusing on the most relevant parts of the input for decision-making. The proposed Temporal Point Process based Attention Mechanism uses temporal information to compute evolving importance of neighbors to a node over time. The Temporal Point Process based Attention Mechanism utilizes temporal information to calculate attention coefficients for structural edges between nodes, which are then used to compute the aggregate quantity required for embedding propagation. The mechanism involves a stochastic matrix capturing the strength between pairs of vertices at a given time, influencing node communication and selection processes. The construction of h u struct relies on implications related to node associations and neighborhood definitions. The proposed conditional intensity based attention layer utilizes a matrix to compute attention coefficients over the 1-hop neighborhood of a node. These coefficients are used to aggregate information from neighbors, with parameters governing the information propagation. The mechanism involves a temporally evolving attention weight for each neighbor, leading to an attended aggregation mechanism for embedding propagation. The proposed temporal attention layer utilizes max operator for node embeddings, capturing different aspects of the neighborhood effectively. It is inspired by Graph Attention Networks (GAT) and Gated Attention Networks (GaAN) for applying non-uniform attention over the neighborhood. GAT employs multi-head non-uniform attention, while GaAN applies different weights to different heads in the multi-head attention formulation. The proposed model introduces a parameterized attention mechanism using a point process based temporal quantity S to drive the impact of each neighbor on a node. Unlike static methods, attention coefficients are used as input to compute the temporal-structural effect of the neighborhood. The construction and update of the stochastic matrix S capture complex temporal information, evolving over time to capture multiple representation spaces. The model introduces a parameterized attention mechanism using a temporal quantity S to impact each neighbor on a node. S is constructed directly from A at the initial timepoint and updated based on observed events. Temporal attention is applied only to the structural neighborhood of a node, with values updated in specific scenarios involving interactions or associations between nodes. Background attention for each edge changes based on neighborhood size when events involving a node occur. The model introduces a parameterized attention mechanism using a temporal quantity S to impact each neighbor on a node. Whenever an event involving a node occurs, the attention values for corresponding entries are updated based on the intensity of the event. The background attention for each edge also adjusts as the neighborhood size grows. This update process resembles a standard temporal point process formulation, with background attention and endogenous intensity-based attention playing key roles. Updates to the attention matrix A in a directed graph are not symmetric, affecting the neighborhood structure and attention flow for a node. Appendix A provides a visual representation of the complete DyRep framework. The complete DyRep framework introduces a parameterized attention mechanism using a temporal quantity S to impact each neighbor on a node. The model includes an extensive ablation study to discern the contribution of its components. Parameters are learned by minimizing negative log likelihood, with a focus on event intensity and survival probability. Local optimization is achieved using mini-batch stochastic gradient descent and a novel sampling technique. Algorithm 2 in Appendix H utilizes a Monte Carlo trick to compute the survival term of the log-likelihood equation. The DyRep framework introduces a parameterized attention mechanism using a temporal quantity S to impact each neighbor on a node. Algorithm 2 utilizes a Monte Carlo trick to compute the survival term of the log-likelihood equation in each mini-batch, with complexity O(2mkN). The training procedure is adopted from BID14, maintaining dependencies between events across sequences. #Communications: 604649 and Clustering Coefficient: 0.087 are mentioned. The DyRep framework introduces a parameterized attention mechanism using a temporal quantity S to impact each neighbor on a node. The datasets used cover a range of configurations, with the Social Dataset being a small network with high clustering coefficient and over 2M events, and the Github dataset being a large network with low clustering coefficient and sparse events. The effectiveness of DyRep is evaluated on tasks of dynamic link prediction and event time prediction. The DyRep framework introduces a parameterized attention mechanism using a temporal quantity S to impact each neighbor on a node. To understand how well the model captures these phenomena, questions are asked about the most likely node u to undergo an event with node v governed by dynamics k at time t. The conditional density of such an event at time t can be computed as \u03bb(s)ds, where t is the time of the most recent event on either dimension u or v. The conditional density is used to find the most likely node. Mean Average Rank (MAR) and HITS(@10) metric are reported for dynamic link prediction, as well as Event Time Prediction. The DyRep framework introduces a parameterized attention mechanism using a temporal quantity S to impact each neighbor on a node. Event Time Prediction involves computing the next time point for a specific event type between nodes u and v. The model estimates the conditional density at time t using a Monte Carlo trick and calculates the next event time. Mean Absolute Error (MAE) is reported for the prediction against the ground truth. GAT is utilized for supervised learning, with results shown in the ablation studies. The model is compared against Know-Evolve for performance evaluation. The model compares against Know-Evolve and Multi-dimensional Hawkes Process (MHP) BID8 for time prediction in dynamic graphs. Test sets are divided into time slots for comprehensive evaluation of different methods. Dynamic and static baselines are trained using a sliding window approach with warm-start method. The model outperforms baselines in predicting communication events between nodes without permanent edges. It shows better performance over time but drops slightly due to temporal effects on node evolution. However, features learned through edge-level modeling may limit its predictive capacity in the long run. The inability of DynGem, DynTrd, and GraphSage to outperform Node2vec indicates that snapshot-based models struggle to capture communication event dynamics. Despite low overall performance on the Rank metric due to the sparse Github dataset, our method excels when there is communication history available, as shown by superior performance on the HITS@10 metric. Our model excels in accurate prediction for nodes with better history, capturing evolving topology effects missed by Know-Evolve. It maintains strong performance on HITS@10 metric, outperforming baselines in association event prediction across datasets. The model's robustness in learning from data properties is demonstrated by its performance on Social evolution dataset with a small number of association events. The model shows strong performance in capturing evolving topology effects and generalizing across new nodes on the Github dataset. It outperforms baselines in event time prediction and demonstrates better performance than state-of-the-art baselines. Know-Evolve does not explicitly capture variance in time scales or consider neighborhood influence, leading to weaker temporal-structural dynamics across the graph. DyRep's evolving embeddings show more discriminative power compared to GraphSage embeddings, as they can capture distinctive and evolving structural features over time. This is demonstrated through qualitative analysis and visualization of tSNE embeddings. DyRep keeps embeddings of associated nodes nearby, even if not in the same cluster, showcasing its ability to capture persistent edge relationships. Our novel modeling framework for dynamic graphs effectively learns node representations by bridging topological evolution and node interactions. A deep temporal point process model with a temporally attentive representation network encodes structural-temporal information into low dimensional representations. Superior evaluation performance compared to state-of-the-art methods is demonstrated. The novel modeling framework for dynamic graphs effectively learns node representations by bridging topological evolution and node interactions. It presents a generic and unified representation learning framework that supports a wide range of dynamic graph characteristics. Additionally, a novel temporal point process based attention mechanism is proposed to attend over neighborhoods based on communication and association events in the graph. DyRep currently does not support network shrinkage due to challenges in obtaining fine-grained deletion time stamps and the sophistication required by the temporal point process model. Future directions could include augmenting the model with a survival process formulation to account for the lack of nodes/edges at future times. Future directions for the modeling framework include supporting higher order dynamic structures and evolving embeddings for information flow between nodes. Interaction events create temporary pathways while topological events create permanent pathways, influencing the structural evolution of the graph. The difference in importance of nodes u and v is shown by the number of blue arrows. The neighborhood only includes nodes connected by structural edges. When an event occurs, the embeddings of the two nodes involved are updated using Eq 4. Node v relays information to node u, but not equally from all its neighbors. Node v receives information to relay based on its communication history with neighbors, computed using attention coefficients on structural edges. The attention module is parameterized with temporal point process Suv. DyRep computes the evolving attention between connected nodes based on their association and communication history. The attention coefficient function is parameterized by the intensity of events, allowing the importance of neighbors to evolve over time. This mechanism aligns with real-world phenomena and has a connection to Marked Point Process. DyRep computes evolving attention between connected nodes based on their association and communication history, parameterized by event intensity. This aligns with real-world phenomena and has a connection to Marked Point Process. In a mathematical viewpoint, events can be described as a marked process, but from a machine learning perspective, extracting information from the mark space is crucial for discovering the structure in point processes efficiently. The nodes in the graph represent dimensions of the point process, allowing for capturing dependencies between nodes. The topological evolution of networks occurs at a different temporal scale than activities on a fixed topology network. Updates in attention and neighborhood size occur based on event intensity, affecting the evolution of both processes. The abstracting of k to associate it with different scales of evolution facilitates modeling dynamic graphs at two time scales. This allows capturing the influential dynamics of topological evolution on network activities and vice versa. The distinction in the use of mark information is important for learning representations for nodes but not for k, which represents two different scales of event dynamics. Our method models dynamic graph processes at two time scales, unlike BID14 which only models interaction dynamics at a single time scale. While both use point processes for temporal dynamics, our approach explicitly captures topological evolution. Adding type as a component in the mark space represents events, with k signifying different dynamic scales. The use of a softplus function in our Deep Point Process Model enables modeling of dynamical processes over graphs at multiple time scales. Our method employs a softplus function with a dynamic scale parameter to model dynamical processes over graphs at multiple time scales. This two-time scale model induces modularization and allows for complex, nonlinear dynamics towards a non-zero steady state intensity. Additionally, representation learning over graphs aims to capture both global position and local neighborhood structural information of nodes. DyRep introduces a novel Localized Embedding Propagation principle for incorporating graph structure in node representation, while BID14 focuses on single edge level information. Deep Temporal Point Process Based Self-Attention emphasizes the importance of nodes in dynamic graphs, advancing over existing static graph approaches like Graph Attention Networks. DyRep introduces a novel Localized Embedding Propagation principle for incorporating graph structure in node representation, supporting node attributes and edge types. The model can handle different types of edges and node attributes, including high-dimensional ones. DyRep introduces a novel approach for incorporating graph structure in node representation by utilizing edge type information and supporting new nodes with initial embeddings based on raw features or random initialization. This allows for the computation of intensity functions for events involving new nodes. The framework in Eq 1 allows for computing the embedding of new nodes using Eq 4. Two cases are possible: one new node or both nodes are new. For one new node, h struct is computed using the neighborhood of the existing node, while for both new nodes, h struct is the feature vector of the other nodes. Algorithm 1 treats new nodes the same as existing nodes. Both A and S are qualified by time. The DyRep framework updates matrices for new nodes based on time. It suggests two ways to specify matrix dimensions: either construct matrices with total possible nodes or expand dimensions as new nodes appear. DyRep unifies components for effective node representation in dynamic graphs. Ablation study shows its three main parts: Multi-time scale point process model, Representation Update Formulation. The DyRep framework consists of three main parts: Multi-time scale point process model, Representation Update Formulation, and Conditional Intensity Based Attention Mechanism. The model is evaluated on a large github dataset, with focus on design choices in each component. The DyRep-Comm variant removes time-scale independence and trains only on Communication Events, showing a decline in prediction performance for both communication and association events. The DyRep-Assoc variant also shows a decline in performance, particularly for Association events. The DyRep framework consists of three main parts: Multi-time scale point process model, Representation Update Formulation, and Conditional Intensity Based Attention Mechanism. DyRep-Assoc variant trains only on Association Events, showing a decline in prediction performance for both communication and association events. The performance degrades more for Communication events compared to Association Events, indicating the importance of considering events at different time scales. In the Representation Update Formulation of the DyRep framework, different variants are explored by switching off components to observe their effects. DyRep-No-SP variant shows that self-propagation has weak features and mainly captures the recurrent evolution of latent features independently. DyRep-No-Struct variant, which removes the structural part of the model, results in a drastic drop in performance in both scenarios. The necessity of sophisticated structural encoders for dynamic graphs is highlighted by the drastic drop in performance when the attention component is removed. Different variants of the DyRep framework are explored, showing the importance of the S matrix in mediating communication and association events for optimal performance. The experiment on the Social dataset assessed the quality of learned embeddings and the impact of Association and Communication events. Nodes 46 and 76 became associated during the first test slot, reducing cosine distance in both models. This highlights the importance of using both processes for optimal performance in dynamic graphs. DyRep and GraphSage models were tested on association events between nodes 0 and 1. DyRep showed a significant reduction in cosine distance from 1.231 to 0.005, with embeddings converging to the same cluster. GraphSage also reduced cosine distance but embeddings remained in original clusters. Visualization in FIG10 demonstrates effective capture of association events by embeddings. DyRep considered nodes 27 and 70, with many communication events, to be in the same cluster with a cosine distance of 0.005, indicating similar features over time. Nodes with a large number of events tend to develop similar features over time. DyRep embeddings show the temporal evolution of nodes as they form and break from clusters. Representation learning approaches for static graphs can be categorized into node embedding and sub-graph embedding techniques. GraphSage is an inductive method for learning functions to compute node representations that can be generalized to unseen nodes. Various approaches exist to encode higher order graph structures into low dimensional vector representations. NetWalk is a dynamic embedding approach for anomaly detection, while Know-Evolve is a deep recurrent architecture for modeling multi-relational timestamped edges. These methods aim to preserve the optimality of skip-gram objective and address communication processes in relational graphs. Several network embedding methods have been proposed for dynamic environments, including DANE BID14, which focuses on node attribute changes, and a model by Zuo et al. that uses a Hawkes process to capture temporal evolution of neighborhood for nodes. Other models consider temporal information in different ways for dynamic networks. Several approaches in graph mining and temporal relational learning communities focus on dynamic networks but are orthogonal to the current work. Research in learning dynamic embeddings has advanced in the linguistic community, aiming to learn temporally evolving word embeddings. Some models propose learning dynamic embeddings in graph data but do not capture both topological evolution and interactions at a finer time level. The text discusses novel neural network architectures for supervised learning that focus on subgraph patterns in data. It also mentions a dynamic graph model for videos and a probabilistic model for user-item recommendation. Additionally, it touches on the challenges of fixed parametric forms in point processes. The text discusses challenges with fixed parametric forms in point processes and proposes a data-driven alternative to learn the conditional intensity function. It mentions increased interest in using deep learning and GANs for learning with generative temporal point process models. Implementation details include hyperparameter search for optimal performance on social datasets. The text discusses challenges with fixed parametric forms in point processes and proposes a data-driven alternative to learn the conditional intensity function. Implementation details include hyperparameter search for optimal performance on social datasets. For github dataset: Num nodes = 12328, Num Dynamics = 2, bptt = 300, embed_size = 256, hidden_unit_size = 256, nsamples = 5, gradient_clip = 100. Baselines used max_iter = {1000, 5000, 10000}, bptt = {100, 200, 300}, lr = {0.0005, 0.005, 0.1, 1}, embed_E = {32, 64, 128, 256}, embed_R = {32, 64, 128, 256}, hidden = {32, 64, 128, 256}, warm = 0, t_scale = 0.0001, w_scale = 0.1, num_epochs = {10, 50, 100, 500, 1000}. GraphSage code was implemented in Tensorflow for unsupervised train module. Node2Vec was also discussed. The authors used unsupervised training to generate embeddings with Node2Vec, making few changes in hyperparameters."
}