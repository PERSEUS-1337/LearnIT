{
    "title": "SygjB3AcYX",
    "content": "The paper proposes a generalized label propagation (GLP) framework for semi-supervised learning, which integrates graph and data feature information. GLP decomposes label propagation into signal, filter, and classifier components, offering flexibility in selecting filters and classifiers for different applications. It also sheds light on the workings of graph convolutional networks. Extensive experiments on various datasets validate the effectiveness of GLP. Experiments on citation networks, a knowledge graph, and an image dataset show the efficiency of GLP in semi-supervised learning. Leveraging unlabeled data can greatly improve learning performance, with label propagation being a popular method for graph-based semi-supervised learning. LP is a tool used in scientific research and industrial applications for non-oriented graphs. It aims to predict labels of vertices by finding a smooth prediction matrix that agrees with labeled data. The method involves graph Laplacian regularization to ensure smoothness. Graph Laplacian regularization, also known as Tikhonov regularization, is used in LP to make predictions based on graph information. The performance of LP depends on how well the underlying graph structure represents the class information of data vertices. Different applications may require different types of graphs, with some exhibiting a natural structure while others may need to construct a graph using data features. However, in many cases, graphs only partially encode the data. Manifold regularization BID1 trains a support vector machine with a focus on jointly modeling graph and feature information to improve classification performance. In this paper, the authors propose a generalized label propagation (GLP) framework for semi-supervised learning by extending the modeling capability of LP in graph signal processing. GLP applies a low-pass graph filter on vertex features to produce smooth features, enhancing classification performance. GLP extends LP by applying a low-pass graph filter on vertex features for smooth features, improving classification with few labeled examples. It allows for designing graph filters and using domain-specific classifiers. GLP clarifies the mechanisms of GCN, showing its convolutional filter design and parameter setting. GLP extends LP by applying a low-pass graph filter on vertex features for smooth features, improving classification with few labeled examples. Extensive experiments show substantial improvement of GLP over GCN and other baselines for semi-supervised classification. The paper is organized into sections discussing LP in the context of graph signal processing, the proposed GLP framework, revisiting GCN under GLP, designing graph filters for GLP, experimental results, related works, and conclusion. In graph signal processing, the graph Laplacian matrix can be eigen-decomposed as: L = \u03a6\u039b\u03a6 \u22121, where \u039b represents eigenvalues. The graph Laplacian matrix can be eigen-decomposed as: L = \u03a6\u039b\u03a6 \u22121, where \u039b = diag(\u03bb1, \u00b7 \u00b7 \u00b7 , \u03bbn) are the eigenvalues in an increasing order, and \u03a6 = (\u03c61, \u00b7 \u00b7 \u00b7 , \u03c6n) are the associated orthogonal eigenvectors. A graph signal is a real-valued function defined on the vertex set of a graph, which can be decomposed into a linear combination of basis functions. A graph filter is defined as a matrix G \u2208 Rn\u00d7n and is linear shift-invariant. A linear shift-invariant graph filter G can be defined as G = \u03a6p(\u039b)\u03a6 \u22121, where p(\u00b7) is a function that scales the coefficients of basis functions based on their eigenvalues. Low-frequency components in a smooth signal should be preserved while high-frequency components are filtered out, requiring p(\u00b7) to behave like a low-pass filter. The prediction matrix of LP is obtained by solving an optimization problem and setting it to zero. Each vertex is classified by comparing elements in the matrix Z. LP in graph signal processing consists of signal, filter, and classifier components. The input signal matrix is the labeling matrix Y, with columns representing graph signals. The graph filter used in LP has a frequency response function. LP is a low-pass filter with a frequency response function that produces smooth embeddings. The balancing parameter \u03b1 controls the regularization degree, making the filter more low-pass as \u03b1 increases. A nonparametric classifier is applied to classify unlabeled vertices based on the embeddings. The proposed generalized label propagation (GLP) framework extends these concepts. The proposed Generalized Label Propagation (GLP) framework extends the concepts of LP by using the feature matrix X as input signal, applying a low-pass linear filter G, and training a supervised classifier on the filtered features of labeled data to predict labels for unlabeled data. The Generalized Label Propagation (GLP) framework utilizes a low-pass filter G on the feature matrix X to learn representative feature vectors for each class, making downstream classification easier with few samples. By filtering vertices in the same class, they become more similar, allowing for the training of a good classifier. The Graph Convolutional Networks (GCN) model can be interpreted under the GLP framework, explaining its design features like the number of layers and choice of graph Laplacian. The model involves a renormalization trick on the adjacency matrix and symmetric normalization to obtain activations in each layer. The Graph Convolutional Networks (GCN) model involves graph convolution by multiplying input with the renormalized adjacency matrix, followed by feeding into a projection matrix and applying a softmax function for prediction. The model uses a low-pass filter for feature smoothing and is trained using cross-entropy loss on labeled instances. The frequency response function of the filter in the GCN model is linear and low-pass on the interval [0, 1], but not on [1, 2]. Removing the ReLU activation function reveals that GCN is a special case of GLP. The eigenvalues of the normalized Laplacians fall into the interval [0, 2], while the unnormalized Laplacian has eigenvalues in [0, +\u221e]. Using the unnormalized Laplacian will amplify eigenvalues in [2, +\u221e], introducing noise. The GCN model stacks two convolutional layers for feature extraction. The GCN model stacks two convolutional layers, filtering the feature matrix by (I \u2212L s ) 2 with response function (1 \u2212 \u03bb) 2 . This explains why GCNs with two convolutional layers perform better. The renormalization trick shrinks the eigenvalues range from [0, 2] to [0, 1.5], reducing noise and improving performance. The design and computation of low-pass graph filters for GLP are discussed in this section. The design and computation of low-pass graph filters for GLP are discussed. The Auto-Regressive (AR) filter is used in LP, involving matrix inversion with complexity O(n^3). Approximating p ar using its polynomial expansion reduces computational complexity. The renormalization (RNM) filter is an exponential function of the renormalized adjacency filter used in GCN. The renormalized adjacency filter in GCN shrinks the range of eigenvalues of L, resembling a low-pass filter. The exponent parameter k controls the low-pass effect, with computational complexity O(N mk). A random walk filter is also proposed, based on a stochastic matrix for a lazy random walk. The response function of p_rw is derived from the k-step transition probability matrix. p_rw acts as a low-pass filter on [0, 2], with similar eigenvalues to L_s. The curves of (1\u2212\u03bb)^2 and (1\u22121/2\u03bb)^4 are close, suggesting that k in p_rw should be twice as large as in p_rnm for the same low-pass effect. The complexity of RW is O(N mk) like RNM. Filter design for GLP involves controlling filter strength with parameters \u03b1 and k, aiming for filtered features to be closer to class mean with limited labeled data. In this section, GLP is tested on various datasets including citation networks (Cora, CiteSeer, PubMed), a knowledge graph (NELL), and a handwritten digit image dataset (MNIST). Baselines such as GCN, LP, MLP, Planetoid, DeepWalk, ManiReg, SemiEmb, and ICA are compared against GLP on different datasets. The parameters \u03b1 and k should be set large for smoother features but relatively small to preserve feature diversity for accurate class boundaries. GLP is compared against GCN, LP, MLP, and CNN on MNIST. Filter parameters for GLP should be set based on label rate. Parameters for RNM, RW, and AR filters are set differently based on label rate. Classifier selection for GLP is based on highest training accuracy in 200 steps. No validation set is used for classifier model selection. GLP outperforms GCN and other baselines on various datasets without using a validation set. It achieves top classification accuracies on all datasets, especially on NELL with RW and RNM filters. Performance details are provided in TAB0 in the study. GLP outperforms baselines on MNIST without using a validation set. Running times are reported in TAB0, showing GLP is faster than GCN on most datasets. The performance gains of GLP come from leveraging both graph and feature information, with visualizations of raw and filtered features in FIG5. GLP improves classification accuracy by filtering features, leading to a more compact cluster structure. This allows for the use of stronger filters to extract higher-level data representations, resulting in performance gains compared to GCN. Additionally, GLP enables the adoption of domain-specific classifiers such as CNN. The deep GCN is challenging to train, while GLP allows for the use of domain-specific classifiers like CNN for vision tasks. CNN trained on raw features of labeled data shows competitive performance. Various graph-based semi-supervised learning methods assume nearby vertices share labels and use techniques like Markov random walks, Laplacian eigenmaps, spectral kernels, and context-based methods for smooth low-dimensional embedding. Another approach involves graph partitioning to align cuts with labeled data in low-density regions. A popular idea is a quadratic regularization framework to enforce consistency with labeled data and cluster assumptions. To improve predictions, methods like label propagation combined with external classifiers, iterative classification algorithms, manifold regularization, deep semi-supervised embedding, and graph convolutional networks are used. Follow-up works include graph attention networks and attention-based graph neural networks. The proposed GLP framework for semi-supervised learning is simple, flexible, and efficient. It offers new insights and possibilities for future research in graph-based methods. Future research directions for GLP include designing graph filters for different applications, making GLP applicable to inductive problems, developing faster algorithms, and applying GLP to solve real-world problems. The citation networks tested on include CiteSeer, Cora, and PubMed, where vertices represent documents and edges represent citation links. Each vertex is associated with a feature vector indicating word presence in the document. The statistics of datasets from Never Ending Language Learning (NELL) and MNIST are summarized in TAB3. NELL is a knowledge graph converted into a single relation graph with unique one-hot representations for relation vertices. MNIST contains 70,000 images of handwritten digits represented by dense 784-dimensional vectors. The text discusses the construction of a 5-NN graph based on gray intensity pixel values in images. Experimental details are provided for reproducibility, including parameters set for different methods and networks used for citation networks, NELL, and MNIST datasets. The CNN used consists of six layers with specified structure. A learning rate of 0.003 and dropout of 0.5 are utilized. Results on MNIST are averaged over 10 runs. Various supervised classifiers including SVM, DT, LR, and MLP are trained with raw and filtered features, showing a significant improvement in classification accuracy with filtered features. This highlights the advantage of using filtered features over raw features. In an experiment comparing filtered features to raw features, different learning rates and regularization parameters were used for LR, SVM, and DT. The performance of GLP was tested with varying filter parameters k and \u03b1, showing improved classification accuracy over GCN."
}