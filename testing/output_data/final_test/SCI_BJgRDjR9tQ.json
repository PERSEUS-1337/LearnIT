{
    "title": "BJgRDjR9tQ",
    "content": "Robust estimation under Huber's $\\epsilon$-contamination model is a key topic in statistics and theoretical computer science. Rate-optimal procedures like Tukey's median are computationally impractical. This paper establishes a connection between f-GANs and depth functions for robust estimators through f-Learning. Depth functions leading to rate-optimal estimators can be seen as variational lower bounds of total variation distance in f-Learning. This allows for computing robust estimators using GAN training tools. JS-GAN with a neural network discriminator can achieve minimax rate of robust mean estimation under Huber's $\\epsilon$-contamination model. The presence of an unknown contamination distribution poses challenges to robust mean estimation under Huber's $\\epsilon$-contamination model. Neural net structure in the discriminator class is necessary for robust estimation in this setting. Observations are i.i.d and drawn from a mixture of distributions, requiring estimation of the model parameter \u03b8. The contamination distribution Q can lead to significant deviations from the true mean, even with robust estimators. Recent developments in statistics and computer science focus on finding computationally feasible algorithms for estimating \u03b8 under Huber's \u03b5-contamination model. Even robust estimators like coordinatewise median and geometric median are suboptimal in this setting. The minimax rate for normal mean estimation with squared 2 loss is shown to be p n \u2228 2, achieved by Tukey's median, although its computation is not tractable. The search for statistically optimal and computationally feasible procedures remains a fundamental problem. Recent developments in statistics and computer science have focused on developing computationally feasible algorithms for estimating \u03b8 under Huber's \u03b5-contamination model. Two key papers introduced the \"iterative filtering\" and \"dimension halving\" strategies for robustly estimating the normal mean, achieving the minimax rate p n \u2228 2 in polynomial time. These methods rely on the efficient certification of a robust moment estimator by higher moments, leading to the development of robust and computable procedures for various problems. However, many existing procedures for robust mean estimation require knowledge of the covariance matrix. Recent developments in statistics and computer science have focused on developing computationally feasible algorithms for robust mean estimation. Tukey's median and other depth-based estimators have advantages in terms of statistical properties, with clear objective functions and adaptability to unknown nuisance parameters. These procedures offer a robust alternative to traditional methods that rely on knowledge of the covariance matrix. This paper focuses on developing computational strategies for depth-like procedures, such as halfspace depth and regression depth, which are robust estimators derived under the framework of f-GAN. These depth-based estimators offer desirable statistical properties for robust estimation under general settings like elliptical distributions. The paper discusses the use of depth-based estimators under the framework of f-GAN for robust estimation. Theoretical results show how appropriate neural network classes can lead to optimal estimation under Huber's -contamination model. Numerical experiments demonstrate the success of these GANs in robustly estimating parameters like the Gaussian mean and locations under elliptical distributions. The paper defines f-GANs using f-divergence and provides numerical experiments to demonstrate their success. The convex conjugate of f is used to derive a variational lower bound, which holds for any class T and becomes an equality when T contains the function f(p/q). The lower bound naturally leads to optimal estimation under Huber's -contamination model with i.i.d. observations X1, ..., Xn ~ P. The variational lower bound (3) leads to the f-GAN learning method, an extension of GANs, where the goal is to make the best discriminator unable to differentiate between the true distribution P and the empirical distribution. The f-Learning framework is based on a special case of the variational lower bound, allowing the class Q to depend on the distribution Q. The inequality (5) becomes an equality when P is in Q. The learning method f-Learning is a general framework covering important learning procedures. The maximum likelihood estimator (MLE) is obtained when Q approaches Q in some neighborhood. TV-Learning, related to total variation distance, is discussed with a special generator f(x) = (x-1)+. TV-Learning in the f-Learning framework leads to robust estimators maximizing various depth functions like Tukey's depth, regression depth, and covariance depth. Special cases of TV-Learning include Tukey's halfspace depth, regression depth, and covariance matrix depth. The formula for Tukey's median is recognized as the maximizer of Tukey's halfspace depth, aiming to be close to the centers of all one-dimensional projections of the data. In the f-Learning framework, robust estimators maximizing various depth functions are derived, including regression depth and covariance matrix depth. These estimators achieve the minimax rate under Huber's contamination model and highlight the importance of TV-Learning in robust estimation. However, depth-based estimators are known to be computationally challenging, limiting their practical applications. The f-GAN framework offers computational advantages over depth-based procedures in robust estimation, allowing for flexible design of discriminator classes using neural networks. This approach has been successfully applied to learn complex distributions and images. In this section, the focus is on robust mean estimation under Huber's contamination model, exploring how the choice of discriminators impacts robustness. The study aims to analyze the impact of discriminator class choice on robustness and statistical optimality in a simple setting. Data is contaminated with i.i.d. observations and the goal is to estimate the unknown location. The minimax rate is targeted with respect to squared loss across all parameters and distributions. Samples are drawn from a specific distribution, showing surface and heatmap plots with distinct maxima. The study analyzes the impact of discriminator class choice on robustness and statistical optimality in a simple setting. Data is contaminated with i.i.d. observations, aiming to estimate the unknown location with a targeted minimax rate. Samples are drawn from a specific distribution, showing surface and heatmap plots with distinct maxima. The global maximum for F(w) transitions from w = +\u221e to w = \u2212\u221e as \u03b7 grows, with examples of achieving maxima at different values of w and \u03b7. Gradient ascents on \u03b7 increase its value, leading to w being stuck in the positive half space. The total variation GAN (TV-GAN) with a specific function is used, and logistic regression is one of the simplest discriminator classes considered. The logistic regression procedure can be seen as a smoothed version of TV-Learning. It minimizes a sharper variational lower bound compared to other methods. Jensen-Shannon GAN (JS-GAN) is used for normal mean estimation, offering advantages over other forms. JS-GAN is advantageous for normal mean estimation over other GAN forms. Experiment results show that using a neural network discriminator improves robustness compared to logistic regression. The concept of restricted Jensen-Shannon divergence is introduced to explain how the discriminator class affects JS-GAN's robustness. The logistic regression classifier distinguishes between P and Q using feature g(X), where JS g (P, Q) is a variational lower bound of Jensen-Shannon divergence. Proposition 3.1 states that JS g (P, Q) = 0 if E P g(X) = E Q g(X), showing that JS g (\u00b7, \u00b7) cannot differentiate P and Q if g(X) has the same expected value under both distributions. This moment matching effect has also been studied for general f-GANs. The JS-GAN aims to match the values of DISPLAYFORM2 for the feature g(X) in logistic regression. A neural net without hidden layers is equivalent to logistic regression with a linear feature g(X) = (X T , 1) DISPLAYFORM3, where the sample mean is a global maximizer. However, a neural net with at least one hidden layer involves a nonlinear feature function g(X) that leads to robustness. It is shown that a neural net with one hidden layer is sufficient for robustness and optimality. The class of discriminators consists of two-layer neural network functions with bounded weights and a nonlinear activation function \u03c3(\u00b7). The bounded activation functions used in the study include indicator and ramp functions. The estimator \u03b8 is considered under certain assumptions with universal constants. Tukey's median and JS-GAN show optimal robustness under elliptical distributions. Elliptical distributions are defined by a random vector X and a representation involving a uniformly distributed variable U and an independent random variable \u03be. The elliptical distribution is fully parametrized by the triplet (\u03b8, \u03a3, h), where h is a density function independent of v. The JS-GAN estimator is defined in terms of positive semi-definite matrices. The parametrization (\u03b8, \u03a3, h) is made identifiable by restricting h to a specific class, denoted as EC(\u03b8, \u03a3, h). The estimator \u03b8 is defined for positive semi-definite matrices with bounded spectral norm. The result holds under the strong contamination model, ensuring the same convergence rate for all elliptical distributions, including multivariate Cauchy. This distinguishes it from previous methods designed only for robust mean estimation. The estimator FORMULA0 is different from (14) only in the generator class, sharing the same discriminator class (15). The overall statistical complexity of the estimator is determined by the discriminator class. Extensive numerical studies of robust mean estimation via GAN are presented, with theoretical results verified using TV-GAN and JS-GAN. Comparison with other methods in the literature is also provided. In Section 5.3, comparison with other methods on robust mean estimation is discussed. Various network structures and adaptation to unknown covariance are studied in Sections 5.4 and 5.5. Additionally, adaptation to elliptical distributions is explored in Section 5.6 using the f-GAN algorithmic framework. The implementation of JS-GAN and TV-GAN for robust mean estimation involves a generator G\u03b7(Z) = Z + \u03b7 and a discriminator D that is a multilayer perceptron (MLP). Details on algorithms, tuning, hyper-parameters, and discriminator network structures are provided. The implementation details and experimental verification of TV-GAN and JS-GAN for robust mean estimation are discussed in Appendix B.1. The PyTorch implementation can be found at https://github.com/zhuwzh/Robust-GAN-Center. The experiments consider different contamination distributions and scenarios, with details on covariance matrix construction provided in Appendix B.2. The experimental results for robust mean estimation using TV-GAN and JS-GAN are summarized in FIG5, showing errors linearly related to parameters. Additional numerical results can be found in Tables 7, 8, and 9 in the Appendix. The study explores the error relation with parameters like p, n, and Q, confirming the conclusions of Theorem 3.1 and Theorem 3.2. The experimental results for robust mean estimation using TV-GAN and JS-GAN are compared with other methods like dimension halving and iterative filtering. The method does not require knowledge of nuisance parameters. Table 1 shows the performances of different methods with the network structure specified. The experimental results compare robust mean estimation using TV-GAN and JS-GAN with other methods like dimension halving and iterative filtering. TV-GAN is effective for non-separable cases but struggles with optimization issues when Q is far from N. JS-GAN performs well in separable cases and competes in non-separable ones, showing stable performance with various neural network structures. Tuning networks with one hidden layer becomes challenging as the dimension increases. Tuning networks with one-hidden layer becomes challenging as the dimension grows, but a deeper network can improve the situation. Experiment results for JS-GAN in high dimensions are provided in Table 2. The robust mean estimator from JS-GAN is adaptive to unknown covariance structures and rate-optimal even when the true covariance matrix is unknown. Additional theoretical results for deep neural nets are given in the appendices. The text discusses numerical experiments for robust mean estimation with unknown covariance using a JS-GAN. It focuses on the estimation of the location parameter \u03b8 in an elliptical distribution and compares it with a Cauchy distribution. The generator choice in the algorithm is highlighted, emphasizing the use of a non-negative neural network. The text investigates the performance of deep neural nets with ReLU activation for robust mean estimation under Cauchy distribution. Regularizations are crucial to prevent overfitting, inspired by previous work on network design. The text discusses the use of regularizations in deep neural networks with ReLU activation for robust mean estimation under the Cauchy distribution. Theoretical guarantees are provided for a neural network class with multiple layers, showing that JS-GAN can achieve a low error rate with respect to squared loss. The condition for the ReLU network can be easily met with data preprocessing steps. The text discusses the use of regularizations in deep neural networks with ReLU activation for robust mean estimation under the Cauchy distribution. Theoretical guarantees are provided for a neural network class with multiple layers, showing that JS-GAN can achieve a low error rate with respect to squared loss. The condition for the ReLU network can be easily met with data preprocessing steps. The data is split into two halves, a coordinatewise median \u03b8 is calculated, and experiments show that the preprocessing step may not be necessary. The assumption \u03b8 \u221e \u2264 \u221a log p in the analysis of Rademacher complexity may be dropped with a more careful analysis. The implementation for JS-GAN is provided in Algorithm 1, and a simple modification of the objective function leads to that of TV-GAN. The text discusses the use of regularizations in deep neural networks with ReLU activation for robust mean estimation under the Cauchy distribution. A simple modification of the objective function leads to that of TV-GAN. The implementation details include tuning parameters such as learning rates for convergence rate. The strategy involves using estimators and corresponding discriminator networks. The text discusses the use of regularizations in deep neural networks with ReLU activation for robust mean estimation under the Cauchy distribution. The strategy involves using estimators and corresponding discriminator networks. Gradient descent is applied to the discriminator networks with a few epochs to prevent overfitting. The smallest value of the objective function is selected to train the discriminator alone with a fixed \u03b7, using an early stopping strategy to avoid overfitting. The experiments show that hyper-parameters are robust across different models. In the context of deep neural networks with ReLU activation for robust mean estimation under the Cauchy distribution, the text discusses critical parameters for reproducing experiment results and challenges in judging convergence in GAN training. It also explores network structure design, showing that deeper structures can improve performance. JS-GAN with deep network structures can significantly improve performance over shallow networks, especially with large dimensions. The choice of hidden layer width depends on sample size, with fewer units performing better for smaller sample sizes. To stabilize and accelerate TV-GAN, outlier removal before training and spectral normalization can be helpful. In order to accelerate the optimization of TV-GAN, a regularized version inspired by Proposition 3.1 is adopted. A feature extractor matching nonlinear moments of P and Q is used, along with a regularization term to improve training and performance. The discriminator network and corresponding feature extractor are defined, leading to a regularization term. Contamination distributions Q are introduced, with experiments using different values for \u00b5. In experiments, different values of \u00b5 are used to test the algorithm and verify error rates. A Gaussian distribution with a non-trivial covariance matrix \u03a3 is considered, generated from a sparse precision matrix \u0393. The precision matrix is made symmetric and positive definite, and the covariance matrix is defined as \u03a3 = \u0393^-1. Additionally, a Cauchy distribution with independent components is also examined. In experiments, different values of \u00b5 are used to test the algorithm and verify error rates. A Gaussian distribution with a non-trivial covariance matrix \u03a3 is considered, generated from a sparse precision matrix \u0393. The precision matrix is made symmetric and positive definite, and the covariance matrix is defined as \u03a3 = \u0393^-1. Additionally, a Cauchy distribution with independent components is also examined. GANs are compared with dimension halving and iterative filtering methods using specific hyper-parameters and network structures. In experiments, different values of \u00b5 are used to test the algorithm and verify error rates with Gaussian and Cauchy distributions. TV-GAN with one hidden layer improves performance, but adding more layers does not always help. JS-GAN's lack of robustness without hidden layers is noted. Optimal width of hidden layer depends on sample size. In the first example, Q is a Gaussian location family subset in a local neighborhood. The event q(X)/q(X) \u2265 1 is equivalent to X \u2212 \u03b7 2 \u2264 X \u2212 \u03b7 2. The exact formula of Tukey's median is obtained by letting r \u2192 0. The next example is a linear model where the exact formula of regression depth is derived by letting r \u2192 0. The derivation of regression depth does not depend on the marginal distribution. For covariance matrix estimation, a rank-one neighborhood is proposed, leading to a Fisher consistent definition. The formula can also be derived using a different approach. The derivation of regression depth is independent of the marginal distribution. The formula for the inverse covariance matrix can be derived using the Sherman-Morrison formula. Proposition 2.1 is completed by deriving formula (21) from TV-GAN. Proposition 3.1 is proven by defining F(w) and showing that max w\u2208W F(w) is a convex optimization problem. In this section, the proofs of main theorems are presented. Useful lemmas are established first, and then the proofs of main theorems are given. Lemma D.1 states that for i.i.d. observations X1, ..., Xn ~ P and function class D, with probability at least 1 - \u03b4, a certain inequality holds. By utilizing McDiarmid's inequality and a symmetrization technique, a bound involving Rademacher complexity is obtained. The Rademacher complexity can be bounded by Dudley's integral entropy bound, leading to a desired result with probability at least 1 - \u03b4. By utilizing McDiarmid's inequality, the Rademacher complexity is further bounded, ensuring the inequality holds for i.i.d. observations X1, ..., Xn ~ P and function class D. The Rademacher complexity is bounded by Dudley's integral entropy bound, ensuring the inequality holds for i.i.d. observations X1, ..., Xn ~ P and function class D. The Lipschitz function satisfies certain conditions, leading to the desired result with probability at least 1 - \u03b4. The proof involves analyzing Rademacher complexity and introducing notations to minimize the empirical distribution. The Rademacher complexity is bounded by Dudley's integral entropy bound for i.i.d. observations X1, ..., Xn ~ P and function class D. The Lipschitz function conditions lead to the desired result with probability at least 1 - \u03b4. The proof involves analyzing Rademacher complexity and introducing notations to minimize the empirical distribution. With probability at least 1 - \u03b4, the inequalities hold, showing that F w,b (P \u03b8 , \u03b8) \u2264 2 + 2C p n + log(1/\u03b4) n for all w \u2208 R p and b \u2208 R. The Rademacher complexity is bounded by Dudley's integral entropy bound for i.i.d. observations X1, ..., Xn ~ P and function class D. The Lipschitz function conditions lead to the desired result with probability at least 1 - \u03b4. The proof involves analyzing Rademacher complexity and introducing notations to minimize the empirical distribution. With probability at least 1 - \u03b4, the inequalities hold, showing that F w,b (P \u03b8 , \u03b8) \u2264 2 + 2C p n + log(1/\u03b4) n for all w \u2208 R p and b \u2208 R. This implies DISPLAYFORM1 with probability at least 1 \u2212 \u03b4. The proof is complete. Proof of Theorem 3.2. The explicit construction of Q 1 , Q 2 is given in the proof of Theorem 5.1 of Chen et al. (2018). The Rademacher complexity is bounded by Dudley's integral entropy bound for i.i.d. observations X1, ..., Xn ~ P and function class D. The Lipschitz function conditions lead to the desired result with probability at least 1 - \u03b4. The proof involves analyzing Rademacher complexity and introducing notations to minimize the empirical distribution. With probability at least 1 - \u03b4, the inequalities hold, showing that F w,b (P \u03b8 , \u03b8) \u2264 2 + 2C p n + log(1/\u03b4) n for all w \u2208 R p and b \u2208 R. This implies DISPLAYFORM1 with probability at least 1 \u2212 \u03b4. The proof is complete. The construction of Q 1 , Q 2 is given in the proof of Theorem 5.1 of Chen et al. (2018). This implies that |F (P, (\u03b7, \u0393, g)) \u2212 F (P \u03b8,\u03a3,h , (\u03b7, \u0393, g))|\u2264 sup |E Q2 log(2D(X)) \u2212 E Q1 log(2D(X))| \u2264 2\u03ba."
}