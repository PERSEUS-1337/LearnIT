{
    "title": "SJxpsxrYPS",
    "content": "Learning rich representations from data is crucial for deep generative models like variational auto-encoders (VAE). A strategy is proposed to progressively learn independent hierarchical representations from high to low levels of abstractions. The model starts with the most abstract representation and grows the network architecture to introduce new representations at different levels. The model shows improved disentanglement compared to existing works on benchmark datasets using three disentanglement metrics, including a new proposed metric. Both qualitative and quantitative results are presented. The text discusses improving disentanglement in hierarchical representations using a variational auto-encoder (VAE) by progressively growing its capacity. VAE has shown promise in learning interpretable representations but struggles with utilizing neural network depth due to conflicts in the inference and generation processes. This approach aims to address these limitations by leveraging hierarchical representation learning and progressive learning techniques. The text discusses addressing the challenge of extracting and disentangling generative factors at different abstraction levels in VAEs. Recent works have proposed methods such as using memory modules or hierarchical latent representations to enable VAEs to generate from details added at different depths of the network. However, learning to extract and disentangle all generative factors simultaneously remains difficult. Inspired by human cognition, starting small in the learning process of neural networks is suggested as a way to incrementally train with data and tasks. The text discusses the incremental learning strategy for simplifying complex tasks in neural networks, focusing on curriculum learning and progressive growth of network capacity in generative adversarial networks (GANs) for high-resolution image generation. Starting small and incrementally increasing network depth have shown advantages in generating detailed images. Progressive learning strategies have been used to improve image generation in GANs by growing their depth. Previous works have focused on progressively increasing network capacity in VAE for image generation, but not for extracting and disentangling hierarchical representations. In contrast to previous works focusing on increasing network capacity in VAE for image generation, this work introduces a progressive training strategy called pro-VLAE. It aims to extract hierarchical representations from different depths of the model, improving disentanglement on benchmark datasets. The proposed metric, pro-VLAE, complements MIG and shows improved disentanglement in hierarchical representations compared to other models like \u03b2-VAE and VLAE. It focuses on learning abstract representations first and then disentangling factors at lower levels, forming a hierarchy of feature maps. This approach is similar to stacked discriminative models and stacked-VAEs in DGM for image generation. The stacked hierarchy in generation models is challenging to train as depth increases and may not offer clear benefits for learning hierarchical or disentangled representations. Instead of a hierarchy of latent variables, independent hierarchical representations at different abstraction levels can be used for generation. This approach aligns with existing works in learning independent hierarchical representation. Progressive learning strategies have been successful in high-quality image generation, particularly in the context of GANs. These strategies can be categorized into two groups: incremental input and incremental memory. Incremental input involves dividing the task of image generation into progressive tasks of generating low-resolution to high-resolution images with multi-scale supervision. On the other hand, incremental memory involves growing the architecture of GANs from a shallow network to a deep network capable of generating images. Progressive learning strategies in image generation have shown success, especially in GANs. Two approaches include incremental input, which generates images from low to high resolution with multi-scale supervision, and incremental memory, which grows the network's capacity to generate images. Previous works have demonstrated the benefits of progressively increasing network capacity for image generation, but there is limited exploration on growing network capacity for learning hierarchical representations. Some studies have looked into incremental learning of representations in VAE, such as using recurrent networks with attention mechanisms to refine details in generated images. In Lezama (2019), a teacher-student strategy was used to grow the latent representations in VAE by learning additional nuisance variables without compromising the disentangling ability of the teacher model. The model's capacity is limited to extracting nuisance variables, unlike the desired significant growth in VAE capacity to improve disentangling of important factors of variations. The presented pro-VLAE aims to address this by considering learning different levels of abstractions at different depths of the network. The pro-VLAE introduces a simpler training strategy for progressive representation learning by decomposing latent variables into different levels of abstractions. This approach aims to improve disentanglement in VAE models, offering a novel method different from existing techniques. The hierarchical generative model in the pro-VLAE decomposes latent variables into different abstraction levels, with independent z l 's representing generative factors. An inference model approximates the posterior, parameterized with an encoding-decoding structure. The goal is to maximize a modified evidence lower bound (ELBO) of the marginal likelihood of data x. The prior p(z) is set to isotropic Gaussian N (0, I) and \u03b2 is a hyperparameter for disentangling. A progressive learning strategy is presented to learn latent variables z l progressively from highest to lowest levels of abstractions. The model starts with learning the most abstract representations at layer L, resembling a vanilla VAE with latent variables z L at the deepest layer. The dimension of z L is kept small to start with a limited capacity for learning latent representations. The model uses a progressive learning strategy to learn latent variables z l from high to low abstraction levels. It starts with learning abstract representations at the deepest layer and incrementally grows the model to extract and generate hierarchical latent representations. The model implements progressive representation learning by smoothly blending new components into a trained network using a \"fade-in\" method and stabilizing training with weak constraints. The model implements progressive representation learning by smoothly blending new components into a trained network using a \"fade-in\" method and stabilizing training with weak constraints. The training objective includes a KL penalty on latent variables and regularization of unused variables before they are added to the network. Various metrics for measuring disentanglement have been proposed in related studies. The recently proposed MIG metrics measure the gap of mutual information between the top two latent dimensions. A new disentanglement metric, MIG-sup, is introduced to recognize the entanglement of multiple generative factors into the same latent dimension. Combining MIG and MIG-sup provides a more complete measure of disentanglement. The MIG and MIG-sup metrics aim to measure disentanglement by accounting for the splitting of factors into dimensions and encoding multiple factors into the same dimension. In an ideal scenario, both MIG and MIG-sup should be 1, indicating a one-to-one relationship between generative factors and latent dimensions. The presented pro-VLAE was tested on various benchmark datasets to evaluate its performance. The disentangling ability of pro-VLAE was compared to existing models using quantitative metrics on benchmark datasets like MNIST and CelebA. Comparisons were made with beta-VAE, VLAE, and the teacher-student model. The study compared the disentangling ability of pro-VLAE with beta-VAE, VLAE, and the teacher-student model on benchmark datasets like MNIST and CelebA. Different models were evaluated at various values of \u03b2, with a focus on learning hierarchical representations in a progressive manner. Fair comparisons were ensured by using the same number of latent variables and training iterations for all models. Multiple experiments were conducted for each model at each \u03b2 value, with the top three results averaged for reporting quantitative outcomes. The study compared the disentangling ability of pro-VLAE, beta-VAE, and VLAE on benchmark datasets. Pro-VLAE generally outperformed beta-VAE, with a clear margin of improvement. The teacher-student strategy had a low to moderate disentangling score, especially on 3DShapes. Pro-VLAE performed better with smaller values of beta, suggesting progressive learning had a positive effect. The study compared the disentangling ability of pro-VLAE, beta-VAE, and VLAE on benchmark datasets. Pro-VLAE outperformed beta-VAE, with a clear margin of improvement. Results showed that progressive learning had a positive effect on promoting disentangling. The images generated by pro-VLAE demonstrated improved disentangling ability, especially in separating object, wall, and floor colors. The VLAE distributed six generative factors over nine latent dimensions, with color split across each progression. The teacher-student model was less disentangled, and mutual information was used to track information learned in each hierarchy of latent variables during progressive learning. The approach in Chen et al. (2018) was used to estimate mutual information through stratified sampling. In the example from 3DShapes, pro-VAE learned latent variables progressively, disentangling factors like color, shape, and rotation. Mutual information between input and latent variables changed as learning progressed, indicating the model's ability to capture more detailed information. In the VLAE model, generative factors were distributed across latent dimensions, with mutual information tracking information learned in each hierarchy during progressive learning. The allocation of mutual information remained nearly unchanged, indicating entanglement. The teacher-student model was less effective in dragging entangled representations to new latent dimensions. Visualization of hierarchical features for MNIST and CelebA data shows abstraction levels encoded in latent dimensions. The teacher-student model struggles to learn new representations due to limitations in network capacity growth and Jacobian supervision. Pro-VLAE was qualitatively examined on both simple (MNIST) and complex data. On MNIST, the deepest latent representations encode digit identity while shallower levels encode writing styles. In CelebA, latent representations progressively learn from high to low abstractions, with disentangling demonstrated within each level. Deeper representations disentangle gender and race, while shallower ones disentangle features like eye-shadow. The number of distinct representations learned decreases from deep to shallow levels. The presented progressive strategy for learning and disentangling hierarchical representations shows improvement in disentangling abstract representations followed by lower levels of abstractions. Starting from a simple VAE, the model progressively learns independent representations from high to low levels of abstraction. This is the first demonstration of a model's ability to disentangle individual latent factors in a hierarchical manner. The presented method demonstrates the progressive learning and disentangling of hierarchical representations, showing improvement in abstract representations followed by lower levels of abstractions. Future work includes stronger guidance for allocating information across different levels of abstraction. The presented method showcases progressive learning and disentangling of hierarchical representations, with improvement in abstract representations followed by lower levels of abstractions. In one dimension, multiple factors are encoded, leading to high MIG scores, but splitting factors across dimensions results in lower scores. The network has 3 layers with 2-dimensional latent codes at each layer, generating images by traversing the latent codes. The presented method demonstrates progressive learning and disentangling of hierarchical representations with improvement in abstract representations followed by lower levels of abstractions. It involves a network with 3 layers and 2-dimensional latent codes at each layer, generating images by traversing the latent codes. The representation learnt in this method suggests smoother traversing on digits compared to VLAE, with similar results for digit width and stroke width. The method allocates information in a clear descending order, as shown by the Mutual information I(x; z l ) between data x and latent codes z l at each depth of the network. The presented method demonstrates progressive learning and disentangling of hierarchical representations with improvement in abstract representations. Experiments on 3DShapes and MNIST datasets show information flow among latent variables in a clear descending order, aligning with the progressive learning strategy. The method captures information in a clear descending order for all hierarchical architectures, with information tending to flow from previous layers. The study demonstrates progressive learning and disentangling of hierarchical representations, showing clear information flow among latent variables in a descending order. Models with small latent codes struggle to learn as much information as those with larger latent codes, leading to high variance in information. Large latent codes may result in unclear information flow after adding certain layers. The goal is to design models that effectively capture K generative factors with D dimensions available. In the study, the model aims to design with D dimensions where K is unknown, making L and z hyperparameters to be tuned. Results show information flow among latent variables in descending order, with models struggling with small latent codes. Large latent codes may lead to unclear information flow."
}