{
    "title": "SyX0IeWAW",
    "content": "We develop a metalearning approach for learning hierarchically structured policies, improving sample efficiency on unseen tasks through shared primitives. These primitives are switched between by task-specific policies, leading to an optimization problem for quickly reaching high rewards on new tasks. An algorithm is presented to solve this problem end-to-end using any reinforcement learning method. The approach successfully discovers meaningful motor primitives for directional movement of four-legged robots in mazes and demonstrates transferability to solve long-timescale sparse-reward obstacle courses. In a metalearning approach, shared primitives are used to improve sample efficiency on new tasks. These primitives are switched between by task-specific policies, allowing for quick adaptation to different tasks. The goal is to learn new tasks quickly by leveraging prior knowledge and optimizing for high rewards. In a metalearning approach, shared primitives are utilized to enhance sample efficiency on new tasks by switching between them with task-specific policies. The proposed model contains shared sub-policies switched by task-specific master policies, allowing for quick adaptation to different tasks. The optimization problem aims to find low-level motor primitives enabling fast learning by the high-level master policy. An optimization algorithm is proposed to solve this problem by repeatedly resetting the master policy for adapting sub-policies for quick learning on new tasks. The proposed method, MLSH, utilizes a hierarchical architecture and optimization algorithm to enable fast learning in various environments. Experiments demonstrate its effectiveness in tasks such as 2D continuous movement, gridworld navigation, and 3D physics tasks with humanoid and 4-legged robots. The method outperforms previous algorithms by learning meaningful sub-policies through interaction with diverse tasks. It is efficient in complex physics environments with long time horizons and robust in transferring learned policies. The curr_chunk discusses previous work in hierarchical reinforcement learning, focusing on learning sub-policies in complex physics environments with long time horizons. Various methods are mentioned, including discovering primitives through training on simple tasks, learning a master policy with sub-policies based on information-maximizing statistics, and end-to-end learning of hierarchy through the options framework. These methods aim to decompose complicated tasks into sub-goals but mostly focus on single-task settings without considering multi-task scenarios. Our work focuses on leveraging the multi-task setting to learn temporally extended primitives, unlike prior works that mainly concentrate on single-task settings. Previous studies have explored using recurrent networks and temporal convolutions for metalearning, aiming to optimize the learning process and fine-tune shared policies. Our method, MLSH, focuses on optimizing learning quickly over a large number of policy gradient updates in the RL setting, a regime not explored by prior work. It defines an optimization problem with a distribution over tasks to find parameters for an agent to learn efficiently. MLS focuses on optimizing learning quickly over many policy gradient updates in RL. An agent consists of a reinforcement learning algorithm that updates parameters \u03c6 and \u03b8 for a stochastic policy \u03c0. \u03c6 is shared across tasks, while \u03b8 is learned per-task from scratch. The agent interacts with a sampled MDP, updating its \u03b8 parameters. The hierarchical sub-policy agent structure is shown in Figure 1, with \u03b8 representing the master policy selecting a sub-policy for action. The active sub-policy \u03c6 3 guides actions, with \u03c6 as shared parameters and \u03b8 as per-task parameters. The meta-learning objective aims to optimize return over an agent's lifetime across tasks. An architecture inspired by hierarchical reinforcement learning incorporates shared parameters \u03c6 and per-task parameters \u03b8. The architecture involves a shared parameter vector \u03c6 with sub-policies \u03c6 1 , \u03c6 2 , . . . , \u03c6 K, and a separate neural network parameter \u03b8 for switching between sub-policies. The master policy, controlled by \u03b8, selects actions at a slower pace than the sub-policies, sampling actions at fixed intervals. This hierarchical approach allows for updating the master policy \u03b8 to handle new tasks efficiently. The MLSH algorithm aims to iteratively learn sub-policies for agents to achieve maximum reward over T-step interactions in various tasks. The algorithm updates sub-policy parameters \u03c6 to achieve high performance and robustness across tasks, while also enabling quick learning of master policies. Starting from random initialization, the algorithm iteratively updates \u03c6 to meet these objectives. The MLSH algorithm iteratively updates sub-policies to optimize performance and robustness across tasks. It involves a warmup period to optimize master policy parameters \u03b8, followed by a joint update period optimizing both \u03b8 and \u03c6. The update process includes sampling a task, initializing an agent with sub-policies parameterized by \u03c6, and a master policy with randomly-initialized parameters \u03b8. The MLSH algorithm involves a warmup period to optimize the master policy \u03b8 and a joint update period for both \u03b8 and sub-policies \u03c6. During the warmup period, experience is collected using the sub-policies \u03c6, and \u03b8 is updated to maximize reward using reinforcement learning algorithms. The MLSH algorithm involves a warmup period to optimize the master policy \u03b8 and a joint update period for both \u03b8 and sub-policies \u03c6. Experience is collected and optimized for \u03b8 during the warmup period, while the sub-policies are updated based on the master policy's decisions in a N-timestep slice. This framework aims to quickly achieve high rewards when learning on a new task. The MLSH algorithm optimizes for reward within a single episode by updating sub-policies \u03c6 when master policy \u03b8 is near-optimal. This approach assumes that the warmup period of \u03b8 will learn an optimal master policy. The inclusion of a warmup period ensures \u03c6 is updated effectively. The MLSH algorithm optimizes reward within a single episode by updating sub-policies \u03c6 when master policy \u03b8 is near-optimal. The optimal sub-policies lead the agent to the midpoint of destinations based on information in \u03b8. The update period is limited to U iterations to account for changes in \u03c6 and re-training \u03b8. The method aims to learn meaningful sub-policies efficiently for complex physics domains and transfer them to tasks outside the training distribution. In a series of experiments, the MLSH algorithm is tested against baselines and past methods with hierarchy using shared and task-specific information. The agent utilizes sub-policies (\u03c6) trained on the entire distribution and updates the master policy (\u03b8) for new tasks. Comparison is made against fine-tuning a single policy optimized across all tasks and training a new single policy from scratch. The MLSH algorithm uses 2 layer MLPs for master and sub-policies, trained with PPO BID16. A batchsize of D=2000 timesteps is used, with a larger learning rate for \u03b8 (0.01) than for \u03c6 (0.0003). Experiments are run in parallel using 120 cores split into 10 groups, with \u03c6 parameters shared. The MLSH algorithm optimizes a shared set of \u03c6 parameters towards 10 sampled tasks in parallel. Warmup periods are staggered to ensure continuous gradient updates. Tasks are sampled from 2D moving bandits, with shared initialization of \u03b8. Warmup and joint-update lengths vary per environment distribution. The MLSH algorithm optimizes a shared set of parameters towards 10 tasks in parallel, with staggered warmup periods for continuous gradient updates. In a 2D moving bandits task, an agent moves towards randomly placed points to receive rewards. Each episode lasts 50 timesteps, with master policy actions lasting for 10. Sub-policies have a warmup duration of 9 and a joint-update duration of 1. Training a master policy is faster than training a single policy from scratch. Training a shared policy in the MLSH algorithm results in an agent moving towards a certain goal point, cutting expected reward by half. A comparison is made with an RL 2 policy that encounters the same issue. Ablation tests in the 2D moving bandits task show that removing the warmup period results in both sub-policies initially moving to the same goal point, then gradually shifting. Running the master policy on the same timescale as the sub-policies mimics learning a shared policy, highlighting the importance of temporal extension. Hyperparameter comparison tests the influence of the sub-policy. In the MLSH algorithm, training a shared policy leads to the agent moving towards a goal point, reducing expected reward by half. A hyperparameter comparison is conducted to test the impact of sub-policy count and warmup duration. The four-rooms domain is used for comparison, with the agent starting at a specific spot and randomly assigned a goal position. Reward is given for reaching the goal state, with episodes lasting 100 timesteps and master policy actions lasting 25. MLSH outperforms PPO in reaching high rewards faster, while Option Critic performs similarly to its baseline. In the MLSH algorithm, high rewards are reached faster compared to the PPO baseline. The scalability of the algorithm is tested on physics-based tasks simulated through Mujoco BID21. Diverse subpolicies are discovered, and performance is tested on a four-legged ant in Twowalk tasks. Reward is based on negative distance to a destination point placed in the world. The task involves Ant robots moving to destination points in mazes without human supervision. They learn directional movement sub-policies and combine them to solve the mazes. In the Walk/Crawl task, Humanoid robots learn different movement styles like walking and crawling. The transfer of learned behaviors is also tested. The Humanoid agent learns to walk and crawl in the Walk/Crawl task, displaying superior performance compared to baselines. MLSH shows scalability in complex physics domains, while Ant robots learn movement primitives for maze exploration. Diverse Humanoid sub-policies are discovered, and a challenging task unsolvable with naive PPO is presented involving an Ant robot navigating an obstacle course. The Ant robot navigates an obstacle course to reach the top-right corner for a reward. Sub-policies learned in previous tasks are used to fine-tune the master policy. MLSH allows for exploration over sub-policies, making it easier to discover rewarding sequences. In contrast, naive PPO struggles in the sparse reward setting. In this work, an approach for end-to-end metalearning of hierarchical policies is formulated. A model is presented for representing shared information as sub-policies and training them over distributions of environments. Significant speedups in learning are achieved without optimizing towards the true objective. Diverse sub-policies are naturally discovered without hand engineering, utilizing hard one-hot communication in the MLSH model. The lack of gradient signal between master and sub-policies allows MLSH to be learning-method agnostic. Master or sub-policies can be trained with evolution or Q-learning, making the training framework versatile. The training framework presented in this work involves joint optimization over two sets of parameters, applicable to various scenarios beyond learning sub-policies. It draws inspiration from hierarchical reinforcement learning and metalearning, exploring the intersection of architecture space. The approach includes condensing sub-policies into a single neural network and addressing sample efficiency issues for a more unbiased estimator. The training framework in this work involves joint optimization over two sets of parameters, drawing inspiration from hierarchical reinforcement learning and metalearning. It condenses sub-policies into a single neural network to address sample efficiency issues for a more unbiased estimator. The goal is to maximize reward over the entire T-timesteps, opening up new directions in training agents for quick task adaptation."
}