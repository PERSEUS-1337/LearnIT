{
    "title": "B1MUroRct7",
    "content": "Online learning has gained attention for its ability to learn and evolve, especially in high-dimensional data processing where dimension reduction is necessary. This paper introduces new online learning approaches for supervised dimension reduction, including the incremental sliced inverse regression (ISIR) algorithm and the incremental overlapping sliced inverse regression (IOSIR) algorithm. These algorithms update the subspace of significant factors efficiently as new observations come in. Dimension reduction methods are crucial for exploring low dimensional representation in high dimensional data. They help in visualizing data structure and improving predictive performance in machine learning. Unsupervised methods like PCA are popular, while other methods include kernel PCA, multidimensional scaling, and manifold learning. The incremental overlapping sliced inverse regression (IOSIR) algorithm is introduced as an efficient technique for dimension reduction. Supervised dimension reduction methods involve a response variable and aim to find lower-dimensional representations relevant for predicting response values. These methods have evolved over the last twenty years and have been successfully applied in various scientific domains. In the era of big data, challenges include processing large volumes of data at high speeds due to advancements in information technology. In the era of big data, online learning approaches for dimension reduction are necessary due to the large volume and fast velocity of data. PCA and LDA are commonly used techniques, with incremental PCA and LDA algorithms proposed. A new online learning approach for supervised dimension reduction, implementing sliced inverse regression (SIR) incrementally, is the focus of this paper. The motivation is to implement sliced inverse regression (SIR) incrementally, a highly efficient supervised dimension reduction method. SIR has been successful in various scientific areas like bioinformatics and physics. The method involves solving a generalized eigen-decomposition problem and transforming it into a standard eigendecomposition problem for online implementation, overcoming challenges in data transformation. The paper introduces an incremental SIR method to address challenges in transforming data for PCA. It also proposes an incremental overlapping SIR algorithm. The goal is to find a lower-dimensional subspace for predicting the response variable. The method is refined in subsequent sections, with simulations and discussions to follow. Supervised dimension reduction aims to learn effective dimension reduction (EDR) directions from data. Unlike classical regression, sliced inverse regression regresses x against y. The EDR directions can be recovered by solving a specific equation under certain assumptions. Supervised dimension reduction aims to learn effective EDR directions from data by solving a generalized eigenvalue decomposition problem. The SIR algorithm involves binning observations, computing sample probabilities and means, and solving a generalized eigen-decomposition problem to estimate EDR directions. The algorithm is not very sensitive to the choice of parameter H as long as it is sufficiently larger than K and not greater than n^2, with root-n consistency usually promised. PCA looks for directions with the largest variances in the data. Principal components are eigenvectors corresponding to the largest eigenvalues. In incremental PCA, the eigen-system is updated when a new observation is received. The system is updated by computing a residual vector to define the component of the new observation. In incremental PCA, the system is updated by computing a residual vector to define the component of the new observation perpendicular to the current eigenspace. This involves normalizing vectors and solving an eigen-decomposition problem to update principal components. Increasing the number of principal components is achieved by updating the system accordingly. The incremental PCA algorithm updates the system by adding a new principal component when the sample size increases. The development of incremental sliced inverse regression (ISIR) is motivated by reformulating the SIR problem to a PCA problem. Challenges include transforming new observations to standardized inverse regression curve and updating \u03a3 \u2212 1 2 in an online manner. The ISIR implementation addresses challenges of transforming new observations and updating \u03a3 \u2212 1 2 online. It involves assigning new observations to slices based on distance to mean values and updating sample slice probabilities accordingly. The ISIR implementation updates slice mean values and eigen-decomposition system to incorporate new observations. It avoids updating \u03a3 directly by using the Sherman-Morisson formula and approximating quantities incrementally. The EDR space is updated without updating the inverse square root of the covariance matrix. When n is large, ISIR converges as a corollary of IPCA. For small n, ISIR may have less accurate approximations compared to SIR. It is recommended to use ISIR with a warm start, starting with SIR on a small dataset. Memory requirements include storing \u03a3 (x 0 \u2212x) with a complexity of O(p 2 ). Instead of storing and updating \u0393, use the fact \u0393 = M P M. Overlapping technique in SIR algorithm improves EDR space estimation accuracy. The IOSIR algorithm is an extension of the ISIR algorithm that uses an overlapping technique to improve EDR space estimation accuracy. It duplicates observations to reduce deviations from the EDR subspace and refines ISIR by placing each observation in two slices. This approach aims to enhance the accuracy of EDR space estimation. The IOSIR algorithm extends ISIR by duplicating observations and placing them in two slices to improve EDR space estimation accuracy. It is used for regression problems with numeric response variables. Comparisons will be made between ISIR, IOSIR, and SIR using simulations on artificial and real-world data to verify effectiveness. In simulations with artificial data, the IOSIR algorithm measures performance by accuracy of estimated EDR space using trace correlation. The model considered has 2 effective dimensions with specific beta values. The simulation is conducted in a 10-dimensional space with 10 slices. The algorithm is given a warm start with initial EDR space guess from SIR applied to a small dataset, then updated with 400 new observations. The IOSIR algorithm is evaluated for performance using trace correlation on artificial data with 2 effective dimensions and specific beta values. The algorithm is updated with 400 new observations and compared to SIR and ISIR, showing that IOSIR slightly outperforms ISIR and SIR in accuracy while sacrificing computation time. ISIR is faster than SIR, validating the convergence and efficiency of ISIR and IOSIR. The reliability of ISIR is confirmed on two real datasets: Concrete Compressive Strength and Cpusmall. The study compares supervised dimension reduction algorithms on Concrete Compressive Strength and Cpusmall datasets. Using H=10 and K=3, ISIR and IOSIR are evaluated with warm start on 50 observations. EDR space is estimated, training set is projected, regression model is built using k-nearest neighbor, and MSE is computed on test data. This process is repeated 100 times and average MSE is reported. The study evaluated ISIR and IOSIR algorithms for supervised dimension reduction on Concrete Compressive Strength and Cpusmall datasets. With 12 predictors and 8192 samples, experiments were conducted with H=10, K=3, and different numbers of observations for warm start, training, and testing. Results showed that both ISIR and IOSIR were effective, with IOSIR outperforming ISIR in regression problems. The algorithms were designed based on data standardization and the Sherman Morrison formula for online updates of \u03a3 \u22121. The purpose of ISIR and IOSIR is to maintain dimension reduction accuracy in situations where batch learning is not suitable, especially for streaming data. ISIR and IOSIR may not be more efficient than SIR when the entire dataset is available and only the EDR space is needed. Two open problems include the bottleneck of storing and using \u03a3 \u22121 in ISIR and IOSIR for high-dimensional data, and determining the intrinsic dimension K in SIR and other batch dimension reduction methods. Many methods have been proposed to determine the intrinsic dimension K in batch dimension reduction methods, but they are impractical for incremental learning. Solutions to these problems are currently unknown and left for future research."
}