{
    "title": "HJtPtdqQG",
    "content": "Hyperparameter tuning is crucial for achieving top performance in deep networks, especially in optimizing learning rates. Fixed learning rate schedules are commonly used during training, but Hyperdyn proposes a dynamic method that adjusts learning rates at the end of each epoch. By combining Bayesian optimization with a rejection strategy, Hyperdyn achieves state-of-the-art accuracy on CIFAR and Imagenet datasets with faster training compared to manually tuned networks. Hyperparameter optimization (HPO) aims to automate the tuning process in deep neural networks, eliminating the need for manual adjustments. Existing HPO frameworks often treat all hyperparameters uniformly without distinguishing between fixed structural parameters and those that vary during training. This distinction is crucial as it impacts network performance and optimization algorithms. In this work, the focus is on optimizing time-varying hyperparameters to improve training efficiency, stability, and accuracy. The proposed approach, Hyperdyn, dynamically tunes hyperparameters during training using Bayesian optimization and a rejection strategy. It aims to enhance the training process by selecting new hyperparameters at the end of each epoch. Hyperdyn is a framework that dynamically tunes hyperparameters during training using Gaussian processes and rejection tests. It achieves state-of-the-art performance on image classification benchmarks by improving training speed and automatically selecting learning rates. The text discusses a rejection framework used in the Hyperdyn framework to dynamically tune hyperparameters during training. This rejection test helps in deciding whether to accept new proposals based on Bayesian optimization or stick to the previously chosen learning rate, based on validation accuracy improvements over past epochs. This framework is crucial for achieving good performance and avoiding training instability and poor generalization. Hyperdyn utilizes a rejection framework similar to hyperband to dynamically tune learning rates for SGD and Adam on CIFAR-10 and Imagenet datasets. Results show faster training and improved validation accuracy compared to manual tuning. Hyperdyn utilizes a rejection framework similar to hyperband to dynamically tune learning rates for SGD and Adam on CIFAR-10 and Imagenet datasets. The method is stable, trains quickly even with larger batch sizes, and suggests learning rates that eventually decay but are not always monotonic. Algorithms like Adam are more beneficial in the beginning than at a later stage. Learning rates chosen by Hyperdyn generally increase with batch size and converge to values ensuring theoretical convergence. SGD tuned with Hyperdyn outperforms manually tuned sophisticated algorithms, highlighting the importance of tuning for good learning rates. Bayesian Optimization, commonly using Gaussian processes, is widely used to optimize blackbox functions. Hyperband focuses on speeding up configuration evaluations with a random search strategy. Hyperband focuses on speeding up configuration evaluations with a random search strategy by adaptively allocating resources using an early stopping mechanism. Hyperdyn selects new configurations at the end of each epoch without requiring training to finish, providing more flexibility in finding the best learning rates. Most previous works only compare results across different HPO frameworks, not with manually tuned networks. Around 80% validation accuracy achieved by the hyperband algorithm on CIFAR-10, with worse results for other methods like SMAC, TPE, and Spearmint. Previous works do not report results on large datasets like Imagenet. Different methods for finding optimal learning rates have been proposed, including Adam and SALERA algorithms. Adam shows better performance in some cases but requires more hyperparameters, while SALERA automatically adjusts the learning rate using an exponential moving average and handles catastrophic events in SGD training. The learning rate update rule is fixed unless there is a catastrophic event, limiting adaptivity. Experiments were conducted on smaller datasets like MNIST and CIFAR10, so algorithm behavior on larger datasets and batch sizes is unclear. BID2, BID17 proposed methods for large batch training on Imagenet achieving state-of-the-art accuracy, but were restricted to designing a learning rate scheduler for SGD. Hyperdyn is a general framework for hyperparameter optimization across various training methods. The paper discusses hyperparameter optimization problems, training methods, and the concept of meta-learning. LSTMs are used for gradient descent updates but may not scale to standard benchmark datasets. The rest of the paper reviews Bayesian optimization and Gaussian processes in Hyperdyn, detailing experiments on minimizing black box functions. Bayesian optimization algorithm is used to minimize black box functions by suggesting query points based on an acquisition function. The algorithm involves using a Gaussian Process model with initial mean, variance, and kernel function. The sequence of points and noisy observations are considered, with auxiliary variables introduced for modeling. The algorithm involves using a Gaussian Process model with auxiliary variables for modeling noisy observations. Expected improvement function is used to suggest query points based on improvement over a threshold. Non-stationary kernels can be used for Bayesian Optimization. In Bayesian Optimization, non-stationary kernels can be used to address limitations such as the inability to use multiple learning rate parameters and lack of stationarity in neural network training. Hyperdyn utilizes different temporal estimates of loss function change to mitigate these issues. Hyperdyn introduces a new set of hyperparameters that includes the epoch number of the algorithm, allowing for the creation of composite kernels that are non-stationary without the computational burden associated with general non-stationary kernels. The new hyperparameters in Hyperdyn include the epoch number, allowing for non-stationary composite kernels in Bayesian optimization. The time kernel, K2(\u00b7,\u00b7), introduces non-stationarity by changing the search space for epoch numbers. The construction of Hyperdyn is motivated by the observation that SGD decays as O(1/N 1/2) in N iterations, with \u03b1 = 1, \u03b2 = 1/2. The algorithm employs both exploratory and exploitative approaches to update weights and improve accuracy on the validation set. The function Update Statistical Model in Section 2.1 describes switching hyperparameter values at every epoch. However, a myopic switching strategy hinders generalization performance. Algorithm 5, Check Accuracy Trend, prevents unnecessary hyperparameter changes by considering validation accuracy trends. An initial offset allows for occasional hyperparameter value changes even when current values perform well. Experiments were conducted on Amazon AWS p2x16 EC2 machines using data parallelism for gradient computation in SGD. In experiments on Amazon AWS p2x16 EC2 machines, data parallelism was used for gradient computation in SGD across different GPUs. The datasets CIFAR10 and Imagenet 1K were utilized with simple augmentations. Results were considered state of the art, with hyperparameter optimization using learning rate and momentum on Hyperdyn. The window size, temperature, and offset parameters were set for the experiments. In experiments on Amazon AWS p2x16 EC2 machines, data parallelism was used for gradient computation in SGD across different GPUs. The datasets CIFAR10 and Imagenet 1K were utilized with simple augmentations. Results were considered state of the art, with hyperparameter optimization using learning rate and momentum on Hyperdyn. Comparison to manually tuned networks showed that Hyperdyn-tuned momentum-SGD outperformed the manually tuned version on a ResNet-20. Hyperdyn tuning was faster at reaching superior generalization performance, with no additional tuning required, saving hours of trial and error. Hyperdyn tuning automatically adjusts learning rates for different batch sizes, with larger batches requiring larger learning rates. However, this trend does not continue indefinitely, possibly due to being in a very large batch regime. Hyperdyn outperforms manual tuning, especially when the momentum term is set to 0. Hyperdyn outperforms manually tuned SGD with momentum, emphasizing the importance of finding good learning rates for a simple algorithm. The wait and watch strategy of Hyperdyn incorporates past information for stability, similar to momentum. Comparing Hyperdyn to a greedy version, where the best hyperparameter is switched in every epoch, shows that epoch-based Bayesian optimization has poor performance. The epoch-based Bayesian optimization has poor generalization, leading to the need for a \"wait-and-watch\" strategy implemented by the Check Accuracy Trend module. Hyperdyn outperforms in the long run compared to random search, especially at larger batch sizes, as shown in FIG1. Random search 5x shows higher variance than Hyperdyn. A variant of Hyperband with reduced search space for learning rate did not yield any improvement. Hyperdyn outperforms random search 5x in optimizing learning rates for Adam. It converges to small values for \u03b21 and around 1 for \u03b22, leading to faster training and better generalization on ResNet-20 with batch size 128. Hyperdyn outperforms random search in optimizing learning rates for Adam, converging to small values for \u03b21 and around 1 for \u03b22. This leads to faster training and better generalization on ResNet-20 with batch size 128. For Imagenet, a batch size of 1000 was used, larger than typical experiments, resulting in faster training and achieving the same accuracy as manually tuned processes. Additionally, Hyperdyn shows the best performance up to 40 epochs, introducing a general framework for hyperparameter optimization. The algorithm proposed for hyperparameter optimization is fast, flexible, and stable on larger batch sizes. It outperforms SGD and Adam in terms of speed and accuracy. Hyperdyn demonstrates stability on Imagenet with larger batch sizes and introduces a \"wait-and-watch\" strategy to prevent over-exploration. Validation accuracy computation can be easily parallelized. In the general case, parallelizing multiple suggestions similar to BID12 is effective. Epoch-based BO in FIG1 performs better than Hyperdyn initially. One potential future direction is to incorporate time-varying temperature in Check Accuracy Trend based on epoch. Exploiting temporal gains with a backtrack algorithm in later stages when accuracy is high but more tuning is needed for best error rates is also suggested."
}