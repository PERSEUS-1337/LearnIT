{
    "title": "B1gabhRcYX",
    "content": "This paper presents a network architecture for solving the structure-from-motion (SfM) problem using feature-metric bundle adjustment (BA) to enforce multi-view geometry constraints. The network is differentiable, allowing it to learn features that improve the tractability of the BA problem. Additionally, a novel depth parameterization is introduced to recover dense per-pixel depth. The network generates basis depth maps from input images and optimizes the final depth through feature-metric BA. The system combines domain knowledge and deep learning to address the challenging dense SfM problem successfully. The proposed method successfully addresses the challenging dense Structure-from-Motion (SfM) problem by formulating Bundle Adjustment (BA) as a differentiable layer, the BA-Layer, to enforce geometric constraints between 3D structures and camera motion in deep learning approaches. The BA-Layer bridges classic methods and deep learning approaches by using a multilayer perceptron to predict damping factor in the LM algorithm. It minimizes distance between aligned CNN feature maps to optimize scene structures and camera motion, addressing limitations of geometric and photometric BA. This feature-metric BA allows back-propagation of loss to learn suitable features for structure-from-motion and bundle adjustment. Our network hard-codes multi-view geometry constraints in the BA-Layer and learns feature representations from training data to estimate dense per-pixel depth, crucial for tasks like object detection and robot navigation. Training a network to generate basis depth maps allows for a compact parameterization, addressing the challenge of computational expense in direct per-pixel depth estimation. Monocular Depth Estimation Networks face the ill-posed problem of predicting depth from a single image, with deep learning methods now offering solutions to this issue. Before the rise of deep learning methods, depth prediction from a single image was done using various techniques like MRF, semantic segmentation, and manually designed features. Different approaches have been proposed, such as a multi-scale method with two CNNs for depth prediction and the use of ResNet for depth prediction. Our approach focuses on monocular image depth estimation, producing basis depth maps that are further optimized for improved results. Additionally, CNNs have been used to solve the Structure-from-Motion problem in recent works. Recent works have utilized CNNs to address the Structure-from-Motion (SfM) problem. Different methods have been proposed, such as using CNNs for depth and camera motion estimation, incorporating optical flow features for better generalization, and employing a LSTM-RNN optimizer for nonlinear least squares in two-view SfM. A new approach introduces the BA-Layer to simultaneously predict scene depth and camera motion. The BA-Layer is proposed to predict scene depth and camera motion from CNN features, enforcing multi-view geometry constraints. This method can reconstruct more than two images and minimizes feature-metric error for robustness. The BA-Net architecture is introduced after revisiting classic BA for a better understanding of its challenges. The LM algorithm minimizes re-projection error by updating the solution iteratively using the Jacobian matrix and a regularization parameter. The regularization parameter \u03bb controls the strength in minimizing the re-projection error using the Schur-Complement BID6 method. The traditional structure-from-motion approach relies on matching image features like corners, blobs, or line segments, leading to outliers that require RANSAC for rejection. Direct methods like photometric BA BID17 BID13 aim to eliminate feature matching by directly minimizing the photometric error of aligned pixels. The direct methods in computer vision aim to upgrade pixels to their 3D coordinates using optimization parameters. These methods have advantages in using pixels with sufficient gradient magnitude, but are sensitive to initialization, camera changes, and outliers. To address these challenges, a feature-metric BA algorithm is proposed to estimate scene depth and camera motion parameters while minimizing the feature-metric difference of aligned pixels. The proposed BA-Net utilizes a backbone network, Basis Depth Maps Generator, Feature Pyramid Constructor, and a BA-Layer to optimize depth maps and camera poses through a differentiable LM algorithm. Features suitable for SfM are learned via back-propagation instead of pre-trained CNN features. The BA-Layer predicts camera poses and dense depth maps, back-propagating loss for training. Multiple images are fed to the backbone DRN-54 for processing. The BA-Net utilizes DRN-54 BID49 to process multiple images, generating smoother feature maps for BA optimization. Dilation convolutions are replaced with ordinary convolutions with strides to improve memory efficiency. A feature pyramid is constructed for each input image, along with basis depth maps. The BA-Layer optimizes camera poses and dense depth maps by minimizing feature-metric error, making the pipeline end-to-end trainable. Equation FORMULA6 enables end-to-end training of the pipeline by utilizing a feature pyramid built from the DRN-54 backbone. The top-down architecture with lateral connections propagates context information across scales, leading to a feature-metric BA with a larger convergence radius. The feature pyramid is constructed by upsampling and concatenating feature maps iteratively until the finest level, followed by a 3 \u00d7 3 convolution. After constructing a feature pyramid using the DRN-54 backbone, a 3 \u00d7 3 convolution is applied to reduce dimensionality. The feature pyramid is visualized with smoother channels after training with a BA-Layer, showing higher resolution than FPN for precise alignment. The benefits of BA optimization from learned features are demonstrated through visualizing different distances. After constructing a feature pyramid using the DRN-54 backbone and applying a 3 \u00d7 3 convolution for dimensionality reduction, distances evaluated from raw RGB values, pretrained feature C 3, and learned feature F 3 are visualized. The RGB distance lacks a clear global minimum, while the pretrained feature C 3 has both global and local minimums. In contrast, the learned feature F 3 exhibits a clear global minimum and smooth basin, aiding in gradient-based optimization. Camera poses and dense depth maps are optimized by minimizing feature-metric error. The original LM algorithm faces difficulties due to non-differentiability, making feature learning by back-propagation impossible. This is addressed by fixing the number of iterations in the optimization process. The author proposes fixing the number of iterations in the optimization process to address non-differentiability. A new technique is introduced to soften if-else decisions and make the optimization differentiable, predicting a better damping factor \u03bb to reach a better solution within limited iterations. The author introduces a technique to make the optimization process differentiable by predicting a damping factor \u03bb using feature pyramids and current solution X. This allows for a better solution within a fixed number of iterations. The solution after the k-th iteration is obtained by updating parameters using g(X ; F). Equation (6) allows for back-propagation through the pipeline for feature learning. A MLP predicts \u03bb from a 128D vector with ReLU activation. Feature-metric BA is solved using a coarse-to-fine strategy with LM algorithm for 5 iterations at each pyramid level. Camera poses are initialized with identity rotation and zero translation. In Section 4.4, the initialization of depth map is introduced. Parameterizing a dense depth map by per-pixel values is impractical due to the large number of parameters and training difficulties. To address this, a convolutional network is used for monocular image depth estimation, with DRN-54 as the encoder and a standard encoder-decoder architecture for learning. The decoder modifies the last convolutional feature maps of BID30 to 128 channels for depth map optimization. The final depth map is a linear combination of basis depth maps, with weights optimized in the BA-Layer. The ReLU activation function ensures non-negative depth values. The feature-metric distance is calculated using the basis depth maps. Initial weight w0 is learned for faster convergence. The BA-Net learns the feature pyramid, damping factor predictor, and basis depth maps generator in a supervised manner. Commonly used loss functions for training include camera rotation and translation losses, as well as the berHu Loss for dense depth maps. The network is initialized from DRN-54 and trained with ADAM from scratch. The dataset used for training the BA-Net model is ScanNet, a large-scale indoor dataset with 1,513 sequences in 706 different scenes. Camera poses and depth maps are estimated via BundleFusion. Image pairs for training are sampled using a filtering process to avoid errors and ensure quality. The dataset is split into training and testing sets, with the training set containing the first 1,413 sequences. The dataset used for training the BA-Net model is split into training and testing sets. The training set contains the first 1,413 sequences, while the testing set contains the remaining 100 sequences. Ground truth camera poses are computed using LibVISO2 from the KITTI dataset. Evaluation of results is done using depth error metrics suggested in BID14. In BID14, depth error metrics are used to evaluate the performance of the model. The errors in camera poses are measured by rotation error, translation direction error, and absolute position error. Comparison with DeMoN and classic BA is shown in Table 1. The model is trained on the training set described in BID44 for fair comparison. Our BA-Net outperforms DeMoN in depth estimation, even when trained on the same data. We also compare with geometric and photometric BA methods, showing superior results. Geometric BA struggles in indoor scenes due to feature matching difficulties, while photometric BA faces challenges in optimizing the non-convex objective function. Evaluation metrics are consistent with comparisons on ScanNet. In this experiment, short sequences of 5 frames are created by computing two-view reconstructions from BA-Net and aligning them in the coordinate system anchored at the first frame to minimize photometric error. Results on KITTI show that our method outperforms both supervised and unsupervised methods, achieving more accurate camera trajectories due to feature-metric BA with features tailored for the SfM problem. This paper introduces BA-Net, a network that enforces multi-view geometry constraints for structure-from-motion. It optimizes scene depths and camera motion through feature-metric bundle adjustment, with a differentiable pipeline for end-to-end training. The network combines domain knowledge with deep learning to facilitate structure-from-motion. The DRN-54 backbone architecture includes modified dilated convolutions and a depth basis generator. The network utilizes learned feature representation and outperforms conventional methods. The DRN-54 backbone architecture incorporates modified dilated convolutions and a depth basis generator. A decoder uses the output of C 6 as input and stacks five up-projection blocks to generate 128 basis depth maps, each half the resolution of the input image. Evaluation time shows the method is slightly faster than DeMoN, with the current bottleneck being the BA-Layer due to a large amount of matrix operations. The BA-Layer in the DRN-54 backbone architecture optimizes depth and camera poses jointly, allowing for reuse of network structures in tasks like semantic segmentation and object detection. Pre-trained features without feature learning result in larger errors. Without joint optimization, both depth maps and camera poses are worse, highlighting the importance of bundle adjustment optimization. The Levenberg-Marquardt algorithm is made differentiable for end-to-end training by learning the damping factor from the network. Comparing it to vanilla Gauss-Newton without damping factor shows that Gauss-Newton generates larger errors due to non-convex optimization. Using QR decomposition instead of Cholesky decomposition helps with training using Gauss-Newton. The differentiable Levenberg-Marquardt algorithm is compared to the Gauss-Newton algorithm for feature-metric BA. Increasing the damping factor \u03bb decreases rotation and translation errors until \u03bb = 0.5, then increases. A larger \u03bb reduces depth errors by making the final depth closer to the initial depth. The differentiable Levenberg-Marquardt algorithm is compared to the Gauss-Newton algorithm for feature-metric BA. Increasing the damping factor \u03bb decreases rotation and translation errors until \u03bb = 0.5, then increases. A larger \u03bb reduces depth errors by making the final depth closer to the initial depth. Results on the DeMoN dataset are summarized in Table 5, comparing various approaches including Oracle, SIFT, FF, and Matlab for solving camera poses. Our method outperforms DeMoN in camera motion and scene depth on the DeMoN dataset, except for the 'Scenes11' data. The poorer results on the 'Scene11' dataset are due to the use of synthesized images with random objects. Compared to LS-Net, our method achieves similar accuracy on camera poses but better scene depth, showcasing the superiority of our feature-metric BA. Our method can reconstruct multiple images and has been evaluated on the ScanNet dataset in a multi-view setting. Our method improves accuracy in multi-view reconstruction on the ScanNet BID11 dataset by randomly selecting two-view image pairs that share a common image to construct N-view sequences. The accuracy consistently increases with more views, showcasing the strength of multi-view geometry constraints. Compared to CodeSLAM, our method learns the feature pyramid and basis depth maps generator through feature-metric, resulting in better performance. Our method outperforms CodeSLAM in trajectory estimation on the EuroC MH02 sequence, with a median error less than half of CodeSLAM's error. End-to-end learning with feature pyramid and feature-metric BA shows superiority over learning depth parameterization only. Visualizations of basis depth maps further support our method's performance. Our method outperforms CodeSLAM in trajectory estimation on the EuroC MH02 sequence, with a median error less than half of CodeSLAM's error. End-to-end learning with feature pyramid and feature-metric BA shows superiority over learning depth parameterization only. Visualization of basis depth maps reveals that our learned basis depth maps have captured the latent structures of scenes, showing more shape details compared to previous methods. The quantitative results in Table 2 show shape details in the results, reflected in FORMULA0 and BID21."
}