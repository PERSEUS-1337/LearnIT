{
    "title": "ryepUj0qtX",
    "content": "Network Embeddings (NEs) aim to map nodes in a network to a $d$-dimensional Euclidean space $\\mathbb{R}^d, where similar nodes are placed close together. This mapping is useful for tasks like link prediction and classification. Various methods for NE have been developed, all involving defining node similarity, distance measures, and a loss function to optimize the embedding. Conditional Network Embeddings (CNEs) are introduced as a solution to embedding networks with challenging structural properties. A Bayesian approach is used to maximize information addition based on structural properties like node degrees. A block stochastic gradient descent algorithm is proposed for efficient fitting. Conditional Network Embeddings (CNEs) are shown to outperform state-of-the-art methods in link prediction and multi-label classification without adding complexity. CNEs also demonstrate potential for network visualization by mapping nodes into a d-dimensional Euclidean space. Network Embeddings (NEs) enable the use of machine learning methods on networks by measuring similarity between nodes based on adjacency, overlapping neighborhoods, or functional properties. Network Embeddings (NEs) compare similarity between nodes in a network using a metric in the embedding space. The challenge lies in networks being more expressive than embeddings in Euclidean spaces, especially in bipartite or k-partite networks. This highlights the difficulty in finding embeddings where nodes are close to their links while being far from other nodes. Conditional Network Embedding (CNE) is a probabilistic approach that addresses limitations of Network Embeddings (NEs) by optimizing embeddings based on prior knowledge about the network, derived from a prior distribution over the links. This approach overcomes challenges in modeling first-order and second-order similarity in networks, such as power law degree distributions. Conditional Network Embedding (CNE) utilizes a combined representation of structural information and Euclidean embedding to address challenges in network modeling. It allows for deriving knowledge from the network itself without external information. CNE can optimize embeddings based on prior knowledge, enabling better visualization and filtering of network information. The paper introduces the concept of NE conditional on prior knowledge about the network, using CNE ('Conditional Network Embedding') to combine a prior distribution with a probabilistic model for Euclidean embedding. This allows for better visualization and filtering of network information. The paper introduces CNE, a method for network embedding that combines a prior distribution with a probabilistic model for Euclidean embedding. It discusses a scalable algorithm for maximizing posterior probability and reports superior link prediction accuracy and multi-label classification performance compared to baselines. Additionally, it includes a brief overview of related work. The paper introduces CNE, a method for network embedding that aims to find an embedding X maximally informative about the given network G through a Maximum Likelihood estimation problem. The method combines a prior distribution with a probabilistic model for Euclidean embedding and includes code for repeating experiments and links to datasets. CNE is an innovative method for network embedding that utilizes a generic approach to derive prior distributions for the network and postulates the density function for the data conditional on the network. This allows for the incorporation of prior knowledge about the network into the formulation. The embedding does not need to represent information already captured by the prior distribution. Various types of prior information can be modeled for use by CNE, along with a possible conditional distribution and posterior distribution. In this paper, the posterior distribution is described, focusing on modeling different types of prior knowledge for the network. Three common types include knowledge about network density, individual node degrees, and edge density within subsets of nodes. These constraints on subsets of elements from the adjacency matrix do not fully determine the prior distribution P(G). The constraints on subsets of elements from the adjacency matrix do not fully determine the prior distribution P(G). Adriaens et al. (2017); van Leeuwen et al. (2016) showed that finding the distribution with maximum entropy from all distributions satisfying these constraints is a convex optimization problem that can be efficiently solved, particularly for sparse networks. The resulting distribution is a product of independent Bernoulli distributions, with probabilities Pij \u2208 [0, 1] for each link {i, j} in the network. These probabilities can be expressed in terms of a limited number of unique Lagrange multipliers. The Lagrange multipliers are used to impose constraints on subsets of elements in the adjacency matrix, allowing for the expression of prior knowledge on network properties. This includes relationships between connectedness and distance in node order or network assortativity. Further analysis on these constraints is left for future work. Moving on, the conditional density P(X|G) is postulated, with the understanding that any rotation or translation of an embedding is equally valid. The proposed model assumes that pairwise distances between points in the embedding should be considered equally good, with connected node pairs tending to be embedded closer together and disconnected pairs further apart. The conditional distribution for distances d_ij given {i, j} \u2208 E is modeled as a half-normal distribution with spread parameter \u03c31 > 0, while distances d_kl with {k, l} \u2208 E have a spread parameter \u03c32 > \u03c31. The embedding model assumes that connected nodes are closer together while disconnected nodes are further apart. Pairwise distances are modeled as half-normal distributions with spread parameters \u03c31 and \u03c32. The joint distribution of distances is represented as the product of marginal densities, despite constraints from Euclidean geometry. The improper density function used in the embedding model violates Euclidean geometry constraints, leading to certain pairwise distances being geometrically impossible. Despite this, the posterior for the network can still be derived using Bayes' rule. The likelihood function for maximizing the ML embedding is obtained from the improper density function, but normalization with the marginal density results in a properly normalized distribution. Maximizing the likelihood function poses a non-convex optimization problem, which can be solved using a proposed method. The likelihood function P (G|X) is a non-convex optimization problem that can be solved using a block stochastic gradient descent approach. The gradient of the likelihood function with respect to the embedding of a node is explained, where the first summation pulls the node towards connected nodes and the second pushes it away from unconnected nodes. The gradient terms are affected by parameter \u03c3 2 and prior P (G), with a large \u03c3 2 giving a stronger effect. In experiments, \u03c3 2 is always set to 2. To address computational challenges, the objective is approximated by sampling a subset of nodes from connected and unconnected sets. This sampling method ensures efficiency and concentration around the expected average. The text discusses the concentration of average embedding around its expectation, the choice of k for computational constraints, and the evaluation of CNE on downstream tasks compared to other NE methods. The experiments involved using various network embedding methods with default parameters and d = 128. Node2vec hyperparameters p and q were tuned using 10-fold cross validation. Link prediction was done by removing 50% of the links for training and testing. Node indices were shuffled for different train-test splits in each experiment. In the experiment, node indices are shuffled for different train-test splits. CNE is compared with other methods based on AUC, working well with small dimensionality and sample size. AUC is calculated by computing posterior probabilities of test links based on training network embeddings. CNE is compared against four simple baselines for link prediction. The link prediction results are shown in TAB1. CNE performs better than all baselines, even with a uniform prior. AUC scores are computed by comparing link probabilities from logistic regression with true labels. CNE outperforms baselines on 5 out of 7 networks with a uniform prior and on all networks with a degree prior. It encodes block structure knowledge in multi-relational networks, improving AUC by 3.91%. In terms of runtime, CNE is fastest in two cases and takes approximately twice as long in four cases compared to the fastest baseline. CNE outperforms baselines on multiple networks with different priors, showing improved AUC by 3.91%. In terms of runtime, CNE is fastest in some cases and takes twice as long in others compared to the fastest baseline. Multi-label classification can be framed as a link prediction problem by adding nodes for each label. CNE outperforms baselines on link prediction tasks, showing improved performance on less frequent labels. It facilitates visual exploration of multi-relational data by embedding datasets into 2-dimensional space. CNE embeds datasets into 2-dimensional space, with larger \u03c3 2 providing better visual separation between node clusters. The embedding separates bachelor student/courses/program nodes from master's nodes, influenced by node degrees. Individual node degrees are encoded as prior, showing courses grouped around different programs. The embedding in CNE separates bachelor and master program nodes based on node degrees, with courses grouped around different programs. NE methods typically have three components: similarity measure between nodes, metric in embedding space, and loss function comparing proximity between nodes. Early NE methods optimize mean-squared-error loss between Euclidean distances. CNE, unlike other NE methods, unifies proximity in embedding space and node similarity using a probabilistic measure, leading to more informative ML embeddings. CNE provides meaningful 2-d embeddings for direct visualization, conveying maximum information about the network. Conditional Network Embeddings (CNEs) aim to provide embeddings of networks that incorporate prior knowledge, which traditional Euclidean embeddings cannot accurately capture. This novel approach combines structural information with Euclidean embeddings through a simple probabilistic model, resulting in meaningful 2-d visualizations that convey maximum information about the network. Conditional Network Embeddings (CNEs) combine structural prior knowledge with Euclidean embeddings, showing superior performance in link prediction and multi-label classification tasks. Future work will explore alternative optimization strategies and different types of structural information as prior knowledge. The derivative calculations for Euclidean distance between points and log posterior are essential for understanding the model's behavior. The partial derivative for {i, j} / \u2208 E is nonzero only when m = i and n = j, resulting in the final gradient for maximizing the likelihood P (G|X). The effects of parameters \u03c3 1 and \u03c3 2 are illustrated in the posterior distribution plots, showing how larger \u03c3 2 values lead to stronger optimization effects. The flat area in the plot allows connected nodes to maintain some small. The experiments used various baseline embedding algorithms like Deepwalk, LINE, node2vec, and metapath2vec++ to learn node similarities based on random walks or network adjacencies. These algorithms offer different parameters to adjust the importance of different types of random walks in the learning process. metapath2vec++ and struc2vec BID19 are two approaches for heterogeneous node embedding. metapath2vec++ uses random walks between different node types, while struc2vec BID19 measures structural information and constructs embeddings using a multilayer graph. Benchmark networks like Facebook BID15 and arXiv ASTRO-PH BID15 were used in the experiments. The network datasets mentioned include arXiv ASTRO-PH, studentdb from University of Antwerp, Gowalla, and BlogCatalog. These datasets vary in the number of nodes and links, representing authors, students, professors, and bloggers in different collaboration or friendship networks. The curr_chunk discusses various social networks and their characteristics, such as BlogCatalog with 10,312 nodes and 333,983 links, Protein-Protein Interactions subnetwork with 3,890 nodes and 76,584 links, and Wikipedia with 4,777 nodes and 184,812 links. These networks have labels representing interests, Part-of-Speech tags, and are used for multi-label classifications. The remaining nodes in the network need label predictions. CNE and baselines are trained on the full network, with 50% of nodes used for training a logistic regression classifier. Regularization strength is determined through 10-fold cross-validation. Results are reported based on Macro-F 1 and Micro-F 1 scores. For CNE, embeddings are obtained with d = 32 and k = 150. Despite not being optimized, CNE performs comparably to state-of-the-art graph embedding methods. CNE performs well in comparison to state-of-the-art graph embedded methods, ranking third out of five on BlogCatalog and fourth out of five on PPI and Wikipedia. Despite not reflecting certain node information like degree, CNE's embeddings are effective for multi-label classification by treating it as a link prediction problem. By incorporating a 'block' prior, CNE can improve its performance in this task. CNE, a graph embedding method, performs well in multi-label classification by treating it as a link prediction problem. By incorporating a 'block' prior, CNE consistently outperforms baselines on Macro-F1 and is best on two datasets (BlogCatalog and PPI) for Micro-F1. The link prediction approach benefits CNE but not other methods. The superior performance of CNE-LP for multi-label classification is not solely due to the link prediction approach but also to a more informative embedding when combined with the prior. The runtime comparison of CNE with other baselines shows consistent benefits, with parameters set for optimal performance in link prediction tasks. TAB1 summarizes the runtime of all methods against various datasets used in the paper, showing that CNE is fastest in two cases, 12% slower than the fastest in one case (metapath2vec++), and approximately twice slower in the remaining four cases."
}