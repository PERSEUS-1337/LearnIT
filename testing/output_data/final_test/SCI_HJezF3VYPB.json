{
    "title": "HJezF3VYPB",
    "content": "Federated learning enhances data privacy and efficiency in machine learning across distributed devices like mobile phones and IoT. However, models trained with federated learning may struggle to generalize due to domain shift. To address this, a principled approach to federated domain adaptation is proposed, aligning representations learned among nodes with the target node's data distribution. This approach incorporates adversarial adaptation techniques, a dynamic attention mechanism, and feature disentanglement to improve knowledge transfer. Extensive experiments on image and text classification tasks demonstrate promising results. Experiments on image and text classification tasks show promising results in unsupervised federated domain adaptation. Federated Learning (FL) allows decentralized data and computation resources to train machine learning models while preserving privacy. Existing methods in FL overlook the non-i.i.d nature of data collected on each node. In this paper, the problem of transferring knowledge from decentralized nodes to a new node with a different data domain is addressed without additional user supervision. The novel problem of Unsupervised Federated Domain Adaptation (UFDA) is defined, presenting challenges due to locally stored data that cannot be shared, hindering mainstream domain adaptation methods. Federated Adversarial Domain Adaptation (FADA) is proposed to address domain shift in a federated learning system through adversarial techniques. This approach aims to preserve data privacy by training one model per source node and updating the target model with the aggregation of source gradients, reducing domain shift. The proposed Federated Adversarial Domain Adaptation (FADA) approach addresses domain shift in federated learning by training separate models on each source domain and aggregating their gradients with a dynamic attention mechanism. The FADA model focuses on extracting domain-invariant features through adversarial domain alignment and feature disentanglement. Extensive experiments on real-world datasets validate the efficiency of this adaptation algorithm. The Federated Adversarial Domain Adaptation (FADA) approach improves adaptation performance in federated learning by training separate models on each source domain and aggregating their gradients with a dynamic attention mechanism. Extensive experiments on real-world datasets validate the effectiveness of the devised model for unsupervised domain adaptation. The text discusses the use of adversarial training and domain adaptation in machine learning. It mentions the limitations of centralized data and introduces Federated Learning as a decentralized approach. CryptoNets are proposed to enhance privacy in machine learning models. The text discusses various methods to enhance privacy and efficiency in federated learning. CryptoNets, SecureML, and federated multi-task learning are introduced to address different challenges in collaborative training. These methods aim to improve machine learning models in a decentralized setting, with a focus on privacy preservation and handling non-i.i.d. data. The proposed work introduces a federated learning framework for unsupervised domain adaptation, focusing on feature disentanglement in deep neural networks. Previous studies have explored the use of generative adversarial networks (GANs) and variational autoencoders (VAEs) to learn interpretable representations. This includes methods like auxiliary classifier GAN (AC-GAN) and a unified feature disentanglement framework for domain-invariant learning. The proposed method aims to disentangle domain-invariant features from domain-specific features using adversarial training and minimizing mutual information between them. It builds upon previous works on feature disentanglement and theoretical error bounds for single-source domain adaptation. Blitzer et al., 2008) devised by Ben-David et al. Our derived error bound for unsupervised federated domain adaptation focuses on the high-level interpretation of the error bound. Notation: D S and D T are source and target distributions on input space X with a ground-truth labeling function g : X \u2192 {0, 1}. Hypothesis h : X \u2192 {0, 1} has error w.r.t ground-truth labeling function g. Risk and empirical risk of hypothesis h on D S are denoted as S (h) and S (h), and on D T as T (h) and T (h). H-divergence between two distributions D and D is defined, where H is a hypothesis class for input space X. Symmetric difference space H\u2206H is defined as: H\u2206H := {h(x) \u2295 h (x))|h, h \u2208 H}. The symmetric difference space H\u2206H is defined as: H\u2206H := {h(x) \u2295 h (x))|h, h \u2208 H}. The optimal hypothesis achieving minimum risk on the source and target is denoted as h * := arg min h\u2208H S (h) + T (h), with error \u03bb := S (h * ) + T (h * ). Blitzer et al. (2007b) prove an error bound on the target domain for a hypothesis space H of V C-dimension d. In a UFDA system, D S is distributed on N nodes and data are not shareable. Classical domain adaptation algorithms aim to minimize target risk T (h), but in UFDA, one model cannot directly access the data. In a UFDA system, separate models are learned for each distributed node due to security and privacy concerns. The target hypothesis is an aggregation of parameters from source models. An error bound is derived for federated domain adaptation, with a hypothesis class H having VC-dimension d. The weighted error bound considers empirical distributions from source and target domains in a federated learning system. The risk of the optimal hypothesis on the mixture of source and target domains is denoted by \u03bb i. The error bound in (2) extends from (1) and is equivalent when N = 1 source domain. Mansour et al. (2009) provide a bound for multiple-source domain adaptation with a novel target domain, involving H\u2206H discrepancy and VC-dimensional constraint. Blitzer et al. (2007b) propose a bound for semi-supervised multi-source domain adaptation with partial target labels, while our bound is for unsupervised learning. Zhao et al. (2018) introduce error bounds for multi-source domain. The error bounds introduced by Zhao et al. (2018) for multi-source domain adaptation assume shared hypotheses, while our approach involves multiple hypotheses. Our dynamic attention model learns weights and uses federated adversarial alignment to minimize domain discrepancies, enhancing knowledge transfer through representation disentanglement. The dynamic attention model proposed addresses the issue of nodes with no contribution or negative transfer in the target domain by using a mask on gradients from source domains. It increases the weight of beneficial nodes and limits the weight of detrimental ones based on gap statistics evaluation of target features. The gap statistics gain between source domains is measured to improve cluster performance before and after updating the target model. Existing work proposes adversarial training to minimize domain discrepancy in machine learning models. In UFDA, federated adversarial alignment is proposed to address the issue of accessing source and target data simultaneously. The optimization is divided into two steps: training domain-specific local feature extractors and a global discriminator. For each domain, a local feature extractor is trained, and an adversarial domain identifier is trained to align distributions in an adversarial manner between source and target domains. Representation Disentanglement is employed in UFDA to extract domain-invariant features by disentangling the features extracted by (G i , G t ) into domain-invariant and domain-specific features using a disentangler D i. The disentangler D i separates features into two branches for training a K-way classifier and class identifier to predict labels. The objective is to confuse the class identifier by generating domain-specific features and enhance disentanglement by minimizing mutual information between domain-invariant and domain-specific features. The mutual information is a pivotal measure in different distributions, but only tractable for discrete variables. It is defined as I(f di ; f ds ) = P\u00d7Q log dP PQ dP P \u2297P Q dP PQ, where P PQ is the joint probability distribution of (f di , f ds), and P P = Q dP PQ, P Q = Q dP PQ are the marginals. The Mutual Information Neural Estimator (MINE) is used to estimate mutual information by leveraging a neural network. Monte-Carlo integration is used to avoid computing integrals, with (p, q) sampled from joint distribution and q sampled from marginal distribution. Domain-invariant and domain-specific features are reconstructed with L2 loss to maintain representation integrity. The model is trained using Stochastic Gradient Descent for federated alignment and representation disentanglement. Tasks include digit classification, object recognition, and sentiment analysis. Experiments are conducted on a 10 Titan-Xp GPU cluster with data samples shown in Figure 2 and domain data distribution in Table 9. Our experiments are conducted on a 10 Titan-Xp GPU cluster and simulated on a single machine. The model is implemented with PyTorch and experiments are repeated multiple times on different datasets. Three different ablations are proposed to explore the model's effectiveness. The detailed architecture of the model can be found in Table 7. Many DA models require access to data from different domains, which is infeasible. Popular domain adaptation baselines include Domain Adversarial Neural Network (DANN), Deep Adaptation Network (DAN), Automatic Domain Alignment Layers (AutoDIAL), and Adaptive Batch Normalization (AdaBN). DANN minimizes the domain gap with a gradient reversal layer, DAN uses multi-kernel MMD loss, AutoDIAL introduces domain alignment layers, and AdaBN applies Batch Normalization for knowledge transfer between domains. In the context of popular domain adaptation baselines like DANN and DAN, the study explores federated domain adaptation (f-DAN and f-DANN) and multi-source domain adaptation experiments. The experimental results show that the model's feature embeddings have smaller intra-class variance and larger inter-class variance compared to f-DANN and f-DAN. Our model demonstrates the capability to generate desired feature embeddings and extract domain-invariant features across different domains. The Office-Caltech10 dataset contains four domains: Caltech, Amazon, Webcam, and DSLR. We use AlexNet and ResNet as the backbone feature generator, pre-trained on ImageNet. Our model achieves high accuracy on different datasets using AlexNet and ResNet backbones, outperforming compared models. The study demonstrates improved accuracy using a ResNet backbone and a dynamic attention mechanism for training. The FADA features show smaller A-distanced values compared to other features, indicating better adaptation between source and target domains. The study shows that dynamic attention reduces error, especially in A,C,W\u2192D setting. Confusion matrices reveal f-DAN confuses \"calculator\" vs. \"keyboard\" and \"backpack\" with \"headphones\", while FADA can distinguish them. Ablation study confirms the importance of dynamic attention module in the model. Results from the experiment on DomainNet show that the model outperforms baselines with 28.9% and 30.3% accuracy using AlexNet and ResNet backbone. The dataset contains around 0.6 million images, making even a one-percent performance improvement significant. However, performance drops when infograph and quickdraw are selected as target domains due to large domain shifts. The dataset includes reviews from amazon.com users for Books, DVDs, Electronics, and Kitchen appliances. A 400-dimensional bag-of-words representation is used with a deep neural network, achieving 78.9% accuracy on sentiment analysis. The model performs well on linguistic tasks and benefits from dynamic attention and federated adversarial alignment. The dynamic attention and federated adversarial alignment improve performance, but the boost from Model II to Model III is limited. Linguistic features are harder to disentangle compared to visual features. Ablation study analysis demonstrates the effectiveness of dynamic attention, showing performance drops without it. The model adjusts weights based on convergence rates in federated learning systems and domain shifts. In this paper, a novel unsupervised federated domain adaptation (UFDA) problem is introduced, along with a theoretical generalization bound for UFDA. A model called Federated Adversarial Domain Adaptation (FADA) is proposed to transfer knowledge from distributed source domains to an unlabeled target domain using dynamic attention. Feature disentanglement is shown to enhance FADA performance in UFDA tasks through empirical evaluation on vision and linguistic benchmarks. Detailed model architecture and notations are provided for clarity. The architecture for digit recognition task (\"Digit-Five\" dataset) and image recognition task (Office-Caltech10 and DomainNet) is detailed, including input/output dimensions, kernel size, stride, padding, and dropout probabilities. Train/test splits, number of images, and review details are also provided for different datasets. Theoretical error bound for unsupervised federated domain adaptation is provided in Theorem 2, assuming source data forms a mixture source domain. Data on different nodes cannot be shared in federated learning. The error bound is valid only when model weights on all nodes are fully synchronized."
}