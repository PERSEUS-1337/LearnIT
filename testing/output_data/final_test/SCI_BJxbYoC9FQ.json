{
    "title": "BJxbYoC9FQ",
    "content": "The proposed approach for saliency map extraction is classifier-agnostic, extracting all image parts usable by any classifier. It outperforms existing weakly-supervised localization techniques and sets a new state of the art result on the ImageNet dataset. The gradient of the class-specific score of a classifier can be used to create saliency maps of images, but these maps may not highlight all relevant evidence and can be noisy. Recent work has focused on regularization techniques to improve these maps, such as averaging multiple saliency maps for perturbed images. However, simply adding tricks to existing methods may not be the most principled approach to obtaining saliency maps that capture all useful information. In this work, the focus is on generating saliency maps that highlight pixels crucial for classification, regardless of the classifier used. The traditional method of using a generative model to confuse a specific classifier has limitations, leading to the proposal of a class-agnostic saliency map extraction approach. This new method aims to create saliency maps that are not heavily dependent on any particular classifier. Our approach focuses on generating classifier-agnostic saliency maps, which extract high-quality evidence from input images without relying on specific classifiers. This method outperforms existing techniques in object localization, without the need for complex regularization penalties or image preprocessing tricks. The proposed approach outperforms weakly-supervised techniques on the ImageNet dataset and approaches the performance of strongly supervised models in localization. It also works well for unseen classes and has potential applications in medical image analysis. The method focuses on extracting salient regions of an input image without relying on specific classifiers. The mapping m should retain relevant pixels for classification (1) and mask irrelevant pixels (0). Previous work focused on classifiers, but the proposed approach aims to confuse a classifier f by finding a mapping m. This classifier-dependent saliency map extraction differs based on the classifier used. The mapping m should retain relevant pixels for classification and mask irrelevant pixels. Two equally good classifiers may use different subsets of input pixels for classification, as illustrated by an example with two identical copies of images concatenated together. The mapping m should retain relevant pixels for classification and mask irrelevant pixels. To address the issue of saliency mapping's dependence on a single classifier, the objective function is altered to consider all possible classifiers weighted by their posterior probabilities. This optimization problem involves searching over the space of all possible classifiers to find a mapping that works with all of them. The proposed approach involves a classifier-agnostic saliency map extraction, considering all possible classifiers weighted by their posterior probabilities. The optimization problem is intractable due to the expectation over the posterior distribution, making it challenging to find a mapping that works with all classifiers. The optimization problem is challenging due to the expectation inside the loop for the mapping m. To solve this, we simultaneously estimate the mapping m and the expected objective by sampling from the posterior distribution and updating parameters alternately until convergence. This approach is similar to the training procedure of GANs. The optimization problem is challenging due to the expectation inside the loop for the mapping m. To solve this, simultaneous estimation of the mapping m and the expected objective is done by sampling from the posterior distribution and updating parameters alternately until convergence, similar to the training procedure of GANs. The role of a generator is taken by m, while the classifier f acts as a discriminator. Adversarial training has been used to improve saliency maps, with the score function S(m, f) evaluating the quality of the saliency map by balancing precision and recall. In our approach, we propose using entropy H(f ((1 \u2212 m(x)) x)) to generate masks that cover all salient pixels in the input image, avoiding adversarial artifacts. We introduce a regularization term, such as total variation or L1 norm, to prevent trivial solutions in the saliency map output. The approach involves using entropy to generate masks for salient pixels in the input image, with a regularization term like total variation or L1 norm to avoid trivial solutions in the saliency map. Strategies for thinning a set of classifiers are proposed and evaluated, including keeping the first classifier only, the first and last classifiers, or a growing set with periodic removals. Classification loss is also considered in the evaluation. In experiments with L100, a new classification loss formulation inspired by adversarial training was found to work better. Models were trained on ImageNet dataset and evaluated on the validation set using ground truth class labels. The architecture for the classifier and mapping in the experiments involves using ResNet-50 as the classifier and an encoder-decoder structure for the mapping. The encoder, also a ResNet-50, shares weights with the classifier. A deep deconvolutional network serves as the decoder to output image masks. Regularization coefficient \u03bb R is optimized using an adaptive strategy to avoid manual selection. The code is publicly available for reproducibility. In the experiments, the regularization coefficient \u03bb R is manually set to control the average number of relevant pixels. This approach ensures stability and effectiveness in training the mapping m, especially for images where the classifier is not easily confused. The proposed method also addresses issues with small objects in images, as observed in previous studies. Only a single architecture is used in the experiments, with ResNet-50 as the classifier and an encoder-decoder structure for the mapping. The experiments use a single architecture, denoted as CASM, and a baseline model trained with a fixed classifier. The mask is discretized to focus on the dominant object, and the learned mapping is visualized through saliency maps. The saliency map of each image is visualized in three ways: masked-in image, masked-out image, and inpainted masked-out image. An inpainting algorithm is used to reconstruct masked-out objects. Multiple classifiers are evaluated on these images to test the classifier-agnostic saliency mapping. The goal is to break classifiers with inpainted masked-out images while maintaining minimal performance degradation with masked-in images. The saliency map is used for object localization, evaluated on the ILSVRC'14 task. Three metrics are used to measure localization quality: official metric (OM), localization error (LE), and original saliency evaluation. The saliency map is evaluated for object localization using various metrics. The original saliency map is assessed using the continuous F1 score, precision, and recall. The proposed approach outperforms the Baseline method, producing better saliency maps. Visualization of the images shows the effectiveness of the proposed approach. The proposed approach, CASM, generates more regular masks with lower total variation and smaller entropy of mask pixel intensities compared to the Baseline method. CASM also produces saliency maps of varying sizes, as indicated by the larger standard deviation of the masked out volume. Classification accuracy of convolutional networks on masked-in, masked-out, and inpainted masked-out images is shown in Figure 3. Orange dots represent ResNet-50 models, while blue dots represent other convolutional networks. The Baseline method fails to mask out all relevant pixels, making inpainted masked-out images easier to classify than those using CASM. Classifiers struggle with masked-out images from Baseline due to adversarial saliency maps. Simple inpainting of masked-out images significantly increases accuracy. Inpainting of masked-out images improves accuracy with Baseline saliency maps, while CASM does not benefit from inpainting. CASM outperforms prior approaches, including Baseline, in localization performance. Thinning strategies are compared in Table 2. In Table 2, thinning strategies L100 and L1000 outperform others in terms of LE and OM metrics, closely followed by strategy L. Sharing encoder and classifier parameters is beneficial, as shown in Table 2. Using entropy as a score function results in better mapping for OM and LE metrics. Classification loss is better for F1, focusing on the dominant object. The proposed approach for object localization in images focuses on the dominant object, leading to higher scores in F1. It does not require prior knowledge of object classes, allowing it to be used with unseen classes. Testing on subsets of ImageNet training data shows the effectiveness of the approach across different class distributions. The proposed approach for object localization in images focuses on the dominant object, leading to higher scores in F1. Testing on subsets of ImageNet training data shows the effectiveness of the approach across different class distributions, with generalization across seen and unseen classes. The difference in performance between seen and unseen classes is minimal, except for the model trained on 20% of classes. The proposed model serves as a class-agnostic saliency map, showing promising results for localizing objects from previously unseen classes. The network Fan et al. (2017) is closely related to our work, but we have key differences. We use entropy for training the mapping, tie weights for faster training, prevent classifier shift, and work solely on raw pixels for object localization. Their reliance on superpixels for masks may lead to missed details and complexity. Our approach focuses on raw pixel data for saliency map extraction, unlike Dabkowski & Gal (2017) who use a classifier-dependent method with complex training objectives. We proposed a classifier-agnostic framework that simultaneously trains a classifier and a saliency mapping using stochastic gradient descent, aiming to work for all possible classifiers weighted by their posterior probabilities. Our approach focuses on raw pixel data for saliency map extraction, using a classifier-agnostic framework that trains a classifier and a saliency mapping simultaneously with stochastic gradient descent. The proposed approach outperforms existing weakly supervised methods for object localization and can be applied to images with unseen classes, paving the way for class-agnostic saliency map extraction. ResNet-50 is used as the classifier, and an encoder-decoder architecture is employed for mapping, with the encoder sharing weights with the classifier for improved performance. The decoder in the architecture is a deep deconvolutional network that outputs the mask of an input image. It utilizes hidden layers of the encoder, upsampling them and concatenating into a single feature map H. The upsampling process involves convolution, batch normalization, ReLU activation, and rescaling. A final convolutional filter with sigmoid activation is applied to H, resulting in a 224x224 pixel-sized mask. The training procedure involves initializing the classifier on the entire training set, which aids in learning, especially in the early stages. The pretraining strategy involves using a pretrained ResNet-50 model for learning, with specific optimization techniques such as SGD and Adam. The training process includes a fixed number of epochs and a resizing policy for images to 224x224 pixels. The image resizing methods discussed include resizing the original size to 224x224 pixels with a central crop, preserving the aspect ratio but potentially limiting IOU accuracy, and resizing directly to 224x224 pixels without cropping, ensuring IOU accuracy but distorting the aspect ratio. The difference in LE scores for different resizing strategies is minimal, with a 0.6% variation for CASM. The study reports results for the first method and replicates Fig. 2 for sixteen randomly chosen samples in the appendix. In the appendix, Fig. 2 is replicated for sixteen randomly chosen classes. Saliency maps for seven consecutive images from the validation set are visualized in each figure. Original images are in the first row, followed by masked-in, masked-out, and inpainted masked-out images. Two instances of CASM (using different thinning strategies - L or L100) and Baseline are used."
}