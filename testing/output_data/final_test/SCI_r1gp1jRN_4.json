{
    "title": "r1gp1jRN_4",
    "content": "Supervised deep learning methods require large-scale datasets with clean labels, but collecting such data is challenging. Two frameworks, semi-supervised learning and robust learning with noisy labels, aim to address this issue independently. This study introduces learning from bi-quality data, where a small portion is clean and the rest is corrupt. Comparing recent algorithms, semi-supervised learning performs better than robust learning. A training strategy involving mixup techniques is proposed for effective learning from imperfect data. Learning from imperfect data is crucial for machine learning applications, especially deep learning. Semi-supervised learning (SSL) utilizes a small amount of labeled data along with a large amount of unlabeled data to improve supervised learning performance. In contrast, robust learning with noisy labels (RLL) focuses on enhancing performance despite mislabeled data. SSL employs consistency regularization methods, while RLL aims to prevent performance deterioration from corrupted labels. In this paper, the authors aim to unify semi-supervised learning (SSL) and robust learning with noisy labels (RLL) by introducing a generalization based on the concept of trusted data in the literature of RLL. The goal is to address the limited-data problem by leveraging a small amount of clean data to estimate noise transition matrix or select correctly-labeled samples. The study aims to unify SSL and RLL by leveraging clean data to estimate noise transition matrix or select correctly-labeled samples. Recent SSL and RLL algorithms were compared using image classification, showing that RLL methods with clean data cannot outperform SSL. A baseline algorithm combining mixup losses for SSL and RLL was proposed, yielding comparable results to specific methods and effectively utilizing information from noisy labels. Learning from bi-quality data involves utilizing both trusted data with correct labels and untrusted data with potentially incorrect labels. The ratio of trusted to untrusted data is denoted as p, with quality measured by the conditional probabilities of labels given the inputs. Quality is represented by a parameter q, where q = 0 indicates independence of labels from input features. Learning from bi-quality data involves utilizing both trusted data with correct labels and untrusted data with potentially incorrect labels. The quality of untrusted data q U is in [0, 1], while trusted data q T is 1. One strategy is to divide the data into high-quality labeled D T and lower-quality labeled D U. SSL is when q = 0, ignoring labels of untrusted data. RLL without trustable data is when p = 0 and 0 < q < 1. In the context of learning from bi-quality data, RLL without trustable data involves utilizing untrusted data with varying levels of quality. When q is high, there may be performance gains by using somewhat informative labels. However, when q is close to zero, untrusted data becomes uninformative, and better generalization may be achieved with standard SSL. To address this challenge, an adaptive mechanism is needed to combine SSL and RLL effectively. This can be achieved by using a convex combination of loss functions for SSL and RLL, with an additional hyperparameter \u03b3 to adjust the fusion of the two learning strategies. In this study, a regularization technique called mixup is used for SSL and RLL to improve performance under label corruption. The neural network models are trained on virtual training pairs, where \u03bb is sampled from a Beta distribution. Mixup is applied as a consistency regularization for SSL and the details are shown in Algorithm 1. The parameters of neural networks are updated with a combination of losses on trusted data and untrusted data. The method used in this study, called mixmixup, combines losses on trusted and untrusted data using sigmoid scheduling. The training dataset is split into trusted and untrusted data, with labels randomly replaced in untrusted samples. WRN-28-2 is used as the image classifier, and SGD is used for optimization. The study introduces the mixmixup method for combining losses on trusted and untrusted data using sigmoid scheduling. WRN-28-2 is utilized as the image classifier, and SGD is employed for optimization. The model is implemented with PyTorch v1.0 and hyperparameters are tuned with a Bayesian optimization algorithm in Optuna v0.8. The results show training WRN-28-2 with bi-quality data of different quality levels, with various label replacement scenarios. The study introduces the mixmixup method for combining losses on trusted and untrusted data using sigmoid scheduling. The quality of the data ranges from 0.6 to 0.0, corresponding to different label scenarios. Results suggest that SSL methods can outperform RLL methods with trusted data, highlighting the challenge of learning from partially corrupted labels. In this paper, a novel framework of weakly supervised learning is introduced by unifying SSL and RLL. The mixmixup method is proposed to handle the difficulty of learning from partially corrupted labels, achieving competitive results with semisupervised and robust learning methodologies. Results show that mixmixup can effectively use information from corrupted labels, outperforming SSL methods in some scenarios. The proposed method in this work aims to exploit information from noisy labels without using estimated quality, instead introducing hyperparameters. Quality estimation's role in hyperparameter tuning remains an open question. This research was supported by JSPS KAKENHI Grant Number JP19H04166."
}