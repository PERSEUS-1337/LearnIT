{
    "title": "ryup8-WCW",
    "content": "Many neural networks use a large number of parameters for good performance. This paper explores the intrinsic dimension of the objective landscape by training networks in a smaller subspace. The approach is simple and produces interesting conclusions, showing that many problems have smaller intrinsic dimensions than expected. The intrinsic dimension of the objective landscape is explored by training networks in a smaller subspace, revealing that extra parameters increase the dimensionality of the solution manifold. This method provides a new cartography of objective landscapes and a technique for obtaining an upper bound on the minimum description length of a solution. It also offers a simple approach for compressing networks, sometimes by more than 100 times. Training a neural network involves choosing a loss function and architecture, initializing weights, and adjusting them to minimize loss. The objective landscape is determined by the dataset and architecture, with parameter initialization and optimization steps exploring this landscape in D dimensions. Training a neural network involves navigating a landscape of \"hills and valleys\" in D dimensions, where each point corresponds to a loss value. Understanding these high-dimensional landscapes is crucial for neural network researchers, as extrapolating low-dimensional intuitions can lead to unreliable conclusions. Several papers have highlighted flaws in common extrapolation methods, emphasizing the importance of interpreting these landscapes accurately. BID4 and BID9 challenge common assumptions about navigating high-dimensional landscapes in neural network optimization. BID4 reveals that critical points are often saddlepoints, not valleys, while BID9 shows that paths from start to finish are typically monotonically decreasing. This suggests that the landscape may be simpler than previously thought, with fewer obstacles to optimization. In this paper, the authors explore the structure of the objective landscape by optimizing in randomly generated subspaces of the full parameter space. They investigate the intrinsic dimension of problems by gradually increasing the subspace dimension until solutions appear. This approach provides new insights into optimization problems in neural networks. In this study, the authors delve into the optimization problems arising from neural network models by defining intrinsic dimension as a measure of landscape difficulty. They measure intrinsic dimension across various network types and datasets, drawing insights on network behavior. An illustrative toy problem is introduced to showcase the concept of intrinsic dimension. The error cost function requires specific sums for groups of elements in a vector. Solutions to this problem are redundant, forming a 990 dimensional hyperplane. The intrinsic dimensionality of the solution set is 10, reflecting the constraints on the parameter vector. Despite the large space, only a few things need to be correct. Random subspace optimization provides a method to measure or approximate d int for complex problems, such as neural network training. Instead of directly evaluating the gradient of a loss with respect to \u03b8 (D), training in a random subspace involves defining \u03b8 (D) using a randomly generated projection matrix P. This limits the system to d degrees of freedom, with \u03b8 (d) initialized to zeros. This approach is important for neural network training. Random subspace optimization involves defining \u03b8 (D) using a randomly generated projection matrix P, limiting the system to d degrees of freedom with \u03b8 (d) initialized to zeros. This allows neural networks to benefit from good initialization schemes and well-conditioned parameter space for efficient gradient descent. The columns of P are normalized to unit length, forming an approximately orthonormal basis for a randomly oriented d dimensional subspace of R D. Random subspace optimization involves defining \u03b8 (D) using a randomly generated projection matrix P, limiting the system to d degrees of freedom. The path taken by optimizers like RMSProp and Adam in \u03b8 (D) space depends on the rotation chosen. Solutions may not be found if d is less than the codimension of the solution, but if d \u2265 D \u2212s and the solution set is a hyperplane, solutions will be found. When d \u2265 D \u2212s, solutions may intersect the subspace for hyperplane solution sets, but not guaranteed for arbitrary topology sets. By iteratively increasing d and re-running optimization, one estimate of d int is obtained. In a toy problem, solutions were found at d = 10, confirming d int = 10. This method is applied to measure intrinsic dimensions in neural network problems and analyze objective landscapes and solution sets. In neural network problems, intrinsic dimensions are determined by thresholding network performance relative to a baseline model. Validation accuracy or total reward is used for performance measurement to ensure real-world grounding and allow model comparison. The intrinsic dimension of the \"100%\" solution is defined as solutions statistically indistinguishable from baseline solutions. When measuring the intrinsic dimension of solutions in neural network problems, performance can vary widely due to different factors. To address this, the intrinsic dimension of the \"90%\" solution is defined as solutions with performance at least 90% of the baseline. This approach allows for a more practical and useful measurement of the basic difficulty of problems and the degrees of freedom needed to solve them. The intrinsic dimension of solutions in neural network problems is measured using a threshold (FIG2) to ensure robustness to small noise in performance. A fully connected classifier trained on MNIST with layer sizes 784-200-200-10 is analyzed, with experiments showing monotonically increasing subspace dimension d. Researchers may find it useful to measure d using different thresholds in the future. The experiments show that increasing subspace dimension d leads to improved performance, with the network's intrinsic dimension d int90 measured at about 750. This low dimensionality allows for significant compression, with only 0.4% of degrees of freedom being used to achieve 90% performance. This approach offers a new way to create and train compressed networks, requiring only the storage of a tuple of three items. The paper introduces a simpler approach to network compression compared to previous methods. It operates in the entire parameter space and reduces the number of degrees of freedom. Unlike Bayesian methods, it does not achieve as high compression levels but offers a more straightforward process. The approach discussed focuses on reducing the number of degrees of freedom in network compression. It does not decrease the number of bits needed to store each degree of freedom, unlike quantizing weights. Various methods, such as weight pruning and weight tying, are related to subspace training. Additionally, there are papers on compressing networks for computational savings during the forward pass. The subspace is restricted to lie along the equidistant \"diagonals\" between axes tied together, showing robustness of intrinsic dimension across FC networks with varying layers and widths. A grid sweep of networks with different hidden layers and widths was performed, revealing that intrinsic dimension changes little even as models grow in width or depth. The study found that adding extra parameters to a neural network only increases redundancy in the solution. Larger models have greater redundancy and cover more space. The intrinsic dimension of fully connected networks on MNIST remains constant over a wide range of hyperparameters. The study explores the efficiency of random subspaces for fully connected networks. Despite generating small networks to match performance, a gap still exists compared to larger networks. The intrinsic dimension of a convolutional network is also measured, showing validation accuracy vs. subspace dimension. The study examines the efficiency of random subspaces for fully connected networks and convolutional networks. Validation accuracy vs. subspace dimension is measured, with a compression rate of about 150\u00d7 for the network. The performance gap between direct and subspace training methods narrows for fixed budgets. The random subspace training method naturally leads to a compressed representation of the network, with d floating point numbers needing to be stored. The relationship between intrinsic dimension and Minimum Description Length is discussed. The study discusses the relationship between intrinsic dimension and Minimum Description Length (MDL) in evaluating model efficiency. By comparing the MDL of different models, such as LeNet and FC networks, it can help determine which solutions are better suited for a problem. This method may be particularly useful as models become larger and more complex. The study explores the importance of Minimum Description Length (MDL) in evaluating model efficiency, especially for complex models like LeNet and FC networks. It suggests that approximating MDL can guide model exploration for datasets beyond MNIST and for models with separate sub-models. This approach differs from measuring the intrinsic dimension of a dataset and focuses on the degrees of freedom needed to represent a model for part of the dataset. The study discusses the importance of Minimum Description Length (MDL) in evaluating model efficiency for complex models like LeNet and FC networks. It explores how large networks can be trained to memorize entire training sets with randomly assigned labels or shuffled pixels. Zhang et al. (2017) showed that externally similar networks may have different intrinsic dimensions, even if their training loss is identical. When training on datasets with shuffled pixels or labels, the intrinsic dimension of convnets increases significantly compared to FC networks, indicating the need for more degrees of freedom to model the underlying distribution. This reveals that convnets are better suited for classifying images with local structure, while FC networks remain invariant to input permutation. When training on datasets with shuffled pixels or labels, convnets have a higher intrinsic dimension compared to FC networks, indicating the need for more degrees of freedom. Memorizing random labels on the MNIST training set requires a high dimension of 190,000, or 3.8 floats per label. Generalization within a training set may be occurring as the network builds a shared infrastructure for more efficient memorization. Scaling to larger datasets like CIFAR-10 and ImageNet requires more efficient methods for generating and memorizing labels. In the context of training on datasets with shuffled pixels or labels, convnets have a higher intrinsic dimension compared to FC networks, indicating the need for more degrees of freedom. To address this, more efficient methods of generating and projecting from random subspaces are necessary. Three methods of projection are discussed: dense matrix projection, sparse matrix projection, and the Fastfood transform. Sparse projection is used for training networks on CIFAR-10, while the Fastfood transform is used for ImageNet. Measuring intrinsic dimension helps in comparing supervised learning and reinforcement learning tasks. The intrinsic dimension of control tasks is measured using both value-based (DQN) and policy-based algorithms. The Deep Q-Network (DQN) and Evolutionary Strategies (ES) are evaluated for various tasks, with performance measured by the mean evaluation reward. The inverted pendulum task is solved with only four parameters, while the walking humanoid task requires dimension 700. Playing Pong on Atari is also discussed. In this paper, the intrinsic dimension of objective landscapes is defined and a method for approximating it using random subspace training is presented. This approach is used to compare problem difficulty within and across domains, showing that in some cases intrinsic dimension allows for network compression while in others it suggests better models suited to the problem. Further research could explore alternative methods for creating subspaces for reparameterization. In this study, random linear subspaces were chosen for reparameterization. Other linear or non-linear subspaces could be constructed for better solutions. As networks become larger and more diverse, methods like measuring intrinsic dimension can provide insight into individual module properties. The study aimed to find the intrinsic dimension across 20 FC networks with varying depths and widths through a grid sweep analysis. The results are presented in FIG7, showing the mean and variance for each dimension. The study focused on analyzing the intrinsic dimension of FC networks through a grid sweep analysis. Results in FIG7 display mean and variance for each dimension, showing that variance is generally small and performance increases with dimensionality. The impact of lucky vs. unlucky random projections is minimal compared to subspace dimensionality. Variance due to different P matrices is expected to be smaller than variance from random initial parameter vectors. In some experiments, single runs are used to estimate intrinsic dimension, with slightly more accurate results possible with multiple runs. Figure 8 shows the relationship between d int90 and D across 20 networks using a per-model baseline. Results are similar to Figure 3 but with slightly lower intrinsic dimension. Shuffled MNIST datasets are considered, including shuffled pixel datasets where FC networks perform as easily as the base dataset. The shuffled label dataset involves randomly shuffling labels for training sets, leading to training accuracy evaluation only. An FC network trained on the full shuffled label MNIST dataset yielded an intrinsic dimension of 190k, requiring 3.8 floats to memorize each random label at 90% accuracy. The intrinsic dimension was estimated on shuffled label versions of MNIST at different scales, showing interesting results. The dataset memorized becomes smaller, the number of floats required to memorize each label increases. The intrinsic dimension also increases with dataset size, but not linearly. Networks memorizing large training sets may use shared machinery for memorization, leading to non-negligible generalization within the training set. Random subspace training can make optimization more stable in some cases. Random subspace training can improve optimization stability, especially for deeper networks. Training results for FC networks with up to 10 layers using SGD with ReLUs and He initialization showed failures at depths higher than 4. Additionally, for MNIST with shuffled labels, direct training with SGD struggles to achieve high accuracy, while subspace training with SGD or Adam reliably reach 100% memorization. The optimization problem may be better conditioned in the subspace case due to projecting across all parameters. The downside of projecting across parameters with varying scales could lead to ignoring dimensions with tiny gradients. Methods like RMSProp and Adam address this issue by rescaling step sizes per dimension. Further work is needed to enhance network adaptability to subspace training, such as scaling direct parameters similarly or adding a pre-scaling layer. Experiments on MNIST FC networks reveal insights on the impact of optimizers, with SGD and ADAM showing differences in achieving intrinsic dimension. The impact of stochastic optimizers on intrinsic dimension is investigated using two optimizers and two baselines. DQN on Cartpole, a classic control game in OpenAI Gym, is used as a starting point. The goal is to prevent a pendulum from falling over by applying force to the cart. Two easier environments, Pole and Cart, are also created. The study investigates the impact of stochastic optimizers on intrinsic dimension using DQN on Cartpole. Two easier environments, Pole and Cart, are also examined. Results show that the difficulty of optimization landscape is low, with intrinsic dimensions of 25, 23, and 7 for CartPole, Pole, and Cart, respectively. Insights include the observation that driving a cart is easier than keeping a pole straight. Additional tasks include InvertedPendulum\u2212v1, Humanoid\u2212v1, and Pong\u2212v0 in ES 3 RL. The study explores the impact of stochastic optimizers on intrinsic dimension using DQN on Cartpole and other environments like InvertedPendulum\u2212v1, Humanoid\u2212v1, and Pong\u2212v0 in ES 3 RL tasks. The intrinsic dimensionality of InvertedPendulum\u2212v1 is found to be smaller than CartPole\u2212v0, with a measured value of d int90 = 4. The study compares the intrinsic dimensionality of different environments in RL tasks using ES. Learning to walk in Humanoid-v1 is found to have a similar intrinsic dimensionality to MNIST on a fully-connected network, but less than a convnet trained on CIFAR-10. Hyperparameters for training RL tasks using ES are provided in Table S3. Training runs reach the threshold early at d=400. In training runs, the threshold is reached early at d=400, with median performance increasing steadily. Using a base convnet of approximately D=1M in the Pong-v0 environment, the agent moves the paddle UP or DOWN. The random subspace training procedure is scaled to large problems by mapping from R^d into a random d-dimensional subspace of R^D. This involves left-multiplying a parameter vector v in R^d by a random matrix M in R^Dxd, adding an offset vector \u03b80 in R^D. The random matrix M can be generated using a sparse approach, avoiding the need for dense matrices. This method allows for the columns to be approximately orthonormal, even with sparse matrices. The limitations of using dense matrices include scaling issues in terms of matrix-vector multiply time and storage, especially for high-dimensional models like LeNet applied to CIFAR-10. The method allows for very sparse random projections to construct a D \u00d7 d matrix with a density determined by probability. Implementing this procedure revealed an intrinsic dimension of d=2,500 for CIFAR-10 using LeNet. However, using Tensorflow's SparseTensor implementation did not achieve the expected time complexity improvement. Nonzero elements have a significant memory footprint, limiting scalability to larger problems. The Fastfood transform avoids the need to explicitly form and store the transformation matrix. The Fastfood transform (Le et al., 2013) efficiently computes a nonlinear, high-dimensional feature map \u03c6(x) for a vector x without explicitly forming and storing the transformation matrix. It involves implicitly generating a D \u00d7 d matrix with uncorrelated standard normal entries, using specialized methods for multiplication. This method utilizes Hadamard matrices and Gaussian vectors to behave like dense Gaussian matrices for efficient computation. In practice, a Hadamard matrix can be used for fast multiplication. Subspace training is more computationally expensive due to signal propagation through neural network layers and projections. Efforts have been made to reduce the extra computational cost, such as using sparse projections. The CIFAR-10 dataset was used to test different training methods. The CIFAR-10 dataset was used to test FC and LeNet architectures. For FC networks, intrinsic dimension changes with respect to a global baseline of 50% validation accuracy. Various FC networks share similar intrinsic dimensions to achieve the same task performance. For LeNet on CIFAR-10, the compression rate is 5%, which is 10 times larger than LeNet on MNIST. The CIFAR-10 dataset is more challenging than MNIST, with a baseline performance defined as 99% accuracy on MNIST and 58% accuracy on CIFAR-10. As the dimensionality increases in subspace training on CIFAR-10, overfitting occurs. ResNets are more parameter-efficient than LeNet, with a smaller structure reaching LeNet's baseline with lower dimensions. Subspace training acts as a regularizer in this context. Regularizers play a crucial role in subspace training by restricting the solution set. Comparing the effects of different regularizers on an FC network (L=2, W=200) using the CIFAR-10 dataset, weight decay and Dropout were studied. Varying amounts of weight decay showed a reduction in the performance gap between training and testing, eventually closing it at a penalty of 0.01. Subspace training demonstrated strong regularization, especially with smaller dimensions, leading to a smaller performance gap. Different dropout rates were also considered, showing varying effects on accuracy and negative log-likelihood. Various dropout rates from {0.5, 0.4, 0.3, 0.2, 0.1, 0} are considered in FIG1, showing that larger dropout rates reduce the gap between training and testing performance for both direct and subspace training methods. Subspace training acts as implicit regularization by restricting the solution set, outperforming the direct method when d is properly chosen. However, it can also overfit the training dataset when d is large. Weight decay enforces weights to concentrate around zeros, while subspace training directly reduces the number of parameters. The study focused on measuring the intrinsic dimension (d int90) for an ImageNet classification network using SqueezeNet with 1.24M parameters. Training for each intrinsic dimension took 6 to 7 days on 4 GPUs, with d int90 estimated to be over 500k. The learned d int90 can be used to evaluate neural network architectures for specific tasks. The study analyzed the contribution of components in convolutional networks for image classification. Local receptive fields and weight-tying are key aspects of convolutional networks. Control experiments were conducted using different variants of LeNet to investigate the impact of each component. In Keras, the LocallyConnected2D layer replaces the Conv2D layer. FCTied-LeNet uses the same set of filters for different patches of the input, breaking local connections. FC-LeNet mimics LeNet with fully connected layers. Experiments show different d int90 values for achieving accuracy thresholds on MNIST and CIFAR-10 datasets. Experiments show the importance of tied-weights and local connections in models for achieving accuracy thresholds on MNIST and CIFAR-10 datasets. Convnets are more efficient than FC nets due to local connectivity and weight tying. The minimum number of dimensions required to solve the problem is summarized in TAB7 and FIG2 for different neural network architectures. The intrinsic dimension of objective landscapes for various dataset and network combinations is illustrated in FIG2."
}