{
    "title": "Hygfmc5U-7",
    "content": "Recent research suggests that neural machine translation achieves parity with professional human translation on the WMT Chinese--English news translation task. Empirical testing with alternative evaluation protocols shows that human raters prefer human translation over machine translation when evaluating entire documents compared to isolated sentences. This highlights the importance of shifting towards document-level evaluation as machine translation improves. Neural machine translation has become the standard in machine translation, outperforming earlier approaches in various settings and tasks. Recent research suggests that neural machine translation has achieved human parity in some translation tasks, with claims supported by publicly released data and adherence to best practices in evaluation protocols. The study focuses on evaluating the quality of professional human translation versus machine translation, considering factors like test set selection and rater inconsistency. The results suggest that a statistical tie between machine translation systems is less concerning than a tie between machine translation and professional human translation. The evaluation protocol is examined to understand why human raters may not detect quality differences between the two. The study compares professional human translation with machine translation, focusing on evaluation protocols and rater sensitivity. It is noted that the evaluation is different from previous studies, aiming to indirectly assess the accuracy of different evaluation methods. The underlying assumption is that professional human translation is superior to neural machine translation, but the sensitivity of human raters to quality differences depends on the evaluation protocol used. Machine translation evaluation focuses on adequacy and target language fluency. Controversy surrounds evaluation protocols, with key aspects being granularity of measurement, raters, and experimental unit. Ranking translations leads to better agreement than absolute judgement on Likert scales. Continuous scales for direct assessment of translation quality have been suggested by Graham et al. (2013), using a slider between 0 (Not at all) and 100 (Perfectly). This method yields scores on a 100-point interval scale in practice, with ratings standardized for increased homogeneity. Source-based direct assessment is used by Hassan et al. (2018) to avoid bias towards reference translations. In shared task evaluations, raters are asked to assess how accurately candidate texts convey the semantics of the source text, with translations by humans and machines rated independently. Machine translation quality is often assessed through crowdsourcing, which has been found to be more reliable than automatic metrics. However, the effectiveness of crowdsourcing for evaluating translations produced by Neural Machine Translation (NMT) systems is uncertain due to increased fluency and difficulty in identifying errors. Expert translators are highlighted as important for machine translation evaluation. Machine translation quality is often evaluated through crowdsourcing, with raters assessing single sentences for cost and experimental validity reasons. While machine translation systems operate at the sentence level, human translators and expert raters consider document-level context for better evaluation. In a quality evaluation experiment, the impact of source text availability and experimental unit on ratings by professional translators is tested. Raters judge translations in pairwise ranking, comparing human and machine translations. The evaluation includes human translation, making it reference-free. Two conditions are evaluated: adequacy, where raters see source texts and translations. Professional translators with at least three years of experience evaluate translations in pairwise ranking, considering source texts and fluency. Context is tested by evaluating entire documents and single sentences randomly. Quality control includes converting some items into spam to prevent random ratings. Statistical analysis is conducted to determine preference for human translations. Statistical analysis was conducted to test the preference for human translations over machine translations. The study involved evaluating Chinese to English translations of news articles using a sample of 55 documents and 2\u00d7120 sentences from the WMT 2017 test set. Professional translators rated the documents and sentences, with human translations labeled as REFERENCE-HT and machine translations as COMBO-6. Professional translators were recruited to rate sentences for adequacy and fluency. Translators had an average of 13.7 years of experience and received payment for rating documents and sentences. Some raters were excluded from analysis due to poor performance or overlap with annotated documents. Experimental data is publicly available for validation. In the adequacy condition, human and machine translations were not significantly different. In the adequacy condition, human and machine translations were not significantly different at the sentence level, but raters preferred human translations at the document level. In the fluency condition, raters preferred human translations at both sentence and document levels. The study found that inter-annotator agreement ranged from 0.13 to 0.32, emphasizing the importance of suprasentential context in evaluating machine translation. Despite using professional translators and pairwise ranking, raters still struggled to differentiate between human and machine translations at the sentence level. The study found that there was no clear preference between human and machine translations at the sentence level. However, there was a significant preference for human translations when evaluating entire documents, revealing errors that are hard to spot at the sentence level. An example of lexical coherence was found in an article about a new app \"\u5fae\u4fe1\u632a\u8f66\", consistently translated as \"WeChat Move the Car\" by human translators. The article discusses the translation of the app \"\u5fae\u4fe1\u632a\u8f66\" as \"WeChat Move the Car\" by human translators. Different translations were found in machine translation, raising questions about fluency and adequacy. Fluency raters preferred human translations, suggesting a potential strength of machine translation in fluency over adequacy. In a study comparing human and machine translation, machine translation (MT) is favored by raters in bilingual conditions due to L1 interference. Document-level context significantly impacts fluency assessment. Human translations are preferred over machine translations at the document level, indicating a potential quality gap. This suggests a need for improvement in machine translation practices. The study highlights the need to shift towards document-level evaluation in machine translation. Using professional translators and pairwise ranking, the evaluation protocol aims for maximal validity. Future work could explore eliciting accurate quality judgments on the document-level through crowdsourcing or alternative evaluation protocols. The data released by Hassan et al. (2018) could be used for this purpose. The study emphasizes the importance of document-level evaluation in machine translation. Future research should focus on creating document-level training data, developing appropriate models, and using discourse-aware automatic metrics to narrow the quality gap between machine and human translation. TAB1 displays detailed results for all experimental conditions, while TAB3 shows interrater agreement. Cohen's kappa coefficient is calculated to measure inter-rater agreement in pairwise rankings of machine translation outputs. Lower agreement is observed in three out of four conditions, attributed to high translation quality and lack of guidelines for error severity interpretation among raters. In document-level adequacy and fluency assessments, raters handle ties differently, leading to low inter-annotator agreement. Quality control measures show careful assessment by raters, with minimal misses in marking spam items. Aggregating ratings from different annotators is a common practice in situations of low interrater agreement."
}