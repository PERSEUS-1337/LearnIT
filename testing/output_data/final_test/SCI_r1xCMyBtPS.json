{
    "title": "r1xCMyBtPS",
    "content": "We propose procedures for evaluating and strengthening contextual embedding alignment in multilingual BERT. After our alignment procedure, BERT shows improved zero-shot performance on XNLI, matching pseudo-fully-supervised translate-train models for Bulgarian and Greek. We introduce a contextual word retrieval task to measure alignment, which correlates with downstream zero-shot transfer. Analysis reveals systematic deficiencies in BERT, such as worse alignment for open-class parts-of-speech and word pairs in different scripts, corrected by the alignment procedure. These results support contextual alignment for understanding large multilingual pre-trained models. Large multilingual pre-trained models like BERT show improved alignment after a proposed procedure, enhancing cross-lingual transfer. Contextual alignment is crucial for understanding these models, which have largely replaced word vectors due to their context-dependent nature. Multilingual BERT, pre-trained on 104 languages, shows significant cross-lingual transfer capabilities. It achieves high accuracy on downstream tasks even in languages with different scripts, demonstrating the effectiveness of contextual embedding alignment. In this paper, the authors explore how multilingual BERT achieves high zero-shot performance and propose using contextual embedding alignment to improve cross-lingual transfer capabilities. The authors propose a fine-tuning-based alignment procedure using parallel data from the Europarl corpus to improve multilingual BERT's performance. This alignment significantly enhances zero-shot transfer accuracy, matching translate-train models for certain languages. The authors propose a fine-tuning-based alignment procedure using parallel data from the Europarl corpus to improve multilingual BERT's performance. They compare the performance of fine-tuned BERT, fastText, and rotation-aligned BERT in different scenarios, showing that fine-tuned BERT outperforms fastText when there are multiple occurrences per word, while matching its performance when there is only one occurrence per word. The study compares the performance of fine-tuned BERT, fastText, and rotation-aligned BERT in various scenarios. Fine-tuned BERT aligns well at the type level without the need for context disambiguation, showing its effectiveness. Additionally, base BERT struggles with aligning open-class parts-of-speech and word pairs with usage frequency differences, highlighting areas for improvement in pre-training procedures. This research emphasizes the importance of contextual alignment in understanding large multilingual pre-trained models. One popular method for aligning word vectors across languages involves learning a mapping from source language vectors to target language vectors using a bilingual dictionary as supervision. The optimal mapping can be solved in closed form when constrained to be an orthogonal linear transformation. Alignment is evaluated using bilingual lexicon induction, with proposed solutions to mitigate the hubness problem in nearest neighbors. Recent works have shown that this mapping can be learned with minimal supervision. Incorporating context into alignment is a key challenge in making alignment context aware. Past studies have addressed this by aligning the \"average sense\" of a word, removing context. For example, Schuster et al. (2019) aligned contextual ELMo embeddings by learning a rotation to improve multilingual dependency parsing. Aldarmaki & Diab (2019) learned a rotation on sentence vectors by averaging word vectors. In this paper, the authors align word and context using more expressive methods than rotation, departing from previous works. They incorporate parallel texts into pre-training and propose contextual pre-training procedures that are cross-lingually aware. The authors propose a method that aligns existing pre-trained models for cross-lingual transfer, using less parallel data compared to previous approaches. Pires et al. analyze multilingual BERT and find that transfer is possible between languages, especially those that are typologically similar. They conclude that BERT is multilingual but may not perform well for certain language pairs. The model is trained on 104 languages using masked word prediction and next sentence prediction tasks. Contextual alignment is defined as having similar representations for word pairs in parallel sentences. The contextual alignment of a pre-trained model is measured using a parallel corpus to find translations of words in different languages. This involves using a nearest neighbor retrieval function to determine accuracy in contextual word retrieval. The model is trained on multiple languages for masked word prediction and next sentence prediction tasks. The procedure involves using a modified cosine similarity function called CSLS to find translations of words in different languages. The method can be adjusted based on the corpus context, and word pairs can be obtained unsupervisedly from parallel data using standard techniques. Noise can be reduced by running the algorithm in both source-target and target-source directions and keeping only the intersecting word pairs. To improve model alignment with the corpus, a loss function sums similarities between word pairs. Using squared error loss instead of CSLS metric, a regularization term penalizes target language embeddings deviating from initialization. Minimizing L(f ; C) while maintaining f's usefulness is key, achieved by fine-tuning on downstream tasks. In experiments, language embeddings are aligned towards target embeddings with gradient steps on weights, preventing drift. In multilingual cases, batches from parallel corpora are used to move non-English embeddings towards English, departing from prior rotation methods. The Procrustes problem involves minimizing distance between word pairs in embeddings by rotating vectors, preserving semantic information. However, contextual pre-trained models may not be isometric, making independent alignment per language pair inferior to joint training. Our method aligns all languages simultaneously using Europarl corpora for English. Our method aligns all languages simultaneously using Europarl corpora for English, producing word pairs and dividing the dataset into test, development, and training sets. Test set accuracy calculation is modified to exclude word pairs seen in the training set, and exact matches are removed. Results are reported for different amounts of parallel sentences to address limited data for low-resource language pairs. The study compares BERT to a sentence-augmented fastText baseline using word vector alignment methods. Sentence vectors are computed by concatenating average, maximum, and minimum word vectors in the sentence. Different methods are experimented with for word and sentence retrieval. The study compares BERT to fastText using word vector alignment methods. FastText vectors are 1200-dimensional, while BERT vectors are 768-dimensional. The next step is to improve cross-lingual transfer. XNLI dataset is used for the downstream task, predicting entailment, neutral, or contradiction between sentences in multiple languages. Training on English and evaluating on Bulgarian, German, Greek, Spanish, and French. Architecture involves a linear layer followed by softmax. The architecture involves a linear layer followed by softmax on the [CLS] embedding of the sentence pair, producing scores for three labels. The model is trained using cross-entropy loss and compared to fully-supervised models trained and tested on the same language. Additionally, rotation-based methods and the current state-of-the-art zero-shot by XLM are included in the comparison. The current state-of-the-art zero-shot accuracy achieved by XLM is improved through alignment procedures, with Bulgarian and Greek matching the translate-train ceiling. Alignment greatly enhances multilingual BERT models, boosting accuracies for all languages by at least 1%. Bulgarian and Greek see a significant increase of almost 5% each, aligning with translate-train numbers. The alignment procedure significantly improves multilingual BERT models, especially for languages initially difficult for BERT. Rotation-based methods show small gains for some languages but are sub-optimal overall. The alignment method enhances transfer with as little as 10K sentences per language. The alignment method significantly enhances transfer for low-resource languages with as little as 10K sentences per language. Fine-tuned BERT outperforms fastText in contextual retrieval, matching the type-level alignment of fastText while also aligning context. In contextual retrieval, fine-tuned BERT outperforms fastText and rotation-aligned BERT. The alignment procedure enhances transfer for low-resource languages. Before alignment, BERT's performance varies between languages, but after alignment, it is consistently effective. The alignment procedure boosts transfer for low-resource languages, as shown in XNLI numbers. Word retrieval accuracies correlate with zero-shot performance, supporting the evaluation measure's predictive ability. Pires et al. (2019) suggest that shared vocabulary forces shared words to have the same representation, impacting cross-lingual transfer. The alignment procedure enhances transfer for low-resource languages, as evidenced by XNLI results. Word retrieval accuracies align with zero-shot performance, indicating the measure's predictive power. The study confirms that shared vocabulary leads to shared word representations, influencing cross-lingual transfer. The accuracy of base BERT decreases for larger differences between source and target, indicating alignment dependency on word pairs with similar usage statistics. Multilingual BERT shows high alignment for groups with high lexical overlap like numerals, punctuation, and proper nouns due to shared vocabulary. Parts-of-speech are divided into closed-class and open-class categories, with closed-class parts serving grammatical functions and open-class parts consisting of lexical words. The accuracy of base BERT varies for closed-class and open-class parts-of-speech, but alignment equalizes the discrepancy. BERT's alignment is influenced by the similarity of word usage statistics, explaining the difference between closed-class and open-class words. Closed-class words are grammatical and used similarly across languages, while open-class words are lexical and can be substituted. The usage statistics of closed-class and open-class words differ in BERT alignment. Closed-class words have similar usage across languages, while open-class words can be substituted. Alignment equalizes discrepancies in word usage statistics. The alignment method improves multilingual BERT by correcting systematic deficiencies and providing new insights into the pre-training procedure. Zero-shot accuracy on the XNLI test set confirms the effectiveness of the alignment method for distant languages and various parallel corpora. For alignment and XNLI optimization, a learning rate of 5 \u00d7 10 \u22125 with Adam hyperparameters is used. The model is trained for one epoch for alignment and 3 epochs for XNLI. Different corpora are used for various languages, confirming the effectiveness of the alignment method. The alignment method works effectively for various languages and corpora, including the Tanzil corpus with sentences from the Quran. Aligned BERT can disambiguate between different meanings of a word, as shown with examples from the English-German Europarl test set. The speaker supports the call for the arms embargo to remain and mentions the Commission and the Council. The national political elite had to make a detour in Ambon to reach the civil governor's residence by warship. The European Union's interest in being surrounded by neighboring countries is highlighted. The European Union's limited tools to achieve stability in neighboring regions are noted. Expectations for the new Indonesian government include restoring public order, prosecuting those responsible for violence, and engaging in political dialogue with the opposition. Army reform and a stable system of law and order are also highlighted. The need for army reform and a stable system of law and order is emphasized. Financial support is required for poor countries to participate in court activities. Support is needed to implement an action plan for organizations. A resolution condemning the conditions of prisoners and civilians in Djibouti is urged. The resolution condemns the conditions of prisoners and civilians in Djibouti, criticizing the allocation of aid to large farms and discussing the implications of shutting down companies associated with the arms industry. The resolution criticizes the allocation of aid to large farms and discusses the implications of shutting down companies associated with the arms industry in Djibouti and Ireland. If all American and Japanese software companies on the Irish island were closed, it would have catastrophic consequences."
}