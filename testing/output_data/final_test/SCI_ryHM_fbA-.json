{
    "title": "ryHM_fbA-",
    "content": "This paper introduces a new document embedding model based on convolutional neural networks, which allows for deeper architectures that are fully parallelizable. The model demonstrates superior results on two benchmarks and will release full code with the final paper. The approach aims to create fixed-length vector representations preserving semantic information for various NLP tasks. The text discusses the use of embeddings for NLP tasks like sentiment analysis and information retrieval. Popular approaches include doc2vec and skip-thought models. Doc2vec extends word2vec by adding document-specific vectors, but requires iterative optimization for each new document. These models are trained using small localized contexts. The text discusses the limitations of using small localized contexts in document embeddings and introduces the skip-thought model, which uses a recurrent neural network to capture long-range semantic relationships in the entire document. However, the sequential nature of RNNs can slow down training and inference processes. The text introduces a new embedding model that addresses the limitations of RNN models like skip-thought. It proposes a CNN embedding model that leverages recent advances in language modeling to capture long-range relationships in documents efficiently. The CNN architecture allows for parallel processing of the entire document, accelerating learning and inference. Variable length input problem can be addressed with padding or global pooling in the last convolutional layer. Deeper architectures capture longer range dependencies in the document. Bag-of-words (BOW) models are popular but suffer from high dimensionality and loss of word order. The limitations of Bag-of-Words (BOW) models are evident in their destruction of word order and semantic structure, leading to the proposal of alternative models like LSA, pLSA, and LDA which produce more effective embeddings. However, these models still rely on the BOW representation of documents, limiting their representational power. Distributed language approaches have gained popularity for their ability to capture more complex semantic structures. Recently, distributed language approaches have become popular, representing words with vectors concatenated into documents. Embedding models are trained on these sequences to create compact representations preserving semantic structure. This solves the word order issue of BOW models and avoids dimensionality explosion. Models like doc2vec, skip-thought, and others achieve state-of-the-art results with embeddings of a few hundred dimensions. However, doc2vec requires iterative optimization during inference, making deployment challenging. Skip-thought and other RNN approaches avoid this issue, but most RNN architectures use gating mechanisms. RNN architectures commonly use gating mechanisms that prioritize later words in documents, making them inefficient for long texts. The goal is to learn an embedding function that generates compact summaries of each document's semantic aspects for NLP tasks. Training for this function is unsupervised, with each document containing a sequence of words mapped to vectors. The model aims to map words to vector representations and learn a fixed-length vector for each document. Inspired by language modeling, the model factors the probability of a document into conditional probabilities using the chain rule. The model factors the probability of a document into conditional probabilities using the chain rule and is trained to predict the next word given a subsequence. There is a close relationship between language modeling and document embedding, with recent advances in language modeling forming the basis for exploring this relationship. Recently, Dauphin et al. proposed a CNN-based language model that outperforms RNN models in efficiency and accuracy by utilizing deep CNN architectures with up to 14 layers. This suggests that CNN models are a more effective and efficient alternative to RNNs for language tasks. Recent work by BID28 and BID13 simplified language modeling by reusing word representations as weights in the softmax layer, reducing parameters by 30% while maintaining performance on benchmarks. Recent work by BID28 and BID13 simplified language modeling by reusing word representations as weights in the softmax layer, reducing parameters by 30% while maintaining performance on benchmarks. This work proposes a CNN model for document embedding, aiming to predict multiple words forward to capture long-range semantic structure in the document. The model aims to capture long-range semantic structure in the document by using a simplified softmax formulation. It frames the problem as binary classification and focuses on raising the probability of words that immediately follow a given word while lowering it for others. This approach is similar to the negative sampling skip-gram model in word2vec/doc2vec, with differences in forward prediction and the use of embeddings based on all words up to a certain point. Our model simplifies inference by using a CNN for forward pass through f, with multiple convolution layers applied to \u03c6(D) to capture long-range dependencies in the document. Gated linear units (GLUs) are used for learning deeper models. The model uses Gated Linear Units (GLUs) for learning deeper models with activation functions to ensure gradient flow. Traditional CNN models are not designed for variable length input, so two approaches are investigated to address this issue. One approach is to apply zero padding to convert the input into fixed length sequences. The model utilizes Gated Linear Units (GLUs) for deep learning with activation functions to maintain gradient flow. Two approaches are explored to handle variable length input in CNN models. One method involves applying zero padding to convert input into fixed length sequences. Another approach involves using convolutional layers for variable length input, with an aggregating function applied before passing activations to fully connected layers. The model utilizes Gated Linear Units (GLUs) for deep learning with activation functions to maintain gradient flow. To handle variable length input in CNN models, an aggregating function like mean or max pooling can be applied to convert the matrix to a fixed length output. This eliminates the need for input padding/truncation, making the model more flexible and efficient. Training involves learning f by minimizing the loss in Equation 4, with stochastic sampling yielding better results than fixing prediction points for each document. The learning algorithm utilizes stochastic sampling for efficient training. It involves alternating between sampling documents and prediction points, updating the model using gradients. To speed up learning, mini batches are used, and gradients are averaged. Negative word samples are used to address the computational cost of computing a summation over all words. Using small samples of 50 words has been found to yield good results on all datasets. The learning algorithm is simple and requires tuning only two parameters. No sentence tokenizer is needed. Experiments were conducted on IMDB and Amazon datasets using TensorFlow. Word embeddings initialized with word2vec vectors showed faster learning and better performance. The model architecture for the experiments includes a six layer CNN with 600 kernels per layer, residual connections, GLU activations, and batch normalization. The code for the full model will be released with the final draft of the paper. The experiments address the variable length input problem using padding and global pooling approaches. Max pooling is found to produce better results than average pooling. Embeddings from all models are evaluated by training a classifier and test set classification accuracy is reported. Inference speed in tokens per second for uni-directional and bi-directional skip-thought models is shown in TAB1. The experiments compared the inference speed of CNN, uni-directional skip-thought, and bi-directional skip-thought models. CNN was found to be over 10x faster than uni-directional skip-thought and over 20x faster than bi-directional skip-thought. The IMDB dataset consists of 50,000 movie reviews split evenly into training and test sets with binarized sentiment labels. The dataset includes labeled reviews with scores \u2264 4 considered negative and scores \u2265 7 considered positive. There are also 50,000 unlabeled reviews for unsupervised training. The average review is about 230 words long, with input lengths experimented at k = [400, 500, 600]. Setting k = 400 yielded good results, with over 90% of words matched to word2vec vectors. The goal is to evaluate the CNN model's ability to capture semantic aspects of documents using both labeled and unlabeled training reviews. The training process involves using mini batch gradient descent with a batch size of 100 and Adam optimizer. Predicting h = 10 words forward with an offset of \u03b4 = 10 and 50 negative words produced good results. The model is compared to word2vec, skip-thought, and doc2vec baselines. The training process involves mini batch gradient descent with a batch size of 100 and Adam optimizer. Predicting h = 10 words forward with an offset of \u03b4 = 10 and 50 negative words produced good results. The model is compared to word2vec, skip-thought, and doc2vec baselines. Additionally, larger embeddings of size 2400 are used in the combined model, which includes both unidirectional and bidirectional encoders. A doc2vec model is also trained with an embedding size of 300 using distributed bag-of-words and distributed memory methods. The final embeddings have a dimension of 600, twice the size of the model, and are initialized with pre-trained word2vec. Results from experiments using pre-trained word2vec as initialization are shown in TAB3. Our approach outperforms word2vec and skip-thought baselines, performing comparably to doc2vec. CNN-pool shows similar performance to CNN-pad, indicating max pooling effectively captures semantic information. The model can generate embeddings for new documents quickly and deterministically. The Amazon Fine Food Reviews dataset consists of 568,454 reviews with ratings from 1 to 5. To address the dataset's imbalance, a split was performed with 16,000 documents for training and 4,000 for testing. The CNN model successfully modeled sequences of over 2K words. Our model was trained using the same architecture and training method as in IMDB experiments, with a slight adjustment of setting k = 200 for CNN-pad due to shorter reviews in AFFR dataset. We compared our approach against word2vec, skip-thought-2400, skip-thought-600, and doc2vec-600 baselines, achieving competitive performance in binary and 5-class sentiment classification tasks. Our method showed promising results in the classification task, indicating its effectiveness. Our method demonstrates competitive performance in learning fine-grained sentiment differences from unlabeled text, supported by robust embeddings that generalize well on NLP tasks. Retrieval results on the AFFR dataset show the effectiveness of our CNN architecture and learning framework in producing quality embeddings for information retrieval. Using the AFFR dataset, reviews are selected as queries to retrieve top-3 most similar results based on embedding cosine distance. Results show high relevance in content and sentiment to each query. Different groups express dissatisfaction with seafood, drinks, and cat food products, with consistent sentiment across varying products. The model successfully encodes topic and sentiment into embeddings without supervision. The CNN model for document embedding uses t-SNE to visualize sentiment separation in IMDB test set embeddings. The model encodes sentiment information, making classes linearly separable. The model is a prediction learning algorithm trained to predict successive words for randomly chosen subsequences within a document. It has few hyperparameters to tune, is straightforward to implement, takes full advantage of parallel execution, and achieves better performance and speed compared to current RNN models."
}