{
    "title": "rygfnn4twS",
    "content": "Network quantization is a hardware-friendly technique for deploying CNNs on low-power mobile devices. Recent techniques quantize weight kernels independently in convolutional layers for higher accuracy. The quantization bitwidth directly affects accuracy, latency, energy, and hardware overhead. Prior works use only one bitwidth for each layer or the entire CNN due to the complexity of searching for different bitwidths for each weight kernel. In this paper, a hierarchical-DRL-based kernel-wise network quantization technique, AutoQ, is proposed to automatically search for a QBN for each weight kernel and choose another QBN for each activation layer. Compared to state-of-the-art DRL-based schemes, models quantized by AutoQ reduce inference latency by 54.06% and decrease energy consumption by 50.69% while maintaining the same accuracy. Deploying CNNs on mobile devices with limited hardware resources is challenging. Deploying CNNs on mobile devices with limited hardware resources is challenging due to their high computing overhead. Various approaches like pruning and network quantization have been proposed to reduce this overhead. Network quantization, specifically layer-wise quantization, assigns a quantization bit number to the weights and activations of each convolutional layer, leading to more hardware-friendly CNN acceleration techniques. Recent works have found that different weight kernels in a convolutional layer exhibit varying variances, leading to different amounts of redundancy. To address this, they propose quantizing each weight kernel independently by calculating a scaling factor vector for each kernel, rather than globally quantizing all kernels of a layer together. This approach aims to improve accuracy by reducing redundancy among different weight kernels. Recent works propose quantizing each weight kernel independently in a CNN by assigning a Quantization Bit Number (QBN) to each kernel. This kernel-wise quantization approach improves inference accuracy by around 2% compared to layer-wise quantization, with the same computing overhead. Choosing the right QBN for each weight kernel is crucial for accuracy, latency, and hardware overhead. Hand-crafted heuristics for determining QBNs are complex due to their significant impact on model performance. Determining a QBN for each weight kernel via hand-crafted heuristics is complex, leading to sub-optimal results. Recent works propose kernel-wise network quantization to enable efficient deployment of deep CNNs on mobile devices. This approach assigns a QBN to each weight kernel and activation layer, reducing inference computing overhead. AutoQ is a hierarchical-DRL-based agent proposed to automate the search for a QBN for each weight kernel and activation layer in a CNN. It consists of a high-level controller (HLC) and a low-level controller (LLC) to efficiently perform kernel-wise network quantization. The HLC selects a QBN for each activation layer and sets a goal for each layer, while the LLC quantizes each weight kernel based on the goal. This approach aims to reduce the time taken for a DRL agent to find suitable QBNs, especially as CNN architectures grow deeper. AutoQ is a hierarchical-DRL-based agent that automates the search for a QBN for each weight kernel and activation layer in a CNN. It penalizes inference accuracy loss while rewarding a smaller QBN. The extrinsic reward considers inference latency, energy consumption, and hardware cost. Recent works quantize real-valued weights and activations to fixed-point representations to reduce model size and computing overhead. Tang et al. (2017) and Rastegari et al. (2016) quantize weights and activations into multi-bit binary codes for inferences using XNORs and popcounts. Sasaki et al. (2019) quantize each weight kernel independently for higher accuracy in CNNs. Kernel-wise network quantization techniques search for a QBN for each kernel and activation layer, but face a large search space size. AutoQ is the first work to automatically quantize each weight kernel and activation layer of a pre-trained CNN model for mobile devices using hierarchical DRL. It outperforms human-designed neural networks and prior DRL-based techniques for quantization and pruning. AutoQ is a hierarchical DRL technique that automatically quantizes weights in a kernel-wise manner and activations in a layer-wise fashion for mobile devices. It considers inference latency, energy, and FPGA area overhead for low-cost devices. Future quantization techniques can be integrated to improve inference accuracy. AutoQ is a hierarchical DRL technique that automatically quantizes weights and activations for mobile devices, considering inference latency, energy, and FPGA area overhead. It consists of a high-level controller (HLC) and a low-level controller (LLC, which quantizes the network layer by layer and searches for a QBN for each weight kernel in a layer. The HLC sets goals for QBNs in the activation layer, while the LLC quantizes weight kernels. The environment provides quantization and hardware details to estimate energy consumption, area overhead, and latency. Extrinsic rewards are generated to evaluate LLC actions, while intrinsic rewards assess goal achievement by the HLC. The HLC provides intrinsic rewards to evaluate goal implementation by the LLC. State space is defined by layer index, weight kernel index, input channels, kernels, kernel size, stride, feature map size, convolution type, weight/activation, goal, and action. Variables are normalized to [0, 1]. The HLC sets goals for QBNs in activation layers, while the LLC quantizes weight kernels. The LLC generates a QBN for each weight kernel in a layer as an action, using a continuous space to capture the relative order. Extrinsic rewards are provided after an action is taken, leading to a new state. The HLC aims to maximize the accumulative extrinsic reward by considering factors such as network configuration, hardware configuration, accuracy, latency, energy, and FPGA area. User-defined factors determine the impact of these factors on the extrinsic reward, allowing for resource-constrained and accuracy-guaranteed searches. AutoQ implements resource-constrained and accuracy-guaranteed searches by adjusting parameters like latency, energy, and FPGA area. It limits the action space for lower QBNs and adjusts settings for different applications like low-power drones and fingerprint locks. The LLC produces actions to maximize extrinsic rewards in these scenarios. The LLC aims to maximize extrinsic rewards by completing goals efficiently. To address low extrinsic rewards at the start of AutoQ training, a shaped intrinsic reward is proposed to consider goal completion and extrinsic rewards, enabling fine-grained behavior learning. The intrinsic reward is represented by a user-defined factor \u03b6, which increases from 0.1 to 0.8 with training epochs. A hardware overhead estimator estimates latency and energy for FPGA accelerators. AutoQ utilizes fast and accurate FPGA latency, area, and power models to predict inference latency, energy, and FPGA area for different network and hardware configurations. Machine-learning-based models can estimate hardware overhead within milliseconds. AutoQ employs Hierarchical Reinforcement Learning with Off-policy correction to implement Hierarchical and Low-Level Controllers, aiming to minimize error in the low-level Q-value function. AutoQ utilizes Hierarchical Reinforcement Learning with Off-policy correction to minimize error in the low-level Q-value function. It stores state-goal-reward transitions in a replay buffer and introduces a correction to align old transitions with the current controller. The model is fine-tuned for accuracy using stochastic gradient descent. AutoQ utilizes Hierarchical Reinforcement Learning with Off-policy correction to minimize error in the low-level Q-value function. The AutoQ agent consists of an actor network and a critic network with two hidden layers each. The model is quantized with the best policy found by AutoQ and fine-tuned on the full dataset. The actor network has a sigmoid function producing an output in the range of [0, 1]. AutoQ trains the networks with a batch size of 64 and a replay buffer size of 2000. Storage Cost: A 4-bit QBN ranging from 0 to 8 needs to be recorded for each category. AutoQ utilizes Hierarchical Reinforcement Learning with Off-policy correction to minimize error in the low-level Q-value function. The storage overhead of AutoQ is \u223c 0.1% of the size of various CNN models, with ResNet-18 requiring 8.3MB to store its quantized model. Several CNN models were evaluated on ImageNet for inference performance, energy consumption, and FPGA area when quantized by AutoQ on a Xilinx Zynq-7020 embedded FPGA. AutoQ utilizes Hierarchical Reinforcement Learning with Off-policy correction to minimize error in the low-level Q-value function. The accelerator Umuroglu et al. (2019b) uses bit-serial multipliers to compute with one-bit digits from multiple weights and activations in parallel. AutoQ performs resource-constrained searches by imposing a latency constraint and comparing kernel-wise quantized models against other quantization methods. The network-wise scheme uses 4-bit quantization for the entire model. AutoQ improves inference accuracy by > 1.25% compared to layer-wise quantization, maintaining similar latency. It quantizes weights and activations separately, enhancing performance over 16-bit models. The average QBN of weights and activations is calculated, showing the effectiveness of AutoQ in model optimization. AutoQ quantizes models to reduce inference latency significantly while maintaining accuracy. It assigns QBN to each kernel for optimization, achieving better performance compared to 16-bit models. SqueezeNetV1 experiences larger accuracy degradation in AutoQ's accuracy-guaranteed search. AutoQ searches for optimal QBN values for each weight kernel in a convolutional layer, resulting in similar accuracy to network-wise or layer-wise quantization techniques but with smaller QBN values. It automatically identifies weight kernels with less redundancy and assigns appropriate QBN values. AutoQ assigns QBN values to weight kernels based on their variance, achieving >70% accuracy in kernel-wise quantization. Compared to traditional techniques like HAQ, AutoQ supports accuracy-guaranteed searches through hierarchical-DRL-based optimization. AutoQ utilizes a hierarchical-DRL approach to accelerate kernel-wise network quantization, achieving >70% accuracy in just 200 episodes. In contrast, the DDPG-based DRL struggles with 20% accuracy until 250 episodes. AutoQ introduces a novel intrinsic reward system to expedite the search process, outperforming traditional methods like HIRO by reaching 60% accuracy in fewer episodes. AutoQ uses extrinsic rewards to balance accuracy, latency, energy consumption, and FPGA area during an accuracy-guaranteed search. It can identify the best configuration by considering all factors, unlike traditional methods like HAQ. Kernel-wise quantized models, identified by AutoQ, can significantly reduce inference latency and energy consumption on spatial CNN accelerators compared to layer-wise quantized models. This approach involves dividing weight kernels into subkernels and quantizing each independently, resulting in a 39.04% reduction in latency and a 33.34% decrease in energy consumption. In this paper, a hierarchical-DRL-based kernel-wise network quantization technique, AutoQ, is proposed to reduce inference latency and energy consumption on spatial CNN accelerators. The approach involves dividing weight kernels into subkernels and quantizing each independently, showing that fine-grained quantization schemes may not further improve latency or energy efficiency on CNN accelerators. AutoQ is a kernel-wise network quantization technique that consists of a HLC and a LLC. The HLC searches for average weight QBN and activation QBN for each convolutional layer, while the LLC generates a QBN for each weight kernel. It uses intrinsic and extrinsic rewards to learn efficiently and balance inference accuracy, latency, energy consumption, and FPGA area. AutoQ reduces inference latency by 54.06% and decreases energy consumption by 50.69% while maintaining the same accuracy compared to DRL-based schemes."
}