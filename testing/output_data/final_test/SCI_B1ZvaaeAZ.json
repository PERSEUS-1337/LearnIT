{
    "title": "B1ZvaaeAZ",
    "content": "Reducing precision of activations in deep neural networks can help reduce memory footprint during training and inference. Training networks from scratch with reduced-precision activations can maintain or even improve model accuracy. By decreasing activation map precision and increasing filter maps, efficiency in execution can be significantly enhanced. Our scheme WRPN, wide reduced-precision networks, aims to reduce memory and computational energy requirements while speeding up training and inference processes. It outperforms previously reported accuracies on the ILSVRC-12 dataset and is computationally less expensive. Low-precision algorithms can lower compute and memory requirements for deep learning workloads, with many works proposing low-precision DNNs, even down to 1-bit binary mode. Our scheme WRPN, wide reduced-precision networks, aims to reduce memory and computational energy requirements while speeding up training and inference processes. It outperforms previously reported accuracies on the ILSVRC-12 dataset and is computationally less expensive. Low-precision algorithms can lower compute and memory requirements for deep learning workloads, with many works proposing low-precision DNNs, even down to 1-bit binary mode. Majority of existing works in low-precision DNNs sacrifice accuracy over full-precision networks. We study schemes for training and inference using low-precision DNNs where we reduce the precision of activation maps as well as the model parameters without sacrificing network accuracy. Wide reduced-precision networks (WRPN) increase filter maps in layers to improve accuracy compared to full-precision networks. Despite more compute operations, WRPN is more efficient, offering better accuracies with lower computational costs. Results on various models show that 4-bits precision is sufficient for training deep and wide models. With 4-bit activation and 2-bit weights, accuracy is comparable to full-precision networks. Increasing network width and using 1-bit precision closes the accuracy gap with state-of-the-art results for ResNet-34 and AlexNet. Reported accuracies with binary networks and 4-bit precision are the highest to date. Reduced-precision quantization scheme is hardware-friendly for efficient implementations on Titan X GPU, Arria-10 FPGA, and ASIC, delivering significant efficiency gains over FP32 operations. Reduced-precision networks with low-precision weights can deliver efficiency gains over FP32 operations. Activation maps occupy a larger memory footprint with mini-batches of inputs during training and inference steps. Memory requirements increase as batch size changes for different networks. During training, memory is allocated for activation maps and weight tensors in device memory for forward and backward passes. Inference phase allocates memory for input and output feature maps for each layer, with reuse across layers. Memory requirements vary based on batch size and network architecture. During training, memory is allocated for activation maps and weight tensors in device memory for forward and backward passes. Inference phase allocates memory for input and output feature maps for each layer, with reuse across layers. Memory requirements vary based on batch size and network architecture. The total memory allocation during inference is the maximum of IFM and OFM required across all layers plus the sum of all W-tensors. Activations start to occupy more than 98% of total memory footprint during training at batch sizes 128 and above. Reducing precision of activations and weights can reduce memory footprint, bandwidth, and storage, simplifying hardware requirements. By reducing the precision of activations, training and inference steps can be sped up, and memory requirements can be cut down. However, a straightforward reduction in precision of activation maps can lead to a significant decrease in model accuracy. Reducing precision of activation maps and model weights for AlexNet using TTQ technique can lower accuracy, with activations being more sensitive than weights. TTQ allows for 2b precision on weights without accuracy loss. Reducing precision of activation maps and model weights for AlexNet using TTQ technique can lower accuracy. Widening filter maps in a layer can help regain model accuracy with reduced-precision operands, reducing memory requirements, bandwidth, and computational energy. Inspired by Wide ResNet BID26, our approach maintains network depth while increasing layer width. Our approach, WRPN, widens filter maps in a layer while maintaining network depth. By reducing precision and widening filters, the total compute cost remains at-par with baseline networks. Doubling the number of filter maps in AlexNet with reduced precision shows accuracy comparable to full-precision networks. Operating with 4-bits weights and activations surpasses baseline accuracy by 1.44%, and using binary weights and activations improves accuracy by 4% compared to XNOR-NET BID16. When doubling the number of filter maps in AlexNet with reduced precision, the overall compute complexity is significantly lower compared to the full-precision baseline. Experimenting with different widening factors shows that with 1.3x widening of filters and 4-bits of activation precision, one can achieve similar accuracy with only 8-bits of weight precision. Additionally, widening filters by 2x requires lowering precision to maintain accuracy levels. In our work, there is a trade-off between widening filters and reducing the precision of network parameters to maintain model accuracy. Widening filter maps also improves the efficiency of convolution operations. Our scheme applies to deeper networks like ResNet-34 and batch-normalized Inception. Our study focuses on ResNet-34 BID7 and batch-normalized Inception BID8, finding that 2-bits weight and 4-bits activations maintain accuracy. Using TensorFlow BID0 and tensorpack with ILSVRC-12 dataset, ResNet-34 has 3x3 filters with shortcut connections of 1x1. Binarizing weights and activations, except for the first and last layers, yields a top-1 accuracy of 60.5%. For ResNet-18, XNOR-NET (1b weights and activations) is 18% worse than full-precision network. Binarized ResNet-34 outperforms single-precision AlexNet. Doubling filters and reducing precision improves accuracy. 2-bits weights and activations match baseline accuracy. Binarizing with 2x wide filters achieves 69.85% accuracy, 3.7% lower than full-precision network at 15% of the cost. Widening filters by 3x and binarizing weights and activations reduces the accuracy gap to 1.2%, with the 3x wide network costing 30% of the full-precision baseline network. Advocating for 4-bits activation precision and 2-bits weight precision allows for simpler hardware implementation and lower compute cost compared to baseline networks. Ternary weights eliminate multipliers, maintaining accuracy, while binary circuits offer efficient hardware implementation and save 32x in bandwidth for weights and activations. Our ResNet binary and ternary (2-bits or 4-bits activation) accuracies are state-of-the-art, with results surpassing unpublished reports. Applying WRPN scheme to batch-normalized Inception network, we achieved promising results. Using 4-bits activations and 2-bits weight, doubling filter banks, and wide network with binary weights and activations, we maintained accuracy close to baseline networks. Adopting the straight-through estimator approach in our work, we quantized effectively. The straight-through estimator (STE) approach is used in quantizing real numbers to k-bits, overcoming zero gradients issue. Various methods like TWN, TTQ, XNOR-NET, and DoReFa are used to quantize weights to ternary or binary domains with different approaches. The DoReFa method uses a scaling factor for quantizing weights to k-bits, with a tanh operation to constrain values between -1 and +1. A simpler scheme is proposed, where weight tensors are constrained to [-1, 1] and activation tensors to [0, 1] before quantization into k-bit numbers. The DoReFa method proposes quantizing weights and activations to k-bits using affine transformations. For k > 1, weights are stored as signed data-types and activations as unsigned data-types. Convolution operations can be done using quantized values followed by scaling with floating-point constants. BWN approach is used for binary weights when k = 1. When using WRPN for convolution operations, matrix multiplication involves k-bits signed and unsigned operands. Gradient values are kept in 32-bits floating-point format. Hard clipping of tensors when k > 1 maps efficiently to hardware units. TTQ and DoRefa BID28 schemes involve division and computing maximum values in tensors, which can be computationally expensive in hardware. In low-precision computing, the quantization scheme proposed involves clipping and rounding instead of costly operations like TTQ. Efficiency depends on hardware, with evaluations done on GPU and FPGA using WRPN. Performance numbers are collected from various analyses, and a DNN accelerator architecture is implemented for FPGA. The accelerator consists of a systolic array of processing elements (PEs) for matrix and vector operations, supporting different precisions like FP32, INT4, TER2, and BIN1. The ternary PE operates on values of +1, 0, -1 with only an adder, while the binary PE uses XNOR and bitcount. Efficiency improvements are shown in FIG2 based on the number of bits used in the operation. The design targets Arria-10 1150 FPGA and Intel 14 nm process technology for ASIC synthesis. The efficiency gains from reducing precision depend on hardware support. GPUs can only achieve up to 4x improvements over FP32 with low-precision operations. FPGAs can take advantage of low precisions like BIN1, exceeding first-order estimates. Reducing precision simplifies compute unit design. Reducing precision in compute units on FPGA boards leads to improved throughput and efficiency. FPGA outperforms GPU in low-precision operations, especially in terms of performance/watt. ASIC allows for customized hardware implementation, showing significant efficiency benefits from low-precision operations. Lower precision offers 2 to 3 orders of magnitude efficiency improvement compared to baseline FP32 PE. Reducing precision in compute units on FPGA boards leads to improved throughput and efficiency, with FPGA and ASIC being well suited for the WRPN approach. WRPN approach at 2x wide requires 4x more total operations than the original network, but for INT4 or lower precision, each operation is 6.5x more efficient than FP32 for FPGA and ASIC. Reduced-precision DNNs are actively researched, with schemes like Binary connect (BC), Ternary-weight networks (TWN), and INQ targeting precision reduction of network weights while maintaining full-precision activations. Accuracy may be degraded when quantizing weights, with TWN losing 5% top-1 accuracy for AlexNet on Imagenet. Fine-tuning methods like INQ, BID20, and BID13 are used to quantize network weights. Recent research focuses on reducing precision in compute units on FPGA boards for improved throughput and efficiency. Various schemes like INQ, BID20, and BID13 use fine-tuning to quantize network weights without sacrificing accuracy. However, approaches like XNOR-NET and DoReFa show decreased accuracy when quantizing both weights and activations to 1-bit. Newer methods like BID4 target low-precision activations and report accuracy within 1% of baseline with 5-bits precision. Fine-tuning can narrow this gap to within 0.6%, but not all layers are quantized. Non-multiples of two for operand values introduce hardware inefficiency in memory usage. Our work focuses on reducing precision in deep and wide networks for end-to-end training and inference. We aim to maintain accuracy while using simple quantization methods, achieving state-of-the-art results with 4-bits activations and 2-bits weights. Unlike previous approaches, we target sub-8b training and advocate for low precision fixed-point numbers. In our work, we introduce the Wide Reduced-Precision Networks (WRPN) scheme for DNNs, focusing on sub-8b training. We quantize gradients before communication in distributed computing, using full precision gradients during the backward pass. This approach reduces communication traffic and maintains network accuracy with 2-bit weights and 4-bit activations. Our work introduces the Wide Reduced-Precision Networks (WRPN) scheme for DNNs, focusing on sub-8b training. By using 2-bit weights and 4-bit activations, we maintain baseline accuracy across various networks. This is achieved through a new quantization scheme and increasing the number of filter maps in each reduced-precision layer. The interplay between layer width and precision helps control overfitting and regularization, reducing memory footprint and compute complexity significantly. The WRPN quantization scheme reduces compute complexity by 40% with 2-bit weights and 4-bit activations. It is hardware-friendly for embedded systems and cloud-based servers, improving performance and energy-efficiency across different hardware implementations. Reducing precision allows for custom compute units and lower buffering requirements, leading to increased throughput."
}