{
    "title": "HyzMyhCcK7",
    "content": "To enable deep neural networks on resource-constrained devices, quantizing models with low-precision weights is beneficial. The straight-through gradient method is a common technique for quantization, allowing back-propagation through the quantization mapping. Despite its success, little is known about why this method works. Building upon the straight-through gradient method for quantization, ProxQuant proposes a more principled approach by formulating quantized network training as a regularized learning problem. It outperforms state-of-the-art results on binary quantization for ResNets and LSTMs, and converges to stationary points under mild smoothness assumptions. Deep neural networks (DNNs) with many layers and parameters can be compressed for use in memory and computationally limited environments through quantization, where weights and activations are represented in low precision. For example, in a binary neural network, weights are restricted to {\u00b11}. Quantized networks reduce memory usage by restricting weights to low precision values. Training involves designing a quantizer and using the straight-through gradient method for back-propagation. The structuredness of quantized weight matrices enables faster matrix-vector products, accelerating inference. The BinaryConnect method BID4 uses the straight-through gradient method for training binary networks, but it is unclear why this method is effective. Gradients in the discrete set {\u00b11} do not necessarily provide information about function values, leading to challenges in finding minimizers for simple convex functions. The BinaryConnect method BID4 utilizes the straight-through gradient method for training binary networks, but the effectiveness of this method is unclear. Gradients in the discrete set {\u00b11} may not provide information about function values, posing challenges in finding minimizers for simple convex functions. The BinaryConnect method BID4 uses the straight-through gradient method for training binary networks, but the effectiveness of this approach is uncertain due to challenges in finding minimizers for simple convex functions with gradients in the discrete set {\u00b11}. The BinaryConnect method uses the straight-through gradient method for training binary networks. It faces challenges in finding minimizers for simple convex functions with gradients in the discrete set {\u00b11}. The method computes the gradient at the quantized vector and updates the original real vector. Another approach, PROXQUANT, updates the real vector first and then applies a prox step to encourage quantizedness. A toy failure case involves two functions with coinciding derivatives at {\u22121, 1}, leading to identical behaviors but different minimizers. In this paper, the problem of model quantization is formulated as a regularized learning problem. A unified framework is presented for defining regularization functionals that encourage binary, ternary, and multi-bit quantized parameters. The proposed method, PROXQUANT, is a stochastic proximal gradient method with a homotopy scheme for training quantized networks. Compared to the straight-through gradient method, PROXQUANT has access to additional gradient information. PROXQUANT, a stochastic proximal gradient method with a homotopy scheme, outperforms state-of-the-art results on binary quantization and is comparable on ternary and multi-bit quantization. The method converges to stationary points under mild smoothness assumptions, unlike BinaryRelax BID25 which fails to converge. BinaryConnect has a stringent condition to converge to fixed points, verified through a sign change experiment. Deep Compression compresses DNNs using sparsification, clustering, and Huffman coding, leading to efficient hardware for inference. Binary neural networks can be trained using BinaryConnect, extended to ternary quantization. Quantized nets with activation quantization have shown impressive performance on tasks like ImageNet classification and object detection. In NLP, quantized language models have been successfully trained using multi-bit techniques. In the NLP field, quantized language models have been trained successfully using multi-bit quantization techniques. The effectiveness of binary networks is demonstrated through preserved angles between high-dimensional vectors when binarized, allowing for high-quality feature extraction. Principled methods utilize Wasserstein regularization and adversarial representation for model quantization, with potential for generalization but difficulty in tuning due to inner maximization instability. Prior to our work, several proximal or regularization-based quantization algorithms were proposed as alternatives to the straight-through gradient method. BID25 introduced BinaryRelax, a lazy proximal gradient descent method. BID12 and BID11 proposed a proximal Newton method with a diagonal approximate Hessian. Carreira-Perpin\u00e1n and Idelbayev formulated quantized network training as a constrained optimization problem solved via augmented Lagrangian methods. Our algorithm differs by using non-lazy and \"soft\" proximal gradient descent with a choice of 1 or 2 regularization, demonstrating advantages over lazy prox-gradient methods both theoretically and experimentally. The optimization difficulty of training quantized models lies in the discrete parameter space, making efficient local-search methods challenging. Training a binary neural network involves minimizing L(\u03b8) for \u03b8 \u2208 {\u00b11} d, where traditional methods like Projected SGD or greedy nearest-neighbor search are impractical. Quantized training can be seen as minimizing L(q(\u03b8)) for \u03b8 \u2208 R d, but the non-differentiable nature of the quantizer q often hinders back-propagation. BinaryConnect BID4 proposed a solution to this problem. The pioneering work of BinaryConnect BID4 introduces the straight-through gradient method to address the optimization challenge of training quantized models. This method allows the real vector \u03b8 to move in the entire Euclidean space, resulting in a valid quantized model at the end of training. Despite its empirical success, the information-theoretical basis of the straight-through method remains unclear, and it may fail on simple convex Lipschitz functions. It is observed that the straight-through gradient method is equivalent to a dual-averaging method in the binary case. The lazy projected SGD minimizes L(\u03b8) over Q = {\u00b11} d by taking the gradient at the quantized vector and updating the original real vector. A projection can be seen as a limiting proximal operator with a regularizer, allowing for more generality. The proximal operator with respect to a regularizer R and strength \u03bb > 0 minimizes \u03b8 \u2212 \u03b8 0 2 2 over \u03b8 \u2208 Q in the limiting case \u03bb = \u221e. The PROXQUANT algorithm introduces a quantization-inducing regularizer onto the loss and optimizes using the proximal gradient method with a finite \u03bb. This approach allows for a \"soft\" projection towards the discrete set Q, providing a less aggressive alternative to the \"hard\" projection with the potential advantage of avoiding overshoot early in training. The prox operator does not strictly enforce quantizedness, allowing for querying gradients at every point in the space and accessing more information than the straight-through gradient method. The PROXQUANT algorithm introduces a quantization-inducing regularizer onto the loss and optimizes using the proximal gradient method with a finite \u03bb. Compared to usual full-precision training, PROXQUANT adds a prox step after each stochastic gradient step, allowing for easy implementation on existing training methods. The method adapts to various stochastic optimizers such as Adam, providing a flexible approach to quantization. The PROXQUANT algorithm introduces quantization-inducing regularizers onto the loss function, optimizing with a finite \u03bb. It adds a prox step after each stochastic gradient step, compatible with various optimizers like Adam, offering a flexible approach to quantization. The proposed method generalizes existing algorithms for model quantization by incorporating regularizers with strength \u03bb < \u221e and non-lazy prox operators. The regularizers aim to vanish on a set of quantized parameter vectors Q and reflect distance to Q when \u03b8 is not in Q, using L1 and L2 regularizers. This framework allows for designing regularizers by specifying Q and choosing between L1 and L2, enabling the encoding of desired quantization structures. The proposed method introduces regularizers for quantization, allowing for the design of desired quantization structures by specifying Q and choosing between L1 and L2. Examples include binary weights, ternary weights, and multi-bit quantization, with efficient algorithms for solving the prox operators. The binary regularizer R bin induces quantization in a similar way to L1 regularization for sparsity. It is beneficial to keep biases and BatchNorm layers at full-precision. The prox operator for R bin has a simple analytical solution. The choice between L1 and squared L2 versions is not unique. Multi-bit quantization with adaptive levels is considered. Following BID23, k-bit quantized parameters with adaptively-chosen quantization levels are considered. The prox operator for the squared L2 regularizer is derived, leading to a joint minimization problem. An alternating minimization schedule is adopted to solve it, with a trade-off parameter \u03bb governing the quantization and closeness to \u03b8. This procedure generalizes the alternating minimization in BID23. The prox operator for ternary quantization is derived with a trade-off parameter \u03bb controlling the movement towards quantized weights. The regularization rate \u03bb increases linearly to gradually move the stochastic gradient steps towards exact quantizedness. PROXQUANT is a method that starts with full-precision training and gradually moves towards quantized weights using a linear increasing scheme. The parameter \u03bb is tuned by minimizing validation loss to control the aggressiveness of falling onto the quantization constraint. PROXQUANT is evaluated on image classification with ResNets and language modeling with LSTMs, showing comparable or better results than the default straight-through gradient method. Image classification is performed on the CIFAR-10 dataset with data augmentation strategies applied. Our models are ResNets of depth 20, 32, 44, and 56 with weights quantized to binary or ternary using PROXQUANT (PQ-B and PQ-T). Regularization strength \u03bb = 10 \u22124 and Adam optimizer with learning rate 0.01 are used. Comparison is made with BinaryConnect (BC) and Trained Ternary Quantization (TTQ) BID27. BC and PROXQUANT are initialized at the same pre-trained full precision for binary quantization. Our PROXQUANT and BinaryConnect are initialized at the same pre-trained full precision nets and trained for 300 epochs. PROXQUANT consistently outperforms BinaryConnect in binary quantization, with a performance drop of about 1% compared to full precision nets. Ternary quantization results are deferred to Appendix B.1. Language modeling is performed with LSTMs on the Penn Treebank dataset. The language modeling with LSTMs on the Penn Treebank dataset involves training quantized LSTMs with the encoder, transition matrix, and decoder quantized to k-bits. The model is compared to the state-of-the-art alternating minimization algorithm. Training is initialized at a pre-trained full-precision LSTM and uses SGD optimizer with specific parameters. The study involves training quantized LSTMs with specific parameters such as BPTT 30, dropout probability 0.5, and gradient norms clipped to 0.25. The regularization rate \u03bb is tuned for best performance on the validation set. Results show that PROXQUANT performs comparably to Straight-through gradient method and outperforms BinaryConnect on Binary LSTMs. Theoretical analysis in Section 5.1 demonstrates PROXQUANT's convergence under mild smoothness assumptions. The study compares PROXQUANT to BinaryConnect and BinaryRelax BID25 algorithms in terms of convergence. PROXQUANT is shown to outperform BinaryConnect under certain conditions, with theoretical analysis demonstrating its convergence under mild smoothness assumptions. PROXQUANT outperforms BinaryConnect and BinaryRelax BID25 algorithms in terms of convergence under certain conditions. The method involves a clever trick of rescaling the alternating quantizer, resulting in better performance without the need for a scaling step. The convergence guarantee of PROXQUANT requires smoothness of both the loss and regularizer. Smoothness of the loss can be achieved with a smooth activation function like tanh. The regularizer can be made differentiable by using a smoothed version of quantization-inducing regularizers. The lazy prox-gradient algorithm for solving the problem is a variant that accumulates gradients at original points, known to hold for convex problems but generally not for smooth non-convex problems. Lazy prox-gradient algorithm does not converge on smooth non-convex problems, even when PROXQUANT converges ergodically. A specific initialization can cause oscillation between non-stationary points, preventing convergence in an ergodic sense. The construction example provided is simple and not adversarial, with details in Appendix D.2. For BinaryConnect, the concept of stationary points is not applicable due to isolated target points. BinaryConnect has a stringent convergence condition, converging to a fixed point with specific learning rates. The algorithm's convergence is defined by the existence of a fixed point, with results dependent on the grid size for quantization. The original BinaryConnect method is contrasted with a version that only yields useful results when the grid size is small. The PROXQUANT method is proposed as an alternative to BinaryConnect for training quantized networks. Results show that PROXQUANT offers better convergence properties than the straightthrough gradient method. Future work includes exploring alternative regularizers for PROXQUANT on larger tasks. The minimization problem is coordinate-wise separable. The PROXQUANT method is proposed as an alternative to BinaryConnect for training quantized networks, offering better convergence properties. The minimization problem is coordinate-wise separable, with specific penalty terms and quadratic terms for \u03b8 j. The solution to the prox satisfies certain conditions, leading to equations for different regularizers such as SoftThreshold and squared L2 versions. Ternary quantization involves an approximate alternating prox operator for computing \u03b8. The PROXQUANT method offers better convergence properties than BinaryConnect for training quantized networks. Ternary quantization involves an approximate alternating prox operator for computing \u03b8, using a ternary quantizer. The top-1 classification errors for ternary quantization are comparable to TTQ, with the best performance achieved after two rounds of alternating computation. Our method outperforms TTQ in terms of performance. We compare the training dynamics of PROXQUANT-Binary and BinaryConnect using the sign change metric, which measures the closeness of binarized parameters. In our experiments, we initialize at a full-precision net and end with a converged binary network. PROXQUANT produces binary nets with lower sign change compared to BinaryConnect. PROXQUANT produces binary nets with lower sign changes and higher performances compared to BinaryConnect. This suggests that BinaryConnect suffers from higher optimization instability than PROXQUANT. The finding is consistent across all layers, different warm starts, and runs. The signs in BinaryConnect keep changing until manually frozen at epoch 400."
}