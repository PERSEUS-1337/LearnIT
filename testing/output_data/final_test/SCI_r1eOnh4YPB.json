{
    "title": "r1eOnh4YPB",
    "content": "Learning rate decay (lrDecay) is a technique for training neural networks by starting with a large learning rate and then decaying it multiple times. It is observed to help optimization and generalization. Common beliefs about how lrDecay works include accelerating training with a large initial learning rate and helping the network converge to a local minimum by decaying the learning rate. However, experiments suggest that these beliefs are insufficient in explaining the effectiveness of lrDecay in training modern neural networks. Another explanation is proposed: a large initial learning rate prevents the network from memorizing noisy data, while decaying the learning rate improves the learning of complex patterns. The proposed explanation for the effectiveness of learning rate decay (lrDecay) in training modern neural networks is that a large initial learning rate prevents memorization of noisy data, while decaying the learning rate improves learning of complex patterns. This alternative explanation is validated on a carefully-constructed dataset with tractable pattern complexity and justified in real-world datasets. It sheds light on designing better training strategies for deep, wide, and nonconvex neural networks, which are powerful tools for representation learning and top-performing models in various applications. The difficulty of applying conventional machine learning wisdom to deep learning is highlighted in pioneering works. Understanding deep learning is crucial in the AI field, with magic modules in neural networks like batch normalization remaining mysterious. The importance of learning rate in training neural networks is emphasized, with learning rate decay playing a significant role. Learning rate decay (lrDecay) is a key technique in training modern neural networks, where the learning rate is initially set high and then decreased by a certain factor after a set number of epochs. Popular deep networks like ResNet and DenseNet are trained using Stochastic Gradient Descent (SGD) with lrDecay. This method is widely used due to its simplicity and effectiveness, with performance improvements observed after each decay in learning rate. The effectiveness of learning rate decay (lrDecay) in training neural networks is widely acknowledged. However, recent experiments challenge common beliefs about its impact on escaping local minima and accelerating training. Instead, a new perspective suggests that the learning rate magnitude is linked to the complexity of learnable patterns. The effectiveness of learning rate decay (lrDecay) in training neural networks is widely acknowledged. A new perspective suggests that the learning rate magnitude is linked to the complexity of learnable patterns. An initially large learning rate suppresses noisy data memorization, while decaying the rate improves learning complex patterns. This explanation is validated on a carefully-constructed dataset with tractable pattern complexity. The implication that additional patterns learned in later stages of lrDecay are more complex and less transferable across datasets is also supported empirically. This new perspective challenges common beliefs and provides a fresh analysis of learning rate decay. The paper introduces a new perspective on learning rate decay, highlighting the importance of pattern complexity in modern neural networks. Existing explanations are deemed insufficient, and a novel explanation based on pattern complexity is proposed and validated on different datasets. The study suggests that complex patterns are only learnable after learning rate decay, challenging common beliefs and providing a fresh analysis of this training technique. In a study on learning rate decay, the behavior of Stochastic Gradient Descent (SGD) in deep models is analyzed. Different from previous works, the research focuses on the impact of lrDecay in a WideResNet model, showing that learning rate decay improves learning of complex patterns by defining pattern complexity through expected class conditional entropy. The study focuses on pattern complexity in deep models using learning rate decay in Stochastic Gradient Descent (SGD). It contrasts adaptive learning rate methods like AdaGrad, AdaDelta, and ADAM, highlighting the importance of studying SGD behavior and lrDecay's impact on training algorithms. The study examines the impact of learning rate decay in Stochastic Gradient Descent (SGD) on deep models. Different learning rate strategies, such as cyclic strategy and warm restart, are discussed, with a focus on the simplicity and effectiveness of lrDecay. Training models on one dataset for transferability to others remains a key goal in AI research. The exploration of model transferability in AI research has attracted attention. Various studies have shown the effectiveness of transferring deep features for object detection and the impact of network architectures on transferability. Additionally, the evolution of transferability during training with lrDecay is investigated in this study. The practice of lrDecay in training neural networks dates back to LeCun et al. (1998) and is based on the optimization analysis of Gradient Descent (GD). Learning rates are characterized by their relationship with eigenvalues of the Hessian at a local minimum. The learning rate in neural networks is crucial for convergence. If 0 < \u03b7 < 2/\u03bb for all eigenvalues of the Hessian, the network converges quickly. Otherwise, divergence or oscillation may occur, requiring lrDecay to avoid these issues and achieve faster convergence. The learning rate in neural networks is crucial for convergence. It is believed that with a high learning rate, the system may not settle into deeper parts of the loss function. Recent studies show that an initially large learning rate helps escape spurious local minima, and with learning rate decay, the probability of reaching the minimum increases. SGD is proven to be equivalent to the convolution of the loss surface, with the learning rate acting as the kernel size. Proper learning rate selection can help smooth out spurious local minima and aid in escaping bad local minima. The decay of learning rate helps neural networks escape bad local minima and converge around the minimum. Using carefully-designed experiments, it is shown that explanations in previous sections are insufficient to explain the efficacy of learning rate decay in modern neural networks. WideResNet is used in most experiments, a deep, wide, nonconvex network suitable for datasets like CIFAR10. The WideResNet model is trained on the CIFAR10 dataset using gradient descent with learning rate decay. Results show that learning rate decay has minimal impact on optimization or generalization. The initial instability is attributed to a high loss wall, while the decay helps avoid oscillation. The effect of lrDecay in training modern neural networks is not well explained by the Gradient Descent (GD) explanation. The divergence factor for the largest eigenvalue is too large, leading to either convergence in a local minimum or divergence. Stable solutions are observable when the learning rate is small enough, making learning rate decay unnecessary. Increasing the learning rate mildly results in diverging learning curves. The effect of lrDecay in training modern neural networks is not well explained by the Gradient Descent (GD) explanation. According to the SGD explanation, lrDecay increases the probability of reaching a good minimum. However, Figure 7 shows that the best performances before and after lrDecay are different by a noticeable margin, contradicting the SGD explanation. The performance boost after learning rate decay is widely observed. The performance boost after learning rate decay is widely observed, but the SGD explanation cannot fully explain it. New research suggests that SGD learns from easy to complex patterns, and the complexity of learned patterns is related to the magnitude of learning rates. Learning rate decay improves learning complex patterns while initially large learning rates help avoid memorization of noisy data. This novel explanation is supported by careful construction of evidence. The complexity of patterns in a dataset is defined by the expected class conditional entropy, where higher complexity indicates more patterns to recognize. This concept is formalized to explain the impact of learning rate decay on learning easy and complex patterns. The complexity of patterns in a dataset is defined by the expected class conditional entropy. A Pattern Separation 10 (PS10) dataset is constructed with ten categories, including simple and complex patterns. The dataset is visualized in R3, with simple and complex patterns identified by different colors. The simple and complex sub-datasets are merged using a method that feeds them into different channels of the WideResNet. This mimics the idea of different patterns having different locations, like the eye and nose patterns on a human face. In a study on convolutional computation, the effect of decaying the learning rate was examined by comparing experiments with and without lrDecay. Results showed that decaying the learning rate helped the network learn complex patterns more effectively. The model first learned simple patterns quickly, with a boost in total accuracy mainly coming from gains in accuracy on complex patterns. This suggests that a smaller learning rate allows the network to learn more complex patterns. Adding 10% noisy data to mimic real-world datasets, a study found that an initially large learning rate can help suppress overfitting of noisy data, thus improving accuracy on complex patterns. This contrasts with the negative impact of memorizing noisy data on learning complex patterns. Li et al. (2019) empirically support the use of an initially large learning rate with decay. Our experiments suggest that noisy data may lead to spurious local minima, and an initially large learning rate helps suppress memorization of noisy data. This explanation is validated on real-world datasets, where complex patterns are harder to generalize. SGD with lrDecay learns patterns of increasing complexity, making transferability more challenging. SGD with lrDecay learns patterns of decreasing transferability, validated through transfer-learning experiments on real-world datasets. Models trained on ImageNet are transferred to different target datasets, including Caltech256, CUB-200, MITIndoors, and Sketch250. Sketch250 is the most dissimilar to ImageNet due to its sketch paintings. The study compares transfer learning strategies \"fix\" and \"finetune\" using ImageNet snapshot models on various datasets. Transferability of patterns learned in different stages is analyzed, with results showing decreasing transferability in later stages. Notably, transferability of patterns in Sketch250 dataset is negative. The transferability of additional patterns learned in later stages of lrDecay is found to be negative, supporting the claim that these patterns are more complex and less transferable. The study suggests providing pre-trained model snapshots in different stages for users to choose the most transferable model. The research delves into how lrDecay benefits neural networks by improving the learning of complex patterns and avoiding noisy data memorization. Experiments on datasets validate these findings. The study proposes AutoDecay to expedite training by decaying the learning rate when the loss plateaus, without affecting performance. Deciding when to decay is challenging due to noise in statistics, but observing the training process helps determine the optimal moment. The observed training loss is formalized into two parts: the ground truth loss and the noise introduced by SGD. The noise causes the training loss to vibrate in a short time window but decrease in a long time window. The task is to determine if the loss value is stable in the presence of noise. Exponential Decay Moving Average (EDMA) with Bias Correction is considered to compute a low-variance statistic. The text discusses using Exponential Decay Moving Average (EDMA) with Bias Correction to compute a low-variance statistic for stabilizing the training loss in the presence of noise. By adopting a moving average approach with a decay factor \u03b2, the statistic \u011d(t) is computed online and represents the unobservable g(t). The variance of \u011d(t) decreases monotonically with \u03b2 typically set at 0.9, allowing for rapid convergence to a smaller variance than the noise. The stability of \u011d(t) indicates the stability of g(t), providing a criterion for determining stability. The text discusses the stability of \u011d(t) in the context of training loss stabilization. It introduces criteria for determining stability based on the decay of the learning rate and significant drops in the learning rate. The AutoDecay procedure is described, and its application on ImageNet is tested for effectiveness. AutoDecay was tested on ImageNet to expedite training without sacrificing performance. Results showed a 10% reduction in training time without performance loss, validating the proposed explanation. Increasing the learning rate for Gradient Descent led to diverging learning curves due to the large spectrum norm in modern neural networks. Training modern neural networks with Gradient Descent requires a small learning rate to prevent divergence, as even a slight increase can lead to a large spectrum norm at a local minimum. Learning rate decay is not recommended in this scenario."
}