{
    "title": "S1ldYJh4FH",
    "content": "Learning in Gaussian Process models involves adapting hyperparameters of the mean and covariance function. Two approaches for learning are Type II maximum likelihood and Fully Bayesian Gaussian Process Regression (GPR). This work explores approximations to the hyperparameter posterior using Hamiltonian Monte Carlo (HMC) and Variational Inference (VI). Predictive performance of fully Bayesian GPR is analyzed on benchmark datasets, showing the influence of hyperparameter choices on the GP posterior. The Gaussian process posterior heavily depends on the choice of covariance function and hyperparameters, known as the model selection problem. The marginal likelihood is often maximized for model selection, allowing for closed-form predictions. However, the non-convexity of the marginal likelihood surface can present challenges due to multiple modes. The presence of multiple modes in the likelihood surface poses challenges for ML-II, leading to overfitting and sensitivity to hyperparameters. Weakly identified hyperparameters can result in flat ridges in the marginal likelihood surface, making optimization sensitive to starting values. Bayesian treatment of hyperparameters in a hierarchical framework faces intractable posteriors, requiring numerical methods like Monte Carlo integration to handle predictive posterior, yielding a mixture of GPs. The hierarchical GP framework involves a non-Gaussian predictive posterior that results in a mixture of GPs. It examines uncertainty around covariance hyperparameters and their effect on the posterior predictive distribution. The generative model implies a joint posterior over unknowns, with a predictive distribution integrating over this joint posterior. The inner integral reduces to the standard GP predictive posterior. The hierarchical predictive posterior in the context of Gaussian Processes involves integrating out analytically and draws from the hyperparameter posterior. It is a multivariate mixture of Gaussians and utilizes the No-U-TurnSampler (NUTS) for efficient sampling. In experiments, the No-U-TurnSampler (NUTS) is used as a self-tuning variant of HMC, adjusting path length for each iteration. NUTS works as well as hand-tuned HMC, avoiding the need to determine step-size and path length values. Identity mass matrix is used with 500 warm-up iterations and 4 chains to detect mode switching. Primary variables are declared as log of hyperparameters to eliminate positivity constraints. Computational cost is dominated by inverting the covariance matrix. Hyperparameters are transformed to real space for efficiency. The variational family for hyperparameters is defined in real space with unconstrained variational parameters. The ELBO is maximized using stochastic gradient ascent in the transformed \u03b7 space, with intractable expectations approximated using Monte Carlo integration. The computational cost per iteration is determined by the Cholesky factorization of the covariance matrix. The computational cost per iteration for the re-parametrization trick is O(N M J), where J is the number of hyperparameters and M is the number of MC samples. Evaluating 4 UCI benchmark regression datasets under fully Bayesian GPR, HMC and full-rank variational schemes outperform ML-II, with HMC generating wider function spans. The mean-field (MF) performance is inferior to HMC and full-rank (FR) VI but dominates ML-II. VI schemes provide a close approximation to HMC in terms of error. The higher RMSE of MF compared to FR and HMC shows that considering correlations between hyperparameters improves prediction quality. Bayesian GPR in Gaussian likelihood setting for high-dimensional data sets with composite kernels is feasible. VI schemes based on Gaussian variational family are marginally inferior to HMC in predictive performance. HMC can generate samples from multi-modal posteriors using tempered transitions, leading to invariant predictions across different hyperparameter modes. In GP inference, the trade-off between computational cost, accuracy, and robustness of uncertainty intervals must be considered, especially in real-world applications with hand-crafted kernels involving many hyperparameters. Overfitting risks are higher and harder to detect in such cases. Integrating over hyperparameters to compute predictive intervals that reflect uncertainties in Gaussian Processes (GPs) can provide a more robust solution. Conducting inference over hierarchies in GPs may increase expressivity and representational power compared to deep GPs with point estimate hyperparameters. Different approximation schemes, such as warped GPs or deep kernel learning, can be considered to enhance the diversity of models consistent with the data. In early accounts, various methods like HMC and slice sampling have been used for integrating over covariance hyperparameters in Gaussian Processes. Different MCMC schemes have been explored for the full Bayesian treatment of GP models, including variationally sparse GPs and full Bayesian inference frameworks for regression. Recent work has focused on variational learning for sparse Gaussian Process Regression (GPR), extending the Bayesian treatment to hyperparameters. Inducing points and hyperparameters are jointly selected, with the posterior over hyperparameters obtained as a side-effect. Composite kernels are used in all datasets, with vague N(0,3) priors for hyperparameters in log space. Some hyperparameters in the Airline dataset were weakly identified, leading to constrained inference. In order to constrain inference, tighter normal and Gamma(2, 0.1) priors were used for hyperparameters in the experiments conducted in python using pymc3. Different training/test splits were used for various datasets, with specific kernels employed in the UCI experiments. In the UCI experiments, different kernels were used with specific hyperparameters. The marginal posteriors of the hyperparameters in the Airline kernel show that sampling and variational optimization do not converge to the same region as ML-II. Full Bayesian schemes provide better predictions, indicating ML-II is in an inferior local optimum. The mean-field marginal posteriors are narrower than the full rank and HMC posteriors. The noise std. deviation distribution learnt under the full Bayesian schemes is higher than ML-II point estimate, indicating overfitting in this example."
}