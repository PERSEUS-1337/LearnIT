{
    "title": "SJmAXkgCb",
    "content": "In this paper, a method is introduced to compress intermediate feature maps of deep neural networks (DNNs) by converting fixed-point activations into vectors over the smallest GF(2) finite field and using nonlinear dimensionality reduction (NDR) layers. This approach results in more compact feature maps by exploiting quantization redundancies. Experiments on ImageNet classification and PASCAL VOC object detection show a 2x decrease in memory requirements with minor accuracy degradation and only bitwise computations added. The paper introduces a method to compress DNN feature maps by converting activations into vectors over GF(2) field and using NDR layers. This leads to more compact feature maps with reduced memory requirements and minor accuracy loss. The focus is on minimizing memory footprint without adding significant computations or encoding-decoding schemes. In this work, the focus is on compressing DNN feature maps by converting activations into binary vectors over GF(2) field and using NDR layers. This method aims to reduce memory footprint and bandwidth during DNN inference for high-resolution cameras without significant accuracy loss. The proposed method involves compressing DNN feature maps by converting activations into binary vectors over GF(2) field and using NDR layers. This allows for a reduction in memory footprint and bandwidth during DNN inference for high-resolution cameras. The method utilizes compression and decompression layers over GF(2) within a network architecture resembling autoencoder-type structures. The network can be trained end-to-end and implements binary conversion and compression-decompression layers in the Caffe BID12 framework. Feature Map Compression using Quantization is a method that involves compressing DNN feature maps through various quantization schemes such as fixed-point, binary, and power-of-two quantization. This approach aims to reduce memory requirements and improve compression rates for network inputs. The quantized network is retrained to restore accuracy, but struggles to find more compact representations due to unchanged base architecture. Binary networks face accuracy drops for large datasets or compact architectures. A method for learned quantization using binary representation is proposed, along with NDR layers for weight and feature map compression. Our work extends NDR-type layers to work over GF(2) for a more compact feature map representation in DNN accelerator architectures. The fusion approach presented aims to decrease memory footprint and bandwidth by keeping only a subset of intermediate feature maps, despite the cost of recomputing convolutions. This approach complements existing strategies to reduce off-chip DRAM access power consumption. Our approach proposes to keep compressed feature maps with minimal additional computations, stored in a dense form without the need for special control and encoding-decoding logic. The input feature map of a convolutional layer in DNNs is represented by a tensor, which is then convolved with a weight tensor. The input feature map of a convolutional layer in DNNs is convolved with a weight tensor and bias vector, followed by a nonlinear function like ReLU. Multiple convolutional layers can be processed sequentially in a network. The input feature map of a convolutional layer in DNNs is convolved with weight tensor and bias vector, followed by a nonlinear function like ReLU. Fusion of N sequential layers can be done in a channel-wise fashion using memory buffers smaller than the whole feature map. Feature map X l can be quantized into Q using a nonlinear quantization function q(), potentially reducing accuracy. The network can be finetuned to restore some accuracy. NDR can be performed using an additional convolutional layer with projection weights P l. In DNNs, feature maps can be compressed using projection weights to save memory. The compressed feature map can be projected back using inverse steps. The number of bits for quantization depends on the dataset and network architecture. For example, AlexNet may only need 1 or 2 bits for small datasets but may lose accuracy for larger ones like ImageNet. The modified AlexNet's top-1 accuracy is reduced by 12.4% and 6.8% for 1-bit XNOR-Net BID17 and 2-bit DoReFa-Net BID20, respectively. Efficient network architectures like BID11 using NDR layers need 6-8 bits for fixed-point quantization on ImageNet. The paper focuses on selecting an efficient base network architecture and adding compression layers for smaller feature maps instead of starting with an over-parametrized network. Feature map quantization involves a scalar-to-scalar mapping using a nonlinear function. The text discusses a new representation using linear binarization functions and feature map compression methods for efficient network architectures. It proposes a method to convert a scalar over a higher cardinality finite field to a vector over a finite field with two elements. This involves quantizing activations and applying transformations to obtain a compressed feature map representation. The proposed method involves applying a sequence of layers with weights to obtain a compressed representation over GF(2). Compressed feature maps are stored in memory during inference, while non-compressed feature maps can be processed sequentially. The inverse function using convolutional layers and inverse quantization undo the compression and quantization steps. The graph model explains the inference and backpropagation phases of the newly introduced functions, with challenges related to the non-differentiable quantization function. The function is not differentiable, but studies show that a mini-batch-averaged floating-point gradient can be represented as gates making hard decisions similar to ReLU. The gradient can be calculated using results from previous studies. The gradient is a scaled sum of the gradient vector, with a scaling factor based on statistical information. Weight tensors can be initialized by an identity function for training. The proposed approach involves using compressed feature maps in a base network architecture based on a quantized SqueezeNet network. The architecture consists of \"fire\" modules with concatenated \"expand\" layers and a \"squeeze\" layer for NDR. The size of the layers is compressed by a factor of 8 along the channel dimension. Only the activations of the \"expand 1\u00d71\" layer are stored during inference. The proposed method extends the base architecture by introducing NDR layers with different kernel sizes to optimize quantization and reduce redundancies. The additional compression rate is defined by selecting parameters, with 1\u00d71 kernels addressing channel redundancies and 3\u00d73 kernels addressing spatial redundancies. The implementation includes stride 2 using convolutional-deconvolutional layers to decrease feature map size along spatial dimensions. In the experiments, the SqueezeNet v1.1 architecture is compressed by binarization and quantization layers. The input resolution is 227\u00d7227, weights are floating-point, and models are retrained on the ImageNet dataset for 100,000 iterations. The compressed models derived from the 8-bit quantized model were retrained iteratively. The quantized models experienced accuracy drops of -0.2%, 0.6%, and 3.5% for 8-bit, 6-bit, and 4-bit quantization, respectively, after retraining. The quantized computations were emulated on GPU using fp32, showing emulation speed for additional layers. The proposed compression method using 1 \u00d7 1 kernels restores top-1 accuracy by 1.0% and 2.4% for 6-bit and 4-bit versions, with a slight increase in weights and convolutions. A model with a convolutional layer followed by a deconvolutional layer reduces feature maps by a factor of 4, resulting in a drop in accuracy. Object detection using Pascal VOC BID3 dataset is evaluated for a more realistic application. Object detection using Pascal VOC BID3 dataset is evaluated with a SSD512 model BID13 architecture. SqueezeNet pretrained on ImageNet is used as a feature extractor, reducing parameters and inference time. Quantized and compressed models are generated for comparison, with quantization and compression applied to specific layers for memory reduction. The fusion technique compresses feature map memory by more than 80% due to their large spatial dimensions. Compressed models are derived from the 8-bit quantized model and retrained for 10,000 iterations using SGD solver. Results show a slight drop in accuracy for quantized models with retraining, with 2-bit models showing the largest decrease in accuracy. Compression-decompression layers with 1\u00d71 kernel improve mAP for 6-bit model by 0.5% and decrease mAP for 4-bit model by 0.5%. Channel dimension compression not beneficial for SSD due to low quantization redundancy or hyper-parameter choices. Spatial-dimension compression with 2\u00d72 kernel and stride 2 outperforms 3\u00d73 kernel with fewer parameters. An 8-bit model with 2\u00d72 kernel achieves 1% higher mAP than 3\u00d73 kernel. Memory footprint benefits summarized in TAB2 for evaluated SSD models. The evaluated SSD models show memory footprint benefits with feature map compression techniques. Fusion allows a 19x compression factor, while 8-bit and 4-bit models reduce size by 4x and 8x respectively. A proposed model with 2x2 stride 2 kernel achieves an additional 2x compression compared to the 4-bit model with minimal mAP degradation. Learned quantization along channel and spatial dimensions further enhances compression gain. The feature extractor reduces memory footprint significantly by compressing network feature maps using binary representation and autoencoder-inspired layers within DNNs. This compression strategy can be implemented on GPUs, CPUs, or other devices for inference. The evaluated compression strategy for inference can be adopted for GPUs, CPUs, or custom accelerators. Existing binary networks can be extended for higher accuracy in applications like object detection. Comparison of ImageNet results for networks compressing feature maps is done, with most using AlexNet while ours is based on SqueezeNet. Accuracy results for base networks and their quantized versions are shown in TAB3. Different approaches like Binary XNOR-Net BID17 and DoReFa-Net BID20 are discussed, with the latter offering flexibility in adjusting the number of bits for weights and activations. BID19 addresses the issue of binarizing last layer weights. The most recent work BID19 focuses on binarizing the last layer weights in low-precision networks based on AlexNet. These networks achieve varying levels of accuracy with 1-bit, 2-bit, and 4-bit activations. The memory footprint is largely defined by the first two layers of AlexNet, making fusion techniques challenging. The NIN-based network from BID19 achieves 51.4% accuracy with 2-bit activations, but has larger activation memory compared to AlexNet. BID17 explores binarization of activations in the first and last layers. The SqueezeNet-based models in TAB3 are finetuned from the corresponding models in TAB0 for 40,000 iterations with a mini-batch size of 1024, and SGD solver with a step-policy learning rate starting from 1e-4 divided by 10 every 10,000 iterations. The model with fusion and 8-bit quantized weights and activations outperforms state-of-the-art networks in terms of weight and activation memory. Compression-decompression layers added to \"fire2,3/squeeze\" modules decrease activation memory from 189.9KB to 110.3KB depending on the compression configuration. Compression-decompression layers added to \"fire2,3/squeeze\" modules reduce activation memory from 189.9KB to 110.3KB based on the compression configuration. More compression is achievable by applying the proposed approach to other \"squeeze\" layers."
}