{
    "title": "ByGuynAct7",
    "content": "In this work, a new type of prior distribution called deep weight prior (DWP) is proposed for convolutional neural networks. DWP utilizes generative models to shape the structure of trained convolutional filters, such as spatial correlations of weights. The DWP is defined as an implicit distribution, and a method for variational inference with implicit priors is introduced. Experimental results demonstrate that DWP enhances the performance of Bayesian neural networks with limited training data, and initializing weights with DWP samples speeds up training of conventional convolutional neural networks. Bayesian inference transforms a prior distribution to a posterior distribution after observing training data. Stochastic variational inference has been successful in obtaining a variational approximation of a posterior distribution over weights of deep neural networks. The development of Bayesian deep learning focuses on improving approximate inference and designing prior distributions for sparsification, quantization, and compression of deep learning models. In Bayesian deep learning, prior distributions are used for sparsification, quantization, and compression of deep learning models. However, these prior distributions are limited to fully-factorized structures, which cannot enforce the spatial structure of convolutional filters. Convolutional neural networks benefit from more flexible prior distributions due to the correlation of weights, especially in learning similar convolutional kernels on different datasets within the same domain. This leads to the concept of a source kernel distribution, which can be efficiently approximated with convolutional kernels of models trained within a specific data domain. The text discusses a method for estimating the source kernel distribution in an implicit form to perform variational inference with specific implicit priors. The proposed deep weight prior framework incorporates prior knowledge about the structure of convolutional filters and uses an implicit form of this prior. Additionally, a method for variational inference with implicit priors is developed. In experiments, variational inference with deep weight prior improves classification performance with limited training data. Initializing convolution networks with samples from deep weight prior leads to faster convergence and better feature extraction. Bayesian setting aims to transform prior knowledge of distribution parameters to posterior distribution after observing a dataset. Variational Inference approximates the posterior distribution to solve computationally intractable integrals. Variational Inference (Jordan et al., 1999) is an approximation method that reduces inference to an optimization problem by optimizing parameters \u03b8 of a variational approximation q \u03b8 (\u03c9) to minimize KL-divergence with p(\u03c9 | D). The variational lower bound L(\u03b8) of the marginal log-likelihood consists of the expected log likelihood L D and a regularizer. In cases of intractable expectations, the variational lower bound and its gradients cannot be computed in a closed form. Kingma & Welling (2013) and BID28 proposed a mini-batch based approach to stochastic variational inference, known as stochastic gradient variational Bayes. Variational inference involves stochastic gradient variational Bayes, using reparameterization to efficiently compute unbiased stochastic gradients for deep neural networks. The framework maximizes the variational lower bound with respect to the parameters of a neural network. Variational inference involves using variational distributions to approximate the posterior distribution in Bayesian neural networks. The variational auto-encoder proposed by Kingma & Welling (2013) maximizes a variational lower bound on the marginal log-likelihood through amortized variational inference. The vanilla VAE defines fully-factorized distributions for latent variables and object space. Richer variational approximations and prior distributions have been proposed. The deep weight prior is introduced as an expressive prior distribution based on generative models, favoring the structure of learned convolutional filters in a neural network with multiple convolutional layers. The deep weight prior introduces a prior distribution that favors the structure of learned convolutional filters in a neural network with multiple convolutional layers. The prior distribution is not factorized over spatial dimensions of the filters, and a source kernel distribution is defined for the trained convolutional kernels. The prior distribution p l (w l ij) for convolutional kernels of the l-th layer is a natural candidate but its probability density function is inaccessible for most inference methods. To approximate this, kernels from models trained on external datasets in the same domain can be used. Generative models can help approximate the intractable probability density function of the source kernel distribution. Generative models are used to approximate the probability density function of the source kernel distribution in convolutional neural networks. Kernels from the l-th layer are treated as samples from this distribution, and explicit and implicit approximations are discussed. Generative models can use explicit or implicit approximations to evaluate probability density functions. Explicit models like Kernel Density Estimation and Normalizing Flows allow for the estimation of KL-divergence without bias, but can be computationally expensive. Implicit models are more computationally efficient but do not provide access to an explicit form of the probability density function. The conditional distribution p(w | z; \u03c6 l ) is modeled by a differentiable function, such as a neural network, with parameters \u03c6 l that can be fitted using variational autoencoder framework. Variational autoencoders offer low memory cost and fast sampling but cannot provide an unbiased estimate of the logarithm of the probability density function logp l (w). This limitation prevents the construction of an unbiased estimator of the variational lower bound. The text discusses a modification of variational inference for implicit prior distributions to address the limitation of not being able to build an unbiased estimator of the variational lower bound. It highlights the challenge of estimating the KL-divergence without bias in the case of an implicit prior distribution. The process of learning the prior distribution over kernels of one convolutional layer is illustrated in Figure 1. The text discusses the process of learning the prior distribution over kernels of one convolutional layer using a VAE framework. An auxiliary lower bound on the KL-divergence is introduced to make the computation tractable. The final auxiliary variational lower bound is tight when the KL-divergence between the auxiliary reverse model and the intractable posterior distribution over latent variables is zero. The text discusses stochastic variational inference with an implicit prior distribution for learning the prior distribution over kernels of one convolutional layer using a VAE framework. The auxiliary variational lower bound is maximized with respect to the parameters of the variational approximation and reverse models. Parameters of the prior distribution are fixed during variational inference. The algorithm aims to reduce the variance of the gradient estimation by explicitly defining the prior distribution over kernels of a convolutional layer. Training deep weight prior models involves collecting source datasets of kernels and training reconstruction models on them. The algorithm focuses on reducing gradient estimation variance by defining prior distributions over kernels of convolutional layers. Training involves collecting source datasets and using reconstruction models to construct priors for each layer. Regularization is crucial for learning structured kernels, with deep weight prior distribution outperforming standard normal and log-uniform distributions in mean test accuracy. In experiments, inference models use fully-factorized normal distributions, while reconstruction models are also modeled by a fully-factorized normal distribution. The network architecture includes convolutional layers, ELU BID3 layers, and max-pooling layers. The network architecture for prior models is similar to the inference model, using transposed convolutions. Fully-factorized standard Gaussian priors are used for latent variables. Deep weight prior is applied to variational inference and initialization of neural networks. Experiments were conducted on MNIST, NotMNIST, CIFAR-10, and CIFAR-100 datasets using PyTorch. Adam optimizer with default hyperparameters was used for optimization. Prior distributions were trained on source networks from different initial points on NotMNIST and CIFAR-100 datasets. In this experiment, variational inference was performed over weights of a discriminative convolutional neural network with different prior distributions for the convolutional layers. The influence of initialization of convolutional filters on random feature extraction performance was studied by randomly initializing and fixing the weights. Initializations were sampled from deep weight prior, learned filters, and Xavier distribution. The experiment was conducted for models of different sizes by scaling the number of filters in all convolutional layers. In this experiment, models of different sizes were created by scaling the number of filters in convolutional layers. Results showed that initialization with samples from deep weight prior and learned filters outperformed Xavier initialization. The method can be combined with more complex variational approximations to improve variational inference. Results for MNIST and CIFAR-10 datasets are presented. The experiment involved using neural networks with convolutional layers of different sizes and configurations, followed by fully connected layers. Variational inference with deep weight prior showed better accuracy compared to other prior distributions, especially with smaller training datasets. Random initialization of convolutional layers still produced useful features. In this experiment, different random initializations of convolutional layers were studied for their impact on the performance of convolutional networks of varying sizes. Initializations with samples from deep weight prior and learned kernels outperformed the standard Xavier initialization for smaller networks. Results on MNIST and CIFAR-10 for different network sizes were shown in FIG1, highlighting the sensitivity of deep learning models to weight initialization. In this experiment, the influence of weight initialization on the convergence speed of deep learning models was studied. Results showed that using weights from deep weight priors or learned filters improved training speed compared to Xavier initialization. The study compared different weight initializations for convolutional layers on MNIST and CIFAR-10 datasets. The study compared the performance of deep weight prior and learned filters initialization with standard Xavier initialization on convolutional networks trained on MNIST and CIFAR-10 datasets. Results showed that both alternatives led to faster convergence, with deep weight prior initialization not requiring storage of a large set of filters. Transfer learning with convolutional networks was also discussed, showing similar filters produced during training on different datasets within the same domain. Unlike Bayesian techniques, these methods do not provide a posterior distribution over model parameters. The Bayesian approach incorporates prior knowledge about model weights by choosing a prior distribution. Empirical Bayes tunes parameters of the prior distribution on training data. While widely used for regularization in linear models, these methods do not consider the structure of model weights in deep neural networks, such as spatial correlations in convolutional networks. Our approach enables variational inference with an implicit prior distribution based on observed convolutional kernels. In contrast to empirical Bayes, our work proposes using an implicit prior distribution for stochastic variational inference. This approach allows for adjusting parameters before variational inference and remains fixed throughout. Implicit models have been applied to variational inference, including flexible variational distributions like semi-implicit BID38 and Markov chain BID14 approximations. Our method can be generalized to prior distributions in the form of a Markov chain, demonstrating how to learn within this framework. In this work, a deep weight prior framework is proposed for designing a prior distribution for convolutional neural networks, leveraging prior knowledge about the structure of learned convolutional filters. The factorization of the deep weight prior does not consider inter-layer dependencies of the weights, which could be improved for CNNs. Accounting for these dependencies may allow for recovering a distribution in the space of trained networks rather than just trained kernels, but estimating more complex factorizations may require additional data and computational resources. The topic of semi-implicit variational inference BID38 and doubly semi-implicit variational inference is discussed as potential alternatives for variational inference with a deep weight prior. The method was originally developed for semi-implicit variational approximations, but recent work has generalized it for implicit prior distributions. However, further investigation is needed in this area. The lower bound L aux for the transition operator DISPLAYFORM0 is constructed to estimate gradients efficiently, with a tight bound achieved when the KL-divergence between the auxiliary reverse model and the posterior intractable distribution is minimized. The KL divergence between the auxiliary reverse model and the posterior intractable distribution is zero. The deep weight prior is a special Markov chain prior for T = 0. The auxiliary variational bound has a specific form for efficient gradient estimation. Variational inference with deep weight prior optimizes a new auxiliary lower bound on the evidence lower bound. The quality of inference depends on the gap between the original and auxiliary lower bounds, which can be estimated using a tighter but less computationally efficient lower bound. The estimate of the gap between the original and auxiliary lower bounds is not very accurate and is considered a sanity check. A multivariate normal distribution is examined, and a closed-form maximum-likelihood estimation is used for parameters over the source dataset. The gaussian prior performs marginally worse than other priors and fails to approximate the source distribution well. The network architectures for MNIST and CIFAR-10/CIFAR-100 datasets are detailed in Table 3, including Conv and ConvT layers with specific parameters."
}