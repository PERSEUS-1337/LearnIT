{
    "title": "SJgRf659Ur",
    "content": "In 2019, many businesses still receive scanned documents in non-digital format with ink artifacts that can interfere with recognition algorithms. DeepErase is a neural preprocessor that erases ink artifacts from text images, improving downstream recognition accuracy by OCR software like Tesseract 4.0. The method involves training a segmentation network in a weakly supervised manner and testing on out-of-distribution datasets like scanned IRS tax return forms. DeepErase improves recognition accuracy on scanned IRS tax return forms by removing ink artifacts, benefiting OCR software like Tesseract 4.0. OCR tools are widely used for transcribing text images to text strings, especially in industries still reliant on paper documents for data entry. Handwriting entries in documents pose a challenge due to handwritten strokes. When extracting text regions from documents with handwriting entries, spurious strokes can encroach into other text regions, degrading recognition algorithm performance. Many document text recognition datasets lack artifacts, but real-world documents often contain ink artifacts that can affect OCR software accuracy. DeepErase is a deep learning model designed to remove ink artifacts from document text images. Training is weakly supervised using a simple artifact assembler program. The performance of DeepErase is evaluated by passing the images with artifacts through the model. DeepErase improves word accuracy on text recognition tools by 40-60% on validation set and 14% on NIST datasets. It is related to semantic segmentation for document analysis. The works of Calvo-Zaragoza et al. [2017] and K\u00f6lsch et al. [2018] leverage fully convolutional architectures for semantic segmentation tasks in document analysis. Our task is more challenging as it involves distinguishing complex artifacts within documents. Our approach to artifact removal in printed and handwritten text images is novel and utilizes a weakly supervised method, unlike previous works that require full supervision. Our method focuses on distinguishing complex artifacts within documents, using binarized images and neighborhood spatial structure for segmentation. Our approach to artifact removal in text images utilizes a weakly supervised method, achieving low test error and improving recognition accuracy on text recognition engines like Tesseract 4.0. Using a fully convolutional network, we map the input image to a binary segmentation mask to identify artifacts, effectively cleansing the image. The approach to artifact removal in text images involves using a fully convolutional network to map the input image to a binary segmentation mask, cleansing it from artifacts. A corpus of dirty images paired with segmentation masks is automatically assembled for training data, including printed and handwritten text. The network is trained, validated, and tested on this data, with experiments code available at https://github.com/yikeqicn/DeepErase. Various types of artifacts are focused on, such as machine-printed underlines, fill-in-the-blank boxes, smudges, and handwritten spurious marks. The process involves creating segmentation masks for artifact removal in text images by superimposing art onto the image. Various types of artifacts are used, including machine-printed underlines, fill-in-the-blank boxes, smudges, and handwritten marks. Sampling from the IAM handwriting dataset is used for artifacts, and 5000 crops of lines and boxes are extracted from scanned forms. Automated extraction is done by applying image registration and cropping techniques. The process involves creating segmentation masks for artifact removal in text images by superimposing art onto the image. Various types of artifacts are used, including machine-printed underlines, fill-in-the-blank boxes, smudges, and handwritten marks. The dataset is sampled from the IAM handwriting dataset, and 5000 crops of lines and boxes are extracted from scanned forms. The clean and artifact images are binarized to prevent reliance on shading differences. An offset is sampled to translate the artifact image, ensuring randomness and overlap with text characters. The artifact image is then superimposed onto the clean images by taking the lower intensity pixel. The process involves creating segmentation masks for artifact removal in text images by superimposing art onto the image. A segmentation mask should contain all the markings of the artifact image minus the markings of the clean image. A U-net architecture is used to predict a segmentation mask of artifact or no-artifact for each pixel. The segmentation mask prediction model uses a U-net architecture to downsample feature maps via maxpooling and upsample them via deconvolution. The training objective is to minimize cross entropy loss with median frequency balancing to address class imbalance. No regularizers are used in the training process. DeepErase utilizes data augmentation and the RMSProp optimizer to minimize the objective. It is compared to Hough-transform line detector and Manual Supervision CNN approaches without ImageNet pretraining. The authors manually annotated document text images for training, highlighting the need for weakly supervised approaches to achieve high model performance. The Manual Supervision approach was evaluated alongside other methods on datasets containing only line artifacts for a fair comparison. The error rates for Manual Supervision and DeepErase were lower on line-artifacts-only data. Validation of the Hough approach was done on a split of the full validation set. The IRS dataset consists solely of line artifacts. Two metrics were used for performance evaluation: segmentation error and baseline comparison on clean text images. The baseline for evaluating artifact detection performance includes ground-truth segmentation masks with no-artifact annotations. Recognition error metrics, such as Character Error Rate (CER) and Word Error Rate (WER), are used to assess the effectiveness of cleaning artifacts for improved recognition accuracy. The recognition baseline is established using clean original images without artifacts, utilizing Tesseract v4 for printed text and SimpleHTR for handwritten text. Performance is evaluated on a held-out set from dirty datasets, with LSTM-CTC architecture used in both software. The dirty dataset is derived from a base dataset, and the recognition models are tested on the original base images for performance evaluation. Using DeepErase, segmentation error is less than 5% on printed and handwritten text, achieving better results than Hough transform-based line removal and Manual Supervision approaches. Good segmentation leads to improved recognition performance, with recognition accuracy improving by 60.56% and 31.20% when artifacts are erased before inputting into Tesseract or SimpleHTR. DeepErase-cleaned images also achieve 20-60% lower downstream recognition word error compared to other cleaning methods. The DeepErase method achieves better segmentation results with less than 5% error on printed and handwritten text, leading to improved recognition performance. Cleaned images show 20-60% lower recognition word error compared to other methods. However, when compared to the \"gold standard\" base images, cleansed images have about 15-30% higher recognition error. The system is tested on IRS tax return forms to evaluate its performance in the wild, where data distribution shifts can impact model performance. DeepErase algorithm significantly reduces recognition errors on IRS printed and handwritten data compared to other methods, showcasing its effectiveness in artifact removal from document text images. Despite challenges with handwritten data, visually inspected erased images indicate successful artifact removal for improved downstream recognition engine performance. The method presented programmatically assembles realistic text artifact images to train DeepErase for semantic segmentation, resulting in a 40 to 60% boost in recognition accuracy for printed and handwritten text. On the IRS dataset, DeepErase improves recognition accuracy by about 14% for both types of text. Visual inspection shows visually convincing results. Future steps include better modeling of the test distribution during artifact generation to improve model performance at test time. The appendix provides further details on the benchmark \"Manual Supervision CNN\" model's training method. The model, although less accurate than DeepErase, was able to help remove artifacts with limited labeled training data and potentially less accurate manual labeling. The \"Manual Supervision CNN\" model, although less accurate than DeepErase, was able to help remove artifacts with limited labeled training data and potentially less accurate manual labeling. The removal could be incomplete or overactive. See result images in Figure 6 and Figure 7."
}