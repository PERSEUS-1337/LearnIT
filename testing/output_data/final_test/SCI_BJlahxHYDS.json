{
    "title": "BJlahxHYDS",
    "content": "Obtaining high-quality uncertainty estimates is crucial for deep neural networks. A scheme for estimating uncertainties based on sampling from a prior distribution is theoretically justified in this paper. The uncertainty estimates are conservative and converge to zero with more data. Experimental evaluation shows that uncertainty estimates from random priors outperform deep ensembles in computer vision tasks. Deep neural networks require high-quality uncertainty estimates for decision-making systems. Good uncertainty estimates are crucial for safety in automated systems that can potentially harm humans. Sequential decision-making also relies on uncertainty estimates for quick and optimal performance. Non-Bayesian inference processes may be sub-optimal, emphasizing the importance of relatable Bayesian uncertainty estimates. Deep ensembles struggle to relate uncertainty estimates to Bayesian inference due to training on the same data, leading to overconfident estimates. Monte-Carlo dropout offers a form of Bayesian inference but requires a limit or generalization of variational inference. In practice, Monte-Carlo dropout can lead to overconfident estimates. Bayesian Neural Networks maintain a distribution over weights but are difficult to train and achieve a good posterior approximation. Another method for obtaining uncertainties in deep networks is through fitting random priors, which are easy to train and have shown practical success. To obtain uncertainty estimates, a predictor network is trained to fit a prior, with examples shown in Figure 1. Uncertainties are obtained from the difference between predictors and priors, with errors measured against a novel input point. This method of estimating uncertainties has practical success and goes beyond Bayesian linear regression. The text provides a theoretical framework for obtaining uncertainty estimates by fitting random priors, specifically for neural networks with any architecture. The uncertainty estimates are shown to be conservative and converge to zero with infinite data. Empirical evaluation demonstrates improved performance over deep ensembles and MC dropout in computer vision tasks. The approach is based on stochastic processes. The text introduces the notations for stochastic processes, emphasizing exchangeable outputs and finite-dimensional distributions. Inference on stochastic processes is compared to probability distributions, where prior processes can be updated with observed training points and labels. The text discusses the posterior process of a set of training points and labels, denoting the variance and mean of the Gaussian process. Uncertainties from random priors are explained with the use of networks in Figure 1. The text explains how uncertainties are estimated by training a predictor network to match a prior network. The squared error between the prior and predictor is used to obtain uncertainties, which are shown as shaded blue areas in Figure 1. The error decreases as more data is observed, similar to Gaussian process inference. The prediction mean is obtained using a separate neural network to avoid overconfidence. The process of obtaining network uncertainties involves randomly initialized prior networks, which are a crucial component of our method. Randomly initialized networks have been used as priors to achieve state-of-the-art performance in computer vision tasks. The use of random priors has practical and theoretical benefits, as they do not overfit and convey information about the relationship between data points. The process of training predictor networks involves using random priors, which have been shown to achieve state-of-the-art performance in computer vision tasks. Random priors satisfy the likelihood principle and are a safe choice, assuming the network architecture is suitable for the task. Training involves generating random priors, using them as labels for supervised learning, and fitting the model. After training predictor networks with random priors, uncertainty estimates are obtained using a formula involving sample mean and standard deviation of squared errors. The hyper-parameter \u03b2 controls the consideration of uncertainty. Each predictor-prior pair provides independent uncertainty estimates. Network architecture details are discussed in Section 5. In Section 3, a process for obtaining uncertainties in deep learning is introduced. The expected uncertainties are defined as \u03c3 2 \u00b5, related to Bayesian inference. The uncertainties obtained are proven to be conservative in Section 4.1, ensuring the algorithm is never overly certain. This is further elaborated in Section 4.2. In Section 4.2, uncertainties concentrate with more data, justifying their use in applications. It is safer to overestimate uncertainty than underestimate it. Amortized conservatism ensures \u03c3 2 \u00b5 is never smaller than the average posterior uncertainty. The text discusses amortized conservatism in relation to neural networks and Gaussian Processes. It highlights the relationship between prior processes and neural network initialization, showing that randomly initialized neural networks approach GPs as layer width increases. This connection provides additional structure to the prior process used in Algorithm 1. Neal (1996) extended the concept to various network architectures, including CNNs and RNNs. Recent studies show that as hidden layer size increases, randomly initialized neural networks converge to a Gaussian Process. This connection enhances the prior process in Algorithm 1 for uncertainty estimates. In the context of uncertainty estimates, it is reasonable to consider the prior to be a Gaussian Process (GP) for a large enough network. The GP assumption only needs to hold for the prior network, which is never trained. Strict conservatism is defined by the posterior GP variance, ensuring uncertainty estimates are never too small. This confirms that random priors do not overfit, as outlined in Proposition 1 for guaranteeing strict conservatism. The text discusses conservatism in uncertainty estimates for Gaussian Processes (GPs). It introduces a lemma showing conservatism for expected uncertainties and provides a guarantee for conservatism even with just one bootstrap. The text also mentions the requirement of knowledge of an upper bound for variance and provides an upper bound lemma. The text discusses conservatism in uncertainty estimates for Gaussian Processes (GPs) and introduces a lemma for expected uncertainties. It mentions the need for an upper bound for variance and provides a sample-based estimate for conservatism. The text also emphasizes the importance of concentration for uncertainty estimates to be useful. The text discusses concentration for uncertainty estimates in neural networks, assuming Lipschitz continuity and bounded predictors. Proposition 2 shows that uncertainties concentrate as training error approaches zero for large training sets. The text discusses the reasonableness of assuming zero training error for large training sets in neural networks. Recent results on deep learning optimization support the idea that stochastic gradient descent can find representable functions. The algorithm defined in Section 3 aims to improve uncertainty estimates using this theory. The conservatism guarantee in Proposition 1 applies to any predictor architecture. In theory, the predictor for uncertainty estimates could be arbitrary and doesn't need to be a deep network. The predictor architecture mirrors the prior but with additional layers for more representational power. Choosing the number of bootstraps B is required, but even using B = 1 produces high-quality uncertainty estimates. Proposition 1 and Proposition 2 apply to any Gaussian Process prior, allowing for modeling of both epistemic and aleatoric uncertainty. The predictor architecture mirrors the prior with additional layers for more representational power. The amount of aleatoric uncertainty can be adjusted by choosing \u03c3 2 A. The architecture allows for modeling both epistemic and aleatoric uncertainty. After training on two points, predictors agree with priors everywhere, leading to zero uncertainty estimates. The architecture of prior networks must be expressive enough to avoid weight copying. Inspired by Randomised Prior Functions, our work builds on it but differs in theoretical justification and application beyond Bayesian linear regression. Our results are more general than Randomised Prior Functions (RPFs) as they apply to any deep network with or without added noise. While RPFs sample functions from the posterior, our algorithm estimates posterior uncertainty at a test point. We justify uncertainty estimates and quantify errors, unlike Burda et al. (2018) who focused on Reinforcement Learning with RPFs. They only evaluated on MNIST, while we provide a formal justification and error quantification. In Section 7, uncertainties are evaluated on complex vision tasks, contrasting with the evaluation on the MNIST dataset. Prior networks refer to deep networks outputting prior distribution parameters, different from the approach discussed. Deep ensembles, an alternative method for uncertainty in deep learning, maintain multiple models to measure uncertainty. However, they can lead to overfitting due to learning similar representations for inputs with similar labels. In practice, deep ensembles can give overconfident uncertainty estimates in vision tasks, especially on points with the same label as training data. Our method provides conservative estimates and complements deep ensembles, particularly when conservative estimates are more important. Deep ensembles require more bootstraps for the same out-of-distribution performance and lack theoretical support when all members are trained on the same data. Dropout is used in practice for obtaining uncertainties when training multiple networks is not feasible. Monte-Carlo dropout is seen as a form of approximate Bayesian inference, but requires unnatural approximations. It can also be interpreted in terms of MAP inference and as an ensemble method with shared parameters. This view is considered as natural as the Bayesian interpretation. Bayesian Neural Networks (BNNs) explicitly model weight distributions in neural networks, providing a link between deep learning and Bayesian inference. Despite technical optimizations, BNNs are slow to train compared to supervised learning. Modern convolutional Bayesian Neural Networks (BNNs) still face accuracy penalties in realistic settings of prior variance. The study evaluates uncertainties for calibration and out-of-distribution detection, comparing to deep ensembles and spatial concrete dropout methods. The study compared uncertainty estimates for out-of-distribution detection using deep ensembles, spatial concrete dropout, and dropout methods. Results showed that random priors performed slightly better than deep ensembles with adversarial training for B = 1 and about the same for B = 10. Dropout was found to be cheaper but performed worse in the evaluation. The study compared uncertainty estimates for out-of-distribution detection using deep ensembles, spatial concrete dropout, and dropout methods. Dropout was cheaper to train but performed worse. Uncertainties from random priors were well-separated with B = 1, while deep ensembles needed more bootstraps for separation. Calibration was measured for accuracy based on uncertainty levels. Additional experimental results included OOD accuracy and evaluation on CIFAR 100. The study compared uncertainty estimates for out-of-distribution detection using deep ensembles, spatial concrete dropout, and dropout methods on CIFAR-10 dataset. Results showed that uncertainty estimates from random priors were more stable and showed monotonicity on a finer scale. Increasing the number of bootstraps only slightly improved calibration. Out-of-distribution AUROC for the models on subsampled data was also evaluated. In a previous experiment, architectural and optimization choices were kept constant across algorithms to ensure a fair comparison. However, to evaluate random priors with near-zero training error, a smaller set of training images was used while maintaining the same network architecture. This led to near-complete convergence. The uncertainty estimates for various algorithms were compared using seen and unseen data from CIFAR-10, with random priors showing uncertainties as \u03c3^2 and other algorithms using 1 - max(p \u00b5) for ensemble models. In a sub-sampled regime, the random prior method outperformed competing approaches, showing better calibration and good separation between seen and unseen data. Out-of-distribution benchmarks also favored the random prior method over baselines. An ablation test was conducted to assess the algorithm's robustness to weight initialization scaling, with results showing a relationship between initialization scale and AUROC performance on the CIFAR-10 task. The random prior method demonstrated robust out-of-distribution performance and competitive uncertainty estimates with fewer bootstraps. It outperformed deep ensembles in a regime where near-zero loss training is achievable, showing conservative uncertainties across neural network architectures. The method's theoretical justification supports its use for uncertainty estimation in deep learning. In the 1D regression experiment, feed-forward neural networks with 2 layers of 128 units each were used, achieving near-zero training loss. For CIFAR-10 experiments, the model architecture was adapted from cifar10-fast, with the same architecture for the network predicting the mean. The uncertainty estimators utilized a prior network with a final linear layer for uncertainties and predictor networks with additional layers. In the CIFAR-10 experiment, predictor networks were modified with additional layers for uncertainty estimation. Output size was set to M = 512, Adam optimizer with learning rate 0.0001 was used. Initialization scale was optimized as 2.0. Training error was 0.57 \u00b1 0.20 for CIFAR-10 and 0.03 \u00b1 0.02 for the sub-sampled ablation experiment. The training error was 0.57 \u00b1 0.20 on the CIFAR experiment and 0.03 \u00b1 0.02 on the sub-sampled ablation. Out-of-distribution classification was done using AUROC measurements with confidence intervals provided in Table 3 for different methods like random priors, deep ensembles, deep ensembles with adversarial training, and spatial concrete dropout. Accuracy figures on the same OOD tasks were also provided. Accuracy figures for out-of-distribution tasks were obtained through cross-validation and are presented in Tables 4 and 5. In addition, experiments were conducted on the CIFAR-100 dataset, with results reported in Figures 8 and 9, and Tables 7 and 8. The results from experiments on the CIFAR-100 dataset support the same conclusions as previous experiments. The relationship between uncertainty and accuracy for different values of B is shown. In well-calibrated models, accuracy increases as uncertainty decreases. The minimizer of the Mean Squared Error is the expected value of f. Gaussian Processes (GPs) are used for regression to learn unknown functions from noisy observations. GPs express the posterior process in a tractable way using a mean function and covariance function. They model epistemic uncertainty (lack of knowledge) and aleatoric uncertainty (reflecting variability). When performing regression with Gaussian Processes (GPs), we start with a zero-mean prior GP(0, k) and observe training points X = {x i } and labels y = {y i }. The posterior process on GP(\u00b5 Xy , k X ) models aleatoric noise, with the mean and covariance expressed as \u22121 y and (13) respectively. The posterior covariance is independent of y, using the kernel matrix for train-test correlations. The text discusses the notation and symbols used for variance in Gaussian Processes regression. It also presents a proof involving conditional probability and posterior mean, highlighting the exchangeable outputs assumption. The text presents a proof involving Gaussian Processes regression and conditional probability, showing strict conservatism in expectation and finite bootstraps. The text presents a proof involving Gaussian Processes regression and conditional probability, showing strict conservatism in expectation and finite bootstraps. In the proof, a class of predictor networks called H U of Lipschitz networks is defined with specific properties. Lemma 4 provides a bound on the expected uncertainty for predictor networks that follow specific conditions, ensuring Lipschitz continuity, output exchangeability, symmetry around zero, and boundedness. The lemma considers a target function with a restricted domain and introduces a constant U to limit the outputs. The proof utilizes Rademacher tools to establish a bound on the expected uncertainty for predictor networks. It introduces the Rademacher complexity of a function class and applies a generic Rademacher bound to analyze possible errors of the predictor. The function class models per-output squared error and does not depend on the output index. Talagrand's Lemma is used to further analyze the situation. Lemma 4 relates training set error to test set error, showing they are closer for small Lipschitz constant L. Proposition 2 states that if training converges, uncertainties concentrate as N and B approach infinity. Lemma 4 shows the relationship between training set error and test set error, indicating they are similar for small Lipschitz constant L. Proposition 2 suggests that uncertainties concentrate as N and B increase towards infinity. This implies that \u03c3^2 \u00b5 is continuous in x due to the continuity of f and h Xf. The expression under the expectation is non-negative for every x, and the right-hand side does not depend on B. Additionally, the definition of v \u03c3 is considered."
}