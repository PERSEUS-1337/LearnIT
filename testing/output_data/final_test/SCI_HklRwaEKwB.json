{
    "title": "HklRwaEKwB",
    "content": "In ridge regression, we analyze the structure of the estimator, optimize cross-validation for regularization parameter selection, and improve computation speed while maintaining accuracy in a large-data linear model. Ridge regression is represented as a linear combination of the true parameter and noise dependent on the covariance matrix. We propose a bias-correction for choosing the regularization parameter in $K$-fold cross-validation. Primal and dual sketching for ridge regression are shown to be accurate. Ridge regression is widely used for prediction in high-dimensional data. Ridge regression is Bayes optimal for quadratic loss in a Gaussian linear model. The regularization parameter selection is crucial in practice. In ridge regression, regularization parameter selection is crucial. Cross-validation is commonly used for this purpose. Fast approximate algorithms using sketching methods have been developed. The method works well in linear models with n, p \u2192 \u221e. Ridge regression is approximated by a linear scaling of true parameters perturbed by noise. In ridge regression, regularization parameter selection is crucial. Cross-validation is commonly used for this purpose. The scaling matrices are functions of the population-level covariance of the features. Formulas for training error and bias-variance tradeoff of ridge are derived. Bias of cross-validation for selecting the regularization parameter is studied, with a proposed bias correction procedure. Randomized sketching algorithms for ridge regression are analyzed for accuracy loss and computational cost reduction. Our work leverages recent results from asymptotic random matrix theory and free probability theory to improve ridge regression performance by reducing costs and error rates. The analysis involves finding the limit of trace tr (\u03a3 1 + \u03a3 \u22121 \u22121 /p, where \u03a3 1 and \u03a3 2 are independent sample covariance matrices of Gaussian random vectors. This work is connected to prior research on ridge regression in high-dimensional statistics and wireless communications. El Karoui (2018) and Dicker (2016) focus on ridge regression estimators for identity covariance. Hastie et al. (2019) study \"ridgeless\" regression with regularization parameter approaching zero. Sketching, a popular research topic, is explored by various authors such as Vempala (2005), Halko et al. (2011), Mahoney (2011), Woodruff (2014), and Drineas & Mahoney (2017). Zhang et al. (2013a; b) study sketched ridge regression in a finite-sample setting. Chen et al. (2015) propose an algorithm combining sparse embedding and SRHT for relative approximation bounds. Wang et al. (2017) analyze iterative sketching algorithms for primal and dual problems. Dobriban & Liu (2018) investigate sketching using random matrix theory for unregularized cases. In Liu (2018), sketching is studied using random matrix theory for unregularized linear regression. Chowdhury et al. (2018) propose a data-dependent algorithm based on ridge leverage scores. The paper discusses results on representation, risk, and bias-variance tradeoff, bias of cross-validation for regularization parameter selection, and accuracy of randomized primal and dual sketching. Additional proofs and simulations are provided in the Appendix. The linear regression model Y = X\u03b2 + \u03b5 is used, with X as datapoints in p dimensions and Y as continuous responses. Code for experiments can be found at https://github.com/liusf15/RidgeRegression. In ridge regression, we estimate the coefficient \u03b2 by solving an optimization problem with a regularization parameter \u03bb. The empirical spectral distribution (ESD) of a symmetric matrix \u03a3 is used as a tool in random matrix theory. The ESD is defined by the eigenvalues of \u03a3. The study works in a \"big data\" asymptotic limit where both the dimension p and sample size n tend to infinity. The ESD is a useful tool derived from the eigenvalues of a matrix. It summarizes information such as the mean and range of the distribution. In models with a sequence of covariance matrices converging to a limiting distribution, results become simpler. The ESD of the n \u00d7 p matrix X is the ESD of X X/n. Specific data models assume X = U \u03a3 1/2, where U has iid entries. This implies that the datapoints have a specific form x i = \u03a3 1/2 u i, where u i have iid entries. The \"true\" covariance matrix \u03a3 is typically not known. The ESD is a useful tool derived from the eigenvalues of a matrix, summarizing information such as the mean and range of the distribution. In models with a sequence of covariance matrices converging to a limiting distribution, results become simpler. The \"true\" covariance matrix \u03a3 is typically not observed, and models for the data in random matrix theory are common. Deviations between the empirical covariance matrix \u03a3 = n \u22121 X X and the population covariance matrix \u03a3 can be characterized precisely, with applications in estimating the true covariance matrix and studying high dimensional statistical learning problems. Our work focuses on statistical learning problems like classification and regression. We define deterministic equivalents for random vectors u n and v n, allowing for linear combinations to be well approximated. By extending scalar functions to matrices, we utilize prior results from random matrix theory. The text discusses the representation of the ridge estimator in statistical learning problems, focusing on spectral decomposition and the design matrix. It considers scenarios with random designs and Gaussian noise, emphasizing the convergence of the empirical spectral distribution. The ridge regression estimator is represented as a sum of two terms: the true coefficient vector scaled by matrix A(\u03a3, \u03bb), and the noise vector Z scaled by matrix B(\u03a3, \u03bb). The noise term Z is directly linked to the original regression problem's noise, making the estimator dependent on it. This representation is precise and highlights the recovery of the \"signal\" by ridge regression. The proof relies on random matrix theory and the \"calculus of deterministic equivalents\". For n close to p, the empirical covariance matrix \u03a3 is not a good estimator of the true covariance matrix. The resolvent bias factor c p quantifies the deviation of linear functionals of \u03a3. The resolvent bias factor c p quantifies the deviation of linear functionals of \u03a3. It is well defined and can be viewed as a resolvent bias factor that affects the evaluation of the resolvent (\u03a3 + \u03bbI) \u22121. For uncorrelated features, the ridge regression estimator simplifies to scalar multiplication, allowing for the determination of bias in individual coordinates. The theorem characterizes the behavior of linear combinations of estimator coordinates in ridge regression. It can be used to find bias but not variance directly. Additional assumptions on the parameter structure can help derive the MSE of the estimator in other ways. The regression parameter \u03b2 is random with zero mean and normalized variance. The optimal \u03bb for this setting is always \u03b3\u03c3^2/\u03b1^2. Ridge regression estimator with \u03bb = p\u03c3^2/(n\u03b1^2) is the posterior mean of \u03b2. Loss functions include mean squared estimation error and training error. Theorem 2.2 discusses MSE and training error of ridge regression under certain assumptions. The asymptotic MSE and residual error of the ridge regression estimator \u03b2(\u03bb) show a bias-variance tradeoff. Large \u03bb increases regularization, reducing variance but increasing bias. Explicit formulas for bias and variance as a function of \u03bb are derived. Optimal \u03bb * = \u03b3\u03c3^2/\u03b1^2 demonstrates the effect of dimensionality on the problem. Variance initially increases then decreases with \u03b3. This analysis provides insight into high-dimensional asymptotic problems. The variance in ridge regression increases then decreases with \u03b3, showing a bias-variance tradeoff. In high-dimensional cases, most risk is due to bias, while in low-dimensional cases, it is due to variance. This representation may have applications in statistical inference, such as debiasing techniques for regression coefficient inference. Choosing the regularization parameter is commonly done through cross-validation, but it may have bias in estimation. In this section, a bias-correction method for the optimal regularization parameter in cross-validation is proposed. The data is split into subsets for training and validation, with ridge regression used for error estimation. The study involves 1000 datapoints, 90 sub-datasets, and a test error analysis. The study proposes a bias-correction method for the optimal regularization parameter in cross-validation. It involves splitting data into subsets for training and validation, using ridge regression for error estimation. The analysis includes 1000 datapoints, 90 sub-datasets, and test error evaluation. The study proposes a bias-correction method for the optimal regularization parameter in cross-validation. The bias-corrected parameter is calculated as \u03bb * = \u03b3\u03c3 2 /\u03b1 2. This correction does not depend on unknown parameters and aims to improve the minimizer of the cross-validation estimator. Numerical examples show that the debiased estimator gets closer to the optimal \u03bb, but may not significantly improve test error in all cases. The bias-correction method aims to improve the minimizer of the cross-validation estimator by shrinking the regularization parameter. Simulation results show a decrease in test error, but significant improvement is not always observed. The bias-correction idea also applies to train-test validation and has potential beyond ridge regression. In ridge regression, CV selects regularization parameters that may be too large. The computation complexity of ridge regression can be intractable in modern large-scale data analysis. Sketching is a popular approach to reduce time complexity by random projection or sampling. Primal sketching approximates the sample covariance matrix to improve efficiency. Dual sketched ridge regression reduces computation cost by approximating the Gram matrix XX with XRR X using orthogonal or Gaussian sketching matrices. Asymptotic MSE is studied for both methods, with full sketching also mentioned. Primal sketching with orthogonal projections can be implemented through subsampling or Haar distributed methods. Sketching with orthogonal projections can be implemented using subsampling, Haar distributed matrices, or subsampled randomized Hadamard transforms. The standard Marchenko-Pastur law is the limit of the ESD of a matrix with iid standard Gaussian entries. The primal sketched ridge regression is computed with an orthogonal matrix L, with MSE having a limit determined by the standard Marchenko-Pastur law. The proof in Section A.6 decomposes the MSE into variance and squared bias, determined by the Marchenko-Pastur law F \u03b3 and \u03bb. An orthogonal complement L 1 simplifies calculations, introducing Gaussian random variables. Figure 3 confirms theory, showing sketching minimally affects MSE. The sample size is halved, increasing MSE by a factor of 1.05, demonstrating the effectiveness of sketching. Variance is compromised more than bias. The robustness to tuning parameter \u03bb is explored, showing sketching is robust to different regularization parameters. The next theorem discusses dual orthogonal sketching in the context of increasing dimensions. The proof structure and simulations follow a similar path to previous sections. The simulation results in Figure 11 support the theory that sketching has favorable properties, with bias increasing less than variance. Optimal tuning parameters for primal and dual sketching are challenging to determine analytically, so a numerical approach is used. Investigating extreme projections where the sketching dimension is much smaller than the sample size is of special interest. The special case with \u03b6 = 0 simplifies the formula for MSE significantly. The optimal \u03bb * is \u03b3\u03c3 2 /\u03b1 2 + 1 + \u03b3, with the optimal MSE being M (\u03bb). Marginal regression's optimal MSE is small compared to the zero estimator \u03b1 2 when \u03b3(\u03c3 2 /\u03b1 2 + 1) + 1 is large. Marginal regression performs well when SNR is small or aspect ratio \u03b3 is large. For example, with \u03b1 2 = \u03c3 2 = 1 and \u03b3 = 0.7, the marginal MSE is approximately 0.58. The optimal ridge MSE is around 0.52, resulting in a ratio of about 1.1. In this section, Gaussian sketching is studied, focusing on the bias of dual Gaussian sketching in the high SNR regime. The proof, based on free probability theory, characterizes performance when \u03b1/\u03c3 \u2192 \u221e. The same result holds for matrices with iid non-Gaussian entries, with a more technical proof. The function m is the Stieltjes transform of the free additive convolution of standard and scaled inverse MP laws. Theoretical results on the convolution of standard and scaled inverse MP laws are validated through numerical simulations. The MSE of dual sketching can be lower than ridge regression when the regularization parameter is suboptimal. Gaussian dual sketching converges to ridge regression as d grows. The bias of primal Gaussian sketching is discussed using free probability theory in the Appendix. Additional simulations are presented for validation. In additional simulations, primal and dual sketching can reduce computational cost depending on the relationship between the dimensions p and n. The unique positive solution of the fixed point equation is denoted by cp. The equivalence of two sequences of growing matrices is discussed using the calculus of deterministic equivalents. Linear combinations of entries in one matrix can be approximated by entries in another matrix. Linear combinations of entries in matrix A can be approximated by matrix B. The rank of matrix M is at most n when n < p, but cp\u03a3(cp\u03a3 + \u03bbI)\u22121 can be a full rank matrix. For random matrix X, the null space is a random max(p \u2212 n, 0) dimensional linear space. The matrix M will not contain a fixed vector \u03b2 in its null space with high probability. Asymptotic equivalence for ridge regression is derived in Figure 5 simulation with n = 1000 and \u03bb = 0.3. The simulation in Figure 5 for ridge regression with n = 1000 and \u03bb = 0.3 shows the theoretical lines plotted according to Theorem 2.2. The MSE is normalized by the norm of \u03b2, and the derivative c p = dc p /dz has been calculated in Dobriban & Sheng (2019). The final answer for risk analysis is shown in Figure 5. The final answer for risk analysis is derived by proving Theorem 2.2, showing a good match between theory and simulation. The MSE of \u03b2 is analyzed under certain assumptions, leading to the decomposition of the limiting MSE into bias and variance components. Specific forms of these components are illustrated in Figure 1. The bias increases with \u03bb, starting at zero for \u03bb = 0 and increasing to \u03b1 2 as \u03bb \u2192 \u221e. The variance decreases with \u03bb, from \u03b3\u03c3 2 x \u22121 dF \u03b3 (x) to zero. When \u03b1 2 and \u03c3 2 are comparable, the bias is influenced more by \u03bb when \u03b3 is small, and the variance is influenced more when \u03b3 is large. The bias and variance change with \u03b3 at the optimal \u03bb * = \u03b3\u03c3 2 /\u03b1 2, representing the effects of dimensionality on the problem. The analysis explores the behavior of the best estimator (ridge regression) with increasing dimension. The overall risk increases as the problem becomes harder. The bias-variance tradeoff is summarized by an equation showing the dependence of bias and variance on \u03bb. The variance initially increases then decreases with \u03b3, indicating that in high-dimensional cases, bias contributes more to the risk than variance. In high-dimensional cases, bias plays a significant role in the overall risk of the best estimator (ridge regression). A bias-correction procedure is used to shrink \u03bb in the correct direction and reduce test error. The one-standard-error rule does not perform well in this scenario. Training and test sets are generated, and cross-validation is performed to determine the optimal \u03bb. The green dashed line shows the optimal \u03bb for ridge regression. Using the average of ridge estimators from cross-validation with CV-optimal regularization parameters can improve prediction accuracy without refitting. However, bias in the regularization parameter affects train-test validation results. Retraining the ridge regression estimator on the whole dataset can significantly enhance performance. The text discusses using ridge regression for prediction and the shortcut for leave-one-out estimation. The method described has the same asymptotic performance as a train-test split. Figure 8 compares different cross-validation methods for ridge regression with n = 500, p = 550, \u03b1 = 20, \u03c3 = 1, K = 5. The methods include kf, kf refit, kf bic, and tt validation. The error bars represent mean and standard deviation over 20 repetitions. Regularization parameter comparison between different cross-validation methods for ridge regression with n = 500, p = 550, \u03b1 = 20, \u03c3 = 1, K = 5. Naive estimators can be inaccurate without refitting or bias correction, but accuracy improves with these adjustments. No significant difference observed between methods. Proof of Theorem 4.1 involves the resolvent of the sketched matrix under certain assumptions. The bias of \u03b2 is calculated using the properties of Wishart matrices. The AMSE of \u03b2p is determined in the isotropic case where X has iid N(0,1) entries. The proof of Theorem 4.2 involves the calculation of the sketched data matrix P. The bias of \u03b2 is calculated using Wishart matrices properties. The sketched data matrix P is calculated by observing that P = LX has iid normal entries. The operator norm of P P/n vanishes, leading to the trace concentrating around p/\u03bb 2. The optimal \u03bb and its objective value can be found elementary. The law of p) needs to be determined, and W and G \u22121 are asymptotically freely independent. The additive free convolution W \u1e20 needs to be found, involving the R-transform of distributions. The Stieltjes transform of G \u22121 involves free probability notions. A series expansion is used to find an expression for A. A and B are asymptotically freely independent, leading to an alternating sequence involving free random variables. The text discusses free random variables a and b with their respective laws, being freely independent. The results of primal and dual sketching techniques are compared, showing that primal sketch is preferred in this case. Optimal regularization parameters are found for both techniques, with both increasing bias but decreasing variance. The text discusses primal and dual sketching techniques, showing that both increase bias but decrease variance. Optimal regularization parameters are found for both techniques. For all settings, refer to Figure 13."
}