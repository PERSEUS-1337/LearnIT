{
    "title": "r1lUOzWCW",
    "content": "The study focuses on training and performance of generative adversarial networks using Maximum Mean Discrepancy (MMD) as critic, known as MMD GANs. It clarifies bias issues in GAN loss functions and discusses the choice of kernel for the MMD critic. MMD GANs benefit from training strategies developed for Wasserstein GANs and show comparable performance with simpler and faster training. An improved measure of GAN convergence is also proposed. Generative Adversarial Networks (GANs) provide a powerful method for generative modeling of datasets by learning a generator function to mimic a target distribution. The Kernel Inception Distance is introduced as a measure of GAN convergence, allowing for dynamic adaptation of learning rates during training. GANs offer an alternative to standard maximum likelihood approaches, focusing on producing high-quality samples rather than likelihoods. Generative Adversarial Networks (GANs) focus on producing plausible samples, with impressive examples of image generation. GANs are challenging to train due to the discriminator class and minimizing divergences between generator and target distributions. The GAN and reference probability measures are likely supported on manifolds within a larger space. The Jensen-Shannon divergence, KL and reverse-KL divergences may not provide useful gradients for GAN training due to the lack of sensitivity to distance between probability measures. Integral Probability Metrics (IPMs) address this issue by measuring the distance between distributions via well-behaved witness functions, signaling proximity in probability mass. The Wasserstein distance and maximum mean discrepancy are integral probability metrics used in GAN variants to measure the distance between distributions. WGAN-GP improves on the original Wasserstein GAN implementation by directly constraining the gradient of the discriminator network, making it more stable and easier to train. Generative adversarial models based on minimizing the Maximum Mean Discrepancy (MMD) have been explored in previous works. These models optimize a generator to minimize the MMD with a fixed kernel, but struggle with complex natural images. Adversarial training of the MMD loss, defined on the output of a convolutional network, has shown promise in advancing these methods. Recent work has utilized the Integral Probability Metric (IPM) representation of the MMD for regularization strategies. The MMD representation in generative adversarial models employs witness function regularization strategies to stabilize convolutional features during training. Various constraints, such as weight clipping and gradient constraints, have been utilized to improve model performance. The Cram\u00e9r GAN method also incorporates gradient constraint strategies in its discriminator network. The Cram\u00e9r GAN critic incorporates gradient constraint strategies in its discriminator network, which differs from the energy distance. Various optimization approaches, such as variance features and constraints, can also be effective in GAN training. The Wasserstein distance and MMD have differences in their use for GAN training, as optimizing the empirical Wasserstein distance can lead to biased gradients for the generator. The energy distance is proposed as a solution to biased gradients in optimization, particularly in comparison to the Wasserstein GAN. The MMD GANs offer advantages over Wasserstein GANs, with unbiased gradients when using a fixed deep network representation. In comparison to Wasserstein GANs, MMD GANs show strong discriminative performance between distributions when using characteristic kernels. They achieve the same generator performance as WGAN-GPs with smaller discriminator networks, resulting in GANs with fewer parameters and faster training. The MMD GAN discriminator combines convolutional mappings and a kernel layer for faster training. It uses the maximum mean discrepancy as a metric to measure the distance between probability measures. The integral probability metrics family includes Wasserstein and Kolmogorov metrics, using witness functions to distinguish between probability measures. The Wasserstein-1 metric is defined by 1-Lipschitz functions, total variation by functions with absolute value bounded by 1, and Kolmogorov metric by functions of bounded variation 1. The witness function class F in this work is the unit ball in a reproducing kernel Hilbert space H with a positive definite kernel. The reproducing property of the Hilbert space is defined as f(x) = f, k(x, \u00b7) H for all f \u2208 H. The mean embedding of a probability measure P in a reproducing kernel Hilbert space H is defined as \u00b5 P = E X\u223cP k(\u00b7, X). The maximum mean discrepancy (MMD) is an integral probability metric with the unit ball in H. The characteristic kernel, like the Gaussian RBF kernel, is injective when the kernel is characteristic. However, exponential decay of the kernel and its derivatives can cause issues in high dimensions and gradient-based representations. The rational quadratic kernel with \u03b1 > 0 is a scaled mixture of exponentiated quadratic kernels, with a Gamma(\u03b1, 1) prior on the inverse lengthscale. It is superior to the exponentiated quadratic kernel in tail behavior and is commonly used in experiments. The MMD has been widely used as a critic in GANs, with recent works focusing on computing MMD through a mapping function before comparing samples. The MMD with kernel \u03ba(x, y) = k(h(x), h(y)) is crucial in maximizing the MMD without collapsing the critic early in training. Strategies from training the Wasserstein GAN critic can be applied to training the MMD critic, with weight clipping being a common approach, although it has drawbacks such as oversimplifying loss functions and requiring slower optimizers. The approach of regularizing the critic witness by constraining its gradient norm to be nearly 1 along randomly chosen convex combinations of generator and reference points is preferred over standard approaches like Adam. This regularization helps prevent the critic function from becoming too flat, making it difficult for the generator to follow its gradient. This approach was also followed by Bellemare et al. (2017). The energy distance is proposed as the critic in an adversarial network, measuring divergence between probability measures. Various GAN variants fall under the framework of IPMs, motivating the application of gradient penalty to original GANs. Biased gradients in GANs were highlighted, showing bias in the gradients of the empirical Wasserstein distance for finite sample sizes. The energy distance is proposed as the critic in an adversarial network, measuring divergence between probability measures. Bellemare et al. highlighted biased gradients in GANs, showing bias in the gradients of the empirical Wasserstein distance for finite sample sizes. They claimed that the energy distance used in the Cram\u00e9r GAN critic does not suffer from these problems. The proof that the gradient of the energy distance is unbiased was incomplete, with the essential step in the reasoning assumed. It is shown that one can exchange the order of expectations and derivatives under mild assumptions about the distributions, network form, and kernel. The text discusses the use of the energy distance as a critic in adversarial networks, highlighting biased gradients in GANs. It mentions the interchangeability of expectations and derivatives under certain assumptions about distributions, network form, and kernel functions. The proof of unbiasedness in the gradient of the energy distance is incomplete, with a complex proof provided in Appendix C. The proof of unbiasedness in the gradient of the energy distance in WGANs is complex due to the non-differentiability of ReLU-like functions. The generator minimizes the loss function based on an estimator, which is shown to be biased in Theorem 2. This bias likely extends to the gradient as well, as shown in Theorem 4. The bias in the estimator of the energy distance in WGANs extends to the gradients as well, as shown in Theorem 4. The bias diminishes with better selection of parameters, and no bias is introduced by using a fixed minibatch. The bias in WGANs is not introduced by using a fixed minibatch size but by the optimization procedure for \u03b8 and the total number of samples seen in training the discriminator. MMD GANs may be considered \"less biased\" than WGANs. Holding the critic fixed and optimizing the generator in MMD GANs leads to the GMMN model, where the correct distribution is always optimal. In comparing GAN models, quantitative comparisons are challenging. The Inception score, proposed by Salimans et al. (2016), evaluates GAN methods based on the predictive distribution of images. This metric correlates somewhat with human judgement. The Fr\u00e9chet Inception Distance (FID), proposed by Heusel et al. (2017), measures the similarity of samples' representations in the Inception architecture to those of samples from the target distribution. It fits a Gaussian distribution to hidden activations for each distribution and computes the Fr\u00e9chet distance between those Gaussians. Unlike the Inception score, the FID worsens monotonically with the addition of artifacts to images. The Kernel Inception Distance (KID) is proposed as a metric similar to the Fr\u00e9chet Inception Distance (FID), measuring the squared MMD between Inception representations using a polynomial kernel to avoid correlations with the objective of MMD GANs. Unlike the FID, the KID does not assume a parametric form for the distribution of activations, providing several advantages. The Kernel Inception Distance (KID) is a metric similar to the Fr\u00e9chet Inception Distance (FID) but does not assume a parametric form for the distribution of activations. It compares skewness, mean, and variance, converges quickly to its true value, and has a simple unbiased estimator. The KID demonstrates empirical bias of the FID and unbiasedness by comparing CIFAR-10 train and test sets. The FID estimator shows persistent bias even at large sample sizes, highlighting the need for comparisons at the same sample size. Differences in bias can be observed when comparing different pairs of distributions, as demonstrated in the appendix. The FID estimator is biased even with large sample sizes, emphasizing the importance of comparing distributions at the same sample size. The estimator may be positive but never negative, and no unbiased estimator of FID exists. The MMD estimator scales like O(n^2d), suggesting the use of a small n and averaging over estimates. The FID estimator is slower for d = 2048, and Monte Carlo estimates of variance may be misleading. Comparing FID estimates poses risks due to potential bias and misleading low variance. In contrast, KID estimates are unbiased and asymptotically normal. In models on MNIST, Inception featurization is replaced with features from a LeNet-like convolutional classifier. The diagnostic test of Arora & Zhang estimates the number of \"distinct\" images by a GAN, but subjectivity in defining duplicate images makes reliable comparisons challenging. Further exploration is needed to address subconscious biases in model comparisons. In supervised deep learning, it is common practice to dynamically reduce the learning rate of an optimizer when it has stopped improving the metric on a validation set. However, this is not common in GAN-type models, so learning rate schedules must be tuned manually. A proposed adaptive scheme suggests comparing the KID score for samples from previous iterations to the current iteration to determine if the model is closer to the validation set. If not, it is marked as a failure. The text discusses using the KID score to determine model performance in GAN-type models. It compares samples generated by MMD GAN with WGAN-GP and Cram\u00e9r GAN on various datasets using different architectures. In the study, the number of neurons in the critic for MMD losses was optimized, with 16 top-layer neurons used. The generator utilized standard convolutional filters, while the critic compared networks with 16 and 64 filters in the first convolutional layer. The architecture of the DCGAN involved doubling the number of filters in each consecutive layer. In the higher-resolution model for the CelebA dataset, a 5-layer DCGAN critic and a 10-layer ResNet generator were used, with 64 convolutional filters in the last/first layer. The critic size significantly impacts training runtime, with smaller critics running faster. Different MMD GAN kernel functions were evaluated in experiments, including the linear kernel. In experiments, various MMD GAN kernel functions were used, including linear, exponentiated quadratic, rational quadratic, and distance-induced kernels. Different lengthscales and parameters were explored, with the addition of a linear kernel to the rational quadratic mixture. Models were trained with batch sizes of 64 and specific numbers of discriminator updates per generator update. Training durations varied for different datasets. For LSUN and CelebA, models were trained for 150,000 generator updates, while for MNIST it was 50,000. The learning rate was initially set to 10^-4 and adjusted using an adaptive scheme. KID was compared between the current model and the one 20,000 generator steps earlier. The gradient penalty was scaled by 1 for MMD models and by 10 for distance kernel models. Quantitative scores were based on 25,000 generator samples and compared to dataset elements or test sets. The models were trained for LSUN and CelebA datasets with 150,000 generator updates, while for MNIST it was 50,000. Inception and FID scores were computed using bootstrap resamplings and KID score was estimated based on sampling 1,000 elements without replacement. Models achieved good results visually and quantitatively, with some models showing slower convergence than others. The evolution of quantitative criteria throughout the training process is shown in FIG3. Based on the training results for LSUN, CelebA, and MNIST datasets, it is recommended to use rq kernels over rbf in MMD GANs. Small-critic MMD GAN models perform comparably to large-critic WGAN-GP models at a lower computational cost. It is suggested to limit experiments to rq and dist kernels for other datasets. Additionally, an activation penalty was found to be helpful in certain MMD models to prevent floating-point precision issues. In LSUN Bedrooms dataset, MMD GANs outperform Cram\u00e9r and WGAN-GP in terms of KID and FID scores. Small-critic MMD GANs produce good samples, while small-critic Cram\u00e9r GANs have less sharp objects in the pictures compared to MMD rq* samples. Inception scores are not meaningful for LSUN dataset due to domain differences from ImageNet class labels. The MMD GAN with rq* kernel outperforms WGAN-GP and Cram\u00e9r GAN in KID and FID scores for the CelebA dataset. The use of MMD on deep convolutional features as a GAN critic results in visually pleasing generated images with fewer unrealistic ones compared to WGAN-GP and Cram\u00e9r GAN. The RQ kernel outperforms the distance kernel in GAN training experiments. The surrogate loss interpretation is linked to the score function associated with the energy distance. The expected score under Q is greater or equal to the expected score for P when a score is proper. A divergence measure can be defined based on this score, with the energy distance arising from the score function. The interpretation involves comparing the average distance of a reference sample to a generator sample. The Cram\u00e9r GAN critic does not directly use the energy distance in its approach. The Cram\u00e9r GAN critic utilizes a mapping function h to maximize discriminative performance. The energy distance is broken down as D e (P, Q) = S(Q, Q) \u2212 S(P, Q). Training the discriminator aims to maximize divergence by learning h. An alternative objective is to optimize the average score (17) to increase the distance between generator and reference samples. Control over the term encoding is lost in this approach. The text discusses controlling the expected norm to mitigate the variability in generator samples. It compares this approach to the Cram\u00e9r GAN critic but notes differences in scaling and an additional term being maximized. The necessity of the additional term for the function to be a witness of an integral probability metric is also mentioned. The text discusses biased estimators of IPM-like distances and their gradients, showing that they are invariably biased in nontrivial cases. It also demonstrates that any estimator with non-constant bias yields a biased gradient estimator, with specific examples provided for Wasserstein and maximized-MMD distances. Additionally, a slight generalization of IPMs is defined to analyze MMD GANs. The text discusses biased estimators of IPM-like distances and their gradients, showing that they are invariably biased in nontrivial cases. It also demonstrates a generalization of IPMs to analyze MMD GANs, involving a class of probability measures on a domain X with a parameter set F and an objective function J. The estimator for the distance between two distributions P and Q is unbiased for fixed functions f. The text discusses the selection of a critic function independently of test data, with three key components: train-test split sizes, selection procedure for the critic function, and the estimator. Different selection procedures can be used, such as regularization, with an approximate optimizer. This approach was used in a two-sample testing setting by Lopez-Paz & Oquab (2017). The training/test split matches the GAN training process, where the loss is computed on a minibatch while the critic's parameters are based on previous data. The text discusses the bias in data-splitting estimators for GANs, showing that unbiased estimators do not exist. The selection procedure for the critic function and the estimator are key components in this analysis. The text discusses bias in data-splitting estimators for GANs, showing that unbiased estimators do not exist. The estimator has a downward bias if X tr , Y tr are independent of X te , Y te. The bias vanishes as X tr , Y tr converge to their optimum. The minibatch size only determines X te , Y te, not the bias. Many estimators of IPMs do not perform data splitting, instead estimating D(P, Q) with the distance between empirical distributions. The text discusses bias in data-splitting estimators for GANs, showing that unbiased estimators do not exist. Estimators of IPMs often estimate D(P, Q) using the distance between empirical distributions. The standard biased estimator of the MMD, IPM estimators of BID, and empirical Wasserstein estimator are all biased. There is no unbiased estimator of D on a class of distributions. The text discusses the challenges of finding unbiased estimators for IPMs in GANs. It highlights the limitations of existing estimators and the difficulty in extending unbiased estimators to generalized IPMs. The focus is on the biased nature of estimators and the implications for gradient bias in GAN settings. The text discusses challenges in finding unbiased estimators for IPMs in GANs, focusing on the biased nature of existing estimators and the difficulty in extending unbiased estimators to generalized IPMs. Theorem 4 shows that unbiased gradients are crucial for accurate estimation, but bias dependency on parameters makes this unlikely. The text discusses challenges in finding unbiased estimators for IPMs in GANs, focusing on the biased nature of existing estimators and the difficulty in extending unbiased estimators to generalized IPMs. Theorems 2 and 3 hold for WGANs, showing that paths of certain forms are sufficient for connected components of \u03a8. Theorems also apply to WGANs or WGAN-GPs with F as the set of functions attainable by the critic architecture. The estimate is biased downwards for nontrivial distributions P and Q, and Theorem 3 demonstrates this on reasonable families of input distributions. The bias in gradients for IPMs in GANs is not constant, as shown by Theorem 3. An example using Wasserstein case with 1-Lipschitz functions reveals how critic selection affects estimates, with a \"stubborn\" selection procedure needed to achieve a specific result. The bias in gradients for IPMs in GANs is not constant, as shown by Theorem 3. The selection procedure for critics can affect estimates, with a \"stubborn\" approach needed to achieve specific results. The optimization scheme for parameters may not always yield perfect answers, leading to biased gradients. The bias in gradients for IPMs in GANs is not constant, as shown by Theorem 4. An example is provided with a one-dimensional projection of two-dimensional data using a linear kernel. The MMD GAN estimator is discussed, with a numerical simulation showing a value around 0.6. Theorem 1 is proven as a corollary to Theorem 5, which exchanges gradients and expectations of deep networks. The gradient and expectation can be guaranteed using measure theory (see Proposition 1) and the Dominated Convergence theorem (Proposition 2). The property Proposition 1.(ii) is crucial for differentiability around \u03b8. However, for a neural network with ReLU activation, this assumption may not hold. An example is given with a simple function where differentiability fails in a ball of possible \u03b8 values. The function is not differentiable on a set of possible \u03b8 values, which can have positive measure for many distributions. Theorem 5 proves that derivatives and expectations can be exchanged outside of a \"bad set\" \u0398 P without relying on Proposition 1.(ii). Lemma 1 controls the rate of change in neural networks without using the mean value theorem. Proposition 3 shows that the set \u0398 P has zero Lebesgue measure. The text discusses Proposition 1 and Proposition 2, which involve differentiation and convergence theorems in probability distributions on R^d. These propositions require technical considerations in topology and differential geometry. The text discusses general feed-forward networks with a directed acyclic computation graph G, consisting of nodes sorted in a topological order. Each node computes a function based on its input, and the network factorizes according to the graph G recursively. The functions can be affine transforms or linear modules, involving known linear operators on weights. The text discusses non-linear functions in feed-forward networks, denoting augmented vectors and parameters. It mentions conditions on non-linear functions and defines the feature vector of the network. The top-level function applied to certain inputs is also discussed. The function K in DISPLAYFORM5 represents the kernel function of an MMD GAN, where X is the two inputs stacked together and processed in parallel. K has different smoothness assumptions than preceding layers, with growth conditions and disjoint sets D. An example is max-pooling computation with K = 2. When f i computes max-pooling on two inputs, K i = 2 and each domain D k i corresponds to a half plane. The function f i is analytic on the whole space, defined by a single function of G i,1,1. This case applies to differentiable functions in deep learning like softmax, sigmoid, and batch normalization. Other activation functions like ELU are piecewise-analytic. The main result implies Theorem 1 via Corollaries 1 to 3, with the proof depending on various intermediate results. The proof shows that for almost all \u03b8 0, the function h \u03b8 (X) is differentiable. By considering a converging sequence (\u03b8 n ) to \u03b8 0, it is shown that E P [K (h \u03b8 (X))] is differentiable at \u03b8 0. This result is applied to specific GAN architectures, such as WGANs using the noise distribution Z. The distribution Z represents the noise distribution in WGANs. Corollary 1 states that for most (\u03b8, \u03c8), certain conditions are satisfied. Corollary 2 applies to Original GANs with bounded output from the discriminator network. The log function is real analytic and Lipschitz on a certain interval. A kernel condition implies another assumption. MMD GANs involve distributions P and Z, with generator and critic networks satisfying specific assumptions. Augmented networks are considered for the proof. The functions h(1), h(2), and h(3) satisfy specific assumptions, and Theorem 5 applies to each. Various kernels in the paper meet Assumption E with \u03b1 at most 2. The proof of Theorem 1 is finalized by the existence of moments and lower-order moments. When i is not a linear layer, f(i) is M-Lipschitz by Assumption C. When i is not a linear layer, f(i) is M-Lipschitz by Assumption C. Functions can be obtained by recursion under Assumptions A to C. Lemma 2 states properties for \u03b8 in B(\u03b8, R) and X in R^d. The proof involves inequalities and derivatives, concluding with functions being continuous and bounded on the ball B(\u03b8, R). Lemma 3 states that a real-valued function f is differentiable at \u03b8 0 with a differential g. The proof involves the sequential characterization of limits. Proposition 3 introduces additional notation for paths through the network's computational graph. The computational graph defines backward trajectories starting from nodes, sets of parameters on boundaries, critical parameters where the network is not differentiable, and input vectors where the function is not differentiable. The set of parameters for which the network is not differentiable has 0 Lebesgue measure for any distribution. By using Fubini's theorem, it can be shown that the set of nondifferentiability of continuous functions is measurable, leading to the conclusion that the set of parameters where the network is not differentiable also has Lebesgue measure of 0. By applying Fubini's theorem, it is proven that the set of parameters where the network is not differentiable has Lebesgue measure of 0. This is further supported by Lemma 4, which states that under Assumption D, the set \u0398X has 0 Lebesgue measure. Additionally, Lemma 5 shows that if \u03b8 is not on the boundary of a specific set, there exists a trajectory where the network is differentiable, contradicting the assumption. The proof involves constructing a trajectory within a ball B(\u03b8, \u03b7) where a real analytic function f_q is defined, replacing nonlinear functions with analytic ones. By recursion on network nodes, if i = 0, h_0\u03b8 = X is real analytic. If \u03b8 is not on the boundary of a set, there exists a trajectory where the network is differentiable. The proof involves showing that for small enough \u03b7, a trajectory within a ball B(\u03b8, \u03b7) is contained in a set of index triples. By contradiction, it is demonstrated that \u03b8 belongs to a specific set, leading to the conclusion that h_i\u03b8 = f_q0(\u03b8) for all \u03b8 in B(\u03b8, min(\u03b7, \u03b7)). The proof involves showing that for small enough \u03b7, a trajectory within a ball B(\u03b8, \u03b7) is contained in a set of index triples. By contradiction, it is demonstrated that \u03b8 belongs to a specific set, leading to the conclusion that h_i\u03b8 = f_q0(\u03b8) for all \u03b8 in B(\u03b8, min(\u03b7, \u03b7)). The proof then proceeds by recursion, showing that for any \u03b8 in a certain set, there exists a pair (p, q) such that \u03b8 is in M_s with zero Lebesgue measure. The text discusses the bias behavior of the FID estimator and the Fr\u00e9chet Inception Distance between distributions. It mentions the relationship between normal distributions and the application of FID to Inception coding layers. The proof involves showing that a trajectory within a ball is contained in a set of index triples, leading to the conclusion that certain sets have zero Lebesgue measure. The FID is a well-defined pseudometric between distributions with existing first two moments. The plug-in estimator estimates mean and covariance to calculate FID. In some cases, FID(P1, Q) < FID(P2, Q), but the estimator may show the opposite with an equal number of samples. The FID is a pseudometric between distributions with first two moments. The plug-in estimator calculates FID using mean and covariance. Appendix D.3 shows there is no unbiased estimator for FID. The estimator can perform poorly even with simple distributions. The FID is a pseudometric between distributions with first two moments, calculated using mean and covariance. The estimator can perform poorly even with simple distributions. In a more realistic setup, the hidden codes of an Inception coding network are better modeled by a censored normal distribution ReLU(X), where X \u223c N (\u00b5, \u03a3) and ReLU(X) i = max(0, X i ). The FID estimator's performance can vary with different sample sizes and distributions. Comparing models based on FID estimates may not reliably determine the correct ordering, especially with close true values. The FID estimator's unreliable performance in determining model comparison emphasizes the importance of unbiased estimators like the natural KID estimator. There is no unbiased estimator for all distributions, as shown by Bickel & Lehmann (1969). The one-dimensional case illustrates the mean and variance of a mixture distribution. The FID estimator is not a polynomial function when dealing with two-component Gaussian mixtures, making it impossible to have an unbiased estimator for an arbitrary fixed normal distribution. This analysis does not provide insight into the existence of an unbiased estimator for normal distributions. Due to the practical use of non-normal distributions for FID, a practical unbiased estimator is not feasible. The behavior of Inception and FID scores is examined as images are increasingly disturbed, with consideration of the KID. The FID, KID, and Inception scores for CelebA and CIFAR-10 are compared as disturbance level \u03b1 is increased. In a slightly different setting from previous research, the Inception score shows a monotonic relationship with increasing noise on more disturbance types. After training on the CIFAR-10 dataset, it was observed that the Inception score's non-monotonicity is sensitive to experimental settings. Different variants achieved reasonable results on the MNIST dataset after 50,000 generator iterations. The rbf kernel often produced blurry outputs during training due to fast gradient decay. Scores for various models trained on CIFAR-10 are shown in TAB5. Rational-quadratic and Gaussian kernels retain sample quality with reduced discriminator complexity, generating good quality samples with the standard DCGAN discriminator."
}