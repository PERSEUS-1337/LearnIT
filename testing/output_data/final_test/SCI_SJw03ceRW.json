{
    "title": "SJw03ceRW",
    "content": "In this work, a learning framework is introduced to expand a pre-trained deep network for classifying novel classes with few examples. The method, called hard distillation, adds additional weights to the base network for classifying new classes while maintaining the old class responses. This approach excels in low-shot training scenarios due to the small number of weights that need to be trained. Distillation is effective for low-shot training and does not harm classification performance on base classes. Low-shot network expansion can be achieved with a small memory footprint using a compact generative model of base classes' training data. In scenarios where a deep network needs to classify new classes quickly, a simple and fast classifier expansion is necessary. This can be achieved by updating the classifier with only a few images of the new object, without accessing the entire training set. Fine-tuning the network is a common solution for classifier expansion but requires keeping a large amount of base training data in memory. In scenarios where a deep network needs to classify new classes quickly, a simple and fast classifier expansion is necessary. This can be achieved by updating the classifier with only a few images of the new object, without accessing the entire training set. Fine-tuning the network is a common solution for classifier expansion but requires keeping a large amount of base training data in memory. However, fine-tuning can lead to degradation of network accuracy on base classes, known as catastrophic forgetting. Humans can instantly learn novel categories with few training examples without compromising previously learned abilities. This work introduces a low-shot network expansion technique to augment the capability of an existing network trained on base classes. The technique involves augmenting an existing network trained on base classes by adding parameters in the last layers to classify novel classes. A hard distillation framework is used to ensure low-shot and no-forgetting constraints are met. Hard distillation enforces that the output of the trained network matches the output of the mimicked network as a hard constraint by learning only the newly added weights. Network expansion with hard distillation enhances the base network's knowledge to classify new classes with minimal additional parameters. In low-shot scenarios, hard distillation outperforms soft distillation, maintaining inference time similar to the base network. To save memory, a compact generative model using a Gaussian Mixture Model is utilized, reducing low-shot training time. The paper introduces a fast method for low-shot network expansion using a generative GMM model. A benchmark for low-shot expansion is defined, with tests of increasing complexity. The main contributions are a hard-distillation solution, GMM as a generative model, and a new benchmark. A Nearest Class Mean (NCM) classifier was proposed by BID7 to address the class-incremental learning problem, using a single prototype example for each class. This method overcomes the fixed feature representation issue by learning a new distance function in the feature space through metric learning. The NN classifier in this paper aims to expand existing deep classifiers to classify novel classes, focusing on low-shot network expansion. This is different from one-shot learning approaches like Matching Networks. BID3 introduced low-shot learning with a regularization technique and hallucination of training examples, while our method maximizes performance in low-shot scenarios by expanding the representation based on novel data. In this work, the iCaRL method is discussed as a class-incremental learning approach for large training sets, focusing on low-shot network expansion. It involves updating feature representation and class means from a small stored number of examples, minimizing a combined classification and distillation loss. The method is compared to other approaches for low-shot network expansion. BID11 proposed the Progressive Network for adding new tasks without affecting the performance of old tasks by freezing parameters trained on old tasks and expanding the network with additional layers. BID15 introduced Progressive learning to solve online sequential learning in extreme learning machines paradigm by incrementally learning the last fully-connected layer with additional parameters for novel class samples. BID14 proposed an incremental learning technique augmenting the base network with additional parameters in the last fully connected layer to classify novel classes. In contrast to previous methods like iCaRL and phantom sampling, our proposed solution involves expanding the last fully connected layer of a base network to classify novel classes and improve feature representation without retraining the base network parameters. The extended feature representation is learned from samples of both base and novel classes. The feature representation is learned from samples of base and novel classes. A deep neural network is trained on K base classes with the full set of training data, partitioned into a feature extraction network and a classification network. The whole network is represented as a composition of two networks. The feature representation is learned from samples of base and novel classes, with a deep neural network trained on K base classes. The network is composed of two networks, f net (x) = f cls (f rep (x)), where f cls is the classification network. Expanding the classification network to classify additional classes involves adding a new weight vector and a new normalization factor. The goal is to maintain classification accuracy on base classes while classifying new ones in a low-shot scenario. During training, the classification network is expanded to classify new classes while preserving the base classes weight vectors to prevent over-fitting. A generative model of the base classes is used to draw samples during training due to memory constraints. During training, a generative model of the base classes is utilized to draw samples. The Gaussian Mixture Model (GMM) is used as an approximate generative model of the data from the base classes, focusing on class conditional distribution of feature representation. This approach aims to reduce training time by allowing fast sampling. During training, a GMM is used as a generative model for base classes' data, focusing on class conditional distribution of feature representation. The Deep Feature GMM model assumes feature independence with diagonal covariance matrices. Memory requirements for storing base class information are O(M KN). Despite its compact representation, the Deep Features GMM shows minimal accuracy degradation compared to using full data for training a classifier. The proposed method involves applying data augmentation to input samples of novel classes, passing them through a feature extraction network, and expanding the classification subnetwork to classify novel classes. Training batches consist of base classes feature vectors. The training batch for the proposed method includes base classes feature vectors and samples of a novel class. Each iteration updates weights using SGD with gradient dropout, which helps escape saddle points. The procedure described in the previous subsections expands the last classification layer without changing the feature representation space. To expand the feature representation, new parameters are added to deeper layers of the network, requiring an appropriate expansion of subsequent layers. The feature representation expansion does not affect the network output for the base classes, as weights connecting the expanded representation to the base classes are set to zero. In this section, a low-shot network expansion method is evaluated on classification tasks with different scenarios. The benchmark measures the performance of alternative low-shot methods in real-life problem scenarios, ranging from easier tasks to harder ones. Three scenarios are defined: generic novel classes, domain-specific with similar novel classes, and domain-specific with dissimilar novel classes. The expansion of the feature representation space is detailed in Appendix D. The study evaluates a low-shot network expansion method on classification tasks with different scenarios. The scenarios include generic novel classes, domain-specific with similar novel classes, and domain-specific with dissimilar novel classes. The proposed method is compared to alternative low-shot learning methods using the ImageNet dataset. The study uses the UT-Zappos50K BID17 shoes dataset for fine-grained classification, selecting 10 classes with over 1,000 training images each. The base network is fine-tuned on these classes, and similarity is measured using a confusion matrix. The classes are then randomly partitioned into 5 base and 2 novel classes, enforcing similarity between classes in different scenarios. Visual similarities are represented in a confusion matrix and illustrated in FIG0 in Appendix C. In the proposed method, the VGG-19 network trained on ImageNet is used for feature extraction. The classification subnetwork is fine-tuned on 5 selected base classes while freezing the rest of the layers. The method is referred to as Generative Low-Shot Network Expansion (Gen-LSNE) and is compared to NCM and Prototype-kNN methods. Multiple prototypes are used for each class in the extended NCM classifier. The Prototype-kNN classifier BID4 uses multiple prototypes for each class in a fixed feature space of the VGG-19 network's FC2 layer. A Deep Features GMM model with 20 mixtures is fitted for each base class, and the centroids of the base and novel feature vectors are considered as prototypes. The classification rule is based on a majority vote among the k nearest neighbors, with k being the smallest number of prototypes per class. In low-shot learning scenarios, a soft distillation method inspired by iCaRL is proposed as an alternative to the hard distillation constraint. The method updates only the last two fully connected layers while using a combination of distillation and classification loss. This adaptation is necessary due to the limited number of novel class samples in low-shot scenarios. The soft distillation method proposed in low-shot learning scenarios updates the last two fully connected layers with a combination of distillation and classification loss, aligning with BID14 and BID15. This method allows for adjustment to new data, while the hard-distillation method freezes the last two layers and trains only the new parameters without a distillation loss. In low-shot learning scenarios, the soft distillation method updates the last two fully connected layers with a combination of distillation and classification loss. This allows for adjustment to new data. A technique proposed in Section 3.3 involves using gradient dropout regularization on SGD to improve convergence and overcome overfitting. Ablation experiments are conducted to assess the importance of gradient dropout, comparing soft distillation (Soft-Dis) and proposed hard distillation (Gen-LSNE) with and without gradient dropout regularization. In Scenario 1, the base classification network is trained on five base classes and then expanded to classify two novel classes chosen at random. The results of this experiment are presented in Figure 2. The experiment results in Figure 2 show that Prototype-kNN and Soft-Dis perform better on base classes, while the proposed method excels on novel classes with improved overall test error, especially with a small number of samples. Gradient dropout enhances accuracy, particularly with fewer than 3 novel samples, and also improves Soft-Dis results. Prototype-kNN maintains base class accuracy with high numbers of novel samples and base class prototypes. The addition of novel samples/base class prototypes has a greater impact on base class accuracy. NCM outperforms Prototype-kNN but struggles with utilizing more novel samples. Gen-LSNE performs better with fewer novel samples and outperforms all tested methods. Expansion of deeper layers is explored for a 10-class classification task. In a 10-class classification task, the feature representation after the FC1 layer of VGG-19 is expanded with 5 new features. Results show a marginal gain in Scenario 1, but a significant gain in Scenario 2 and 3 with increased samples. Gen-LSNE outperforms other methods tested, especially with fewer novel samples. The addition of five shared representation features has no effect on Soft-Dis. In an evaluation experiment, the base network is trained with full data and feature vectors are collected before the FC1 layer. A GMM model is fitted to the feature vectors of each base class, and the last two FC layers are trained from randomly initialized weights based on the GMM. The top-1 accuracy is measured on test sets for networks trained with GMM models and the base network trained with full data. Results show that learning with GMM samples causes only a negligible degradation compared to learning with full training data. Gen-LSNE is a technique for low-shot network expansion based on hard-distillation, maintaining pre-trained base parameters while training a small number of parameters for novel classes. It offers increased accuracy (up to 20%) on novel classes, minimal forgetting on base classes (<3% drop), avoids overfitting, and has fast training. This method excels with few novel images, making it practical and efficient. The Deep-Features GMM method allows for efficient base class memorization without storing the entire training set. It involves training the network from a generative compact feature-space representation of the base classes. An extended version of network expansion updates the representation by training the last deeper layers, showing effectiveness with more images of novel classes. However, increasing sample size may lead to longer training, larger memory usage, and potential forgetting. Future exploration is planned in this area. In the future, the research aims to explore hard-distillation methods for extremely low-shot classifier expansion. The fully connected layer FC1 is parametrized with weight matrix W, where V is the dimensionality of input feature vector \u03bd and N is the dimensionality of the output feature representation vector v. The weight matrix W is expanded with additional weights to be learned from novel data. The expansion of the feature representation in the FC1 layer is initialized with non-zero responses for novel samples. Weight initialization is crucial for convergence in a Low-Shot scenario. The subsequent layer FC2 is initialized with a weight matrix for base classes. The expansion of the FC1 layer is initialized with non-zero responses for novel samples, while the FC2 layer is initialized with a weight matrix for base classes. The representation vector v is expanded with additional features to allow classification of novel classes, resulting in an expanded W matrix. The hard distillation constraint requires zero-padding of W to avoid influence of expanded features on the base network output. The expansion of W, denoted as W exp, is encouraged to produce larger responses to improve learning. The initialization technique for W exp involves setting u with probability 0.5 and an amplification parameter \u0393. This technique is crucial for convergence of added weights and improving classification results in a low-shot setting."
}