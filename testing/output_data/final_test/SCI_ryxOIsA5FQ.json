{
    "title": "ryxOIsA5FQ",
    "content": "Transfer learning is used in machine learning when there are insufficient samples in the target domain, leading to poor generalization. Existing algorithms aim to reduce domain differences but still face limitations. A proposed two-phase transfer learning approach based on ensemble learning addresses these challenges by training weak learners with existing algorithms in the first stage. Transfer learning algorithms are used to train weak learners in the first stage and then use the predictions of target data to train the final learner in the second stage. The proposed method was evaluated on public datasets, showing its effectiveness and robustness. Transfer learning has gained attention since 1995 and is an important field in machine learning, aiming to solve the problem of insufficient distributed data in practical applications. Transfer learning can be categorized into four types: instance-based, feature-based, parameter-based, and. In this paper, the focus is on enhancing the performance of instance-based and feature-based transfer learning with limited labeled data from the target domain. Existing methods to prevent negative transfer learning are based on similarity measures, but similarity and transferability are not the same. The proposed novel transfer learning approach aims to address these issues. The proposed novel transfer learning architecture aims to improve the fitting capability of the final learner on the target domain by using ensemble learning and weak learners to enhance generalization. Traditional transfer learning algorithms are first applied to diversify training data, which is then fed to weak classifiers to improve generalization. The predictions of target data are vectorized for the final estimator, resulting in improved performance of transfer learning algorithms. The proposed transfer learning architecture uses ensemble learning and weak learners to improve the final learner's fitting capability on the target domain. It combines source and target data for training weak learners, followed by using a super learner to fit target data predictions. This approach simplifies parameter setting and outperforms individual estimators. The effectiveness of the method was tested by modifying TrAdaboost BID1 and BDA BID16 as base algorithms for data diversification, achieving the desired results. TransferBoost was proposed to address the rapid convergence issue of TrAdaboost by utilizing ensemble learning and weak learners to improve the final learner's fitting capability on the target domain. This method combines source and target data for training weak learners and uses a super learner to fit target data predictions, outperforming individual estimators. TransferBoost was introduced to tackle the rapid convergence problem of TrAdaboost by implementing a 2-phase training process to detect negative transfer and adjust weights accordingly. BID0 incorporated an adaptive factor in weight updates to slow down convergence, while BID19 proposed multisource TrAdaboost to enhance transfer performance using instances from multiple source domains. Feature-based transfer learning focuses on reducing distribution differences between source and target domains through methods like MMD. BID12 introduced transfer components analysis (TCA) for knowledge transfer through mapping. Transfer components analysis (TCA) introduced in 2011 achieves knowledge transfer by mapping features from source and target domains to a new space minimizing the MMD between domains. Joint distribution adaptation (JDA) addresses the limitation of TCA by fitting both marginal and conditional distributions simultaneously, utilizing pseudo-labels for unlabeled target data. BID16 extended JDA for imbalanced data, while in neural networks, knowledge transfer is facilitated through pre-training and fine-tuning. BID15, BID9, and BID10 incorporate MMD in higher layers to extract task-specific features for improved learning performance in the target domain using the source data. Researchers have highlighted the importance of determining the effectiveness of transferring knowledge from a source domain to a target domain to avoid negative transfer. Methods to evaluate relatedness between domains include introducing predefined parameters or directly examining the relatedness. This process can be labor-intensive and time-consuming when selecting proper values manually. The notion of positive transferability was introduced in BID13 to assess synergy between source and target domains in prediction models. BID14 proposed a kernel method to evaluate task relatedness and instance similarities to prevent negative transfer. BID4 suggested a method to detect negative transfer occurrences and delay them in transfer learning. BID3 warned against treating knowledge from all source domains as valuable contributions, as it could increase the risk of negative transfer. A two-phase multiple source transfer framework was proposed to effectively reduce contributions from irrelevant domains. In this paper, a source transfer framework is proposed to downgrade irrelevant contributions from source domains and evaluate their importance even with imbalanced class distributions. Stacking, an ensemble learning method, combines weak classifiers for improved performance. Diversification in stacking can be achieved through different input data subsets, output categories, and classification models. The proposed architecture can be seen as a stacking model using transfer learning algorithms for input diversification. Instance-based transfer learning is applied in this context. In this section, instance-based transfer learning is applied to the proposed architecture using TrAdaboost as an example. A modification is made to turn it into stacking. The hybrid training data set is defined, and the weight vector is initialized and updated in each iteration. The original TrAdaboost is extended to a multi-classification algorithm for experimental analysis and comparison. The Multi-class Adaboost algorithm proposed in BID21 utilizes \u03b2 t and beta for class number K. Final output for each class is defined by Equation 5. Softmax is used for single-label problems to transfer P k (x) to probabilities. TransferBoost by BID2 uses all weak classifiers for ensemble, but early stop can improve performance. Dynamic TrAdaboost by BID0 introduces an adaptive factor to limit convergence rate of source sample weight. Theoretical upper bound of training error in target domain remains unchanged in dynamic TrAdaboost. Stacked generalization for instance-based transfer learning is outlined in Algorithm 1. Algorithm 1 presents stacked generalization for instance-based transfer learning. Learner is called with labeled target data set w t to obtain a hypothesis of S. Error on S is calculated, followed by obtaining a subset of source task S t \u222a T. A hypothesis of T is obtained, and error on T is calculated. The weight vector is updated iteratively to improve generalization capability. Dynamic TrAdaboost can improve weights but may increase error rate on source domain, leading to negative transfer. Stacking is used to address these issues, with TrAdaboost used in data diversification stage. In the data diversification stage, TrAdaboost is used to reweight samples for each weak classifier. A two-phase sampling mechanism is introduced to avoid negative effects of high source weights on the task. Stacking for instance-based transfer learning introduces a two-phase sampling process and an extra parameter \u03bb, utilizing more source samples as iterations increase. Stacking is used instead of TrAdaboost. Stacking is used for instance-based transfer learning instead of TrAdaboost. It constructs a feature matrix using weak learners' outputs on target data and fits the labels with a super learner like LogitRegression. Stacking is more robust and performs better when the source domain is not closely related. It allows all weak classifiers to be used and minimizes training error on the target set. In transfer learning, various methods like transfer component analysis (TCA), joint distribution adaptation (JDA), and balanced distribution adaptation aim to align distributions between source and target domains using techniques such as MMD and kernel matrices. In domain adaptation, performance is sensitive to parameter selection such as kernel type and regularization. Using BDA as a base algorithm, the proposed architecture achieves data diversification with different kernel types and settings. By leveraging stacking, better transfer performance is achieved than with a single algorithm. The paper discusses how kernel parameters are chosen in experiments. In ensemble learning, selecting kernel parameters is crucial for performance. Weak classifiers should have diverse capabilities but not too different performances. The kernel parameter is chosen within a range where performance is acceptable. Steps include finding the best \u03c3 value, setting a threshold \u03bb, and selecting N parameters in a range through sampling. In ensemble learning, selecting kernel parameters is crucial for performance. The method involves choosing parameter sets for different types of kernels separately. The settings of \u03bb and N are important considerations, with \u03bb affecting learner performance and N impacting training data diversity. Algorithm 2 details the process of constructing kernel matrices, solving eigendecomposition problems, and training learners for feature-based transfer learning. Algorithm 2 details the method for feature-based transfer learning, where the kernel function \u03ba t is used to map the feature space. The matrix A is obtained using BDA, and predictions from A \u22a4 K tar are concatenated as features for the super learner. Modified BDA is used to adapt conditional distribution with real labels. Experiment results on 6 public datasets and 11 domains show the effectiveness of the method. The experiment results show that stacking outperforms TrAdaboost in transfer learning between mnist and USPS datasets. Stacking method achieves higher accuracy with fewer labeled target samples and requires fewer iterations to converge. It performs significantly better in both transfer tasks, with the introduction of source data leading to improved performance. In the experiment, the performance of TrAdaboost and stacking methods in transfer learning tasks was compared using different kernel functions. The ratio of source to target data was set to 5%, and various kernel types were tested. The feature representations learned by BDA under different kernel parameters were visualized to observe their impact on feature distribution. In this paper, the kernel parameters and types were varied to adapt domains A and C, showing changes in data distribution and similarity. A kernel set was constructed by sampling parameters within a specified range. Different kernel types were used to provide diversity, with 10 weak classifiers selected for each type. Comparison between single algorithms and ensemble learning was done, with Randomforest and LogitRegression as weak learners. The best accuracy, average accuracy of weak learners, and accuracy of ensemble learning were evaluated. The study compared single algorithms with ensemble learning using different kernel types and weak learners. Ensemble learning outperformed single learners in all tasks, with the integration of multiple kernels improving performance. The proposed method reduces bias introduced by hybrid training and achieves data diversification for improved classification ability in feature-based transfer learning. In this paper, a 2-phase transfer learning architecture is proposed to achieve data diversification and fitting ability on target data simultaneously. The experiment on 11 domains validates the method's effectiveness, offering advantages such as minimizing training error on target data, reducing negative transfer risk, and integrating ensemble learning for improved performance. The framework can accommodate most existing transfer learning algorithms. Further study is needed to address problems in the algorithm integration within the proposed framework. Data diversification methods like changing parameters in BDA and using multiple transfer learning algorithms could enhance the model for multi-source transfer learning."
}