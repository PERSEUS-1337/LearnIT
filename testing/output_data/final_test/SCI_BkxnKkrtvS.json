{
    "title": "BkxnKkrtvS",
    "content": "We investigate semi-supervised learning of a neural linear-chain CRF for Named Entity Recognition by incorporating a CRF in a VAE and comparing models on the Ontonotes5 NER dataset. Our best model improves performance by approximately 1% F1 in low- and moderate-resource scenarios. Named entity recognition is crucial for various NLP tasks like information extraction and entity linking. Latent-variable generative models of sentences are used to extract more signal from unlabeled data in natural language understanding tasks in NLP, such as information extraction and entity linking. State-of-the-art models treat NER as a tagging problem and have become accurate on benchmark datasets, but utilizing them for new tasks is still expensive. Extensive pretraining of high-capacity sentence encoders has helped address this issue. Latent-variable generative models of sentences, such as Variational Autoencoder (VAE), address challenges in learning and prediction by treating tags for unlabeled data as latent variables. By utilizing a neural tagging CRF as the approximate posterior, optimization issues with discrete latent tag sequences are resolved. The Perturb-and-MAP algorithm is relaxed to allow for end-to-end optimization via backpropagation and SGD. This learning approach removes the need to restrict the generative model family and explores the use of deep generative models for text with tag sequences to enhance NER performance. The VAE framework is used to learn in a biased annotation scenario, addressing semi-supervised learning for NER by utilizing a neural CRF as the approximate posterior in a discrete structured VAE. The study explores deep generative models for text tagging, showing improved performance with a joint tag-encoding Transformer architecture. The approach corrects for model performance issues in partially supervised settings and achieves high F1 scores without additional labeled data. The method is effective in low- and high-resource scenarios, demonstrating competitiveness with state-of-the-art results. In this work, the authors introduce a tagging model for Named Entity Recognition (NER) using a BILOU tag-span encoding. The model aims to predict tag sequences for tokenized text sequences, assigning tags for different span categories. The authors introduce a tagging model for Named Entity Recognition (NER) using a BILOU tag-span encoding. The model predicts tag sequences for tokenized text sequences, utilizing a neural encoding followed by a linear-chain CRF decoding layer. The experiments involve using pretrained language models such as GPT2-SM and RoBERTa-LG. The CRF model used for Named Entity Recognition involves down-projecting states, combining potentials, and computing joint distribution with trainable parameters learned on annotated sentences. The CRF-VAE treats the tagging CRF as an approximate posterior in a Variational Autoencoder, addressing loss formulations for semi-supervised and partially supervised data optimization. In this work, the focus is on optimizing objectives for semi-supervised and partially supervised data using backpropagation and the Relaxed Perturb-and-MAP algorithm. The goal is to explore generative models to improve tagging performance by estimating q \u03c6 in semi-supervised data regimes. The approach involves maximizing the evidence lower bound (ELBO) with an approximate variational posterior distribution, leading to objectives that balance supervised and unsupervised losses. The model aims to balance supervised and unsupervised losses using scalar hyper-parameters \u03b1 and \u03b2. It addresses the challenge of learning a named entity tagger on partially labeled sentences, where not all entity spans are annotated. This setup deviates from traditional VAEs by not including continuous latent variables. The model optimizes the CRF on observed tags to avoid degeneracy, incorporating a variational framework with a KL term. It reformulates the objective to handle partially observed tag sequences, optimizing using constrained algorithms. This approach can also be used for regularization of the CRF posterior. The CRF posterior is optimized by omitting the token model, leading to a loss function for a single datum. Optimization involves backpropagation and SGD, with challenges in optimizing expectation terms. Monte Carlo approximation is used with a single sample from q \u03c6, but its non-differentiability blocks gradient computation. Differentiable approximate samples from q \u03c6 are computed using the Relaxed method. The Relaxed Perturb-and-MAP algorithm is used to compute differentiable samples from q \u03c6. The prior distribution of tag sequences is modeled as a categorical distribution. Various architectures for p \u03b8 (x 1:N |y 1:N) are experimented with, including the CRF Autoencoder as a baseline model. The MF model is a generative model that embeds tag samples into vectors and computes token probabilities using an inner product. It is an extension of the CRF Autoencoder architecture but has a restrictive factorization that limits its effectiveness in capturing information from nearby tags. To address this limitation, the MF model is extended to incorporate the full tag context for better token discrimination. The MF model is extended to incorporate the full tag context by encoding the embedded tag sequence using a two-layer transformer with four attention heads per layer. The extension includes adding pretrained language modeling parameters from GPT2 to the token scores and normalizing the scales of factors to prevent GPT2 scores from overshadowing tag-encoding scores. Autoregressive extensions are also applied to MT and MT-GPT2-PoE models. The autoregressive extension of MT-GPT2 uses a product of experts factorization. The MT-GPT2-Residual model couples GPT2 with tag encoding to predict a residual. Factorizations are chosen to prevent conditioning on previous word information. GPT2 parameters are frozen to improve generative likelihood through tag encoding. The models are experimented with for SSL and PSL in a moderately resourced regime. The proposed generative models for SSL and PSL are evaluated using 10% labeled data. The best model, MT, is compared with a bidirectional encoder language model in low- and high-resource settings. OntoNotes 5 NER corpus is used for data, with various baselines including supervised and CRF Autoencoder models. AE-Approx: Tag-token pair parameterization used by CRF Autoencoder, trained with approximate ELBO objective. Simulate moderate-resource SSL with 10% labeled data. Results in Table 1 show models' performance. Prior tag distributions explored for supervised and MT models. Including a prior with a small weight of \u03b2 = 0.01 marginally improved performance in supervised and MT models. Two varieties of prior tag distributions were explored: the \"gold\" empirical tag distribution (Emp) and a hand-crafted prior (Sim) that outperformed the empirical prior. The CRF Autoencoder AE-Approx training approach outperformed the exact approach by nearly 2% F1. The addition of the joint encoding transformer architecture MT provided the most gains in performance. Evaluating supervised and transformer-based generative models on a difficult setup showed that the transformer model MT outperformed the supervised-only model by +1.3% F1. The addition of the joint encoding transformer architecture MT improved performance by +1.3% F1 compared to the supervised-only model. The MT models require higher prior weights \u03b2 = 0.1 to prevent divergence towards using the O tag more uniformly. In SSL experiments, \u03b2 > 0.01 had a negative impact on performance. The empirical prior assigns 85% mass to the O tag. The proposed MT* enhances performance in SSL and PSL by +1.1% F1 and +1.3% F1, respectively. Further exploration of MT and the supervised baseline in low-and high-resource settings is conducted. In the previous experiment, the joint encoding transformer architecture MT improved performance by +1.3% F1 compared to the supervised-only model. In the current experiment, the use of RoBERTa and a higher-capacity tagging model, S-LG, was evaluated. The CRF-VAE showed a 0.9% F1 improvement in the 1% labeled data setting, but adding additional data from Wikipedia had a negative impact due to domain mismatch. In the high-resource setting, RoBERTa significantly improves NER performance compared to GPT2. The use of S-LG further enhances performance. Semi-supervised training with Wikipedia sentences does not show significant improvement. Previous studies have explored utilizing unlabeled data for NER through a two-stage process. In the context of NER performance improvement, various approaches involve utilizing unlabeled data to create silver-labeled data for partially supervised learning. Distant supervision methods using databases or gazetteers have been explored to handle partially labeled sequences and optimize distantly annotated tags. Various methods have been explored to improve NER performance by utilizing unlabeled data for partially supervised learning. These approaches include using distantly annotated tags, cross-validated self-training, marginal likelihood objectives, and generative latent-variable models. The framework of variational autoencoders (VAEs) has shown promise in various applications such as document representation learning, sentence generation, compression, translation, and parsing. However, challenges exist in optimizing VAEs with discrete latent variables, particularly in obtaining gradients with respect to the inference model parameters. The relaxed Gumbel-max trick has been proposed as a solution for successful optimization, and it has been extended to latent tree-CRFs for sequence CRFs. In this work, a novel generative model for semi-supervised learning in Named Entity Recognition (NER) is proposed. By using a neural CRF as the variational posterior in the generative model and incorporating a transformer architecture, significant performance gains were achieved on semi-supervised and partially-supervised datasets. The inclusion of a pretrained bidirectional encoder was found to offer substantial performance improvements. Future work includes exploring larger unlabeled corpora and incorporating latent-variable CRFs in joint semi-supervised models for tasks like relation extraction and entity linking. The temperature \u03c4 can be used to anneal the perturbed joint distribution to a sample from the unperturbed distribution, allowing for end-to-end optimization. This approach enables the approximation of expectations with a relaxed single-sample Monte Carlo estimate. Carlo estimate: log p \u03b8 (x|y) computes a weighted combination of input vector representations for y \u2208 Y, similar to an attention mechanism. This generalizes the Gumbel-softmax trick to structured joint distributions. Practical implications include computing argmax, sampling, partition function, and marginal tag distributions using the same algorithm with temperature and noise control. The algorithm is detailed in Appendix B. The algorithm in Appendix B details the stable, log-space implementation of the generalized forwardbackward algorithm for computing argmax, sampling, partition function, and marginal tag distributions. Real implementations should have separate routines for computing the partition function and the discrete Viterbi algorithm for numerical stability and efficiency. Additionally, Algorithm 2 includes the dynamic program for computing the constrained KL divergence between q \u03c6 and p(y). The use of a CRF for SSL has been explored before by Ammar et al. (2014) and Zhang et al. (2017). The text discusses using free logit parameters and softmax normalization for modeling token-tag pairs in a CRF model. Training involves early-stopping based on F1 score and optimization using Adam or transformers with a learning rate of 0.001 and batch size of 128. The text discusses training models with different configurations such as using Adam or transformers with a learning rate of 0.001 and batch sizes of 128 or 32. Models are implemented in PyTorch using AllenNLP and Hugging Face frameworks, with code and data available for reproducibility. Additional training data from Wikipedia is gathered for experiments in \u00a73.2. The text describes a process of aligning English Wikipedia with Wikidata to gather sentences that align with Ontonotes5 data. This involves extracting Wikipedia entities likely to be instances of NER classes based on their ontology relationships."
}