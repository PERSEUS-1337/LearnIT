{
    "title": "HkgxheBFDS",
    "content": "Neural reading comprehension models have shown impressive generalization results but struggle with adversarial input. This work focuses on excessive prediction undersensitivity, where the model fails to change its prediction despite meaningful changes in the input text. A noisy adversarial attack was formulated to find semantic variations of comprehension questions that still yield the same incorrect answer with higher probability. SQuAD2.0 and NewsQA models were found to be vulnerable to this attack, making a significant number of errors on unanswerable questions. In Natural Language Processing (NLP), neural networks can be vulnerable to adversarial input perturbations. Adversarial training and data augmentation can decrease a model's vulnerability to undersensitivity attacks. Adversarially robust models generalize better in biased data settings and outperform conventional models by up to 11% F1. In NLP, adversarial attacks can take various forms such as character perturbations, semantically invariant reformulations, and adversarial text insertions. Model's inability to handle adversarial input text challenges generalization results and language understanding abilities. In NLP, adversarial attacks challenge model generalization and language understanding. Previous works show models can produce the same output even with drastically changed input. This work probes undersensitivity in reading comprehension models using natural language questions to improve sensitivity and robustness. SQuAD2.0 and NewsQA models are tested for sensitivity in reading comprehension by altering questions to increase model confidence, even with irrelevant information. Adversarial search is used to find alternative questions with different answers while maintaining grammatical and semantic consistency. The study found that SQuAD2.0 and NewsQA models can be attacked on a significant number of samples, indicating undersensitivity in question comprehension. Training models to defend against these attacks with data augmentation and adversarial training improves robustness without compromising standard performance. Additionally, the models show increased performance in scenarios with dataset bias, with up to an 11% improvement in F1 score. Adversarial attacks targeting the undersensitivity of neural RC models are shown to be effective. Two defense strategies, data augmentation and adversarial training, reduce undersensitivity errors without sacrificing standard performance. Robust models generalize better in biased data scenarios, improving their ability to answer questions with many possible answers. In this work, the focus is on the undersensitivity of neural RC models to semantic perturbations of the input. Various perturbations such as typos, paraphrasing, and character-level adversarial perturbations have been explored. The method is based on the idea that modifying named entities in a question can change its meaning and the answer. Our approach focuses on undersensitivity in neural RC models, where altering input can still produce the same predictions. Previous studies have addressed this issue using different approaches, such as invertible networks and max-margin training. Our work builds upon existing research by aiming to identify minimal input word sequences that do not change a model's predictions. Our work focuses on undersensitivity in neural RC models, addressing it with dedicated training objectives and testing defense methods like data augmentation. We build upon previous studies by identifying minimal input word sequences that do not alter model predictions. Additionally, datasets like SQuAD2.0 and NewsQA provide unanswerable questions for adversarial testing. Our work addresses undersensitivity in neural RC models by training on datasets like SQuAD2.0 and NewsQA, which contain unanswerable questions for adversarial testing. Despite improvements in handling unanswerable questions on test sets, the problem persists when adversarially choosing from a larger question space. In a text comprehension setting with a large set of possible answers, the goal is to find cases where the model's prediction remains stable despite changing inputs. The attack aims to discover inputs where the model erroneously predicts the same output, even with different inputs. The goal is to find inputs where the model predicts the same output despite changes. A perturbation space X T (x) is considered, containing alternative model inputs derived from x. The transformation function family T is chosen to change the correct label of these new inputs. Later, inputs x which retain the same prediction as x are searched for in X T (x). The study focuses on finding inputs that result in the same prediction despite changes, using Part-of-Speech perturbations. The perturbation space X T P (x) is generated by swapping tokens with PoS-consistent alternatives. Qualitative analysis shows that while this is a valid concern, attacks based on correct questions exist for the majority of samples. Named Entity Perturbations involve substituting named entities in questions with different type-consistent entities, creating new questions with altered specifics and different answer requirements. While not guaranteed, perturbed questions often result in unanswerable or requiring a different answer, as shown in qualitative analysis. In a qualitative analysis, perturbed questions often lead to unanswerable or different answer requirements. A search is conducted in perturbation spaces to find altered questions that result in a higher model probability for the same answer as the original question, known as undersensitivity attacks. Adversarial Search in Perturbation Space involves searching for lexical alterations to maximize prediction probability. Iterative replacements can lead to texts with larger lexical distance. Beam search is applied to narrow the search space and maximize the difference in perturbation spaces. The attack strategy involves applying beam search to maximize the difference in perturbation spaces, focusing on undersensitivity rather than semantic invariance like other attacks. This approach aims to find perturbations that alter the question meaning to destabilize the model's predictions. The attack strategy focuses on undersensitivity by altering question meaning to destabilize model predictions. Evaluating perturbed inputs with a standard forward pass is exact but less efficient, not requiring white-box access to model parameters. Experiments are conducted on a BERT model fine-tuned on SQuAD2.0 to investigate model undersensitivity when adversarially choosing input perturbations. In a preliminary pilot experiment, a BERT LARGE model is trained on a subset of the training data and achieves 78.32% EM and 81.44% F1 scores. A different training setup is then chosen to conduct adversarial attacks on a separate 5% development set, with the remaining 95% used for training. Hyperparameters are tuned using the development data, and early stopping is performed. The model is evaluated on the SQuAD2.0 development set, reaching 73.0% EM and 76.5% F1 scores. The model achieves 73.0% EM and 76.5% F1 scores on evaluation data. Perturbation spaces are computed using large sets of string expressions from Named Entity and PoS types. This results in an average of 5126 entities per entity type and 2337 tokens per PoS tag. Some PoS types are disregarded for perturbations to avoid minor changes or incorrect expressions. The study limits beam search to a maximum of randomly chosen entities or tokens at each step. A beam width of 5 is used, with a bound on computation spent on adversarial search. Adversarial vulnerability is measured by the Adversarial Error Rate, with budgets reaching high attack success rates. Named Entity substitution perturbations show lower attack success rates. The study found that BERT is vulnerable to adversarial attacks, with more than half of the samples successfully attacked. Named Entity perturbations resulted in high attack success rates, demonstrating vulnerability in SQuAD1.1. Undersensitivity attacks were also observed, with an adversarial error rate of 70% on SQuAD1.1. The study revealed that BERT is susceptible to adversarial attacks, with over half of the samples being successfully attacked. Named Entity perturbations had high success rates, showing vulnerability in SQuAD1.1. Undersensitivity attacks were also noted, with a 70% adversarial error rate on SQuAD1.1. The effectiveness of unanswerable questions added during training in SQuAD2.0 was demonstrated by a notable drop between datasets. Qualitative analysis of the attacks showed that they are noisy and may result in syntax errors or semantically incoherent questions. The study found that BERT is vulnerable to adversarial attacks, with a high success rate in named entity perturbations. Models are shown to be susceptible to undersensitivity attacks as well. The attacks are noisy and may lead to syntax errors or semantically incoherent questions. Our study found that models are vulnerable to undersensitivity adversaries, with a direct inverse link between a model's original prediction probability and sample vulnerability to an attack. Vulnerable samples have lower original prediction probabilities compared to unattackable samples. Adversarially chosen questions had a notable gap in probability compared to original questions. Additionally, vulnerable samples are less likely to be given the correct prediction overall. The evaluation metrics for vulnerable examples are 56.4%/69.6% EM/F1, compared to 73.0%/76.5% on the whole dataset. Attackable questions have an average of 12.3 tokens, while unattackable ones have 11.1 tokens on average. The most common question type is \"What,\" more prevalent among unattacked questions (56.4%) than successfully attacked questions (42.1%). This open-ended question type may explain why it is more prevalent among defended samples. The prevalence of \"What\" questions among defended samples indicates that the model is less reliant on type constraints alone for predictions, making it less prone to exploitation. Undersensitivity can occur with various entity types, particularly geopolitical entities, due to word embeddings clustering these entities closely together. This can make them challenging to distinguish for the model. Methods for mitigating model undersensitivity include data augmentation and adversarial training. A robustness objective may impact standard test metrics, with a trade-off between performance on a test set and adversarial inputs. Data augmentation involves sampling perturbed input questions, while adversarial training adds a loss term to the training objective. In adversarial training, an adversarial search is used to identify perturbed input questions. Alternative data points are fit to a NULL label for NoAnswer prediction, with continuous updates to reflect adversarial samples. Experimental setup involves training the BERT LARGE model on SQuAD2.0, tuning hyperparameter \u03bb, and using named entity perturbations for defence methods. Batch size is 16, with a cheap adversarial attack budget. Experimental Setup: The BERT BASE model is tested on NewsQA using a budget of \u03b7 = 32 and \u03c1 = 1 for adversarial attacks. Validation is done every 5000 steps with early stopping after 5 patience. Data is filtered for agreement among annotators. Results are shown in Table 3 and Table 4, indicating a reduction in undersensitivity errors with data augmentation and adversarial training. The study shows that adversarial training and data augmentation effectively reduce undersensitivity errors in the model across different datasets and budgets. Data augmentation is found to be a more effective defense strategy than adversarial training, improving robustness without sacrificing standard performance metrics. The study suggests that modified training objectives improve performance on unanswerable samples but may sacrifice some performance on answerable samples. Results show potential overfitting to specific perturbations used during training, raising concerns about generalization to new sets of data. The study evaluates the generalization of defences to new perturbations by testing models on attacks with respect to new perturbation spaces. Adversarial attacks using these new spaces show vulnerability rates and defence success transfer. For example, vulnerability ratios on SQuAD2 were 51.7%, 20.7%, and 23.8% for standard training, data augmentation, and adversarial training, respectively. More detailed results can be found in Appendix B. Results for different values of \u03b7 and NewsQA can be found in Appendix B. High-level NLP datasets often have annotation biases, leading models to exploit dataset-specific shortcuts. For instance, models may learn to pick out numbers from text to solve tasks, without considering other relevant information in the question. This lack of generalization can be seen when models trained on SQuAD1.1 struggle with articles mentioning multiple numbers. In a biased data scenario, a model trained on SQuAD1.1 questions with single type-consistent answers is tested with multiple possible answers. The test set is split into development and test data, and different models are evaluated, including a vanilla fine-tuned BERT BASE transformer model and a model trained to be less vulnerable to undersensitivity attacks. A control experiment is also conducted to assess data bias. In a biased data scenario, training a model with unanswerable samples leads to improved performance across metrics and answer type categories. Negative training signals from unanswerable questions help the model better consider relevant information, allowing it to distinguish among answer possibilities. Evaluating BERT LARGE and BERT LARGE + Augmentation Training on adversarially composed samples shows improved EM and F1 scores. Table 10 in the Appendix demonstrates that BERT LARGE with robust training enhances EM and F1 scores on both datasets, increasing F1 by 3.7 and 1.6 points. RoBERTa model trained on SQuAD2 shows lower attack rates compared to BERT, with a vulnerability rate of 90.7%. Undersensitivity in RC models can be reduced with defenses like adversarial training, leading to more robust models without compromising performance. Future research should focus on understanding the causes and improving defenses against model undersensitivity. The curr_chunk discusses the causes and defenses against model undersensitivity, providing an alternative viewpoint on evaluating a model's RC capabilities. It includes an experiment with setup details and results on undersensitivity error rates. Additionally, it mentions vulnerability results for new perturbation spaces and rare cases where model predictions increase. The curr_chunk discusses successful adversarial attacks for NER perturbations on SQuAD2.0, biased data setup experiments, and vulnerability analysis on NewsQA using named entity perturbations."
}