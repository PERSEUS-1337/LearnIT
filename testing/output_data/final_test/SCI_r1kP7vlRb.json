{
    "title": "r1kP7vlRb",
    "content": "Designing a metric manually for unsupervised sequence generation tasks, such as text generation, is challenging. Learning a metric from data is a possible solution, as proposed by SeqGAN for unsupervised sequence generation. This paper introduces two proposals to improve upon SeqGAN's metric learning: partial reward function and expert-based reward function training. By combining long-scale and short-scale partial reward functions, the goal is to evaluate both partial correctness and coherence of a sequence. In expert-based reward function training, a reward function is trained to discriminate between an expert sequence and a fake sequence. This method is not part of GAN frameworks, making generator optimization easier. The study shows improvements over SeqGAN and MLE models on synthetic and real text data. Our best model outperforms SeqGAN by 3.02 in NLL and 0.250 in BLEU on synthetic data. Generating sequential data is a key focus in machine learning, especially with RNNs. Sequential generative models with recurrent neural networks (RNNs) have been successful in sequence generation tasks. The common training method for RNNs is maximum log likelihood estimation (MLE), but it leads to exposure bias during inference. To address this bias, sequence training methods using reinforcement learning (RL) have been proposed. RL allows the RNN generator to optimize based on task-specific metrics like BLEU, rather than just log likelihood. This application of RL to sequence generation tasks is gaining importance in machine learning. The paper discusses the importance of using reinforcement learning (RL) for sequence generation tasks, particularly in cases where manually designing a task-specific metric is difficult. SeqGAN is proposed as a solution to learn a metric from data and optimize the generator accordingly, inspired by generative adversarial nets (GANs) and RL. SeqGAN employs a discriminator to distinguish between true and generated sequences, training the generator with policy gradient. It can be seen as a form of inverse reinforcement learning. The study proposes two methods to improve upon SeqGAN's metric: partial reward function and expert-based reward function training. The partial reward function evaluates incomplete sequences, addressing the burden on SeqGAN's reward function. The study introduces a partial reward function to evaluate both coherence and partial correctness of sequences, addressing the burden on SeqGAN's reward function. Unlike SeqGAN, expert-based reward function training discriminates between expert and fake sequences without using generator samples. The study investigates the effectiveness of partial reward function and expert-based reward function training for improving a generator's performance in the GAN framework. Experimental results show that models with these proposals outperform SeqGAN in both synthetic and text data experiments. TextGAN is a recent study on GAN-based text generation that assumes the task specific reward is unavailable. It employs RL for sequence generation training without access to the task specific reward. The study focuses on GAN-based text generation, optimizing the generator using the discriminator's loss gradient. It emphasizes the importance of training the discriminator in GAN context and compares the model with SeqGAN in experiments. The proposal's applicability to GAN-based sequence generation like textGAN is discussed, highlighting the novelty of using edited expert trajectories for training a neural network reward function. In SeqGAN, the dynamics of generating sequences are known, allowing for expert-based reward function training. The generator is optimized using a GAN framework and RL, learning the metric purely from data. Given a dataset of real-world sequences, the parameterized generator is trained to produce sequences from a vocabulary of candidate tokens. SeqGAN trains a generator G \u03b8 to produce sequences from a vocabulary of candidate tokens. It considers the problem as RL, with G \u03b8 producing actions (next token y t ) given states (previously generated tokens Y 1:t\u22121 ). A discriminator D \u03c6 is also trained to distinguish between real and generated data, while G \u03b8 is trained via policy gradient using D \u03c6 as a reward function. The objective of RL is to maximize the reward by updating the discriminator and generator iteratively until convergence. In sequence generation, the generator ensures sample variety by adding an entropy regularizer to prevent determinism. The partial reward function distinguishes between real and fake data for sequences of a certain length. SeqGAN utilizes a yellow reward function in its framework. SeqGAN uses a convolutional neural network (CNN) as the partial reward function, which is also employed as the discriminator in SeqGAN and textGAN. The one-hot vector representation of tokens is embedded into a k-dimensional vector, and a sequence of embedded tokens is represented as a matrix. The partial reward function takes a sequence of a certain length as input and applies filters to produce a feature map. The convolutional neural network (CNN) in SeqGAN is used as the partial reward function and discriminator. It takes a sequence as input, applies filters to produce a feature map, and then applies max-over-time pooling to get the maximum value over timesteps. The output is generated by a fully connected layer, with the highway architecture added for performance enhancement. The generator is updated using policy gradient, estimating Q(s, a) for the reward function. Figure 2 illustrates how to calculate the action-state value for the reward function. The action-state value for the partial reward function is calculated similarly to SeqGAN. It involves generating a complete sequence, observing the actual reward, and using the REINFORCE algorithm. The process includes rollout trajectories and adjusting the importance of the reward function with a hyperparameter. Expert-based reward function training involves editing expert data to create fake data and using a pseudo reward function based on negative hamming distance. The pseudo reward function is based on the negative hamming distance, counting edited tokens. Discount value is set at 1.0 to simplify calculations. Future partial sequences without y t+1 are ignored. This simplification reduces calculation costs without significantly impacting the estimation of Q Di. In cases where partial sequences cannot be taken, they are ignored, and the denominator L Di is subtracted. For example, when estimating Q Di (s = Y 1:2 , a = y 3 ) with L Di = 4, the partial sequence Y 0:3 is ignored. SeqGAN utilizes a GAN framework for training, but it can be challenging. An alternative is expert-based reward function training, which discriminates between expert and fake data. This method does not use the generator's samples and does not rely on GAN frameworks. Fake sequences can be generated from expert data by editing tokens. The fake data for training a reward function is generated by editing tokens from the expert sequence. This approach helps prevent overfitting and allows the reward function to be trained with various fake data. The reward function is commonly trained to minimize binary cross entropy, but modifications can be made to improve it. Expert-based reward function training allows for knowing which part of the fake sequence is edited from the expert. When training the generator with a policy gradient, a smooth reward function is preferred over a strict one. The smooth reward function gives some reward to sequences with mistakes, while the strict one rewards only perfect sequences. To measure the quality of edited sequences, a pseudo reward function is used, such as the negative hamming distance. The negative hamming distance is used as a pseudo function to calculate the quality of edited sequences. The quality function q(x) assigns a value close to 1 for sequences similar to an expert one, and close to 0 for heavily edited sequences. The parameter \u03c4 controls how quickly the quality function decreases as a sequence is edited from an expert one. The reward function objective is formulated based on the quality function, where the weight w(x) decreases as the sequence is little edited from the expert, and increases as the sequence is heavily edited. The parameter \u03c4 controls the smoothness of the learned reward function, with larger values easing the loss for slightly edited sequences. The modified binary cross entropy has no theoretical background but is effective in improving sequence generation when \u03c4 is large. Experiments on synthetic and real text data validate the effectiveness of the modified reward function training. In experiments conducted with synthetic and real text data, a model called the oracle, using RNN with LSTM, is used as a data generator. The generator is trained to fit the oracle's true data distribution. The performance of the generator is evaluated based on log likelihood. Evaluating the generator's performance with human review is time-consuming. The performance of the generator is evaluated based on log likelihood using the oracle test. RNN with LSTM is used as the data generator, trained on the oracle's true data distribution. Two reward functions are provided: short-term for sequences of length 4 and long-term for sequences of length 20. The completed sentences are always of length 20. The reward function for the completed sequence is the same as SeqGAN's. L2 regularization is applied to the reward function during adversarial training with a regularization term \u03bb of 0.01. The generator and oracle have 32 units in the hidden and embedding layers. Batch size is 40, reward scaler \u03b1 is always 1.0, and no entropy regularization term is used. Adam BID13 is the optimizer for both generator and reward functions. The generator G \u03b8 is trained with policy gradient after pretraining with MLE to address initial randomness. The reward function is pretrained with G \u03b8 and iteratively trained. Expert-based reward function training involves discriminating datasets S and edited expert dataset S until convergence. The generator G \u03b8 is then trained with policy gradient until convergence. The reward function is fixed during the training of the generator. An edited expert dataset is created by changing tokens with a 25-percent chance. Results show models with proposed methods outperform SeqGAN and MLE. Introducing partial reward function is effective, with PG S exp being the best model. Further analysis on long-term and short-term reward functions is conducted. The short-term reward function provides correct rewards to sequences that the long-term reward function cannot, benefiting sequence generation. Using only the short-term reward function outperforms using both long-term and short-term rewards, indicating the effectiveness of partial optimization in sequence generation. The partial reward function leads to the optimization of the whole sequence in the oracle test. Expert-based reward function training is effective, showing significant improvements over models trained with adversarial training. The performance of expert-based reward function training depends on a temperature \u03c4. The oracle score and optimal temperature values for training are shown in TAB2. Short-term partial reward function performs well with \u03c4 0, indicating a preference for strict rewards. Long-term partial reward function performs well with \u03c4 = 1.5, suggesting a preference for smooth rewards. Expert-based reward training shows successful optimization of the generator with a fixed reward function. NLL decreases as returned reward increases, indicating effective optimization. In the experiment, the generator's optimization is successful as the NLL decreases with an increase in returned reward. The dataset for text generation consists of BBC news articles, with a corpus of articles on technology and business. The training set has 11,163 sequences, and the test set has 1,500 sequences. Different models are compared in the experiment, using various reward functions and training methods. According to figures, new measures are being implemented to record people in childcare strategies and taxes. The industry is part of the xbox consoles and will be used to prevent conflicts. Despite scoring the best BLEU-3, the output sentence lacks coherence. In experiments with PG SL exp models, coherence and partial correctness are balanced. The scaler parameter for short-term reward \u03b1 S is adjusted, while \u03b1 L is always set to 1.0. Quality function temperature parameters are set to 0.001 and 1.5 for short-term and long-term rewards. Hidden layer and embedding layer units in the generator are set to 200. Expert-based reward function training involves creating an edited sequences dataset. In expert-based reward function training, a dataset S is created with edited sequences. Each token of the expert is changed with a 15% chance, sampled from the word occurrence distribution in the training dataset. An entropy regularization term is added to prevent the policy from becoming deterministic. The hyperparameter \u03b2 is initially set to 0.02 and increased to 0.05 after 15 epochs to optimize reward. At the beginning of the generator training, optimization with respect to reward does not occur. Evaluation of the generator's performance is done by calculating the BLEU score of the generated sequence. Results show that models with expert-based reward training outperform SeqGAN and MLE, with PG S exp scoring the best in BLEU-3. This indicates that expert-based reward training is effective in generating more comprehensible sentences. PG S exp gives good scores in BLEU 3 as it prioritizes generating the sequence, but fails to produce a coherent sequence. PG L exp has the fourth best BLEU score but maintains sequence coherence. PG SL exp models ensure both partial correctness and coherence of sequence by adjusting the short-term reward scaler \u03b1 S. Additional samples show variety. Two proposals for better metric learning are discussed: partial reward function and expert-based reward function training. In the study, expert-based reward function training was found to be effective for optimizing sequence generation. The balance between short-term and long-term rewards is crucial, with the hyperparameter \u03b1 Di used to tune this balance. Tuning \u03b1 Di is challenging, as prioritizing short-term rewards too much can lead to partially correct but incoherent sequences, and vice versa. One practical strategy to balance partial correctness and coherence in sequence generation is to separate the process into two stages. Initially, a coherent sequence is produced using a generator trained with long-term rewards, followed by adjustments using a short-term reward function to correct mistakes. Updating the reward function to be more strict as the generator improves is desirable. Updating the reward function to be more strict is desirable. One idea is to decrease the probability of changing a token in the expert-based reward function training. This can lead to a more sophisticated sequence generation in a GAN framework. The partial reward function can be directly applied to GAN-based text generation, similar to techniques used in image generation. Expert-based reward function may help stabilize GAN training and generate a variety of edited expert sequences. Past generator samples can also be used to ensure sample variety during training. The use of past generator samples in GAN training can help stabilize the training process and ensure sample variety. This technique can also be applied to edited expert sequences for improved sequence generation. The firm will fund animation prices in novel ways, P2P is a human according to the stability pact, tourism tactics creating Vedomosti newspaper, Caribbean ability of $2 amid virus-fighting program allegations, London's future digital images identity theft, Chicago data given, automatic syncing worldwide. The federal reserve is struggling with treasury and organisations have said you play online in a hydroelectric-power generator the airline said in the election, you would then its broadcasting attraction,\" said research, a number of stagnation and more than 1 million copies of the personal firewalls,\" he said. \"It is us lawyers claimed that the mac mini and is being piloted by the royal national hi-tech crime unit yuganskneftegas (yugansk) \"it will be disruptive to music, employment for mobile firm, almost three-quarters of job creation, the airline PG L exp \"a literate and qualified turkish population,\" insisted the year to meet he has been security to be. The company plans to enter the UK market with financial advice, facing a problem of insufficient demand in Europe. Yukos claims a ban on the market will impact its investments. Skype's success in Russia is attributed to lower prices. Mr. Ghosn will allocate 40% of funds to Google. The company may face injunctions for retaliating on the network. The world's largest car maker plans to invest in recycling, with prices falling as part of the service. The company aims to raise awareness and prevent conflicts in the industry. Metal Slug 3 is a cheap option in the global entertainment industry. The global entertainment industry is expected to see a boost in the UK, with a focus on broadband connections and reducing costs. Microsoft is projected to reach $4.35bn in revenue. There are efforts to prevent conflicts in the industry, with a new record expected in advisor work. The company is also looking to expand broadband connections for national users. According to a report, the company is planning to discuss partnerships in digital entertainment to enhance its core businesses. In December, a decision was made to launch a new rental subscription service. The company aims to transfer files securely amidst concerns of Russian threats. The largest US giant earned $630m, and there are plans to collaborate with other companies for digital innovation. The company is considering partnerships in digital entertainment and technology, with plans to collaborate with other carmakers. They earned $630m in January and are experimenting with window size and kernel numbers in their technology. They are also implementing a highway architecture with specific parameters to determine the smoothness of their reward function. The learned reward function in the context of digital entertainment and technology partnerships is crucial for policy gradient optimization. It can be strict or smooth depending on the similarity between edited and expert sequences, impacting the effectiveness of generating long sequences."
}