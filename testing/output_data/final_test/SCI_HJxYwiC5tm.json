{
    "title": "HJxYwiC5tm",
    "content": "Deep convolutional network architectures like VGG16, ResNet50, and InceptionResNetV2 may not guarantee generalization for small image translations and deformations. Modern CNNs can significantly change their output with slight image translations, indicating a lack of generalization. This issue is more common in newer networks and is attributed to the neglect of the classical sampling theorem in their architecture. Biases in image datasets also hinder CNNs from learning to be invariant to these transformations, suggesting that CNNs fall short in object recognition compared to human generalization capabilities. Object recognition in deep convolutional neural networks (CNNs) has achieved \"superhuman\" performance, but falls short of human generalization capabilities. The choice of architecture in CNNs, particularly convolution and pooling, aims to make the networks invariant to irrelevant cues like image translations and deformations. The neocognitron architecture from the 1980s, which inspired modern CNNs, emphasized this inductive bias. Despite the excellent performance of CNNs on object recognition, they are vulnerable to adversarial attacks, indicating that superficial changes can lead to significant shifts in predictions. CNNs are not invariant to cues that are irrelevant to object identity, as shown by results involving image filtering in the Fourier domain. Adversarial attacks often involve unnatural transformations to input images, which may explain why CNNs are not invariant to these changes. Preliminary evidence suggests that modern deep convolutional neural networks may lack robustness in object classification and detection. Studies indicate that data augmentation plays a significant role in CNN invariance, especially with small networks and the MNIST dataset. Indirect methods, such as measuring the linearity of learned representations under natural transformations, can help probe for invariances in CNNs. Modern deep convolutional neural networks may lack robustness in object classification and detection. Studies show that small changes in objects or postures can significantly affect the network's predicted score. Adversarial attacks using simple transformations like rotations and translations can fool neural networks. Advanced data augmentation methods can improve network robustness. The question remains: why are modern CNNs not invariant to these variations? Modern deep CNNs are not invariant to translations, scalings, and other realistic image transformations, which is related to the subsampling operation and biases in image datasets. Even tiny transformations can cause abrupt failures in predictions, as shown in examples with the InceptionResNet-V2 CNN. The study demonstrates that modern deep CNNs are not invariant to translations and other image transformations, leading to abrupt failures in predictions. This is illustrated through examples with the InceptionResNet-V2 CNN, where even small transformations can cause significant changes in the network's output. The researchers conducted experiments using images from the ImageNet validation set and systematically varied vertical translations, showing how the network's estimate of the correct class probability changes. The study found that modern deep CNNs are not invariant to translations, leading to abrupt changes in predictions. They introduced a measure called \"jaggedness\" to quantify this lack of invariance, showing that approximately 30% of images exhibit this behavior. Deeper networks show higher jaggedness compared to the VGG16 network, indicating a trade-off between test accuracy and invariance. The study revealed that modern CNNs lack invariance to translations, resulting in significant changes in predictions. They tested different protocols to address potential criticisms related to image resizing and inpainting procedures, showing that regardless of the protocol used, CNNs often exhibit substantial output changes with small translations or scalings. Modern CNNs lack translation invariance, leading to significant changes in output with small image translations or scalings. The intuition that convolutional layers should translate representations is flawed due to the subsampling operation, known as \"stride\", which disrupts translation invariance. In deep networks, exact translation invariance is limited by large subsampling factors, leading to only a fraction of translations being invariant. Shiftability, a weaker form of translation invariance, can hold for systems with subsampling. Global pooling can yield invariant representations when shiftability holds. Global pooling can yield invariant representations when shiftability holds, defined as the response of a feature detector being \"convolutional\" if translating the image results in a translation of the response by the same amount. This includes linear and nonlinear operations without subsampling. If r(x) is convolutional, then global pooling r = x r(x) is translation invariant. Global pooling on the sampling grid is translation invariant if the feature detector is shiftable, meaning the detector output can be linearly interpolated from responses on the sampling grid. This property extends to piecewise constant transformations as well. Global pooling on the sampling grid is invariant to transformations if the feature detector is shiftable and the support of the receptive field is contained in the same subregion for all transformations in a set of constant transformations. This ensures translation invariance in CNNs. The sharpness of the tuning function determines if the feature map can be subsampled while maintaining shiftability. Subsampling can affect translation invariance in CNNs, as seen with part detectors and Fourier transforms. The sharpness of the tuning function determines shiftability in feature maps. Shiftable representations preserve global sum of activities during transformations, while nonshiftable representations do not. Modern CNNs are examined for invariance or shiftability in learned representations. The top row displays a vertically translated image, while the bottom three rows show representations in different layers of three CNNs. VGG16 shows a shift in representation with the object, maintaining global sum even in the final layer. Modern networks have sharper responses but lose shiftability in later layers, with final layers showing invariance to some translations. The many layers of subsampling result in a final response that is not shiftable. The nonshiftability of feature maps in CNNs changes significantly as the input is shifted, with deeper layers in modern networks exhibiting larger nonshiftability compared to VGG16. To ensure shiftable representations in CNNs, feature maps using stride should not contain frequencies above the Nyquist frequency. Blurring input images could help achieve this if CNNs were purely linear. CNNs can add high frequencies due to nonlinearities, so blurring before subsampling is crucial. Stride should always be combined with pooling to ensure shiftable layers in deep CNNs. Inappropriate pooling can lead to non-shiftable layers even with appropriate pooling. Pooling in CNNs can affect shiftability, even with appropriate pooling, nonlinearities may introduce high frequencies. Replacing max pooling with average pooling can make representations approximately shiftable, but may reduce detail and impact recognition performance. Invariance to translations and rescalings of input is achieved, but at the cost of decreased performance. The discussion highlights the importance of avoiding aliasing in subsampling by ensuring no high frequencies are present in feature maps. Ruderman et al. suggest that networks can learn smooth filters to reduce sensitivity to transformations. The CNN architecture may not inherently provide translation invariance, but it can learn it from training examples if the dataset is invariant to irrelevant transformations. The degree of invariance in the ImageNet training set was examined by manually labeling images in five categories. The ImageNet dataset is not invariant to translations and rescalings, as shown by the distribution of distances between the eyes of a \"Tibetan terrier\" and the center point between the dog's eyes. Object location and size were found to be highly non-uniform for over 900 out of 1000 categories, indicating strong biases in the dataset. The training set for the CNN is highly non-uniform, making it challenging for the system to learn invariance. Data augmentation can help make the training set invariant, but not all augmentation methods are effective. To achieve complete invariance to translation, a system would need to see 3600 augmented versions of each training example due to the subsampling factor of approximately 60 in modern networks. The lack of invariance to rotations and scalings in CNNs leads to poor performance on challenging datasets like ImageNet. While data augmentation methods can increase invariance to translation and rotation, CNNs still struggle with generalizing for small image transformations. Despite this, their performance on the ImageNet test set remains remarkably good. The ImageNet test set reveals that modern CNNs struggle with generalizing to different sizes and locations, leading to poor performance when images are scaled down and translated. Human performance remains unaffected by these changes, highlighting the limitations of current CNNs in handling variations in image size and position. CNN architectures were designed with the belief that convolutional structures and pooling operations would provide invariance to translations and small image deformations. However, the use of subsampling or \"stride\" breaks down this intuition, leading to modern CNNs not displaying the desired invariances. The ImageNet training and testing examples show significant photographer's bias, making it unlikely for a system to learn invariance using these examples. The sampling theorem suggests a way to impose translation invariance. The sampling theorem suggests imposing translation invariance to overcome failures in CNN architectures. Blurred representations may decrease performance due to photographer's bias in datasets. Alternatively, specially designed features or neural network architectures can enforce invariance. Preprocessing video frames involves resizing and standard Keras preprocessing. The frames are preprocessed using the standard Keras preprocessing function before using the predictions of the InceptionV3 model to demonstrate jagged behavior. Nonshiftability is measured as a function of depth in the networks to determine the shiftable representation. The nonshiftability of representation is demonstrated in different layers in response to a randomly selected image."
}