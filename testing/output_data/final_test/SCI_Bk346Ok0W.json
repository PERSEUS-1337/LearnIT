{
    "title": "Bk346Ok0W",
    "content": "Recent work on encoder-decoder models has shown that integrating temporal and spatial attention mechanisms into neural networks improves performance. A new network architecture, STAN, applies attention to sensor selection for multi-sensor setups, reducing word error rates on audio and visual tasks in noisy scenarios. The STAN architecture dynamically responds to changing noise levels, leading to improved performance on datasets like TIDIGITS, GRID, and CHiME-3. The proposed STAN architecture introduces advantages like ease of sensor removal, attentional interpretability, and increased robustness to noise. Attention mechanisms have shown improved performance in various applications such as image captioning, speech recognition, and machine translation. Dynamic attention on salient attributes improves image captioning, while an attention-based network can replace HMM in speech recognition for better results. The STAN architecture embeds an attentional mechanism for sensor selection, allowing dynamic focus on sensors with higher SNR. It supports multi-sensor and multi-modal inputs, remains operational even when sensors are removed, and is attractive for tasks using multi-sensor integration. This work generalizes existing network types with attention in multi-sensor setups. The STAN architecture introduces an attentional mechanism for sensor selection, demonstrating its functionality in synthetic and real-world datasets. It consists of five building blocks and is robust to altered sensor configurations. The STAN architecture utilizes attention values computed on a per-frame basis for sequence-to-sequence mapping on time-series data. Recurrent neural networks like GRUs or LSTMs are used for studying input stream dynamics, while transformation layers vary based on input modality. The STAN architecture uses attention values for sequence-to-sequence mapping on time-series data, with experiments showing its performance in environments with changing noise levels. Multiple audio and video sensors were tested, with sensors cloned to create a multi-sensor scenario. Each sensor was corrupted with different Gaussian noise sources, requiring the network to attend to less noisy sensors for better task performance. The STAN architecture utilizes attention values for sequence-to-sequence mapping on time-series data, demonstrating its effectiveness in environments with varying noise levels. In experiments, multiple sensors were tested, with the network focusing on less noisy sensors for improved task performance. Gaussian noise with varying levels was added to the data during training, with the TIDIGITS dataset used for evaluation. The study evaluated five models using Mel-frequency cepstral coefficients for speech recognition. STAN models with attention modules were compared to simpler sensor concatenation models. Connectionist Temporal Classification objective was used for training all models with the ADAM optimizer. The study compared STAN models with attention modules to sensor concatenation models for speech recognition using Mel-frequency cepstral coefficients. The STAN models outperformed in error rate, showing a generalization across various noise types. The attention response of a Double Audio STAN model demonstrated a negative correlation between noise level and attention. The attention modules in the STAN models focus on sensors with the lowest noise levels, even in situations where noise levels are low for both sensors. The models are evaluated against baseline single sensor and concatenation models under clean and noisy conditions, showing low error rates on clean signals and generalization across noise types. The STAN models outperform concatenation models in noisy conditions, reducing WER by 71% with two sensors and 78% with three sensors. The GRID BID5 corpus is used for speech processing studies, containing 1000 sentences per speaker with a vocabulary of 51 classes. Video recordings were processed with Dlib face detector and resized to 48 x 48 pixels per frame. The video sequences from the GRID database were resized to 48 x 48 pixels and normalized. The models were trained using the CTC objective with the ADAM optimizer for up to 100 epochs. Two video models with CNN feature transformation and STAN attention modules were evaluated. The STAN model demonstrates effective attentional mechanism for video data on random walk noise and generalizes well to different types of noise, reducing Word Error Rate (WER) by 13% on clean test set and 28% on noisy test set. Evaluation on CHiME-3 corpus BID2 shows promising results for multi-channel automatic speech recognition in real-world noisy speech scenarios. The corpus provides real and simulated noisy data from four environments: a cafe, street junction, public transport, and pedestrian area. The noisy speech data consists of 6-channel recordings of sentences from the WSJ0 corpus BID7 spoken in these environments. Tablet device with 6 microphones used for recording. Simulated data constructed by mixing clean samples of WSJ0 with environment background recordings. Real and simulated noisy speech data used for training and testing STANs on natural noise. Samples preprocessed into 123-dimensional filterbank features. The curr_chunk discusses the comparison of two STAN variants with a sensor concatenation model for classification. Both STAN variants use 6 sensors with attention modules, while the concatenation model combines all 6 sensors into a 738-dimensional feature representation. All models use bidirectional LSTMs for classification into 59 output classes. The curr_chunk describes the training procedure for models using the CTC objective to learn alignments between speech frames and label sequences. Models are trained with the ADAM optimizer for 150 epochs, with regularization techniques such as Gaussian noise, dropout, and weight decay applied. The network output is decoded using a trigram language model based on WFSTs. The models are compared against a DNN/HMM hybrid baseline model with a more complex training procedure. The training procedure for models using the CTC objective includes applying noise on inputs, dropout, and weight decay. Results show that STAN variants outperform the baseline model in terms of average WER. Attention weights per channel are analyzed, with STAN-shared spreading attention more equally than STAN-default. The STAN variants demonstrate the ability to adjust attention towards more useful sensors, with STAN-shared showing better differentiation between channels compared to STAN-default. This highlights the informative output of the attention modules in identifying sub-optimal sensors, particularly channel 2. The STAN variants, STAN-shared and STAN-default, show the ability to adjust attention towards sensors with high SNR and low SNR, even in the presence of natural noise. STAN-shared demonstrates more interpretable attention weights compared to STAN-default, as seen in the attention signals for a sample with channel corruption. The STAN variants, STAN-shared and STAN-default, can adjust attention towards sensors with different signal-to-noise ratios, even in the presence of natural noise. The effectiveness of the attentional mechanism is demonstrated by the ability of STANs to reduce attention on corrupted channels. The CHiME-3 dataset reveals that real recordings suffer from channel corruption, which explains why STANs achieved only a slight improvement in Word Error Rate compared to the concatenation model. The STAN variants, STAN-shared and STAN-default, can adjust attention towards sensors with different signal-to-noise ratios, reducing attention on corrupted channels. The attention response of STAN-shared is more interpretable. Performance is assessed based on the corruption of samples using cross correlation coefficients. The standard deviation indicates the likelihood of corrupted channels, allowing for ranking of corrupted samples. Listening tests confirmed the ranking's accuracy. STANs outperform concatenation models when including fewer corrupted samples, with WER reductions of 12% (STAN-default) and 9% (STAN-shared) for the 50 most corrupted samples. In specific environments, WER reductions are even higher, such as 23% on STR (STAN-default) and 14% on PED (STAN-shared). STANs show improved performance in controlled environments like CAF and BUS, with WER reductions of 23% on STR and 14% on PED. They exhibit robustness to channel removal due to their modular architecture, allowing flexibility in sensor configurations without additional training. This flexibility is harder to achieve for concatenation models. The results in TAB3 show that removing channels from STAN models has varying effects on WER. Removing one channel at a time results in WER increases of up to 5.7%, except for channel 2 which decreases WER by 2%. Removing multiple channels sequentially shows stable WER up to three channels removed, but a 26% increase with five channels removed. This demonstrates the model's ability to adapt to channel changes with acceptable performance. The sensor transformation attention network (STAN) architecture shows a deterioration in performance when sensors are removed, leading to an increase in standard deviation. STANs exhibit robustness to dynamic noise sources and have features for sensor selection through attentional mechanisms. The sensor transformation attention network (STAN) architecture demonstrates improved accuracy by training with dynamic noise sources. STANs outperform concatenation models in noise-corrupted environments, with attention modules indicating optimal sensor placement. STANs are flexible with sensor configuration, performing well even with sensor removal post-training. This allows for energy savings and reduced computational load on multi-sensor systems like mobile robots. The attention mechanism in STANs is encouraged to learn by adding random walk noise to sensors, ensuring uniform noise coverage without settle-in time. The noise level in STANs is uniformly covered over a range [0, \u03c3 max ) without settle-in time. The noise standard deviation \u03c3 for an input sequence of t timesteps is calculated with specific parameters. The reflection function \u03c6(a, \u03c3 max ) maintains values within the desired range and maps them to [0, \u03c3 max ) to avoid discontinuities. The input data x at feature index k and time index t is mixed with noise sampled from a normal distribution. The reflection function \u03c6(a, \u03c3 max ) creates a constrained random walk by adding normally distributed random noise to input data x at feature index k and time index t. This noise model varies over time, allowing for periods of high and low noise levels to tune the attentional mechanism of STAN models. An example of random walk noise added during training is depicted in Figure 7. In (a), the cumulative sum of random variables forms a random walk, which becomes bounded after applying the reflection operator \u03c6. Visualization of noise drawn at each time point is shown in four sub-panels. 6 samples from the 'et05_real' evaluation set are plotted, with corrupted channels identified. The figures display filterbank features, attention response of STAN models, and output of the sensor merge module. The real evaluation set contains data with natural noise. Filterbank features and merge layer images are clipped to -3 to 3 for better visibility. Table 5 shows sample keys and corrupted channels, with samples from the 'et05_real' evaluation set. STAN-shared can detect corrupted channels like backward channel 2. Highly recommend viewing on a high-resolution screen or printout and listening to audio samples if available. STAN-shared can detect corrupted channels like backward channel 2, even in the presence of natural noise. The attention response of STAN-shared follows the signal quality, with clear suppression on corrupted channels 1, 2, and 4. The attention on channel 4 is initially high but then suppressed when temporarily corrupted after frame 120."
}