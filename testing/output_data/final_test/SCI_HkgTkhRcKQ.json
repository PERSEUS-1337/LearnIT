{
    "title": "HkgTkhRcKQ",
    "content": "In this paper, a new insight into the non-convergence issue of Adam and other adaptive learning rate methods is provided. The inappropriate correlation between gradient $g_t$ and the second moment term $v_t$ in Adam leads to unbalanced step sizes, causing non-convergence. Decorrelating $v_t$ and $g_t$ is proposed as a solution, leading to the development of AdaShift, a novel adaptive learning rate method. AdaShift is a new adaptive learning rate method that addresses the non-convergence issue of Adam by decorrelating $v_t$ and $g_t through temporal shifting. It maintains competitive performance with Adam in terms of training speed and generalization. First-order optimization algorithms with adaptive learning rates are crucial in deep learning for efficiently solving large-scale optimization problems. Adam is a popular adaptive learning rate method in deep learning that uses exponential moving averages of first and second moments for efficient optimization. It is robust and efficient in both dense and sparse gradient cases, making it a common choice in research. Adam is a popular adaptive learning rate method in deep learning, efficient in both dense and sparse gradient cases. Reddi et al. (2018) identified a key issue in Adam's convergence proof related to the quantity DISPLAYFORM1, which may not always be positive. They proposed AMSGrad and AdamNC as variants to address this issue by ensuring \u0393 t remains positive. AMSGrad defines v t as the historical maximum of v t, while AdamNC gives v t a \"long-term memory\" of past gradients. In this paper, a new insight into adaptive learning rate methods is provided to address the non-convergence issue of Adam. By analyzing the step sizes of gradients, it is observed that a large gradient tends to have a small step size, while a small gradient has a large step size due to the positive correlation between v t and g t. This imbalance is addressed to improve the efficiency of training processes. The inappropriate positive correlation between v t and g t causes non-convergence in Adam. AdaShift, a decorrelated variant, solves this issue by calculating v t using temporally shifted gradients. It maintains decent performance compared to Adam in terms of training speed and generalization. The exponential decay rates for m t and v t are adjusted to avoid bias in estimation. Kingma & Ba (2014) propose bias correction for m t and v t. Online optimization involves predicting parameters \u03b8 t and evaluating on cost functions f t (\u03b8). Regret R(T) measures the performance by comparing predictions to the best fixed-point parameter prediction. The counterexamples presented by Reddi et al. (2018) show that Adam may not converge to the optimal solution in certain online optimization problems. This is due to the accumulated update of the parameter \u03b8 in Adam, which can be opposite to what is needed for convergence. Reddi et al. (2018) argue that Adam may not converge due to the positive assumption of \u0393 t not always holding. They extend counterexamples to stochastic cases, showing that for a one-dimensional case with i.i.d. functions, decreasing \u03b8 minimizes loss. When C is large enough, Adam's parameter update results in increasing \u03b8. The positive accumulation of parameter updates in Adam leads to an increase in \u03b8. Reddi et al. (2018) suggest maintaining the strict positiveness of \u0393 t by ensuring v t is non-decreasing or using increasing \u03b2 2. It is noted that Adam will converge for any fixed sequential online optimization problem with sufficiently large \u03b2 1. Theorem 1 states that for such problems, there exists a \u03b2 1 that ensures Adam has average regret \u2264 2. The analysis in this section addresses the non-convergence issue through counterexamples. In this section, the non-convergence issue is analyzed by examining counterexamples provided by Reddi et al. (2018). The fundamental problem with common adaptive learning rate methods is the imbalance in step sizes due to the positive correlation between v t and the gradient scale g t. This imbalance leads to non-convergence. The net update factor is defined to analyze the influence of each gradient g t, and this analysis is applied to Adam using Equation 6 as an example. The argument is extended to stochastic online optimization and general cases. The net update factor is a new tool for analyzing the influence of each gradient in the optimization process. In Momentum, the net update factor is equivalent to the step size, leading to similar convergence as vanilla SGD. However, in adaptive learning rate methods like Adam, the convergence is more complex due to the function of past gradients in determining the net update factor. In Adam optimization, the net update factor is analyzed to understand the influence of each gradient. The formula for v_t is derived in a deterministic setting, showing how the net update factor changes with each epoch. The limit of the net update factor monotonically increases with the index i in each epoch, affecting the gradient updates. In Adam optimization, the net update factor is analyzed to understand the influence of each gradient. The net update factors are unbalanced, with the largest gradient having the smallest net update factor in the epoch, potentially leading to incorrect accumulated updates. In Adam optimization, unbalanced net update factors may lead to incorrect accumulated updates. The expectation of the net update factor for each gradient is analyzed, showing that the influence of gradient C is smaller than gradient -1. The net update factor in Adam optimization may lead to incorrect accumulated updates, with the influence of gradient C being smaller than gradient -1 due to inappropriate correlation between v t and g t. This results in a smaller net update factor for large gradients and a larger net update factor for small gradients. The inappropriate correlation between v t and g t in Adam optimization leads to unbalanced net update factors, causing non-convergence issues. Large gradients result in smaller updates while small gradients lead to larger updates, potentially causing parameter updates in the wrong direction. The inappropriate correlation between v t and g t in Adam optimization leads to unbalanced net update factors, causing non-convergence issues. Possible solutions include making v t act like a constant or using a large \u03b2 1 to mitigate the impact of unbalanced net update factors. However, neither of these solutions fundamentally solves the problem, leading to a dilemma in rethinking the role of v t in adaptive learning rate methods. The current scheme of v t in adaptive learning rate methods causes a positive correlation with g t, leading to non-convergence issues. To address this, v t should reflect the scale of gradients while being decorrelated with g t for convergence. In adaptive learning rate methods, the correlation between v t and g t can lead to non-convergence issues. To address this, v t should reflect the gradient scale while being decorrelated with g t for convergence. Momentum can be seen as setting v t as a constant to make it independent of g t. The proposed solution to address the correlation between v t and g t in adaptive learning rate methods involves making v t independent of g t through temporal decorrelation and utilizing spatial information of gradients. The algorithm AdaShift is introduced for this purpose, incorporating first moment estimation. In practical settings, the update rule for v t can be changed to involve g t\u2212n instead of g t, making v t and g t temporally shifted and decorrelated. This assumption generally holds in stochastic online optimization problems and practical neural network settings, where the dimension of parameters is high. In practical settings, the update rule for v t can involve g t\u2212n instead of g t, making v t and g t temporally shifted and decorrelated. This assumption holds in stochastic online optimization problems and practical neural network settings with high-dimensional parameters. The function \u03c6 is introduced to compute v t using all elements of g 2 t\u2212n, with \u03c6(x) = max i x[i] being a commonly used choice. In practical settings, the update rule for v t can involve g t\u2212n instead of g t, making v t and g t temporally shifted and decorrelated. The function \u03c6 is introduced to compute v t using all elements of g 2 t\u2212n, with \u03c6(x) = max i x[i] being a commonly used choice. In deep neural networks, different gradient scales for layers make it challenging to find a suitable learning rate for all layers with SGD and Momentum methods. Traditional adaptive learning rate methods apply element-wise rescaling for each gradient dimension to achieve rescaling-invariance. In a temporal decorrelation with spatial operation scheme, the algorithm addresses the issue of different gradient scales by applying \u03c6 block-wise and outputting a shared adaptive learning rate scalar v t [i] for each block. This approach mimics an adaptive learning rate SGD, ensuring relative gradient scales within blocks remain unchanged. Parameters \u03b8 t are divided into M blocks, with each block containing parameters of the same type or layer in the neural network. Incorporating first moment estimation in optimization algorithms involves decorrelating v t with g t and ensuring independence between v t and m t. The algorithm updates v t and m t using a decay rate \u03b2 1 for temporal elements, allowing for the use of large \u03b2 1 without the risk of using outdated gradients. The proposed algorithm unifies various techniques by incorporating a spatial operation \u03c6, n \u2208 N + , \u03b2 1 \u2208 [0, 1], \u03b2 2 \u2208 [0, 1), and \u03b1 t. It differs from Adam by temporally shifting the gradient g t for n-step, ensuring decorrelation between v t and m t to solve nonconvergence issues. The calculation of v t involves spatial elements of previous gradients, introducing the spatial operation \u03c6. The proposed method, AdaShift, incorporates a spatial operation \u03c6 to maintain relative gradient scale in each block. It is closely related to SGD and aims to decorrelate v t and m t. Empirical studies comparing AdaShift with Adam, AMSGrad, and SGD show promising results in terms of training performance and generalization. The algorithm is validated on the stochastic online optimization problem with specific parameters set. The code is available at http://bit.ly/2NDXX6x. In an experiment comparing Adam, AMSGrad, and AdaShift, with specific parameters set, AdaShift updates \u03b8 in the correct direction and converges faster than AMSGrad. Adam tends to increase \u03b8, but can converge with larger \u03b2 values. AdaShift still converges fastest even with a small \u03b2 value. To compare optimization algorithms, experiments were conducted on Logistic Regression and Multilayer Perceptron using MNIST dataset. Results showed similar performance in Logistic Regression, while in Multilayer Perceptron, max-AdaShift achieved the lowest training loss. The study compared optimization algorithms on Logistic Regression and Multilayer Perceptron with MNIST dataset. Max-AdaShift had the lowest training loss in Multilayer Perceptron. In further experiments with ResNet and DenseNet on CIFAR-10 datasets, non-AdaShift showed better generalization due to its unstable step size regularization effect. ResNet and DenseNet are efficient modern neural networks tested in the study. AdaShift outperformed AMSGrad in terms of training speed and generalization, especially in test accuracy for ResNet. Results show that AdaShift performs slightly better than other algorithms, especially in test accuracy for ResNet and training loss for DenseNet. The study also compared Adam, AMSGrad, and AdaShift on Tiny-ImageNet dataset, with AdaShift achieving higher test accuracy than Adam. Additionally, the algorithms were tested on generative and recurrent models, showing AdaShift's superiority in training WGAN-GP. In a comparison of Adam, AMSGrad, and AdaShift for training the WGAN-GP discriminator, AdaShift outperforms Adam significantly, while AMSGrad's performance is unsatisfactory. AdaShift also achieves higher BLEU scores than Adam and AMSGrad in NMT testing. The study addresses the non-convergence issue of adaptive learning rate methods by analyzing the correlation between v t and g t, showing that unbalanced step sizes are the root cause of Adam's non-convergence. Decorrelating v t and g t leads to unbiased expected step sizes, solving the non-convergence problem of Adam. AdaShift is a novel adaptive learning rate method that addresses the non-convergence issue of Adam by decorrelating v t and g t through calculating v t using temporally shifted gradient g t\u2212n. It introduces a new perspective on adaptive learning rate methods where v t is no longer necessarily the second moment of g t, but a random variable independent of g t. AdaShift maintains overall gradient scale by calculating v t with spatial elements of previous gradients, resulting in an algorithm closely related to SGD. Experimental results show AdaShift solves the non-convergence problem of Adam and achieves competitive performance. AdaShift addresses the non-convergence issue of Adam by decorrelating v t and g t through calculating v t using temporally shifted gradient g t\u2212n. Experimental results show AdaShift achieves competitive performance compared to Adam. The direction and speed of Adam optimization process are determined by \u03b2 1 and \u03b2 2. Larger values of \u03b2 1 or \u03b2 2 require a larger C to make Adam stride in the opposite direction, reducing non-convergence occurrences. The relation among C, \u03b2 1, \u03b2 2, and the convergence behavior of Adam is analyzed in stochastic problems. In the experiment, the relation between C, \u03b2 1, \u03b2 2, and Adam's convergence behavior is analyzed. Results show that larger C leads to non-convergence more easily, while larger \u03b2 1 or \u03b2 2 help resolve non-convergence. Lemma 6 defines a critical condition in the optimization problem, showing the importance of \u03b2 1 and \u03b2 2. Algorithm 2, AdaShift, uses a queue to address the non-convergence issue of Adam. Algorithm 2, AdaShift, utilizes a queue to address non-convergence in Adam. It involves pushing and popping vectors in a window of length n, calculating weight vectors, and verifying temporal and spatial correlations between variables. Results are shown in tables for correlation coefficients. The formulation of m t with bias correction is calculated from 1 to 10 to find the average correlation coefficient of variables i. The optimal parameter is \u03b8 * with a maximum step size. As n approaches infinity, the limits of m nd+i and v nd+i are derived. The forward difference of k(g nd+i) is monotonically decreasing, with a weighted summation that decreases from positive to negative. As nd approaches infinity, lim k(g nd+1) is the maximum update, while lim is the minimum. Lemma 7.1 states the expectation of a differentiable function f(X) for a bounded random variable X. The expectation of f(X) is defined where D(X) is the variance of X, and R3 is the distribution function of X. In the stochastic online optimization problem, the gradient affects the distribution. The expectation of gi and variance of vi are discussed under the assumption of i.i.d. gradients. The net update factor for gradient gi is determined. The expectation of f(Xt) is calculated according to lemma 7. In lemma 7, the expectation of f(Xt) is calculated. The net update factor for gradient C is determined. The hyper-parameter settings for AdaShift are discussed, including the sensitivity of the learning rate \u03b1t. The sensitivity of AdaShift's hyperparameters, including \u03b1, \u03b21, and \u03b22, is discussed. Results show low sensitivity to \u03b21 and \u03b22, with suggested parameters of n=10, \u03b21=0.9, and \u03b22=0.999. Additionally, the n sensitivity is explored, with results indicating that using the latest m gradients can improve performance. In the proposed algorithm, a spatial operation is applied to the temporally shifted gradient to update v t based on the temporal independent assumption. The independence of g t\u2212n from g t is emphasized to avoid issues with rare samples and large gradients in the mini-batch. Extended experiments were conducted on two variants of AdaShift: AdaShift (temporal-only) and AdaShift (spatial-only). AdaShift (temporal-only) showed less stability compared to AdaShift, with instances of explosive gradients requiring a lower learning rate. AdaShift (spatial-only) performed similarly to Adam, with further experiments and comparisons with Nadam included in the next section. In this section, experiments were conducted comparing Nadam and AdaShift (spatial-only) with Adam. Results showed similar performance among the three. Rahimi & Recht discussed the limitations of gradient descent in optimization, showcasing a challenging ill-conditioned quadratic problem. Testing with SGD, Adam, and AdaShift revealed that all methods converge given sufficient training time. The experiments compared Nadam and AdaShift with Adam, showing similar performance. The analysis revealed that adaptive learning rate methods like Adam and AdaShift struggle to converge well in ill-conditioned problems, while SGD performs better due to its decreasing step-size."
}