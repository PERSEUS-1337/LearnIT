{
    "title": "BJlc6iA5YX",
    "content": "The checkerboard phenomenon is a well-known visual artifact in computer vision. This paper revisits checkerboard artifacts in the gradient space, which are a weak point in network architecture. A defense method called Artificial Checkerboard Enhancer (ACE) is proposed to induce adversarial attacks on designated pixels, effectively deflecting attacks with a single pixel shift. Extensive experiments support the effectiveness of ACE against various attack scenarios. The checkerboard phenomenon in deep neural networks can lead to visual artifacts. Research has shown that these artifacts are caused by uneven overlap in deconvolution operations. A defense method called Artificial Checkerboard Enhancer (ACE) has been proposed to counter adversarial attacks by inducing attacks on specific pixels. This method has been proven effective against various attack scenarios, including large-scale datasets like ImageNet. Additionally, a potential link between checkerboard artifacts and neural network robustness has been suggested but not thoroughly explored. The gradient checkerboard artifacts phenomenon in neural networks, particularly in the gradient space during the backward pass of convolution operations, has not been extensively investigated. This phenomenon affects network robustness and is a weak point in contemporary architectures like ResNet. The artifacts significantly impact the loss surface shape and are image-agnostic. An Artificial Checkerboard Enhancer is proposed to address this issue. The proposed Artificial Checkerboard Enhancer module, ACE, boosts or creates checkerboard artifacts in the target network and manipulates gradients. It guides attacks to the intended environment, allowing defenders to easily dodge attacks with minimal accuracy loss. The module is scalable to large datasets like ImageNet and transferable to other models without additional fine-tuning. This practical module can be plugged into any pretrained architecture. The proposed Artificial Checkerboard Enhancer (ACE) module shapes the gradient into a checkerboard pattern to attract adversarial attacks. By padding the image with a single row/column and discarding the opposite row/column, attacks can be easily deflected during inference. The module analyzes gradient checkerboard artifacts in network architectures, offering a novel defense strategy. The Artificial Checkerboard Enhancer (ACE) module strengthens gradient artifacts to induce adversarial attacks and defends against them by one-pixel padding. It is scalable to various datasets without the need for fine-tuning, successfully defending against attacks on CIFAR-10 and ImageNet BID6 datasets. This defense method successfully defends against attacks using the projected gradient descent algorithm on the ImageNet dataset. Adversarial attacks can vary based on the adversary's access to the target model, with gradient-based attacks being effective when gradients are available. Score-based and decision-based attacks are also discussed in the context of generating adversarial examples. Defense methods against adversarial attacks can be categorized into gradient masking and adversarial training. Gradient masking methods aim to make it difficult for adversaries to compute exact gradients, while adversarial training helps make models robust against perturbations, albeit at the cost of accuracy. Input transformations focus on the input image to enhance defense strategies. Gradient Overlap (\u2126(x i )) represents the number of parameters associated with a single pixel in the input x. GCA is a phenomenon showing checkerboard patterns in gradients, introduced in BID21. The experiment introduced a module called ACE, motivated by a single pixel perturbation attack on images using LeNet and ResNet-18 BID12 models. The attack success rate showed checkerboard patterns, considered image-agnostic, on the CIFAR-10 dataset test images. The uneven gradient overlap in the test images of the CIFAR-10 dataset is speculated to be caused by the number of parameters connected to a single pixel. Checkerboard patterns in the gradient overlap are observed, making it susceptible to adversarial attacks. Supporting results will be provided in the following sections. The success rate of adversarial attacks on CIFAR-10 dataset is analyzed by perturbing pixels to white. Patterns in gradient overlap are observed on a toy model and ResNet-18. Top-1 test accuracy after various attacks on pixels and their subsets is compared, showing minimal differences. The vulnerability of pixels with high gradient overlaps to adversarial attacks is demonstrated by generating examples on G(p) and evaluating accuracy on CIFAR-10 dataset. Restricting attacks to G(0.3) shows similar success rates as attacks on G(1.0), indicating susceptibility of high gradient overlap pixels. Leveraging this vulnerability for defense involves intentionally imposing GCA on model inputs to induce attacks that can be easily dodged with a padding operation. The Artificial Checkerboard Enhancer (ACE) module artificially enhances the checkerboard pattern in input gradients to induce attacks that can be dodged easily. It is based on a convolutional autoencoder with a hyperparameter \u03bb to control the magnitude of checkerboard artifacts. The module is added in front of a base convolutional network to enhance checkerboard artifacts on input gradients. The Artificial Checkerboard Enhancer (ACE) module uses a hyperparameter \u03bb to control checkerboard artifacts in input gradients. Increasing \u03bb artificially enhances the checkerboard pattern, as shown in heatmaps of ResNet-18. The module is designed to induce attacks that can be easily avoided, with only a small decrease in accuracy even with a large \u03bb. The ACE module uses \u03bb to control checkerboard artifacts in input gradients, inducing attacks that can be avoided with a small accuracy decrease. The module builds a high gradient overlap with a checkerboard pattern to cage adversarial attacks into checkerboard artifacts. The effects of \u03bb are studied by visualizing classified labels based on perturbation magnitude on pixels in checkerboard artifacts C and non-checkerboard artifacts X\\C. The experiment involves training an ACE module as an autoencoder using ImageNet datasets and integrating it into a pretrained ResNet-152. Classified labels are plotted for perturbed images, showing that artifact pixels are more susceptible to label changes compared to non-artifact pixels when \u03bb > 0. The asymmetry between artifact and non-artifact pixels becomes more pronounced as \u03bb increases. The proposed defense method involves using an ACE module to enhance gradient checkerboard artifacts in images. By inducing vulnerable pixels to align with the checkerboard pattern, perturbations can be shifted into non-vulnerable domains. This defense mechanism is similar to one introduced in BID36 but is more effective due to the ACE module. The defense method involves inducing vulnerable pixels to checkerboard artifacts using the ACE module, making only one-pixel padding sufficient to prevent adversarial attacks. Large \u03bb values result in more robust defense but may lead to significant gradient checkerboard artifacts. The defense method is evaluated in vanilla, transfer, and adaptive attack scenarios, with results detailed in Section 5. In the adaptive attack scenario, the adversary exploits the defense method directly. Five attack methods are used for evaluation, including OnePixel, JSMA, DeepFool, CW, and PGD. Experiments are conducted on CIFAR-10 and ImageNet datasets. Models in CIFAR-10 are trained with a two-layered ACE module from scratch. The ACE module with specific parameters is used to enhance gradient checkerboard artifacts. VGG-11 BID30 and ResNet-18 BID12 with ACE module are evaluated for accuracy on different \u03bb values. To address the cost of training on large datasets like ImageNet, the ACE module is trained as an autoencoder with UNet architecture and plugged into a pretrained network without additional training. The ACE module is modified to handle large-scale datasets by constraining \u03bb and adjusting the skip connection. Two evaluation metrics, attack survival rate and defense success rate, are introduced to measure top-1 accuracy. The ACE module with specific parameters enhances gradient checkerboard artifacts in VGG-11 and ResNet-18 models. Evaluation metrics include attack survival rate and defense success rate, measured on CIFAR-10 and ImageNet datasets with varying \u03bb values. Top-1 accuracy after attack and defense is reported, with experimental results reproduced by the researchers. The ACE module is utilized in experiments on CIFAR-10 and ImageNet datasets, with successful defense against various attack methods shown in Table 3. By adjusting the parameter \u03bb, the top-1 accuracy remains high after defense. Experiments on ImageNet dataset using pretrained VGG-19 and ResNet-152 models are also reported in Table 4. Results for OnePixel, JSMA, and DeepFool attacks are abbreviated due to time and memory constraints. Comparison with other defense methods for both datasets is detailed in Appendix E. Defense methods BID25, BID36, and BID4 for CIFAR-10 and ImageNet datasets are evaluated in Appendix E. The effectiveness of \u03bb in improving defense success rate against PGD attacks is studied, with results showing an increase in defense success rate as \u03bb increases. This work achieves defense success rates of up to 98% against transfer attacks from source to target models via JSMA, CW, and PGD. The method is shown to be robust against transfer attacks proposed by BID23. Our method is robust against transfer attacks proposed by BID23, as shown in experiments with VGG-11 and ResNet-18 on CIFAR-10 dataset. Defense success rates against various attacks are reported in FIG2 and Appendix G, demonstrating that generated adversarial samples are not transferable to other models. Additionally, our method shows robustness against l0, l2, and l\u221e bounded adversaries, as well as in adaptive white-box settings, as evidenced by results against PGD attacks in Appendix H. The study introduces the Artificial Checkerboard Enhancer (ACE) as a defense method against gradient checkerboard artifacts (GCA) to enhance network robustness. ACE can be easily integrated into pretrained models with minimal performance impact and is effective in deflecting various attack scenarios, even on large-scale datasets like ImageNet. Extensive experiments demonstrate the method's ability to achieve remarkable defense rates compared to other approaches. The toy model presented in Section 3 consists of three convolutional layers followed by ReLUs and two fully-connected layers. Checkerboard artifacts in the gradients are visualized through average heatmaps over test images, showing patterns before strided convolutional layers. The study visualizes checkerboard artifacts in gradients before strided convolutional layers and their propagation to front layers. The defense mechanism prevents accuracy drop with padding sizes of one, three, and five, supporting the avoidance of adversarial perturbations induced by ACE. The study visualizes checkerboard artifacts in gradients before strided convolutional layers and their propagation to front layers. The defense mechanism prevents accuracy drop with padding sizes of one, three, and five, supporting the avoidance of adversarial perturbations induced by ACE. In this section, the results of implementing different defense methods (BID25, BID36, BID4) are presented, including settings and outcomes. The classified label map after pixel perturbation on artifact and nonartifact pixels is plotted, showing the effectiveness of the defense method under various attack scenarios. In the adaptive white-box setting, experiments were conducted to defend against attacks by shifting images in random directions near safe points. By combining PGD adversarial training for robustness, the method can defend against adaptive attacks like EOT. Despite a loss in Top-1 accuracy with high \u03bb, the method offers advantages in defending against vanilla attacks simultaneously. Top-1 accuracy (%) after EOT attack on the defense method with adversarial training on CIFAR-10 dataset is shown in Table 10. The format of conducted attacks is PGD-norm-iterations. ACE module only displays training accuracy loss due to high \u03bb."
}