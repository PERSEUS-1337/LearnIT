{
    "title": "B1CQGfZ0b",
    "content": "Program synthesis involves finding a source-code program that accurately maps inputs to outputs. It is often treated as a constraint satisfaction problem and solved using a constraint solver. A key challenge is scalability, as constraining the entire set of examples can be time and memory-intensive. This paper proposes a method to construct a small representative subset of examples that effectively constrains the solver. By using a trained discriminator to predict the probability of unchosen examples, the subset is built one example at a time, with the least probable example being added. In program synthesis, a method is proposed to construct a small representative subset of examples that effectively constrains the solver. By using a trained discriminator to predict the probability of unchosen examples, the subset is built one example at a time, with the least probable example being added. This approach aims to address scalability issues in synthesizing complex programs from large datasets. In program synthesis, a technique involves selecting a representative subset from a large dataset to efficiently synthesize a correct program. This process involves using a domain-specific language (DSL) and a specification, typically expressed as input-output pairs. Gradient-descent based approaches are ineffective due to the precise and combinatorial nature of synthesis problems, requiring an explicit search over the solution space. In program synthesis, a DSL is used to represent a parametrized function encoded as a logical formula. Input-output examples are expressed as constraints for the instantiated program to satisfy. A constraint solver like Z3 is then used to find valid parameter values for the DSL, enabling the creation of a concrete executable program. Scalability is a challenge in framing synthesis as a CSP due to the need to efficiently search the constrained search space. In program synthesis, a DSL is used to represent a parametrized function encoded as a logical formula. Input-output examples are expressed as constraints for the instantiated program to satisfy. Efforts were made to simplify and re-write the constraint formula for a compact representation to avoid exceeding computer memory. To limit the number of examples, CEGIS is used, which solves the synthesis problem with two adversarial sub-routines, a synthesizer, and a checker. CEGIS is a method used in program synthesis to iteratively improve a candidate program by generating adversarial counter examples. The process reduces the size of constraints and ensures correctness over a subset of examples. However, it may face challenges due to the phase transition property of SAT formulas. In this paper, a different approach is proposed to construct a representative subset for solving CSP problems. Instead of using a constraint solver like CEGIS, a neural network is used to learn relationships between input-output examples. The neural network calculates probabilities for examples not in the subset and adds the most \"surprising\" example to the subset. This helps in maximizing search space pruning. The approach proposed in this paper uses a neural network to learn relationships between input-output examples for solving CSP problems. The network adds examples until all examples in the dataset have a high probability, then the subset is given to a constraint solver. Experimental results show that the neural network can represent domain-specific relationships effectively, leading to a cheaper computational cost and improved solution time compared to CEGIS. An example of a diagram drawing DSL is used to illustrate the approach. The paper proposes using a neural network to learn relationships between input-output examples for solving CSP problems. The network adds examples until all have a high probability, then the subset is given to a constraint solver. Experimental results show the network effectively represents domain-specific relationships, reducing computational cost and improving solution time. The drawing DSL defines parameters for creating diagrams, with the synthesis problem being to discover hidden parameter values to reproduce a rendering. The paper proposes an algorithm that outputs a representative subset of input-output examples for solving CSP problems. The algorithm uses a neural network model to compute the probability of all examples, reducing computational cost and improving solution time. The algorithm uses a neural network model to compute the probability of examples in a subset, adding the least probable example to restrict possible solutions. The process stops when the network is confident in reconstructing the target rendering. The paper elaborates on the example selection scheme for generating a representative subset of examples. The algorithm described in the paper uses a neural network model to select examples that restrict possible solutions, aiming to find a sufficient subset close in size to the smallest necessary subset. An approximate algorithm with a count oracle is presented, which greedily constructs the subset by choosing examples that maximally restrict the solution space. The algorithm in the paper uses a neural network model to select examples that restrict possible solutions, aiming to find a sufficient subset close in size to the smallest necessary subset. The count function c(D') is shown to be monotonic and submodular, with the selection criteria in Algorithm 1 using the count oracle c. This poses a practical issue, which is addressed by adopting an alternative selection criteria. The algorithm in the paper uses a neural network model to select examples efficiently, replacing the impractical count oracle. The selection criteria based on probability is used to grow the subset D', with a termination condition for selection. The algorithm in the paper uses a neural network model to efficiently select examples for growing the subset D'. The termination condition for selection is based on the probability of all input-output examples being completely determined given D'. The process involves modeling P r((x, y)| D') with a neural network and utilizing a count-based approach to approximate it before achieving generalization properties. The algorithm utilizes a neural network to select examples for growing the subset D'. It approximates P r((x, y)| D') with a neural network due to the need for sufficient samples for any subset of inputs. The neural network architecture is flexible for different domains, and during training, input and output neuron values are set accordingly. Our neural network architecture resembles a feed-forward auto-encoder with explicitly enumerated input and output neurons. The training task is to predict output values for all possible input values in dom(x), with some input-output values known and the rest encoded as unknowns. The network computes softmax values for all output neurons to obtain probabilities. The neural network architecture resembles a feed-forward auto-encoder, predicting output values for all possible inputs. It computes softmax values for all output neurons to obtain probabilities. An examples reduction algorithm selects input-output examples to build a sufficient subset, approximating the selection probability with a neural network. The termination condition for the algorithm is based on the average probability of examples being greater than a threshold \u03b2. CEGIS ensures a correct solution by selecting examples one at a time and checking against all examples in the set D. The algorithm uses two subroutines, synthesize and check, to iteratively grow a subset of examples D'. CEGIS ensures a correct solution by selecting examples one at a time and checking against all examples in the set D. The algorithm uses two subroutines, synthesize and check, to iteratively grow a subset of examples D'. The routine check finds a counter example that invalidates the candidate solution, prompting the synthesizer to improve its solution. CEGIS terminates when no counter example can be found, guaranteeing pruning of the solution space with each counter-example added. The main drawback is the repeated calls to the constraint solver in the synthesis step. Our synthesis algorithm combines example selections and CEGIS for more efficient solution generation. Our synthesis algorithm combines example selections and CEGIS to efficiently find correct solutions. Example selection is run until a threshold is reached, then the sampled examples are given to CEGIS. CEGIS calls the constraint solver for candidate solutions, checking against the example set until a correct solution is found. By initializing CEGIS with representative examples, the correct solution is found faster. Experiments measure speed, stability, and representativeness of examples produced. The algorithm is evaluated against 400 randomly generated images. Our algorithm combines example selection and CEGIS to efficiently find correct solutions. Experiments evaluate the algorithm against 400 randomly generated images using various synthesis algorithms. The results show the average time breakdown, median, variance, and number of examples selected for each algorithm. The algorithm combines example selection and CEGIS to efficiently find correct solutions. Experiments evaluate the algorithm against 400 randomly generated images using various synthesis algorithms. Results show the average time breakdown, with our algorithm finishing the fastest and cegis a close second. Other algorithms have significantly longer solving time and shorter building time. The algorithm combines example selection and CEGIS to efficiently find correct solutions. Our approach has a drawback of taking around 6 seconds to produce a representative subset, but compared to the overhead cost of constructing the constraint, it is justified. In the median and variance plot, our algorithm has a smaller variance in overall time, achieving higher stability than cegis. The small difference in adding counterexamples results in a huge difference in overall time performance between cegis and rcegis. The ordering of choosing counter-examples greatly impacts performance. Cegis leverages this ordering, producing representative examples efficiently. Rcegis, without this ordering, experiences a significant increase in solving time. Rcegis solves with the least examples but performs poorly in overall solving time. Our approach, along with cegis, selects a larger number of representative examples compared to rand+cegis. The subset chosen by our algorithm is almost perfect, requiring only 1.5 additional counter-examples to arrive at a correct solution. This results in a quick and stable solution, making our algorithm superior to existing ones. In recent years, there has been increased interest in program induction. Unlike existing algorithms, our work assumes a non-differentiable programming model, allowing for the use of expressive program constructs without defining their differentiable counterparts. While other works rely on strong supervision with complete execution traces, our approach only requires labeled input-output pairs. Additionally, our method learns relationships between input-output examples and the syntactic structures of the program, enabling efficient pruning. Our approach in program induction focuses on learning relationships between input-output examples in the semantic domain, unlike other approaches that consider both semantic and syntactic domains. The synthesized images from the drawing program show target images, observations chosen by the neural network, network's estimations, and recovered parameters. The neural network approximates the render based on observations, with recovered parameters on the right."
}