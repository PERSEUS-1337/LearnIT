{
    "title": "ryGgSsAcFQ",
    "content": "In order to select an effective neural network architecture for a modeling problem, understanding the limitations of each option is crucial. These limitations are often described in terms of information theoretic bounds or by comparing complexity needed to approximate functions. This paper explores the topological constraints imposed by a neural network's architecture on the functions it can approximate, independent of network depth for certain activation functions. Neural networks are popular in machine learning due to their flexibility, but choosing architectures and hyperparameters is typically a trial and error process. Understanding the limits of neural network architectures is crucial for selecting an effective model. Neural networks with a single hidden layer can approximate any continuous function with high accuracy, but in practice, networks with multiple low-dimensional hidden layers are more effective. This paper explores whether neural networks with an arbitrary number of hidden layers of bounded dimension can also be universal approximators. Neural networks with bounded dimension hidden layers can approximate functions with L \u221e norm on compact subsets of Euclidean input space. The analysis aims to understand the limitations of deep networks compared to shallow networks on different datasets. The paper discusses the limitations of deep, skinny neural network architectures in approximating functions with bounded level sets in the input space. This result is independent of the number of hidden layers and sheds light on the understanding of neural network limitations. The paper explores the limitations of deep, skinny neural network architectures in approximating functions with bounded level sets in the input space. This result is independent of the number of layers and introduces topological constraints on widely used models, suggesting the potential to apply topological ideas in understanding machine learning algorithms. The experimental results in Sections 5 and 6 demonstrate constraints on neural networks' ability to approximate functions. Previous papers have shown limitations on network architectures, with Lu et al BID15 providing a non-approximation result for ReLu-based deep networks. Their findings are based on the L 1 norm over the entire space R n, while this paper focuses on L \u221e on a compact subset, making Theorem 1 a stronger result even for ReLu networks. Theorem 1 in this paper provides a stronger result compared to existing research, showing that for multi-label classification problems, the region defining each class must be connected if all hidden layers of a neural network have dimension less than or equal to the input dimension. This result could potentially be extended to other activation functions discussed in the paper. Additionally, Rojas demonstrated that discrete classes of points can be separated by a decision boundary of a function defined by a deep, skinny network with single perceptrons in each layer. The curr_chunk discusses the limitations of layered feed-forward networks and the ability of deep belief networks to approximate any function over binary vectors. It also mentions the unexpected constraint on skinny deep nets and raises questions about their practical effectiveness. The curr_chunk discusses the efficiency of deep networks compared to shallow networks, especially for data sets common in physics. It suggests that deep networks are more effective due to their restricted search space around functions that model data shapes likely to appear in practice. This conclusion is supported by research showing that deep networks can approximate functions that shallow networks with a larger number of nodes cannot. Collins, Sohl-Dickstein, and Sussillo found that differences in RNN performance are mainly due to training effectiveness rather than network expressiveness. The effectiveness of model families depends on the ability of algorithms like gradient descent to find accurate models within the search space. Model families are subsets of continuous function spaces, with parametric and non-parametric distinctions being irrelevant in this context. In this section, parametric and non-parametric families are distinguished. A function f approximates another function g if their difference is less than a given value for all x in a compact subset. Model families approximate functions similarly. The paper focuses on the ( , A) definition. Layered neural networks are described using notation with an activation function and a sequence of positive integers. The family of functions N \u03d5,n0,n1,...,n\u03ba approximates any continuous function according to Hornik et al's results. Deep networks with bounded dimensional layers are studied by considering N * \u03d5,n as the union of model families with restricted layer dimensions. Activation functions that can be uniformly approximated by one-to-one functions are of interest for the main result. The main result of the paper is a topological constraint on the level sets of any function in the family of models N * \u03d5,n. This constraint relates to path connectedness and path components in topology. The paper's main result states that deep, skinny neural networks can only approximate functions with unbounded level components. This stricter definition ensures that the property is preserved by limits, unlike just requiring bounded level sets. Theorem 1 shows that layered feed-forward neural networks with limited hidden layer dimensions cannot approximate functions with bounded path components in their level sets. The proof of Theorem 1 shows that deep, skinny neural networks cannot approximate functions with bounded path components in their level sets. The family of functions examined in Section 5 consists of one-to-one activation functions with non-singular transition matrices. Lemmas 2 and 4 establish that any function approximated by N * \u03d5,n can also be approximated by functions in this smaller family, and their level sets have unbounded components. The proof of Theorem 1 demonstrates that deep, skinny neural networks are unable to approximate functions with bounded path components in their level sets. The family of functions analyzed consists of one-to-one activation functions with non-singular transition matrices. Lemmas 2 and 4 establish that any function approximated by N * \u03d5,n can also be approximated by functions in this smaller family, where their level sets have unbounded components. Furthermore, any function in this smaller family can be expressed as a composition of a one-to-one function and a linear projection, implying that each level set/decision boundary in the full function is defined by the intersection of the image of the one-to-one function with a hyperplane. The proof of Theorem 1 is a concatenation of these ideas, including Lemma 5 which states that the limit of functions with unbounded level components also has unbounded level components. The proof of Theorem 1 shows that deep, skinny neural networks cannot approximate functions with bounded level sets. Lemmas 2, 4, and 5 are key in this proof, demonstrating that functions in N * \u03d5,n can be approximated by functions with unbounded level components. The family of non-singular functions N n is defined as the union of all non-singular functions in N * \u03d5,n for all activation functions \u03d5 and a fixed n. Lemma 2 states that if a function g can be approximated by N * \u03d5,n, then it can also be approximated by N n. The proof involves using point-set topology and induction on the composition of functions. The details of the proof are not included. Proof of Lemma 2 involves showing that any function approximated by N * \u03d5,n can also be approximated by N n. By choosing appropriate functions and subsets, we can ensure the approximation holds. If any hidden layers have dimension less than n, we can adjust the network accordingly. The hidden layers in the network have dimension n, with linear functions that may be singular. Functions \u03bdi and \u02c6i need to (\u03b4, Ai)-approximate each other and be non-singular for the composition to be in Nn. We can perturb weights to make them non-singular and ensure function values change by less than \u03b4 on Ai. \u03bdi should be a direct product of continuous, one-to-one activation functions that approximate \u03d5. The tolerance for the approximation needs to be small for \u03bdi to approximate each other. Choosing a single activation function for all nonlinear layers on compact sets allows for the composition to approximate g. Every function in Nn has level sets with unbounded components, extending to any function it approximates. If f is a function in Nn, every level set f^-1(y) is homeomorphic to an open subset of Rn-1, implying unbounded level components. The function f is a composition of linear and nonlinear functions, making it one-to-one and a homeomorphism onto its image If, which is an open subset of Rn. The preimage f^-1(y) for any y in R is an (n-1)-dimensional plane in Rn. The preimage f^-1(y) is an (n-1)-dimensional plane in Rn, which is homeomorphic to an open subset of Rn-1 and closed as a subset of Rn. It is either unbounded or empty, proving that any component of f is unbounded. The preimage f^-1(y) in Rn is an (n-1)-dimensional plane, proving any component of f is unbounded. To extend this property to functions approximated by M, if every function in M has unbounded level components, then any function approximated by M also has unbounded level components. The set F is closed, bounded, and compact, making g(F) a compact subset of R with an open complement. This implies the existence of an open interval disjoint from g(F). The component \u0108 of g^-1(U) containing C is bounded and intersects \u03b7C but is disjoint from its frontier. Each level set intersecting \u0108 has a compact component in \u0108. Each level set intersecting \u0108 has a compact component in \u0108. Let x be a point in C \u2282 \u0108. Assume for contradiction that g is approximated by a model family M with unbounded level components. Choose R > r and let B_R(x) be a closed ball of radius R, centered at x. Since g is approximated by M, choose a function f \u2208 M that approximates g within \u03b5/2. Then |f(x) \u2212 g(x)| < \u03b5/2, defining y = f(x). Every path component of f^-1(y) is unbounded, with a path from x to a point distance R from x. This ensures |f(x) \u2212 g(x)| < \u03b5/2. The function g cannot be approximated by a model family M with unbounded level components, as it leads to a contradiction in the distance from x. The decision boundary learned with six, two-dimensional hidden layers extends outside the visible region, while a network with a single three-dimensional hidden layer learns a bounded decision boundary easily. The effect of Theorem 1 was demonstrated using the TensorFlow Neural Network Playground to train two networks on a synthetic dataset. The decision boundary of a network with six two-dimensional hidden layers extended beyond the data points, while a single three-dimensional hidden layer learned a bounded decision boundary easily. Theorem 1 proves that a network with a bounded loop around points cannot approximate a function with such a level set. The decision boundary shown in the Figure is the closest it can get, with extra hidden layers allowing it to curve around and minimize the blue region's neck. The second network, with a single hidden layer of dimension three, closely approximates the ideal decision boundary with a rounded loop. Increasing the hidden layer dimension would make the boundary rounder, but in this case, it is not necessary. In this paper, topological limitations on functions approximated by deep, skinny neural networks are discussed. A sigmoid neural network with one hidden layer of dimension n + 1 can define a function that cannot be approximated by any deep network with an arbitrary number of hidden layers of dimension at most n. The results are proven using set theoretic topology and visually demonstrated with examples."
}