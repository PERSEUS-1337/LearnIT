{
    "title": "HkgxW0EYDS",
    "content": "We present a neural network weight compression method that utilizes a latent space representation with a learned probability model to compress network parameters. This approach maximizes classification accuracy and model compressibility without complex procedures, achieving state-of-the-art compression on various classification benchmarks. Neural network weight compression methods aim to reduce the size of ANN parameters for practical deployment. Various authors have proposed model compression algorithms to minimize size while maintaining classification accuracy, simplicity, and scalability to large models. Neural network weight compression methods focus on reducing the size of ANN parameters for practical deployment. Compression algorithms aim to maintain accuracy, simplicity, and scalability to large models. Practical compression involves quantization and entropy coding to create a binary representation for storage or transmission. Shannon's source coding theorem establishes entropy as a lower bound on the average length of the binary sequence. Arithmetic coding achieves this bound asymptotically, making entropy a good proxy for the expected model size. The type of quantization scheme used impacts the compression process. The type of quantization scheme used impacts the fidelity of the representation and the bit rate in neural network weight compression. Existing approaches involve scalar quantization (SQ) and vector quantization (VQ), with the latter closely related to k-means clustering. Vector quantization (VQ) is closely related to k-means clustering and involves quantizing each vector of filter coefficients jointly. This results in a finite set of representers, allowing for better adaptation to the parameter distribution compared to scalar quantization (SQ). Vector quantization (VQ) is more general than scalar quantization (SQ) as representers can be placed arbitrarily. However, VQ suffers from the \"curse of dimensionality\" due to the exponential growth of necessary states with dimensions. The strengths of SQ and VQ can be combined by representing data in a \"latent\" space, which can be a rescaling, rotation, or warping of the original data space. The paper introduces a novel model compression method using scalar quantization and entropy penalization in a reparameterized space of model parameters. This approach allows for efficient quantization while maintaining flexibility in representing the model parameters. Additionally, the paper showcases state-of-the-art results on various network architectures and datasets. The method presented does not require complex strategies like pretraining or sparsification to achieve good performance. It can scale to large image datasets and neural network architectures like ResNet-50 on ImageNet. The goal is to minimize the cross-entropy classification loss over the model parameters by compressing each parameter indirectly. The method presented compresses reparameterized forms of each parameter in the set \u0398, using \u03a6 tensors stored in their compressed form. During inference, they are uncompressed and transformed via f into \u0398, the usual parameters of a convolutional or dense layer. The internals of f conv and f dense use different dimensionalities, with f conv using an affine transform and f dense using a scalar shift and scale captured in \u03a8. The number of parameters of f itself (labeled as \u03c8) is significantly smaller than the size of the parameters. The method compresses reparameterized forms of model parameters using a partitioning approach, interpreting groups of parameters as samples from learned distributions. Parameter decoders map reparameterization space to parameter space, with learnable parameters denoted as \u03a8. Group assignments are fixed a priori for parameter sharing within factors of the distribution and corresponding decoders. The method compresses reparameterized forms of model parameters using a partitioning approach. Group assignments are fixed a priori for parameter sharing within factors of the distribution and corresponding decoders. Reparameterizations are defined as rank-2 tensors, with each row representing a sample from the learned distribution. The decoder applies the same transformation to each row, aiding in compression. The size of the groups parameterizes a trade-off between the number of parameters assumed i.i.d. The method compresses reparameterized forms of model parameters using a partitioning approach. The size of the groups parameterizes a trade-off between compressibility and overhead. Encoding all parameters with one decoder and scalar distribution minimizes overhead but may lead to suboptimal compressibility. The group structure of each network is described in more detail in the experiments section. The method compresses reparameterized forms of model parameters efficiently using a discrete alphabet of representers and associated probabilities. Each representer is fixed to integers, and expressivity is achieved via parameter decoders. Fitting probability models to each column of the matrix is done by minimizing negative log-likelihood. Shannon's source coding theorem states that the minimal length of a bit sequence encoding the matrix is the self-information under the distribution. The method efficiently compresses reparameterized forms of model parameters using a discrete alphabet of representers and probabilities. By minimizing negative log-likelihood, probability models are fitted to each column of the matrix. The self-information under the distribution determines the minimal length of a bit sequence encoding the matrix. This method achieves two goals: fitting q to model parameters in a maximum likelihood sense and optimizing parameters for compressibility. An arithmetic code is designed for q post-training to compress the model parameters with minimal overhead. The overall loss function combines cross-entropy classification loss with self-information of reparameterizations, known as the rate loss. Varying \u03bb explores compressed model size vs. accuracy trade-off. Continuous surrogates are used for optimizing the classification loss. The \"straight-through\" gradient estimator is used for the classification loss, rounding the continuous surrogate during training. A relaxation approach is adopted for estimating the rate term and its gradient, replacing probability mass functions with continuous density functions based on small ANNs. This method works well in practice by fitting density models to noise-affected values. The \"straight-through\" gradient estimator is used for classification loss, fitting density models to noise-affected values. The density models are trained, and probability mass functions are derived from substitutesq i and stored in a table. Parameters ofq i are no longer needed after training. The method is evaluated on various image classification networks for MNIST, CIFAR-10, and ImageNet experiments. In evaluating our method on ResNet-18 and ResNet-50 networks, we train models from scratch without pre-trained weights. We use two separate optimizers: one for the probability model and one for the network, with Adam always used for the probability model. The probability model variables only receive gradients from the entropy loss scaled by a rate penalty \u03bb. Adam normalizes out this scale, making the learning rate independent. Adam normalizes the scale for the learning rate of the probability model, making it independent of hyperparameters. The method is applied to LeNet300-100 and LeNet5-Caffe, trained using Adam with a constant learning rate. EMA is used to reduce training noise from quantization. LeNet300-100 is partitioned into three parameter groups, while LeNet5-Caffe consists of convolutional and fully connected layers with max pooling. Our method outperforms baselines for the LeNet300-100 model and is second only to Minimal Random Code Learning for the LeNet5-Caffe model on MNIST datasets. The number of probability distributions is 1 in each parameter group, including convolutional layers, to optimize model size. For MNIST models, simple scalar affine transforms were sufficient for each subcomponent of F. Flexible scale and shift lead to flexible SQ due to quantized integers. More complex transformation functions led to overhead for small networks. Our method was applied to VGG-16 and ResNet-20-4, with specific learning rate schedules and no tuning other than verifying convergence of training loss. VGG-16 and ResNet-20-4 models were not tuned other than verifying training loss convergence. VGG-16 has 13 convolutional layers and 3 fully connected layers, split into four parameter groups. ResNet-20-4 consists of 3 ResNet groups with 3 residual blocks each, partitioned into two parameter groups. Biases were not compressed and added to reported numbers. The inverse real-valued discrete Fourier transform (DFT) outperforms SQ and random orthogonal matrix for training acceleration. Reparameterization benefits high compression regime. VGG-16 and ResNet-20-4 include batch normalization layers without moving averages. Bias term \u03b2 is included in reported numbers with \u03b3 set to 1. Training setup and hyperparameters for ImageNet dataset follow He et al. (2016a). In experiments similar to CIFAR, ResNet models are divided into parameter groups. Comparison is made between SQ, random orthogonal matrices, and DFT for reparameterizing convolution kernels. Various pruning methods are discussed for model compression. In experiments comparing different methods for model compression, Louizos, Ullrich, et al. (2017) and Molchanov et al. (2017) proposed Bayesian Compression and Sparse Variational Dropout. Our Method (SQ) and Minimal Random Code Learning (Havasi et al., 2018) were also evaluated for compression efficiency. DeepCABAC (Wiedemann, Kirchhoffer, et al., 2019) and ResNet-20-4 (CIFAR-10) (Dubey et al., 2018) were included in the comparison. In experiments on model compression, various methods were compared, including Bayesian Compression, Sparse Variational Dropout, Minimal Random Code Learning, and DeepCABAC. Our focus is on efficiently representing parameters for good prediction accuracy, separate from pruning techniques. This work can be combined with pruning methods if reducing the number of units is desired. Quantization involves restricting parameters to a small set of unique values, such as binarizing or ternarizing. Quantization involves restricting parameters to a small set of unique values, with various methods like binarizing or ternarizing networks. Differentiable quantization procedures have been introduced to relax quantization, with the use of straight-through heuristic or other stochastic approaches. Some works extend to non-uniform quantization, while others share weights and quantize by clustering or randomly enforce weight sharing. Coding methods like Huffman coding and arithmetic coding are used for compression in representing weights in the frequency domain. Entropy coding methods exploit data's probabilistic structure to produce optimized binary sequences. Authors often represent quantized values directly as binary numbers with few digits. Some recent work focuses on coding with quantization, aiming to compress models effectively. Han et al. (2015) introduced a four-stage training process for model compression, influencing subsequent publications. Many current high-performing methods have complex implementations and multi-stage training processes. Havasi et al. (2018) and Wiedemann, Kirchhoffer, et al. (2019) require multiple training stages and use computationally expensive techniques. Our method, in contrast, only needs a single training stage and utilizes royalty-free arithmetic coding. Additionally, we will release the source code for easy reproducibility. Our method focuses on model compression by optimizing compressibility and classification performance in a single training stage. It involves reparameterization of model parameters for increased flexibility, outperforming complex methods. The approach focuses on model compression by optimizing compressibility and classification performance in a single training stage. It outperforms complex methods and is particularly suitable for larger models like VGG and ResNets. Future work may explore even more flexible parameter decoders."
}