{
    "title": "HkMwHsCctm",
    "content": "Deep Learning has gained attention for its performance in various tasks, but its theoretical understanding is limited. This study demonstrates that deep neural networks with different architectures, activation functions, and loss functions can be trained effectively using linear programming, with time complexity exponential in input and parameter space dimensions. Improving the dependence on parameter space dimension in training deep neural networks with various architectures, activation functions, and loss functions remains a challenge. This work presents polynomial time algorithms for training fixed network architectures, broadening the applicability to empirical risk minimization problems. The study addresses the complexity of training deep neural networks, including DNNs, CNNs, Binary Neural Networks, and ResNets, with different activation functions and loss functions. Deep Learning is a powerful tool for modeling complex learning tasks, with the ability to train various architectures, activation functions, and loss functions to near optimality using linear programming. The time complexity is exponential in input data and parameter space dimension, but improvements in input dimension are unlikely assuming P \u2260 NP. Polynomial time algorithms are obtained for training fixed network architectures, broadening the applicability to empirical risk minimization problems. Deep Learning methods show exceptional performance across domains, but their fundamental behavior remains poorly understood. Recent focus has been on the computational complexity of training neural networks, formulated as an empirical risk minimization problem. The general risk minimization problem is often unsolvable due to data inaccessibility. In this work, a method is proposed to convert the empirical risk minimization problem associated with training various architectures into a linear programming problem. This method complements and extends previous work by providing a principled approach in the proper learning setting. The obtained linear programming formulations for training neural networks are exponential in input and parameter space dimensions, providing new bounds on computational complexity. Previous work showed that 1-regularized networks can be learned in polynomial time for networks with ReLU-like activations, later generalized to actual ReLU activations. BID5 focused on exact learning for k=2, while this work considers proper learning for any number of layers and various activations, loss functions, and architectures. The approach uses BID9 to reformulate non-convex optimization problems as approximate linear programming formulations, a novel method for training neural networks. Proposed uses of Mixed-Integer and Linear Programming technology in Deep Learning include feature visualization, generating adversarial examples, counting linear regions of a Deep Neural Network, performing inference, and providing strong convex relaxations for trained neural networks. A general framework is established to reformulate ERM problems in Deep Learning into approximate linear programs with explicit bounds on complexity, overcoming limitations of previous approaches. The approach involves handling the accuracy of approximations of non-linearities in approximation functions to achieve target accuracy through principled training using linear programs. The linear program describes a polyhedron P where optimizing a linear function over a specific face solves the problem to optimality, ensuring proper learning in Neural Networks. The linear program for proper learning in Neural Networks involves fixing variables and modifying the objective function to optimize over the data face. The size of the program is measured in bit complexity and the algorithm is formulated and solved using the ellipsoid method. The ellipsoid method is used for proper learning in Neural Networks, with running time polynomial in input size. Size bounds can be improved for specific architectures based on network structure. The constant L measures Lipschitzness of the ERM training problem. Lipschitz constants are linked to generalization. Solutions obtained via linear programming generalize using stochastic optimization techniques. Significant improvement is seen when approximating general risk minimization compared to previous methods. The curr_chunk discusses the assumptions made in the study and highlights three key features of the results obtained. The study provides a solution method with optimality guarantees for the ERM problem, ensures generalization, and linear dependency on data without assuming convexity. The study presents a solution method with optimality guarantees for the ERM problem in a non-convex setting. The linear program constructed is data-independent and can approximate all possible data sets, showing a remarkable linear dependence on the sample size. The approach can be extended to handle regularizers. The study presents a solution method with optimality guarantees for the ERM problem in a non-convex setting. Complexity results for various neural network architectures are discussed, with computations being linear and activation functions including ReLU, Leaky ReLU, eLU, Tanh. Certain improvements in the results can be obtained by further specifying the ERM problem. The study discusses complexity results for neural network architectures with linear computations and various activation functions. Improvements in results can be achieved by specifying the ERM problem further, with the choice of loss functions and nature of output data playing a key role. The training problem for Fully Connected DNNs is extensively studied, with training algorithms having a polynomial dependency on architecture parameters. The study discusses complexity results for neural network architectures with linear computations and various activation functions. Results show a polynomial dependence on architecture parameters, answering an open question regarding training algorithms with polynomial dependence on D. Additionally, a uniform LP can be obtained without compromising the dependence on D. The exponential dependence on other parameters such as input dimension and parameter space dimension is unlikely to be improved. The NP-hardness result in BID10 suggests that improving the input dimension is unlikely, while a recent paper BID4 presents an NP-hard DNN training problem that becomes solvable with a fixed input dimension. The study defines key functions and expectations with respect to random variables. The ERM problem involves finding the solution for a loss function using a model parametrized by \u03c6 \u2208 \u03a6. The Lipschitz constant is defined for the ERM problem. Data-dependent entries are considered as variables in addition to parameters \u03a6. A neural network is a function defined over a directed graph. A neural network is a function defined over a directed graph that maps inputs to outputs. The graph represents the network architecture with layers, including input, output, and hidden layers. The graph can be acyclic or have connections between non-adjacent layers. In feed-forward networks, the graph is acyclic. The treewidth of a graph measures how tree-like it is and is important for solving binary optimization problems efficiently. The treewidth of a graph is a parameter used to measure how tree-like a given graph is. It is defined by a tree-decomposition, which consists of a tree and a family of subsets of the graph's vertices. The treewidth is the minimum width over all tree-decompositions of the graph. An example of a tree-decomposition is provided in the appendix. Additionally, the size of the tree-decomposition, given by the number of vertices in the tree, is also important. The treewidth of a graph is a measure of how tree-like it is, defined by a tree-decomposition. The intersection graph for an instance of BO has vertices for x variables and edges for pairs of x variables in common constraints. The sparsity of a problem is determined by the treewidth of its intersection graph. Theorem 2.5 generalizes a theorem in BID9, distinguishing y variables fully determined by binary x variables. An approximate LP formulation for the ERM problem is discussed. An approximate LP formulation for the ERM problem is data-independent, allowing for the LP to be written before seeing the actual data. This prevents trivial solutions and ensures efficient computability. For a more detailed discussion, refer to BID13, BID12, and BID11. The LP formulation for the ERM problem is data-independent, preventing trivial solutions and ensuring efficient computability. The variables denote data variables assigned values based on a data set. An approximation method is used to represent the data variables. Our strategy is to represent x, y, \u03c6 variables with binary approximations and approximate S(D, \u03a6, , f ). The error of this approximation can be described in the ERM problem induced by discretization. By replacing (x, y, \u03c6) with z variables, we reformulate the convex hull of S (D, \u03a6, , f ) as a linear program, exploiting small treewidth. After replacing the (x, y, \u03c6) variables with z variables, the intersection graph of S (D, \u03a6, , f ) is shown in Figure 1a. A valid tree-decomposition for this graph is depicted in Figure 1b with size D and width N \u03b3 (n + m + N) \u2212 1. The main theorem is proven using N \u03b3 = log(2L/ ) and the tree-decomposition from Figure 1b. Parts (b), (c), and (d) rely on an explicit construction detailed in Appendix D. Optimizing over the face can be achieved by modifying the objective function. If \u03a6 has a network structure, treewidth-based sparsity can reduce the LP size. Additional input structure can further improve the LP size. Optimizing over the face can be achieved by modifying the objective function. Additional input structure can further improve the LP size by potentially improving the n + m exponent on the LP size. The ReLU activation function is used in the network, with each T i being an affine linear function. The node computation in layer i is of the form T z + b, where T is a row of A i and b is a component of b i. The dimension of the parameter space \u03a6 is equal to the number of edges in the network. A technical Lemma is presented to establish a corollary. The architecture Lipschitz constant is proven to be L \u221e ( )w O(k 2 ) by showing that node computations have a Lipschitz constant at most DISPLAYFORM7. This implies that the Lipschitz constants can be multiplied layer-by-layer to obtain the overall architecture Lipschitz constant. ReLUs have a Lipschitz constant equal to 1. A simpler bound was chosen for presentation purposes. The previous lemma showed the architecture Lipschitz constant is L \u221e ( )w O(k 2 ). The current text highlights key differences in the algorithm obtained from solving the LP in Corollary 4.2: (a) Our result has a benign dependency on D, with a polynomial dependency on data-size regardless of architecture. (b) Our approach constructs an LP before seeing the data. (c) The algorithm's dependency on w is polynomial, with an exponential dependency on the number of edges N in the Neural Network. (d) Our algorithm can handle any output dimension m and any number of layers k. (e) Convexity of the loss function is not assumed, affecting the resulting LP size based on its behavior. The LP size in BID5 depends on Lipschitzness of the loss function. BID5 offers advantages over our result, such as no boundedness assumption on coefficients and providing a globally optimal solution. Convolutional Neural Networks (CNN) use convolutional layers to reduce parameters. Lemma 4.5 can be applied to CNNs, with differences in parameter N. Explicit LP sizes for common architectures are provided in TAB0. Our novel framework for training neural networks efficiently depends on Lipschitz constants of loss functions. The approach involves linear programming and relates data set training to the face structure of a polytope. This method improves existing algorithms for neural network training and combines graph theory, polyhedral geometry, and non-convex optimization for Deep Learning. The theoretical foundations laid out in our approach combining graph theory, polyhedral geometry, and non-convex optimization for Deep Learning have practical implications. The architecture dependent terms are worst-case bounds that can be improved by assuming more structure in the problems. Understanding the structure of Linear Programming is crucial for developing incremental solution strategies. Our LP approach connects with stochastic gradient descent, decomposing the problem for each data point and merging them back without losing information. This linear dependence on D resembles SGD, offering a new perspective on algorithm effectiveness and potential synergy between the two approaches. This insight could pave the way for practical implementation. An undirected graph has treewidth \u2264 \u03c9 if there exists a chordal graph with clique number \u2264 \u03c9 + 1. The treewidth of a graph can be verified using a chordal completion. Important results in Section C.1 and Section F include the existence of a solution satisfying certain conditions. The article discusses proper learning in a Neural Network, where the graph defining the network can be partitioned into layers. Each node in a layer performs a computation based on its input nodes. The article discusses proper learning in a Neural Network, where nodes in each layer perform computations based on input nodes. Node computations involve smooth functions and activations, with values passed to out-nodes. The construction of the LP is independent of specific data to prevent designing algorithms for specific datasets. The article discusses proper learning in a Neural Network, where nodes in each layer perform computations based on input nodes. Node computations involve smooth functions and activations, with values passed to out-nodes. The construction of the LP is independent of specific data to prevent designing algorithms for specific datasets. In contrast, a correct algorithm for a specific data set in ERM problem would be a simple print statement of the optimal configuration. LPs resemble a circuit model of computation and can be data-dependent, allowing for the construction of a specific linear program after seeing the data set. Our approach shows that including input data as a variable does not lead to an exponential increase in data set size. It shares similarities with stochastic gradient descent in integrating information from individual data points into the overall solution. Our approach leverages low treewidth to combine solutions from individual training data points, reflected in the linear dependence in problem size. Lemmas 3.1, 4.1, and 4.5 provide proofs for the approximation and verification of results based on Lipschitzness and input dimensions. The input dimension of a node computation is limited to \u2206 instead of w. An activation function with Lipchitz constant 1 and a(0) = 0 satisfies |a(z)| \u2264 |z|. The layer-by-layer argument is applicable in feed-forward networks. The proof involves modifying the tree-decomposition to include y j variables, creating a new tree-decomposition of width at most \u03c9 + 1. The text discusses creating a new tree-decomposition with a width of at most \u03c9 + 1, where variables are contained in single bags. A linear optimization problem is formulated, with the addition of y variables justified by their appearance only in leaves of the tree decomposition. The proof that LBO is equivalent to BO is based on arguments from a previous source. The text explains how variables x and y are placed in separate bags in a tree decomposition, leading to the formulation of a linear optimization problem. The proof of the equivalence between LBO and BO relies on the fact that y variables only appear in leaves of the tree decomposition. The text discusses constructing a linear optimization problem by indexing bags with d and enumerating binary vectors for discretization. The bottleneck lies in creating \u03bb variables, with a time complexity of O((2L/ ) n+m+N D). The LP size estimation includes all possible discretized values of x and y in [\u22121, 1] n+m. Different discretization methods could improve the exponential dependency on the LP size. The text discusses improving the exponential dependency on the input dimension of the LP size in a fully-connected neural network. It introduces binary variables zx, z\u0177 for approximation and defines sets for discretization. The quality of approximation to the ERM problem using these sets is shown in a Lemma. The text discusses improving the exponential dependency on the input dimension of the LP size in a fully-connected neural network by introducing binary variables for approximation. The quality of approximation to the ERM problem using these sets is shown in a Lemma. The proof involves binary approximation and Lipschitzness to establish the optimality of the solution. The optimization problem is equivalent when replacing a function by its convex hull, leading to the face property of the data-independent polytope. The text discusses the use of regularizer terms in optimization problems to prevent over-fitting and promote generalization. Regularization involves adding a function, typically a norm, to the objective function with a parameter \u03bb to control its strength. This technique is commonly used in empirical risk minimization to improve the generalization of the obtained solution. In the context of optimization problems, regularizer terms are used to prevent over-fitting and promote generalization by adding a function to the objective function with a parameter \u03bb. This technique is commonly applied in empirical risk minimization to improve solution generalization. The discussion extends to the reformulation of the ERM problem and tree-decomposition of its intersection graph, with a focus on ERM under Network Structure, specifically in Neural Network training. Exploiting the sparsity of the network can lead to better LP formulations. In the context of optimization problems, regularizer terms are used to prevent over-fitting and promote generalization. Exploiting the sparsity of the network can lead to better LP formulations for Neural Network training. The network is defined by a graph G, and additional auxiliary variables are used to describe node computations and activations. Assumption F.1 restricts the case to Neural Networks with normalized coefficients and bounded node computations. This allows for a better use of the network structure in deriving an analog description of the node-based representation of S(D, \u03a6, , f). The text discusses the use of regularizer terms in optimization problems to prevent over-fitting and promote generalization in Neural Networks. It describes the construction of a tree-decomposition of a graph \u0393, based on an intersection graph \u0393\u03c6, using auxiliary variables and a binary representation. The text also provides insights into the architecture variables associated with edges of the graph G and their constraints. The text discusses the construction of a tree-decomposition of a graph \u0393 based on an intersection graph \u0393\u03c6. It relates the line graph of a graph G to the treewidth of the base graph. The Lipschitz constant for various common loss functions is computed, focusing on both the output layer and data-dependent variables. The text discusses the Lipschitz constant for common loss functions, considering both the output layer and data-dependent variables. It includes Quadratic Loss, Absolute Loss, Cross Entropy Loss with Soft-max Layer, and Hinge Loss. The Lipschitz constant is computed for each function with respect to the infinity norm. A Binarized activation unit (BiU) is defined by a set of parameters and binary input/output values. When forming a network using BiUs, the training problem involves binary input vectors, labels, and a parametrization choice for each unit. For a specific network configuration, it was shown to be NP-hard to achieve zero loss even with restricted parameter values. The authors demonstrated that the problem of training networks with Binarized activation units (BiUs) remains NP-Hard even with restricted parameter values. They applied their techniques to formulate an exact polynomial-size data-independent solution for each fixed network configuration. By reformulating the problem using an epigraph formulation, they were able to show that the optimization problem is binary. This result was proven by applying a theorem directly to the epigraph formulation of BiU. The construction time and data-independence in training neural networks are discussed. A finite sum problem is solved for empirical risk minimization. A small sample size can be chosen to ensure a certain probability of success. The size of the linear program for training depends linearly on the data points. Generalization arguments in stochastic programming are summarized. The construction time and data-independence in training neural networks are discussed, along with solving a finite sum problem for empirical risk minimization. A small sample size can ensure a certain probability of success. The size of the linear program for training depends linearly on data points. A result on the approximation to the GRM problem for neural networks is provided, highlighting key differences in the proper learning setting. The dependency on input dimension, Lipschitz constant, and k in (12) is better than in (13). The algorithm by BID23 extends results by BID18 for depth-2 ReLU networks with convex loss functions, providing a much better dependence. The algorithm by BID23 improves dependency on input dimension, Lipschitz constant, and k compared to previous results for depth-2 ReLU networks with convex loss functions. It achieves better dependence without relying on convexity of the loss function or constant depth of the neural network."
}