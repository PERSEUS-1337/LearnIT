{
    "title": "BJlgNh0qKQ",
    "content": "Human annotation for syntactic parsing is expensive, and resources are limited for many languages. A novel latent-variable generative model for semi-supervised syntactic dependency parsing is proposed, using differentiable dynamic programming over stochastically perturbed edge scores. The method, called Differentiable Perturb-and-Parse, shows effectiveness in experiments on English, French, and Swedish. The representation of bi-lexical relations between words has been widely studied in NLP, leading to efficient parsers for tasks like semantic parsing, machine translation, and question answering. However, syntactic annotation is costly and time-consuming, resulting in small datasets for many languages, making the use of unlabeled texts appealing. The idea of using unlabeled texts as an additional source of supervision is attractive, especially for languages like Vietnamese and Telugu. In the past, semi-supervised parsing was tackled with two-step algorithms, but now neural parsers use word embeddings for unsupervised feature extraction. The question arises whether unlabeled data can be utilized in neural parsers beyond just inducing word representations. The method described is a semi-supervised Variational Auto-Encoder (VAE) that uses a probabilistic model parametrized with a neural network to generate sentences based on a latent dependency tree. The model approximates the posterior distribution over the latent trees through the encoder component of VAE. The generative model and parser parameters are estimated by maximizing the likelihood of unlabeled sentences, ensuring consistency with treebank annotation. The method proposed is a semi-supervised Variational Auto-Encoder (VAE) that aims to generate sentences based on a latent dependency tree. To train the VAE, a combination of maximizing the likelihood of gold parse trees in labeled data and a differentiable Monte-Carlo approach called Differentiable Perturb-and-Parse is used. This approach involves perturbing weights of candidate dependencies and performing structured argmax inference with differentiable dynamic programming to obtain a differentiable relaxation of an approximate sample. Our model introduces a variational autoencoder for semi-supervised dependency parsing using the Differentiable Perturb-and-Parse method. It samples a single tree with soft selection of arcs to preserve higher-order statistics, improving over a supervised baseline in English, French, and Swedish. The approach introduces a novel generative model for learning latent syntactic structures by computing the dependency tree of a sentence using a matrix of booleans. The approach introduces a novel generative model for learning latent syntactic structures by computing the dependency tree of a sentence using random variables in a probabilistic model. The model includes variables for sentences, dependency trees, and sentence embeddings, with a focus on the generative process and posterior distributions over latent variables. The decoder utilizes LSTM and GCN to compute the probability distribution of words in a sentence based on syntactic dependencies. The dependency tree of a sentence is computed using a softmax function. A valid dependency tree is a v0-rooted spanning arborescence, where the graph is connected and each vertex has at most one incoming arc. Projective dependency trees combine contiguous phrases without crossing arcs, maximizing a weighting function to compute the tree. The optimal solution for computing a projective dependency tree can be achieved with a time complexity of O(n^3) using dynamic programming. Restricting the search space to projective trees enforces a structural constraint that can improve accuracy, especially in low-resource scenarios. Eisner's algorithm serves as a deduction system that unifies various parsing algorithms. In this paper, the focus is on projective dependency trees and the learning problem of estimating the matrix W. A generative model is introduced to incorporate unlabeled data in the learning process, allowing for the maximization of likelihood even when the ground-truth dependency tree is unknown. Parameters are learned using a variational Bayes approximation and a discriminative objective on labeled data. The generative process of sentence embedding involves latent variables T and z, with a Bayesian network shown in Figure 2a. The model learns a parametrized distribution p \u03b8 (s|T , z) to fit given samples, with a posterior distribution p \u03b8 (T , z|s) representing underlying representations. The Variational Auto-Encoder (VAE) framework is used to estimate quantities from data by introducing a variational distribution q \u03c6 (T , z|s) similar to p \u03b8 (T , z|s). The Evidence Lower Bound (ELBO) is maximized to minimize the KL divergence between q \u03c6 (T , z|s) and p \u03b8 (T , z|s). The ELBO term is maximized to minimize the KL divergence between q \u03c6 (T , z|s) and p \u03b8 (T , z|s). A surrogate objective is defined to replace the original objective. The ELBO has two components: the KL divergence with the prior and a non-trivial term. Training involves using the Monte-Carlo method for estimation. Sampling from q \u03c6 (T , z|s) encodes observations into the latent space, while regenerating a sentence decodes from the latent space. Differentiable sampling is required for VAE training, with q \u03c6 (z|s) defined as a diagonal Gaussian. VAEs are useful for semi-supervised learning in NLP. The reparametrization trick allows backpropagation through the sampling process. However, this approach cannot be applied to dependency tree sampling. To estimate the posterior distribution over dependency trees, a discriminative training term is added to the overall loss. The loss function for training a semi-supervised VAE involves a discriminative training term. The encoder and decoder distributions are described using neural parametrization. The encoder is factorized into components for dependency trees. The sentence embedding model is a diagonal Gaussian parametrized by an LSTM. An autoregressive decoder is used, combining an LSTM and a Graph Convolutional. The autoregressive decoder in the model combines an LSTM and a Graph Convolutional Network (GCN) to incorporate syntactic dependencies. The LSTM keeps track of generated words' history, while the GCN utilizes information from the latent variable z for sentence embedding. The LSTM's hidden state at each position i is used to predict the word, which is then transformed to consider the syntactic structure described by the latent variable T. The output of the LSTM is passed through multi-layer perceptrons before being fed to the GCN. The output of the LSTM is fed to distinct multi-layer perceptrons that characterize syntactic relations in the GCN. The GCN output vector is used to estimate the probability of word s i. The VAE in the neural architecture contains stochastic nodes that require a Monte-Carlo estimation of the gradient for optimization. In this section, a Differentiable Perturb-and-Parse operator is introduced to handle distribution over dependency trees. An approximate sampling process is proposed by computing the best parse tree with independently perturbed arc weights. Additionally, a differentiable surrogate of the parsing algorithm is suggested. The technique of perturb-and-map is also discussed for sampling from categorical distributions. The algorithm perturb-and-map is used for dependency tree sampling by expressing arc-factored dependency parsing as a MRF. The technique involves setting P i,j = \u2212 log(\u2212 log U i,j ) where U i,j \u223c Uniform(0, 1). Additionally, a function is introduced to search for the best split point for constructing an element given its span. The algorithm perturb-and-map is used for dependency tree sampling by expressing arc-factored dependency parsing as a MRF. It sets T i,j to 1 and propagates contribution information to antecedents. A Monte-Carlo estimation of the expectation is defined, with a sampling process outside the backpropagation path. The EISNER algorithm, using ONE-HOT-ARGMAX operations, has ill-defined partial derivatives, leading to a proposal for a differentiable surrogate. A continuous relaxation of the projective dependency parsing algorithm is then introduced. The parsing-as-deduction formalism provides a unified presentation of parsing algorithms, defined as deductive systems with axioms and deduction rules. Implementation typically involves dynamic programming to deduce items in a bottom-up fashion and store intermediate results in a global chart. For projective dependency parsing, the algorithm builds a chart representing sub-analyses based on word relationships. The algorithm for projective dependency parsing involves computing maximum weights of items in a bottom-up fashion. It includes two stages: the first stage computes weights, and the second stage retrieves arcs contributing to the optimal objective. The algorithm constructs a computational graph with sets of nodes representing variables. The algorithm for projective dependency parsing involves computing maximum weights of items in a bottom-up fashion. It includes two stages: the first stage computes weights, and the second stage retrieves arcs contributing to the optimal objective. The algorithm constructs a computational graph with sets of nodes representing variables. This graph includes ONE-HOT-ARGMAX operations that are not differentiable. A recent trend in differentiable approximation replaces it with the PEAKED-SOFTMAX operator, making the parsing algorithm fully differentiable. However, the outputs are no longer valid dependency trees as they contain continuous values representing soft selection of arcs. An alternative approach for tagging with the Viterbi algorithm has been introduced. The continuous relaxation of EISNER's algorithm involves using a soft selection of arcs in a GCN decoder, which simplifies message passing between nodes. This approach is more efficient than using Recursive LSTMs, as it maintains computational cost while avoiding the need for building a large set of RNN-cells. The study conducted experiments on semi-supervised dependency parsing in English, French, and Swedish. The method used dynamic programming with weighted inputs in a non-GPU-friendly manner. The hyperparameters and settings for the network were detailed in the appendices, with a focus on avoiding bias towards the semi-supervised scenario. The approach aimed to be language-agnostic and did not rely on part-of-speech tags in the network. The study focused on semi-supervised dependency parsing in English, French, and Swedish without using part-of-speech tags in the network. Training times for the supervised and semi-supervised parsers were compared, with results presented in Table 2. Evaluation metrics for dependency lengths and labels were also reported for both scenarios. In the semi-supervised setting, experiments were conducted with and without latent sentence embedding z. Comparison was made only to the BID30 model, with recent more accurate models like BID12 being orthogonal to the proposal. Despite experimenting with bi-affine attention from BID12, it did not benefit the low-resource setting. Results were reported with a structured hinge loss baseline (BID30 BID76), showing score increases in all three languages. In the semi-supervised setting, experiments were conducted with and without latent sentence embedding. VAE performs slightly better without latent sentence embedding, possibly due to fewer information leaks. Training with structured hinge loss gives stronger results than the supervised baseline. Qualitative analyses for English show improvements in dependency lengths. The semi-supervised parser corrects errors, especially in root attachments. The semi-supervised parser shows higher precision and better recall for long distance relations compared to the supervised parser. It also improves dependency parsing for multi-word expressions, which are known to be challenging in NLP. Dependency parsing in the low-resource scenario is of interest in the NLP community due to annotation costs. Transfer approaches use a delexicalized parser for a resource-rich language to parse a low-resource one, while grammar induction learns a dependency parser unsupervisedly. Variational Auto-Encoders have been explored in semi-supervised NLP settings. Variational methods have been used in semi-supervised NLP tasks like morphological re-inflection and semantic parsing. Previous work relied on categorical random variables or the REINFORCE score estimator, but no prior research has explored continuous relaxation of dynamic programming latent variables in the VAE setting. The main challenge is backpropagation through discrete random variables, with the Gumbel-Softmax operator introduced for categorical distributions. Building a reparametrization of the sampling process is necessary for more complex discrete distributions. Building on previous work in variational methods for semi-supervised NLP tasks, the current research focuses on the sampling process and the creation of a differentiable surrogate for the structured arg max operator. Various approaches have been proposed, including using structured attention and dynamic program smoothing algorithms. The study introduces a continuous relaxation method using parsing-as-deduction formalism and explores the replacement of true gradients with proxies to satisfy constraints on the arg max operator. Additionally, there is a parallel line of work on differentiable sparse structures. The research focuses on a generative learning approach for semi-supervised dependency parsing, using a VAE to model the dependency structure of a sentence as a latent variable. Future work includes exploring informative priors for the dependency tree distribution and extending the technique to unsupervised scenarios. Techniques like the reparametrization trick and Gumbel-Max trick are used for sampling from distributions. The Gumbel-Max trick involves adding random Gumbel noise to log-probabilities to sample from a distribution. Sampling from a unit-simplex can be re-expressed using the Gumbel distribution. Stanford Dependency conversion is used on the Penn Treebank for training, development, and testing. Annotation is limited to 10% of the training set to simulate a low-resource setting. In a low-resource setting, annotation is limited to 10% of the training set. French and Swedish datasets are used with specific train/dev/test splits. The Talbanken dataset is split into labeled and unlabeled sections for training. Word embeddings of size 100 are concatenated for encoding. The encoder for the model includes trainable word embeddings of size 100, external word embeddings, a dependency parser with a two-stack BiLSTM, and a sentence embedding using a left-to-right LSTM with a hidden size of 100. No part-of-speech tags are used as input in any part of the network. The model utilizes a left-to-right LSTM with a hidden size of 100 for sentence encoding. The hidden layer is then passed to two single-layer perceptrons to compute means and standard deviations of a Gaussian distribution. The decoder uses fixed pre-trained embeddings and a LSTM with a hidden layer size of 100. Training involves bootstrapping with labeled data, starting with discriminative loss, then adding supervised ELBO term, and finally unsupervised ELBO term after the 6th epoch. In the 6th epoch, the network is trained using stochastic gradient descent for 30 epochs with Adadelta. In the semisupervised scenario, labeled and unlabeled instances are alternated. The PEAKED-SOFTMAX operator's temperature is fixed at \u03c4 = 1. Dynamic programs for parsing use different semirings like R, max, +. The backpointer trick maintains an optimal O(n^3) complexity by implicitly constructing sets. The continuous relaxation of projective dependency parsing in a neural network replaces the max operator with a smooth surrogate. The output variables represent a soft selection of dependencies, and during back-propagation, partial derivatives of the loss with respect to arc weights are computed efficiently without explicitly calculating the Jacobian matrix. The continuous relaxation of projective dependency parsing in a neural network replaces the max operator with a smooth surrogate. The forward and backward passes have cubic complexity similar to Eisner's algorithm. The inside algorithm computes weight, soft backpointers, and cumulative weight for each item. The backpointer reconstruction algorithm computes the contribution of each arc. The backpointer reconstruction algorithm computes the contribution of each arc by following backpointers in reverse order. During the backward pass, partial derivatives of variables are computed using the chain rule in reverse order of creation. Backpropagation through the backpointer reconstruction algorithm is straightforward to compute, as well as the partial derivatives of the inside algorithm's variables."
}