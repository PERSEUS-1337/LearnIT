{
    "title": "SygLehCqtm",
    "content": "Inferring protein structural properties from amino acid sequences is a challenging task. Existing methods struggle with recognizing structural patterns in diverged sequences. A new approach using representation learning maps protein sequences to vector embeddings encoding structural information. LSTM models are trained with feedback from global structural similarity and pairwise residue contact maps. The study introduces a novel similarity measure for protein sequences based on soft symmetric alignment. Their method outperforms other sequence-based approaches and even a top structure-based alignment method in predicting structural similarity. The learned embeddings also enhance transmembrane domain prediction, showcasing the versatility of the approach. Proteins fold into specific 3D conformations based on amino acid sequences, determining functions like binding specificity and catalytic activity. Experimental structure determination is costly and limited, hindering the study of protein mechanisms. Finding proteins with related structures from sequences is challenging due to loose relationship between sequence and structure similarity. Transfer of knowledge between proteins with similar structures is restricted. In this work, the focus is on learning protein sequence embeddings using weak supervision from global structural similarity for the first time. The goal is to develop a bidirectional LSTM (biLSTM) embedding model that maps amino acid sequences to vector representations, aiming to bring residues in similar structural contexts closer in the embedding space. To address the lack of position-level correspondences between sequences, a whole sequence similarity measure is defined from vector embeddings, involving alignment and pairwise comparison of positions. A soft symmetric alignment (SSA) mechanism is proposed for alignment, enhancing the understanding of protein structures. The multitask framework extends the directional alignment in attention mechanisms to include position-level supervision from residue contacts within proteins. It leverages global structural similarity and residue-residue contacts for training embedding models. The similarity prediction module aligns protein sequences based on vector embeddings and predicts their shared SCOP level. The contact prediction module uses vector embeddings to predict contacts between amino acid positions within each protein. The model uses embeddings to predict contacts between amino acid positions in proteins and compares these predictions with observed contacts in protein structures. It outperforms other methods in predicting structural similarity and even surpasses methods that require known protein structures. An ablation study is conducted to assess the importance of different modeling components for structural similarity. The study evaluates the importance of modeling components for structural similarity prediction and secondary structure prediction. The SSA method outperforms other alignment methods and including contact prediction training improves performance. The learned embeddings are applicable to other protein machine learning tasks, enhancing transmembrane prediction. This work bridges the sequence-structure gap with representation learning, a departure from traditional unsupervised k-mer co-occurrence approaches for protein sequence embeddings. The study focuses on learning fixed sized vector representations for protein sequences using unsupervised methods. Unlike manual feature engineering approaches, the goal is to capture the full structural context of amino acids. This approach aims to bridge the sequence-structure gap and enhance protein machine learning tasks such as transmembrane prediction. The study applies a language model inspired by successful unsupervised contextual embedding models to protein sequences within a supervised framework. Cross lingual word embeddings and document similarity problems have also been explored, with bilingual word embeddings learned jointly with a word alignment model. The study explores cross lingual word embeddings and document similarity using a FastAlign BID20 model and BilBOWA BID21. Gouws et al. assume uniform word alignments, while Word Mover's Distance (WMD) aligns words between documents. However, WMD is not suitable for neural network embedding models due to high computational costs. The proposed SSA addresses these issues efficiently. The SSA method efficiently solves problems using alignment mechanisms inspired by previous work on soft alignments and attention mechanisms for sequence modeling. Unlike other methods, SSA is symmetric, memoryless, and designed for learning interpretable embeddings based on sequence element similarity. It is fast, memory efficient, and scales with the product of sequence lengths. Protein fold recognition involves classifying proteins into folds based on their sequences, traditionally relying on sequence homology. Our focus is on learning protein sequence representations that capture structure information for improved detection of structural similarity from sequence. This approach aims to produce transferable features for various learning tasks, unlike methods based on sequence homology. The framework described includes components such as a multi-layer bidirectional LSTM with inputs from a pretrained LSTM language model, soft symmetric alignment, ordinal regression for structural similarity, and a convolutional neural network for residue-residue contact prediction. The BiLSTM encoder encodes protein sequences into vector representations, and the pretrained language model is utilized in the process. The framework includes a multi-layer bidirectional LSTM with inputs from a pretrained LSTM language model for encoding protein sequences into vector representations. The language model is pretrained on raw protein sequences to predict amino acids at each position. The framework incorporates a biLSTM sequence encoder with a learned linear transformation of position and amino acid representations. The model is trained to predict global structural similarity between protein sequences based on shared membership in the SCOP hierarchy. The framework involves a multiclass classification problem for protein similarity based on SCOP hierarchy levels. Protein sequences are compared using vector embeddings and soft symmetric alignment to predict similarity classes through ordinal regression. The framework involves a multiclass classification problem for protein similarity based on SCOP hierarchy levels. Protein sequences are compared using vector embeddings and soft symmetric alignment to predict similarity classes through ordinal regression. The alignment score is related to ordinal structural similarities using an ordinal regression framework with binary classifiers. The framework involves multiclass classification for protein similarity based on SCOP hierarchy levels. It uses vector embeddings and soft symmetric alignment to predict similarity classes through ordinal regression. The structural similarity loss is calculated with constraints, and parameters are fit jointly with the sequence encoder. Position-level correspondence is inferred between sequences, and a secondary task of within protein residue-residue contact prediction is introduced to improve embeddings. Contact prediction is a binary classification problem. In contact prediction, residues at positions i and j are classified based on whether they make contact in the 3D structure. Pairwise features tensor is created from embedding vectors for each pair of positions in an arbitrary protein of length N. This featurization is symmetric and widely used for pairwise comparison models in NLP. In contact prediction, pairwise comparison models in NLP use embedding vectors transformed through a hidden layer for contact predictions. A 7x7 filter is convolved over the tensor to predict contact probabilities. The contact prediction loss is defined as the cross entropy between observed labels and predicted probabilities. The multitask objective combines structural similarity and contact prediction losses with a parameter \u03bb. The error signal is backpropagated through contact prediction parameters. Our encoder consists of 3 biLSTM layers with 512 hidden units each and a final output embedding dimension of 100. Language model hidden states are projected into a 512 dimension vector before being fed into the encoder. In the contact prediction module, a hidden layer with dimension 50 is used. These hyperparameters were chosen to fit on a single GPU with reasonable minibatch size. Performance comparisons with simpler encoder architectures are discussed in section 4.2, with potential for further improvement through architecture search. Sequence embedding models are trained for 100 epochs using ADAM with a learning rate of 0.001 and default parameters provided by PyTorch. Each epoch consists of 100,000 examples sampled from the SCOP structural similarity training set with smoothing of the similarity level distribution of 0.5. The probability of sampling a pair of sequences with similarity level t is proportional to N 0.5 t where N t is the number of sequence pairs with t similarity in the training set. This upweights sampling of highly similar pairs of sequences. The structural similarity component of the loss is estimated with minibatches of 64 pairs of sequences. When using the full multitask objective, the contact prediction component uses minibatches of 10. During training, the contact prediction component uses minibatches of 10 sequences with \u03bb = 0.1 and a small perturbation by resampling amino acids at each position with probability 0.05. Models were implemented in PyTorch and trained on a single NVIDIA Tesla V100 GPU for roughly 3 days, requiring 16 GB of GPU RAM. The 3-layer biLSTM encoder trained with the full framework is referred to as \"SSA (full)\" and without contact prediction as \"SSA (no contact prediction).\" Performance evaluation was done on the SCOP dataset for predicting structural similarity between amino acid sequences. The study benchmarks an embedding model for comparing amino acid sequences against various protein comparison methods, including NW-align, phmmer, HHalign, and TMalign. Accuracy, Pearson's correlation, Spearman's rank correlation, and average precision scores are used for evaluation. The model is trained on a single GPU for 3 days and tested on the SCOP dataset for predicting structural similarity. The study evaluates an embedding model by comparing amino acid sequences using various protein comparison methods. It includes accuracy, correlation scores, and average precision for retrieving protein pairs with structural similarity at different levels. SCOP benchmark datasets are created from SCOPe ASTRAL datasets, with a new test set from SCOPe 2.07 release. The study evaluates an embedding model for comparing amino acid sequences using different protein comparison methods. The model outperforms other methods in terms of prediction accuracy, correlation scores, and retrieving proteins with similar folds. The new test set from SCOPe 2.07 release consists of 688 protein sequences with an average percent identity of 13%. The full SSA embedding model improves overall prediction accuracy, Pearson's correlation, and Spearman's rank correlation compared to other methods without requiring a database search. The full SSA embedding model outperforms TMalign in predicting shared SCOP membership, even though it only uses sequence information. The model shows significant improvement at the SCOP class level compared to TMalign. Additionally, the model components are evaluated on structure similarity prediction and secondary structure prediction tasks. The study evaluates the performance of different model components for structure similarity and secondary structure prediction tasks on a protein sequence dataset. The dataset is split into training and testing sequences, with each position treated as an independent datapoint with features. The full SSA embedding model outperforms TMalign in predicting shared SCOP membership, showing significant improvement at the SCOP class level. The study evaluates model components for structure similarity and secondary structure prediction tasks on protein sequences. Each position is treated as an independent datapoint with features for training a neural network. The SSA mechanism outperforms alternative methods like uniform alignment and mean embedding. The study compares different methods for structure similarity and secondary structure prediction tasks on protein sequences. The SSA mechanism is found to be superior to alternative methods like uniform alignment and mean embedding. The ME model performs well on secondary structure prediction but poorly on SCOP similarity prediction, while the UA model shows the opposite trend. The study compares different methods for structure similarity and secondary structure prediction tasks on protein sequences. The SSA mechanism captures the best of both ME and UA models, improving SCOP similarity prediction and secondary structure prediction. Including contact prediction task significantly enhances the quality of the embeddings, leading to improved accuracy in secondary structure prediction. The contact prediction task improves the quality of local embeddings on top of weak supervision from whole structure comparison. Results for transmembrane prediction methods are compared using BiLSTM+CRF models with different embeddings. Pretrained language models play a crucial role in transferring information to supervised sequence modeling problems. The inclusion of language model hidden layer inputs significantly improves performance in supervised sequence modeling problems, as shown by SCOP similarity classification results for SSA embedding models. However, LM hidden states alone are not sufficient for high performance on structural similarity tasks, with simpler models showing lower accuracy compared to a 3-layer biLSTM architecture. This highlights the potential utility of protein sequence embedding models for transferring structural information to other sequence prediction problems. In transmembrane prediction, the goal is to identify segments of amino acid sequences that cross the lipid bilayer in proteins integrated into the cell membrane. Methods involve HMMs with complex hidden state and emission distributions. Newer approaches also focus on detecting signal peptides at the start of protein sequences. To evaluate embedding vectors, a CRF model is developed to assess their performance. The study develops a CRF model with a single layer biLSTM for predicting hidden states based on embedding vectors. Transition probabilities are evaluated using a biLSTM + CRF model with 1-hot encodings. Results are reported for predicting different protein regions, and transmembrane state labels are predicted using Viterbi decoding. The model is evaluated on the TOPCONS transmembrane benchmark dataset through 10-fold cross validation. The study develops a CRF model with a single layer biLSTM for predicting hidden states based on embedding vectors. Transition probabilities are evaluated using a biLSTM + CRF model with 1-hot encodings. Results are reported for predicting different protein regions, and transmembrane state labels are predicted using Viterbi decoding. The model is evaluated on the TOPCONS transmembrane benchmark dataset through 10-fold cross validation. Our transmembrane predictions rank first or tied for first in 3 out of the 4 categories (SP+TM, Globular, and Globular+SP) and ranks second for the TM category, with a prediction accuracy of 0.89 vs 0.87 for TOPCONS. In this work, a novel alignment approach using weak supervision from a global similarity measure is proposed. The SSA model outperforms competition in predicting protein structural similarity, including structure alignment with TMalign. Training using SCOP focuses exclusively on single-domain protein sequences. The biLSTM encoder focuses on single-domain protein sequences, with potential for improvement in modeling multi-domain contexts. The resulting embeddings can enhance transmembrane region prediction and other protein tasks. These embeddings can be integrated into methods using HMM profiles or scoring matrices, and can be applied to non-biological tasks as well. The LSTM language model was trained on protein domain sequences in the Pfam database, consisting of 21,827,419 total sequences. The model architecture included a 2-layer LSTM with 1024 units in each layer, followed by a linear transformation for amino acid prediction. Parameters were shared between forward and reverse components. The model was trained for a single epoch using ADAM with a learning rate of 0.001 and minibatch size of 32. Resampling probability and \u03bb hyperparameters were selected based on optimization. The resampling probability and \u03bb hyperparameters were chosen based on structural similarity prediction accuracy on a validation set from the SCOP ASTRAL 2.06 training set. 2,240 random sequences were held out for validation, with 100,000 pairs randomly sampled. Evaluation of amino acid resampling probability and contact prediction loss weight, \u03bb, showed that a resampling probability of 0.05 improved structural similarity prediction. Models trained with different \u03bb values also showed varying performance. Based on the evaluation of amino acid resampling probability and contact prediction loss weight, a resampling probability of 0.05 improved structural similarity prediction. Models trained with different \u03bb values also showed varying performance, with all models trained with contact prediction outperforming those trained without it. The best \u03bb value of 0.1 was chosen for training models with the full framework. Different methods were used for computing similarity between protein sequences, including NW-align, phmmer, HHalign, and TMalign. The overall HHalign score was calculated by averaging the scores of two structures. Similarly, the overall TMalign score for each pair of proteins was obtained by averaging the query->target and target->query alignments. Classification accuracy was determined by binning scores into similarity levels using 100,000 pairs of sequences from the ASTRAL 2.06 training set. Results for contact prediction using the trained contact prediction module were also included, with precision, recall, and F1 score calculated. The text discusses the contact prediction performance of the full SSA model on the SCOP ASTRAL 2.06 test set and the 2.07 new test set. Precision, recall, F1 score, and the area under the precision-recall curve (AUPR) are reported for predicting all contacts and distant contacts in the test set proteins. Additionally, the precision of the top L, L/2, and L/5 predicted contacts is also mentioned. The full SSA model outperforms other co-evolution based methods in predicting all contacts but performs worse in predicting distant contacts. This is because the model is trained to predict all contacts, where local contacts are more abundant than distant contacts. Our model, tuned for structural similarity prediction, outperforms the co-evolution method GREMLIN in predicting distant contacts. The contact prediction module is simple but could potentially achieve better performance with a more sophisticated architecture. The results suggest that our embeddings could be useful in combination with co-evolution features for improving contact prediction models on both local and distant contacts. The study compares CASP12 free modelling targets with co-evolution based methods, reporting precision, recall, F1 score, and AUPR for predicting contacts."
}