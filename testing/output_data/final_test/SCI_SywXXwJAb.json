{
    "title": "SywXXwJAb",
    "content": "In this work, a fundamental connection between quantum physics and deep learning is established to gain insights into the inductive bias of convolutional networks. The study shows a structural equivalence between ConvAC and quantum wave functions, allowing for the use of quantum entanglement measures to assess a network's ability to model correlations. Additionally, a deep ConvAC can be constructed using a quantum Tensor Network, enabling a graph-theoretic analysis that links network expressiveness to its underlying graph structure. The study establishes a connection between quantum physics and deep learning to understand the inductive bias of convolutional networks. It demonstrates control over inductive bias through the number of channels in each layer and validates findings on standard convolutional networks. This work introduces interdisciplinary bridges by describing deep convolutional networks using graph-theoretic tools and linking them to quantum entanglement. The inductive bias in convolutional networks is crucial for machine learning tasks, with architectural features like the number of layers and channels playing a significant role. The inductive bias of convolutional networks is influenced by architectural features like the number of layers and channels. Deep Convolutional Arithmetic Circuits (ConvACs) support exponential separation-ranks for certain input partitions but are limited to polynomial separation-ranks for others. ConvACs are a special class of convolutional networks with linear activations and product pooling. They play a key role in theoretical analyses of convolutional networks due to their algebraic structure. The structural equivalence between a function modeled by a ConvAC and a many-body quantum wave function allows for the use of quantum entanglement measures in deep learning. The use of quantum entanglement in deep learning involves compact representations of many-body wave functions through Tensor Networks (TNs). TNs represent higher-order tensors through inner-products among lower-order tensors, allowing for a natural representation through an underlying graph. This differs from traditional tensor decompositions used in analyzing correlations modeled by deep convolutional networks. The ConvAC can be mapped to a TN, connecting deep convolutional networks to graph theory. Results show the relationship between network architecture and prior knowledge, avoiding bottlenecks by setting the number of channels in each layer based on task requirements. The ability of a ConvAC to represent correlations is linked to a min-cut over edge-cut sets in the associated TN, allowing for tailored network architecture. Theoretical analysis of deep ConvAC architecture shows applicability to conventional deep convolutional networks. Insights on network performance related to channel numbers and bottlenecks are provided, connecting ConvACs to tensor networks and graph theory. The potential for min-cut analysis and connections to quantum entanglement in deep networks are highlighted. The use of tensor networks in machine learning, specifically in the context of training a matrix product state architecture for supervised learning tasks, is gaining interest in the physics community. There is a theoretical mapping between restricted Boltzmann machines and tensor networks, connecting entanglement bounds to expressiveness. Tensor analysis background is provided for understanding the analyses of ConvACs and tensor networks in this paper. A tensor is a multi-dimensional array with modes and dimensions. Matricization of a tensor is arranging its elements as a matrix. A TN is a weighted graph where each node corresponds to a tensor. A TN is a weighted graph where each node corresponds to a tensor, with edges representing different modes of the tensor. Connectivity properties involve edges representing operations between tensors, contracted indices for summation, and open indices for calculations. A tensor network (TN) can be calculated by summing over contracted indices. In a TN for multiplying a vector by a matrix, the result is an order 1 tensor. This operation is a generalization of matrix multiplication and is used in quantum mechanics and machine learning for expressing complex relations. In this section, the ConvAC architecture is introduced, which is a deep convolutional network with linear activations and product pooling layers. It is closely related to SimNets and has shown effectiveness in various practical settings such as optimal classification with missing data and compressed networks. The ConvAC architecture is a deep convolutional network with linear activations and product pooling layers, closely related to SimNets. It has been effective in practical settings like optimal classification with missing data and compressed networks. Through generalized tensor decompositions, ConvAC can be transformed into a standard convolutional network with ReLU activation and average/max pooling. This architecture was chosen for analysis due to its tensorial structure resembling the quantum many-body wave function. The input space of the network is represented as an image, with local patches denoted by x j. The network outputs, h y (x 1 , ..., x N ), have a specific form involving tensors A y and A (rank-1) of order N and dimension M in each mode. The conv-weights tensor A y entries are polynomials in the network's convolutional weights. The quantum mechanical properties of a many-body system are described by physicists using wave functions denoted by |\u03c8. States in finite dimensional Hilbert spaces are discussed, with each particle corresponding to a local Hilbert space H j of dimension M. The orthonormal basis of the local Hilbert space is denoted by {|\u03c8. The many-body wave function |\u03c8 in a system of N particles with no quantum correlations can be written as a product state |\u03c8 ps = |\u03c6 1 \u2297 \u00b7 \u00b7 \u00b7 \u2297 |\u03c6 N, where each local state |\u03c6 j \u2208 H j is expanded in the respective basis. The many-body wave function can be represented as a product state with local states expanded in respective bases. The inner product between the quantum state and the tailored product state is related to a convolutional network's function. The conv-weights tensor is similar to the coefficients tensor of the many-body wave function. This analogy helps analyze the expressiveness of a convolutional network through its underlying tensor properties. The concept of quantum entanglement measures is used to analyze correlations in deep convolutional networks. The separation-rank is a tool for measuring correlations between different parts of the input in a function. The separation rank is a measure of correlations in a function between different parts of the input. It allows control over the inductive bias when designing deep network architecture to model characteristic correlations in the input. In the physics domain, attention is given to inter-particle correlation structures in many-body wave functions. Quantum entanglement measures quantify these correlations, similar to separation ranks. Partitions of particles into subsystems A and B are analyzed using Hilbert spaces and Schmidt decomposition. The quantum wave function can be expressed in terms of singular values and vectors obtained through a singular value decomposition. Entanglement measures quantify correlations between partitions of particles, with entanglement entropy being one such measure. The entanglement entropy method for quantifying quantum correlations can be applied to machine learning. The conv-weights tensor in convolutional networks can be quantified using entanglement measures based on singular values. The separation rank of the conv-weights tensor is equal to the Schmidt number, suggesting more sensitive correlation measures can be borrowed from physics. The separation rank can be borrowed to provide a more sensitive algebraic perspective on convolutional networks' hypothesis space, considering the relative magnitudes of non-zero singular values. Physicists can use knowledge of quantum entanglement measures to design computational representations of quantum wave functions, which can be applied to convolutional networks. The coefficients of the conv-weights tensor encapsulate information about correlations in the many-body quantum wave function or function realized by a ConvAC, allowing for control of the network's inductive bias. The curse of dimensionality in quantum many-body systems makes it impractical to investigate wave functions with many interacting particles. Tensor Networks are used in physics to represent complex wave functions with fewer resources. A ConvAC can be constructed as a Tensor Network to enhance its expressivity by tailoring the number of channels in each layer. The ConvAC can enhance its expressivity by tailoring the number of channels in each layer of the deep network to fit the function realized by it. Parameters can be efficiently distributed based on prior knowledge of the input, matching the inductive bias to the task at hand. The ConvAC is represented as a Tensor Network with special tensors enforcing channel pooling attributes. The Tensor Network (TN) is a weighted graph with edges corresponding to the number of channels in each layer of a deep ConvAC. This allows for modeling known input correlations and applying graph-theoretic tools to the network. The key accomplishment is the ability to relate quantum entanglement to TNs, enabling the design of networks that support specific input correlations. The Tensor Network (TN) is a weighted graph representing the number of channels in each layer of a deep ConvAC. It relates quantum entanglement to TNs, allowing for the design of networks supporting specific input correlations. The network's ability to model correlations between input regions A and B is tied to the multiplicative minimal cut separating them, with practical implications for constructing deep network architectures. Choosing channel numbers in a Tensor Network to support correlations between input regions A and B is crucial for modeling intricate correlation structures. The multiplicative minimal cut weight plays a key role in determining network expressiveness, with implications for network architecture design. Selecting a small number of channels for deeper layers can create undesired shortcuts that hinder the network's ability to model long-range correlations. Different partitions, such as left-right or interleaved, require careful consideration to ensure optimal network performance. The multiplicative minimal cut weight in a Tensor Network is crucial for network expressiveness and architecture design. Different partitions, like left-right or interleaved, impact network performance by supporting correlations between input regions A and B. The min-cut result applies to any partition, allowing conclusions on layer widths for various correlation length-scales. Factors contributing to the min-cut between A and B include channel numbers and segment lengths. The addition of more parameters to layers in a Tensor Network above a certain threshold does not increase the capacity of edges in the network that belong to the min-cut. Partitions with length scales smaller than the characteristic size of features guarantee separation between different parts of a feature, requiring a network to model strong dependencies between these parts for accurate classification. The network must support a high measure of entanglement for partitions separating features, described by a min-cut in the TN graph. Concentrating parameters in layers up to log 2 D is advised, with deeper layers needing more width. This analysis serves as an incentive to develop ways to characterize relevant data effectively. The min-cut analysis on the TN for a deep ConvAC helps model correlations among input variables, guiding the design of channel numbers per layer. Empirical evidence shows these theoretical findings apply to regular convolutional networks with ReLU activations and pooling. Two tasks, 'local' and 'global', based on MNIST data-set, were designed to validate these concepts. The study focused on designing network architectures for local and global tasks using resized binary MNIST images. Two different tasks were created to test the networks, with the local task being more challenging. The networks had specific layer configurations to match the correlation structure of each task, with shared convolutional layers and hidden layers with ReLU activations and max pooling. The study designed network architectures for local and global tasks using resized binary MNIST images. The networks had specific layer configurations to match the correlation structure of each task, with shared convolutional layers, ReLU activations, and max pooling. The difference between the wide-base (WB) and wide-tip (WT) networks lies in the channel ordering, with WB starting wider and narrowing down, while WT follows the opposite trend. The choice of widths towards deeper layers in the WT network fits the global task, where all layers are important, while the WB network fits the local task, where shallower layers are more important. The study compared the performance of WB and WT networks on local and global tasks using MNIST images. WB outperformed WT in the local task due to its focus on short correlation lengths, while WT performed better in the global task, emphasizing longer correlation lengths. The experiments demonstrate how prior knowledge can tailor the inductive bias of a deep convolutional network by designing layer widths. The theoretical analysis presented in the paper manifests on prevalent convolutional network architectures. The construction of a deep ConvAC using a TN allows for a graph-theoretic analysis tying expressiveness to a minimal cut in the graph. The construction of a deep ConvAC involves a structural equivalence with quantum many-body wave functions, utilizing tools like TNs and entanglement measures to quantify the network's ability to model input correlations. The number of channels in each layer impacts the network's expressiveness, especially when prior knowledge of input correlations is available. This work also highlights interdisciplinary connections and practical implications for designing deep network architectures. The results show interdisciplinary bridges between ConvACs and network expressivity measures, as well as a mathematical connection between quantum physics and deep learning. The potential application of graph-theoretic tools to analyze deep convolutional networks and the use of tensor networks in quantum physics offer insights for transferring knowledge between the two domains. The MERA architecture, known for its high expressiveness and ability to model correlations, introduces overlapping 'disentangling' operations before pooling. This allows for efficient representation of functions with high correlations at all length scales. This connection between tensor networks and deep learning highlights the role of overlaps in disentangling intricate correlations in data. Physicists can benefit from using overlapping convolutional windows in deep learning to find expressive representations of quantum wave functions. This work serves as a bridge for transferring tools and ideas between fields, promoting interdisciplinary discourse. In quantum mechanics, a system's state is described as a ray in a Hilbert space, which is a vector space over the complex numbers. Vectors are denoted in 'ket' notation, with an inner product mapping two vectors to a scalar. A ray is an equivalence class of vectors differing by a nonzero scalar, with a representative chosen to have a unit norm. In quantum mechanics, vectors are represented in 'ket' notation with a unit norm. The 'bra' notation represents the dual vector, acting as a linear mapping between vectors and scalars. Quantum states reside in finite dimensional Hilbert spaces, crucial for analogy to convolutional networks. Single particle states can be represented as a linear combination of orthonormal basis vectors in the Hilbert space. In quantum mechanics, vectors are represented in 'ket' notation with a unit norm. The 'bra' notation represents the dual vector, acting as a linear mapping between vectors and scalars. Quantum states reside in finite dimensional Hilbert spaces, crucial for analogy to convolutional networks. Single particle states can be represented as a linear combination of orthonormal basis vectors in the Hilbert space. The extension to N particles with wave functions in local Hilbert spaces involves a tensor product to define a many-body Hilbert space. The scalar product in the tensor product space is specified, assuming equal dimensions for all local Hilbert spaces. In the case of N electrons with the same spin, the dimensions are set to M=2. The many-body quantum wave function |\u03c8 \u2208 H = \u2297 N j=1 Hj can be written as a partition of the system of N particles into two disjoint subsystems, with a singular value decomposition resulting in the Schmidt decomposition. A Tensor Network (TN) is formally represented by an undirected graph with special attributes, where tensors are the basic building blocks represented by nodes with legs corresponding to their order. A Tensor Network (TN) is a collection of tensors represented by nodes with legs, where each edge is associated with a bond dimension. The edges in a TN can be connected to nodes or between nodes, with indices denoting contracted or open indices. The contraction of a Tensor Network involves summing over contracted indices, resulting in a condensed representation of the network. This can be seen in examples where a TN multiplies a vector by a matrix, yielding an order 1 tensor. More complex TNs can represent higher order tensors, such as tensor trains or matrix product states, allowing for the representation of tensors of any order. The MPS is a tensor network architecture that decomposes higher order tensors into lower order tensors, reducing the curse of dimensionality. It allows for the representation of tensors with a linear amount of parameters. The CP decomposition of conv-weights tensor corresponds to a ConvAC network. The CP decomposition of conv-weights tensor corresponds to a ConvAC network with global pooling, while the deep version involves a hierarchical Tucker decomposition. The pooling scheme in ConvAC only involves entries from the same channel, unlike the general HT decomposition. The pooling operations involve entries from different channels, with a focus on one-dimensional alignment and window size 2. The decomposition of a deep ConvAC network is defined recursively through tensor products, constructing conv-weights tensors incrementally. This process corresponds to specific levels and locations in the feature map of the network. The ConvAC network in fig. 6 involves tensors at different levels and locations, with convolutional weights in hidden layers and final dense layer. The central \u03b4 tensor enforces channel pooling in the l th layer, playing a crucial role in the analysis of channel numbers. The tensor \u03c6 l,j,\u03b3 is of order 2 l, assuming simplicity in the calculations. The tensor \u03c6 l,j,\u03b3 is of order 2 l, with parameters {a DISPLAYFORM4, DISPLAYFORM5, DISPLAYFORM6 for decomposition. The TN equivalent of ConvAC involves defining the \u03b4 tensor as a super-diagonal matrix. G holds the final layer's convolutional weight vector, while A (j) holds intermediate layer matrices. The final layer's vector v y \u2208 R K is represented by the tensor product of N vectors in A (j) matrices, multiplied by a weight for class y. The TN equivalent of the CP decomposition is shown in FIG7, illustrating the same quantity as eq. 10 in a different form. The TN equivalent of the CP decomposition involves contractions between matrices A (j), G, and the \u03b4 tensor, enforcing same channel pooling. Switching the \u03b4 tensor in eq. 13 with a general tensor G would lead to a Tucker decomposition. The ConvAC calculation involves an inner-product between two tensors: the conv-weights tensor A. The conv-weights tensor A is represented in a TN construction with matrices and a delta tensor, enforcing same channel pooling. The lower block in the TN assumes the form of a coefficients tensor in a product state TN representation. The product state assumes a TN representation for the conv-weights tensor A, with matrices and a delta tensor enforcing same channel pooling. The final contraction of indices results in class scores vector calculated by ConvAC. The TN representation defines the calculations performed by a ConvAC, with a conjecture that an equality holds for certain conditions. The ConvAC operation involves matrix multiplication and multiplicative pooling of activation vectors from previous layers. The output of the network is determined by this operation, which follows a conv\u2192pool\u2192 ... \u2192conv\u2192pool scheme. The ConvAC operation involves matrix multiplication and multiplicative pooling of activation vectors from previous layers, following a conv\u2192pool\u2192 ... \u2192conv\u2192pool scheme. In conclusion, the computation performed by a ConvAC is translated to a TN, with convolutional weights arranged as matrices and \u03b4 tensors enabling channel pooling. The bond dimension of each level in the TN is equal to the number of feature maps in the corresponding ConvAC architecture. Upper and lower bounds on the ability of a deep ConvAC to model correlations are provided using the Schmidt entanglement measure. The weights tensor A y of a ConvAC is described by a graph G(V, E) and a bond dimensions function c. The bond dimension of edges in each layer is equal to the number of channels in the ConvAC. The node set decomposes into V tn and V inputs, with V inputs being 'virtual' vertices for completeness. The open edge at the top tensor is omitted as it does not affect the analysis. The Tensor Networks tool allows for representing a deep ConvAC as a 'legal' graph, connecting it to quantum entanglement measures. This graph-theoretic description ties the expressiveness of ConvAC to a minimal cut in the graph, utilizing the 'Quantum min-cut max-flow' concept. The quantum max-flow between A and B in Tensor Networks measures correlations, with the quantum min-cut bounding this ability. The TN describing A is transformed into a graph by removing the output edge and adding virtual vertices. The graph is represented as G(V, E) with two subsets V = Vtn \u222a Vinputs, where Vinputs are N virtual vertices. The set of nodes in the Tensor Networks is divided into two subsets: Vtn and Vinputs. Vinputs are virtual vertices with degree-1. A function c associates a number with each edge in the graph, equal to the bond dimension of the edge in the TN. An edge-cut set is defined with respect to a partition of the nodes, with a cut weight associated with it. The weight is calculated by multiplying the bond dimensions of all edges in the set. The weight definition in eq. 18 is a multiplication of bond dimensions in a cut. A max-flow/min-cut analysis on \u03c6 yields new results on the expressivity of deep ConvAC. Claim 1 provides an upper bound on modeling correlations, related to TN attributes. The example in FIG0 illustrates a general TN with arbitrary connectivity. The edges are marked by associated indices running between 1 and their bond dimensions. For a given partition (A, B), the corresponding external indices are denoted as IA and IB. The dimensions of spaces A and B are determined by the configurations of indices in IA and IB. The graph in FIG0 shows A on the left and B on the right, separated by a cut C with indices IC. The text discusses the contraction of tensors in a tensor network (TN) with a cut separating two higher order tensors. The matricization of the tensors can be written as a multiplication of two matrices, with cut indices translating as blocks of the identity matrix. The rank of the matricization is bounded by the minimum of the dimensions of the cut indices, providing insights into the modeling capabilities of a deep ConvAC TN. The upper bound alerts us when a deep ConvAC is too weak to model a desired correlation structure based on the number of channels in each layer. A lower bound ensures that the entanglement measure cannot fall below a certain value for any specific arrangement of channels per layer. The upper bound is saturated when all channel numbers are powers of an integer p, while a general arrangement may not be tight. The rank will not be lower than that of any ConvAC architecture with channel numbers that are powers of p but not higher than the original ConvAC channel numbers. The lower bound for ConvAC channel numbers ensures entanglement measure does not drop below a certain value. Simulations suggest deviations from the upper bound are rare. The proof strategy for theorem 2 involves dealing with restricted \u03b4 tensors in ConvAC networks. Relevant results include claim 2 on matrix rank and Undirected Menger's Theorem on edge disjoint paths in graphs. The text discusses the relationship between edge disjoint paths in undirected graphs and the minimal cut cardinality. It also explores the rank of tensors represented by \u03c6 and \u03c6 p, with implications for ConvAC network weights. The proof involves restricted \u03b4 tensors in ConvAC networks and relevant results such as claim 2 on matrix rank and Undirected Menger's Theorem. The text presents Undirected Menger's Theorem, which states that the maximum number of edge disjoint paths connecting two sets of vertices in an undirected graph is equal to the minimum cardinality of all edge-cut sets between the two sets. This theorem will be used in the proof of lemma 2, involving an auxiliary graph. Lemma 1 states that a tensor containing another tensor will not have a lower matricization rank. This is proven by considering specific assignments of tensors in a network, leading to certain rank values. The deep ConvAC TN network consists of tensors and weight matrices A (l,j) \u2208 R r l \u00d7r l\u22121, with bond dimensions denoted by r p l. The assignment of tensors in the network determines the entries of the weight matrices. The bond dimension in \u03c6 p cannot exceed the corresponding level in \u03c6, ensuring that matrices in \u03c6 do not have lower dimensions than those in \u03c6 p. The construction of the deep ConvAC network involves assigning weights to matrices based on tensor entries. Lemma 1 suggests that proving the upper bound on the rank of the conv-weights tensor is tight for channel numbers as powers of integer p is sufficient to establish the lower bound in theorem 2. Lemma 2 further refines this by setting minC WC as a lower bound on the rank of the conv-weights tensor for channel numbers restricted to powers of p. This lemma is crucial in proving the lower bound. The lemma proves that the upper bound on the rank of conv-weights tensor is tight for channel numbers restricted to powers of integer p. The proof is organized by constructing a TN \u03c6* from the original network \u03c6, analyzing \u03c6* to define \u03b4 restricted edge disjoint paths from A to B compliant with the form of a \u03b4. The lemma establishes the tight upper bound on the rank of conv-weights tensor for channel numbers restricted to powers of integer p. It involves constructing a TN \u03c6* from the original network \u03c6 and defining \u03b4 restricted edge disjoint paths from A to B compliant with the form of a \u03b4 tensor. The paths are used to assign indices for tensors in \u03c6* while maintaining the required \u03b4 condition. In constructing the network \u03c6*, parallel edges are added to maintain the network architecture. The analysis of \u03c6* is crucial, as it suffices for the proof. Each edge e in E is translated into ne edges in E*, preserving the distribution of degrees of freedom. Index numbers in A* correspond to edges in E*. In constructing the network \u03c6*, parallel edges are added to maintain the network architecture. The analysis of \u03c6* is crucial for the proof. Index numbers in A* correspond to edges in E*. The rank of the matricization of A w.r.t. the partition (A, B) is equal to the rank of the matricization of A* w.r.t. the partition (A*, B*). The construction of network \u03c6* involves adding parallel edges to maintain the architecture. Index numbers in A* correspond to edges in E*. The rank of the matricization of A with respect to the partition (A, B) is equal to the rank of the matricization of A* with respect to the partition (A*, B*). In \u03c6*, there is a one-to-one correspondence between elements in K*j and kj. This correspondence can be written as an assignment to ensure the claim regarding rank(AA,B) is equivalent to rank(A*A*,B*). The form of the \u03b4 tensor in \u03c6* corresponds to an order 3 \u03b4 tensor in \u03c6, with non-zero entries in \u03c4*v\u03b4. The \u03b4 tensor in \u03c6* corresponds to a \u03b4 tensor in \u03c6, with non-zero entries in \u03c4*v\u03b4. A minimal multiplicative cut search can be seen as a classical min-cut problem with defined maximum capacities for each edge. A classical min-cut problem is defined with maximum capacities for each edge in \u03c6*. The min-cut/max-flow value is obtained in a graph with additive capacities, resulting in a network with all edges assigned capacity 1. The maximal number of edge disjoint paths between V A* and V B* in the graph G* is equal to the cardinality of the minimum edge-cut set C*min, which is L. The concept of \u03b4 restricted edge disjoint paths between V A* and V B* in graph G* is introduced, where paths can share vertices but not edges. Each path passing through a vertex v in G* must have a unique number assignment and follow specific restrictions on entering and leaving edges. This \u03b4 restriction on the indices of tensors in \u03c6* ensures paths only contain edges of the same type. In the context of introducing \u03b4 restricted edge disjoint paths in graph G*, it is shown that L edge disjoint paths can be chosen to uphold the \u03b4 restriction. A weight assignment compliant with the \u03b4 tensors in the network can guarantee the lemma's requirements, even if most entries in the \u03b4 tensors are zero. The set X*e, originating from edge e in G, is denoted. It is demonstrated that paths can be chosen such that no set of edges X*e corresponds to edges e\u2208E where two edges in X*e belong to paths flowing in opposite directions. The classical max-flow in graph G is observed when assigning maximum capacities to edges. In graph G, assigning maximum capacities to edges allows for a maximum flow of L between V A and V B. Paths from V A to V B can transfer a maximum capacity of L, without the need to be edge disjoint. Integral flow theorem guarantees the existence of paths with integer capacity on each edge. The sum of capacities transferred on an edge must be less than the edge's maximum capacity. In a flow directed from V A to V B, paths can transfer a maximum capacity of L without needing to be edge disjoint. The paths must have the same flow direction in all edges originating from a certain edge. By defining a maximum additive capacity for each edge, paths can be constructed recursively in G. = Starting with a single quanta of flow along a path in G, construct a path in G* where each edge transfers one quanta of flow. Remove full edges in G* and reduce capacities of original path in G by one until L capacity is transferred in both graphs. Paths in G* correspond to integer capacity paths in G, are edge disjoint, and follow integral flow theorem. In G*, edge disjoint paths ensure flow in the same direction, originating from the same edge in G. By choosing paths with this condition, flow is transferred consistently. Each set of edges in G* passes flow from a common edge in G. In G*, edge disjoint paths ensure consistent flow direction from a common edge in G. Sets of edges in G* allow flow from V A* to V B* in the same direction, with a maximum of ne paths passing through the delta tensor. The paths are categorized as 'I' or 'O' based on their direction, limiting the number of edge disjoint paths flowing in or out of the delta tensor. In G*, edge disjoint paths ensure consistent flow direction from a common edge in G, with sets of edges allowing flow from V A* to V B*. The paths are categorized as 'I' or 'O' based on their direction, limiting the number of edge disjoint paths flowing in or out of the delta tensor. This distribution of paths upholds the delta restriction, ensuring each path receives a different value t \u2208 [ne], with a maximal number of \u03b4 restricted edge disjoint paths between V A* and V B* in the graph G* being L. The proof of theorem 2 in graph G* involves modifying bond dimensions in network \u03c6 p to the closest power of p. An upper bound on the rank of matricization of conv-weights tensor is shown for a general pooling window, reducing the upper bound when many cut edges belong to the same \u03b4 tensor. This analysis does not affect the deep network above. In the analyzed deep ConvAC, cuts with multiple \u03b4 tensor legs can be replaced by equivalent cuts with only one leg, without changing minC WC value. For ConvAC with general sized pooling windows, restrictions imposed by \u03b4 tensors must be considered. The object underlying a ConvAC-weights TN with general sized pooling windows is defined as an undirected graph G(V, E) with vertices V divided into V tn and V inputs, where V inputs are virtual vertices and V tn corresponds to tensors of the TN. The function f assigns numbers to edges in the ConvAC TN based on the delta tensors present. Each edge is grouped based on the delta tensor it is incident to, and a bond dimension is associated with each group of edges. The function f assigns numbers to edges in the ConvAC TN based on delta tensors. Each edge is grouped based on the delta tensor it is incident to, and a bond dimension is associated with each group of edges. Observing an edge-cut set with respect to the partition (A, B) and the corresponding set G C = {f (e) : e \u2208 C}, the modified weight of such a set is defined. The weight is a multiplication of bond dimensions of edges in a cut, with a restriction on edges connected to a \u03b4 tensor. An example in FIG0 shows how replacing a general tensor with a \u03b4 tensor reduces the minimal cut. An upper bound on the weight is provided in the following claim. The upper bound on the ability of a ConvAC with a general pooling window to model correlations is determined by the rank of the matricization A y A,B, where C is a cut w.r.t (V A , V B ) and WC is the modified multiplicative weight. This is illustrated in FIG0 with an example of the effect of a \u03b4 tensor on the rank of the matricization of the overall tensor represented by a TN. The upper bound is reduced when changing to a \u03b4 tensor. The upper bound on the ability of a ConvAC with a general pooling window to model correlations is determined by the rank of the matricization A y A,B, where C is a cut w.r.t (V A , V B ) and WC is the modified multiplicative weight. This is illustrated in FIG0 with an example of the effect of a \u03b4 tensor on the rank of the matricization of the overall tensor represented by a TN. The centrality of the \u03b4 tensor in the TN compliant with a shallow ConvAC limits the expressiveness of the network. The proof of the upper bound presented in claim 3 involves introducing \u03b4 tensors to derive a tighter lower bound. The modification to the proof focuses on the coalescence of the cut indices IC into a single index m. The cut indices IC are coalesced into a single index m \u2208 [q] in the tensor network. When ki and kj are different, the term vanishes due to the \u03b4 tensor constraint. Representative index k\u03b1 \u2208 [q] can replace ki, kj connected to the same \u03b4 tensor in the summation. Group index \u03b1 \u2208 GC with c(\u03b1) = q is defined by \u03b1 = f(eki) = f(ekj). The single index m in the tensor network is defined in the range m \u2208 [WC], where WC is determined by eq. 32. The matricization A A,B can be expressed as a multiplication of two matrices. The rank of A A,B is limited by min CW C. In the shallow ConvAC architecture, a single \u03b4 tensor at the center implies a minimal cut of W min C = min(M N/2, k) for any partition of inputs (A, B) with |A| = |B| = N/2. To achieve exponential entanglement in N, the number of channels in the single hidden layer needs to be increased. In section 5, the shallow network's hidden layer must grow exponentially with N to match the expressiveness of a deep network. Simulations on an N = 16 deep ConvAC TN aimed to quantify deviations from the upper bound on ranks of conv-weights tensor matricization. The upper bound is tight when channel numbers are powers of an integer p, but not guaranteed for general settings. Theoretical guarantee that the upper bound is tight for a general TN representing a ConvAC is uncertain, as shown by BID8's counter example. Simulations on an N = 16 ConvAC TN with randomized weight matrices indicate negligible deviations from the upper bound. Bond dimensions were selected from the first 6 prime numbers. In order to test the tightness of the upper bound for a general TN representing a ConvAC, bond dimensions were drawn from the first 6 prime numbers. A total of 360 different arrangements were checked, resulting in 2.3166 million configurations. The logarithm of the upper bound on the rank of the conv-weights tensor matricization is related to the max-flow in a network with edge capacities equal to the logarithm of the bond dimensions. The study tested the tightness of the upper bound for a general TN representing a ConvAC by calculating the max-flow independently for each configuration using the Ford-Fulkerson algorithm. Out of 2.3166 million configurations, only 1300 failed to reach the upper bound, with deviations not exceeding 10% of the value. The results show the robustness of the upper bound, supporting the minimal weight over all cuts in the network. The study tested the tightness of the upper bound for a general TN representing a ConvAC. Experimentally, the minimal weight over all cuts in the network was found to be the effective indication for the matricization rank of the conv-weights tensor. The results show the robustness of the upper bound, with discrepancies potentially increasing for larger networks. Future theoretical analysis is needed, but theorem 2 guarantees a positive result regarding the rank of the matricization of the conv-weights tensor in all cases."
}