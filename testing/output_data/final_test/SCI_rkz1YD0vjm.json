{
    "title": "rkz1YD0vjm",
    "content": "In this paper, the sparsification of CNNs is explored through three model-independent methods that reduce model weights by up to 73% without retraining. These on-the-fly methods maintain accuracy with only a 5% loss in Top-5 accuracy. Fine-tuning results in an 8% increase in sparsity, showing the effectiveness of the fast on-the-fly methods. Several techniques exist for \"pruning\" or \"sparsifying\" CNNs to compress the model and save computations during inference, including iterative pruning, Huffman coding, exploiting granularity, structural pruning of network connections, and Knowledge Distillation. These techniques require retraining of the model to fine-tune the remaining non-zero weights and maintain accuracy, which may not be feasible in industrial contexts like mobile platforms where the model is embedded within an app. The machine learning model embedded in an app for a platform requires on-the-fly sparsification without retraining. Fast retraining-free sparsification methods are developed to achieve large sparsity with minimal loss in inference accuracy. Three model-independent sparsification methods are proposed. Sparsity in convolutional neural networks (CNNs) can be achieved without retraining through model-independent sparsification methods. These methods, such as flat, triangular, and relative, can force up to 81% of layer weights to zero with only a 5% loss in inference accuracy. Different methods are more effective for different models, highlighting the need for autotuning to select the optimal method and hyperparameters at runtime. In this paper, the focus is on sparsity in Conv and fully connected layers of CNNs. A sparsification function, S, is used to modify weights based on a threshold \u03c4 l from a vector of thresholds T. The threshold \u03c4 l forces weights within a certain range to become zero. The use of thresholds to sparsify layers in CNNs is motivated by the distribution of weights around 0. Three sparsification methods are compared: flat, triangular, and relative, each defining thresholds differently. The high-level workflow of the sparsification framework is illustrated in Figure 1a. The sparsification framework in CNNs uses different methods like flat, triangular, and relative to define thresholds for layer sparsity. The flat method sets a constant threshold \u03c4 for all layers, while the triangular method uses thresholds \u03c4 min and \u03c4 max.\u03b4 is a parameter that can be adjusted to control the degree of model sparsity. The triangular method in CNNs uses thresholds \u03c4 min and \u03c4 max for the first convolution layer and the last fully connected layer. These thresholds are determined by the weight spans in each layer, with \u03b4 conv and \u03b4 f c as fractions ranging between 0 and 1. The remaining layers' thresholds are determined by their position in the network. The study evaluates sparsification methods in CNNs using TensorFlow v1.4 with CUDA runtime and driver v8. Top-5 accuracy is assessed on an NVIDIA's GeForce GTX 1060 with Ubuntu 14.04. Results show significant sparsity can be achieved with minimal accuracy reduction, validating the proposed methods. The study evaluates sparsification methods in CNNs using TensorFlow v1.4 with CUDA runtime and driver v8. Top-5 accuracy is assessed on an NVIDIA's GeForce GTX 1060 with Ubuntu 14.04. Results show significant sparsity can be achieved with minimal accuracy reduction, validating the proposed methods. The models achieved 50% (2\u00d7), 62% (2.63\u00d7), 70% (3.33\u00d7), and 73% (3.7\u00d7) sparsity. The relative method outperforms the other two methods for some models, while the triangular method is more effective for others due to the structure of the models. The authors focused on pruning and retraining CNNs to compensate for accuracy loss, achieving around 80% sparsity in Alexnet with L2 regularization. Their method without retraining is not specified, but our evaluation confirms their results across other models using on-the-fly methods. Fine-tuning without retraining was explored to increase sparsity, showing the effectiveness of the on-the-fly methods. The study focused on varying sparsity levels in different layers of CNN models to maintain a 5% drop in accuracy. Results showed an 8% increase in overall model sparsity, with varying gains for different models. However, the increased computations required for exploring different sparsity ratios make this approach impractical in the contexts explored. In this paper, three model-independent methods for exploring sparsification of CNNs without retraining were proposed. Experimental results showed up to 73% sparsity with less than 5% drop in accuracy. Fine-tuning the methods can further increase sparsity without significant accuracy loss, but it cannot be done on-the-fly. Future work includes exploring heuristics for method selection based on the CNN model and implementing sparsity benefits in the NNlib library. The sparsity in the model's implementation benefits the NNlib library by offloading neural network operations from TensorFlow to Qualcomm's Hexagon-DSP."
}