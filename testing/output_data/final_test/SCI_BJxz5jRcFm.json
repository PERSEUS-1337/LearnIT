{
    "title": "BJxz5jRcFm",
    "content": "The text discusses the importance of semi-supervised learning in modern machine learning due to the challenges of obtaining labeled data for large datasets. A novel regularization technique called the tangent-normal adversarial regularization is proposed to utilize unlabeled data effectively by enforcing smoothness along different directions on the data manifold. Our method utilizes virtual adversarial training to impose robustness on the classifier against noise in the observed data, achieving state-of-the-art performance in semi-supervised learning tasks. Semi-supervised learning is crucial as it requires only a small portion of labeled data, making it practical for various applications where obtaining annotated labels is costly. Semi-supervised learning (SSL) is important due to its reliance on limited labeled data and a larger pool of unlabeled data. The main challenge in SSL is leveraging the unlabeled data to enhance the prediction model. Three main research streams address this issue, with one approach treating SSL as a missing data imputation task using probabilistic models and Bayesian inference techniques. The approach in SSL involves leveraging unlabeled data to optimize the model using Bayesian inference techniques. Regularization methods like adversarial training and virtual adversarial training (VAT) are used to induce smoothness in the classifier, making it robust to adversarial examples in high-dimensional input spaces. Regularization methods like manifold regularization and generative adversarial networks (GAN) are alternative approaches to smooth the classifier in SSL, avoiding potential harm caused by overly regularizing in input space like VAT. Manifold regularization encourages invariance on the manifold representation of data, while GAN-based approaches modify the discriminator to include a classifier for improved classification. Our work focuses on distinguishing real from fake examples using features that benefit supervised classification tasks. We assume that data lies on a lower-dimensional manifold and that classification relies solely on this manifold. Additionally, we decompose observed data into a manifold-supported part and noise. The text discusses the assumption that the classifier depends only on the underlying manifold M, with noise potentially affecting the learning process. It also mentions the semi-supervised learning assumption that points close in manifold distance have similar conditional probabilities. The text chunk discusses the assumption that the classifier depends on the underlying manifold M, with noise potentially affecting the learning process. It also mentions the semi-supervised learning assumption that points close in manifold distance have similar conditional probabilities. The curr_chunk contains a string of alphanumeric characters and symbols that do not provide any meaningful information. The curr_chunk consists of a string of alphanumeric characters and symbols that do not convey any meaningful information. The curr_chunk contains a string of alphanumeric characters and symbols. The curr_chunk discusses a novel regularization technique called tangent-normal adversarial regularization, which aims to induce invariance and robustness in classifiers against noise. It involves observed data x = x0 + n, where x0 is supported on a manifold M and n is noise independent of x0. Adversarial perturbations r and r \u22a5 are introduced along the tangent and normal spaces, respectively. The novel regularization technique called tangent-normal adversarial regularization (TNAR) aims to induce smoothness and robustness in classifiers against noise. It consists of two parts: tangent adversarial regularization (TAR) for smoothness along the manifold's tangent space, and normal adversarial regularization (NAR) for robustness against noise. These two regularization terms improve generalization performance by enforcing different aspects of the classifier's smoothness. Challenges include estimating the underlying manifold and efficiently implementing TNAR using generative models with an extra encoder. The first issue involves using generative models with an extra encoder to characterize the coordinate chart of manifold BID11 BID13 BID20. Variational autoencoder (VAE) and localized GAN are utilized for this purpose. The second problem introduces an adversarial regularization approach based on virtual adversarial training (VAT), performed in tangent space and normal space separately. This method does not require evaluating the Jacobian of the classifier, only the derivative of matrix vector product. The text discusses the calculation of the derivative of matrix vector product in a classification model. It introduces the labeled and unlabeled datasets, the classification model output, and the regularization term. It also mentions the manifold representation of data and the encoder-decoder relationship. The text discusses the perturbation on the manifold representation of data and its tangent space, introducing the virtual adversarial loss as an effective regularization method for SSL. The virtual adversarial loss in VAT for SSL imposes a smoothness condition on the classifier by measuring the robustness against local perturbations in the input space. It is defined as the maximum distance between the distributions of predictions with and without perturbations. The optimization of the VAT loss involves finding the virtual adversarial example through second-order Taylor's expansion around zero perturbation. The Hessian of F with respect to r around r = 0 is denoted as H. At r = 0, both the value and gradient of F (x, r, \u03b8) are zero. The direction of r * can be found by solving an eigenvalue problem. A generative model with encoder h and decoder g estimates the data manifold M and its tangent space TxM. The encoder helps identify the manifold coordinate z = h(x). The encoder is crucial for identifying the manifold coordinate z = h(x) for point x \u2208 M. Using VAE BID9 and localized GAN, the targeted data manifold M is learned. VAE consists of an encoder and decoder, trained by optimizing the variational lower bound of log likelihood. The lower bound includes a reconstruction term and a regularization term, optimized using gradient-based methods. The localized GAN suggests using a localized generator to replace the global generator in vanilla GAN. It learns a distinguishing local coordinate chart for each point on the data manifold, satisfying locality and orthogonality conditions. This is achieved through a penalty during training to define a local coordinate chart for each point separately. The proposed tangent-normal adversarial regularization (TNAR) strategy for SSL involves minimizing the TNAR loss, which includes tangent adversarial regularization (TAR) and normal adversarial regularization (NAR). This approach eliminates the need for an extra encoder to provide the manifold representation of x, as each x has its own local coordinate chart defined by G(x, z). The proposed tangent adversarial regularization (TAR) in SSL aims to enforce manifold invariance of the classifier by restricting virtual adversarial training to the tangent space of the underlying manifold. This approach optimizes the classifier by considering specific tangent directions and perturbations. The proposed method introduces a regularization term to control the orthogonality of perturbations in virtual adversarial training. Power iteration is used to solve an eigenvalue problem, with an added identity matrix to maintain semipositive definiteness. The optimal solution is found as r\u22a5, leading to a normalized classifier output. Entropy regularization is also added to the loss function for more determinate predictions and improved virtual adversarial training. The proposed TNAR regularization method combines TAR and NAR to enhance SSL performance. Experiments on FashionMNIST dataset show superior results with TNAR-VAE and TNAR-LGAN achieving lower loss. Our proposed TNAR methods (TNAR-VAE, TNAR-LGAN) outperform VAT in classification errors with different labeled data. TNAR-VAE shows larger improvement than TNAR-LGAN by producing better diverse examples. Ablation study on FashionMNIST datasets demonstrates the importance of both tangent adversarial regularization (NAR) and normal adversarial regularization (TAR) for SSL. The final performance of TNAR in SSL experiments on SVHN and CIFAR-10 datasets with limited labeled data is demonstrated. Adversarial perturbations are shown to focus on different aspects of smoothness, with TNAR outperforming VAT in classification errors. The proposed TNAR with Localized GAN outperforms other SSL methods on CIFAR-10 and SVHN datasets. TNAR utilizes tangent-normal adversarial regularization for semisupervised learning, enforcing regularization on the tangent and normal space separately. The proposed TNAR with Localized GAN utilizes tangent-normal adversarial regularization for semi-supervised learning, enforcing regularization on the tangent and normal space separately. Experiments show its superiority over other state-of-the-art methods, with performance reliant on the quality of estimating the underlying manifold. Future work includes exploring breakthroughs in modeling data manifold to further benefit semi-supervised learning strategies. In FashionMNIST experiments, 100 data points are preserved for validation from the original training dataset. Images are scaled into 0 \u223c 1 for pre-processing. The classification neural network includes convolution filters, max pooling layers, and local response normalization. The network is trained with labeled data using batch sizes of 32, and unlabeled data using batch sizes of 128. The optimizer is ADAM with an initial learning rate of 0.001, and hyperparameters are tuned for optimal performance. The hyperparameters tuned include the magnitude of adversarial perturbations and \u03bb in Eq. (11). \u03bb is tuned from {1, 0.1, 0.01, 0.001}, and 1 , 2 randomly from [0.05, 20]. Convolutional and fully connected layers have batch normalization, with lReLU slopes of 0.1. VAE encoder is LeNet-like with 2 conv layers and 1 fully connected layer, latent dimensionality is 128. GAN for identifying manifold is similar to BID20. The implementation of the model is based on a similar manifold as in BID20, with a modified latent dimensionality of 128. Experiments on SVHN and CIFAR-10 datasets involved using 1,000 labeled data for training and validation each. Data preprocessing included scaling pixel values to 0-1 without data augmentation. The classification neural network structure is identical to BID16. Different batch sizes were used for labeled and unlabeled data, with specific update and training parameters for each dataset. The optimizer used was ADAM with an initial learning rate of 0.001. The model is trained for 200,000 updates using ADAM optimizer with a learning rate of 0.001. Hyperparameters tuned include the magnitude of adversarial perturbations and \u03bb in Eq. (11). The VAE for identifying the underlying manifold for SVHN is implemented with a regularization term coefficient of 1. The localized GAN for identifying the underlying manifold for SVHN and CIFAR-10 is modified with a latent dimensionality of 512. More adversarial perturbations and examples are generated in tangent space and normal. Adversarial perturbations and examples in tangent and normal space are shown in FIG12 and FIG13. Perturbations are scaled for better visibility. Original example, tangent adversarial perturbation, normal adversarial perturbation, tangent adversarial example, and normal adversarial example are displayed."
}