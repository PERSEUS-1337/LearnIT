{
    "title": "SJlgRqjssQ",
    "content": "The paper proposes a framework to validate the robustness of Question Answering models using model explainers. It tests the model's invariance to alterations in questions and compares state-of-the-art models for robustness. Additionally, it suggests adversarial training scenarios to improve model sensitivity. The study proposes adversarial training scenarios to enhance model sensitivity to true synonyms by up to 7% accuracy. A new dataset for validating Q&A model robustness has been created and published. Recent advancements in natural language processing highlight the potential for high accuracy, but research on adversarial examples exposes neural network model shortcomings. In this work, a model agnostic framework is proposed to test the stability of three state-of-the-art Q&A architectures in responding to semantically similar questions. The study aims to understand the source of lacking robustness in Q&A models and suggests adversarial training scenarios to enhance model sensitivity to true synonyms. The research emphasizes the importance of benchmarking performance on adversarial attacks to overcome this problem. A robust Q&A model should be invariant to input changes inducing semantic differences, with higher sensitivity to true semantics than numerical stability. Adversarial attacks are possible with close word embeddings, so model robustness is tested against semantic and numerical changes. Two adversarial training approaches increase model sensitivity by up to 7% in accuracy. In this study, the model's sensitivity to true semantic differences was tested, showing a maximum 7% accuracy change. A collection of 1500 semantically coherent questions from the SQuAD dataset was released as a reference dataset. The study used measures and tools in experiments on Q&A architectures, introducing modifications to improve the ability to answer similar questions. The importance of question words was highlighted, with the use of the LIME framework to assess their contribution and construct adversaries. LIME framework assesses the importance of words in deep learning models by fitting a linear model in the local surrounding of each example. It identifies the most important word, known as the keyword, to measure interpretability. Input stability in numerical inputs is crucial, as embedded vectors in models exhibit proximity to other words, including antonyms and synonyms. Overstability to numerical inputs should be considered a significant factor. The study focuses on the vulnerability of deep learning models to adversarial attacks by swapping keywords with closest words in SQuAD questions. Using GloVe embeddings, 37% of created questions were found to be potentially vulnerability-inducing. To ensure semantic input stability, genuinely synonymous sentences are constructed based on Wordnet BID4 synonyms and context-dependent ELMO BID7 embeddings, validated by human annotators. The study investigates the vulnerability of deep learning models to adversarial attacks by swapping keywords in SQuAD questions. Three popular architectures are tested on a dataset with modified questions, each altered with different methods. 1500 test examples are produced, with modifications induced by swapping keywords with synonyms, closest words in GloVe embedding space, or random words. In experiments testing deep learning models' vulnerability to adversarial attacks, 500 test examples were generated by each of 3 algorithms, resulting in 3 separate datasets. Models showed varying responses to semantic and numeric changes, with all models being more stable to numeric manipulations than recognizing true synonyms. The study found that models are more vulnerable to attacks with words close to the keyword, leading to a significant decrease in performance. To improve model robustness, two changes were made to the algorithm and the DrQA model was retrained. The new models were validated on all datasets and the changes can be implemented in any algorithm. BID12 trains Q&A models with enriched datasets containing questions in multiple forms to improve performance on syn-Dataset. Propagating gradients back to input embeddings can enhance model knowledge of synonymy. Fine-tuning may be used to further boost model performance. Fine-tuning embeddings can increase semantic stability and synonym recognition, leading to improved numeric accuracy. Adversarial testing showed that models with gradient modifications were less affected by numeric changes compared to the base model. The study found that REMx2 and REMx3 methods increased model acceptance to numeric manipulations without affecting traditional metrics like EM and F1. It highlights the need for specific procedures to test model robustness independently from performance tests. Recent research has focused on manipulating semantics in Q&A systems, with studies showing the effectiveness of adversarial attacks and keyword detection methods. In the Q&A task, BID14 generates paraphrases of adversarial questions to test model robustness. They propose an augmentation framework and focus on the ability to answer semantically coherent questions. Models show decreased performance when important question words are changed, but perform better when tokens are swapped for words close in their embedding space. Our work focuses on improving model accuracy in answering semantically correct questions by enhancing the model's ability with GloVe embeddings. We found that popular embeddings lack knowledge of the real meaning of words, only incorporating context knowledge. This research paves the way for future studies on more semantically-conscious word representations."
}