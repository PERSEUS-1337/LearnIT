{
    "title": "BkghJoRNO4",
    "content": "Most approaches in generalized zero-shot learning focus on cross-modal mapping between image features and class embeddings or generating artificial features. A shared cross-modal embedding is learned by aligning modality-specific autoencoders, and a model with aligned variational autoencoders is proposed for generating latent features to train a softmax classifier. The learned latent features achieve a new state of the art in generalized zero-shot and few-shot learning, with good generalization on large-scale datasets like ImageNet. In generalized zero-shot learning, various approaches are used to learn cross-modal embeddings, such as mapping images to class embeddings, generating artificial image features, and transforming modalities to latent spaces for multimodal fusion. These techniques aim to improve performance on tasks like visual question answering. In this work, VAEs are trained to encode and decode features from different modalities, aligning their latent spaces by matching distributions and enforcing cross-modal reconstruction. This enables training a zero-shot visual classifier using latent space features from semantic data. In this work, VAEs are trained to align latent spaces of different modalities for zero-shot visual classifier training using semantic data features. Contributions include Generalized Zero-shot Learning and focusing on the challenging setup of generalized zero-shot learning (GZSL). The Objective Function CADA-VAE is trained with pairs of data. The Objective Function CADA-VAE is trained with pairs of image features and attribute vectors of seen classes to align latent spaces for zero-shot visual classifier training. The encoders transform data into a shared latent space, and a softmax classifier is trained on both seen image data and unseen attributes. Latent features are oversampled for unseen classes, and the classifier is tested using the predicted means of the latent representation. The Objective Function CADA-VAE is derived for training VAEs on different modalities like image features and attributes. It includes loss functions for encoding and decoding data, minimizing cross-reconstruction loss, and aligning latent distributions. The VAE is trained with a final objective combining various loss components. The VAE is trained using the final objective combining different loss components. Variants include CADA-VAE, CA-VAE, and DA-VAE with a latent size of 64 (128 for ImageNet). Evaluation is done on zero-shot learning datasets using ResNet-101 features and attribute vectors as class embeddings. Hyperparameters are chosen based on a validation set. The harmonic mean (H) between seen (S) and unseen (U) average per-class accuracy is reported using embeddings provided by BID3. A comparison with 11 state-of-the-art models in Generalized Zero-Shot Learning is made, including models that generate artificial visual data and those that use linear compatibility functions or neural networks for embedding learning. ReViSE BID16 proposes shared latent manifold learning using an autoencoder. CADA-VAE outperforms all other methods on all datasets in the GZSL setting, achieving significant improvements over feature generating models, particularly on CUB. The model leads to at least a 100% improvement in harmonic mean accuracies compared to classic ZSL methods. The shared representation learned in a weakly supervised fashion through a cross-reconstruction objective is believed to explain the performance increase. The model aims to learn an encoding that retains information from all modalities, reducing bias towards seen class image features. It generates latent features per class using non-deterministic encoders, similar to data-generating approaches but in a lower dimensional space. Evaluation splits with increasing granularity and size were proposed. The model aims to learn an encoding that retains information from all modalities, reducing bias towards seen class image features. Evaluation splits with increasing granularity and size were proposed, measuring accuracies of unseen class images in the GZSL search space. Various models like SJE, SYNC, DeViSE, and f-CLSWGAN are compared using Word2Vec features as class embeddings. The CADA-VAE model achieves state-of-the-art performance on ImageNet, improving accuracy across different class splits. The 128-dim latent feature space proves to be a robust and generalizable representation, surpassing current benchmarks. In this work, the CADA-VAE model proposes a cross-modal embedding framework for generalized zero-shot learning. By aligning modality-specific latent distributions and using cross-reconstruction, encoders can encode features into a cross-modal embedding space for training a linear softmax classifier. Different variants of VAEs are presented, achieving new state-of-the-art results in zero-shot learning on benchmark datasets and ImageNet. The cross-modal embedding model outperforms data-generating methods, establishing a new state of the art."
}