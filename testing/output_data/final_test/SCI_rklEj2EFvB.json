{
    "title": "rklEj2EFvB",
    "content": "We present an unbiased estimator for expectations over discrete random variables based on sampling without replacement, reducing variance by avoiding duplicate samples. This estimator can be derived as the Rao-Blackwellization of three different estimators and combined with REINFORCE to obtain a policy gradient estimator. Variance is further reduced using a control variate without additional model evaluations. Experimental results show that our estimator consistently outperforms others in various settings. The problem of estimating the expectation of a function over discrete random variables with a categorical distribution is common in various tasks like reinforcement learning and structured prediction. The distribution has parameters that are learned through gradient descent, requiring the estimation of the gradient using a set of samples. The gradient estimate is unbiased when samples are independently sampled. In this paper, an unbiased gradient estimator is derived to reduce variance by avoiding duplicate samples. Sampling without replacement is challenging due to dependencies, but a built-in control variate further reduces variance without needing additional samples. Previous work includes algorithms like REINFORCE and biased gradients based on continuous relaxations of discrete distributions. Various methods, such as REBAR and RELAX, aim to reduce the variance of REINFORCE by using control variates. Some approaches involve explicit summation of expectations or finite difference approximation to the gradient. ARSM, introduced by Yin et al. (2019), utilizes multiple model evaluations to improve performance. In 2019, Yin et al. introduced ARSM, a method that adapts automatically to uncertainty using multiple model evaluations. Various algorithms exist for optimizing quantities under discrete decisions with weak supervision or multiple samples. These algorithms often rely on pretraining and transitioning from supervised to reinforcement learning. Gumbel-Softmax based approaches in sequential settings face challenges due to bias accumulation from mixing errors. When sampling without replacement, a set C \u2282 D is removed from the domain, denoted as p D\\C. An ordered sample without replacement B k is generated by sampling elements one by one without replacement. This can be seen as a ranking according to the PlackettLuce model. An unordered sample without replacement S k is also possible. When sampling without replacement, a set C \u2282 D is removed from the domain, denoted as p D\\C. An unordered sample without replacement S k \u2286 D can be generated by discarding the order of an ordered sample. The probability for sampling S k is given by a formula, and the Gumbel-Top-k trick can be used as an alternative method for sampling B k and S k efficiently. The perturbed log-probability method provides an alternative sampling approach, using a non-differentiable reparameterization for efficient computation. The unordered set policy gradient estimator is derived for unbiased estimation based on an unordered sample without replacement. The text discusses deriving estimators for E[f(x)] using different sampling methods, including a single sample estimator and Rao-Blackwellization to improve efficiency. The approach combines the estimator with REINFORCE and introduces a baseline to reduce variance. The single sample estimator is based on the first element of an ordered sample, which is unbiased but inefficient. Rao-Blackwellization significantly improves the estimator by considering the conditional distribution over ordered samples. The Rao-Blackwellized single sample estimator computes the inner conditional expectation exactly by considering the conditional distribution over ordered samples. The leave-one-out ratio is introduced for notational convenience. The leave-one-out ratio is defined as the probability of sampling s first, given S k. The unordered set estimator is the Rao-Blackwellized version of the single-sample estimator, which is an unbiased estimator of E[f(x)]. Taking multiple samples reduces variance according to the Rao-Blackwell Theorem. The unordered set estimator is derived from two unbiased estimators: the stochastic sum-and-sample estimator and the importance-weighted estimator. It involves summing expectation terms for a set of categories explicitly and using down-weighted samples to estimate the remaining terms. Selecting categories to minimize variance is crucial, as discussed by Liu et al. (2019) and Fearnhead & Clifford (2003). The ability to optimize C depends on whether p(c) can be computed efficiently a-priori in high-dimensional settings. An alternative is to select C stochastically, and we choose C = B k\u22121 to define the stochastic sum-and-sample estimator. Sampling without replacement, it holds that the unbiasedness follows from equation 13 by separating the expectation over B k into expectations over B k\u22121 and b k |B k\u22121. A sum-and-sample estimator reduces variance if the probability mass is concentrated on the summed categories. The stochastic sum-and-sample estimator focuses on summing high probability categories first, similar to the deterministic sum-and-sample estimator. Rao-Blackwellizing this estimator results in the unordered set estimator, which has equal or lower variance than the optimal stochastic sum-and-sample estimator. The importance-weighted estimator, based on priority sampling, does not rely on sample order and uses the Gumbel-Top-k trick for sampling. The Gumbel-Top-k trick is used for sampling in the estimator, which requires access to \u03ba, the threshold for perturbed log-probabilities. Normalizing importance weights leads to biased estimates, but Rao-Blackwellization eliminates stochasticity by \u03ba, resulting in the unordered set estimator with equal or lower variance. Combining this with REINFORCE gives the unordered set policy gradient estimator. The unordered set policy gradient estimator is an unbiased estimate of the policy gradient. Variance reduction can be achieved by using a baseline from other independent samples. The estimator with baseline is an unbiased estimate of the policy gradient. The theorem explains how to incorporate a built-in baseline for dependent samples without bias. It compares the value of a sample against its expected value based on other samples, estimating the advantage. Sampling without replacement forces the estimator to compare different alternatives and reinforce the best one. If the function depends on \u03b8, the gradient is calculated accordingly. The gradient can be calculated using the unordered set policy gradient estimator, which includes a second term that can be estimated with the standard unordered set estimator. Implementing Equation 20 is straightforward with automatic differentiation libraries. It is important to use STOP GRADIENT (DETACH in PyTorch) on the baseline but not on f \u03b8 (s). Gradients should not be tracked through the leave-one-out ratio R(S k , s) for efficient computation in pure inference mode. The unordered set estimator can be used for any discrete distribution that allows sampling without replacement, such as sequence models using Stochastic Beam Search. The computation of leave-one-out ratios adds overhead but can be computed efficiently, even for large k. For moderately sized models, model evaluation and backpropagation costs dominate the estimator computation. The 'vanilla' unordered set estimator is a special case of Murthy's estimator, known for estimating a population total. Murthy's estimator can be used to estimate expectations. Murthy's estimator, derived from a convex combination of Raj (1956) estimators, can be used to estimate expectations. It also provides an unbiased estimator of the variance. This estimator can be used with arbitrary sampling distributions, allowing for importance-sampling versions of estimators. The leave-one-out ratio can be computed efficiently, even for large k. The practical computation of p(S) is discussed, along with its relation to the importance-weighted estimator. The text discusses the computation of p(S) and its relation to the importance-weighted estimator, extending to estimating policy gradients with a built-in baseline. It compares the risk estimator to the empirical risk loss, highlighting differences in normalization and gradient computation. The gradient through the normalization factor forces samples to compete for probability mass, akin to using a built-in baseline. Theorem 3 shows that the risk estimator has a built-in baseline by considering the gradient w.r.t. the normalization factor. It emphasizes the similarity between biased and unbiased estimators, with differences in term weighting. Edunov et al. (2018) implementation introduces biases like length normalization not compatible with the unbiased estimator. This analysis aims to develop unbiased estimators for structured prediction, contrasting with VIMCO (Mnih & Rezende, 2016) using k samples for optimization. VIMCO reduces variance by using a local baseline for each sample. REINFORCE without replacement with built-in baseline is an unbiased estimator. The unordered set estimator weights terms by p \u03b8 (s)R(S k , s) instead of 1 k to compensate for sampling without replacement. Our estimator reduces variance by sampling without replacement, allowing for direct control over computational cost and wider applicability compared to ARSM. It aims to change the sampling distribution for multiple samples, similar to stratified or systematic sampling methods. The text discusses sampling methods such as stratified or systematic sampling to partition the domain and take samples. It mentions the challenge of sampling from high-dimensional or structured domains and the importance of including a built-in baseline for the estimator to perform well. The goal is to minimize loss in a Bernoulli toy experiment using different estimators. The text compares different estimators in a Bernoulli toy experiment, focusing on the variance of the scalar gradient. The unordered set estimator shows consistently low variance in both high and low entropy regimes, suggesting it combines advantages effectively. The text compares different gradient estimators in a Bernoulli toy experiment, showing that the unordered set estimator has low variance for different sample sizes. The experiment also includes training a categorical VAE with a 20-dimensional latent space and using Stochastic Beam Search for sampling. In Table 1, variance of gradient estimators with k = 4 samples on a trained model is reported. The unordered set estimator has the lowest variance in both low and high entropy settings, performing similarly to the stochastic sum-and-sample estimator and REINFORCE with replacement. Results are consistent with experiments in Appendix G.2. Different estimators are used to optimize the ELBO, with the estimator performing comparably to REINFORCE with replacement and outperforming others in some cases. The results show that the unordered set estimator has the lowest variance in both low and high entropy settings, performing similarly to other estimators like stochastic sum-and-sample and REINFORCE with replacement. Overfitting is noted, but regularization is considered a separate issue. The results are based on MNIST binarized at a threshold of 0.5, with comparisons made to other datasets and optimization methods. In Figure 3a, training progress is compared using biased risk estimator with different methods, showing superior performance. Figure 3b demonstrates the ability to train with less data and computational cost using the unordered set estimator. Our estimator, a sampling-based alternative to the biased GumbelSoftmax estimator, is the result of combining three existing estimators to guarantee equal or lower variance. It is parameter-free except for the sample size k, with competitive performance in both high and low entropy scenarios. In experiments, the REINFORCE with replacement method, inspired by VIMCO, showed similar performance in high entropy settings. This estimator has not been widely considered as a baseline in recent work on gradient estimators for discrete distributions, despite its simplicity. The Gumbel-Top-k and Gumbel-Max tricks are used to sample from the Plackett-Luce distribution efficiently. The Gumbel variables are drawn and the set S_k is obtained based on the largest Gumbel values. The Gumbel-Max trick is also utilized for marginalization, improving numerical integration. The Plackett-Luce distribution can be efficiently sampled using the Gumbel-Top-k and Gumbel-Max tricks. Numerical integration is improved by using the Gumbel-Max trick for marginalization. The computation of various expressions can be done exactly or numerically in O(2^k) complexity. The Plackett-Luce distribution can be efficiently sampled using the Gumbel-Top-k and Gumbel-Max tricks. Numerical integration is improved by using the Gumbel-Max trick for marginalization. Computation of leave-one-out ratios for large k involves numerical integration with a default option of a = 5 for smooth approximation using the trapezoid rule. The Plackett-Luce distribution can be efficiently sampled using the Gumbel-Top-k and Gumbel-Max tricks. Numerical integration is improved by using the Gumbel-Max trick for marginalization. The integrands are computed in logarithmic space and summed using the stable LOGSUMEXP trick. The code also efficiently computes all second-order leave-one-out ratios. The sum-and-sample estimator is proven to be unbiased for any set C \u2282 D. Rao-Blackwellizing the stochastic sum-and-sample estimator results in the unordered set estimator. The event S k \u2229b k = s is sampled using sequential sampling. Lemma 2 and Lemma 3 are used to derive estimators with variance reduction. The stochastic sum-and-sample estimator is introduced to trade off the number of summed terms and sampled terms for variance reduction. The unordered set estimator is used to estimate the sampled term. The unordered set estimator is used to estimate the sampled term without replacement, with variance reduction achieved through the stochastic sum-and-sample estimator. Rao-Blackwellizing the estimator with multiple samples results in the unordered set estimator. The importance-weighted estimator, when Rao-Blackwellized, results in the unordered set estimator. This is proven by showing that the conditional expectation of the estimator is the restricted unordered set estimator for a given set of samples. The importance-weighted estimator is defined as q(s, a) = 1 \u2212 F \u03c6s (a), where F \u03c6s is the CDF of the Gumbel distribution. A lemma is proven regarding perturbed log-probabilities, leading to the computation of the importance-weighted policy gradient estimator combining REINFORCE with the importance-weighted estimator. This section is adapted from a workshop paper by Kool et al. (2019b). The importance-weighted policy gradient estimator is computed using a normalized variant with a baseline to correct for bias. The normalization involves using different baselines for each sample, making it more convenient for implementation. To prove unbiasedness, the expectation of the control variate needs to be shown as 0. To prove unbiasedness, the control variate's expectation must be shown as 0. By applying Bayes' Theorem and using equations, it is demonstrated that the control variate is a result of Rao-Blackwellization. The RISK estimator has a built-in baseline, and the gradient of the ratio is rewritten using the log-derivative trick. The text discusses the computation of KL divergence analytically or using a sample estimate, along with equations for gradient estimates in the context of implementing the REINFORCE estimator. The discussion also mentions the cancellation of terms with a built-in baseline and the importance of careful implementation of gradient estimates. The REINFORCE estimator, ARSM, and unordered set estimator are optimized using automatic differentiation software. Gradients are taken directly through the objective using Gumbel-Softmax and RELAX. The ELBO is optimized for 1000 epochs with the Adam optimizer. Different learning rates are used for each estimator to prevent divergence. Gradient variance is evaluated during training with k = 4 samples. During training with REINFORCE, estimators are computed for the same model parameters. Results show negative ELBO on the validation set, with validation error increasing due to overfitting in a large latent space. Early stopping can be used to prevent this issue. Standard binarized MNIST dataset is used for the results. In a practical setting, early stopping can be used to prevent overfitting. Experimentation with the standard binarized MNIST dataset by Salakhutdinov & Murray (2008) and Larochelle & Murray (2011) showed higher -ELBO scores, but still observed overfitting effects. The Travelling Salesman Problem (TSP) involves optimizing the order of visiting locations to minimize tour length. The Travelling Salesman Problem (TSP) involves optimizing the order of visiting locations, given as x, y coordinates, to minimize the total length of the tour. This can be addressed using supervised or reinforcement learning. Kool et al. (2019a) introduced the Attention Model for TSP instances as a fully connected graph, trained using REINFORCE with a greedy rollout. In a study by et al. (2019a), the Attention Model was trained for the Travelling Salesman Problem (TSP) without hyperparameter optimization. The Adam optimizer with a learning rate of 10^-4 was used for 100 epochs. Different gradient estimators were employed to minimize the expected tour length, with adjustments made for batch sizes based on the number of samples used."
}