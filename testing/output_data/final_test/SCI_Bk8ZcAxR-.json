{
    "title": "Bk8ZcAxR-",
    "content": "In reinforcement learning, options help agents break tasks into subtasks for faster learning. Discovering effective options autonomously remains a challenge. This paper explores using representation learning, specifically eigenoptions, to guide option discovery. The algorithm proposed discovers eigenoptions while learning non-linear state representations from raw pixels, leveraging deep reinforcement learning successes. In reinforcement learning, options are used to break tasks into subtasks for faster learning. Discovering effective options autonomously is a challenge. This paper explores using representation learning, specifically eigenoptions, to guide option discovery. The algorithm proposed discovers eigenoptions while learning non-linear state representations from raw pixels, leveraging deep reinforcement learning successes. The approach is demonstrated using traditional tabular domains and Atari 2600 games. Autonomously identifying good options is still an open problem, known as the problem of option discovery. In this paper, the concept of eigenoptions is further explored, focusing on options learned using diffusive information flow in the environment to improve agent performance. Eigenoptions are defined in terms of proto-value functions and can now be extended to stochastic environments with non-enumerated states. The algorithm introduced in the paper discovers eigenoptions while learning representations that approximate diffusive information flow in stochastic environments. By utilizing the successor representation, it can handle stochastic transitions naturally, improving performance in both tabular domains and Atari 2600 games. In the reinforcement learning setting, the algorithm discussed in the paper introduces eigenoptions and utilizes the successor representation to handle stochastic transitions effectively. It shows promising results in Atari 2600 games and tabular domains, demonstrating its applicability in learning representations from raw pixels. The paper discusses algorithms for learning policies in reinforcement learning by approximating value functions using neural networks, such as the Deep Q-network (DQN). These algorithms aim to maximize the expected discounted return by being greedy with respect to state or state-action value estimates. Options BID29, known as temporally extended actions, are the focus of study. They consist of initiation set I \u03c9, policy \u03c0 \u03c9, and termination set T \u03c9. A meta-policy \u00b5 dictates agent behavior, with actions selected according to \u03c0 \u03c9 until reaching T \u03c9. Eigenoptions maximize eigenpurposes r e i, intrinsic reward functions from the DIF model. Eigenpurposes are intrinsic reward functions derived from the DIF model BID16. These reward functions, defined by eigenvectors, encourage the agent to explore different latent dimensions of the state space. In the tabular case, algorithms learning eigenoptions use the graph Laplacian with a weight matrix W and diagonal matrix D. The blue color in the square matrix represents smaller values, indicating states that are temporally further away. The matrix W represents connections between states, but does not naturally handle stochastic or unidirectional transitions. The eigenvectors of L, also known as proto-value functions (PVFs), are derived from the DIF model. In settings where states cannot be enumerated, the DIF model is represented through a matrix of transitions T, with row i encoding the transition vector \u03c6(s t ) \u2212 \u03c6(s t\u22121). This sampling strategy converges the right eigenvectors of matrix T to PVFs in the tabular case. The algorithm introduced in this paper deals with stochastic environments and learns the environment's DIF model while also learning a representation of the environment from raw pixels. The successor representation (SR) determines state generalization based on the expected future occupancy of a state given the agent's policy and starting state. It defines state similarity in terms of time, capturing differences in reaching states based on obstacles. The successor representation (SR) captures state similarities based on time differences in reaching states. It is defined by an indicator function and can be estimated from samples with temporal-difference error. The SR converges to a value function decomposition and is related to dynamic programming and value-function based methods in reinforcement learning. Its eigenvectors correspond to proto-value functions and slow feature analysis. The successor representation (SR) captures state similarities based on time differences in reaching states. It is related to dynamic programming and value-function based methods in reinforcement learning. The SR may have a role in neuroscience, with suggestions that it is encoded by the hippocampus and entorhinal cortex. To discover eigenoptions, eigenpurposes are obtained through eigenvectors encoding the DIF model in the environment. This is currently done through PVFs. In this paper, a new algorithm is introduced to address the challenges of modeling environment dynamics in stochastic settings, particularly in cases where states cannot be enumerated, such as in function approximation. The existing algorithms are limited in their ability to learn a representation while estimating the DIF model, requiring a fixed representation as input. The new algorithm aims to overcome these limitations. The paper introduces an algorithm to estimate the DIF model through the SR by learning a latent representation of game screens. This neural network discovers eigenoptions without needing the combinatorial Laplacian. The algorithm is discussed in the tabular case, showing the equivalence between PVFs and the SR. The agent learns a representation capturing the DIF model and uses eigenvectors to define eigenpurposes. The algorithm introduced in the paper estimates the DIF model through the successor representation by learning a latent representation of game screens. It defines eigenpurposes as intrinsic reward functions that the agent learns to maximize. The algorithm simplifies the process by estimating the DIF model through the successor representation instead of using the graph Laplacian. The eigenvectors of the normalized Laplacian are equivalent to the eigenvectors of the SR scaled by DISPLAYFORM0. This equivalence ensures that the eigenpurposes extraction and learning steps remain unchanged. The use of the SR addresses limitations of previous work, dealing with stochasticity in the environment and agent's policy naturally, with memory cost independent of the number of samples drawn by the agent. It also does not assume symmetry in actions. PVFs are the eigenvectors of the system. The eigenvectors of the normalized Laplacian are equivalent to the eigenvectors of the successor representation scaled by \u03b3 \u22121 D 1/2. This equivalence was first discussed by BID26. Theorem states the relationship between the eigenvalues and eigenvectors of both approaches. The eigenvectors of the normalized Laplacian and the successor representation are related through their eigenvalues. When using PVFs, the focus is on eigenvectors with the smallest eigenvalues for smoothness, while with the SR, the interest lies in eigenvectors with the largest eigenvalues. This distinction is crucial for estimating relevant eigenvectors and robustness to noise in estimation. The normalized Laplacian and successor representation eigenvectors are related through their eigenvalues. PVFs focus on small eigenvalues for smoothness, while SR focuses on large eigenvalues for robustness to noise. In real-world scenarios, generalization and recognizing similar states are crucial. Inspired by previous work, Alg. 2 is proposed to be replaced by a neural network. In response to previous work, the proposal is to replace Alg. 2 with a neural network to estimate the successor representation from raw pixels, overcoming limitations of linear feature representation. Successor features extend the successor representation to the function approximation setting, encoding the expected value of features when following a policy. This approach allows for a natural extension of the update rule to this definition. The proposed neural network architecture, depicted in FIG0, uses the temporal-difference error as a differentiable loss function to estimate successor features. The network receives raw pixels as input and learns to estimate successor features from a lower-dimension representation. The loss function used is L SR, where \u03c6(s) encodes the learned representation of state s and \u03c8(\u00b7) denotes the estimated successor features. The proposed network architecture uses a differentiable loss function to estimate successor features, with two neural networks and a target network for stability. A reconstruction module is added to predict the next state, aiding in learning a representation that considers controllable pixels, improving performance in RL problems. The network architecture includes a reconstruction module to predict the next state and improve performance in RL problems by estimating successor features through a differentiable loss function. The model is trained using RMSProp and follows a specific initialization protocol. Eigenpurposes are defined in terms of a feature representation and eigenvectors of the SR model, generated by the trained network. The network outputs successor features as a vector instead of a matrix. The agent follows a random policy to store network outputs \u03c8(s t ) for successor features. A matrix T is created with rows corresponding to \u03c8(s t ) and eigenvectors e i are defined. The option discovery problem is then treated as a regular RL problem to maximize rewards. Evaluation of discovered eigenoptions is done quantitatively and qualitatively. The impact of approximating the DIF model through SR is assessed in the traditional rooms domain, and purposeful options are discovered from raw data in Atari 2600 games. The proposed network discovers purposeful options from raw pixels in Atari 2600 games. An experiment evaluates estimating the SR from samples instead of assuming the DIF model. The rooms domain is used to evaluate the method, showing the first eigenvector and corresponding eigenoption. The estimated eigenvector is close to the true one, similar to the PVFs obtained for this domain. Plots for true SR, PVF, and different eigenvectors are provided in the Appendix. Eigenoptions improve the agent's ability to explore. The diffusion time metric is used to validate the agent's ability to explore the environment with eigenoptions. Comparing eigenoptions obtained with PVFs and estimates of the SR shows that SR-based eigenoptions help the agent explore better. The difference in diffusion time is attributed to how corners are handled differently. The SR implicitly models self-loops in states adjacent to walls. More episodes lead to more accurate SR estimates and faster diffusion time. Even with few episodes, useful eigenoptions can be discovered, reducing diffusion time significantly. Discovered options go beyond random subgoal selection. The discovered eigenoptions go beyond random subgoal selection, reducing diffusion time significantly. In experiments, the agent learned the greedy policy over primitive actions while following a uniform random policy over actions and eigenoptions. Q-learning with specific parameters was used, and results compared to regular Q-learning showed improved performance. The results show that eigenoptions can speed up learning by reducing diffusion time and improving agent control performance. This is demonstrated through experiments where the agent learned the greedy policy over primitive actions while following a uniform random policy over actions and eigenoptions. The sample efficiency of this approach is not considered in the results, but it is noted that eigenoptions can be beneficial in lifelong learning settings where they are reused across multiple tasks. The experiments evaluated eigenoptions discovered from raw pixels using a neural network to estimate the successor representation. Training was done under a uniform random policy, resulting in improved agent control performance. The results demonstrate that accurate estimates of the successor representation are not necessary for eigenoptions to be useful. After training the neural network on a dataset of 500,000 samples for each game, the agent followed a uniform random policy for 50,000 steps to store the successor representation output. Eigenoptions were approximated due to computational constraints by using the ALE's internal emulator for a one-step lookahead. This method, while limited in dealing with delayed rewards, still provided valuable insights. The limited options obtained do not consider delayed rewards, but promising results were achieved. Eigenoptions were evaluated qualitatively, showing purposeful behavior in reaching and staying at specific locations. This behavior was observed through different trajectories represented by colors on a screen. The agent's trajectory is concentrated on one location on the screen, indicating a myopic local maximum for that eigenpurpose. The algorithm discovers options that push the agent to corners and other relevant parts of the state space, improving exploration. In MONTEZUMA'S REVENGE, the highlighted options correspond to good subgoals for the game. Additional subgoals may not have been found due to a myopic greedy approach. The algorithm's myopic greedy approach may have hindered the discovery of additional subgoals, impacting its effectiveness in FREEWAY. The ability of myopic policies to navigate to specific locations and stay there suggests that the proposed approach generates dense intrinsic rewards, unlike randomly assigned subgoals. This leads to the discovery of eigenoptions from raw pixels, facilitating the development of associated policies. Our algorithm discovered eigenoptions from raw pixels, similar to RAM-based algorithms. It learned meaningful screen parts and is not constrained by state dimensionality or binary features. Results are promising, with potential for better options in a more general setting. Inspired by BID15, we approximated the SR using a neural network. Our novel architecture approximates the successor representation (SR) using a neural network, inspired by BID15. Unlike previous approaches, we define the SR in terms of states instead of state-action pairs. Our network predicts the next state the agent will observe, implicitly learning representations that consider the agent's control over the screen. This contingency awareness has the potential to improve agent performance. Additionally, we explore the use of bottleneck states as subgoals for options, a concept suggested by BID15 but not further explored. The idea of using bottleneck states as subgoals for options was suggested by BID15 but not explored further. BID11 and BID16 showed that options looking for bottleneck states can be harmful in learning. Machado et al. introduced the concept of building hierarchies based on learned latent representations. BID31 learned hierarchies through an end-to-end system without explicit options. BID12 proposed using slow feature analysis (SFA) to discover options, while BID25 found equivalence between PVFs and SFA with a specific adjacency function. The paper introduces a new algorithm for eigenoption discovery in RL using the successor representation (SR) to estimate diffusive information flow in the environment. This approach overcomes limitations of previous work by building accurate estimates with a constant-cost update rule, handling stochastic MDPs, not requiring symmetric transition matrices, and avoiding handcrafted feature representations. The framework utilizes a neural network to estimate the SR. The proposed framework for eigenoption discovery in RL uses a neural network to estimate the successor representation. Future investigations could explore the compositionality and transferability of eigenoptions in different environments, such as Atari 2600 games. Additionally, there is potential to use eigenoptions for reward accumulation instead of exploration. The supplementary material includes a detailed proof of a theorem, empirical results on the impact of episodes on learning eigenvectors, and evaluation of the reconstruction module. The curr_chunk discusses the evaluation of the reconstruction module in eigenoption discovery in RL, specifically focusing on the relationship between eigenvectors of the successor representation and the normalized Laplacian. It emphasizes the importance of estimating the successor representation quickly for use as the DIF model in the environment. In this section, the eigenvectors of the successor representation in the Rooms domain are plotted and compared to proto-value functions and the eigenvectors of the (I \u2212 \u03b3T ) \u22121 matrix. The orientation of the eigenvectors is matched for easier comparison. After 500 episodes, a near-perfect estimate of the first eigenvectors is obtained in the environment. Learning the successor representation for 100 episodes generates eigenoptions that reduce the agent's diffusion time. The options move the agent towards the correct rooms, as shown in FIG1 and FIG2. The structure of policies is evident, with eigenoptions shifted from corners due to how Machado et al. (2017) handled selfloops. Figure 11a suggests options learned after 100 episodes reduce agent's diffusion time by exploring new state spaces. Incremental methods for option discovery show promise for future work. Agent's reward accumulation post-eigenoptions is evaluated in Section 4.1. In Section 4.1, the agent's reward accumulation post-eigenoptions is evaluated using Q-learning BID33 with parameters \u03bb = 0, \u03b1 = 0.1, and \u03b3 = 0.9. Results comparing the performance of eigenoptions extracted from estimates of the SR after 100, 500, and 1000 episodes are presented. Eigenoptions extracted from rough estimates of the SR after 100 episodes can improve the agent's control performance by reducing diffusion time and increasing coverage of the state space. More accurate predictions of the SR further enhance the agent's performance, especially with a larger number of eigenoptions. The proposed network accurately predicts the general structure of the environment and tracks moving sprites on the screen, although the prediction is noisy. Even an underperforming network can learn useful representations for the algorithm. The proposed network can learn useful representations for the algorithm, leading to better options. Two meaningful eigenoptions were discovered in the game FREEWAY, represented by the avatar's position on the screen in trajectories."
}