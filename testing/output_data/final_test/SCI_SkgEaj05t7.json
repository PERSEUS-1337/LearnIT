{
    "title": "SkgEaj05t7",
    "content": "The paper investigates the curvature of the loss surface during the training of deep neural networks with Stochastic Gradient Descent (SGD). It is found that SGD visits increasingly sharp regions initially, reaching a maximum sharpness determined by the learning rate and batch-size. At this peak, SGD fails to minimize the loss along the sharpest directions. A variant of SGD with reduced learning rate along these directions can improve training speed. The study explores how adjusting the learning rate along the sharpest directions in Stochastic Gradient Descent (SGD) can enhance training speed and lead to sharper and better generalizing solutions compared to standard SGD. It is suggested that SGD's dynamics in the subspace of the sharpest directions influence the regions visited, training speed, and generalization ability of deep neural networks. The study investigates the relationship between the curvature of final minima reached by SGD and generalization. Models with wide minima are believed to generalize better than those with sharp minima. The analysis focuses on the entire training trajectory of SGD, particularly on the evolution of the largest eigenvalues of the Hessian. The study examines the evolution of curvature during training with SGD, showing that sharp directions initially grow, leading to crossing the minimum if steps are too large. The curvature stabilizes or decreases with a peak value influenced by learning rate and batch size. The study investigates the dynamics of SGD in relation to sharp directions, finding that initial regions resemble bowls with curvatures that lead to suboptimal steps. A variant with reduced learning rate along sharp directions optimizes faster and generalizes better compared to vanilla SGD. This paper analyzes the dynamics of SGD in the subspace of sharpest directions, showing how it influences training speed and generalization capability. Experiments were conducted on Resnet-32 and SimpleCNN models on the CIFAR-10 dataset, achieving 86% test accuracy. The study explores training dynamics of various models on CIFAR-10, including VGG-11 and LSTM with dropout regularization. Resnet-32 excludes Batch-Normalization layers and uses scaled initialization. Results show evolution of top eigenvalues of the Hessian for SimpleCNN and Resnet-32. The study examines the top 10 eigenvalues of the Hessian for SimpleCNN and Resnet-32 trained on CIFAR-10 with specific parameters. The evolution of the eigenvalues shows initial growth followed by oscillatory behavior during training. Training and test accuracy of the models are also provided for reference. The study focuses on approximating the top eigenvalues of the Hessian for large models using the Lanczos algorithm. Regularization techniques like dropout and L2 are applied during training and when estimating the eigenvalues. In this study, regularization techniques like dropout and L2 are applied to estimate the Hessian eigenvalues. The code for the project can be found at https://github.com/kudkudak/dnn_sharpest_directions. The research focuses on the eigenvalues of the Hessian of the training loss along the SGD optimization trajectory, showing that SGD steers towards sharper regions until a maximum is reached. SGD explores flatter regions with larger learning rates or smaller batch sizes. The training loss curvature in the sharpest directions is investigated for SimpleCNN and Resnet-32 models. The largest eigenvalues of the Hessian grow initially during training of SimpleCNN and Resnet-32 models using SGD. The eigenvalues show rapid growth in the first epochs, reaching a maximum value before decreasing steadily. The evolution of the curvature is closely tracked, with eigenvalues alternating between decreasing and increasing. The behavior of SGD is reflected in the evolution of accuracy, showing initial difficulty in optimizing due to large curvature. Full-batch gradient descent training of Resnet-32 without Batch-Normalization exhibits instability with a sharp drop in accuracy. Batch-Normalization layers help alleviate this instability. Additional results on late training phases, such as the impact of learning rate schedules, are also reported. The impact of learning rate and batch size on the SGD path is investigated in terms of curvatures. Larger learning rates or smaller batch sizes result in a smaller and earlier peak of the spectral norm and largest eigenvalue. Momentum also affects the spectral norm peak. Similar observations hold for VGG-11 and Resnet-32 with Batch-Normalization. The learning rate and batch size influence the SGD trajectory and spectral norm peak, impacting training dynamics and generalization capability. Future work could focus on theoretical analysis to establish the generality of these results. SGD dynamics are influenced by the shape of the loss surface in the sharpest direction, where the step taken by SGD may be too large compared to curvature to reduce loss effectively. The study focuses on SimpleCNN and Resnet-32 models in the first 6 epochs of training with specific parameters. The relationship between the SGD step and the sharpest direction is investigated, showing that the loss value changes when moving along the sharpest direction. The analysis reveals that E[L(\u03b8(t) \u2212 \u03b1\u03b7g 1 (t))] increases for \u03b1 > 1 and decreases for \u03b1 < 1, impacting the training dynamics. The study examines the impact of different values of \u03b1 on loss for SimpleCNN and Resnet-32 models. Results show that \u03b1 = 2 and \u03b1 = 4 lead to loss increases, while \u03b1 = 0.25 and \u03b1 = 0.5 result in decreases. The alignment of the SGD step with the sharpest directions affects optimization effectiveness. The average cosine between the gradient and the top 5 sharpest directions is high, indicating strong alignment. The gradient is highly aligned with the sharpest direction during optimization, with the maximum average cosine between 0.2 and 0.4. SGD steps often overshoot in the sharpest direction early in training, leading to increased loss when step size is doubled. Experiments on SimpleCNN and Resnet-32 models with different values of \u03b1 show varying impacts on loss. Increasing the step size by a factor of two leads to higher loss on average. Qualitative visualization of the surface along the top eigenvectors for SimpleCNN and Resnet-32 supports that the SGD step length is large compared to the curvature along the top eigenvector. The visualization is scaled using the expected norm of the SGD step. After six epochs, the loss on the scale of \u2206\u03b8 1 (t) shows a bowl-like structure in the largest eigenvalue direction for SimpleCNN and Resnet-32 models. Training and validation accuracy details are provided in Appendix B. SGD tends to steer towards regions where the step aligns with the sharpest directions, potentially increasing loss along those directions. This suggests that optimization is ineffective along the sharpest direction, which will be further explored in Sec. 4. In Sec. 4, the dynamics around the initial growth of the spectral norm of the Hessian are discussed. Variants of SGD optimizing the sharpest directions are compared against a baseline. Parameter updates for the SGD variants are based on the mini-batch gradient. The study compares different variants of SGD that update parameters based on the gradient projection on the top eigenvector direction of the Hessian. Results show that following the top eigenvector leads to wider regions but not lower loss values, while ignoring it leads to lower loss values but sharper regions. SGD updates in the top eigenvector direction strongly influence the training path in the early phase. In the final section, the study explores a variant of SGD called Nudged-SGD (NSGD) to investigate the convergence speed and generalization of the model. NSGD aims to reduce the alignment of the SGD update direction with the sharpest directions. Instead of the standard SGD update, NSGD uses a different update method. NSGD uses a different learning rate along the top K eigenvectors, compared to the normal SGD gradient along other directions. Experiments were conducted with Resnet-32 and SimpleCNN on CIFAR-10, varying the number of sharpest eigenvectors and the rescaling factor. The top eigenvectors are recalculated at the start of each epoch to compare the sharpness of the endpoint using Frobenius norm and spectral norm of the Hessian. NSGD optimizes faster by adjusting learning rates along the top eigenvectors, showing sharper optimization compared to baseline. Increasing K leads to improved performance. Results for Resnet-32 are shown in Fig. 8 and Tab. 1, while full results for SimpleCNN are in Appendix, Tab. 2. Increasing K in NSGD with a fixed \u03b3 = 0.01 improves training speed and sharpness, reaching a maximum \u03bb max of 8 \u00b7 10 3 compared to baseline SGD. NSGD also retains an advantage of over 5% validation accuracy even after 50 epochs. Using \u03b3 < 1 results in a sharper endpoint with slightly improved generalization, while \u03b3 > 1 leads to a wider endpoint and worse generalization. A larger K generally improves performance. Increasing K in NSGD with a fixed \u03b3 = 0.01 improves training speed and sharpness, reaching a maximum \u03bb max of 8 \u00b7 10 3 compared to baseline SGD. NSGD also retains an advantage of over 5% validation accuracy even after 50 epochs. Using \u03b3 < 1 results in a sharper endpoint with slightly improved generalization, while \u03b3 > 1 leads to a wider endpoint and worse generalization. A larger K generally improves performance, with NSGD showing promising results in terms of test accuracy and Hessian norm values. NSGD impact on final generalization can be dataset dependent, steering towards sharper regions and optimizing faster. Results on CIFAR-100, Fashion MNIST, BID25, and IMDB BID14 datasets are included. NSGD can find good generalizing sharp minima compared to vanilla SGD. Additional studies on high base learning and momentum are in Appendix C. The largest eigenvalues of the Hessian of the loss of DNNs were investigated previously but mostly in the late phase of training. Some notable exceptions are: BID13 who first track the Hessian spectral norm, and the initial growth is reported (though not commented on). BID18 report that the spectral norm of the Hessian reduces towards the end of training. BID10 observe that a sharpness metric grows initially for large batch-size, but only decays for small batch-size. Our observations concern the eigenvalues and eigenvectors of the Hessian, which follow the consistent pattern, as discussed in Sec. 3.1. Finally, BID27 study the relation between the. The width of minima found by SGD relates to generalization and batch-size. Reparametrizations of networks can increase sharpness without affecting generalization. Recent work explores the importance of curvature for generalization. Studies on SGD dynamics have been conducted, investigating its relation to generalization. SGD dynamics, such as BID6, BID2, BID26, and BID30, have been studied in relation to escaping sharp minima and the importance of noise along the top eigenvector. While previous work focused on the final minima, we show that SGD visits regions with steps too large compared to curvature from the beginning of training. Our study investigates the relation between SGD dynamics and the sharpest directions, highlighting its importance for training deep neural networks. SGD dynamics are crucial for training deep neural networks. The SGD step is large compared to curvature along the sharpest directions, impacting the regions visited during optimization. Understanding optimization along the sharpest directions can provide insights into generalization properties and help design tailored optimizers for neural networks. Additionally, Batch-Normalization layers can address instability in early full-batch training phases. In this study, the authors evaluated the Hessian in the inference mode for VGG-11 and Batch-Normalized Resnet-32 models. They focused on SGD with a constant learning rate and batch-size, and examined the evolution of the spectral and Frobenius norm of the Hessian using a simple learning rate schedule. Results showed that depending on the learning rate schedule, curvature along the sharpest directions can either decay or grow during training. Training for a shorter time led to a growth of curvature after the learning drop and lower final validation accuracy. Large momentum reduces spectral norm of the Hessian. Additional results for different experiments are reported. In Sec. 4, additional results are presented for experiments using the SimpleCNN and Resnet-32 models on CIFAR-100 and Fashion-MNIST datasets. Results show dataset-dependent behavior, with observations on generalization for different values of \u03b3. NSGD was explored with suboptimal learning rates, large base learning rates, and momentum, showing implicit regularization effects. Results are detailed in Tables 2, 3, 4, 5, and 6. The study explores NSGD with different learning rates and momentum, showing improved training speed and sharper initial regions. The SimpleCNN model used has four convolutional layers with varying filter sizes and activation functions. Max-pooling is applied after certain convolutional layers, followed by linear layers with specific output sizes. Further investigation with higher learning rates or momentum is suggested for future research. In this section, Nudged-SGD (NSGD) is discussed as a second-order method that differs significantly from the Newton method. NSGD does not use an optimal learning rate for curvature, leading to sharper loss surfaces and optimization in sharper regions. Results from a study on NSGD show a sharper loss surface compared to SGD, indicating a different approach to handling curvature. In this section, NSGD is discussed as a second-order method that optimizes over sharper regions compared to SGD. An example is constructed with a diagonal H matrix to show that NSGD with specific parameters is equivalent to Newton's method. Experiments are extended to the text domain using the IMDB binary sentiment classification dataset with a CNN model. In this experiment, the impact of learning rate and batch-size on the Hessian along the training trajectory is examined using a simple CNN model. Results show that the learning rate and batch-size limit the maximum curvature along the trajectory. Nudged SGD with specific parameters optimizes faster and finds a sharper region initially. NSGD for \u03b3 < 1 optimizes significantly faster and finds a sharper region initially, but does not result in finding a better generalizing region. The results are summarized in Tab."
}