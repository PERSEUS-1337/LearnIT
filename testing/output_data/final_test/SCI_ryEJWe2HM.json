{
    "title": "ryEJWe2HM",
    "content": "Automatic melody generation for pop music has been a challenge due to the complexity of representing notes and staying within the acceptable musical range. To address this, a unique 'word' representation for each note is proposed to simplify learning and ensure alignment of properties. Regularization policies are enforced to keep the generated melody close to what is easy for humans to follow. Our model can generate auditorily pleasant songs that closely resemble human-written ones by generating melody conditioned on song part information, replicating the overall structure of a full song. Recent deep learning techniques have greatly impacted vision and language tasks, with a focus on artistic generation. However, the application of deep learning to music is still in its early stages, lacking the quality seen in other fields. This is evident in both physical audio and abstract musical formats like notes or MIDI. The challenges in deep learning-enabled music generation stem from the complexity of musical properties, the blurred line between creativity and clumsiness, and the structural complexity of music genres. In this paper, a new model for music generation is proposed, focusing on symbolic generation of melodies for pop music in MIDI format. Pop music is defined by short song lengths, simple and memorable melodies, and low structural complexity. MIDI format represents a discrete abstraction of musical sound. The proposed model for music generation focuses on symbolic generation of melodies for pop music in MIDI format. It treats notes as unique 'words' and combines their properties to form a melody, inspired by recent successes in image generation. The model for music generation focuses on generating melodies for pop music in MIDI format by treating notes as unique 'words' and using chord sequences and part annotations generated by a hidden markov model. The model combines Bayesian graphical models with deep neural networks, with the HMM solely used for feature input generation. The model proposed for music generation focuses on generating euphonious melodies for pop music by treating each note as a unique \"word\" and using chord sequences and part annotations. Regularization policies are enforced on the range of notes to prevent excessively wide pitches. Training with part annotation helps generate more appropriate melodies even with identical chord sequences. Additional experiments include using generative adversarial networks and multi-track songs. The model focuses on generating melodies for pop music by treating each note as a unique \"word\" and using chord sequences. It implements supplementary models for melody generation and regularization. The dataset with chord and part annotation enables efficient learning. Recent works employ neural networks for music composition, contrasting with rule-based approaches. DeepBach generates Bach-like music using pseudo-Gibbs sampling, discretizing time into sixteenth notes. The model presented in the curr_chunk is a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation to generate natural sentences describing an image. It can handle multiple notes at the same position and does not require awareness of each discrete time step. The model presented is a generative model trained to maximize the likelihood of the target description sentence given the training image. It achieves high accuracy and fluency in language learning solely from image descriptions, with significant improvements in BLEU-1 scores on various datasets. This capability could greatly benefit visually impaired individuals by providing accurate descriptions of image content in properly formed English sentences. NIC is a model designed to help visually impaired individuals understand image content on the web by generating complete sentences in English from input images. It combines a vision CNN with a language generating RNN to achieve accurate and fluent descriptions, benefiting those who rely on image descriptions for comprehension. The work presents a single joint model that takes an image as input and is trained to generate a target sequence of words describing the image. Inspired by advances in machine translation, the model aims to maximize the likelihood of producing accurate descriptions using Recurrent Neural Networks. In this work, a model is proposed that uses a deep convolutional neural network (CNN) instead of an encoder RNN to generate target sequences based on input images. This approach aims to improve performance in generating descriptions of images, drawing inspiration from machine translation techniques. The BID8 model uses reinforcement learning to enhance recurrent neural networks for music generation, while BID0 utilizes a hierarchical RNN for multi-track song production. Our model differs by not requiring preset rules and simplifying control with regularizations. Our model for melody generation is illustrated by an analogy to image captioning tasks. WaveNet, originally designed for text-to-speech conversion, models waveform as a series of audio samples conditioned on previous timesteps, showing promising results in music generation. The model for melody generation is compared to image captioning tasks. Each note and its properties are treated as a unique 'word,' with melody represented as a 'sentence' to be generated. LSTM is used for word generation, conditioned on music-relevant features instead of CNN image features. Maximum log likelihood estimation is performed to find the parameters set for training samples. Our model of melody representation contrasts with the traditional approach of using separate layers for each property. Unlike the previous method, our model does not need to consider intervals without notes, as positional information is already included in the word representation. This gives us an advantage, especially when dealing with simultaneous notes. Our model simplifies melody generation by combining pitch and length information in one layer, allowing for chord formation. The LSTM model uses musical input features and randomly initialized word vectors. The conventional gate functions for LSTM are utilized for non-linearity activation. The LSTM model uses musical input features for non-linearity activation, with h t1 as memory output from the previous timestep, b i as bias, and i t ,f t ,o t as input, forget, output gates. The melody generation model is conditioned on chord sequence and part information, automated through a twofold multinomial Hidden Markov Model (HMM) for input generation. The LSTM model uses musical input features for non-linearity activation. The generated melody is conditioned on chord sequence and part information, with regularization enforced on the pitch range to ensure the generated melody stays within a human-friendly pitch range. The regularization on pitch range in the LSTM model ensures the generated melody stays within a human-friendly pitch range. The pitch range is set between 60 (C4) and 72 (C5), with a regularization coefficient of 0.0001. The dataset consists of 46 MIDI songs, mainly unpublished materials from musicians, with permission obtained for training. The domain is restricted to pop music of major scale for more efficient learning. Adjusting each song's scale to C Major eliminates the risk of mismatch between scale and generated melody. This widens the pitch range of the melody, but can be mitigated by a regularization scheme. Chords were manually annotated for each bar, focusing on major and minor chords with one exception. The system can still generate songs with more complex chords, even when conditioned on C Major. Our system can generate melodies that indirectly form chords like C Major 7th when conditioned on C Major. We experimented with different bar lengths for chord sequences and found that using sequences over 2 bars provided the best balance between thematic continuity and data density. Non-overlapping chord sequences were annotated, and examples of songs conditioned on different bar lengths can be found in our demo. The dataset construction involved non-overlapping chord sequences and discretizing note lengths. Prettymidi framework was used, resulting in 2082 unique 'words' in the vocabulary. Learning rate was set at 0.001 with 1.6M learnable parameters. Dropout was applied to LSTM. Comparison was made with recent works using deep learning for music generation in MIDI format. Human evaluation tasks were conducted on Amazon Mechanical Turk. The study involved human evaluation tasks on Amazon Mechanical Turk comparing outcomes from the model with two baseline models, BID0 and BID8. BID3 was excluded due to a different domain. Participants rated songs based on melody, structure, and preference. A Turing test was conducted to determine if songs were written by human or AI. Results showed the model outperformed BID0 in all aspects. Our model outperformed the baseline models in all aspects except structure, which was likely due to their use of pre-defined rules for musical formality. Despite this, our model was preferred for its natural melodies and frequency of preference. Participants often chose our model over others, suggesting that humanness does not always correlate with musical taste. Expertise in music also influenced preferences, with intermediate to high expertise favoring our model. Our model had the best deception rate among artificial generation models, showing that generating natural melody while preserving structure is key for human-like music generation. Generative Adversarial Networks (GANs) have proven to be powerful in generating music as images. The generator G and discriminator D in GAN receive inputs of random noise z and condition c, with D distinguishing between real data x and generated outputs. D is trained to minimize its error while G is trained to improve its output. The model uses a two-hot feature vector as the condition and implements down-sampling, up-sampling, Adam optimizer, and batch normalization. Despite some advantages in capturing harmonies, the generated results are often out of tune and restricted in melody patterns. Melody generation with GAN can avoid overfitting but faces mode collapse issues. Difficulty in distinguishing single notes or consecutive notes. Problems stem from modalities difference between image and music. Generated instruments sound in tune individually, confirming model applicability to other tracks. Our proposed model can generate instrument tracks with simultaneous notes, but combining them into a 4-track song resulted in dissonant and unorganized songs. This indicates the need for a more advanced model to learn the interrelations among instruments for multi-track song generation. Our model, inspired by image captioning tasks, differs fundamentally in task objective. Matching human-written descriptions is crucial for image captioning performance. In melody generation, it is important to avoid plagiarism by not resembling human-written melodies too closely. Training should strike a balance between learning patterns and avoiding overfitting. Generated songs started to stay in tune after 5 epochs, but after 20 epochs, overfitting was observed with the same melodies as in the training data. Previous approaches have tackled this issue by rewarding adherence to rules while also encouraging off-policy behavior. In melody generation, a balance between learning patterns and avoiding overfitting is crucial. Previous approaches have tackled this dilemma by rewarding rule adherence while encouraging off-policy behavior. To address this, a novel model was proposed to generate pop music melodies using word representation of notes and properties, reducing complexity. A future focus is designing a more suitable loss function to penalize matching melodies in training data. Generating songs with parameters from different training stages leads to diverse melodies, complementing the low-dimensional input representation. Our model for generating pop music melodies uses a regularization model to control outcomes and implements part-dependent melody generation. Experimental results show that our model produces songs with more human-like melodies and better structure than previous models. Future work includes exploring other music styles and properties of notes, as well as modeling correlations among different instruments. \"We aim to update our dataset and repository regularly for future tasks.\""
}