{
    "title": "r1gKNs0qYX",
    "content": "This report introduces a training and recognition scheme for neural networks that uses class-wise discerning to improve performance. Trained with shuffled datasets, the network learns class-wise parameter values to filter objects belonging to specific classes. Classification is inferred from the maximum response of the filters, and multiple checks with different versions of filters reduce fluctuation and enhance performance. This method mimics the classification process through one component picking, offering a promising feature abstraction procedure. The scheme introduced in this work involves training neural networks to function as filters for classifying objects based on their responses. Unlike standard neural networks, which treat all classes equally, these filter networks respond differently to features of different classes. The training data used in this scheme contain subsets of identical sample numbers for each class, resembling a process of sorting colored balls by grouping them based on color. Ensemblization is a key aspect of the recognition scheme involving training neural networks as filters for classification. A classification procedure called DMM (discerning, maximum response, and multiple check) is introduced to improve accuracy by introducing a hierarchy of ensemblization for the filters. This approach aims to ensure the correct filter emerges as the overall winner in the classification process. The DMM scheme enhances accuracy of neural networks by reducing a multiclass problem to a binary classification, making it easier to distinguish the class of interest. This approach can improve performance for networks with varying capacities and training data sizes. Additionally, training specific filters for each class serves as a feature abstraction procedure, leading to overall performance enhancement in feedforward networks. The DMM scheme simplifies neural network training by converting a multiclass problem into binary classification, improving accuracy. It involves training specific filters for each class to enhance feature abstraction in feedforward networks. In a toy model, points in a 2D plane are classified to demonstrate reshaped decision boundaries, with a focus on the maximum response criterion for classification inference. MMD is experimented on CIFAR-10 and MNIST datasets, using a training dataset with points around four centers labeled with different colors. The Adam optimizer with a learning rate of 10 \u22124 and random shuffling of samples are used in training. By filter training, features specific to a class are emphasized while others are suppressed. Retraining the network with a subset of the class results in a model that constantly maps to that class label. Results of retraining the network with the blue subset are shown in figures. The retraining procedure with a subset of the blue class leads to a constant mapping, blurring all features and causing the model to lose recognition capacity. To improve filter training, random label shuffling among samples in unspecified classes or assigning labels of other unspecified classes can be considered. Preparation-1 involves smaller data size, equivalent to the original, with only two alternative classes in the toy model. Preparation-2 results in severe size enlargement as the number of classes increases, with each subclass dataset augmented by a factor of Nc-1. The advantage of Preparation-2 is better randomization of data through multiple labeling. Preparation-1 involves a dataset with definite labeling, while Preparation-2 results in better randomization of data through multiple labeling. Both preparations lead to increased accuracy of the targeted class, with no decision boundary destruction observed during filter training. The training accuracies and decision boundaries follow a similar pattern in both preparations, suggesting the potential use of filter training to abstract features and construct. The filter training in Preparation-2 resulted in a considerable increase in accuracy for the targeted class, with only a slight increase in misassignment. The dynamics of decision boundary construction show how the filter training works by purposefully and selectively drawing in data to adjust the boundary. The filter training in Preparation-2 significantly improved accuracy for the targeted class by adjusting the decision boundary based on gradients. Forces from samples in unspecified classes are diminished, allowing the specified class to dominate the boundary movement. This results in a well-defined boundary that effectively separates the class of interest from others. In filter training, a selection rule is needed to infer classification based on softmax evaluations. Ideally, only one filter should have a strong response, but in reality, multiple filters may give positive indications. The dilemma of choosing the filter with the strongest response arises when outputs have a specific pattern. In filter training, a selection rule is needed to infer classification based on softmax evaluations. Ideally, only one filter should have a strong response, but in reality, multiple filters may give positive indications. The dilemma of choosing the filter with the strongest response arises when outputs have a specific pattern where r 12 > r 11 > r 22 > r 21 (r 23). The first filter has the strongest response at the corresponding digit (r 11 > r 22) but the indication is negative (r 11 < r 12), while the second filter makes a positive indication (r 22 > r 21 (r 23)). Which is superior, the maximum response (MR) or the positive indication (PI)? The third term in the equation ensures a result when no positive indication is made. It does not alter the superiority as any positive indication scores 1. The maximum response criterion dictates the classification when only the third term is present. Different ways to solve the problem of multiple positive indications include consulting an expert or training expert networks with subclass datasets. The study focuses on using peer review and expert networks with subclass datasets to filter out fluctuations in softmax evaluations that can lead to false positive indications. Introducing randomness helps build up the incremental status of real positive indications. In the study, randomness is introduced to filter out fluctuations in softmax evaluations, which can lead to false positive indications. The training process involves retraining the network to build new versions, with scores of different versions averaged. The results show that stronger fluctuations can cause a bigger difference in performance, and improvements near the decision boundary are made by DMM. The Dirichlet distribution is assumed for the output of every filter when randomness is introduced. Randomness is introduced into filter versions through stochastic operations like stochastic gradient descent, random shuffling of training samples, and random label assignment. Outputs in different versions are stochastic due to this randomness. The fixated point as input remains unchanged. In filter training, the decision boundary enclosing the class of interest tends to expand, leading to biased probabilities in less relevant distributions. Specialties are not endowed to certain classes in filter training. Shifts in points between classes can raise biases. When the point shifts between classes, biases are raised between the filters. The probability density peaks of the filters move accordingly, with a shift into class-1 causing a positive change (\u2206 > 0) and a shift into class-2 causing a negative change (\u2206 \u2192 \u2212\u2206). The variation in distribution on these digits is attributed to the secondary effect of varied \u03b1. The randomness in label shuffling diminishes dependence on position and direction, although weak dependence may still exist due to uneven distribution in the training data. It is assumed that r ik > r ij for k > j, beside r 12 > r 11 > r 22 > r 21. The problem involves Bayesian-like inference to determine which class yields higher incidence based on trends. By taking the derivative with respect to \u2206, it is found that a positive shift into class-1 increases the incidence. This enhancement by DMM reduces workload and leads to more delicate decision boundaries. The enhancement by DMM reduces workload and allows for more delicate decision boundaries. Corrections are made near mistakes to maintain accuracy. Trading one wrong for two rights results in a net gain. High capacity is needed for precise discrimination, similar to requiring significant brain power for small gains. Sparse regions show economic considerations, with DMM adding rights without wrongs. Adapting decision boundaries in sparse regions requires more attention, which may not be cost-effective without sufficient resources. DMM is an ensemble learning scheme with specialties in classification and pertinence evaluation. It integrates classifier selection and fusion approaches, making it versatile for ensemble learning. DMM can be incorporated into other schemes for additional ensemblization hierarchy. The task simplification in filter training is unique due to random label shuffling. In filter training, task simplification is achieved by making most data irrelevant, not by partitioning it. Resampling and label randomization can work together to provide diversity and improve filter training. Filters can guide resampling by specifying classes and generating comprehensive metrics. Experiments used a CNN with two convolutional layers and two fully connected layers. The original network consists of two convolutional layers with 32 and 64 filters, a hidden fully connected layer with 128 nodes, and ReLU activation. It is trained for 100 epochs with a batch size of 100. Filter training involves retraining only the output layer, not the entire network. Additional iterations and fine-tuning the output layer do not lead to improvement. Training accuracy and misassignment are evaluated with the training batch. Filter training was initially performed on the output layer for CIFAR-10, showing increasing accuracy for the specified class and decreasing accuracy for an alternative class. However, multiple check results did not show improvement due to insufficient randomness in label shuffling. Similar accuracies for different filter versions indicate small fluctuations in softmax outputs. Filter training on CIFAR-10 showed varying accuracies for different classes, with misassignment highlighted in red. Strategies involving label permutation and preparation methods were implemented to enhance diversity in filter versions. Results indicated successful increments in accuracy, with a shift from preparation-1 to preparation-2 showing promising outcomes. The experiment involved changing preparation methods from preparation-1 to preparation-2, but it did not significantly improve accuracy. More randomization among filter versions is needed for large data sizes. Using preparation-1 yielded better results in subsequent experiments. Training with small data sizes showed overfitting, which was corrected by training multiple filter versions. Dividing the CIFAR-10 dataset into 10 partitions and training 10 filter versions further increased accuracy. Turning off class-wise label permutation did not affect performance, suggesting diversity can enhance results. The diversity in dataset modification, such as augmentation or noise randomization, can improve performance. Results from experiments on MNIST show increased training and test accuracies with DMM. Filter training and maximum response can enhance performance by properly distributing fluctuations among filter versions. Dealing with different types of fluctuations is crucial for optimal performance. DMM, a special ensemble learning scheme, can be integrated into other schemes to handle fluctuations. Integrating similar mechanisms into different network architectures can simplify tasks in intelligence activities."
}