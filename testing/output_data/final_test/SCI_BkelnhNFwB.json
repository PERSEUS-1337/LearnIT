{
    "title": "BkelnhNFwB",
    "content": "Deep learning has been successful in various applications, but there is a theoretical puzzle regarding the generalization ability of deep networks. Training performance may not reflect testing performance, leading to questions about the need for a new theory of generalization. However, research shows that appropriate performance measurement can predict expected performance, aligning with classical machine learning theory. The lack of generalization in deep networks has raised questions about the relationship between training and testing performance. Research has shown that deep networks with more parameters than the training set size can have zero training error but poor testing performance. This discrepancy has led to the need for a new theory of generalization in machine learning. The riddle of large capacity and good predictive performance in deep networks has led to various hypotheses and explanations in research papers. Key concepts such as loss and classification error are defined, highlighting the challenges in explaining the generalization ability of large artificial neural networks. The logistic loss is related to classification error, with the former being an upper bound for the latter. Training minimizes the logistic loss, which can be used as a proxy for testing. For separable data, the logistic loss can be made arbitrarily small by scaling up the value of f. The logistic loss of the normalized network is considered by measuring the performance on the training set in terms of the weights of the network. The performance on the training set serves as a proxy for future performance, with the output normalized by dividing it by the product of the Frobenius norms of the weight matrices at each layer. The cross-entropy test loss is plotted against the training loss for networks with different initializations. Normalizing each network separately tightens the prediction of test loss by training loss. Normalization does not affect classification performance, as it depends only on the sign of f(x). The empirical cross-entropy loss on the training set predicts well how networks will perform on a test set. The training performance can predict the test loss accurately, even when training on randomly labeled data. This challenges previous criticisms of machine learning theory. Normalized train cross-entropy can forecast test performance, showing a linear relationship between them. The deep network defined with K layers uses ReLU activation functions and normalized weight matrices. The network is denoted as f = f(W1, ..., WK; x) where x is the input and WK is the last layer matrix. The labels are y n \u2208 {\u22121, 1} for binary classification. In deep networks with ReLU activation functions and normalized weight matrices, different test losses are obtained depending on initial conditions. The question is whether test performance can be predicted from properties measured on the training set. The theory section discusses binary classification, with results for MNIST and CIFAR100 in the appendix. The \"positive homogeneity\" property of ReLU networks is highlighted, showing a specific structure of the loss for deep networks. In deep networks with ReLU activation functions and normalized weight matrices, different test losses are obtained depending on initial conditions. The theory section discusses binary classification, with results for MNIST and CIFAR100 in the appendix. The \"positive homogeneity\" property of ReLU networks is highlighted, showing a specific structure of the loss for deep networks. The \u03c1 factor continues to increase with time towards infinity during gradient descent on separable data, driving an exponential type loss to zero without affecting classification performance. In deep networks with ReLU activation functions and normalized weight matrices, different test losses are obtained depending on initial conditions. The theory section discusses binary classification, with results for MNIST and CIFAR100 in the appendix. The \"positive homogeneity\" property of ReLU networks is highlighted, showing a specific structure of the loss for deep networks. The \u03c1 factor continues to increase with time towards infinity during gradient descent on separable data, driving an exponential type loss to zero without affecting classification performance. An overparametrized network may have a large number of global minima. The logistic loss is computed to assess f(x), with layer-wise normalization using L2. The results of Figure 2 are explained in terms of classical machine learning theory, with a generalization bound provided. The Rademacher complexity and homogeneity of networks are used to bound the cross-entropy loss for unnormalized and normalized networks. Experimental results show that the unnormalized test loss differs significantly from zero, while the normalized test loss closely matches the train loss. The relationship between the losses is predicted by Equation 6, with experimental verification shown in Figure 2. The prediction is experimentally verified in Figure 2, showing that homogeneity of the ReLU network and separability are key assumptions. Equation 6 demonstrates that training cross-entropy loss is a good proxy for testing loss, leading to generalization for a small number of examples. Classification performance is perfect in training, and scaling does not change network behavior. Performance on randomly labeled training set is poor for normalized networks. The theory does not guarantee that a network with lower cross-entropy loss in training will always have lower classification error at test. Empirical evidence suggests a roughly monotonic relationship between training and testing loss of the normalized network and its expected classification error. The normalized network shows a linear relationship between test cross-entropy loss and classification error. Generalization error decreases with normalization, leading to asymptotic generalization in deep neural networks. This challenges the idea that deep learning is beyond existing machine learning theory. This paper challenges the notion that deep learning is beyond existing machine learning theory by showing that networks are not \"magical\" and can be understood. The generalization gap decreases with normalization, leading to asymptotic generalization in deep neural networks. The cross-entropy loss at training predicts well the cross-entropy loss at test when the function space complexity is reduced. The note discusses the equivalence of consistency and generalization in deep networks, highlighting situations where there is interpolation of training data and good expected error. This implies an implicit regularization effect for deep nets, similar to kernel machines, with a focus on the regime where the number of weights exceeds the number of samples. The complexity of normalized networks is controlled by the optimization process, with a theory proposed for the implicit regularization mechanism. Monitoring the normalized cross-entropy loss during training is recommended for practitioners, as it predicts test performance accurately. This paper confirms the classical theory of Deep Learning. This paper confirms that classical machine learning theory can explain how training performance relates to testing performance in deep networks, specifically in the case of binary classification. The extension to multi-class classification is straightforward, with the neural network represented as a vector f(W;x) \u2208 RC. The main theoretical arguments focus on generalization bounds and multi-class loss. The Softmax function is used to transform network outputs into probabilities, with the cross-entropy loss defined simply. Classification depends on the margin \u03b7n, where if \u03b7n > 0, the example is correctly classified. The loss monotonically decreases with increasing \u03c1 for separable data. The Cross-entropy loss can be driven to 0 by increasing \u03c1 \u2192 \u221e. Testing vs training loss graphs for networks trained on the same data sets show zero classification error at training. A network trained on CIFAR data with randomized labels shows test error at chance level. The networks are 3-layer networks with specific layer configurations. The network architecture consists of three layers: two convolutional layers with 64 filters of size 5x5, stride 2, padding 2, ReLU activation, and a fully connected layer with input size 64*8*8 and output size 10. Training was done on the CIFAR-10 dataset with 50k training examples and 10k testing examples. Testing was also done on a network with randomized labels. No data augmentation was performed, but data normalization was done. Results were similar for ResNet-56 with testing errors ranging from 7% to 9%. Performance in classification was the same for normalized and unnormalized networks. The normalization in ResNet was done by using the norm of the output of each network on one example from CIFAR-10. The networks were trained for 200 epochs with learning rate = 0.01 for the first 100 epochs and learning rate = 0.001 for the second 100 epochs. SGD was used with batch size of 128 and data augmentation. Even with the same architecture and training parameters, networks usually have different test performances due to convergence to different minima in the loss landscape. The differences in test performance of networks trained with the same architecture and parameters are magnified by using random pretraining and different weight initialization techniques. This results in varied test performance on CIFAR-10 and CIFAR-100 datasets. The linear relationship between train loss and test loss for normalized networks is robust under various conditions: independent of initialization method, network architecture, and dataset. Normalization is also independent of training loss, as shown by the consistent linear relationship between train loss and test loss. The linear relationship between train loss and test loss for normalized networks remains consistent, regardless of initialization method, network architecture, or dataset. The upper bound on classification error by the logistic loss can be formalized by considering the excess classification risk. Minimizing the logistic or cross-entropy loss implies minimizing the classification error. The capacity of a network, measured by the product norm of the layers, directly impacts test error. Experimental results on MNIST and CIFAR-100 datasets support this claim, showing a linear relationship between capacity and test loss. The section discusses replicating main results on CIFAR-100 with ResNets, showing different test performance with pretraining on random labels and various initializations. The linear relationship between capacity and test loss holds after normalization, regardless of training method. The term generalization is used to describe the difference offset. The technical definition of \"generalization\" requires the offset between training and test losses to decrease with increasing N. Rademacher complexity decreases as N increases, leading to a decrease in |L(f) - L(f)|. Margin bounds for classification relate to the expected classification error and empirical loss of a surrogate loss. The margin bound relates to the classification error being bounded by the value of the margin on support vectors. Test classification error increases with the inverse of the margin, consistent with the margin bound. Cross-entropy loss values are close to chance for convnets but not for ResNets due to most outputs being close to zero. The model described is a 3-layer convolutional ReLU network with specific filter sizes and no pooling. The average values and standard deviations of the most likely class before and after normalization are provided, showing significant differences in predicted classes. The final layer is fully connected; only the first layer has biases. The network is overparametrized with 188,810 parameters (compared to 50,000 training examples) in a 5-layer convolutional ReLU network with filter sizes of 32, 64, 64, 128. Batch-normalization is used during training."
}