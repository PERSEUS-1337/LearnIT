{
    "title": "rk49Mg-CW",
    "content": "Predicting the future in real-world settings, especially from raw sensory observations like images, is challenging due to the stochastic and unpredictable nature of events. Existing methods often make simplifying assumptions, leading to low-quality predictions. A new stochastic variational video prediction (SV2P) method is developed in this paper, which predicts different possible futures for each sample of latent variables, providing effective stochastic multi-frame prediction for real-world videos. The proposed SV2P method improves video predictions by incorporating stochasticity, outperforming other methods. It aims to accurately predict future frames in real-world videos, enhancing object interaction dynamics understanding. The model's open-source implementation will be available upon publication. Modeling future distributions over images is challenging due to high dimensionality and complex dynamics. Assumptions of deterministic environments are common but may not capture real-world nuances, especially in non-deterministic settings like action prediction tasks. The stochastic nature of video prediction challenges deterministic models, which may lose nuances present in real interactions. Deterministic models predict a statistic of all possible outcomes, resulting in blurry predictions. Stochastic models, like SV2P, with latent sampled from an approximated posterior, can predict correct motion within the range of possible futures. The main contribution of this paper is the SV2P method for video prediction, which predicts different plausible futures for each sample of its latent random variables. It is the first model to successfully predict multiple frames in real-world settings and supports action-conditioned predictions. SV2P is evaluated on various video datasets and emphasizes the importance of stochasticity in video prediction. The SV2P method for video prediction emphasizes the importance of stochasticity and produces improved video predictions compared to deterministic models. The project website allows viewers to see actual experiment videos, and the TensorFlow implementation will be open sourced. The model predicts different plausible futures for each sample of latent random variables, supporting action-conditioned predictions. The motion flow of dynamically masked out objects is extracted from previous frames. Different transformation-based models have been proposed to improve video prediction quality. Some models focus on producing sharper predictions by considering alternative objectives. However, deterministic outcomes assumed in video games may not hold in real-world settings with stochastic dynamics. Auto-regressive models offer sharp image predictions but have high training and inference times, limiting practical use. BID29 proposed a parallelized multi-scale algorithm to improve training and prediction time for video generation. Comparisons show that their method produces better predictions than other approaches using generative adversarial networks (GANs) and variational auto-encoders (VAEs) for stochastic prediction tasks. BID36, BID39, and BID30 use conditional VAEs for stochastic prediction tasks on synthetic datasets. Real images pose challenges due to diverse stochastic events. BID11 compared architectures for multimodal motion forecasting, while our focus is on multi-frame video prediction for complex events like collisions. Our approach is the first to demonstrate stochastic multi-frame video prediction on real world datasets. We construct a stochastic variational video prediction model using a probabilistic graphical model to explain the stochasticity in the video. Video prediction is stochastic due to latent events not observable from context frames alone. Our model introduces latent variables to explain the stochasticity in video prediction, using a conditional Gaussian representation to account for noise in the image. Training involves learning shared parameters for all time steps, while inference is done through an approximation of the posterior with an inference network. The model uses a network to output parameters of a conditionally Gaussian distribution. Training involves optimizing the variational lower bound, similar to a variational autoencoder. The approximated posterior is conditioned on all frames, including future frames. During training, latent variables are sampled from the assumed prior, while at test time they are sampled from the prior for a smoothing-like inference process. Conditioning the inference network on future frames is crucial for recovering latent variables that explain video variability. Filtering-like inference could improve accuracy by inferring latent variables based on conditioning frames, but it is not used during training to incentivize the forward prediction network. The forward prediction network utilizes latent variables for predicting future frames, which must contain unique information not present in context frames. Introducing a time-variant latent variable allows for a more flexible generative model, with inference approximated by a shared parameter model. Sampling frequency differences between the two formulations impact the model's performance. The time-variant latent variable in the generative model allows for better generalization beyond T by sampling every frame, compared to the time-invariant version where sampling occurs once per video. This approach enables accurate sampling of latent variables from p(z) without encoding all video events in one vector. The model's performance is empirically compared in Section 5.2. The proposed training procedure for the BAIR robot pushing dataset results in lower error compared to na\u00efve training. The generative model is modified to be conditioned on action vector at, decreasing future variability. The model can predict stochastic outcomes within a narrower range of possibilities using a deep convolutional neural network for approximating the posterior distribution. The inference network uses a diagonal Gaussian distribution to output mean and standard deviation parameters for the approximated posterior. The latent vector is formed by pairwise independent entries in response maps, sampled using Equation 1. The CDNA architecture predicts the next frame by modeling object motions and merging predictions via masking. The CDNA architecture predicts future frames by modeling object motions and merging predictions via masking. The model is trained in three phases: training the generative network, training the inference network, and end-to-end training to prevent the model from ignoring latent variables. During the training phases, the inference network estimates the approximate posterior without penalization for diverging from the prior. The model shows low reconstruction error initially but is not usable at test time. Adding the KL-loss in the final phase results in a drop in KL-divergence and an increase in reconstruction error, indicating successful training for effective stochastic prediction. Gradual transition from the second phase to the third is facilitated by slowly increasing the KL-loss multiplier. The training procedure for the model involves balancing latent channel capacity and independence constraints with reconstruction accuracy. The model shows stability and consistently converges to desired parameters. A toy video dataset with intentionally stochastic motion highlights the importance of stochasticity in video prediction. The dataset consists of four-frame videos with random shapes that move in various directions. Models BID10 and SV2P are trained to predict future frames based on the initial frame. BID10 predicts the average outcome, while SV2P predicts different possible futures for each latent variable sample. In experiments, SV2P predicts plausible future videos within a specific range. However, overlapping posterior distributions in latent space can lead to ambiguous predictions. The inference network ensures correct parameter estimation for generating accurate outcomes. SV2P is evaluated on real-world video datasets by comparing it to CDNA and a baseline model. It is also compared to VPN, a faster implementation, but still time-consuming. Results are shown for sampling the latent once per video and once per frame. Videos of the results can be viewed at https://goo.gl/iywUHc. SV2P is evaluated on real-world video datasets including the BAIR robot pushing dataset BID8 and Human3.6M BID18. The dataset BID8 contains action-conditioned videos collected by a Sawyer robotic arm pushing objects with unpredictable movements. The Human3.6M BID18 dataset involves humans and animals. Videos of the results can be viewed at https://goo.gl/iywUHc. To study human motion prediction, the Human3.6M dataset is used, consisting of actors performing various actions in a room. The dataset includes videos of humans walking, talking on the phone, and more. Additionally, the robotic pushing prediction dataset BID10 is utilized to compare SV2P with video pixel networks (VPNs) for stochastic prediction methods. VPNs have shown promising results on this dataset in previous studies. In contrast to VPNs, our method includes latent stochastic variables for random events and does not rely on an expensive auto-regressive architecture. Models are trained to predict the next ten frames based on the first two, similar to the BAIR robot pushing dataset. SV2P predicts highly stochastic videos with only three samples needed for higher quality outcomes compared to BID10. In our quantitative evaluation, we assess if our stochastic model can accurately predict a range of possible futures, including the true outcome. While models with more stochasticity may not perform better on standard metrics like PSNR and SSIM, evaluating the best sample from multiple random priors can indicate if uncertain futures contain the true outcome. Increasing the number of predicted futures improves the likelihood of obtaining a sample with higher PSNR compared to the ground truth. In the quantitative evaluation, the stochastic model aims to predict various possible futures, including the true outcome. Increasing the number of predicted futures enhances the chances of obtaining a sample with higher PSNR compared to the ground truth. SV2P outperforms other baselines by generating higher quality videos with higher PSNR and SSIM, especially with time-varying latent sampling. The time-invariant latent collapses after training, but a time-variant latent variable provides stability. The time-invariant model performs better in the Human3.6M dataset due to the consistent latent event. Visual examination of qualitative results highlights differences in predictions by different models. In FIG9, different models show varied predictions for videos in the BAIR robot pushing dataset. BID10 blurs the arm in action-free settings, while SV2P predicts coherent arm movements within valid outcomes. The proposed SV2P model can predict coherent arm movements and interactions with objects in videos, showing more meaningful semantics compared to BID10. In action-conditioned settings, SV2P captures subtle variations in interactions between the arm and objects due to uncertainty in depth, friction, and mass. SV2P captures subtle variations in object movements and generates sharper outputs compared to BID10. The model struggles with perfect reconstruction of future frames due to the compression of information in the posterior distribution. The results compare favorably to single-image VAEs, as shown in Figure 10. In the absence of actions, BID10 separates foreground from background but struggles with accurate predictions. SV2P predicts various outcomes and adjusts the actor accordingly. PSNR and SSIM may not always indicate better predictions, as lower values can still result in high-quality and interesting outputs. Pixel-wise metrics like PSNR and SSIM may not be ideal for semantic evaluation of predictions. The study compares the quality of predicted frames using an object detector's confidence, showing SV2P outperforms BID10 in detecting humans. Results on the Google robot pushing dataset also demonstrate SV2P's superior predictions over VPNs. The best-of-100 metric indicates that SV2P produces more coherent and higher-quality predictions compared to VPNs. The study introduces stochastic variational video prediction (SV2P) as a method for multi-step video prediction based on variational inference. It includes an effective stochastic prediction method with latent variables, a network architecture suitable for natural videos, and a stable optimization training procedure. The method was evaluated on real-world datasets in action-conditioned and action-free settings, showing the importance of stochasticity in video prediction. The study introduces stochastic variational video prediction (SV2P) as a method for multi-step video prediction, emphasizing the importance of stochasticity. Results show higher quality predictions compared to other baselines. Possible improvements include incorporating spatial correlations into the prior and implementing a time-variant posterior approximation. Future research directions include studying how stochastic predictions can be utilized for model-based reinforcement learning in real-world applications, particularly in safety-critical settings like robotics. The proposed method, SV2P, predicts sharper frames and interaction dynamics between objects and movements, showing superior results compared to baseline methods. SV2P predicts different outcomes based on latent input, showing various results such as walking, stopping, and spinning. It outperforms BID10 in generating sharper images with less noise. Training details include network architectures and hyper-parameters found in TAB1. The inference network is disabled in the initial training step. During training, latent values are sampled from N (0, I) in the first step, and from the approximated posterior q \u03c6 (z|x 0:T ) in the second step. The inference network approximates log(\u03c3) for numerical stability. \u03b2 is increased linearly from its starting value to its end value to transition from Step 2 to Step 3."
}