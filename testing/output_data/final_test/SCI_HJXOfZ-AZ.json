{
    "title": "HJXOfZ-AZ",
    "content": "According to parallel distributed processing (PDP) theory, neural networks (NN) learn distributed representations. Recent research has shown the occasional emergence of local codes in artificial and biological neural networks. A systematic survey found that the number of local codes in a feed-forward neural network follows a distribution across hidden layer neurons, influenced by input data size, examples presented, and input data sparsity. Using a 1-hot output code reduces the number of local codes on the hidden layer. The findings suggest that localist coding can emerge from feed-forward PDP networks, indicating resilience to noise. Local codes should not be disregarded, as they have been studied by psychologists and researchers in neural network models. Recent evidence from neuroscience and modern artificial neural networks suggests the existence of interpretable, local codes. Neurons in the hippocampus and cortex have been found to encode information selectively, allowing for quick learning without forgetting and encoding multiple items in short-term memory. Single cells have been observed to fire frequently in response to specific stimuli, indicating that individual neurons can be interpreted. This challenges the belief that artificial neural network neurons are not interpretable. Recent evidence from neuroscience and modern artificial neural networks suggests that individual neurons can be interpreted as localist codes. Studies have shown that PDP models and deep networks learn selective codes, with examples of feature detectors for specific stimuli like creases or faces. Understanding the conditions in which simple networks learn selective units could provide further insights. In this study, the focus is on investigating the emergence of local codes (LCs) in simple feed-forward neural networks. The research aims to understand what factors inhibit or promote the development of LCs by designing input and output data with specific properties. The experiments are structured to model layers within deep neural networks, transitioning input data from pixel space to feature space. The interpretability of neurons is considered based on their activation states. Neurons are considered interpretable if their activation state provides correct information about input classification. Selective hidden layer neurons encode category presence, with some being selectively on or off. Energy is used by biological neurons for encoding, while neural networks have no energy cost for selective off units. In neural networks, selective off units have been observed, indicating selectivity between activations for different categories. Selectivity increases with training but does not change once the network achieves 100% accuracy. The grandmother cell hypothesis suggests that a neuron may respond selectively to certain stimuli, but it is not possible to test every possible stimulus. In neural networks, selectivity between activations for different categories is observed, and it increases with training but remains constant once the network reaches 100% accuracy. In this work, a simple pattern classification task was chosen over image recognition to test a neural network with all possible patterns. Data input to the network is represented as a code with codewords, where the size of the code is related to the number of codewords in the training set. The codeword length is 500 bits, using a binary alphabet with the weight of a codeword defined by the number of '1's. The text discusses interpretable local codes found in a distributed network, showing examples of selectively on and off units with clear separations in activations for different classes. The process of creating a set of classes with structural similarity is also explained using prototype blocks. The text explains the creation of prototype blocks to code for different classes by using a Hamming distance to ensure they span input-data space. Random vectors are then used to generate members of each class clustered around the prototype vector. This process helps in creating a more realistic dataset by perturbing the prototypes. The method involves perturbing prototype blocks to create codewords with known invariant bits. The sparseness of a vector is defined as the fraction of '1's. Neural networks used are three-layer feed-forward networks with 500 input nodes. Local codes were unlikely to appear in the input codes. The neural networks used in the study are three-layer feed-forward networks with 500 input neurons, hidden layer neurons, and either 50 or 10 output neurons. Sigmoidal activation functions were primarily used, with ReLU neurons also tested. The networks achieved 100% accuracy on the task, with outputs interpreted as one or zero based on thresholds. The networks were implemented in Keras with a Tensorflow backend. The neural networks used in the study were trained in Keras with a Tensorflow backend on a Titan X Nvidia GPU. Each data point represents an average of at least 10 trained networks over 45,000 epochs. Neurons with selectivity above 0.05 were considered local codes. The networks had 500 input neurons, hidden layers, and either 50 or 10 output neurons with sigmoidal activation functions. Class prototypes were created for building random codes with known properties. Class prototypes (P1, P2, and P3) with length LP and weight four are created with a sparseness number SP of 1/3. Random vectors (Rx) with length LR, weight WR of two, and sparseness number SR of 1/4 are generated. A new codeword is assembled by choosing a prototype (e.g., P2) and applying 'perturbation' errors. Random vectors are then added to the modified prototype to create an input 'code' with nx codewords of length Lx, where nx = nR and Lx = LP. If the decay values are low, members are created. The number of interpretable local codes in neural networks is not large, usually between 0-10% of the hidden layer. The size of the hidden layer tunes the number of local codes, with a peak at n HLN = 1000 for the standard data set. The percentage of HLNs that are local codes peaks at n HLN = 500. The number of interpretable local codes in neural networks is not large, usually between 0-10% of the hidden layer. More local codes emerge with fewer examples per class, suggesting that it is more efficient to learn a short-cut for that class. The peak shifts to the left with n x = 1000, possibly due to more different types of solutions. Sparseness affects the emergence of local codes, with very low sparseness leading to many more local codes. The range of Hamming distances between members of the same class does not overlap. The Hamming distances between members of the same class do not overlap, and changing the sparseness affects the emergence of local codes in neural networks. Switching to ReLU neurons and 1-hot output encoding also have an impact on the network's performance. Switching to ReLU neurons and using 1-hot output encoding impact the network's performance by increasing the number of local codes.ReLU neurons train quicker and produce more local codes, with a peak shifted to a lower n HLN. Distributed output codes are chosen to prevent artificial induction of local codes. The use of 1-hot output encoding can artificially induce local codes in the hidden layer of the network. However, only a small number of local codes are needed, and emergent local codes are unlikely to be found in the penultimate layer of deep networks. Perturbing the prototype part of the code word decreases the drive to learn local codes. The existence of local codes in a class is predicated on shared invariant features. Decreasing the number of local codes is plotted against perturbation rate, showing that the weight of the prototype block is still twice that of random blocks. The weight of the prototype block is twice that of random blocks, indicating no drive to learn local codes when there is no common factor between categories. Local codes are unlikely to be found at the bottom of deep neural networks, but may emerge in categories with shared features. This raises the question of whether more local codes exist in similar categories compared to random ones. The existence of emergent local codes in neural networks trained on random data was explored. Dropout, a common training technique, was used to prevent overfitting and was found to also introduce training noise. Different dropout values were applied, and results showed varying numbers of local codes. This raises the question of whether more local codes exist in similar categories compared to random ones. The use of dropout in neural network training introduces training noise and leads to varying numbers of local codes. Dropout percentages between 20-50% are commonly used, with higher dropout values resulting in a wider range of solutions and an increase in local codes, suggesting localized encoding offers protection against noise. The use of dropout in neural network training introduces training noise and varying numbers of local codes. Higher dropout values lead to more local codes, suggesting localized encoding offers protection against noise and uncertainty. Increasing dropout increases the number of local codes, providing resilience in noisy networks. The presence of interpretable localised encodings in the hidden layer of neural networks suggests that localized encoding offers advantages against noise. The number of local codes is related to the computing capacity of the network and the complexity of the problem, potentially enhancing the network's computing power. The hidden layer size of neural networks affects the need for local codes. Local codes require more training effort but offer efficient use of capacity. The number of local codes influences shared invariants within categories. Dropout increases the number and range of local codes. Multiple peaks in dropout data suggest different qualitative approaches for solving problems. The data presented here shows that tuning neural network parameters can lead to different distributions of local codes. Simple networks may not fully represent deep neural networks, as the quality of data passed between layers could vary. Preliminary experiments with feed-forward neural networks on standard datasets also reveal the emergence of local codes when there are 'short-cuts' in the data. This suggests that local codes can be seen under dropout conditions with distributed input and output. The observations suggest that local codes may be found in the middle and higher layers of a deep network, not in the penultimate layer where 1-hot output could inhibit them or in the early layers where invariant features have not yet been identified. It is questioned whether local codes could be used diagnostically to indicate generalization or over-training. Learning relevant invariant features could aid in generalization and classification, while learning irrelevant ones could hinder it. Learning relevant invariant features aids in generalization and classification, while learning irrelevant ones hinders it. Discriminating between selective neurons requires careful consideration of reasonable data for testing selectivity."
}