{
    "title": "ryxItJn4FH",
    "content": "The problem of unsupervised learning of a low dimensional, interpretable, latent state of a video with a moving object is addressed by discarding Markov structure and using Gaussian Process Prior Variational Autoencoder. Experiments on a synthetic dataset show the model reconstructs smooth dynamics without the need for beta-annealing or freeze-thaw of training parameters, trained purely end-to-end on the evidence lower bound objective. Training is conducted end-to-end on the evidence lower bound objective without the need for application-specific tricks. The focus is on unsupervised learning of a low-dimensional, interpretable latent state in videos with moving objects, departing from traditional Markov structure approaches. Inspired by Gaussian process dynamical models, the Gaussian Process Prior Variational Autoencoder is utilized to reconstruct smooth dynamics in a synthetic dataset. The Gaussian Process Prior Variational Autoencoder (GPP-VAE) is repurposed for learning interpretable latent dynamics without the need for specific training tricks. It reconstructs smooth dynamics in a synthetic dataset and can learn intermediate latent representations from features to images in a supervised manner. The Gaussian Process Prior Variational Autoencoder (GPP-VAE) is repurposed for learning interpretable latent dynamics without specific training tricks. It reconstructs smooth dynamics in a synthetic dataset and can learn intermediate latent representations from features to images in a supervised manner. The latent representation is unsupervised, aiming to learn an interpretable 2D latent time-series of object coordinates for each frame using a Gaussian process prior over time. The Gaussian Process is defined by mean and covariance functions. A generative model uses Bernoulli distributions over pixels. Kernels are assumed known, and the goal is to learn x and y from a given video. The true posterior distribution P[x 1:T , y 1:T |v 1:T ] cannot be analytically normalized due to the terms B(v t |p \u03b8 (x t , y t )). A variational approximation is proposed to handle fast inference at test time and variable length videos, replacing troublesome terms with Gaussian densities q * \u03c6 (x t , y t |v t). The new Gaussian factors q * \u03c6 (x t , y t |v t) are conjugate to the GP prior, enabling normalization. Parameterized by a recognition network, these factors approximate the true posterior P[x 1:T , y 1:T |v 1:T] using observations and noise. The standard GP equations yield means and variances for regression. The text discusses the independence of x and y as two 1-D Gaussian process regression models. It explains how the latent dimensions are Gaussian processes over time, with Z x (v 1:T ) representing the marginal likelihood of the x GP. The kernels used are squared exponential with hyperparameters l x = l y = 5. These hyperparameters represent the time scale of changes in x and y positions and can be learned from prior knowledge of object movement in videos. The text discusses using different kernels for latent trajectories in Gaussian process regression models. Various kernels can be used based on prior beliefs about the dataset or physical system. Training the neural network involves maximizing the evidence lower bound. The reconstruction term is evaluated using the reparameterization trick, while the Kullback-Leibler divergence is used to compare the GP prior and the inference model. The text discusses training a neural network to maximize the evidence lower bound by evaluating the reconstruction term using the reparameterization trick and comparing the GP prior and the inference model using Kullback-Leibler divergence. A controlled dataset is synthesized to test the ability to learn latent dynamics, with videos generated and rendered onto a binary canvas. A GPP-VAE model is used for inference, while a standard Variational Autoencoder serves as a baseline for model evaluation. The GPP-VAE model is evaluated for how well it recovers the ground truth latent space compared to the generated images. The VAE and GPP-VAE show differences in how they encode regular patterns of images into the latent space. The VAE learns a distorted and discontinuous mapping, while the GPP-VAE learns a continuous mapping with mild distortion. Both methods achieve near-perfect reconstruction of videos. Source code is available at https://github.com/scrambledpie/GPVAE/. The study presents a Gaussian Process Prior within a VAE for learning smooth latent dynamics. The GPP-VAE latent space is more coherent compared to the VAE. Time correlation is not present in the images, so no smoothing is applied. The model is evaluated on a toy dataset. The study considers a toy dataset and a dynamics model to fit the data, aiming to apply the model in less controlled settings and compare with advanced baselines. The KalmanVariational Autoencoder learned dynamics using an LSTM, requiring freeze-thaw of parameters and re-weighting of terms. Extensions to the model involve multiple objects with parabolic motion and training tricks like \u03b2 annealing. Independent factorised Gaussian process priors are assumed for horizontal and vertical positions, with the recognition network returning factorised Gaussian densities for the approximate posterior. The log marginal likelihood for a single video in Gaussian process regression involves input-output pairs from the recognition network with noise variance for each observation. Matrix inversion via Cholesky decomposition is used, with cubic cost in the number of observed frames. Approximate posterior mean and variance can be computed using standard Gaussian process regression equations. The approximate posterior variance for a single point in Gaussian process regression is given by \u03c3^2x(t|v1:T) = kx(t, t|v1:T). The objective function involves evaluating terms using Monte-Carlo sampling and the reparameterization trick. The KL divergence from the approximate posterior is also considered. The KL divergence from the approximate posterior to the prior is computed by substituting in the form of q(x, y|v) = P[x, y]q * (x, y|v)/Z(v). The prior term common to both generative and inference models cancels out, resulting in Equation 9. The first term of Equation 16 is a sum of univariate cross-entropies of Gaussian distributions, with the expectation over the approximate posterior. The reconstruction term and KL divergence can be implemented in modern machine learning frameworks and optimized by gradient ascent. For training data, videos of length T = 30 are generated by sampling two time series x 1:T , y 1:T \u223c N \u00b7 |0, k(T , T ). Each pair is rescaled to pixel space and rendered as a ball onto a binary canvas. The recognition network is a fully connected network with hidden layers and output nodes returning network parameters. The decoder can be implemented in any modern machine learning framework and optimized by gradient ascent. The decoder network, p \u03b8 : R 2 \u2192 [0, 1] 1024, consists of two weight matrices and two bias vectors. Training is done using the Adam optimizer with default parameters and a batch size of 35 videos for 50,000 iterations. Preliminary testing included \u03b2 annealing to stabilize learning, but a value of \u03b2 > 1 caused issues with the model learning the prior distribution. The approximate model learning the prior distribution and never recovering, posterior collapse, and training becoming numerically unstable for \u03b2 < 1. No \u03b2 annealing applied, results are with the unmodified objective. Test set of latent trajectories and their rendered counterparts maintained. Inference model used to yield posterior mean vectors, linear regression used to predict ground truth trajectories. Rotation and translation learned to minimize mean squared error over test set, used to rotate latent trajectories onto true trajectories for figures. True trajectories not used during training. A Gaussian Process Prior Variational Autoencoder is compared to Attentive Neural processes. A stochastic process is a collection of random variables over an index set, like a video generator producing random outputs as images in pixel space. At test time, the model predicts new frames based on a set of video frames and a new timestamp, treating it as a regression problem with high-dimensional output. Given the distribution of realizations and input-output pairs, the model aims to predict outputs for new inputs. The Neural process architecture allows for statistical predictions of output based on input-output pairs from multiple realisations of a generative process. It involves encoding observed points into a new representation and using a decoder to parameterise a distribution over the output. Attentive Neural processes enhance this by applying a self-attention layer and using the new input as a query for attention weights during prediction. The Neural process architecture involves encoding observed points into a new representation and using a decoder to parameterize a distribution over the output. Attentive Neural processes enhance this by applying a self-attention layer and using the new input as a query for attention weights during prediction, augmenting Neural processes with a non-parametric memory."
}