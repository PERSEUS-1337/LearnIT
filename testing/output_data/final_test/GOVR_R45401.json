{
    "title": "R45401",
    "content": "Assessing student achievement in elementary and secondary schools is crucial for informing education policy. Congressional interest extends to national and international assessment programs, including the National Assessment of Educational Progress (NAEP) and participation in international studies like Trends in International Mathematics and Science Study (TIMSS). The report provides background and context for interpreting national and international assessment scores, emphasizing the importance of considering results over time and across multiple assessments. It aims to prevent a narrow and misleading view of student achievement by describing specific assessments, recent results, and clarifying issues in score interpretation. The report discusses the differences between large-scale national and international assessments and smaller-scale state and local assessments in terms of purposes, participation requirements, sampling procedures, and score reporting. Large-scale assessments are used to highlight achievement gaps, track national progress, compare student achievement within the United States, and compare U.S. academic performance to other countries. Results are reported at national or state levels and are well suited for analyzing achievement gaps, particularly differences by race, ethnicity, socioeconomic status, disability status, and gender. Large-scale assessments, including national and state-level assessments, track U.S. progress over time. Statewide assessments may change periodically due to legislative requirements, assessment changes, and vendor assistance. In contrast, national and international assessments have remained stable, allowing for easier year-to-year comparisons. National assessments provide a broader view of educational progress dating back to the 1960s, while international assessments compare U.S. academic performance to other countries. The United States has participated in international assessments since the 1960s, comparing student performance to 30-70 countries. States are required to assess all students in statewide assessment programs, including those with disabilities and English Learners. States receiving Title I-A ESEA funding must also participate in biennial NAEP assessments. States are required to participate in biennial NAEP assessments for 4th and 8th graders, assessing a representative sample of students. Participation is voluntary for both states and students in national and international assessments. Score reporting for large-scale and smaller-scale assessments, whether statewide, national, or international, can all use scaled scores to measure student achievement. Scaled scores allow for comparisons across students, subgroups, and over time on assessments. Educational assessments often prefer scaled scores over raw scores or percent correct due to the standardization and control for student exposure to different test forms. Scaled scores are used in various assessments to compare student performance across different test forms. However, scales can vary, such as national assessments using a 0-300 scale and international assessments using a 0-1000 scale. This makes direct comparisons challenging. To provide context, performance standards are used in large-scale assessments to gauge student achievement levels. Performance standards are used in statewide, national, and international assessments to define levels of performance, such as basic, proficient, and advanced. However, each assessment may have different cut scores and definitions for each level. This means that a student who is \"proficient\" on one assessment may not be \"proficient\" on another. Performance standards in assessments vary, with some including students on the cusp of higher levels. International assessments report scores differently, often using ranks or comparing to an \"international average.\" These measures can change between assessment administrations based on participating countries. Statewide assessments report scores for individual students, while large-scale assessments report scores for groups of students with similar characteristics. Large-scale assessments use a sampling procedure and may only test a representative sample of students. Large-scale assessments are standardized tests administered nationwide or worldwide, including national and international assessments. In the US, students participate in the National Assessment of Educational Progress (NAEP) and international assessments like TIMSS, PIRLS, and PISA. The NAEP, also known as the \"Nation's Report Card,\" is a nationally representative assessment of American students' knowledge and skills in various content areas. It began in 1969, with the first assessment in 1971, authorized by the National Assessment of Educational Progress Authorization Act. The Commissioner of NCES in the U.S. Department of Education administers the NAEP, with policy set by the National Assessment Governing Board appointed by the Secretary of Education. The NAEP program evolved into two separate assessment programs in 1990, with the main NAEP program measuring reading, mathematics, science, and writing. In 1996, NAGB issued a policy statement to redesign the NAEP, splitting it into two \"unconnected\" assessment programs. The LTT NAEP assessment was proposed to track long-term trends over time. The 1996 policy statement introduced performance standards for the main NAEP assessment, including basic, proficient, and advanced levels. NAGB committed to improving these standards and recommended their continued use. The main NAEP and its subprograms use basic, proficient, and advanced performance levels. The national NAEP assesses various subject areas with samples from public and private schools. The state NAEP program assesses reading, mathematics, writing, and science, with changes due to the No Child Left Behind Act in 2001. The Title I-A funding allows states to participate in biennial NAEP assessments of reading and mathematics in 4th and 8th grades. Participation is required for states receiving funding, with a sample of schools and students selected to create a representative sample. The assessments are the same as the national NAEP, with 585,000 students participating in 2017. The TUDA program assesses reading, mathematics, writing, and science, starting in 2002 with six districts. The TUDA program assesses reading, mathematics, and science. Participation has increased over the years, with 27 districts participating in 2017. The assessments are the same as the national NAEP, with over 65,000 students taking the tests. The LTT NAEP program has been tracking long-term trends since 1969, with assessment items remaining unchanged since 1990. The LTT NAEP program tracks long-term trends in reading and mathematics for 9-, 13-, and 17-year old students. It was last administered in 2012 to 53,000 students and is scheduled for 2024. The ED provides reports and data tools for educators, researchers, and the public to explore NAEP results. The NAEP Data Explorer allows public access to customizable tables and graphics for state, district, and content area analysis. It enables basic research analyses like significance testing, gap analysis, and regression analysis. This report focuses on major trends in performance over time and recent trends, examining average scores across different achievement levels. The NAEP Data Explorer provides access to customizable tables and graphics for state, district, and content area analysis. This report discusses major trends in performance over time and recent trends, highlighting achievement levels and achievement gaps. Results from the most recent administration of the main NAEP in 2017 show significant increases in mathematics and reading performance for 4th and 8th graders since 1990 and 1992, respectively. Average scores did not change significantly compared to the 2015 administration. In 2017, there was a small improvement in math performance for 8th grade students on the NAEP assessment. The proficient level is considered mastery of challenging subject matter, with 40% of 4th graders and 34% of 8th graders scoring at or above this level. In 2017, there was a small improvement in math performance for 8th grade students on the NAEP assessment. For 4th grade reading, 37% scored proficient, similar to 2015 but higher than 1990. For 8th grade reading, 36% scored proficient, higher than 2015 and 1992. NAEP scores are reported at different percentiles to track student performance over time. Significant gains on the main NAEP assessment in recent years are driven by higher-achieving percentile groups. 8th grade students in the 75th and 90th percentile groups made gains in math and reading since 2015, while those in the 25th percentile group scored lower in math. 4th grade students in the 25th and 10th percentile groups also scored lower in math and reading since 2015. The trend in NAEP math and reading performance from the 1970s to 2012 shows gains for 9- and 13-year old students. The 2017 NAEP results show significant achievement gaps by gender, race, ethnicity, socioeconomic status, and school factors. The largest gaps are typically by race, ethnicity, and socioeconomic status, with the biggest reported gap between white students and black students in 8th grade. The largest achievement gap is between white students and black students in 8th grade mathematics, followed by students eligible and not eligible for the National Student Lunch Program. Some achievement gaps have increased since the early 1990s, with the largest increase seen between white students and Asian/Pacific Islander students in 4th grade reading and 8th grade mathematics. In 8th grade mathematics, Asian/Pacific Islander students outperformed white students in 1990, and the gap has significantly increased over time. Achievement gaps between white students and black students and white students and Hispanic students have decreased in various subjects and grades. The LTT NAEP tracks achievement gaps over time, showing that gaps have narrowed or remained unchanged. None of the measured achievement gaps in the LTT NAEP have increased significantly over time. The United States participates in international assessments like TIMSS, PIRLS, and PISA to measure student performance over time and compared to other countries. The TIMSS focuses on math and science achievement in 4th and 8th grades, aligning with school curricula. The results are used to highlight issues in large-scale assessments. For more detailed results, refer to Appendix B. The TIMSS assessment is aligned with math and science curricula in participating education systems. The United States has been participating in TIMSS every four years since 1995, with 12th grade students occasionally taking part in the TIMSS Advanced program. In 2015, around 20,250 U.S. students participated in TIMSS and 5,900 in TIMSS Advanced. The U.S. was one of over 60 education systems in TIMSS and one of 9 in TIMSS Advanced. Participation in TIMSS is voluntary, with the next administration scheduled for 2019. TIMSS assessments in the United States are coordinated by the International Association for the Evaluation of Educational Achievement (IEA). Results for TIMSS mathematics and science in 2015 show that the U.S. scored lower than some education systems, higher than others, and not significantly different from a few. In 4th grade, the United States scored significantly lower than 7 education systems, higher than 26 education systems, and not significantly different than 9 education systems. In 8th grade, the US scored significantly lower than 7 education systems, higher than 26 education systems, and not significantly different than 9 education systems. TIMSS reports results over time by achievement level for U.S. students, showing increases in achievement for 4th grade mathematics driven by average or above average groups, but not statistically significant. Performance on 8th grade mathematics increased significantly for average and above average groups, while not significant for below average groups. Science achievement in 4th and 8th grades has been generally flat since 2011, with some significant increases for students between the 25th and 75th percentiles since 2007. Source: U.S. Department of Education, National Center for Education Statistics. PIRLS assesses reading literacy in 4th grade. The PIRLS assessment evaluates reading literacy in 4th grade, focusing on comprehension, reading behavior, and attitudes. It assesses purposes for reading, such as literacy experience and acquiring information. The United States has participated in PIRLS every five years since 2001, with the next assessment scheduled for 2021. In 2016, the U.S. also took part in ePIRLS, a computer-based assessment of online reading skills. About 4,500 students participated in PIRLS, and an additional 4,000 in ePIRLS. The United States participated in PIRLS and ePIRLS assessments, with 4,500 students in PIRLS and 4,000 in ePIRLS. Participation is voluntary, conducted under international assessment activities. Results are reported separately, with the U.S. scoring lower than some education systems in PIRLS and ePIRLS. The PIRLS and PISA assessments measure student achievement in reading, math, and science. PIRLS reports U.S. student performance levels over time, showing relatively flat results from 2001 to 2016. PISA focuses on real-life context problems to assess students' preparation for life, with the U.S. participating every three years since 2000. In 2015, 6,000 U.S. students participated in PISA, one of 72 countries involved. PISA 2018 results are expected in December 2019. The assessment is overseen by the OECD and reports on reading, math, and science literacy. In 2015, the United States scored differently in reading, math, and science literacy compared to other education systems. PISA does not track progress over time for different achievement levels but does track average performance changes over time. The average score changes for U.S. students in mathematics, reading, and science literacy from 2012 to 2015 were analyzed. The results showed a decrease in math scores compared to previous years, while reading and science scores remained relatively stable. Interpreting national and international assessment results can be challenging due to the large volume of data presented in reports. The report provides an overview of U.S. students' achievement in various assessments over time. Interpreting national and international assessment results can be challenging due to the large volume of data. Statistical significance is crucial when ranking states or countries in assessments. Statistical significance is important in interpreting assessment results. The United States scored lower than 10 education systems, higher than 34, and not significantly different from 9. Ranking average scores may not reflect statistically insignificant differences. Statistical significance indicates a change unlikely due to chance, but may not always signify meaningful change. Statistical significance is influenced by factors like sample size. Larger samples make small changes in assessment scores more likely to be statistically significant. Educational significance is subjective and depends on the context of the results. Statistical significance cannot determine the magnitude of the difference and whether it is of educational significance. For example, in NAEP results, there is a significant gap between male and female students in 8th grade math, and between white and black students. Some argue that statistical significance can be misleading for policy decisions as it may not warrant changes. Educational significance is subjective and difficult to define, with researchers using effect size to measure it. When new national and international assessment results are released, there is a tendency to focus on a single assessment or a single result. However, examining differences in results across assessments and trends over time can provide a more meaningful context for interpretation. For example, PISA results show a statistically significant decrease in mathematics literacy scores from 2012 to 2015, indicating a potential decline in mathematics achievement among 15-year-old U.S. students. U.S. students showed conflicting results in mathematics performance on different assessments. While there was a decrease in PISA scores, there were statistically significant gains on TIMSS from 2011 to 2015. It is important to consider these results together as a body of evidence and not as isolated data points. Possible reasons for the discrepancies include differences in the content assessed by each test and the alignment of U.S. curriculum with the assessments. The 2015 PISA scores did not significantly decrease compared to 2009, 2003, or 2006. Long-term trends show no significant decrease since the initial PISA administration. Single assessment reports on U.S. student achievement should be interpreted cautiously as they may not generalize across assessments. International assessments results are based on representative samples of students, but there are significant differences in student characteristics within countries. Socioeconomic inequality, such as the broader income distribution in the United States, can impact the interpretation of assessment results. Some researchers argue that social class inequality, largely determined by income, influences international assessment results. Students from lower-income families perform worse than those from higher-income families in every country analyzed. The relative performance of U.S. students may be better than it seems due to the higher number of lower-income families in the U.S. If U.S. students had a similar income distribution to other countries, their reading scores would be higher and math scores similar. Researchers suggest that examining trends for students at varying income distributions over time would be more useful than comparing average scores across countries. U.S. students participate in various assessments, including statewide and NAEP assessments, raising questions about comparability. International assessments also show overlap in content, timing, and grade levels assessed. The curr_chunk discusses the comparability of NAEP and international assessments like TIMSS and PIRLS in measuring 4th and 8th-grade achievement in reading and mathematics. It highlights the importance of alignment studies and the usefulness of making comparisons across large-scale assessments. The similarities in scaled scores and performance levels may raise questions about redundancy. The curr_chunk discusses the differences between NAEP and statewide assessments in measuring mathematics and reading achievement, emphasizing the alignment of content standards and performance definitions. NAEP's content is determined by various stakeholders and not aligned to specific standards. NAEP is not aligned to specific content standards, unlike statewide assessments which are aligned with each state's standards. Recent studies have shown strong alignment between NAEP and the common core state standards (CCSS) in mathematics for 4th and 8th grade students. Other investigations have looked at alignment with CCSS English language arts standards but did not determine a specific degree of alignment. The CCSS English language arts standards were examined for alignment with NAEP, but the degree of alignment was not determined. Many states do not use CCSS or use a modified version. It is unclear how well NAEP aligns with state content standards in math and reading. NAEP and statewide assessments use different scales, making comparisons challenging. NAEP scores are on a 0-500 scale, while statewide assessments use specific scales. The ACT Aspire, PARCC, and SBAC are state assessments with different scales. Comparing scaled scores between NAEP and state assessments is not valid. Performance standards also differ between the assessments. Performance standards for assessments like NAEP, PARCC, and SBAC are expressed in terms of cut scores such as basic, proficient, and advanced. However, these standards are not comparable across assessments, as each assessment uses its own specific standard-setting process. For example, \"met expectations\" on PARCC may not equate to \"proficient\" on NAEP. The difficulty in comparing performance standards between state assessments and NAEP is evident, especially when states use terminology like \"proficient\" differently. While NAEP includes real-world application and analytical skills in its definition of \"proficient,\" states may define it solely based on grade-level content knowledge. This discrepancy makes it challenging to align state standards with NAEP, as demonstrated in an NCES alignment study. The NCES released an alignment study mapping state performance standards to the NAEP scale, showing that most \"proficient\" state standards align with the NAEP \"basic\" level. This makes comparing state assessments to NAEP challenging, as the meaning of \"proficient\" varies between assessments. The number of students deemed \"proficient\" on NAEP is likely lower than on state assessments due to this discrepancy. The NAEP and state assessments use different frameworks and cut scores to define proficiency levels in reading and math for 4th and 8th graders. Comparing NAEP to international assessments like TIMSS and PISA also requires considering scale, performance standards, and alignment issues. The differences between NAEP and international assessments include scale, performance standards, alignment, target populations, education systems, voluntary student participation, and measurement precision. NAEP and international assessments use different scales and performance standards, making direct comparisons impossible. The assessment frameworks for NAEP and international assessments also differ, affecting the alignment of the assessments. Each assessment was designed for a unique purpose and uses a distinct framework to measure achievement. The differences between NAEP and international assessments include scale, performance standards, alignment, target populations, education systems, voluntary student participation, and measurement precision. NAEP was developed with national interests in mind, while international assessments were developed collaboratively with other countries. NAEP and TIMSS focus on \"school-based learning\" and measure skills like \"knowing, applying, and reasoning.\" PISA measures real-world learning, drawing from both school curricula and learning outside of school. PISA measures mathematics skills focusing on \"reproduction, connections, and reflection,\" while NAEP focuses more on school-based learning. PIRLS and PISA focus on the context and purposes of reading. NAEP and TIMSS sample students by grade, while PISA samples by age, with most 15-year-olds in 10th or 11th grade in the US. Comparisons between assessments may be less appropriate due to different sampling methods. Different assessments have different participating education systems, including countries and sub-national jurisdictions like provinces or states. Governance varies across education systems, with many countries having a more centralized administration than the United States. This can impact the implications of results and the ability to implement system-wide changes based on international assessment results. International assessments vary in how they treat scores of sub-national jurisdictions, making comparisons across time challenging. The average scores for PISA and PIRLS are based on different sets of countries, and the participation of education systems can change from year to year. National and international assessments are voluntary at the individual student level. The participation of students in large-scale assessments can lead to selection bias if certain groups choose not to participate. Exclusion of students with disabilities and English learners can also affect the representativeness of the sample. The Education Department provides exclusion data for these students by state for the NAEP, with exclusion rates varying widely. In 2011, exclusion rates for education systems in TIMSS ranged from 0% to 23%, potentially leading to selection bias and unreliable results. National and international assessments vary in precision, with sample size playing a key role in detecting small changes in performance. International assessments typically sample 5,000 to 25,000 U.S. students per administration. NAEP samples hundreds of thousands of students, allowing for more precise measurement of student performance compared to international assessments. The differences in precision can result in discrepancies in detecting progress or changes in achievement gaps between the two types of assessments. International assessments may not have large enough samples to detect significant increases in student performance or changes in achievement gaps, especially among minority groups, as effectively as NAEP. The NAEP provides more precise measurement of student performance compared to international assessments, making it easier to detect changes in achievement gaps. International assessments may not have large enough samples to detect significant increases in student performance or changes in achievement gaps, especially among minority groups. Each assessment is developed for its own purpose and may highlight different aspects of U.S. students' achievement. Students already participate in various state and local assessments, including those required under ESEA Title I-A. State assessments in reading, mathematics, and science required under ESEA Title I-A must align with state standards. While each state administers its own assessments, the use of different standards makes it challenging to compare student achievement across states. NAEP provides nationally representative data for each state, offering a more precise measurement of student performance compared to international assessments. NAEP provides nationally representative data for each state and large urban districts, allowing comparisons of student achievement across the United States. It enables benchmarking of state performance and informs educational policy and practice by analyzing student subgroup performance. NAEP does not provide data on how students in the United States compare to those in other countries. Participation in international large-scale assessments allows the United States to compare its performance with other countries based on external standards. These assessments provide additional insights into student achievement and academic trends, complementing other assessments. Reasons for participating include benchmarking state standards, tracking educational progress globally, and understanding performance expectations. Data collected also includes information on school environments and instruction. By participating in international assessments, researchers can establish performance expectations, collect information on school environments and instruction, and compare student performance across different countries. This data can help generate hypotheses about American secondary schooling and identify effective policies and practices that may improve student achievement in the United States. The United States participates in international assessments to gather data on school environments and student performance across countries. However, the use of assessment results in shaping education policy is limited, and the relationship between student achievement on these assessments and economic prosperity is unclear. National and international assessment results provide a snapshot of education conditions and tracking results over time can show educational progress. The United States participates in international assessments to gather data on school environments and student performance across countries. However, the use of assessment results in shaping education policy is limited. U.S. students' achievement has significantly decreased over the last two administrations of PISA, which may signal a problem in elementary and secondary education policies and practices. The results may not be particularly helpful in identifying policies that may increase student achievement or aid the United States in meeting other educational goals. One way policymakers may address a decrease in student achievement is by adopting policies from countries that score well on international assessments. This raises questions about whether it is beneficial for the United States to adopt policies from countries with different education systems and student demographics. The feasibility of adopting educational policies from high-performing countries must be considered, taking into account the differences in student demographics and education systems. For instance, Finland, once a top performer on international assessments, saw a decline in performance in recent years, raising questions about the effectiveness of mirroring their policies in the United States. The decline in Finland's performance on international assessments in recent years raises questions about the effectiveness of adopting their policies in the United States. International assessments do not evaluate specific policies or practices within countries, making it difficult to determine the reasons for changes in achievement levels. The United States has a decentralized education system, with state and local authorities making policy decisions. While national and international assessments may highlight issues, state and local assessments aligned with state standards are better suited for evaluating students and schools. State and local assessments aligned with state standards are better suited for addressing teaching and learning issues at the state level compared to national and international assessments. There is ongoing debate about the impact of student achievement on a country's economic prosperity, with analyses attempting to link performance on international assessments to national wealth and prosperity indicators. One analysis in 2007 by Keith Baker examined the relationship between international test scores and national success indicators. Scores from 11 countries in the First International Mathematics Study were used to predict wealth, growth rate, productivity, quality of life, livability, democracy, and creativity 30 years later. The study found that higher scores on international assessments were associated with decreases in most indicators of national success, except for creativity measured by the number of patents issued. Baker analyzed the relationship between international test scores and national success indicators, finding that as test scores increased, creativity indicators also increased. Nations at the PISA average generally outperformed those scoring well above or below average. Hanushek and Woessmann's analysis predicted the impact of achievement on economic growth using PISA scores. They concluded that focusing solely on increasing test scores may divert resources from other factors contributing to national success. The analysis focused on the relationship between international test scores and economic growth. Increasing PISA scores by 25 points in OECD countries could lead to a $115 trillion GDP gain over 80 years. If all countries reached minimal proficiency levels, GDP could increase by $200 trillion. However, the impact of test scores on economic prosperity remains unclear, and policymakers may opt to prioritize other factors for educational achievement and economic success. In the last decade, two Secretaries of Education expressed concern over U.S. performance on the PISA and called for reforms. Secretary Arne Duncan used PISA results to advocate for education policy changes under the Obama Administration, focusing on teacher recruitment, evaluation, and compensation. Secretary Betsy DeVos took a different approach. The Obama Administration focused on effective teachers, while Secretary Betsy DeVos argued for school choice policies based on PISA performance. Education leaders are concerned about U.S. student performance on international assessments, but data does not clearly indicate a policy problem or solution. NAEP and international assessments provide comparative data on student achievement not available at the state and local level. These data can be used to analyze student performance over time and confirm or contradict state and local assessment data. NAEP assessments in the United States may not directly align with state standards or curricula, limiting their value in identifying effective policies. However, they offer states a chance to compare themselves to others in the decentralized educational system. International assessments also provide benchmarking but may not pinpoint specific policies contributing to student success. The data from national and international assessments may not easily translate into effective education policies in the United States. While valuable for identifying areas in need of attention, they may have limited utility for shaping education policy approaches. For more information on NAEP, TIMSS, PIRLS, and PISA assessments, visit the provided links. For more information on NAEP and international assessments, visit the provided link. Additional CRS reports on assessment in elementary and secondary education are also available. The glossary includes acronyms such as CCSS, ED, ESEA, ESRA, ESSA, FIMS, GDP, IEA, LTT NAEP, and NAEP. The curr_chunk provides a list of acronyms related to educational assessments, including NAEP, NAGB, NCES, NCLB, OECD, PARCC, PIRLS, PISA, SBAC, TUDA, and TIMSS."
}