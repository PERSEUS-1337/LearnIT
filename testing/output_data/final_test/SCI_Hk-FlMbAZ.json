{
    "title": "Hk-FlMbAZ",
    "content": "In the adversarial-perturbation problem of neural networks, a small perturbation is applied to a correctly classified point to produce another point that is classified incorrectly. The paper proposes considering the confidence information produced by models in studying adversarial perturbations, where confident regions of a good model should be well separated. Existing robust training objectives are examined in light of this \"goodness property.\" The paper by Madry et al. proposes an embedding method to improve model robustness against adversarial attacks. Their approach aims to distinguish between good and bad points by embedding uncertain points back to confident regions. Experimental results show that their method significantly enhances the model's defense against attacks while maintaining good generalization behavior. In the adversarial perturbation problem of neural networks, researchers have noticed the vulnerability of deep neural networks to attacks that produce incorrect classifications. Despite efforts to strengthen neural networks against these attacks, there is still a gap in defending against advanced attacks. This paper proposes considering the confidence information produced by models when studying adversarial perturbations. The paper proposes considering the confidence information produced by models in studying adversarial perturbations. It suggests using a measure of confidence to determine how confident a model is about its prediction, based on the manifold assumption in unsupervised learning. The ideal model should confidently distinguish points from natural manifolds and not claim confidence in points far away from these manifolds. The paper introduces a goodness property for models, emphasizing well-separated confident regions. It evaluates existing training objectives and highlights BID8's function for encouraging model training that aligns with the goodness property. The analysis also identifies potential low-confidence points in models. Two embedding objectives are proposed to address this issue. The paper introduces the \u03b4-Most Confident Neighbor (MCN \u03b4 ) and p-Nearest Confident Neighbor (NCN p ) methods for end-to-end predictions. Empirical validation on CIFAR10 shows the effectiveness of models trained with a robustness objective. Gradient-based optimization is proposed for defense using MCN \u03b4 or NCN p against attacks like Carlini-Wagner. The paper introduces BID4 for defense against MCN \u03b4 or NCN p attacks, achieving high success rates and maintaining good generalization. It discusses prior work, proposes the goodness property, and validates the robustness of Madry et al.'s model. Embedding objectives and algorithms for low-confidence points are presented, followed by empirical validation and defense results. The susceptibility of deep neural networks to adversarial perturbations was first observed by BID17, leading to a significant amount of research on hardening neural networks against attacks. While some work focuses on improving defense methods, others aim to devise more effective attacks. However, there is still a gap in defending against sophisticated attacks like CarliniWagner attacks. One dominant defense approach involves adding a robustness component to the training objective function to enhance the model's robustness. Our method aims to improve model robustness by leveraging fundamental structures learned during training. Inspired by recent research on neural networks representing low-dimensional manifold data, we go beyond detecting adversarial attacks to compute correct predictions on adversarial examples. The network is specifically constructed to fit the data and extract intrinsic geometric structures, providing evidence that deep networks can do so. The model approximates natural manifolds well, leading to good generalization and adversarial perturbations. The final classification is determined by the neural network after the softmax layer, with confidence levels for each class. The text discusses the definitions of p-confident points and regions, distance metrics, and cross entropy in the context of model confidence and adversarial perturbations. The main argument proposes a goodness property of models. The text discusses the goodness property of models, emphasizing the importance of well-separated confident regions in a good model. It also explores the manifold assumption in unsupervised learning and the ideal characteristics of an ideal model under this assumption. The text discusses the importance of well-separated confident regions in a good model, defining (p, q, \u03b4)-separation and highlighting differences between Definition 4 and Definition 5. Definition 5 depends on the data generating distribution, while Definition 4 is solely a property of the model. Existing robust training can provide guarantees with respect to Definition 5, which depends on the data generating distribution. The objective function of BID8 encourages training a model that satisfies Definition 5 and has weak control over low-confidence but wrong predictions. BID8 uses a cross-entropy loss function in their objective. The probability of a point being p-confident on a different label vanishes as p approaches 1. This implies that p-confident points will be well separated as p increases, even if the model is only somewhat accurate. The text discusses the tight bound on the probability of bad events happening in a neural network model. By setting p = 1/2, a bound \u03b5 is derived, and if \u03c1(\u03b8) \u2264 \u03b5, then a classification network can be defined. Markov's inequality is used to establish a lower bound on a random variable X. The proof shows that the bound is tight, and the contrast between two propositions is highlighted. Contrasting Proposition 1 and 2, the text discusses the impact of confidence levels on predictions in neural networks. Proposition 1 suggests that high-confidence predictions of different classes will be well separated with increasing confidence, while Proposition 2 indicates that there may still be wrong predictions with low confidence. It is noted that controlling low-confidence points is more challenging than separating confident regions due to the nature of natural manifolds. The text discusses the challenge of handling low-confidence points in neural networks, proposing objective functions for embedding these points back into confident regions. The goal is to address the difficulty of controlling uncertain points, which can be distinguished by their low confidence levels. The text discusses objective functions for embedding low-confidence points back into confident regions in neural networks. The MCN \u03b4 objective considers a \u03b4-neighborhood for prediction, while the NCN p objective focuses on the nearest neighbor achieving a confidence threshold. The approach involves wrapping a learned model with a \"shell\" to handle points slightly outside confident regions, with options like weighted majority vote as a shell. Model shells are algorithms that wrap around an existing model to classify points, outputting a softmax layer. In this paper, factor model shells are examined, with a search factor being an algorithm that classifies points based on a model F. A factor model shell is a composition of a base model F and a search factor H, where H is used to find another feature point x = H(F, x) before applying F to produce a final prediction. This approach captures intuitions from MCN or NCN objectives, with specific computations for each objective to provide semantic defenses against wrong predictions. The factor model shells (F \u2022 MCN \u03b4) and (F \u2022 NCN p) are non-differentiable, making existing attacks ineffective. Attacks must either target the base model F and then transfer to the model shell, or use differentiable approximations to attack MCN \u03b4 and NCN p directly. Differentiable approximations or derivative-free optimization are used to attack MCN \u03b4 and NCN p directly. One concrete algorithm is presented to solve the optimization for MCN \u03b4, where the objective function is differentiable in x. This first-order assumption is natural for neural networks, allowing attacks to modify image features to increase model confidence in incorrect labels. The algorithm presented solves the optimization for MCN \u03b4 by modifying image features to increase model confidence in incorrect labels. It uses gradient-based optimization to find the optimal solution. The algorithm presented in the curr_chunk solves the NCN p objective using gradient-based optimization. It aims to separate local minima of the correct class from those of incorrect classes. The algorithm takes a feature vector x, a real parameter p, a base model F, and a gradient-based optimization algorithm O as input to solve the constrained optimization problem. The algorithm presented aims to solve the NCN p objective using gradient-based optimization. It utilizes the augmented Lagrangian method to transform the problem into a series of unconstrained optimizations, with the goal of finding a satisfying solution by adjusting the penalty function coefficient alpha. Other classic constrained optimization algorithms like interior methods can also be used to solve this problem. In this section, an empirical study is conducted on a method that focuses on optimizing convex objectives over non-convex constraints. The study evaluates the defense approach by considering the base model, the embedding procedure, and the chosen attack. Key questions include the satisfaction of the goodness property by the base model, the susceptibility of the base model to the chosen attack, and the effectiveness of the approach in improving the robustness of the base model. The study validates the robustness of the base model by using attacks from BID4 and evaluating on CIFAR10 BID6. The model from BID8 shows significant improvement in robustness compared to models without training. The base model is still susceptible to CW attacks, with accuracy as low as 45.8%, indicating room for improvement. The MCNOracleShell algorithm is used with CW attacks as embedding to further enhance robustness. The study validates the robustness of the base model using MCNOracleShell algorithm with CW attacks as embedding, resulting in CarliniWagnerShell. Embedding significantly improves model robustness, reducing attack success rate from 30% to 1.33% over 5 batches. Generalization is minimally affected, with only 3 points changing predictions from correct to wrong. However, due to model complexity, the embedding procedure is time-consuming, limiting testing to 150 correctly classified points. The study demonstrates the robustness of the base model using MCNOracleShell algorithm with CW attacks as embedding, resulting in CarliniWagnerShell. Embedding improves model robustness, reducing attack success rate from 30% to 1.33% over 5 batches. Experimental setup involves using CIFAR10 dataset with Carlini-Wager attacks for testing. The study compares a robust model trained by BID8 with a natural model, using a norm bound \u03b4 = 8.0 for perturbations. The CarliniWagnerShell implementation tweaks parameters for faster attacks as embedding, making the defense weaker. Two experiments are conducted based on probability values in the comparison method. The method is based on Proposition 1, using a set of probability values P = {p | p \u2208 [0.5, 1]}. Points are sampled in a batch K, and for each p \u2208 P, confident attacks are found in the \u03b4-neighborhood of each point (x, y) in K. The robust model shows better performance with smaller fractions of confident attacks. An evaluation of end-to-end defense is done by measuring susceptibility to CW attacks, robustness, and generalization on test data points. The study evaluates the robustness of the CarliniWagnerShell model by testing its ability to predict correctly on data points. Using a modified CW attack, the model is assessed for its performance in generating adversarial examples within a norm bound. Results show the number of successful attacks with confidence levels of at least 0.9 in batches of 30 random samples. The study evaluates the robustness of the CarliniWagnerShell model by testing its ability to predict correctly on data points. With statistics, the (p, q, \u03b4)-separation of models is estimated. The robust model in BID8 satisfies DISPLAYFORM0 with probability at least .9. Evaluating 5 batches of 30 points each, the base model is susceptible to CW attacks for 37.33% of points. After applying CarliniWagnerShell, the model remains robust on previously robust points and improves on vulnerable points. The attack success rate decreases from 37.33% to 1.33%, with an accuracy of 98%. Three data points where CarliniWagnerShell fails have low confidence. The model has low confidence for three points, making it hard to defend. The points may not be well separated on \"natural manifolds.\" One point remains robust but gives a wrong prediction, with low confidence on the original image and higher confidence on the perturbed image. The model has low confidence for three points, making it hard to defend. Some points may not be well separated on \"natural manifolds.\" Two points give wrong predictions and change after adversarial perturbation. The model has low confidence for three points, making it hard to defend. Some points may not be well separated on \"natural manifolds.\" Two points give wrong predictions and change after adversarial perturbation. The predictions for images resembling different objects are uncertain. The technical contributions include considering model confidence in studying adversarial perturbations, analyzing model goodness properties, proposing an embedding for handling low-confidence points, and validating a model by Madry et al. The final contribution of the study validates Madry et al.'s model in terms of the goodness property and shows that a good model, when wrapped with embedding, achieves good generalization and almost perfect robustness. The results suggest that adversarial perturbations can coexist with good generalization, especially when considering a manifold point of view. The adversarial perturbation problem can be resolved by combining the goodness property with embedding. A \"good model\" outputs the correct label with confidence 1 for training set data points and a uniform distribution for others. This model is robust around training points and performs 1-nearest neighbor search among them. Neural networks can make highly confident predictions on random noises, which aligns with the study's findings. Neural networks can exhibit divergent behaviors on points far from the data distribution, leading to confident predictions on random noises. The adversarial perturbation problem is not well-defined near these points. The section discusses the estimation of (p, q, \u03b4)-separation from statistics in TAB0, where event E b corresponds to a Bernoulli trial. Chebyshev's Inequality states that for independent random variables bounded in [0, 1], the mean \u00b5 falls within a certain range with high probability. This provides guarantees for specific values of \u03b1, \u03b5, and t, such as \u03b1 = .9, \u03b5 = .1, and t \u2265 250."
}