{
    "title": "SJlyta4YPS",
    "content": "Click Through Rate (CTR) prediction is crucial for online social and commerce applications. A novel model called DeepEnFM uses a Transformer encoder to align feature embeddings and generate different similarity functions for field pairs. The model, validated on Criteo and Avazu datasets, achieves state-of-the-art results. This paper focuses on predicting Click Through Rate (CTR) in online advertising and e-commerce. Feature representation is crucial for extracting patterns from training data, with handcrafted features being common until the rise of Deep Neural Networks (DNNs). Representation learning has emerged as a more effective approach for CTR prediction. Neural Networks (DNNs) have been explored for Click Through Rate (CTR) prediction, with recent focus on representation learning. While DNNs and high-order feature-based methods show performance improvement, there is a need to better understand and learn input representations to address issues like \"polysemy\" in feature embeddings. In CTR prediction, the use of Transformer encoder in NLP can help address polysemy issues in feature embeddings. This approach efficiently extracts patterns from contextual word embeddings, offering potential benefits for representation learning in CTR. The Transformer encoder, although rarely applied in CTR prediction, has shown promise in extracting high-order feature interactions. Our work proposes a novel framework to improve the encoder for CTR tasks by learning context-aware feature embeddings that capture clues from other features, solving the \"polysemy\" problem. Unlike AutoInt, we feed the encoder output to FM and use DNN for bit-wise high-order feature interactions in a parallel manner, avoiding stacking vector-wise and bit-wise interactions. The proposed framework, DeepEnFM, enhances the encoder for CTR tasks by generating contextual aligned vectors for Factorization Machine (FM) using Deep Neural Networks (DNN) for bit-wise information supplement. Inspired by DeepFM, the architecture incorporates bilinear attention and max-pooling in the encoder to capture fixed order field features in transactions. The bilinear mechanism in the Transformer encoder replaces the simple dot product in attention, improving feature embeddings for better performance. The proposed model, DeepEnFM, enhances the encoder for CTR tasks by incorporating bilinear functions in attention and max-pooling to capture field features. Experimental results on Criteo and Avazu datasets have shown the effectiveness of the model in predicting user click behavior. The CTR focuses on learning combinatory features for predicting user click behavior. Traditional methods like Logistic Regression rely on manually designed features, while Factorization Machine and other models automate this process. These models aim to capture interactions of various orders explicitly. Deep Neural Networks (DNN) have advanced in CV and NLP tasks by learning representations effectively. DNN-based CTR models use primary field embeddings or product layers with a plain DNN. However, the plain DNN treats field embeddings as a flat layer, limiting interactions between neurons. Attention mechanisms, widely used in Machine Translation and Image Captioning, offer a solution to this issue. The Transformer encoder, known for its multi-head self-attention mechanism, is being applied in CTR models to address the 'polysemy' issue. A new model called DeepEnFM is proposed to utilize the encoder for context-aware field embedding in the CTR task of predicting click behavior. This model differs from AutoInt by focusing on field embedding rather than high-order feature extraction. The training record consists of instance fields x and click behavior y. The goal is to predict the probability of user click behavior y* using a mapping function \u03a8(x*). DeepEnFM framework includes components like embedding layer, encoder, FM, DNN, and prediction layer. The workflow involves projecting input fields into a low dimensional space, updating embedding results with multi-head self-attention, and calculating a score with FM. The DeepEnFM framework includes components like embedding layer, encoder, FM, DNN, and prediction layer. The embedding layer converts input fields into low-dimensional vectors, which are then updated using multi-head self-attention. The FM calculates a score integrating explicit interactions, while the DNN learns implicit feature interactions. Finally, the prediction layer maps intermediate outputs to a final score using a sigmoid function. The encoder in the DeepEnFM framework consists of multiple layers with residual connections, aligning field embeddings based on content. Each layer includes a self-attention module and a feed-forward module. FM is used to capture feature interactions based on encoder outputs. In the DeepEnFM framework, the encoder aligns field embeddings using self-attention and feed-forward modules. FM calculates scores based on encoder outputs, while the DNN captures high-order interactions between fields at a bit-wise level. The prediction layer uses logistic regression to predict the final result, and the MHSA module describes the procedure for multi-head self-attention. The MHSA module in Transformer encoder involves calculating similarity using a scaled dot function and merging output vectors. The design principles focus on generalization, efficiency, and architecture requirements. Comparisons will be made with the DeepEnFM encoder's MHSA implementation. The MHSA module in Transformer encoder calculates similarity between feature pairs equally and merges output vectors using concatenation and linear transformation. The similarity function is personalized for each field pair query and key, with different heads extracting features from different viewpoints. The output is obtained through max-pooling to get the salient output. The MHSA module in Transformer encoder calculates similarity between feature pairs equally and merges output vectors using concatenation and linear transformation. The output is obtained through max-pooling to get the salient output. The position-wise feed-forward module maps the field embedding to a higher dimensional space. Different architectures for ablation study are explored by combining the encoder, FM, and DNN in DeepEnFM. Experiments are conducted on the Criteo dataset. In ablation study, experiments are conducted on two datasets: Criteo and Avazu. Criteo dataset contains 45 million click records with 13 dense features and 26 categorical ones. Avazu dataset contains 40 million click records with 23 fields. Hyper-parameter settings are searched using validation data, and performance is evaluated on test data using AUC. The experiment evaluates performance on test data using AUC and Logloss. The approach is implemented using TensorFlow with specific settings for optimizer, learning rate, regularization, and dropout rate. The model architecture includes batch size, embedding size, and DNN layer size. Xavier and random normal initialization methods are used. The DeepEnFM model is trained on V100 for 1 hour per training epoch. The model is compared against LR, FM, and DNN methods. The curr_chunk discusses various deep learning models for feature interaction, including DNN, IPNN, OPNN, PNN*, DeepFM, AutoInt, and DeepEnFM. These models utilize different techniques such as inner product layers, outer product layers, and interacting layers to learn high-order features. Our DeepEnFM model outperforms AutoInt and DeepFM on Criteo dataset in terms of AUC and LogLoss metrics. It also achieves the second place in AUC and the first place in LogLoss on the Avazu dataset. The improvement of DeepFM over FM highlights the contribution of implicit high-order information in the DNN part. The gap between FM and LR reflects the effectiveness of our bilinear encoder. The DeepEnFM model shows superior performance compared to AutoInt and DeepFM on Criteo and Avazu datasets. It emphasizes the importance of implicit high-order information in the DNN part and the significance of explicit second-order interaction information for the CTR task. Extensive ablation studies demonstrate the impact of different encoder positions on model performance. The DeepEnFM model, which combines DNN, FM, and encoder, outperforms other models on Criteo and Avazu datasets. It highlights the importance of explicit feature interaction learning. DeepEnFM's 'Encoder for FM' approach achieves the best results, showing that the encoder is compatible with FM. The depth of encoder layers in DeepEnFM impacts its performance positively. The performance of DeepEnFM improves with the depth of layers, with the first encoder layer showing a larger gain than the second layer. Implementing DeepEnFM with 1 head achieves the best results, as it maximizes query and key vector dimensions. The balance between performance and cost is achieved with 2 heads. AutoInt shows less influence of head number compared to DeepEnFM. AutoInt shows that the influence of head number is smaller than DeepEnFM due to its use of a scale dot product function. The proposed framework, DeepEnFM, aims to improve vector alignment through an encoder that combines bilinear attention and max-pooling methods. Extensive experiments demonstrate state-of-the-art performance on Criteo and Avazu datasets."
}