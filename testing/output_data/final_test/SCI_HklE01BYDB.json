{
    "title": "HklE01BYDB",
    "content": "Training an agent for control tasks from high-dimensional images using model-free reinforcement learning is challenging. Learning a latent representation and control policy together is crucial, but fitting a high-capacity encoder with limited rewards is inefficient. Improving sample efficiency involves learning good feature representation and using off-policy algorithms. Image reconstruction loss is key for efficient representation learning in image-based RL. An off-policy actor-critic algorithm with an auxiliary decoder achieves state-of-the-art performance on challenging control tasks. Using cameras for image-based RL in complex environments is crucial for effective control. Learning a mapping from pixels to control representation with sparse rewards is a challenge. Deep convolutional encoders can help, but require large training data. Model-free methods with pixel inputs are slow due to poor sample complexity. Adding an auxiliary task like an autoencoder with a pixel reconstruction objective can improve sample efficiency in model-free methods for Atari and DeepMind Control. Prior work has used autoencoders to learn state representations from pixels, enhancing optimization stability. Incorporating an autoencoder with a pixel reconstruction objective can enhance sample efficiency in model-free reinforcement learning. By focusing on off-policy algorithms and conducting thorough experiments, a new method was developed that successfully trains both the latent state representation and policy simultaneously in a stable and efficient manner. The paper demonstrates that adding a simple auxiliary reconstruction loss to a model-free off-policy RL algorithm achieves comparable results to state-of-the-art model-based methods on continuous control tasks. Efficient learning from high-dimensional pixel observations is crucial for model-free RL. Scaling model-free RL to complex continuous control environments has been challenging due to sparse RL signals and sample inefficiency. A PyTorch implementation of a simple method is provided as a strong baseline for researchers and practitioners. Training with auxiliary losses can help alleviate sample inefficiency in reinforcement learning, especially in high-dimensional observation spaces like pixels. Early work explored using deep autoencoders for feature learning but was limited by iterative re-training. More recent methods, like deep autoencoder pretraining on real-world robots, show promise in reducing computational complexity. However, training the linear policy separately from the autoencoder may not perform as well as end-to-end methods. Shelhamer et al. (2016) and Jaderberg et al. (2017) recommend using auxiliary losses in Atari with A3C for better performance in a multi-task setting. They suggest learning dynamics and reward to find a good representation, but this approach relies on easy-to-learn task dynamics. Higgins et al. (2017b) and Nair et al. (2018) use a beta variational autoencoder (\u03b2-VAE) to extend learning methods. The use of model-based methods in reinforcement learning, such as world models, has shown improved sample efficiency but comes with the challenge of balancing various auxiliary losses like dynamics, reward, and decoder losses. These methods are sensitive to hyperparameter settings and can be difficult to train end-to-end. Our goal is to train a model-free off-policy algorithm with auxiliary reconstruction loss in a stable manner to bridge the gap between model-based and model-free image-based RL. The Markov decision process is described by tuple S, A, P, R, \u03b3, where an agent starts in an initial state and takes actions to move through states and receive rewards. The text discusses the goal of standard RL to learn a policy that maximizes cumulative reward, with an entropy term added for exploration and robustness. It builds on Soft Actor-Critic (SAC), an off-policy actor-critic method that uses the maximum entropy framework for soft policy iteration. Soft Actor-Critic (SAC) performs soft policy iteration by evaluating a parametric soft Q-function and improving the policy by minimizing the KL divergence between the policy and a Boltzmann distribution induced by the soft Q-function. The policy is parametrized as a diagonal Gaussian for continuous action spaces, addressing the problem of partial observability in partially observable MDPs when learning from raw images. In partially observable MDPs, the agent receives high-dimensional observations instead of low-dimensional states, complicating RL. To address this, unsupervised pretraining via an image-based autoencoder is explored to learn a compact latent representation for inferring the state efficiently. The autoencoder consists of a convolutional encoder and a deconvolutional decoder, optimizing the reconstruction of the original image. The optimization involves minimizing the reconstruction objective using a variational distribution parametrized as diagonal Gaussian. The latent vector is used by an RL algorithm, such as SAC, to infer temporal statistics. Unlike model-based methods, the focus is on learning representations from the current observation to stay model-free. The exploration delves into how model-free off-policy RL can directly train from pixel observations. In exploring model-free off-policy RL training from pixel observations, a performance drop was observed when using SAC. To improve representation learning, an iterative unsupervised pretraining of an autoencoder was attempted, but found to be sub-optimal. End-to-end training of the \u03b2-VAE with the policy network was deemed ineffective. The \u03b2-VAE with the policy network was found to be unstable in training, especially with larger \u03b2 values. To address this, deterministic forms of the variational autoencoder were used, leading to the development of a new algorithm evaluated on 6 image-based continuous control tasks. The tasks have a maximum total reward of 1000 per episode and last for 1000 steps each, with image observations represented as 3 \u00d7 84 \u00d7 84 RGB renderings. The experiments involve 3 \u00d7 84 \u00d7 84 RGB renderings for image observations, with velocity and acceleration inferred from stacked frames. Hyperparameters are kept constant except for action repeat, which is adjusted for fair comparison. Evaluation is done every 10000 training observations, with 10 random seeds used for each configuration. The experiment compares SAC algorithm on pixels with model-based algorithms PlaNet and SLAC. SAC performs better on proprioceptive state. Different auxiliary reconstruction losses are introduced to close the performance gap. \u03b2-VAE is employed for iterative re-training setup. The study explores pretraining the f enc, f enc std, and f dec networks of the \u03b2-VAE using data from a random policy. Control policy is learned on frozen latent representations z t = f enc (o t). Tuning \u03b2 shows that very small values perform best. Varying the frequency N of updating the representation space shows a positive correlation with final policy performance. Gradients are not shared between the \u03b2-VAE and actor-critic for policy learning. This suggests combining representation pretraining via a \u03b2-VAE with policy learning can be beneficial. Combining representation pretraining via a \u03b2-VAE with policy learning in an end-to-end procedure can lead to better performance. Prior work has shown that using a regularized autoencoder can achieve stable training in the off-policy regime. The stability is achieved by updating a deterministic encoder with gradients from reconstruction and soft Q-learning objectives. Allowing gradient propagation to the encoder of the \u03b2-VAE from the actor-critic, such as SAC, enables end-to-end learning. The text discusses the challenges of end-to-end policy learning with a \u03b2-VAE and introduces a new algorithm, SAC+AE, to enable stable training by updating the pixel autoencoder simultaneously with policy learning. Electing to learn deterministic latent representations with a regularized autoencoder stabilizes end-to-end learning in the off-policy regime. Updating convolutional weights in the target critic network faster than other parameters allows for faster learning while maintaining stability. Sharing encoder's weights between actor and critic networks, with the actor not updating them, results in stable training of an off-policy algorithm with an auxiliary reconstruction loss. SAC+AE algorithm achieves this stability in training from images. The RAE used in the SAC+AE algorithm consists of a 4-layer convolutional and deconvolutional trunk. Actor and critic networks are 3-layer MLPs with a hidden size of 1024. The method is tested on 6 image-based continuous control tasks from DMC. Comparisons are made against other RL algorithms like D4PG, PlaNet, and SLAC. The SAC+AE algorithm utilizes cross-entropy planning for control and SLAC, showing improved performance over baseline SAC:pixel. It matches state-of-the-art model-based methods like PlaNet and SLAC, and outperforms D4PG. The algorithm is stable across ten random seeds and easy to implement. The SAC+AE algorithm significantly improves performance over the baseline SAC:pixel by utilizing cross-entropy planning for control and SLAC. It outperforms D4PG and matches state-of-the-art model-based methods like PlaNet and SLAC. The algorithm is stable across ten random seeds and easy to implement. Ablation studies are conducted to analyze the latent representation space learned by the algorithm, focusing on extracting information from raw images and generalizing to unseen tasks without reconstruction signal. The learned representation space from raw images in SAC+AE:pixel and SAC:pixel encodes information about the environment's internal state. Encoder pretrained with SAC+AE:pixel on walker walk can generalize to unseen tasks like walker stand and walker run. SAC with a pretrained encoder achieves impressive performance compared to the baseline. Linear projections map image observations to proprioceptive states, with reconstructions matching ground truth states. The encoder pretrained with SAC+AE:pixel on walker walk can generalize to tasks like walker stand and walker run. SAC agents without reconstruction loss on walker stand and walker run tasks show improved performance with the pretrained encoder. The first agent's encoder is initialized with weights from a pretrained walker walk encoder, while the second agent's encoder is not. Both agents do not use the reconstruction signal and only backpropagate gradients from the critic to the encoder. Results indicate that this method learns latent representations that generalize well to unseen tasks, helping a SAC agent achieve strong performance. This is the first end-to-end, off-policy, model-free RL algorithm for pixel observations with only a reconstruction loss as an auxiliary task. It is competitive with state-of-the-art model-based methods but simpler and more robust, without requiring learning a dynamics model. Ablations demonstrate the superiority of end-to-end learning over previous methods using a two-step training procedure with separated gradients and the importance of a pixel reconstruction loss. The necessity of a pixel reconstruction loss for lower-dimensional representations is highlighted, with deterministic models outperforming stochastic ones due to introduced instabilities. Deterministic models are preferred for their interpretability and handling of simpler distributions. Results from experiments on various tasks and hyperparameters are provided in the Appendix. The paper evaluates algorithms on the DeepMind control suite, a collection of continuous control tasks, to measure progress reliably. Six domains result in twelve different control tasks, each posing unique challenges to learning algorithms. The ball in cup catch task only provides sparse rewards. The codebase is open-sourced for future research in image-based RL. The DeepMind control suite evaluates algorithms on twelve different tasks with unique challenges. Double Q-learning is used for the critic and actor, with a 3-layer MLP architecture. An encoder architecture similar to Tassa et al. (2018) is employed, with minor differences in convolutional layers and activation functions. The architecture utilizes ReLU activations in conv layers with 32 channels and 3x3 kernels. A single fully-connected layer normalized by LayerNorm is followed by tanh nonlinearity. Actor and critic networks have separate encoders but share conv layer weights. Only the critic optimizer updates these weights. The decoder includes a fully-connected layer and four deconv layers with ReLU activations, producing pixel representation. The architecture includes deconv layers with 3x3 kernels and 32 channels. The critic's encoder is combined with the decoder to form an autoencoder. Training data is collected from seed and current policy observations, with updates made after receiving new data. The number of training observations is reduced when using action repeat. Action repeat values are specified for each environment. The weight matrix of fully-connected layers is initialized orthogonally, while convolutional layers use deltaorthogonal initialization. The autoencoder network is regularized using a scheme proposed in Ghosh et al. (2019), extending the standard reconstruction loss with L2 regularization. Action repeat parameters are specified for each environment task. We extend the reconstruction loss for a deterministic autoencoder with an L2 penalty on the learned representation and weight decay on the decoder parameters. Observational input consists of a 3-stack of consecutive frames, each scaled down to [0, 1) range. Reconstruction targets are preprocessed by reducing bit depth to 5 bits. Iterative pretraining allows for faster representation learning. An unsuccessful attempt was made to propagate gradients from the actor-critic to the encoder of the \u03b2-VAE for end-to-end off-policy training. The learning process of SAC+VAE:pixel showed instability and subpar performance compared to the baseline SAC+VAE:pixel. Various autoencoder capacities were investigated by changing the convolutional trunk of the encoder and deconvolutional trunk of the decoder. Maintaining shared weights across convolutional layers between the actor and critic, the number of convolutional layers and number of layers were modified. SAC+AE is robust to different autoencoder capacities, extracting relevant features for policy learning from low-dimensional proprioceptive observations. Using compact observations as reconstruction targets improves performance and sample efficiency. This approach helps understand the benefits of supervision for representation learning. The auxiliary supervision with a state decoder does not provide the expected benefits for representation learning and can even harm performance in certain tasks. This is due to the low-dimensional supervision not being able to capture the rich reconstruction error needed for the high-capacity convolutional encoder. Seeking a denser auxiliary signal for learning latent representation spaces is necessary. Seeking a denser auxiliary signal for learning latent representation spaces is crucial. Synthetic supervision through reconstructing a low-dimensional state from image observations may not provide sufficient signal for high-capacity encoders, leading to suboptimal performance. The optimality of learned latent representations is measured by their ability to extract and preserve relevant information from pixel observations to facilitate policy learning. The differences in performance between different observation spaces can be attributed to both the observation space itself and the data collected. To determine the impact of transitioning from proprioceptive states to pixel images on policy learning, policies are trained on a fixed replay buffer containing both types of data. Training curves show the performance of the policy used to collect the buffer and the two policies trained on it using proprioceptive observations. Operating in an off-policy regime may result in differences in performance. Our method outperforms proprioceptive observations in policy learning. The learned latent representation from pixel observations surpasses proprioceptive state on a fixed buffer. Repeating nominal actions significantly impacts learning dynamics and final reward. Action repeat is treated as a hyperparameter in prior works, but it effectively decreases the control horizon of the task. Action repeat can impact learning dynamics and final reward by decreasing the control horizon of the task. Finding the optimal value for action repeat is crucial for stabilizing training without limiting control elasticity too much. An ablation study was conducted to compare different action repeat settings on multiple control tasks, showing results against PlaNet with the original action repeat setting. The study evaluates the impact of action repeat on final performance, showing that PlaNet's choice of action repeat is not always optimal for the algorithm. It is better to apply an action only once or twice in some environments, rather than four times as suggested by PlaNet. The study compares various continuous control algorithms, including SAC, TD3, DDPG, and D4PG. They adjusted the batch size for TD3 and DDPG to improve performance. D4PG's performance results were taken from a previous study. In Tassa et al. (2018), SAC, TD3, DDPG, and D4PG were benchmarked on challenging continuous control tasks from DMC. The hyper parameters were kept consistent across tasks to prevent overfitting. Evaluation was done every 10000 steps, with 10 evaluation episodes averaged over 10 random seeds. SAC demonstrates superior performance and sample efficiency over other methods on various tasks from DMC, with results reported after 10 8 environment steps."
}