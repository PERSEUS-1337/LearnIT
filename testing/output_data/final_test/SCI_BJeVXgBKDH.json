{
    "title": "BJeVXgBKDH",
    "content": "Automatic question generation from paragraphs is a challenging problem due to the long context. This paper introduces two hierarchical models for question generation: a hierarchical BiLSTM model with selective attention and a hierarchical Transformer architecture. These models learn hierarchical representations of paragraphs by modeling them in terms of sentences and words. The hierarchical Transformer outperforms the flat transformer model due to its attention and positional encoding mechanisms. Empirical evaluation was conducted on SQuAD and MS MARCO datasets using standard metrics. The hierarchical models outperform flat counterparts in question generation, producing fluent and relevant questions. Question Generation (QG) from text has gained popularity for various applications like conversational agents and improving question answering systems. Neural network based methods are state-of-the-art for automatic question generation, not requiring templates or rules. While most work focuses on sentence-level QG, paragraph-level QG remains a challenging problem due to the larger context needed for generating high-quality questions. Existing question generation methods, like RNN-based models, achieve good results on sentence-level question generation but struggle with paragraph-level questions. Zhao et al. (2018) proposed a paragraph-level QG model that outperforms others on the SQuAD dataset. One potential improvement is to design the encoder to reflect the structure of a paragraph. The design of the encoder in the paragraph-level QG model is hierarchical, with a BiLSTM-based encoder incorporating dynamic paragraph-level contextual information through selective attention. However, the limitations of LSTM models, such as rigidity and slow training, led to the proposal of the Transformer architecture, which addresses these deficiencies by using the attention mechanism and discarding recurrence in RNNs. The Transformer architecture in QG models discards recurrence in RNNs, allowing for effective attention to different parts of a sequence. It is faster to train and test than RNNs. The model incorporates word-level and sentence-level selective attention to generate high-quality questions from paragraphs. Two hierarchical models for encoding paragraphs are presented, including a novel hierarchical Transformer architecture. The curr_chunk discusses a novel hierarchical Transformer architecture and BiLSTM model for question generation. It also introduces attention mechanisms for incorporating contextual information in paragraph encoders. The focus is on generating meaningful and fluent questions from encoded answers. Existing text-based Question Generation (QG) methods can be categorized into rule-based, template-based, and neural network-based approaches. Rule-based methods analyze sentences syntactically and semantically to generate questions using fixed sets of rules. Template-based methods use generic templates/slot fillers for question generation. Neural network-based QG methods employ an RNN-based encoder-decoder architecture for question generation. Neural network-based Question Generation (QG) methods utilize RNN-based encoder-decoder architecture for end-to-end training. Various approaches include Seq2Seq models with linguistic features, context matching, and maxout pointer mechanisms for generating questions. The curr_chunk discusses the comparison between recurrent and non-recurrent architectures in capturing hierarchical structure in Machine Translation. It mentions that LSTM outperforms Transformer in this aspect, while attention-based models like BERT are better at learning hierarchical structure than LSTM-based models. The authors propose a general hierarchical architecture for better paragraph representation at the level of words and sentences. The authors propose a hierarchical architecture for paragraph representation based on BiLSTM and Transformers. Two decoders (LSTM and Transformer) with hierarchical attention are used to generate questions conditioned on an encoded answer. The hierarchical paragraph encoder (HPE) consists of two encoders for sentence representation. The hierarchical paragraph encoder (HPE) consists of two encoders, a word-level encoder (WORDENC) and a sentence-level encoder. The word-level encoder encodes individual words in sentences to produce a sentence-dependent word representation. The sentence-level encoder produces a fixed-dimensional representation for a sentence by summing or averaging contextual word representations. The HPE consists of a higher-level encoder that produces paragraph-dependent representations for sentences. The input to this encoder is the sentence representations from the lower-level encoder. The output is contextual representations for each set of sentences. Two hierarchical encoding architectures are presented: hierarchical BiLSTM and hierarchical transformer. The hierarchical BiLSTM encoder uses word-level and sentence-level attention to obtain the hierarchical paragraph representation. The methodology involves using a Hierarchical BiLSTM encoder with attention mechanisms at word and sentence levels. BiLSTM is used for word and sentence encoders, with a unidirectional LSTM decoder generating target questions. Attention is computed over words using previous hidden states. The final hidden state representation is obtained from the BiLSTM encoder. The methodology utilizes a Hierarchical BiLSTM encoder with attention mechanisms at word and sentence levels. Sentence-level attention is computed over every sentence in the input passage, incorporating the previous decoder hidden state and the sentence encoder's hidden state. The final hidden state representation is obtained from the sentence level encoder. The methodology involves a Transformer decoder that generates target questions by attending to previously generated tokens, encoded answer, and paragraph. A hierarchical attention module is used to identify sentence and word relevance, replacing the attention mechanism in the Transformer decoder. The model includes sentence and paragraph encoders, a decoder, and hierarchical attention modules. The methodology includes a Transformer decoder for generating target questions by attending to encoded answer and paragraph. It utilizes hierarchical attention modules to identify sentence and word relevance. The encoder maps word representations to continuous sentence representations, while the decoder stack has an additional sub-layer for encoder-decoder attention over the output of the paragraph encoder. The methodology involves using a Transformer decoder with hierarchical attention modules to generate target questions by attending to encoded answer and paragraph. The decoder stack includes an encoder-decoder attention layer over the output of the paragraph encoder, transforming it into attention vectors for generating target words. To attend to the hierarchical paragraph representation, a multi-head hierarchical attention module is introduced, replacing the multi-head attention mechanism in the Transformer. The methodology involves using a Transformer decoder with hierarchical attention modules to generate target questions by attending to encoded answer and paragraph. The decoder stack includes an encoder-decoder attention layer over the output of the paragraph encoder, transforming it into attention vectors for generating target words. The vectors of sentence-level query and word-level query are created using non-linear transformations of the decoder state. The module attends to paragraph sentences using their keys and the sentence query vector, then computes an attention vector for the words of each sentence. The methodology utilizes a Transformer decoder with hierarchical attention modules to generate target questions by attending to encoded answer and paragraph. The attention vectors for the words of each sentence are computed using the word values of their attention weights based on sentence-level and word-level attentions. The final representation is passed through a fully connected feed forward neural network for calculating the hierarchical input representation. The methodology uses a Transformer decoder with hierarchical attention modules to generate target questions by attending to encoded answer and paragraph. Experiments were conducted on SQuAD and MS MARCO datasets, with SQuAD containing Wikipedia articles and questions split into train and test sets. MS MARCO dataset includes passages from web documents and BING queries. Our study involved splitting the dataset into train, dev, and test sets for question generation evaluation. We used BLEU and ROUGE-L metrics, as well as human evaluation, to assess the quality of questions generated by different models. Comparisons were made between hierarchical LSTM and Transformer models with their flat counterparts. The models included Seq2Seq + att + AE and HierSeq2Seq + AE, each with specific architectures for question generation. HierSeq2Seq + AE is a hierarchical BiLSTM model with a BiLSTM sentence encoder, a BiLSTM paragraph encoder, and an LSTM decoder conditioned on encoded answer. TransSeq2Seq + AE is a Transformer-based sequence-to-sequence model with a Transformer encoder and decoder conditioned on encoded answer. Human evaluation results and inter-rater agreement for each model on the SQuAD test set are presented in Table 3, showing scores between 0-100 for syntactic correctness, semantic correctness, and relevance to the text. Automatic evaluation results on SQuAD and MS MARCO datasets are shown in Table 1 and Table 2, while human evaluation results are presented in Table 3 and Table 4 respectively. The hierarchical BiLSTM model HierSeq2Seq + AE outperforms other models on BLEU2-BLEU4 metrics on the SQuAD dataset, while the hierarchical Transformer model TransSeq2Seq + AE excels on BLEU1 and ROUGE-L. Hierarchical models consistently perform better than flat LSTM and Transformer models on both SQuAD and MS MARCO datasets. The hierarchical BiLSTM model outperforms other models on BLEU2-BLEU4 metrics on the SQuAD dataset, while the hierarchical Transformer model excels on BLEU1 and ROUGE-L. Hierarchical models consistently perform better than flat LSTM and Transformer models on both SQuAD and MS MARCO datasets. On the MS MARCO dataset, the hierarchical BiLSTM models show the best performance on automatic evaluation metrics, while the hierarchical Transformer model TransSeq2Seq + AE outperforms all other models in human evaluation for both datasets in syntactic and semantic correctness. The hierarchical representation of paragraphs, particularly the Transformer architecture, shows great potential over traditional RNN models like BiLSTM. Continued investigation of hierarchical Transformer is a promising research avenue. Experimental evaluation on SQuAD and MS MARCO datasets demonstrates the effectiveness of hierarchical models over flat counterparts. Hierarchical models like Transformer and BiLSTM outperform flat counterparts on all metrics. Experimental results confirm the benefits of hierarchical selective attention in the BiLSTM model. Different model architectures were implemented using PyTorch, with varying layer configurations and dimensions. A vocabulary size of 45k was used, with words below a frequency of 5 pruned out. The text discusses the use of pretrained GloVe embeddings and beam search decoder in training HierSeq2seq model with SGD optimization. The model is trained for 20 epochs with learning rate adjustments. HierTransSeq2seq is trained using Adam optimizer with a specific learning rate and warmup steps. The best model generates example questions on the MS MARCO test set."
}