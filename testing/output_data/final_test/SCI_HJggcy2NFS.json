{
    "title": "HJggcy2NFS",
    "content": "Variational inference (VI) and Markov chain Monte Carlo (MCMC) are approximate posterior inference algorithms with complementary strengths. A study shows that gradient-based MCMC and VI procedures are more similar than previously thought. The Fokker-Planck equation governing Langevin dynamics MCMC reveals a connection to a variational inference procedure. This suggests that the transient bias of MCMC may resemble that of VI. Empirical evidence supports this finding. Practitioners may achieve more accurate results by running an MCMC procedure rather than a VI procedure, as long as the variance of the MCMC estimator can be managed. Exact inference in Bayesian data analysis is challenging, leading to the use of approximate algorithms like MCMC and VI, which are known for their complementary strengths. VI and MCMC are often seen as complementary methods in Bayesian data analysis. VI is faster but biased, while MCMC is slower but asymptotically unbiased. The key question is whether VI or MCMC will provide more accurate estimates within a given computational budget. Modern computing reality involves parallel computation, which can reduce MCMC error due to variance by running more parallel chains on more cores. In contrast, reducing transient bias in MCMC requires running longer chains, increasing latency. Similarly, reducing variance in stochastic-gradient VI estimators can be achieved through parallel computation. In this paper, the authors compare the efficiency of VI and MCMC algorithms in reaching a certain level of accuracy within a parallel compute budget. They focus on reparameterized black-box VI and Langevin-dynamics MCMC, showing a reinterpretation of BBVI as a parametric approximation to LD MCMC. The study aims to determine which method reaches accuracy faster under parallel computation constraints. The study compares the efficiency of VI and MCMC algorithms in reaching accuracy within a parallel compute budget. It reinterprets BBVI as a parametric approximation to LD MCMC, suggesting that VI may not be faster than MCMC. Empirical findings show that BBVI's transient bias tracks that of LD, and both estimators often converge at similar speeds. Theoretical results indicate that LD and BBVI follow the same gradient flow, with important implications for algorithm performance. The study compares the efficiency of VI and MCMC algorithms in reaching accuracy within a parallel compute budget. It reinterprets BBVI as a parametric approximation to LD MCMC, suggesting that VI may not be faster than MCMC. The results have implications for practitioners choosing between BBVI and gradient-based MCMC algorithms. It is recommended to run as many short MCMC chains as possible, possibly discarding all but the last sample of each chain. As GPUs and TPUs get more powerful, this strategy will apply to more one-off Bayesian-data-analysis problems. The text discusses interpreting BBVI as black-box variational inference with a nonparametric normalizing flow, showing that gradient-based MCMC algorithms and parametric BBVI follow similar gradient signals. The focus is on pushing samples towards high-density regions under the target distribution. The text discusses the gradient-based MCMC algorithms and parametric BBVI following similar gradient signals to push samples towards high-density regions under the target distribution. LD implicitly follows the ideal functional gradient in each iteration by updating the state using Langevin dynamics. The text discusses a deterministic flow for reproducing the behavior of Langevin Dynamics (LD) through variational inference with normalizing flows. It involves recursively applying an ideal functional BBVI gradient step to sample from an initial distribution and update the state. In this section, the empirical evaluation of BBVI and gradient-based MCMC methods is discussed. Various approaches such as vanilla SGD, momentum 0.9, Metropolis-adjusted Langevin, and Hamiltonian Monte Carlo (HMC) with 10 leapfrog steps are compared. The O(\u03b7 2 ) discretization error is not considered, focusing instead on the continuous-time limit. Empirical results using the Metropolis-adjusted algorithm align with intuitions from the continuous-time limit. BBVI is evaluated using a diagonal-covariance Gaussian variational family. We evaluated BBVI and gradient-based MCMC methods using a diagonal-covariance Gaussian variational family. The bias of estimators based on BBVI and MCMC chains was compared for two Bayesian-data analysis problems. BBVI showed effective step sizes twice as large as MALA for an item-response theory model, while MALA was competitive with BBVI for sparse logistic regression. MALA is competitive with BBVI for sparse logistic regression. Results suggest that the implicit distribution in MCMC chains has bias comparable to explicit VI procedures. When running 100 chains, MCMC estimators outperform BBVI in terms of variance. Gradient-based MCMC and VI algorithms follow the same gradient flow. MCMC and VI algorithms exhibit similar transient behavior due to following the same gradient flow. MCMC's main disadvantage is high variance, which can be mitigated by running many parallel chains on modern GPUs. As parallel hardware becomes cheaper, MCMC may become more attractive compared to VI for various problems. The curr_chunk discusses the posterior of a one-parameter-logistic item-response-theory model and sparse logistic regression model applied to the German credit dataset. The curr_chunk by Hoffman et al. (2019) focuses on applying soft masks on regression coefficients using Gamma priors and log-transforming variables. It also discusses a multivariate Gaussian target distribution with an ill-conditioned covariance matrix. The variational family is parameterized using an affine change of variables, with tradeoffs for the scale matrix A. The curr_chunk discusses different parameterizations for the scale matrix A in variational inference. These include simple A = \u03c6, matrix-logarithm A = e \u03c6, and lower-triangular A = diag(e \u03c6 s ) + \u03c6 L. Each parameterization has tradeoffs in terms of numerical issues, computational cost, and geometric properties. BBVI was compared with MALA using different parameterizations for the scale matrix A. Gradients were estimated using a minibatch of 100 samples, and results showed that BBVI with variance-reduced STL updates can achieve geometric convergence. Performance varied depending on parameterization, with the lower-triangular parameterization being slower than the linear-scale. The lower-triangular parameterization is slower than the linear-scale parameterization, while the log-scale parameterization achieves superlinear convergence. MALA's performance is comparable to linear-scale BBVI, but slightly slower due to a smaller step size. BBVI parameterizations require extra work per iteration compared to MALA. Total error of MCMC and BBVI is shown in Figure 3 for sparse logistic regression. Running a single HMC chain and averaging samples from the last half eliminates bias but still has high error due to variance. When comparing VI to MCMC, VI is faster but requires many iterations to match BBVI's accuracy. Parallel computation reduces variance, with discarding first halves of chains or using only the last sample outperforming BBVI. BBVI computes 100 gradients per step on a GPU, taking only a few milliseconds for a sparse logistic regression model."
}