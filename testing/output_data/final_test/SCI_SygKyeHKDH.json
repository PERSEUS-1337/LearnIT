{
    "title": "SygKyeHKDH",
    "content": "This paper introduces R2D3, an agent that efficiently uses demonstrations to solve hard exploration problems in partially observable environments with variable initial conditions. R2D3 outperforms other state-of-the-art methods in tasks where billions of exploration steps fail to produce successful trajectories. Reinforcement learning from demonstrations has been effective in addressing sample efficiency and hard exploration challenges in various domains, such as Montezuma's Revenge and robotics. In this paper, the authors address the challenge of learning from demonstrations in hard exploration tasks in partially observable environments with variable initial conditions. They highlight three key aspects that make learning challenging: sparse rewards, partial observability, and the need for memory. Their approach is able to solve tasks where standard methods struggle to find any non-zero rewards even after billions of steps. Learning from demonstrations in partially observable environments with variable initial conditions is challenging due to the inability to break trajectories into isolated transitions using the Markov property. Generalizing between different initial conditions is difficult, but the approach combines demonstrations with off-policy, recurrent Q-learning to efficiently tackle these problems. Our approach combines off-policy, recurrent Q-learning with expert demonstrations to efficiently learn from a small number of demonstrations in challenging environments with variable initial conditions. The demo-ratio hyper-parameter plays a crucial role in the algorithm's performance, allowing agents to outperform demonstrators and discover new strategies, even exploiting bugs in the environment. The optimal demo ratio is small but non-zero, guiding the agent's exploration in a 3D world with complex tasks. The Hard-Eight suite challenges state-of-the-art methods, showcasing the effectiveness of the approach. The paper introduces a new agent, R2D3, designed to efficiently use demonstrations to solve sparse reward tasks in partially observed environments. The agent samples batches of demonstrations and agent experiences, with the demo ratio being a key hyper-parameter for performance. The R2D3 agent efficiently utilizes demonstrations to solve sparse reward tasks in partially observed environments. It consists of multiple actor processes streaming experiences to a shared replay buffer, prioritized using a mixture of max and mean TD-errors. Additionally, there is a demo replay buffer for expert demonstrations. The learner process maintains a second demo replay buffer with expert demonstrations prioritized using a specific scheme. Separate replay buffers for agent experience and expert demos allow for prioritized sampling. A demo ratio hyperparameter controls the proportion of data from expert demos versus agent experience, implemented at a batch level. Stochastic demo ratio targeting ratios smaller than batch size is crucial for performance optimization. The learner's objective uses n-step optimization. The learner optimizes using n-step, double Q-learning, and a dueling architecture. It updates network priorities and stores sequences of (s,a,r) tuples in replay buffers. Value estimates are generated from online and target networks with recurrent state initialized to zero. A burn-in phase approximates proper recurrent state initialization. In a Hard-Eight task suite, agents interact with objects to reach a reward apple in a procedurally generated 3D environment. Sparse rewards make exploration challenging, with no improvement seen from storing recurrent states in replay. Exploration remains a key challenge in reinforcement learning. Hard-exploration domains like Atari environments pose challenges for classical RL algorithms due to sparse rewards and distracting dead ends. Techniques like intrinsic motivation and count-based exploration are used to encourage exploration but may not scale well as the state space grows. Recent empirical results suggest that traditional exploration methods do not consistently outperform -greedy exploration. The difficulty of exploration is attributed to agents' inability to abstract the world and learn scalable, causal models. Techniques like reward-shaping and curriculum tasks can bias agents towards promising regions of the state space but may lead to unexpected behavior. Techniques like reward-shaping and curriculum tasks can bias agents towards promising regions of the state space but may lead to unexpected behavior. Hard-exploration benchmarks tend to be fully-observable with little episode variation. Alternative methods like random no-ops and \"sticky actions\" aim to increase episode variance in Atari. Our approach can solve tasks with substantial per-episode variability, unlike GAIL, another imitation learning method. To address hard exploration in partially observable environments with variable initial conditions, a collection of eight tasks is introduced. These tasks require completing a sequence of high-level steps involving interaction with physical objects. The Baseball task, in particular, involves a long sequence of steps that must be completed in order, making random exploration unlikely to solve it. The Hard-Eight task suite consists of eight tasks with rich interaction between the agent and environment, leading to increased variability between episodes. Memorizing open loop sequences of actions is unlikely to succeed due to the nature of interaction and limited field of view, requiring the use of memory in the agent. These tasks share common properties such as sparse rewards and first-person visual observations, making them challenging exploration problems. The Hard-Eight task suite consists of eight tasks with rich interaction between the agent and environment, leading to increased variability between episodes. Each task is subject to highly variable initial conditions, with procedural elements like colors, shapes, and configurations of objects. Tasks are designed to prevent simultaneous observation of all relevant information, requiring memory and exploration due to sparse rewards and first-person visual observations. The Hard-Eight task suite consists of eight tasks with rich interaction between the agent and environment, leading to increased variability between episodes. Each task requires the agent to complete a sequence of high-level steps to collect apples. The tasks have variable initial conditions and sparse rewards, necessitating memory and exploration. The agent is rewarded for completing tasks like finding the bat, knocking the ball off the plinth, and collecting the large apple. The study compares the R2D3 agent against baselines like Behavior Cloning and ablations of the method. The curr_chunk discusses ablations of the R2D3 method, specifically removing either recurrence or demonstrations. Behavior Cloning is a baseline method for learning policies from demonstrations using expert trajectories to fit a parameterized policy mapping states to actions. It involves feeding frames into a ResNet and augmenting the output with previous action, reward, and other features for imitation learning. The curr_chunk discusses ablations of the R2D3 method, specifically removing either recurrence or demonstrations. This corresponds to a classification task using cross-entropy loss. BC outperforms recent batch-RL methods when rewards are consistently high. Training the BC agent with the same recurrent neural network architecture as R2D3 enables fair comparison. Removing demonstrations from R2D3 results in a strong baseline, similar to the R2D2 agent. The second ablation involves replacing the recurrent value function of R2D3. The second ablation involves replacing the recurrent value function of R2D3 with a feed-forward reactive network, allowing optimization over the demo ratio. The R2D3 agent's performance is evaluated against deep RL baselines, including BC, R2D2, and DQfD, on Hard-Eight tasks. The code for all agents, including R2D3, will be released. In the study, code for all agents, including R2D3, will be released. Different agents were trained using CPU-based actors and a GPU-based learner process for tasks in the Hard-Eight suite. Each agent had distinct noise parameters and common hyperparameters. The demo ratio was varied for R2D3 and DQfD, while the learning rate was adjusted for BC. All agents operated with an action-repeat factor of 2, as using a factor of 4 was found to make tasks too difficult. Using an action repeat of 4 made the Hard-Eight tasks too difficult for our demonstrators. R2D3 outperformed baselines on tasks like Baseball, Drawbridge, Navigate Cubes, and Wall Sensor, while R2D2 struggled to achieve positive rewards. DQfD and BC agents occasionally saw rewards on some tasks, but not consistently enough to be visible in the plots. The neural network architecture of different agents is illustrated in Figure 4, with a focus on minimizing high action repeats for smooth motion and ease of learning. The training setup includes using the Adam optimizer with a fixed learning rate and distributed training with 256 parallel actors. The BC agent has a different training regime as it does not interact with the environment during training. During training, the BC agent does not interact with the environment and undergoes a hyperparameter sweep over learning rates. It is trained for 500k learner steps, with periodic evaluation of network weights. 100 demonstrations for each task were collected from three different experts using keyboard and mouse controls for behaviour cloning and learning from demonstrations. In a study comparing different methods for learning from demonstrations, R2D3 outperformed baselines in six out of eight tasks, achieving or exceeding human performance in four tasks. Despite using the same demonstrations, BC and DQfD failed to learn any tasks. All methods, including R2D3, struggled with tasks requiring high memory demands. Future work should focus on improving the handling of recurrent states to help R2D3 succeed in tasks with high memory demands. R2D3 outperformed human demonstrators on tasks like Baseball, Drawbridge, Navigate Cubes, and Wall Sensor, with a unique strategy on Wall Sensor Stack by exploiting a bug in the environment implementation. The strategy employed by R2D3 on the Wall Sensor Stack task involved tricking the sensor into remaining active by pressing the key against it in a precise way. Despite attempts to train other models on the task suite, including adding randomized prior functions and using an IMPALA agent with pixel control, all efforts were unsuccessful. In experiments on Hard-Eight tasks, a hyperparameter search was conducted to choose the best parameters for each method independently. The success rate of R2D3 across the task suite was analyzed based on the demo ratio. The goal of each task is to collect a large apple, with success defined as collecting the apple in an episode. An agent is considered successful if at least 75% of its final 25 episodes are successful. In experiments on Hard-Eight tasks, a hyperparameter search was conducted to choose the best parameters for each method independently. Several R2D3 agents were trained on each task with varying demo ratios. Tuning the demo ratio had a significant impact on the success rate, with the best ratio being quite small. Further results can be found in Appendix D.3. In an experiment comparing R2D3 to R2D2 on Hard-Eight tasks, R2D3 shows more task-relevant actions early in training, suggesting that demonstrations bias R2D3 towards exploring relevant parts of the environment. In an experiment comparing R2D3 to R2D2 on Hard-Eight tasks, R2D3 shows more task-relevant actions early in training, suggesting that demonstrations bias R2D3 towards exploring relevant parts of the environment. R2D3 agent efficiently learns in partially observable environments with sparse rewards and variable initial conditions, outperforming state-of-the-art baselines in difficult tasks. Our study compared R2D3 to R2D2 on Hard-Eight tasks, showing that R2D3 outperforms state-of-the-art baselines. We identified the demo ratio as a key parameter for good performance, with the optimal ratio being surprisingly small but non-zero. Future work could explore how this ratio changes with the number of demonstrations and expert trajectories. The mechanism R2D3 uses to efficiently extract information from expert demonstrations is to guide the agent's autonomous exploration. An in-depth analysis of agent behavior on the Hard-Eight task suite can help understand how different RL algorithms utilize information. The R2D3 agent consists of a single learner process that samples from demonstration and agent buffers to update policy parameters. Inputs include expert demonstrations, agent experiences, batch size, sequence length, and number of actors. The R2D3 agent updates policy weights using transition sequences from replay buffers. A target network is used to calculate loss, and parameters are updated through gradient descent. Actor processes interact with the environment to collect data for the agent buffer. Sparse rewards are given in tasks, with most ending episodes successfully after a positive reward. The standard RL algorithms require actors to solve tasks inadvertently with no intermediate signal. Visual observations are first-person, leading to partial observability. Tasks have highly variable initial conditions due to procedurally generated elements. The tasks in the environment have the same observation space with visual inputs and avatar movements. The action space includes displacement, rotation, and grasping actions controlled by the avatar's gaze direction. The avatar in the environment controls the hand's location through gaze direction and additional actions for distance. Manipulation of grasped objects involves rotation and distance control actions. Fine-grained actions allow for careful movements. Additional details on tasks, successful task execution frames, and procedural elements are provided. Videos of task performances can be viewed at a specified link. The agent must use a stick to knock a key object off a plinth to activate a sensor, opening a door to a room with a large apple. The agent navigates through branching platforms with drawbridges that require key objects to activate. Choosing the most rewarding path leads to obtaining a large apple at the end of the level. The agent must navigate through different rooms, pushing blocks and finding ramps to reach large apples that end the episode. The agent spawns near a sensor of a random color and must travel down a long hallway to a room full of blocks. Selecting the correct block matching the sensor color allows access to a large apple, but crossing penalty sensors incurs a small negative reward. The agent spawns in a U-shaped room with objects of different shapes and materials. The agent spawns in different rooms with key objects and sensors. In one room, the agent must throw a key object across a void and carry another to open locked doors. In another room, the agent must pick up a key object and touch it to a sensor to open a door leading to a large apple. The agent must stack two key objects to keep a door open with a sensor. R2D3 agents can surpass human baseline performance on tasks. The R2D3 agents outperform human demonstrators on tasks like the Baseball task and Wall Sensor task by refining their behavior and exploiting bugs in the environment. The R2D3 agents exploit a bug in the sensor to achieve superhuman scores on tasks, including the Wall Sensor task. Additional experiments were conducted to gather more information on unsolved tasks. The agent was trained using higher action repeat and stale lstm states to improve performance on tasks requiring long memory. Videos of these experiments are available at https://bit.ly/2mAAUgg. R2D3 learned policies for tasks like retrieving a random block with occasional success. Additional demos improved success rate to 100%. A bug in the Wall Sensor Stack environment was fixed, allowing proper stacking behavior. Performance of R2D3 agents varied across tasks, with some seeds showing rapid progress while others remained flat. In Figure 10, demo ratio of 1/256 works best for Baseball, Navigate Cubes, Push Blocks, and Wall Sensor Stack tasks. Little variation in exploration behavior across seeds and training time for R2D2 agent. R2D3 policies show substantial variation with demonstrations."
}