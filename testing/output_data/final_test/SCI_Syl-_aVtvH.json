{
    "title": "Syl-_aVtvH",
    "content": "FURL is a method for collaborative personalization in Federated Learning that divides model parameters into federated and private parameters. Private parameters, like user embeddings, are trained locally to preserve privacy and improve memory locality. The approach shows a significant improvement in model quality on two datasets. FURL in Federated Learning improves model quality with significant performance increases on two datasets. User embeddings learned in FL and centralized settings have a similar structure, indicating collaborative learning while preserving privacy. Collaborative personalization enhances neural-network-based model accuracy by adapting to each user's behavior. Training personalized models in a privacy-preserving way is desirable to protect user privacy. To address challenges in personalized Federated Learning (FL), a privacy-preserving scheme called FURL is proposed to extend neural-network personalization. FURL can personalize models by learning task-specific user representations or by personalizing model weights. Research on collaborative personalization in FL has focused on developing new methods. Existing neural-network personalization techniques can be used in Federated Learning (FL) with only a small modification to the Federated Averaging algorithm. However, these techniques do not efficiently train user embeddings in FL due to the transfer and averaging of all parameters on a central server. This approach is resource-expensive and compromises user privacy. FURL introduces the concepts of federated and private parameters to address these issues. FURL introduces federated and private parameters to address issues with existing neural-network personalization techniques in Federated Learning. Private user embedding vectors remain on the user device and are trained jointly with the global model, without being transferred back to the server. This approach reduces global model aggregation time linearly with the number of users, unlike other methods that increase quadratically. FURL leverages the distributed nature of models across users, with little resource overhead in distributing the embedding table. FURL is a method that enables collaborative personalization techniques to work in Federated Learning by using distributed embeddings tables to improve memory locality and preserve user privacy. User embeddings significantly enhance performance in both server training and FL, with user representations showing a similar structure in both settings. Existing collaborative personalization techniques can be adapted for Federated Learning by splitting the model into federated and private parameters. Formal constraints ensure that this parameter splitting does not impact model performance, especially when using Federated Averaging. Empirical results show that FURL improves model performance in FL settings by 8% to 51% on real-world datasets, with performance closely matching centralized training. User embeddings learned in FL are comparable to those in centralized training. Existing collaborative personalization techniques can be adapted for Federated Learning by splitting the model into federated and private parameters. Multi-task formulations of Federated Learning present a way to learn personalized weights among users, but scalability is a concern due to the quadratic increase in parameters with users. Transfer learning for personalization in FL complicates the training process and may require access to global proxy data. FURL is a scalable approach to collaborative personalization in Federated Learning that does not require complex training processes. It leverages existing techniques used to personalize neural networks in a centralized setting and focuses on powerful collaborative personalization while preserving user privacy. In Federated Learning, user embeddings are privacy-sensitive and should not be shared. FURL proposes splitting model parameters into federated and private parts, which does not affect the training algorithm as long as the split-personalization constraint is satisfied. FL algorithms typically involve local training where users update their local model parameters based on global parameters. Global model parameters are updated by individual users through training on their own data, resulting in different models for each user. Locally-trained models are then aggregated to improve the global model on the server, with various aggregation schemes proposed. User embeddings are kept private during aggregation to protect privacy and reduce network communication. The model has federated and private parameters, with a splitpersonalization constraint to ensure consistent results. Two constraints are needed: independent-local-training and independent-aggregation. The former requires loss functions to be independent of other users' private parameters, while the latter ensures global aggregation independence. The constraint for training example k on user i is that the gradient of the local loss function with respect to other users' private parameters is zero. Equation 1 is satisfied by most personalization techniques like collaborative filtering and user-specific model weights. Global regularization of the user representation matrix is impractical in federated learning due to bandwidth and privacy concerns. Dropout regularization and regularization of each user representation separately do not violate the constraint. The independent-aggregation constraint in federated learning requires that global updates for federated parameters depend only on locally trained values and summary statistics. Global updates for private parameters must be independent of other users' private parameters and federated parameters. This constraint ensures no interaction terms between private parameters of different users during aggregation. Scalable FL approaches like Federated Averaging satisfy the independent-aggregation assumption. MTL-FL formulations do not adhere to this constraint. Local training initializes federated and private parameters, which are globally aggregated at the end of each iteration to satisfy the independent-aggregation constraint. FURL's global update rule for federated parameters w f must follow a specific form, while the rule for private parameters w i p has its own form. The function a f is used for federated averaging, and a p is the identity function. FURL's parameter splitting approach is applicable to any FL algorithm meeting certain constraints. FURL works with FL algorithms that meet the split-personalization constraint. The global update rule of Federated Averaging satisfies the independent-aggregation constraint. In practical implementations, user devices send model deltas instead of trained model parameters to the server. The implementation of FURL simplifies the handling of private parameters by ignoring a noisy scaling factor, retaining the value after local training, and keeping the locally updated parameters for each user. The global update rule for private parameters of user i is to keep the locally trained value. The training process involves local training and global aggregation of federated parameters. Private parameters like user embeddings are stored locally on the user device. The user embeddings trained locally are stored on the user device for the next round of training and local inference. FURL performance is evaluated on two document classification tasks using datasets called Sticker and Subreddit. Sticker dataset contains anonymized messages from a messaging app for predicting user action on suggested stickers. Subreddit dataset consists of user comments from the top 256 subreddits on reddit.com. The comment dataset consists of user comments from the top 256 subreddits on reddit.com. Users with fewer than 150 or more than 500 comments are filtered out. Each user's data is split into train/eval/test sets. The task is to predict the subreddit where the comment was posted through multiclass classification using an LSTM-based neural network architecture. The dataset has 942K samples and 3.8K users, with an average of 248 comments per user. The text describes a model using character-level embeddings and a Bidirectional LSTM for encoding input text, along with a Multi-Layer Perceptron for prediction. Parameters are shared across users and trained locally, with user embeddings kept private. User embeddings evolve collaboratively through a shared model. Four configurations were tested for performance evaluation. The experiment involved testing four configurations to evaluate model performance with/without Federated Learning (FL) and personalization. Training was done using SGD for Server-training and Federated Averaging for FL training. Personalization in FL utilized FURL training. Hyperparameter sweeps were conducted to optimize model architectures and learning rates. FL configurations randomly selected 10 users/round for training. The experiment involved testing four configurations with/without Federated Learning (FL) and personalization. Separate hyperparameter sweeps for FL and centralized training resulted in the same optimal embedding dimension. Personalization significantly improved performance, with user embeddings increasing AUC and accuracy in both the Sticker and Subreddit datasets. Personalization in FL showed a larger improvement in accuracy compared to the Server setting. Federated Learning (FL) provides similar performance to server training, with no AUC reduction on the Sticker dataset and only a 3.72% accuracy drop on the Subreddit dataset. The learning curves show FL models approaching server performance, while user embeddings in FL have a similar structure to those in server training. Optimal embedding dimensions were the same for both centralized and FL training. User representations were visualized using t-SNE. The results show that users with similar behaviors are clustered together in both centralized and FL settings. User embeddings reflect commenting behavior on subreddits, with FURL proposed as a technique for model personalization in FL. FURL is a technique for model personalization in Federated Learning that prioritizes user privacy and achieves similar performance to centralized personalized models. Future work includes evaluating FURL on different datasets, addressing the cold start problem, and personalizing for users not participating in global FL aggregation."
}