{
    "title": "H1gax6VtDB",
    "content": "A structured understanding of the world in terms of objects, relations, and hierarchies is crucial for human cognition. Contrastively-trained Structured World Models (C-SWMs) use a contrastive approach for representation learning in environments with compositional structure. Each state embedding is structured as a set of object representations and their relations, allowing objects to be discovered from raw pixel observations without direct supervision. C-SWMs have shown success in compositional environments, Atari games, and multi-object physics simulations. C-SWMs can outperform models based on pixel reconstruction in structured environments by learning interpretable object-based representations. Compositional reasoning, involving objects, relations, and actions, is essential in human cognition and has motivated recent works in machine learning to disentangle scenes into objects and their properties. These structured neural models aid in predicting physical dynamics, understanding consequences of actions, and generalizing to new environments. Recent works have focused on object discovery from visual data in an unsupervised manner, using a generative approach to learn object-based representations through visual predictions or reconstruction. This approach aims to enable models to answer counterfactual questions and adapt to novel environment situations. The text discusses the challenges of optimizing an objective in pixel space for learning abstract state representations. It introduces Contrastively-trained Structured World Models (C-SWMs) as a discriminative approach using contrastive learning to score real against fake experiences. This approach aims to avoid failure modes such as ignoring relevant features or wasting model capacity on irrelevant ones. Structured World Models (C-SWMs) learn abstract state representations from observations in an environment using a graph neural network. A novel object-level contrastive loss is introduced for unsupervised learning of object-based representations, connecting contrastive learning of state abstractions and relational graph embeddings for future model improvements. In a set of experiments using a novel ranking-based evaluation strategy, C-SWMs demonstrate learning interpretable object-level state abstractions, predicting state transitions accurately many steps into the future, and generalizing combinatorially to new environment configurations. The goal is to learn an object-oriented abstraction of observations and an action-conditioned transition model that considers object representations and interactions. The framework for contrastive learning of state abstractions and transition models without object factorization is introduced, followed by a variant utilizing object-factorized state. The Structured World Model utilizes object-factorized state representations in an off-policy setting, operating on a buffer of offline experience. The goal is to learn abstract representations of environment states that predict follow-up states after actions. An encoder maps observed states to abstract representations, and a transition model operates solely on these abstract representations. Rewards are not considered in this framework for simplicity. The graph embedding method TransE embeds facts from a knowledge base consisting of entity-relation-entity triples. The transition model operates on abstract state representations, defining the energy of state-action-state triples. This model provides a bias for modeling action effects as translations in the abstract state space. The use of graph embedding methods like RESCAL, CompleX, or HolE allows for modeling effects as linear transformations or rotations in the abstract state space. The energy-based hinge loss is defined for a single state-action-state triple with a corrupted abstract state, with a margin hyperparameter \u03b3. Unlike previous approaches, the hinge is only placed on the negative term, and the norm of abstract states is not constrained. The overall loss is an expectation over samples from the experience buffer, aiming to consider the compositional nature of visual scenes. Our goal is to learn a relational and object-oriented model of the environment using a factored abstract state space Z = Z1 \u00d7 . . . \u00d7 ZK. The object-factorized action space A = A1 \u00d7 . . . \u00d7 AK allows for efficient sharing of model parameters across objects. The C-SWM model architecture includes a CNN-based object extractor, an MLP-based object encoder, a GNN-based relational transition model, and an object-factorized contrastive loss. The curr_chunk discusses the encoder split into an object extractor and object encoder, with the object extractor using a CNN to create object masks. This allows for encoding complex object features beyond just position/velocity. The object extractor module produces multiple feature maps per object slot, which are then flattened and fed into the object encoder. The transition model is implemented as a graph neural network to model pairwise interactions between object states. Each object in the scene has an abstract state description and an action representation. The transition function in the graph neural network takes object representations and actions as input at a specific time step. Message passing is done on a fully-connected scene graph, but can be reduced to linear complexity by connecting only to nearest neighbors in the abstract state space. The transition function in the graph neural network involves object representations and actions at a specific time step. The energy function is modified to consider the abstract state space factorization, resulting in contrastive loss calculations for positive and negative samples. Related work on object discovery and relational graph embeddings is discussed. Recent advancements in modeling structured environments like multi-object systems have improved predictive accuracy by explicitly considering their structured nature. Recent advancements in modeling structured environments, such as multi-object systems, have focused on improving predictive accuracy by utilizing graph neural networks. These networks incorporate node and edge update functions to capture the dynamics and interactions of individual objects or agents. Some recent works have successfully learned structured models directly from pixels, although they rely on pixel-based loss functions. The COBRA model, for instance, learns an action-conditioned transition policy based on object representations derived from unsupervised object data. COBRA model learns action-conditioned transition policy on object representations from unsupervised object discovery. Object encoder could benefit from an iterative encoding process like MONet. Contrastive learning methods are widely used in graph representation and word learning. Recent works connect learning representations by maximizing mutual information between data and learned representations, applying methods to image, speech, and video data. State representation learning in similar environments is approached by models based on autoencoders or adversarial learning. Some methods learn state representations without requiring a decoder. In recent works, various methods have been proposed to learn state representations without a decoder, such as selectivity, contrastive, mutual information, and distribution matching objectives. Ehrhardt et al. (2018) introduced a method to learn object and physics modules jointly from raw video data. Their approach can track a single object but requires balancing multiple loss functions. The goal of this experimental section is to test if C-SWMs can learn object representations, transition models, and generalize without supervision. The implementation of C-SWMs aims to learn transition models in latent space and generalize to new scenes. Evaluation includes grid world environments, Atari games, and a physics simulation. Observations are provided as color images or tensors. Model performance is assessed using ranking metrics in latent space. The model evaluates performance in latent space using ranking metrics, predicting the next state representation and comparing it to true observations. Hits at Rank 1 (H@1) and Mean Reciprocal Rank (MRR) are reported. Further details on evaluation metrics can be found in Appendix C. The predominant method for state representation learning is based on autoencoders, particularly the VAE model. Inspired by previous work, a World Model baseline uses either a deterministic autoencoder or a VAE to learn state representations, with an MLP predicting the next state. This model is trained with pixel-based reconstruction losses and utilizes a differentiable physics engine in the latent space for 3-body physics environments. C-SWMs are trained on an experience buffer from running a random policy, with 1000 episodes and 100 environment steps chosen for training. For evaluation, a separate experience buffer is populated with 10 environment steps per episode for grid world environments, 100 episodes for Atari environments, and 1000 episodes for the physics environment. Train/test overlap is minimized in Atari environments by 'warm-starting' experience collection with random actions before populating the buffer. Full test set episodes do not coincide exactly with training set episodes due to the large state spaces in grid world environments. Performing well on tasks with large state spaces like grid world environments and continuous state spaces like the physics environment requires generalization to new environment configurations or unseen sequences of states and actions. Models are trained for 100 epochs (200 for Atari games) using the Adam optimizer with a learning rate of 5 \u00b7 10 \u22124 and a batch size of 1024. Qualitative results for grid world environments and the 3-body physics environment are presented, with additional results available in the appendices. In grid world environments, C-SWM discovers object-specific filters without direct supervision. Each object is represented by two coordinates corresponding to the true object position. Abstract state transition graphs are learned for objects in different scenes, capturing the underlying grid structure. The abstract state graph captures the grid structure of the environment, including object-specific latent states and transitions. The model embeds object trajectories and predicts transitions, but struggles with certain actions and deviations from typical trajectories. Orange nodes represent starting states, green links are ground truth transitions, and violet links are model-predicted transitions. The model struggles to predict correct transitions in the abstract state graph, capturing the grid structure of the environment. The latent representation accurately captures object-specific information and the effects of actions. The transition model correctly identifies if an action has an effect, such as a blocked neighboring position. Object-specific encoders and latent representations track location and velocity effectively in the 3-body physics environment. The text discusses representations tracking location and velocity of objects, evaluating object discovery and transition model quality. Results are compared against baselines using different approaches, with a focus on generalization to unseen environments. Performance is measured in latent space with ranking scores reported in Table 1. Baselines utilizing reconstruction losses are examined. Results in Table 1 show that models using reconstruction losses in pixel space, like C-SWM without contrastive loss, struggle to generalize to new scenes. The latent space configuration hinders the transition model's learning. C-SWM performs well in recovering this structure, especially in grid-world environments, where it models latent transitions effectively by considering interactions between object representations. The model struggles with pairwise interactions and generalizing to unseen environments. Variants of grid-world environments are explored to test robustness. Results in Atari 2600 experiments show high variance and difficulty in making long-term predictions. Optimal object slots value (K) varies based on the game, with C-SWM performing best with a single object slot in Atari Pong. In experiments testing robustness, C-SWM excels at short-term predictions in 3-body physics, with a slight edge in 10-step predictions. The PAIG baseline underperforms with recommended hyperparameters. A feed-forward CNN architecture is used for object extraction but cannot disambiguate multiple instances of the same object. The C-SWM architecture excels at short-term predictions in 3-body physics and has a slight edge in 10-step predictions. It uses a feed-forward CNN for object extraction but struggles with disambiguating multiple instances of the same object in a scene. To address this limitation, an iterative disambiguation procedure or dynamic routing approach may be needed. Structured world models (C-SWMs) offer advantages over pure connectionist methods by allowing for stronger inductive biases for generalization. They utilize additional structure to create object-oriented models that generalize better to unseen situations, making effective use of this added complexity. C-SWMs offer advantages over connectionist methods by creating object-oriented models for better generalization. They aim to inspire future work beyond autoencoder-based approaches and address limitations. Visualizations show abstract state transition graphs for the 3D cubes environment and discovered object representations in the 2D shapes dataset. The abstract state transition graphs for a trained C-SWM model on the 3D cubes environment show successful representation of objects despite visual differences. The SWM model without contrastive loss struggles with generalization in unseen environments. In the 3D cubes environment, the C-SWM model successfully represents objects despite visual differences. However, in Atari 2600 environments, latent object representations are less interpretable due to objects having different roles, actions affecting multiple objects indirectly, and multiple objects sharing visual features. See examples in Atari Pong and Space Invaders. Model comparison in pixel space on a hold-out test instance of the 2D shapes environment shows the effectiveness of the C-SWM model compared to the World Model baseline. In a comparison of C-SWM and World Model baseline models in pixel space on the 2D shapes environment, separate decoders were trained for both models. The C-SWM model outperformed the World Model (AE) baseline, showing almost perfect performance in the task. The World Model (AE) baseline often makes mistakes in object positions over time, but still performs well in 1-step prediction. Ranking loss in latent space captures this behavior accurately, unlike mean-squared error in pixel space. Researchers are encouraged to use ranking-based evaluations in latent space. Comparing C-SWM and World Model baseline models in pixel space on the 2D shapes environment, C-SWM outperformed the World Model (AE) baseline. The full-hinge loss in latent space improves results by focusing on the negative energy term, potentially due to the ease of minimizing the loss by increasing embedding norms. Additional constraints like unit-hypersphere embedding could address this issue. In this work, the authors experimented with assigning multiple feature maps per object slot to improve performance on the Space Invaders environment. Results showed no clear advantage in using multiple feature maps for this environment, but it may be beneficial for more complex environments. The authors experimented with two variants of a 2D shapes grid-world environment with five objects to evaluate the robustness of their approach. One variant included a 'no-op' action for each object, while the other variant had one object receiving a random action at every turn to test the model's behavior in the presence of random actions. Adding randomness in the environment poses a challenge for the deterministic model. Results show that a randomly moving object reduces predictive performance. Stability in object representations varies between runs, especially in more complex environments like Atari. Results show that the C-SWM model's training time is comparable to the World Model baseline, with both models training for approximately 1-2 hours on different datasets using a single GPU. The Atari models trained for less than 20 minutes, except for the PAIG baseline model. The PAIG baseline model trained for approximately 6 hours on a NVIDIA TitanX Pascal GPU using recommended settings. State observations are provided as 50x50x3 tensors with RGB color channels, actions as a 4-dim one-hot vector, and only a single object receives an action per time step. For Atari environments, a copy of the one-hot encoded action vector is provided to every object slot. In a 5x5 grid world with 5 different objects, each object is represented by a unique shape/color combination and can be moved along cardinal directions. Pairwise interactions between object properties need to be considered for accurate action predictions. In a block pushing environment using Matplotlib, the model's robustness to partial occlusion and perspective changes is investigated. A visually challenging task is created with different perspectives and occlusions. Additionally, a small environment based on Atari Pong is implemented, focusing on the interaction between the ball and the player-controlled paddle. The first 58 environment interactions are discarded in this setup. In the PONGDETERMINISTIC-V4 variant of the OpenAI Gym environment, the first 58 interactions are discarded. A random policy is used to populate an experience buffer with 10 interactions per episode. Observations consist of two cropped and resized frames. The SPACEINVADERSDETERMINISTIC-V4 variant of the Atari 2600 game Space Invaders is processed similarly to Pong. The first 50 interactions are discarded for each episode in this environment. The environment described is adapted from Jaques et al. (2019) with specific parameters set. It involves concatenating two frames to provide velocity information and scoring based on predicted state representation proximity to true observation. The environment involves ranking reference state representations by distance to the true state representation using different CNN architectures for different environments. The model architecture includes a layer with 16 feature maps followed by a 1x1 convolution with 5 feature maps. A sigmoid activation function is used for object masks. The architecture is consistent across different environments with varying filter sizes and output dimensions. The object encoder consists of an MLP with two hidden layers of 512 units each, followed by ReLU activation functions and LayerNorm. The final output layer is 2-dimensional, reflecting the object state in 2D and velocity. The GNN-based transition model uses MLPs with two hidden layers of 512 units each. The margin in the hinge loss is \u03b3 = 1, and the Euclidean distance in the loss function is multiplied by a factor of 0.5/\u03c3 2 with \u03c3 = 0.5. The World Model baseline is trained in two stages with an autoencoder or VAE with a 32-dim latent space. The encoder in the autoencoder or VAE has the same architecture as the object extractor in the C-SWM model, followed by an MLP. The decoder mirrors this architecture with deconvolutional layers. Binary cross entropy was found to be more stable for optimization and produced better results in reconstruction. In the experimental section, the model parameters of the auto-encoder are frozen and a transition model is trained with mean-squared error on latent representations. The transition model, in the form of an MLP, predicts the latent state difference for better results. The World Model baselines are trained with a smaller batch size of 512 for improved performance. In the experimental section, the World Model baselines are trained with a batch size of 512 for improved performance. Ablations are performed to investigate the necessity of a structured transition model and state factorization. The Physics-as-Inverse-Graphics (PAIG) baseline is trained with standard settings on the dataset provided by the authors. The authors used the PhysicsNet model with specific parameters for the task, including input size of 50x50x3, input steps=2, pred steps=10, and extrap steps=0. The model was trained with four random seeds and evaluated by extracting position representations for all test frames. Velocity information was incorporated by calculating the difference between consecutive position representations. The authors utilized the PhysicsNet model with certain parameters for the task. They obtained a 12-dim representation for each time step by combining object representation with position representation. One training run collapsed all latent representations to a single point, which was excluded from the reported results."
}