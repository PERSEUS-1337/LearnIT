{
    "title": "S1GALrBWtN",
    "content": "This paper discusses the automated learning of Hierarchical Goal Networks in nondeterministic domains, focusing on hierarchical problem-solving representations in various domains such as navigation tasks, reinforcement learning, and abstraction planning. Hierarchical Task Network (HTN) planning involves decomposing complex tasks into simpler subtasks using actions and HTN methods. HTN planners like SHOP and SHOP2 have shown significant performance improvements over standard planners due to their ability to recursively decompose tasks. HTN planners show performance gains by exploiting domain-specific knowledge in various domains such as military planning, computer games, manufacturing, project planning, and UAV planning. However, HTN planning has a representational flaw related to the notion of tasks, which are defined as activities to be performed and syntactically represented as logical atoms. HTN planners rely on domain-specific knowledge for performance gains in various fields. However, a flaw in HTN planning lies in the lack of explicit semantics for tasks, which are represented as logical atoms. This leads to challenges in execution monitoring and the need for a complete set of tasks and methods for every possible scenario. Hierarchical Goal Networks (HGNs) address the limitations of HTN planners by directly representing goals at all levels of the hierarchy. This allows for goal fulfillment to be checked against the current state, unlike tasks in HTN planning. Hierarchical Goal Networks (HGNs) directly represent goals at all levels of the hierarchy, allowing for goal fulfillment to be checked against the current state. HGN methods can decompose goals into subgoals, making it easier to determine if a goal has been achieved. HGNs also relax the complete domain requirement of HTN planning, allowing for incomplete methods to still generate solution plans using standard planning techniques. Well-crafted HGN methods can significantly improve performance over standard planning techniques. Hierarchical Goal Networks (HGNs) offer an improvement in performance over standard planning techniques by directly representing goals at all levels of the hierarchy. When the HGN domain is complete, its expressiveness is equivalent to Simple Hierarchical Ordered Planning, a variant of HTN planning used by popular HTN planners like SHOP and SHOP2. SHOP requires a total order of tasks, while SHOP2 allows partial-order, making it the preferred choice. This work proposes automated learning of HGNs for ND domains, building on previous work on learning HTNs for deterministic domains. Researchers have explored other ways to address the limitation associated with the lack of tasks' semantics, such as TMKs (Task-Method-Knowledge models) which require the semantics of tasks as pairs of preconditions and effects. However, this may increase the knowledge engineering requirement of HTNs. To deal with incomplete HTN domains, methods can be translated into a collection of actions for standard planning techniques to be used, but there are limitations to this approach. The limitations of translating HTN methods into actions for standard planning techniques include incomplete translation in many domains and exponential growth in the number of actions. Hybrid planning approaches like HGNs combine HTN and standard planning, eliminating tasks while maintaining the expressiveness of Simple Hierarchical Ordered Planning. Research has focused on learning hierarchical planning knowledge, with methods like ICARUS using skills represented as Horn clauses to learn HTN methods. The crucial step in learning hierarchical planning knowledge involves using a standard planner to fill gaps in HTN planning knowledge. For instance, if there is no HTN knowledge on how to move a package from location L1 to L2, a standard planner can generate a plan for this task. Skills are then used to learn new HTN methods from the generated plan. Another approach, HTN-Maker, uses task semantics to identify action sequences that achieve tasks and learn task hierarchies. HTN-Maker learns incrementally by identifying action sequences that achieve tasks and learning task hierarchies. HTNLearn BID70 transforms input traces into a constraint satisfaction problem, using (preconditions, effects) as task semantics. Input traces are converted into constraints, with a MAXSAT solver determining the truth value for each constraint, adding preconditions accordingly. Hierarchical reinforcement learning uses hierarchical decompositions similar to HTN planning. Hierarchical goal networks (HGNs) offer an alternative representation to HTNs, decomposing goals at every level of the hierarchy. Hierarchical goal networks (HGNs) decompose goals at every level of the hierarchy, similar to HTN methods. HGNs can fallback to STRIPS planners if the domain description is incomplete. Total-order HGNs are as expressive as total-order HTNs, while the partial-order variant is as expressive as partial-order HTNs. Inductive learning is used to learn goal-subgoal relations in X-learn, similar to learning macro-operators. SOAR learns goal-subgoal relations using annotated behavior trace structures. In our proposed work, we are learning Hierarchical Goal Networks (HGNs) without annotating input traces. Goal regression techniques are used to generate plans recursively by identifying subgoals that must be achieved. The goal-subgoal relations resulting from goal regression are a direct consequence of the domain's operators. In learning Hierarchical Goal Networks (HGNs), the hierarchies of goals represent relations between HGN methods and capture domain-specific knowledge not explicitly represented in actions. This work is related to learning context-free grammars (CFGs) by eliciting production rules from strings. The Greedy Structure Hypothesizer (GSH) BID35 uses probabilistic context-free grammars learning techniques to learn a hierarchical structure of input plan traces. GSH focuses on reflecting user preferences rather than generating grammars for planning. Learning CFGs involves finding production rules to generate a training set, while learning HGNs requires understanding task decomposition and domain-specific knowledge. Learning HGNs involves understanding task decomposition and preconditions, which are more expressive than CFGs. HTNs are similar to context-sensitive grammars, constraining when a decomposition can occur. Planning landmarks, related to the notion of planning, are used in automated planning to identify key actions or states in a solution plan trace. Planning landmarks, such as LAMA and HGN planner GoDel, are used in automated planning for tasks like FOND planning. In FOND domains, actions can have multiple outcomes, like in the Minecraft simulation. HTN learners like ND-HTNMaker use (preconditions,effects) pairs to define tasks and pinpoint task fulfillment locations in traces. ND-HTNMaker enforces a right recursive structure to handle nondeterminism in task decomposition. This structure consists of one primitive task followed by none or one compound task, ensuring correctness in learned methods. ND-SHOP BID33 is a solution for planning in nondeterministic domains, where actions may have multiple outcomes. BID7 proposed categorizations for solutions in such domains, distinguishing between weak, strong cyclic, and strong solutions. A solution is represented as a policy mapping possible states to actions, indicating what action to take for a given state. In planning for nondeterministic domains, a solution policy \u03c0 maps states to actions. A weak solution guarantees reaching a goal state sometimes, like in a Minecraft simulation where assumptions may not cover all possible outcomes. Fairness assumption requires actions to exhibit all effects infinitely often. A strong solution \u03c0 in planning for nondeterministic domains ensures that every terminal state meets the goals and allows for multiple visits to the same state if needed. Strong solutions are preferred as they avoid revisiting the same state, but in some cases, like in Minecraft simulations, they may not exist. In Minecraft simulations, strong solutions may not exist as monsters can repeatedly parry attacks. In the robot navigation domain, a robot navigating between offices may need to repeatedly open closed doors due to nondeterminism. Solving nondeterministic planning problems is challenging due to the explosion of states. Adding domain-specific knowledge can help counter this issue. Hierarchical planning techniques outperform state-of-the-art nondeterministic planner in robot navigation domain, showing significant speedups or solving previously impossible problems. No assumption of probability distribution over actions' outcomes. Hierarchical reinforcement learning can be used to learn probability distribution over goal decompositions. Proposal to learn bridge atoms and hierarchical structure encoding domain-specific knowledge. The learned hierarchical structure must encode the domain's nondeterminism in a sound way. For example, in the logistics transportation domain, there are two possible outcomes when loading a package into a vehicle in a location. Traces will bring the package to the airport, transport it by air to the destination city, and deliver it. To handle nondeterminism correctly, a right-recursive approach is proposed. To handle nondeterminism in learned Hierarchical Goal Networks (HGNs), a right-recursive structure is proposed for lower echelons. This approach combines well with higher decompositions, such as identifying goals like reaching the airport for a package in the transportation domain. The aim is automated learning of HGN methods, including goals, goal-subgoal structures, and applicability conditions. The learning problem involves obtaining correct HGN methods from a set of actions and traces generated from those actions. The correct collection of HGN methods involves methods that generate correct plans for given initial states and goals. An HGN method is defined by its method head, preconditions, and subgoals. An example in the logistics transportation domain shows how an HGN method recursively decomposes goals into subgoals. The HGN method in the logistics transportation domain recursively decomposes goals into subgoals, achieving the final goal by delivering packages to different locations. The HGN method in logistics transportation domain involves maintaining a list of open goals and recursively decomposing them into subgoals. Planning involves choosing goals from the list and either decomposing them further or achieving them through actions. The planner extends this procedure to use standard planning techniques and allow partial ordering between subgoals. The proposed method involves transforming the problem of identifying goals and learning their hierarchical relation into finding relations between word embeddings extracted from text. Plan traces are viewed as sentences, with actions and atoms represented as words in the sentence. Word embeddings are vectors representing words in a multi-dimensional vector space. The text discusses word embeddings, which are vectors representing words in a multi-dimensional space. Various algorithms, like Word2Vec, use vector similarity based on word cooccurrence in text. Word2Vec uses a neural network with a context window to generate these representations. Word2Vec uses dynamic windows to compute similarity with cosine similarity, measuring the orientation of resulting vectors. The change from plan elements to word embeddings is unsupervised, generating vector representations based on context, such as the dynamic window W in Word2Vec. The vector representations of plan elements are generated based on proximity in traces, clustered into groups. Bridge atoms are identified as ideal goals connecting clusters. Establishing a bridge atom hierarchy involves recursively splitting texts around bridge atoms. Training a Word2Vec model on the corpus determines word vectors. We use a Word2Vec model to train on a corpus, determining word vectors and clustering them with Hierarchical Agglomerative Clustering. The number of clusters is currently limited to two, with the cosine distance of each atom in a cluster calculated and the word with the shortest average distance selected. Bridge atoms are identified to connect clusters, and traces are split around them to form new sub-corpora for further analysis. We recursively select bridge atoms to divide traces into sub-corpora based on hierarchical relationships. Short sections are discarded if they fall below a threshold. Methods are built based on the hierarchy, with single-action or multi-action sections forming the lowest level. Each method has subgoals before and after a bridge atom, annotated with preconditions. The preconditions of actions in a plan trace are determined by regressing over the actions in reverse. A variant of the Pyhop HTN planner is used, introducing nondeterminism in actions. Experiments are conducted in a logistics domain where packages are relocated using trucks and airplanes. Nondeterminism is introduced through load and unload operators with two possible outcomes. In a logistics domain, packages are relocated using trucks and airplanes with load and unload operators having two outcomes. Rockets transport packages between cities on different planets. Word2Vec identifies common bridge atoms across texts for package relocation. Increasing epochs and lowering learning rate helps learn bridge atoms from fewer texts. A reasonable first bridge atom involves the package and a rocket or launchpad for transportation. In a logistics domain, packages are relocated using trucks and airplanes with load and unload operators having two outcomes. Rockets transport packages between cities on different planets. Word2Vec identifies common bridge atoms across texts for package relocation. Increasing epochs and lowering learning rate helps learn bridge atoms from fewer texts. Our first bridge atom is the action unload(package, rocket) from a corpus of 700 traces, with 1000 epochs and a learning rate of 0.00025. The trace structure influences the bridge atom hierarchy, with two different variants of state expression within traces being experimented on."
}