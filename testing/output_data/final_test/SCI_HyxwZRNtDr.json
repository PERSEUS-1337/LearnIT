{
    "title": "HyxwZRNtDr",
    "content": "Reinforcement learning algorithms often over-fit to training environments, limiting their real-world application. This paper introduces $\\text{W}\\text{R}^{2}\\text{L}$, a robust algorithm with strong performance on low and high-dimensional control tasks. The method formalizes robust reinforcement learning as a min-max game with a Wasserstein constraint for a convergent solver. Additionally, an efficient and scalable solver is proposed using a novel zero-order optimization method that can be beneficial for numerical optimization in general. We demonstrate significant gains compared to standard algorithms on high-dimensional MuJuCo environments in reinforcement learning. Robustness to changes in transition dynamics is crucial for adaptive and safe RL in real-world environments. Recent literature has proposed various algorithms for robust decision-making, borrowing from game theory to analyze worst-case deviations of agents' policies and environments. In this paper, a generic framework for robust reinforcement learning is proposed, capable of handling both discrete and continuous state and action spaces. The algorithm, called Wasserstein Robust Reinforcement Learning (WR2L), aims to find the best policy by considering worst-case dynamics among candidate dynamics in a set defined by the average Wasserstein ball around a reference dynamics P0. This approach shows improvements in performance across various disturbances such as action uncertainties and dynamical model variations. The algorithm in this paper, called Wasserstein Robust Reinforcement Learning (WR2L), aims to find the best policy by considering worst-case dynamics within a set defined by the average Wasserstein ball around a reference dynamics P0. The policy and dynamics are parameterized, with the policy parameters \u03b8k possibly being the weights of a deep neural network, and the dynamics parameters \u03c6j being the parameters of a simulator or differential equation solver. The algorithm performs estimated descent steps in \u03c6 space and updates policy parameters in \u03b8 space. Access to a simulator with parameterizable dynamics is required for this approach. The algorithm WR2L aims to find the best policy by considering worst-case dynamics within a set defined by the average Wasserstein ball around a reference dynamics P0. Access to a simulator with parameterizable dynamics is required, but the actual reference dynamics P0 need not be known explicitly. The algorithm is in the \"RL setting\" not the \"MDP setting\", where transition probabilities are known a priori. Our contribution frames the robust learning problem in terms of dynamics uncertainty sets defined by Wasserstein distance. Our formulation is suitable for high-dimensional, continuous state and action spaces in the context of MDPs. The solution approach is novel, effective, and does not require extensive model or domain knowledge. It is computationally efficient as it does not build a model of the dynamics and utilizes automatic differentiation engines for matrix operations. The formulation is for MDPs with high-dimensional, continuous state and action spaces. It involves defining state transition probability, reward function, and discount factor. The agent selects actions based on a policy, collects trajectories, and aims to find an optimal policy by maximizing the total accumulated reward. The approach is computationally efficient and does not require extensive domain knowledge. The Wasserstein distance is used to quantify variations in transition density from a reference distribution in MDPs with high-dimensional, continuous spaces. It has desirable properties such as symmetry, flexibility in comparing different types of measures, and consideration of underlying geometry. The Wasserstein distance is defined as the p'th distance between two probability measures on a metric space, with desirable properties for constraining dynamics in a certain way. It quantifies variations in transition density from a reference distribution in MDPs with high-dimensional, continuous spaces. The Wasserstein distance quantifies variations in transition density in MDPs with continuous spaces. It formalizes assumptions about systemic errors and model accuracy. Deep neural networks are used to parameterize policies in this work. In optimizing hyperparameters, policies can be tailored to specific environments like MuJoCo or high-dimensional states. Deep networks can also be used to parameterize dynamics models, but this approach can lead to agents discovering unrealistic transition models. This issue is more pronounced in high-dimensional settings where the number of potential minimizers increases significantly. In high-dimensional settings, modeling physics-based dynamics using deep networks can lead to unrealistic transition models. Some researchers propose incorporating Lagrangian mechanics or modeling dynamics directly with differential equation structures. This paper explores perturbing differential equation solvers and simulators with dynamic specification parameters to reduce parameter space dimensions and ensure valid dynamics. In high-dimensional settings, modeling physics-based dynamics using deep networks can lead to unrealistic transition models. This paper explores perturbing differential equation solvers and simulators with dynamic specification parameters to reduce parameter space dimensions and ensure valid dynamics. A new method for valid and accurate model updates is developed in Section 4, addressing challenges related to computing gradients and Hessians of black-box solvers. Robust reinforcement learning is defined as learning best-case policies under worst-case transitions, with a trajectory density function parameterized by policies and transition models. Our formulation allows for parameterised classes of transition models without additional restrictions on structure or scope. Constraints are introduced to ensure well-behaved optimization objectives by bounding search spaces and ensuring convergence to feasible transition models. The method assumes access to samples from a reference dynamics model and bounds learnt transitions in an -Wasserstein ball around the reference model. The algorithm operates successfully with traces from P0 and specification parameters, allowing for a more flexible framework in reinforcement learning. The set in Equation 6 introduces infinite constraints for continuous state and action spaces, leading to a relaxed version with a bounded average Wasserstein distance constraint. Sampling trajectories is done using reference dynamics P0 and a policy \u03c0. The algorithm samples trajectories using reference dynamics P0 and a policy \u03c0 that selects actions uniformly at random. Wasserstein distance is estimated using empirical data, assuming dynamics are deterministic functions with Gaussian noise. Gaussian distributions simplify estimation in high dimensions, with closed-form expressions for Wasserstein distance. The algorithm estimates Wasserstein distance using empirical data and closed-form expressions for high-dimensional Gaussian distributions. The optimization problem in Wasserstein Robust Reinforcement Learning alternates between updating policy and dynamics parameters, respecting Wasserstein constraints. Approximations are proposed to simplify the computation of constraints. To alleviate the difficulty in computing the constraint, an approximation using a second-order Taylor expansion is proposed. The optimization problem for determining model parameters with fixed policies involves solving quadratic constraints efficiently using interior-point methods. The optimization problem involves determining model parameters with fixed policies using a closed-form solution in Algorithm 1, operating in a descent-ascent fashion. The optimization problem involves determining model parameters with fixed policies using a closed-form solution in Algorithm 1, operating in a descent-ascent fashion. Equation 10 ensures learning rates abide by a step size condition, with the second phase adapting policy parameters using reinforcement learning methods. The termination condition for the inner loop is left to the user, such as a time-out or gradient norm below a threshold. Phase I updates model parameters while fixing the policy, while Phase II updates the policy given new model parameters using standard reinforcement learning algorithms. Phase II involves updating the policy based on new model parameters using standard reinforcement learning algorithms. A simulator with dynamics parameterized by a real vector can be used to generate physics-grounded transitions by altering parameters and executing actions. This approach ensures valid transitions and scalability, as specification parameters typically reside in lower dimensional spaces compared to deep network weights. Gradient estimation is necessary for updating model parameters in Phase I. In Phase I of Algorithm 1, the gradient of the loss function with respect to the environment dynamics is estimated using a Gaussian distribution. The zero-order gradient estimate is computed for fixed parameters, and Hessian estimation is generalized. The Hessian estimator is performed once before executing Algorithm 1. The Hessian of the Wasserstein distance around \u03c6 0 can be estimated based on function evaluations, allowing for gradient and Hessian estimates to be based on simulator value evaluations while perturbing \u03c6 and \u03c6 0. An empirical estimate of the p-Wasserstein distance between two measures \u00b5 and \u03bd can be performed by computing the p-Wasserstein distance between the empirical distributions evaluated at sampled data. The algorithm evaluates WR 2 L on various continuous control benchmarks from the MuJoCo environment, demonstrating superior performance compared to standard and robust reinforcement learning algorithms. It focuses on policy generalization across environments with different dynamics, measuring average test returns on novel systems. Comparisons against standard and robust algorithms help understand the importance of robustness in sequential decision making. The study compares WR 2 L algorithm against standard algorithms like PPO and TRPO, as well as robust algorithms like RARL and PR-MDP. Results are presented in Appendix B.2, with DDPG omitted due to lack of significant robustness performance. Results from experiments on simple systems like the inverted pendulum showed that deep neural networks failed to provide valid physics-grounded dynamics. This issue was more pronounced in high-dimensional systems like Hopper and Walker due to the increased number of possible minima. The study introduces a zero-order method as a scalable alternative for robust solutions, evaluated in low and high-dimensional MuJuCo tasks. Systems such as CartPole, Hopper, and Walker2D were considered, all requiring direct joint-torque control. The study introduces a zero-order method for robust solutions in low and high-dimensional MuJuCo tasks, including CartPole, Hopper, and Walker2D, all requiring direct joint-torque control. The method utilizes existing frameworks without modifications and demonstrates robustness in one-dimensional and two-dimensional simulator variations. The results show that WR 2 L outperforms both robust and non-robust algorithms in one-dimensional variations, while PPO trained on dynamics sampled uniformly from the Wasserstein constraint set displays more robustness than when trained on just the reference dynamics. Our method, WR 2 L, outperforms robust and non-robust algorithms in high dimensions. A gradient-based optimization scheme is more efficient for high-dimensional changes compared to sampling-based methods. Our method, WR 2 L, outperforms robust and non-robust algorithms in high dimensions by altering a few parameters while maintaining suitability. Two additional experiments were conducted on the Hopper and HalfCheetah benchmarks, showing improvements in test returns with carefully chosen degrees of freedom. Our method, WR 2 L, outperforms others in high-dimensional simulator variations by allowing modifications to all parameters, resulting in higher returns across all systems. Figures demonstrate the effectiveness of our method on the Hopper and HalfCheetah benchmarks, with test returns improving significantly. PPO trained with dynamics sampled from the Wasserstein constraint set also shows promising results in the two-dimensional variation case. Our method, WR2L, outperforms others in high-dimensional simulator variations by allowing modifications to all parameters, resulting in higher returns across all systems. Previous work on robust MDPs is not sufficient for the RL setting due to the need for efficient solutions for large state and action spaces. Rajeswaran et al. (2017) introduced Ensemble Policy Optimisation (EPOpt) which alternates between training on a distribution of dynamics and gathering data from the deployed environment. Our method, WR2L, outperforms others in high-dimensional simulator variations by allowing modifications to all parameters, resulting in higher returns across all systems. Rajeswaran et al. (2017) introduced Ensemble Policy Optimisation (EPOpt) which alternates between training on a distribution of dynamics and gathering data from the deployed environment. The algorithm samples dynamics from a distribution and updates the policy parameter based on the worst performing trajectories. Our algorithm takes descent steps in the dynamics space, performing well even in high dimensions. The CVaR criterion is adopted in Pinto et al. (2017), where two policies are trained simultaneously: a \"protagonist\" optimizing performance and an adversary disrupting it. Tessler et al. (2019) studied robustness to action perturbations in PR-MDP and NR-MDP forms. In PR-MDP, a different action is taken with probability \u03b1, while in NR-MDP, noisy actions are introduced. In NR-MDP, perturbations are added to actions, suitable for deep neural networks. Experiments were conducted on InvertedPendulum, Hopper, Walker2d, and Humanoid. Comparison with PR-MDP showed PR-MDP lacking in robustness. Lecarpentier & Rachelson (2019) considered a non-stationary Markov Decision Process model with dynamics changing between time steps, using a Minimax algorithm based on Wasserstein distance constraints. The Risk Averse Tree Search algorithm, based on the Minimax algorithm, projects possible future dynamics using snapshots of the evolving MDP. It utilizes the Wasserstein distance to quantify variations in dynamics and is suitable for deep neural networks in continuous state and action spaces. The algorithm does not require full dynamics, only parameterizable dynamics, and competes well with existing methods. In this paper, a robust reinforcement learning algorithm using Wasserstein constraints for policies is proposed. It outperforms others in test returns on unseen dynamics and demonstrates superior performance in low and high-dimensional MuJuCo environments. Future work includes considering robustness in other components of MDPs and implementing the algorithm on real hardware for sim-to-real experiments. The paper proposes a robust reinforcement learning algorithm using Wasserstein constraints for policies, showing superior performance in MuJuCo environments. The algorithm outperforms others in test returns on unseen dynamics. Future work includes implementing the algorithm on real hardware for sim-to-real experiments. The text discusses the analysis of diagonal and off-diagonal elements of matrix B, with conclusions drawn based on the results. The CartPole benchmark is used to illustrate the dynamics parameterization. The CartPole benchmark involves balancing a pole on a cart by driving it along a rail, with termination conditions based on deviations from upright position or initial cart position. Robustness experiments vary the pole length parameter. In the Hopper benchmark, the agent controls a robot to move forward without falling, with early-stopping for \"unhealthy\" states. The dynamics of different benchmarks were tested by varying parameters such as densities, armature, damping, and friction coefficient. Robustness experiments were conducted by varying torso densities and all dimensional specification parameters. Walker2D and Halfcheetah benchmarks involved similar controlled systems with variations in dynamics and parameters. The simulator parameters for the cheetah robot include 21 dimensions, with 7 representing densities. Experiments involved varying torso-density and floor friction in two-dimensional tests, while high-dimensional tests allowed algorithm control of all 21 variables. Training phases utilized Algorithm 1 to determine robust policies and update transition model parameters. Policies were represented using parametrised Gaussian distributions with means from a neural network and standard derivations from free parameters. The neural network used two hidden layers with 64 units and hyperbolic tangent activations. A critic network with the same structure as the policy was trained. Policy updates collected transitions from worst-case dynamics, with transitions ranging from 5,000 to 10,000. Proximal policy optimization with generalised advantage estimation was used to optimize the policy. Sampling dynamics from a Gaussian distribution centered at the worst-case dynamics model was done to solve the minimization problem. The study utilized a Gaussian distribution centered at the worst-case dynamics model for sampling dynamics and estimating gradients for model updates. Training was terminated when policy entropy dropped below a threshold, and performance was evaluated on unseen dynamics. Policies were tested on various tasks, and the robustness of policies was compared with standard policy gradient methods and robust RL algorithms. In a benchmark comparing WR 2 L with various baselines, including DDPG, WR 2 L outperformed all others. The study also presented a closed form solution to an optimization problem in Section 3.2."
}