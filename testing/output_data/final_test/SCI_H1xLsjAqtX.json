{
    "title": "H1xLsjAqtX",
    "content": "The paper presents a generic framework for robust text classification achieving comparable accuracy to full models at test-time. Our approach focuses on dynamically deleting unimportant words to reduce computational complexity for text classification tasks. We also introduce a new data aggregation method for training classifiers on fragmented word sequences, achieving state-of-the-art performance with linear complexity scaling. Additionally, our framework allows for dynamic tuning of classifier budget levels at inference time. Recent advancements in deep neural networks have enhanced natural language processing tasks like document classification and sentiment analysis. Our method proposes a novel test-time prediction approach for efficient text classification on long documents by mitigating sequential processing. It consists of a selector and a classifier that dynamically delete unimportant words and choose important ones. This method aims to reduce computational complexity and improve processing speed for tasks like document classification and sentiment analysis. The method involves a selector and classifier that delete unimportant words and choose important ones to improve text classification efficiency. The challenge lies in handling fragmented sentences and incompatible inputs for the classifier, which requires joint training of selector and classifier functions. The proposed solution involves a data aggregation framework (DAG) that augments the training corpus with outputs from selectors at different budget levels. By training the classifier on the aggregated structured blank-out text, the classifier learns to fuse fragmented sentences into a feature representation that mirrors the representation obtained on full sentences, leading to high accuracy in text classification. The proposed framework involves a selector to choose relevant words for the task, which are then processed by a classifier. Different approaches for speeding up DNNs have been proposed, such as LSTM-jump and skim-RNN, which modify existing RNN/LSTM architectures. In contrast, the framework modifies the training method of existing classifier architectures. Our framework, inspired by BID11, uses a selector and classifier for text classification, focusing on speed-up. Unlike traditional methods like stop-word removal, we directly learn a selector for sparse features. Our framework introduces a selector to choose salient words for text classification, which are then input to a classifier like LSTM. This approach differs from traditional feature selection methods and focuses on improving classifier performance by aggregating the output from the selector. DAGGER updates its policy iteratively through training a classifier in different reinforcement learning contexts. Our blank-out datasets are derived from training data, aggregated once to create a collection of fragmented sentences. Budgeted learning methods focus on different applications than ours. Our goal is to build a robust classifier with selectors for good performance under test-time budgets. Our framework aims to train a classifier C and selectors S b such that the generated sub-sequence of words by the selector is sufficient to make accurate predictions on the output label. The test-time budget b controls the trade-off between speed and accuracy, different from existing frameworks. Our framework aims to train a classifier C and selectors S b for different budgets. Storing multiple classifiers for various budgets is impractical due to the large parameter size. We propose a data aggregation framework (DAG) to train the classifier C on outputs from different selectors.Selectors are trained without explicit annotations on word selection. The data aggregation framework (DAG) trains a classifier C on outputs from different selectors with varying budget levels. Selectors are trained without explicit annotations on word selection, and the framework generates an aggregated corpus T by applying each selector on the training set. Classifier CT is then trained on the aggregated corpus T, allowing it to make predictions with different budget levels. The training framework involves aggregating selections from multiple selectors to improve the robustness of the trained classifier. This can be done at the word, phrase, or sentence level to enhance accuracy and avoid breaking compound nouns or meaningful phrases. In the training framework, sentence-level aggregation (SAG) is proposed to capture long phrases efficiently. The selector must be computationally efficient and provide informative words for the classifier. Existing works like BID11 do not meet these criteria, as their RCNN selector has high time complexity. In the training framework, sentence-level aggregation (SAG) is proposed to capture long phrases efficiently. The selector must be computationally efficient and provide informative words for the classifier. Existing works like BID11 do not meet these criteria, as their RCNN selector has high time complexity. Two classes of selectors are considered: 1) a selector with word embedding features trained jointly with the classifier using a doubly gradient descent method, and 2) a selector trained by L1-regularized logistic regression with bag-of-words features. Word Embedding (WE) selector uses word embeddings as features to predict if a word should be passed to the classifier, preserving word semantics for semantic-oriented tasks. The text discusses the selection of important words in a document using a selector trained with a classifier. The selector is trained with a classifier to identify informative words independently using word embeddings. The classifier makes predictions based on word sequences sampled from the selector, and the model parameters are denoted as \u03b8C. The classifier minimizes the negative log-likelihood for classification problems. The text discusses using a cross-entropy loss for classification and squared loss for regression problems. It also mentions using 1-regularizers to promote sparsity and continuity in selections, and solving the objective with stochastic gradient descent. Additionally, a bag-of-words selector is considered for identifying important words in a document. The text discusses constructing a feature vector for logistic regression, optimizing the model with a selection budget, and evaluating the approach on text classification datasets. The proposed method is compared with existing budget learning methods, and experiments are conducted on five real-world datasets. The experiments conducted on five datasets include Stanford Sentiment Treebank (SST-2), IMDB, Multi-Aspect, AGNews, and Yelp. Different datasets involve sentiment analysis, movie reviews, news articles, and restaurant reviews. Various methods such as word embeddings, RCNN, and WE selector are applied for data aggregation and evaluation metrics like mean square error (MSE) are used for comparison with other approaches. The framework for text classification evaluation includes word-level aggregation (WAG), sentence-aggregation (SAG), and phrase-aggregation (PAG) schemes. Two neural network architectures, Biattentive Classification Network (BCN) and LSTM, are evaluated using different classifiers. BCN, based on Bi-LSTM, Bi-attention, and Maxout networks, serves as a strong baseline on various datasets. LSTM, widely used for text classification, sequentially processes words in a passage to capture text features. The text discusses evaluating a data aggregation framework with different classifiers such as BCN, LSTM, and RCNN. It compares selectors and data aggregation using various WE selectors with budgets. The evaluation is based on accuracy or error rates. The text discusses evaluating a data aggregation framework with different classifiers such as BCN, LSTM, and RCNN. It compares selectors and data aggregation using various WE selectors with budgets. The evaluation is based on accuracy or error rates. Experiments were conducted with different parameter settings, including filtering out stop-words and using Bag-of-Words selector. LSTM-jump and Skim-RNN performances were not available for the Multi-Aspect dataset. The framework was evaluated with LSTM and BCN models. Our framework is compared with LSTM-jump and Skim-RNN models, showing competitive performance. Skim-RNN is unstable and struggles to balance performance and test-time budget, while our model is more stable and performs well under different budgets. Our model achieves reasonable performance under different budgets and allows for fine-grained annotations. Leveraging sentiment annotations in SST-2, our model achieves high accuracy with speedups for LSTM and BCN. However, using Stop-words for text filtering can lead to a performance drop as important words may be filtered out. Compared to Bag-of-Words, our framework shows better performance, highlighting classifier incompatibility issues. The issue of classifier incompatibility is addressed by training classifiers with a proposed aggregation framework, leading to improved performance. By aggregating fragments selected by selectors, the model emphasizes important words and becomes more robust to noise in input documents. Results show that a simple word embedding selector is competitive with a complex RCNN selector within the data aggregation framework. The text discusses the performance of a word embedding (WE) selector compared to a complex RCNN selector on the Multi-Aspect dataset. The WE selector, when trained with a word-level data aggregation strategy, achieves competitive results with the RCNN selector while requiring only 12% of selected text. Additionally, the WE selector outperforms a standard LSTM classifier on the Multi-Aspect dataset, achieving a lower mean square error (MSE). Our framework outperforms the RCNN selector on the Multi-Aspect dataset with an MSE of 0.01188 using only 28% of text. The WE selector shows similar performance to the RCNN selector on IMDB data, confirming that a simple selector is sufficient for identifying rationales. The model performs best with sentence-level data aggregation, achieving lower error rates than the baseline RCNN model. Despite the RCNN selector's ability to identify important words, its complexity results in a test-time that is 2X higher in all cases. The DNN classifier, acting as a representation learner, is made robust through our data aggregation schema. The proposed framework demonstrates robustness to input distortions, estimating full sentence representations from fragments. Results show improved performance with the classifier trained using DAG, extrapolating features from full-text even with words deleted. The selector's output is interpretable, as shown in examples from the AGNews dataset. The proposed framework shows robustness in identifying key words and phrases for document classification from the AGNews dataset. Results indicate that the RCNN selector is slower than WE due to complexity, while the data aggregation framework (SAG/WAG) performs better with the same test-time budget. Examples include identifying important words like \"Nokia\", \"nuclear\", \"plant\", \"Shane Warne\", and phrases like \"searched by police\" and \"takes six but India established handy lead\". Shane Warne took six wickets as India gained a 141-run lead in the second test. Handset makers, including Nokia, are developing products to protect cell phones from internet viruses. The WE selector identifies key words for document classification efficiently. The WE selector efficiently identifies key words for document classification, with negligible running time. This approach is beneficial for text classification models used in cloud computing settings, where only important words are sent to the cloud server for processing. The proposed budgeted learning framework aims to train a robust classifier under test-time budget constraints by utilizing low-complexity selectors based on word-embedding or bag-of-word models. This approach allows for efficient classification with fragmented input, reducing network latency and bandwidth limitations. Future work includes applying this framework to other text reading tasks and enhancing data aggregation strategies. The proposed framework aims to apply learning to search approaches and improve data aggregation strategies for text reading tasks. Statistics of the datasets are reported."
}