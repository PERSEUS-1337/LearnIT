{
    "title": "rJxMM2C5K7",
    "content": "In distributed training, communication cost is a bottleneck in scaling up processing nodes. Dithered quantization for transmission of stochastic gradients reduces this cost. Training with DQSG is similar to unquantized SGs perturbed by noise. Convergence analysis is simplified compared to other quantization methods. Correlation among SGs computed by workers can reduce communication overhead without performance loss. In distributed training, communication cost is a bottleneck in scaling up processing nodes. Dithered quantization for transmission of stochastic gradients reduces this cost. A simple yet effective quantization scheme, NDQSG, significantly reduces communication without extra information exchange between workers. NDQSG achieves the same quantization variance bound as DQSG with fewer bits. Simulation results confirm the effectiveness of training with DQSG and NDQSG in reducing communication bits or convergence time. Large-scale distributed machine learning has become a viable approach for training deep models due to the increasing size of deep learning problems. However, communication overhead in exchanging gradients or model parameters remains a major bottleneck. Efforts have been made to reduce this overhead, with existing methods categorized into two groups. Efforts to reduce communication bottleneck in large-scale distributed machine learning involve two groups of methods. One group focuses on sparsification, quantization, and compression of gradients to decrease transmission rate. For example, BID15 uses one-bit quantization of stochastic gradients to reduce communication overhead, but may impact convergence rate. Other approaches like using different quantization levels or adaptive quantizers can improve accuracy at the expense of increased communication bits. Additionally, entropy coding algorithms like Huffman coding can further reduce communication bit-rate. BID0 introduced QSGD, which uses probabilistic quantization of gradients to investigate convergence guarantee and trade-offs. The second group of methods aims to alleviate the communication bottleneck by relaxing synchronization between workers, allowing some workers to continue computations while others exchange parameters. Effective scheduling of asynchronous parameter exchange optimizes communication bandwidth and computational power utilization. Examples include DownpourSGD, Hogwild!, Hogwild++, and Stale Synchronous Parallel model. Our work focuses on reducing communication overhead in distributed computations by quantizing and compressing gradients. We introduce dithered quantization and analyze its convergence in stochastic gradient descent algorithms. We also explore exploiting correlations in computed gradients to further reduce communication. In distributed computations, reducing communication overhead is crucial. Our work introduces dithered quantization to compress gradients and analyzes its convergence in stochastic gradient descent algorithms. We also investigate leveraging correlations in computed gradients to further minimize communication. Dithered Quantization involves adding a random signal called dither to the input signal before quantization, improving the statistical behavior of quantization error. The dithered quantizer is defined as x = Q(x + u) - u, where Q is an M-level uniform quantizer with quantization step size \u2206. This method can reduce communication overhead in distributed computations and compress gradients for stochastic gradient descent algorithms. The dithered quantization process involves adding a random signal (dither) to the input signal before quantization. The quantized value x is obtained by subtracting the reproduced random sequence u from Q(x + u). If certain conditions are met, the quantization error e = x - x is uniform over [-\u2206/2, \u2206/2] and independent of the signal x. The dither signal is commonly chosen from a uniform distribution U[\u2206/2, \u2206/2]. This method does not increase the quantization error bound and is beneficial for various applications. The half-dithered quantization method involves applying a dither signal only to the quantizer, not the reconstruction of x. By choosing the dither signal appropriately, the quantization error moments can be made independent of the signal. The relation between dithered quantization, Ternary quantization of BID18, and stochastic quantization in BID0 is explored. The quantizers can be seen as special cases of the half-dithered quantizer. Stochastic quantization adds a dither to the input signal before quantization, affecting the quantization error. The quantization error is not independent of the signal BID6 and its variance depends on the input signal value. The variance of the quantization error varies in the interval [0, 1 4M 2 ] depending on x. Nested quantization is defined as a pair of two quantizers where the fine and coarse quantizers are nested. Nested quantization involves fine and coarse quantizers, with the centers of the coarse quantizer bins being a subset of the fine quantizer bins. In the one-dimensional case, quantization step sizes \u22061 and \u22062 are nested if \u22062 = k\u22061 for a constant integer k > 1. Stochastic gradient g of an objective function L(w) is an unbiased random estimator of the gradient. In distributed training, workers compute stochastic gradients of parameters based on their data, transmit updates to a server or other workers, and receive the average back for further training. In distributed training, workers compute stochastic gradients and use dithered quantization to reduce communication bits without sacrificing accuracy or convergence time. Dithered quantization in distributed training reduces communication bits without sacrificing accuracy or convergence time. The scaled quantization noise is independent from the gradient and uniformly distributed. Adding 1 bit reduces excess variance by a factor of 4. Partitioning the gradient into sub-vectors can reduce variance at the expense of extra communication bits. Partitioning the gradient into sub-vectors can reduce variance in distributed training, but at the cost of extra communication bits. The excess communication bits increase linearly while the excess variance decreases logarithmically. Convergence Analysis: The gradient descent algorithm with dithered quantized stochastic gradients is analyzed for convergence. The quantization noise can improve training of deep models if controlled appropriately. The convergence of DQSGD is similar to ordinary SGD, with proven convergence under certain assumptions. Theorem 4 states that training with DQSGD converges to the solution almost surely under certain conditions. The proposed distributed training scheme with P workers using dithered quantization of SG is summarized in Algorithm 1. Each worker computes the stochastic gradient and quantization index to achieve DQSG. The same random number generator algorithm and seed number are used for reproducibility. The seed number, s p, is updated at all workers and the server during training to prevent repeated random sequences. The convergence time of distributed (DQSGD) algorithm is proven under certain conditions. The algorithm uses a quantization step size \u2206 and workers compute stochastic gradients with variance bound V. The distributed training algorithm using dithered quantization of stochastic gradients involves assigning random seeds to workers, initializing parameters, computing stochastic gradients, generating pseudo-random sequences, computing quantization indices, and updating seeds during training to prevent repeated random sequences. The convergence time of the algorithm is proven under certain conditions with a quantization step size \u2206 and workers computing stochastic gradients with a variance bound V. Nested quantization is proposed for distributed learning, where workers receive average SG and update parameters using preset training algorithms. Correlated signals are efficiently communicated through distributed compression, with nested quantization proven effective for correlated data transmission. The process involves computing quantization indices, reproducing pseudo-random sequences, reconstructing gradients, and updating seed numbers during training. Nested quantization is used for distributed learning, where workers receive average SG and update parameters using preset training algorithms. It involves computing quantization indices, reproducing pseudo-random sequences, reconstructing gradients, and updating seed numbers during training. The quantization process includes generating a random dither, quantizing and encoding the data, and reconstructing the original signal using information provided by y at the receiver. Nested quantization is utilized for distributed learning, where workers receive average SG and update parameters using preset training algorithms. It involves computing quantization indices, reproducing pseudo-random sequences, reconstructing gradients, and updating seed numbers during training. The quantization process includes generating a random dither, quantizing and encoding the data, and reconstructing the original signal using information provided by y at the receiver. In an example of nested quantization, \u2206 1 = 1 and \u2206 2 = 3, with x = \u22124.2 and u = 0.3 as the generated dither. Assuming \u03b1 = 1, the signal to be transmitted is s = Q 1 (\u22123.9) \u2212 Q 2 (\u22123.9) = \u22121. Ambiguity in the signal can be resolved by having access to y = \u22123.4 at the receiver, resulting in x = \u22124.3. This nested quantization scheme reduces the range of quantization indexes from {\u22124, \u22123, . . . , 4} to {\u22121, 0, 1}, increasing accuracy without the need for additional bits. The proposed distributed training using nested dithered quantization involves dividing workers into two groups, with one group using DQSG to provide an initial estimate for the true gradient. This approach aims to overcome the unavailability of the exact gradient in distributed training by utilizing quantization step sizes and worker numbers to control the variance of the averaged DQSG. The workers in P 2 use nested quantizer with specific step-sizes and scale to decode the received Nested Dithered Quantized SG. The receiver uses the average of all SGs received from other workers as side information to compute the SG of each worker. The quantization parameters are determined by Theorem 6, ensuring correct estimation of the SG with high probability. Nested quantization with specific step-sizes and scale is used by workers in P2 to decode the received Nested Dithered Quantized SG. The quantization parameters are determined by Theorem 6 to ensure correct estimation of the SG with high probability. The convergence and number of communication bits used by different learning algorithms based on DQSG and NDQSG are examined for various numbers of workers, compared against baseline methods. In a synchronous setting, the focus is on quantization/compression algorithms to avoid performance degradation from stale gradients in asynchronous updates. Three models are considered, including a fully connected neural network and a Lenet-5 like convolutional network. Entropy coding like Adaptive Arithmetic Coding reduces communication bits close to the entropy limit. Raw communication bits and entropy per worker are compared for different schemes. The communication bits of DQSGD and QSGD are similar, with DQSGD using significantly fewer bits per iteration compared to one-bit quantization. The proposed algorithm outperforms one-bit quantization and is close to non-quantized communication in terms of accuracy. Additionally, the convergence rate of the dithered quantization scheme is compared to baseline methods. The dithered quantization method improves convergence compared to baseline methods, even without analytic proof. As the number of workers increases, the performance gap between quantization methods diminishes. The proposed nested dithered quantizer is compared with other quantization techniques. The nested dithered quantizer is compared with dithered quantization, showing similar learning curves but reduced communication bits by over 30%. In this paper, a nested quantization method called NDQSG is introduced, which reduces communication bits per training iteration without affecting model accuracy. The method is compared to DQSG and shown to perform similarly in terms of accuracy and training speed but with fewer communication bits. The proposed distributed training method is applicable to both synchronous and asynchronous training setups. The nested quantization scheme introduced in this paper can be extended to hierarchical distributed structures. The proof of Lemma 2 shows the quantization points are assigned with the same probability as a stochastic quantizer. Lemma 3 proves the unbiasedness of the method by showing that the error term is independent and uniformly distributed. The variance bound on g is \u03c3^2 / P for P workers. After T iterations of DQSGD with a specific step size, correct decoding occurs with a probability of 1 - p. The probability of correct recovery is 1 - p where \u011d i = g i - (\u03b1e i + (1 - \u03b1^2)z i). After T iterations of DQSGD with a specific step size, correct decoding occurs with a probability of 1 - p where \u011d i = g i - (\u03b1e i + (1 - \u03b1^2)z i)."
}