{
    "title": "ByfbnsA9Km",
    "content": "Neural networks may misclassify inputs close to their training data due to small decision boundary margins. Linear classifiers can also have close decision boundaries with cross-entropy loss. Training on low-dimensional affine subspaces can lead to smaller margins than optimal. Differential training is introduced to improve margins. Differential training improves decision boundary margins in neural networks by using a loss function defined on pairs of points from each class. It shows that linear classifiers trained with this method achieve maximum margin, addressing the issue of adversarial examples caused by cross-entropy loss. Design choices in training neural networks include architecture, loss function, optimization algorithm, and hyperparameters. These choices influence the training outcome and have been extensively studied, except for the choice of loss function in classification tasks. The cross-entropy loss function is commonly used in classification tasks for neural networks due to its theoretical backing and practical success. Recent work has analyzed the dynamics of training linear classifiers with this loss function, showing little motivation to search for alternatives. Most software for neural networks includes an efficient implementation of the cross-entropy loss function. The cross-entropy loss function is commonly used in classification tasks for neural networks. Recent work analyzed the dynamics of training linear classifiers with this loss function, showing little motivation to search for alternatives. However, experiments showed that the decision boundary from cross-entropy minimization can have a poor margin compared to the hard-margin support vector machine solution. The dynamics of gradient descent with cross-entropy loss for a linear classifier can lead to a decision boundary almost orthogonal to the SVM solution, due to misleading notation in previous works. Linearly separable datasets with features in an affine subspace may result in a poor margin when minimizing cross-entropy loss without regularization. When training a neural network with cross-entropy loss on CIFAR-10 dataset, the penultimate layer produces points on an affine subspace. Without effective regularization, weights in the last layer can grow infinitely, leading to 100% confidence in the network for most input points over time. Confidence in the network heavily depends on training duration. The confidence in a neural network heavily depends on training duration. Differential training introduces a new paradigm using a loss function defined on pairs of points from each class. The decision boundary of a linear classifier trained with this method produces the SVM solution with the maximum hard margin. The growth rate and direction of w[k] as k increases are characterized using a continuous-time approximation to the gradient descent algorithm. The normal vector of the decision boundary obtained by minimizing the cross-entropy loss may be almost orthogonal to the SVM solution, leading to a smaller margin between points and the decision boundary. The cross-entropy loss minimization with gradient descent algorithm may result in a decision boundary almost orthogonal to the SVM solution, leading to a smaller margin between points. Preprocessing data to normalize points may not effectively address this issue for neural networks. The lack of effective regularization for neural networks leads to unbounded growth of weights, resulting in high confidence predictions except at decision boundaries. This behavior is expected to be observed in neural network training as well. The confidence of a neural network is highly correlated with training duration. The importance of confidence levels above 50% for predictions is discussed. The binary classification of a linearly separable dataset by minimizing cross-entropy loss function is examined. The iterate of the gradient descent algorithm on the cross-entropy loss function will converge in a certain direction, different from the maximum-margin solution given by the SVM. The decision boundary obtained by cross-entropy minimization may not achieve the maximum hard margin. The cross-entropy minimization does not guarantee the maximum hard margin, as shown by Theorem 3. It states that the margin obtained through cross-entropy minimization can be arbitrarily worse than the maximum margin. If training points lie in an affine subspace, the margin from cross-entropy minimization will be smaller than the optimal margin value. As the dimension of the affine subspace decreases, the set K increases, leading to a potential poor margin in cross-entropy minimization with a gradient method. Theorem 4 relaxes conditions, allowing training points to be near an affine subspace instead of exactly on it, affecting margin comparisons. Theorem 4 relaxes conditions for linearly separable datasets, allowing points to be near an affine subspace. If the dataset is not linearly separable, the normal vector of the decision boundary may have components that converge to a finite vector and diverge, potentially leading to a poor margin. In separable datasets, the margin may be small if points intrude into the opposite class in the same subspace as the optimal normal vector for the decision boundary. The focus is on separable datasets to gain insight into neural networks' issues. Cross-entropy minimization can lead to poor margins due to the bias term in the objective function. To address this, consider the SVM problem and focus on the weight parameter w. By using the set of differences {x i + y j : i \u2208 I, j \u2208 J}, the bias term can be excluded from the SVM problem, leading to a loss function similar to cross-entropy. The direction of w converges to the maximum-margin solution when solving min DISPLAYFORM0 with a gradient descent algorithm. The cost function minimization in SVM yields the weight parameter \u0175 for the decision boundary. The bias parameter can be chosen by analyzing histograms of inner products and adjusting a value for b. Different values of b allow for a tradeoff between Type-I and Type-II errors. The cost function includes a loss on pairs of data points, resembling cross-entropy on a new dataset. The cost function (5) in SVM minimization resembles cross-entropy loss on a new dataset with |I| \u00d7 |J| points. Oversampling or undersampling of classes is not needed, and the algorithm achieves zero training error with few pairs if points are well separated. The computational cost may be higher due to the larger number of pairs, but further computation is only required to improve the margin. Theorem 6 states that with the stochastic gradient method, if 2\u03b3 \u2265 5 max(Rx, Ry), the algorithm produces weight parameters in one iteration satisfying certain inequalities. Experimental results show that only a few percent of points are needed to achieve high accuracy in training a neural network for classifying two classes from the CIFAR-10 dataset. In Figure 2, decision boundaries of two linear classifiers are shown, one trained with cross-entropy loss minimization and the other through differential training. The data does not lie in an affine subspace, with one class having samples from a normal distribution with mean (2, 12) and the other class with mean (40, 50). Cross-entropy minimization yields a smaller margin compared to differential training, even in high-dimensional datasets. The penultimate layer features of a neural network lie in a low-dimensional affine subspace, as shown by empirical evaluation on CIFAR-10 dataset. The dimension of the subspace containing training points is at most 20. The dimension of the subspace containing training points is at most 20, much smaller than the feature space dimension of 84. Cross-entropy minimization with gradient methods may result in poor margins on these features, leading to misclassification of inputs. This issue is attributed to the combination of gradient methods, cross-entropy loss function, and low-dimensionality of the training dataset. The SVM's robustness against adversarial examples is attributed to its high nonlinearity. The effectiveness of differential training for neural networks against adversarial examples is still being researched. The input to the soft-max layer is predominantly in a low-dimensional subspace, with the first 20 principle components capturing almost all the variance in the features. The training algorithm's performance is influenced by the dimension of the affine subspace containing the dataset. Projecting training data onto a low-dimensional subspace before feeding into a neural network improves performance against adversarial examples. However, this method requires knowledge of the low-dimensional domain, prompting the need for alternative algorithms and loss functions suited for low-dimensional data. Using robust optimization techniques to train neural networks has been effective against adversarial examples. These techniques involve inflating training points by a presumed amount and training the classifier with these inflated points. However, the margin of the classifier may still be smaller than it could potentially be. Differential training introduces a method to keep the feature mapping trainable while ensuring a large margin between different classes of points, combining the benefits of neural networks with those of support vector machines. The method combines neural networks with support vector machines, emphasizing the need to differentiate between the hardest pairs of points. Heuristic methods like focusing on a subset of points near the boundary during training can be effective. Training neural networks in this way forces them to find features that distinguish between difficult pairs. The issue of non-linearly separable data remains an open direction for future research, but it is not expected to occur with state-of-the-art networks achieving zero training error. Neural networks can achieve zero training error even on randomly generated datasets, implying linear separability of features. The proof of Theorem 1 involves using Theorem 2 or an independent proof. Gradient descent with learning rate \u03b4 on cross-entropy loss yields a specific formula. Lemmas and proofs are provided to support the theorem. The function f is strictly increasing and convex for w > 0. When b \u2265 a, the only fixed point of f over [0, \u221e) is the origin; when a > b, 0 and (a \u2212 b)/(c \u2212 b) are the fixed points. The curves for u = 0 and w = 0 are shown in Figure 4. The origin (0, 0) is unstable when a > b, and stable otherwise. The proof involves obtaining a lower bound for the square of the denominator. The proof involves obtaining a lower bound for the square of the denominator. By minimizing the cross-entropy loss, the margin obtained is calculated. KKT conditions for the optimality of w and B are required, with specific constraints on \u00b5 and \u03bd. The proof involves showing that for an orthonormal set of vectors {r k } k\u2208K, the margin is bounded by w \u22121. By defining a vector w SVM orthogonal to the decision boundary, it is possible to achieve zero training error with specific constraints on x i and y j. Choosing \u03b3 appropriately guarantees the desired result."
}