{
    "title": "H1a37GWCZ",
    "content": "We introduce a new unsupervised method for learning sentence embeddings that selects influential sentences in a document based on its structure. This approach improves coreference resolution in technical domains by 30% compared to existing methods. The proposed unsupervised sentence embedding framework aims to improve semantic understanding of structured technical documents by leveraging long-distance dependencies between sentences. This approach contrasts with existing methods that focus on small continuous contexts, showing potential for enhancing coreference resolution in technical domains. The proposed unsupervised sentence embedding framework aims to improve semantic understanding of structured technical documents by leveraging long-distance dependencies between sentences. Understanding a sentence often requires knowledge of the broader context, including the document title, previous paragraphs, or related articles. Context obtained from document structure can connect ransomware with payment and four hashes with Locky. Millions of spam emails spread a new ransomware variant known as Locky, detected by Symantec as Trojan.Cryptolocker.AF, since its appearance on February 16. Locky ransomware, spread aggressively through spam campaigns and compromised websites, demands ransom payment to decrypt or not publish victim's data. Believed to be released by the Dridex gang, Locky is well-prepared with mature infrastructure. The malware's spam campaigns resemble those used by the Dridex financial Trojan, with a focus on a specific payload. The Dridex group is known for its financial Trojan activities, using malicious macros in Word documents to steal banking credentials. Their virulent Dridex malware is a significant threat. A new approach leveraging structural elements has advantages in learning from technical documents with various subtopics. Training with a Wikipedia corpus shows improved performance compared to SKIP-THOUGHT in target sentence prediction tasks. The paper proposes a sentence embedding method leveraging long-distance dependencies from document structure, showing improvements in coreference resolution and paraphrase identification tasks. Key contributions include a rule-based annotator for document structure, OOV handling technique, and application to cybersecurity datasets. The experiments demonstrate that the model consistently outperforms existing methods in NLP applications using cybersecurity datasets. Sentence embedding, a distributed representation of sentences, has gained attention for tasks like machine translation and sentiment analysis. Recent approaches in neural networks consider dependencies between words to train the network, which can be continuous or discontinuous, intra-sentence or inter-sentence. Many sentence embedding approaches leverage these dependencies to combine word embeddings. Several researchers have proposed various methods for sentence embedding, including combining word vectors in a continuous context window, using supervised approaches with LSTM networks, employing CNNs over continuous context windows, and incorporating paragraph vectors. Dependency-based embedding methods have also been suggested, such as using a dependency parser to consider discontinuous intra-sentence relationships and utilizing recursive neural networks or dependency-based CNNs. BID26 proposes tree structured long short-term memory networks for sentence embedding, showing that dependency-based networks outperform sequential ones. SKIP-THOUGHT BID12 combines encoder and decoder recurrent neural networks to capture inter-sentential dependencies. A new method is proposed to handle out-of-vocabulary words in sentence embedding based on their position and dependency type. This approach is unique as it incorporates OOV words in formulating the training goal, unlike existing systems. The approach proposed in the current text chunk addresses the out-of-vocabulary (OOV) words in sentence embedding by considering word positions and dependency types. This method differs from previous approaches that focus on intra-sentence dependencies or neighboring sentences. The current text chunk discusses identifying semantically related content to a target sentence based on document structure for sentence embedding. It describes inter-sentence dependencies and techniques to automatically identify them using notations and dependency types. The text discusses identifying titles in a document to incorporate them into the embedding of sentences, using metadata and document content. Titles at different levels govern regions in the document and influence the meaning of sentences. The document discusses how titles influence sentences by using metadata and document content. Titles obtained from document metadata, heading tags, headers, footers, and text styles govern the sentences in a document. The text discusses how sentences in a document can be identified as titles based on text style attributes. Authors often use list structures to present information, with the main concept stated first followed by supporting points in a bulleted, numbered, or in-text list. State Farm offers property and casualty insurance, life and health insurances, and bank products. List items are identified using list tags for HTML documents or number sequences for non-HTML documents. In-text lists are also identified using cue words like \"First(ly), Second(ly), Last(ly).\" Hyperlinks may be included for additional information. In this work, sentences with hyperlinks or references are enriched using the linked document's title or embedding. Footnotes in HTML documents provide additional information through in-document hyperlinks. Sequential dependencies are considered in analyzing sentences in a document. Our model generates a target sentence using a set of governing sentences, considering diverse long-distance context and handling out-of-vocabulary words based on their occurrences. It includes several encoders, a decoder, and an OOV handler. Word vectors are represented as dense vectors, using pre-trained vectors from the CBOW model. Unlike existing methods, we do not map all words to one OOV word but introduce a new approach. The model introduces an OOV handler to map out-of-vocabulary words in governing sentences to variables and extends the vocabulary. It describes the encoders for digesting governing sentences and includes an OOV weight vector for handling out-of-vocabulary words. The model introduces an OOV handler to handle out-of-vocabulary words in governing sentences by mapping them to variables and extending the vocabulary. It includes encoders for processing governing sentences and utilizes an OOV weight vector for managing out-of-vocabulary words. The decoder is defined with a recurrent neural network cell that updates memory and generates output, with parameters for the decoder and softmax function for word prediction. To balance model accuracy and training time, K randomly chosen governing sentences are used for target sentence generation, with cross-entropy optimization between predicted and actual words. In deep learning models, incorporating all words from a large text collection is impractical due to memory and training time constraints. Existing sentence embedding techniques reduce vocabulary size by using high frequency words and collapsing others to an unknown word. However, this approach can lead to loss of important words, including domain-specific and proper nouns, resulting in unsatisfactory results for technical documents. OOV word handling is desired in input embeddings to encode governing sentences. In deep learning models, handling out-of-vocabulary (OOV) words in input embeddings is crucial. The approach of using an average vector for OOV words is common. A new method using two vocabulary sets is proposed to handle OOV words in output logits. This method involves selecting the most frequent words in the training corpus to create an initial vocabulary and mapping OOV words to a smaller vocabulary of OOV variables. In deep learning models, handling out-of-vocabulary (OOV) words is crucial. A method using two vocabulary sets is proposed to handle OOV words in output logits by mapping them to OOV variables based on their positions in sentences. The OOV handler builds a map to convert OOV words to variables and vice versa, considering the first and last \u03b7 OOV words in governing sentences. To handle out-of-vocabulary (OOV) words in deep learning models, a method using two vocabulary sets is proposed. The OOV handler maps OOV words to OOV variables based on their positions in sentences, considering the first and last \u03b7 OOV words. This mapping is many-to-many, as exemplified by mapping OOV words from different sentences to OOV variables. The method proposed handles out-of-vocabulary (OOV) words by mapping them to OOV variables based on their positions in sentences. This mapping allows for multiple correct answers, with a weighting scheme based on the number of words associated with the label. Additionally, a weight function based on occurrences of proper nouns is used to give higher weight to sentences with proper nouns. The approach proposed involves assigning higher weight to sentences with proper nouns by introducing a feature vector representing the number of out-of-vocabulary (OOV) proper nouns. The model learns weights for different dependency types and is evaluated on various NLP tasks using Wikipedia data. The model leverages HTML tags to identify document structures and uses raw HTML files. It is trained for 300K steps with 64-sized batches and the Adagrad optimizer. The evaluation involves using up to 8 governing sentences as context for a target sentence, with a maximum of 30 words per sentence. The model can learn application-independent sentence representations without task-specific labels and is trained to predict a target sentence given context. The comparison between SKIP-THOUGHT and two versions of the model (OURS and OURS-DEP) is done by evaluating prediction losses using cross entropy loss on 640,000 target sentences. The table shows that both OURS and OURS-DEP outperform SKIP-THOUGHT significantly, with the average loss per sentence being measured with and without out-of-vocabulary (OOV) words. In comparison to SKIP-THOUGHT, OURS and OURS-DEP show a significant reduction in loss values. The models were also evaluated on a paraphrase detection task using the Microsoft Research Paraphrase corpus BID19, with accuracy measured based on boolean assessments of sentence pairs. The evaluation focused on the effectiveness of the trained encoder, using logistic regression classifier with embedded sentence features for comparison. Our model, using logistic regression classifier with embedded sentence features, outperforms SKIP-THOUGHT by 5% in paraphrase detection accuracy. The encoder trained with structural dependencies shows effectiveness, especially when compared to models trained on Wikipedia corpus. Coreference resolution traditionally involves supervised pairwise classification or clustering, with recent improvements in the field. Our system, not a coreference resolution tool, utilizes rich sentence embedding for unsupervised coreference resolution across any domain. While dedicated methods may yield better results, our approach can provide a strong foundation of features without supervision for new domains. The coreference resolution problem is treated as an inference problem based on context, assuming entity mentions. Our model treats coreference resolution as an inference problem based on context. Entity mentions are assumed to be detected in advance, and a list of candidate referents is selected for pronouns or generic entity references. Mention type-based filtering is applied to reduce the search space, and the referent with the lowest loss value is chosen as the result. The effectiveness of the unsupervised coreference resolution method is demonstrated by comparing it with the Stanford Deep Coreference Resolution tool using cybersecurity-related documents. Coreference Resolution tool BID2 evaluated on cybersecurity-related documents achieves higher precision and recall than DEEPCOREF. Our model works well for domain-specific entities like 'Malware' and 'Vulnerability', while DEEPCOREF performs better for 'Person' and 'Organization'. In this paper, a novel unsupervised sentence embedding technique is presented, focusing on diverse structural contexts and domain-specific OOV words. The method outperforms existing approaches in various NLP tasks such as coreference resolution, paraphrase detection, and sentence prediction, showcasing the importance of considering structural context for better sentence representations."
}