{
    "title": "Hkxp3JHtPr",
    "content": "In anomaly detection, deep generative models like variational autoencoders (VAEs) are used for unsupervised learning of normal data distribution. A new approach for semi-supervised AD involves training VAEs to separate latent vectors for normal and outlier data, with effective algorithms proposed. These methods are versatile and applicable to various data types, demonstrated on datasets from different fields. Comparisons with state-of-the-art SSAD methods show promising results. Variational autoencoders (VAEs) are utilized in anomaly detection for learning normal data distribution. They have shown significant improvement in outlier detection compared to other state-of-the-art methods. Anomaly detection is crucial in various domains like healthcare, security, and robotics. Deep generative models like VAEs and generative adversarial networks (GANs) have shown promise in learning data distributions for anomaly detection tasks. In the context of anomaly detection, a variational approach is proposed for learning data distributions in semi-supervised anomaly detection (SSAD). The method is based on variational autoencoders (VAEs) and modifies the training objective to incorporate labeled outlier data. Two formulations are presented to address the challenge of classifying anomalies with limited labeled samples. The proposed method for anomaly detection in semi-supervised learning involves two formulations. The first maximizes the log-likelihood of normal samples while minimizing outliers, using a combination of ELBO and CUBO. The second method separates the VAE prior for normal and outlier samples, modifying the VAE encoder to push outliers away from the prior distribution. This approach is versatile and can be applied to various types of data, as demonstrated in the SSAD test-suite by Ruff et al. (2019). Anomaly detection in semi-supervised learning involves a dual-prior VAE method that improves performance in various domains. The method separates normal and outlier samples in the latent space, pushing outliers away from normal samples for better anomaly detection. Recent studies have shown success in anomaly detection using deep learning methods, with a focus on semi-supervised approaches. Various methods such as S2OC-SVM, SVDD-based approaches, and statistical testing have been proposed. Deep semi-supervised anomaly detection (Deep SAD) by Ruff et al. (2019) is a notable approach based on deep SVDD. This method aims to minimize the volume of data around anomalies using a neural network mapping. The proposed methods do not impose restrictions on network architecture and can be combined with any VAE model, showing improved performance across various domains compared to Deep SAD. Previous work on deep anomaly detection includes a hybrid approach combining deep unsupervised learning for feature extraction with shallow AD methods. Our approach extends the VAE method to the SSAD setting, showing improved anomaly detection performance without requiring specific data or pre-trained features. Previous studies have explored deep generative models like GANs and VAEs for AD, while recent work on deep energy-based models has shown promising results. Our results demonstrate that even a small fraction of labeled anomalies can outperform state-of-the-art deep energy-based models in AD scores. In deep unsupervised learning, fitting parameters of a latent variable model is challenging due to intractability. Variational inference methods approximate the maximum likelihood objective using the evidence lower bound (ELBO). The variational autoencoder (VAE) represents the approximate posterior as a neural network with parameters \u03c6. In deep unsupervised learning, the variational autoencoder (VAE) uses the reparameterization trick to maximize the ELBO. The model consists of an encoder (q \u03c6 (z|x)) and a decoder (p \u03b8 (x|z)). Dieng et al. (2017) introduced the CUBO for optimizing the data log-likelihood upper bound. They proposed an unbiased optimization objective L to minimize CUBO using Monte Carlo estimation, ensuring the gradients are unbiased. The number of samples affects the gradient variance in anomaly detection. In anomaly detection, the goal is to detect if a sample is from a normal distribution or an anomaly. In semi-supervised anomaly detection (SSAD), normal and outlier samples are given. An approach for SSAD is to approximate the normal distribution using a deep latent variable model and decide anomaly based on predicted likelihood. Two variational methods for learning the model are proposed, including the max-min likelihood VAE (MML-VAE). The MML-VAE and DP-VAE methods aim to optimize model parameters by maximizing the likelihood of normal samples while minimizing outliers. The objective includes a weighting term \u03b3 to balance the importance of outliers. A latent variable model is used to model the data distribution, with variational lower bounds proposed for optimization. The MML-VAE method optimizes model parameters by maximizing normal sample likelihood while minimizing outliers. It uses a latent variable model with variational lower bounds for optimization, requiring training two encoders and one decoder separately on two datasets. The same variational distribution is used for both loss terms to relax the lower bound, and the loss function includes a CUBO term to affect the variational distribution. The CUBO term in the loss function aims to separate latent distributions for normal and outlier samples by minimizing the KL distance between q(z|X normal ) and p(z|X normal ) and maximizing the \u03c7 n distance between q(z|X outlier ) and p(z|X outlier ). This component pushes the variational distribution towards high-likelihood regions of p(z) for normal samples and away from p(z) for outlier samples. In experiments, computing exp(log E [exp(\u00b7)]) is more stable using the log-sum-exp trick. Updating only the encoder with the CUBO loss improves performance and training stability. CUBO maximizes reconstruction error, leading to better encoder updates. The DP-VAE is a modification for SSAD, assuming normal and outlier data come from a single latent variable model. The DP-VAE model assumes normal and outlier data come from a single latent variable model. The loss function minimizes the separation between latent variables for normal and outlier data, optimizing using the reparametrization trick. The decoder is frozen during the minimization of ELBO outlier to prevent informative reconstruction of the outlier. The SSAD algorithm is based on MML-VAE and DP-VAE. After training, the ELBO approximates normal data likelihood. The score for a sample is based on ELBO normal. ELBO is chosen over CUBO for scoring due to training data distribution differences. In experiments, ensembles of MML-VAE and DP-VAE significantly improve SSAD performance, following evaluation methods proposed by Ruff et al. (2019) for anomaly detection algorithms. The curr_chunk discusses shallow SSAD methods trained on features from a deep autoencoder, evaluated using AUROC on high-dimensional and classic anomaly detection datasets. Ruff et al. (2019) allow shallow methods an advantage in hyper-parameter selection. Comparisons are made with Deep SAD methods. In comparison to the state-of-the-art Deep SAD method, we utilized a different network architecture with bias terms in all layers. However, for the MNIST dataset, we switched to a standard CNN architecture. Our experiments were conducted using PyTorch on an Nvidia RTX 2080 GPU with an ensemble size of K = 5. Different datasets like MNIST, Fashion-MNIST, and CIFAR-10 were used, each with ten classes. Similar to Ruff et al. (2019), we created ten AD setups on each dataset, where one class is considered normal and the rest anomalies. Training involved using normal class samples and a small fraction of data from one anomaly class as outliers. Testing was done on anomalies. At test time, anomalies from all classes are evaluated. Pixels are scaled to [0, 1] for preprocessing. A validation set (20%) is used to tune hyper-parameters. The model's detection ability is evaluated based on the ratio of anomalies presented during training. Different values of \u03b3 l are tested in 90 experiments per scenario. Results are shown in Table 1. Our results in Table 1 show that even a small fraction of labeled outliers (\u03b3 l = 0.01) significantly improves performance compared to standard unsupervised methods on MNIST and CIFAR-10. Our methods also outperform other SSAD baselines in most domains, as seen in the results for image datasets. Additionally, we evaluate on the challenging CatsVsDogs dataset, split into a training set of 10,000 images. Golan & El-Yaniv (2018) split the dataset into a training set of 10,000 images and a test set of 2,500 images per class, rescaled to 64x64. They used a VAE architecture similar to autoencoder and modified the Deep SAD baseline. Results in Table 2 show that even with just 1% labeled outliers, predictions significantly improve, demonstrating the potential of the SSAD approach. In the domain of anomaly detection (AD), incorporating geometric transformations from Golan & El-Yaniv (2018) improves performance without labeled outliers. Future research could explore integrating self-supervised learning concepts into probabilistic AD methods. The CatsVsDogs dataset experiment results show the impact of increasing the ratio of labeled anomalies in the training set. Additionally, evaluating the method on lower-dimensional datasets aims to showcase its versatility for various data types. In anomaly detection, a random train-test split of 60:40 with stratified sampling is used. Standardization of features is performed, and anomalies are treated as one class. Hyperparameters are tuned based on AUROC performance on training data, with a limit of 150 training epochs. Results of the best-performing algorithms are presented, showing that the methods outperform others. Our methods outperform other approaches in anomaly detection on various datasets, showcasing the flexibility of our approach. An ablative analysis of the DP-VAE method is conducted on different datasets, evaluating factors such as frozen vs. unfrozen decoder, separate vs. same encoder for normal data and outliers, and the impact of using ensembles of VAEs. The results highlight the necessity of using the same encoder, as demonstrated in previous research on conditional VAEs for generating robot configurations. The VAE generates 6D robot configurations from obstacle images for motion planning. DP-VAE method improves sample quality for obstacle avoidance. Ensembles enhance results by 2-4% on average in VAE training. Freezing the decoder has minimal impact on outlier data. The Deep SAD ensemble models show little effect due to differences in calibration of network scores. Ablative analysis of the Dual Prior method is detailed in Table 4, showing AUROC results for different datasets. The VAE approach benefits from ensemble methods, enhancing generative capabilities. In a motion planning domain, a conditional VAE is proposed to enhance sampling quality by including outliers during training. The method aims to concentrate sampling in regions where optimal solutions might lie, improving planning efficiency. During training, outliers on obstacle boundaries are excluded to improve motion planning efficiency. DP-VAE method is used to modify CVAE training, resulting in a VAE focused on feasible spaces. Two VAE modifications for negative data examples are proposed for anomaly detection, outperforming state-of-the-art methods on various datasets. Our methods improve anomaly detection by incorporating a small fraction of outlier data. The probabilistic approach using deep generative models shows great potential, especially for data like images. Discriminative approaches are currently best for specific data types, but developing self-supervised methods for generative approaches is a promising direction for future research. Incorporating SSAD within energy-based models is another exciting possibility. Table 5 presents the results of an ensemble of Deep SAD models on CIFAR-10. The ensemble method involves training 5 separate models with their own parameters and averaging the scores. Table 6 shows the results of MML-DP VAE, which combines MML and DP objectives during training. The performance is comparable to other methods like OC-SVM/SVDD for novelty detection. Isolation Forest (IF) is a tree-based anomaly detection method that isolates anomalies explicitly instead of creating a profile of normal instances. The number of trees is set to 100, and the sub-sampling size is 256. The bandwidth of the Gaussian Kernel is selected through 5-fold cross-validation. The experiment results show the average AUROC with standard deviation over 90 experiments at different ratios of labeled anomalies. Various anomaly detection methods are compared, including Semi-Supervised Anomaly Detection (SSAD), Convolutional Autoencoder (CAE), Hybrid Methods, and Unsupervised Deep SVDD. Soft-Boundary Deep SVDD and One-Class Deep SVDD optimize weights of a deep architecture similar to classic SVDD. Deep Semi-supervised Anomaly Detection (Deep SAD) minimizes data volume around a predetermined point using deep SVDD. Semi-Supervised Deep Generative Models (SS-DGM) use a deep variational generative approach for semi-supervised learning. DSEBM is a deep neural technique that outputs the energy function associated with an input. Neural technique outputs energy function for input sample. DAGMM is deep neural network for anomaly detection using Gaussian Mixture Modeling. Architecture similar to convolutional autoencoder in OC-SVM Hybrid. Anomaly detection methods include ADGAN, DADGT, and Inclusive-NRF. ADGAN uses GANs to search for good sample representations. DADGT learns features through geometric transformations. Inclusive-NRF develops neural random fields for continuous data. Our work focuses on developing neural random fields for continuous data, utilizing inclusive-divergence minimized auxiliary generators and stochastic gradient sampling. The model provides a density estimate, making it an efficient tool for anomaly detection. Implementation details include the use of deep variational autoencoders with encoder and decoder networks, LeNet-type CNNs for image datasets, and batch normalization with leaky ReLU activation after each convolutional layer. The CNN architecture includes convolutional layers with batch normalization, leaky ReLU activations, and max pooling. Different configurations are used for Fashion-MNIST, CIFAR-10, MNIST, and CatsVsDogs datasets. The CNN architecture includes convolutional layers with batch normalization and ReLU activation. Different configurations are used for various datasets, with specific numbers of filters and units in each layer. Regular multi-layer perceptrons are used for benchmark datasets, with specific configurations for each dataset. Tuning models in a semi-supervised setting can be challenging due to imbalanced data distribution. The validation set is taken from the training set for image datasets, composed of unseen samples from the normal class and current outlier class. Performance is evaluated on the training set without a validation set for classic AD benchmark datasets with few outlier data. Performance is finally assessed on the test set for all datasets. For all datasets, a batch size of 128 and an ensemble of 5 VAEs were used. 200 epochs were run for image datasets and 150 epochs for classical AD benchmarks. The hyper-parameter \u03b2 CU BO was added to the CUBO term for balancing, contributing to performance. Gradient clipping and learning rate scheduling were used for optimization of the CUBO component. Tables 8 and 9 summarize all hyper-parameters chosen for the model per dataset. The hyper-parameters chosen for the model include a learning rate schedule, \u03c7-divergence definition, reconstruction error calculation, and the use of a balancing hyper-parameter \u03b2 CU BO. The CUBO loss is optimized for anomalous data, with gradient clipping and learning rate scheduling for optimization. The CUBO loss function is optimized for anomalous data using a balancing hyper-parameter \u03b2 CU BO."
}