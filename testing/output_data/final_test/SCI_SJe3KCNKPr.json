{
    "title": "SJe3KCNKPr",
    "content": "Using Recurrent Neural Networks (RNNs) in sequence modeling tasks is challenging due to memory-bound execution patterns. A big-little dual-module inference approach is proposed to skip unnecessary memory access and computation, speeding up RNN inference. By leveraging error-resilient activation functions, a lightweight little module approximates the original RNN layer, reducing memory access by 40% on average and achieving 1.54x to 1.75x speedup on CPU-based servers with minimal impact on model quality. Recurrent Neural Networks (RNNs) are crucial for natural language processing tasks like machine translation and speech recognition. Deploying RNN-based services in latency-sensitive scenarios poses challenges due to low data reuse and resource utilization. Deploying RNN-based services in latency-sensitive scenarios is challenging due to low data reuse and resource utilization. The bottleneck of GEMV-based RNN execution is accessing weight matrix from off-chip memory. Nonlinear activation functions bring error resilience to RNN inference operations. Activation functions in neural networks, like sigmoid and tanh in Gated RNNs, provide error resilience by creating insensitive regions. This error resilience can be leveraged to reduce memory access and achieve speedup. A dual-module inference approach is proposed, using a parameterized little module to approximate the big module and reduce redundant weight accesses. The philosophy is to use approximated results from the little module in the insensitive region and accurate results from the big module. The final outputs are a mixture of the big-little module, with the little module computing for the insensitive region to reduce memory access and computation cost. Approximated results in the insensitive region have a negligible impact on model quality but offer significant acceleration potential. The little module needs to be accurate yet lightweight, achieved through dimension reduction methods like random projection to reduce parameter size and data accesses. Our hybrid approach reduces the parameter size of the little module and quantizes its weights to lower overhead. By focusing on the insensitive region, we achieve aggressive low bit-width without indexing overheads, resulting in practical speedup. Evaluations on RNN-based models show a 40% reduction in memory access data with 1.54x to 1.75x speedup and minimal impact on model quality. The error resilience of RNNs is discussed, highlighting the insensitive regions of activation functions like sigmoid and tanh. The insensitivity of activation functions like sigmoid and tanh in RNNs is discussed. A single LSTM layer for language modeling over PTB dataset is used as an example. Errors introduced in pre-activation accumulation results show differences in sensitive and insensitive regions. Cosine similarity between baseline and error-introduced models is compared after applying nonlinear activation functions. After comparing the cosine similarity between sensitive and insensitive regions post-nonlinear gates, it was found that errors in the insensitive region have minimal impact on quality. The dynamic selection of neurons in these regions is input-dependent, requiring a lightweight criterion for real-time processing. To address this, a dual-model inference method is proposed to efficiently determine these regions, reducing memory access and computational costs. This method is initially explained using a fully-connected layer and later extended to LSTM and GRU models. The text discusses reducing memory access for weight matrices in GEMV-based RNN inference by approximating values in the insensitive region. This approach aims to decrease computational costs and improve efficiency in real-time processing. The text proposes learning a lightweight module, LL, from the original big module, HH, to reduce memory access and computation in the insensitive region of y. The LL module operates in a low-dimensional and low-precision space, approximating the results of the HH module. The final output is a mixture of results from both modules based on a binary mask vector. The proposed method involves constructing a lightweight LL module from the original HH module to reduce memory access and computation in the insensitive region. This is achieved through a hybrid compression approach with dimension reduction and data quantization, ensuring efficiency in computation and storage for the LL module. The LL module is designed to have lower computation and memory overheads than the HH module while accurately approximating its outputs. Sparse random projection is used to reduce the dimension of x, with the parameter size of the LL module being much smaller than that of the HH module. Random projection preserves distances in Euclidean space and the dimension reduction step involves a sparse random matrix. The value of k can be adjusted to balance accuracy loss and inference cost. After reducing the dimension of x using sparse random projection, a lightweight module is constructed to approximate the pre-trained big module. Parameters of the big module are frozen while the little module's parameters are updated to minimize loss. Linear regression is applied to optimize the mean square error between the two modules. The parameter size of the little module is much smaller than the big module, resulting in lower overhead. By reducing the dimension of x through sparse random projection, a lightweight module is created to approximate the pre-trained big module. The memory-bound issue in GEMV-based models is alleviated, and computational complexity is reduced. Data quantization technique is applied to decrease parameter precision and storage space. One-time uniform quantization is used on W LL to simplify calculations, benefiting the dual-module inference. The dual-module inference in Equation (1) benefits from error tolerance due to the small influence of computation in the insensitive region on final outputs. A binary mask m switches between the accurate HH module and the efficient LL module, with the generation of m crucial for adjusting the trade-off between accuracy and efficiency. Nonlinear activation functions in RNNs create unipolar or bipolar output distributions, allowing for the removal of costly computation by setting peak areas as insensitive regions. The design of specific criteria for activation functions is based on constant thresholds, which can be adjusted for accuracy-efficiency trade-off. The dual-module inference process involves dimension reduction, data quantization, and generating a binary mask for switching between accurate and efficient modules. This approach is applied to RNNs like LSTM and GRU for improved computational efficiency. The proposed dual-module inference for an FC layer to RNNs, including LSTM and GRU, involves implementing LSTM with states for forget, input, and output gates, along with input activation. Each gate has its bias vector and weight matrices, with cellular and hidden states denoted as c and h. Sigmoid and tanh functions are used, and GEMV computations are applied with dimension reduction and data quantization. An additional temporal dimension in RNNs requires maintaining approximation performance of the LL module at all time steps. The proposed dual-module inference for an FC layer to RNNs involves implementing LSTM with states for forget, input, and output gates, along with input activation. The linear map works for both x LL (t) = P x x HH (t) and h LL (t \u2212 1) = P h h HH (t \u2212 1). The loss function for constructing the LL module is slightly modified to consider S training samples in each mini-batch and T time steps. Data quantization, switching mask generation, and output assembling are similar for other gates. The input x and hidden state h can have different sizes, termed as d x and d h, respectively. The target of the dual-module inference method is to reduce the approximation performance of the LL module at all time steps. The dual-module inference method aims to reduce off-chip memory access by using a little module to assist a big module. An insensitive ratio is introduced to determine the number of outputs from the little module, which affects memory access. The choice of ratio impacts model inference quality and latency trade-off. The method involves dimension reduction and quantization to minimize overhead. Different magnitudes and precision levels can be chosen for the little module to optimize performance. In Section 4.3 and 4.4, the study compares memory access and operations between single-module and dual-module inference using LSTM and GRU layers. The little module incurs 10% storage overhead and 40% operation overhead compared to the base case. The method evaluates model inference quality, execution time, dimension reduction, and quantization on a CPU-based server platform. The study evaluates model inference quality on a CPU-based server platform using PyTorch. Custom kernel implementation is used for performance measurement with LSTM, GRU, and stacked LSTM models. The study evaluates model inference quality on a CPU-based server platform using PyTorch, focusing on training a little module while freezing the parameters of a big module. Results show a trade-off between quality and performance, with an increase in execution time as the insensitive ratio grows. In language modeling tasks, a dual-module approach with 50% insensitive ratio slightly increases perplexity to 81.36 but provides a 1.67x inference speedup. This approach maintains model quality while reducing memory access, especially effective for larger LSTM layers. Similar trade-offs are observed for GRUs, making the dual-module method applicable to both LSTM and GRU models. The study also explores the application of this approach in Neural Machine Translation tasks. Neural Machine Translation (NMT) is a promising approach for automated translation. The focus is on speeding up the decoder, which is the most memory intensive part. Experiments on the WMT16 English-German dataset show that replacing LSTM layers with dual-module-based LSTM layers can improve model inference quality. The baseline model achieves a BLEU score of 24.32, while the proposed approach aims to enhance overall performance. Our method utilizes dual-module LSTM layers to reduce memory access and improve model quality. It achieves significant speedup in inference while maintaining imperceptible BLEU score degradation. Dimension reduction through Sparse Random Projection is studied to enhance model performance. The study utilizes dual-module LSTM layers for improved model quality and speedup in inference. Sparse Random Projection is used for dimension reduction, showing better performance with higher dimensions in the little module. Reducing dimensions can lead to increased speedup but at the cost of quality degradation. Overhead analysis includes execution time for dimension reduction, computation of the little module, and computation of the big module. In experiments, reducing hidden dimension to 966 results in 22% overhead for little module and 50% reduction in execution time for big module. Default parameter for sparse random projection is set at = 0.5. Further reducing hidden dimension to 266 shows slight speedup improvement but significant quality drop. Quantizing weights of little module helps reduce memory footprint. Different quantization levels impact model quality and parameter size. After reducing hidden dimension to 966, there is a 22% overhead for the little module and a 50% reduction in execution time for the big module. Quantizing weights of the little module can reduce memory footprint, with different quantization levels impacting model quality and parameter size. More aggressive quantization leads to smaller parameter size but compromises the approximation of the little module. INT8 is chosen for quantization level due to leveraging off-the-shelf INT8 GEMM kernel in MKL, with potential speedup by leveraging INT4 compute kernels. The focus is on RNN inference acceleration for memory-bound problems. The discussion focuses on RNN inference acceleration methods, including weight quantization, sparsity, and knowledge distillation. Various techniques such as quantization to binary or ternary, hybrid ternary quantization, and weight pruning are proposed to reduce model parameters. These methods aim to improve efficiency in deploying sequence modeling networks. Our method offers an alternative approach to accelerate RNN inference by avoiding the drawbacks of traditional model compression techniques like weight pruning and knowledge distillation. Our method offers an alternative approach to accelerate RNN inference by leveraging dynamic execution and selective skipping of computations based on certain criteria. Various techniques such as layer-wise early exit, threshold-based pruning on output gates, temporal input sparsity, and selectively skipping hidden state updates have been proposed to reduce memory access and computation during inference. Campos et al. (2018) proposed a method to reduce memory access and computation in Gated RNNs. They introduced a big-little dual-module inference approach to address the memory-bound issue in serving RNN-based models under latency-sensitive scenarios. By leveraging error resilience of activation functions, they achieved a near half reduction in memory access with a 1.54x to 1.75x speedup in wall-clock time without compromising model quality. Our proposed dual-module inference approach achieves better model quality with a practical speedup of 1.54x to 1.75x reduction in wall-clock time on commodity CPUs compared to weight pruning methods. This method can be applied on top of pruned models to further reduce execution time by minimizing memory access."
}