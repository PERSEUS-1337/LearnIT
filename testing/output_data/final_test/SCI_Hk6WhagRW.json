{
    "title": "Hk6WhagRW",
    "content": "Multi-agent reinforcement learning explores communication emergence in agent communities solving problems. Two communication protocols are introduced - one grounded in game semantics, the other ungrounded. Self-interested agents negotiate fairly with pre-grounded communication but struggle with ungrounded cheap talk. Prosocial agents effectively use cheap talk for optimal negotiation, highlighting the importance of cooperation for language emergence. Agent identifiability aids negotiation in communities with varying prosociality levels. The curr_chunk discusses the emergence of communication in multi-agent systems through coordination tasks and communication protocols. It explores how different task structures and communication protocols impact task success. Previous research has mainly focused on communication in cooperative games like referential games. Effective communication is crucial in the negotiation game, where agents aim to establish a mutually acceptable division of items while balancing their own hidden utilities. This game is a model of non-cooperative games in classical game theory. Recent work on deep multi-agent reinforcement learning has shown success in teaching agents complex behavior without a complex environment simulator or demonstration data. By interacting with other agents and learning at the same time, agents can gradually develop complex behavior, including motor skills and linguistic communication. Techniques from the MARL literature are applied to train agents to negotiate using task success as the only supervision signal. This allows selfish agents to learn to negotiate fairly when communicating via a task-specific communication channel with inherent semantics. Cheap talk, a task-independent communication channel similar to language, fails to facilitate negotiation in selfish agents but can aid prosocial agents in effective negotiation. Cooperation is essential for language emergence. In realistic multi-agent scenarios, cheap talk can significantly impact evolutionary dynamics, equilibria, stability, and basins of attractions. In multi-agent scenarios, agents benefit from identifying and modeling beliefs to aid negotiation success. Theory of Mind models show that making inferences about opponents' beliefs can lead to collective benefits. The negotiation task involves agents interacting with different prosociality levels and three types of items: peppers, cherries, and strawberries. Agents in a negotiation task interact with different prosociality levels and three types of items: peppers, cherries, and strawberries. Each agent receives a utility function specifying rewards for each item, and they negotiate by exchanging messages and proposals for a set number of timesteps. Agents can terminate the negotiation at any time with agreement on the most recent proposal. In a negotiation task, agents interact with varying prosociality levels and negotiate over items like peppers, cherries, and strawberries. They exchange messages and proposals for a set number of timesteps, with the option to terminate the negotiation at any time by agreeing to the most recent proposal. If an agent makes an invalid proposal, both agents receive no reward. An upper limit of N negotiation turns is imposed to prevent a \"first-mover\" advantage, where one agent can wait until the last timestep to make a lopsided offer. Agents in a negotiation task communicate through a proposal channel to discuss the division of items. Additionally, they have the option to use a linguistic channel to transmit arbitrary symbols, which is non-binding. In negotiation tasks, agents can communicate through a proposal channel for item division and a linguistic channel for arbitrary symbols, which is non-binding and unverifiable. Different communication configurations are considered, including open channels, closed channels with dummy symbols, and no communication at all. The proposal introduces a \"prosocial\" reward shared between agents to incentivize communication for optimal joint allocation of items. Different reward schemes are variations of a general formula where selfish agents prioritize individual rewards, while prosocial agents value joint utility. The proposer receives three inputs: item context, previous utterance, and previous proposal. These inputs are turned into dense vectors through embedding tables and encoded using LSTM BID14. The resulting vectors are concatenated and fed through for further processing. The proposer receives inputs: item context, previous utterance, previous proposal, encoded using LSTM. Fixed-size vectors h c t, h m t, h p t concatenated and fed through feedforward layer to get hidden state h t. Hidden state used to initialise policies for termination action and linguistic utterances. \u03c0 term parametrised with feedforward layer and sigmoid function for termination probability. \u03c0 utt parametrised by LSTM with hidden state as initial input. The model uses the previous timestep prediction as input for the next timestep to predict the next symbol. The agent's policy for generating proposals is parametrised by three feedforward neural networks, one for each item type. The agent's action at each turn is represented by a triple (e t , m t , p t ), where e t indicates termination, m t is the symbols produced, and p t is the proposal. Training involves finding the policy that maximises an objective function for each agent independently. The model uses the previous timestep prediction as input for the next timestep to predict the next symbol. The agent's policy for generating proposals is parametrised by three feedforward neural networks. Each agent independently finds the policy that maximises an objective function. Agents are trained to negotiate and divide items fairly, exploring different communication channels for negotiation success. The study trains self-interested agents to negotiate for 500k episodes, with each episode consisting of 128 games. Agents learn to divide items fairly during direct exchanges, showing evidence of compromise in negotiations. The proportion of total utility each agent receives is roughly equal, indicating they have learned to keep items with higher utility while giving away items with low utility. The study trains self-interested agents to negotiate for 500k episodes, with each episode consisting of 128 games. Agents learn to compromise during negotiations, adjusting their proposals based on information from the other agent. Self-interested agents do not negotiate optimally when using the linguistic channel. The study shows that self-interested agents do not negotiate optimally, as they struggle to exchange meaningful information about dividing items due to the random alternating behavior observed in negotiations. This is supported by the analysis of messages exchanged by agents, which mainly consist of a single symbol. The study found that when agent interests diverge, communication is not expected. Cooperation is crucial for language emergence. In a non-iterated environment, agents tend to defect in the prisoner's dilemma game. Iterated games allow for more cooperative strategies. Players may sacrifice short-term gains for long-term benefits. Iterating the game incentivizes more verifiable communication. In an iterated game, there is an incentive for more verifiable communication to avoid punishment for mismatched proposals. Self-interested agents can divide items fairly, but a prosocial reward scheme aligns with optimal joint allocation. Communication is biased towards solving the game optimally by pooling hidden utilities. The linguistic channel allows prosocial agents to communicate effectively, resulting in better task success compared to other communication schemes. Communication using cheap talk is a Nash equilibrium for agents with aligned interests, leading to optimal joint allocation. The linguistic channel has unlimited bandwidth, facilitating the exchange of task-specific information. The linguistic channel facilitates effective communication between prosocial agents, leading to better task success. The variance in joint optimality is lower for the linguistic channel, making it more robust. Difficulties in optimization result in similar results for both channels open, but with better optimization, performance can improve. Agent A adjusts its proposal based on agent B's utterances during negotiation. Prosocial agents can still coordinate using the proposal channel, but random termination leads to worse outcomes. Prosocial agents outperform the no-communication baseline by repurposing the proposal channel for information exchange. Agent B shows diversity in symbol usage, while Agent A does not transmit information linguistically. The study shows that prosocial agents in a communication game differentiate into speaker and listener roles, using a simple strategy to share utilities and generate optimal proposals. Selfish agents do not use grounded symbols, leading to no information exchange. The study explores how prosocial agents in a communication game differentiate roles to share utilities and generate optimal proposals. They use a message transcript and item pool to predict hidden utilities and accepted proposals through probe classifiers and LSTM encoding. Averaged per-item accuracy is reported using 10-fold cross-validation and linear classifiers. The study shows that agents can transmit meaningful information through symbols in a communication game, indicating they have learned to give meaning to the symbols. However, purely self-interested agents do not seem to transmit meaningful information using the linguistic channel. In realistic scenarios, negotiation involves identifying prosocial agents for maximizing rewards. Training involved a fixed agent against a community of agents with varying levels of prosociality. The study experimented with different proportions of prosocial agents and communication channels. At test time, the study focuses on whether self-interested agents can exploit prosocial agents and if prosocial agents can cooperate with each other. The experiment involves 10 batches of 128 games each, where the fixed agent plays against prosocial agents in the community. If opposing agents are identifiable, the fixed agent receives this information as a one-hot vector for lookup in an embedding table. The study focuses on whether self-interested agents can exploit prosocial agents and if prosocial agents can cooperate with each other. At test time, the fixed agent plays against prosocial agents in the community, receiving opponent information for better performance. Providing agent ID information helps the fixed agent achieve its aims, especially for selfish agents. However, for cooperative agents, the results are mixed, with some scenarios where having agent ID information harms performance. The study explores how self-interested agents interact with prosocial agents and the impact of providing agent ID information. Results show that even without agent ID information, agents can differentiate based on reward schemes. Prosocial agents developed a language for better negotiation success, as shown in Table 5. The study examines interactions between self-interested and prosocial agents, with a focus on the impact of revealing agent IDs. Prosocial agents developed a language for improved negotiation success, especially when ID information was not provided. The communication protocol varied within the community when prosocial agents utilized the linguistic channel. The study explores interactions between self-interested and prosocial agents, focusing on the impact of communication through a verifiable channel. Different pairs of agents show varying correlations in using bigrams, with cheap talk aiding negotiation in prosocial agents. Future research could investigate the emergence of cheap talk in self-interested agents. The study examines how communication can facilitate cooperation between self-interested and prosocial agents. The signaling mechanism used is heavily engineered, with predefined speech acts and deterministic consequences. It would be interesting to see if a learning algorithm like BID9 can achieve similar results. Another paper takes a top-down approach to learning negotiation through dialogue data, while this study explores a bottom-up method of learning communicative behaviors directly from peer interaction. This opens up the possibility of learning domain-specific reasoning capabilities through interaction, with a general-purpose language layer for producing natural language. Additional figures and tables show joint reward success and average number of turns taken by paired agents negotiating with different reward schemes and communication channels. The study tested different agent reward schemes and communication channels over 10 turns, with results averaged across 20 seeds. Neural network dimensions were set at 100, with separate optimizers for each agent. Different \u03bb values were used for policy regularization. Symbol vocabulary size was 11, with agents generating utterances up to length 6. Exponential moving average baseline smoothing constant was set at 0.7."
}