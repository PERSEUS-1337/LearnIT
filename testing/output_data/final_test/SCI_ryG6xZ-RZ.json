{
    "title": "ryG6xZ-RZ",
    "content": "DLVM is a compiler infrastructure designed for deep learning, offering reliability and performance. It features a linear algebra intermediate representation, algorithmic differentiation, domain-specific optimizations, and GPU code generation via LLVM. DLVM is modular, generic, and supports tensor DSLs with high expressivity. It enables safe and performant frameworks for deep learning, with a prototypical staged DSL embedded in Swift. Most current approaches in the deep learning community utilize high-level frameworks like Torch. Recent projects like TensorFlow XLA compiler and NNVM compiler, including TVM, are applying advanced compiler techniques to simplify computation and optimize deep learning systems. These compilers target LLVM and various back-ends to achieve high performance by merging high-level operators and fusing compatible elementwise operators into a single kernel, reducing latency between kernel launches. DLVM is a new compiler infrastructure for deep learning systems that improves upon existing frameworks by incorporating algorithmic differentiation as a transformation pass in the compiler pipeline, simplifying DSL development and achieving separation of concerns. The solution proposed includes a domain-specific intermediate representation for tensor computation, modern compiler optimization techniques for simplifying neural network computation, code generation through a mature compiler infrastructure, and an embedded DSL supporting static analysis and type safety. It is distinct from existing projects like Apache SystemML BID4. Our work introduces a high-level language and framework for machine learning on Apache Spark, treating neural network creation as a compiler problem. Unlike SystemML and TACO, which focus on low-level optimizations, our approach is centered around the concept of neural networks as programs. Our approach treats neural networks as programs and optimizes tensor computations through a principled compilation pipeline. We use static single assignment form with control flow graph, algorithmic differentiation, domain-specific optimizations, and code generation. Unlike XLA, our intermediate representation is more expressive, supporting full-fledged DSLs and extensive optimizations like inlining and interprocedural optimizations. DLVM differs from XLA by representing composite functions directly through primitive instructions, using SSA form with control flow graph, and having a robust compile-time framework for tensor DSLs. Unlike NNVM and XLA, DLVM has an IR with a textual parsable format and a full-fledged command line toolchain. DLVM is a compiler infrastructure for modern deep learning systems, designed with a multi-stage compiler optimization strategy for high-level linear algebra and low-level parallelism. It uses a textual IR for robust unit testing and is implemented in Swift for a compact codebase. The software stack includes sample front-end deep learning DSLs. The DLVM compiler infrastructure for deep learning systems includes major stages in the compilation pipeline addressing algorithmic differentiation, domain-specific optimizations, and static code generation for various compute architectures. The DLVM Intermediate Representation (IR) uses static single assignment (SSA) form, control flow graphs, high-level types, and linear algebra operators for high-level tensor operations and optimizations. The DLVM compiler infrastructure for deep learning systems includes stages for algorithmic differentiation, domain-specific optimizations, and static code generation. It uses static single assignment form, control flow graphs, high-level types, and linear algebra operators for tensor operations and optimizations, including reverse-mode AD, AD checkpointing, algebra simplification, and linear algebra fusion. NNKit is a staged DSL that demonstrates safe and natural expression of tensor computation alongside the host program, targeting DLVM for differentiation, optimizations, and static code generation. The DLVM IR is a graph-based code representation inspired by LLVM IR and Swift Intermediate Language. It includes modules, functions, basic blocks, and instructions, with a hierarchy of abstractions. Instructions operate on values like globals, function arguments, or virtual registers. Each function has a control flow graph with basic blocks and control flow edges. The DLVM IR is a graph-based code representation with modules, functions, basic blocks, and instructions. Each basic block contains ordered instructions forming a directed acyclic graph. DLVM has a high-level type system with tensors as a first-class type. The virtual instruction set includes domain-specific math operators and general-purpose instructions. Domain-specific operators include unary and binary operators, while composite functions like softmax and sigmoid can be composed using primitive math instructions and control flow constructs. DLVM IR is a graph-based code representation with modules, functions, basic blocks, and instructions. It has a high-level type system with tensors as a first-class type. The virtual instruction set includes domain-specific math operators and general-purpose instructions. Differentiation constructs function definitions from gradient declarations using adjoint code generation. Optimization is performed on the resulting IR, maximizing code performance through various analyses and transformations. Optimizations include domain-specific optimizations like algebra simplification and traditional compiler optimizations. DLVM IR simplifies mathematical operations like x^2 to x*x for efficiency. Matrix multiplication reordering is a classic optimization technique that reduces the number of sub-operations in a chain of matrix multiplications with different dimensionality. DLVM optimizer maximizes performance by fusing linear operations into a single matrix multiplication instruction, such as transforming expressions like Wx + b into a single operation for efficiency. The linear algebra fusion pass optimizes operations in recurrent neural networks by combining multiple matrix multiplications into a single operation, simplifying computations and improving efficiency. This approach can also optimize parameter passing and memory allocation for better performance. Algorithmic Differentiation (AD) involves techniques for obtaining derivatives of a function represented as a computation graph. The derivatives can be computed using the chain rule in either forward or backward direction. Forward-mode AD is commonly used when the output dimension is smaller than the input dimension. In DLVM, the differentiation pass performs reverse-mode AD to calculate the derivative of a function. Functions are marked as automatically differentiable via gradient declarations, which specify how to differentiate with respect to arguments and handle back-propagated gradients. The differentiation pass in DLVM performs reverse-mode AD by canonicalizing gradients to normal function definitions, enabling optimizations and higher order differentiation. Unlike operator overloading, this approach allows for separate gradient computation and Jacobian calculation. The gradient function produced by Automatic Differentiation (AD) in DLVM is a standalone function that computes partial derivatives with respect to the inputs. Unused operations can be eliminated through dead code elimination, and checkpointing can reduce memory consumption during gradient computation. AD in DLVM is configurable, allowing for differentiation with respect to selected arguments and specific output values. The gradient function in DLVM enables differentiation for multiple return values and backpropagated gradients through function composition. Higher-order differentiation is possible by declaring a higher-order gradient function. General-purpose optimizations in DLVM IR are crucial for optimizing linear algebra computations before lowering to LLVM IR. The DLVM enables differentiation for multiple return values and backpropagated gradients through function composition. It aims to target multiple parallel architectures and perform aggressive optimizations by transforming DLVM IR into LLVM IR. LLVM optimizations are hindered by low-level information in LLVM IR, making it challenging to apply optimizations like dead code elimination and common subexpression elimination. The DLVM compiler infrastructure transforms high-level DLVM IR into lower-level stages and ultimately into calls to BLAS and compute kernels in LLVM IR. Future versions of DLVM plan to target the IR of HPVM BID18 for diverse architectures. Front-end software generates DLVM IR for source language programs to be compiled. The DLVM compiler infrastructure includes a command line toolchain for verifying, transforming, and compiling DLVM IR files. It provides a command line interface for optimization and tasks like verification, differentiation, and code generation. The framework can easily perform unit testing using LLVM Integrated Tester (lit) and FileCheck. Future plans include introducing a DLVM bitcode format for compact storage. In future development, a DLVM bitcode format will be introduced for compact storage and high-throughput processing of DLVM code. Existing deep learning DSLs are often embedded in dynamically typed scripting languages like Python and Lua, which can hinder the transition from prototyping to production code. Language and compiler technologies are a proven approach to address this issue, with a focus on static analysis for reliability and efficiency in ML software development. In the initial release of DLVM, a DSL called NNKit is embedded in Swift, providing a safe environment for rapid prototyping with efficient code for deep learning. NNKit utilizes the static type system of Swift to ensure type safety of the DSL, allowing for natural expression of tensor computation alongside the host program. The NNKit DSL in Swift ensures type safety for tensor computations, utilizing Rep<T> for data representation. The NNKit just-in-time compiler has four phases: expression staging, shape specialization, lowering, and function reification. DLVM can be used as a back-end for other deep learning frameworks like TensorFlow. The deep learning research community has various frameworks, with some focusing on compilers for optimization. Many frameworks lack optimal compiler design practices, leading to issues with extensibility and optimization. Despite this, some frameworks have gained widespread adoption. The principled application of optimizing compiler techniques in deep learning frameworks like DLVM can lead to significant improvements for researchers. DLVM currently supports reverse-mode AD and targets NVIDIA GPUs using LLVM. Future plans include expanding hardware support with HPVM and exploring advanced AD techniques."
}