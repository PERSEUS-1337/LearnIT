{
    "title": "r1GAsjC5Fm",
    "content": "The paper introduces a self-monitoring agent for the Vision-and-Language Navigation task, which involves an agent following navigational instructions in unknown environments. The agent uses a visual-textual co-grounding module to locate completed and required instructions and a progress monitor to ensure correct navigation progress. Through ablation studies, the proposed method sets a new state of the art with an 8% absolute increase in performance on a standard benchmark. In the Vision-and-Language Navigation task, our proposed method achieved a new state of the art with an 8% absolute increase in success rate on the unseen test set. The task involves an agent following natural language instructions to navigate through an unknown environment without an explicit representation of the target. The agent needs to be aware of its navigation status to determine if the goal has been reached. The code for our method is available at https://github.com/chihyaoma/selfmonitoring-agent. The agent in the Vision-and-Language Navigation task must be aware of its navigation status by associating visual inputs with instructions. Ambiguity exists in completing sequential instructions, requiring the agent to determine completeness before moving on. In the Vision-and-Language Navigation task, the agent needs to track progress towards completing sequential instructions to determine when to transition to the next action. Previous approaches lack awareness of the next instruction or progress towards the goal, highlighting the need for an agent with the ability to identify these aspects. In this paper, a self-monitoring agent is proposed with abilities such as visual-textual co-grounding and progress monitoring to track progress in Vision-and-Language Navigation tasks. The agent can identify the direction to go, determine completed or ongoing parts of instructions, and estimate progress towards the goal. The proposed self-monitoring agent for Vision-and-Language Navigation tasks includes a visual-textual co-grounding module and a progress monitor to track progress accurately. This agent can determine the direction to go, identify completed or ongoing instructions, and estimate progress towards the goal. The proposed self-monitoring agent for Vision-and-Language Navigation tasks introduces a new objective function for training, allowing the agent to track progress towards the goal by following both past and future instructions. This results in state-of-the-art performance on seen and unseen environments, with an 8% absolute improvement in success rate on the unseen test set. The agent is equipped with a panoramic view and encodes natural language instructions using an LSTM language encoder. The proposed self-monitoring agent includes visual-textual co-grounding, progress monitoring, and action selection modules for Vision-and-Language Navigation tasks. It identifies completed parts of instructions, summarizes surrounding images, ensures progress towards the goal, and selects actions based on direction. The model combines visual and textual features to guide navigation, achieving state-of-the-art performance on seen and unseen environments. The proposed self-monitoring agent for Vision-and-Language Navigation tasks includes a textual co-grounding model using LSTM for information flow. The agent uses visual and textual features to guide navigation and select actions based on directions. The proposed self-monitoring agent for Vision-and-Language Navigation tasks incorporates a textual co-grounding model using LSTM for information flow. It relies on grounded instructions to determine navigation directions, utilizing soft-attention on instruction features to compute attention distribution over words. This allows for the selection of actions based on past or predicted instructions. The agent performs visual attention over surrounding views based on previous hidden vector to locate instructions. Visual features are obtained through weighted sum, and action selection is based on correlation with navigation instruction and current hidden state. The agent uses visual attention to locate instructions and selects actions based on correlation with navigation instructions and current hidden state. Textual and visual grounding is achieved through shared hidden state output containing information from both modalities, improving action selection accuracy. The proposed progress monitor enhances decision-making by estimating navigation progress based on the positions and weights of grounded instructions, ensuring alignment with the goal and correctness of textual-grounding. The progress monitor estimates navigation progress by considering inputs such as history of grounded images and instructions, current observation of surrounding images, and positions of grounded instructions. It computes hidden states using grounded image representations and attention weights to determine proximity to the goal. The progress monitor uses attention weights to estimate the agent's proximity to the goal. The output, representing instruction-following completeness, is computed using learned parameters and LSTM cell state. The training target is the normalized distance from the agent to the goal, optimized with cross-entropy and mean squared error losses. During inference, the agent integrates the progress monitor output into beam search to evaluate trajectories based on action probability and confidence in completing instructions. The progress monitor estimates completeness of instruction-following and is normalized between 0 to 1. Without beam search, the agent uses greedy decoding for action selection based on the progress monitor output. If the progress monitor output decreases, the agent moves back to the previous viewpoint and selects the action with the next highest probability until progress increases. The proposed approach is evaluated on the Room-to-Room (R2R) dataset, which contains 7,189 paths with ground-truth navigation instructions. The dataset is divided into training, validation seen, validation unseen, and test sets unseen. Evaluation metrics are used to assess the performance of the approach. The evaluation metrics for the proposed approach on the Room-to-Room dataset include Navigation Error (NE) and Success Rate (SR). The agent's distance to the goal is considered, and data augmentation is used. The positions and weights of grounded instructions are shown as agents navigate, with a focus on action selection shifting gradually. The success rate is measured by the percentage of final positions less than 3m away from the goal. The proposed approach on the Room-to-Room dataset includes evaluation metrics such as Navigation Error (NE) and Success Rate (SR). The success rate is measured by the percentage of final positions less than 3m away from the goal location. The self-monitoring agent achieves significant performance improvement compared to existing approaches, with a 70% SR on seen environments and 57% on unseen environments. The self-monitoring agent shows significant improvement on both validation and test unseen environments, achieving 3% and 8% better success rates. It is currently ranked #1 on the challenge leader-board. The agent's ability to focus and follow instructions accurately is demonstrated through attention weight distribution. The self-monitoring agent demonstrates improved success rates in both validation and test unseen environments, currently ranked #1 on the leader-board. The agent effectively utilizes attention weights on instructions for action selection, focusing on the beginning and shifting towards the end of the instruction as it moves closer to the goal. The agent's ability to follow instructions accurately is shown through the distribution of attention weights. The self-monitoring agent outperforms the baseline approach by effectively identifying useful parts of instructions for action selection, utilizing both textually and visually grounded content to predict navigable directions. The proposed progress monitor improves agent performance by ensuring grounded instructions reflect progress towards the goal, especially in unseen environments. The progress monitor serves as an indicator for the agent to decide when to move back to the last viewpoint. The progress monitor enhances agent performance by guiding when to return to the previous viewpoint, leading to a 2% improvement in success rates in both seen and unseen environments. Integrating the progress monitor with state-factored beam search results in a significant success rate boost, surpassing state-of-the-art methods without data augmentation. Using beam search with progress estimation increases success rates by 13% on unseen environments compared to 7% without it. The proposed method combines data augmentation with pre-trained speaker BID19 to increase success rates and reduce navigation errors. The performance improvement from data augmentation is smaller than from Speaker-Follower on validation sets, showing the method's data efficiency. A self-monitoring agent successfully navigates unseen environments by following grounded instructions and achieving goals. The progress monitor estimates instruction completeness as the agent approaches the goal, grounding the word \"Stop\" to stop. The agent successfully navigates unseen environments by following grounded instructions and achieving goals. The trajectory shows the agent completing various actions like \"turn right\" and \"walk straight to bedroom\" before stopping at the final destination. The agent successfully navigates unseen environments by following grounded instructions and achieving goals. The trajectory shows the agent completing various actions like \"turn right\" and \"walk straight to bedroom\" before stopping at the final destination. The agent completed another \"turn left\" and successfully stopped at the rug. The completeness estimated by progress monitor gradually increases as the agent navigates toward the goal. In this work, the focus is on Vision-and-Language Navigation task, where an agent is required to follow natural-language instructions in a 3D environment. This task has applications in robotics and differs from traditional map-based navigation systems. In contrast to traditional map-based navigation systems, navigation with instructions offers a flexible way to generalize across different environments. Various approaches have been proposed for the Vision-and-Language Navigation task, including sequence-to-sequence translation models, guided feature transformation for textual grounding, and a planned-head module combining reinforcement learning approaches. A recent approach involves training a speaker to synthesize new instructions for data augmentation and pragmatic inference. This paper introduces a self-monitoring agent for co-grounding in the task. In this paper, a self-monitoring agent is proposed for co-grounding on visual and textual inputs to regulate textual grounding. Visual grounding locates relevant objects in images, while the goal is to choose directions in panoramic photos. Efforts have been made to ground language instructions into action sequences. The self-monitoring agent introduced in the paper focuses on visual-textual co-grounding and progress monitoring to ensure accurate instruction-following. It consists of two modules: visual-textual co-grounding for locating past and future instructions, and a progress monitor for estimating completeness of instruction-following. This approach sets a new state-of-the-art in incorporating structural alignment biases between linguistic structure and action sequences. Our approach sets a new state-of-the-art performance on the Room-to-Room dataset for both seen and unseen environments. The proposed method outperformed existing approaches by a large margin on validation unseen and test sets. Using progress inference for action selection significantly improved performance on the test set, yielding a 13% improvement over the best existing approach. The network architecture includes an embedding dimension of 256 for encoding navigation instructions. The network architecture includes an embedding dimension of 256 for encoding navigation instructions. A dropout layer with ratio 0.5 is used after the embedding layer. The instruction is encoded using a regular LSTM with a hidden state of 512 dimensions. The MLP g is used to project the raw image feature, with a FC layer projecting the input vector to a 1024-d vector. The LSTM for carrying textual and visual information has a hidden state of 512. The attention weights for textual grounding have a dimension of 80. The learnable matrices have dimensions specified in equations 2 to 5. The final selected trajectory from beam search is then logged. The final selected trajectory from beam search is logged, yielding the same success rate and navigation error. Additional qualitative results on the self-monitoring agent navigating seen and unseen environments are discussed, with successful and failure examples provided. In one successful example, the agent focuses on \"walk up\" before shifting attention to \"turn right\" while climbing stairs. The agent pays attention to past and next actions until it grounds on the word \"stop\". It navigates through ambiguous instructions, making decisions based on textual grounding. In one example, the agent stops when it finds a rug inside a room as described in the instruction. The agent must precisely follow step-by-step instructions to successfully complete tasks, stopping only when the correct amount of repeated actions is reached. It focuses on key details in the instructions to make movements, assessing completeness to know when to stop. The agent failed to follow the instruction correctly, as reflected by the progress monitor showing only 16% completion. The instruction was ambiguous, leading to confusion in associating steps with the given directions. Despite this, the agent managed to follow the rest of the instruction accurately. The agent failed to follow the ambiguous instruction accurately, resulting in only 16% completion. The agent missed key steps, such as \"take a left,\" leading to navigation errors. The agent only completed 16% of the instruction accurately, focusing on \"go down\" but failing to connect it with the previous mention of stairs. Despite this, the agent followed the rest of the instruction correctly, reaching a mirror. The final estimated completeness of instruction-following was higher, indicating a lack of awareness of progress towards the goal."
}