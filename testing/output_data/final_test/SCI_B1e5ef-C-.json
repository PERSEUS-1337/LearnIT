{
    "title": "B1e5ef-C-",
    "content": "Low-dimensional vector embeddings, computed using LSTMs or simpler techniques, are a popular approach for capturing the \u201cmeaning\u201d of text and a form of unsupervised learning useful for downstream tasks. The current paper derives formal understanding by looking at the subcase of linear embedding schemes. Using the theory of compressed sensing, it shows that representations combining word vectors are essentially information-preserving linear measurements of Bag-of-n-Grams (BonG) representations of text. This leads to a new theoretical result about LSTMs: low-dimensional embeddings derived from a low-memory LSTM are provably at least as powerful on classification tasks, up to small error, as a linear classifier over BonG vectors. The current study establishes strong unsupervised baselines using word embeddings like GloVe and word2vec, showing their efficiency as sensing matrices for text. Attention has been focused on using LSTMs and similar models to compute text embeddings, with the ability to process text with limited memory and output vectors for measuring text similarity or downstream tasks. The powers and limitations of neural embeddings have not been formally established. The effectiveness of neural embeddings compared to traditional linear classifiers like Bag-of-n-Grams (BonG) is still uncertain. BonG classifiers are known for their strong performance on various tasks, outperforming unsupervised LSTM representations. Linear schemes have shown to provide compact representations that offer similar benefits to word-level LSTM embeddings. Linear schemes involve modifying standard pretrained word embeddings like GloVe or word2vec to create low-dimensional embeddings that preserve n-gram information for text classification tasks. These schemes can also extract original unigram information from the embeddings using sparse recovery techniques. This approach aligns with previous work on distributed representations of structured objects. The new world-view explores the use of random vectors as word embeddings in a linear scheme, proving that low-memory LSTMs are as effective as linear classifiers on the full BonG vector. This theoretical breakthrough challenges previous empirical studies and establishes a novel quantification of text embedding power. The linear embedding scheme improves with pretrained embeddings like GloVe, enhancing Bag-of-Words information preservation. Theoretical justification is provided using sparse recovery property, showing word embeddings outperform random matrices in sensing BoW signals. Empirical results in Section 6 demonstrate competitive accuracy on classification tasks, surpassing previous linear methods. Neural text embeddings outperform previous linear methods in unsupervised word-level representations for sentiment classification tasks. The method utilizes distributed representations that decay gracefully with noise and allow distributed processing, with a focus on fast and simple implementation using standard word embeddings. The approach builds on the holographic distributed representation, which represents structured objects using circular vector convolution and can be quantified using parse trees and graph structures. The method discussed in the curr_chunk focuses on quantifying \"structure\" using parse trees and graph structures. It is related to the sparse distributed memory system of BID18 and involves embedding representations of BonG vectors. Linear projection schemes are used for simplicity and compactness, leveraging word embeddings for efficiency. The goal is to encode BonGs in low-dimensions for easier implementation. The current paper introduces a novel connection to compressed sensing for encoding high-dimensional sparse signals from low-dimensional linear measurements. It defines high-dimensional BonG vectors and low-dimensional embeddings, showing their relationship via compressed sensing. The paper introduces a novel connection to compressed sensing for encoding high-dimensional sparse signals from low-dimensional linear measurements. It defines high-dimensional BonG vectors and low-dimensional embeddings, showing their relationship via compressed sensing. The notation and definitions for vocabulary, n-grams, and Bag-of-Words representation are also provided. The Bag-of-n-Grams (BonG) representation counts the number of times any k-gram for k \u2264 n appears in a document. The BonG approach simplifies by merging n-grams with the same words in a different order, called n-cooccurrences. This modification does not significantly affect performance. The Bag-of-n-Cooccurrences (BonC) vector is defined as the concatenation of n-cooccurrences for a document. Each word has a vector, and given a document, its vector is defined based on the words' vectors. The unigram embedding of a document is a linear measurement of its Bag-of-Words vector. Extending this to n-grams involves representing each n-gram as a tensor product of its constituent words' vectors. The document embedding is then the sum of the tensor representations of all n-grams, but this approach leads to a significant increase in dimensionality. The approach of representing n-grams as tensor products leads to a high-dimensional representation, limiting its practical use beyond n = 2. To address this, a low-dimensional sketch like element-wise multiplication can be used instead of circular convolution. This results in distributed cooccurrence (DisC) embeddings, which preserve the original embedding dimension and have theoretical and practical advantages. The DisC document embedding is defined as the weighted concatenation of DisC vectors of all k-grams in a document, with scaling factors set for equal norm. Comparing with unigram embeddings, LSTMs have been successful in various tasks like classification and sequence-to-sequence. The LSTM model computes document representation by processing words one-by-one with hidden states and activation functions. The final document representation is the state at the last time step. The LSTM requires memory for hidden states and word embeddings. The LSTM model computes document representation by processing words one-by-one with hidden states and activation functions. The final document representation is the state at the last time step. One can initialize an LSTM to construct DisC embeddings, allowing for compressed BonC representations to be computed on the fly and trained using stochastic gradient descent with the same memory. The LSTM model computes document representation by processing words one-by-one with hidden states and activation functions. The final document representation is the state at the last time step. Our main contribution is to provide a rigorous analysis of the performance of text embeddings, showing that they can match BonC performance as \u03b5 \u2192 0 by increasing the embedding dimension d. This theoretical bound demonstrates that LSTMs can provide performance on downstream classification tasks at least as well as any linear classifier over BonCs. Compressed sensing aims to recover high-dimensional sparse signals from linear measurements using the Restricted Isometry Property (RIP) condition. This condition allows for tractable recovery through convex relaxation, providing a tighter bound for recovery. Additionally, a \u03bb-Lipschitz convex loss function is used to minimize the empirical loss function for linear classifiers. The regularized empirical loss function for linear classifiers is minimized in the original space by comparing it to the loss in the compressed space. The minimizer in the original space is a bounded-coefficient linear combination of samples, with its loss depending on inner products between points. By using RIP and a generalization error result, the loss of the regularized classifier in the compressed domain can be bounded. To achieve a bound for k-sparse inputs, the design matrix A transforms BonCs into DisC embeddings. The design matrix A transforms BonCs into DisC embeddings to satisfy the RIP condition. Theorem 4.1 is proven using random vectors as word embeddings. The classifier trained using regularized logistic loss will satisfy the required bound. In practice, LSTMs are often initialized with standard word vectors like GloVe, which do not satisfy traditional compressed sensing properties. Pretrained word embeddings are found to be more efficient than random vectors at encoding and recovering Bag of Words information through compressed sensing. This surprising empirical finding suggests a potential explanation for the efficacy of real-life LSTMs. The study explores the preservation of sparse signals in low-dimensional linear measurements using Basis Pursuit (BP) on subsampled documents from classification datasets. Success is measured by the F1 score of retrieved words, with comparisons to Squared Norm (SN) vectors and Rademacher vectors. The study examines the preservation of sparse signals in low-dimensional linear measurements using Basis Pursuit on subsampled documents from classification datasets. Pretrained word embeddings require lower dimensionality than random vectors for near-perfect recovery, indicating the importance of meaningful word sets in sparse recovery. The study explores the importance of meaningful word sets in sparse recovery using embeddings trained on co-occurrences. Different objectives and corpora yield similar results, with a sensitivity to sparse recovery methods. Pretrained vectors show uncertainty in n-gram embeddings for sparse recovery, suggesting training on word tuples for potential success. The study discusses the challenges of using pretrained word embeddings for sparse recovery, highlighting the limitations of using statistical RIP/incoherence ideas to explain the phenomenon. Instead, the focus is on the local nullspace property (NSP) and related properties for perfect recovery conditions. Additional structure is imposed to formulate an intuitive and verifiable recovery condition in this setting. The study introduces a perfect recovery condition for nonnegative signals using nonnegative BP. Theorem 5.1 states that a nonnegative vector can be recovered from a matrix by BP+ if the columns of the matrix form a k-dimensional face of the convex hull with the origin. This condition holds for incoherent columns. The Supporting Hyperplane Property (SHP) is a weak property implied by NSP, which can be checked using convex optimization to see if a hyperplane exists. It characterizes recovery using BP+ by showing equivalence with the column set forming a k-dimensional face of the convex hull. The Supporting Hyperplane Property (SHP) characterizes recovery using BP+ by showing equivalence with the column set forming a k-dimensional face of the convex hull. It implies the existence of a hyperplane separating the vertices of the face for perfect recovery of a BoW via BP+. The Supporting Hyperplane Property (SHP) characterizes recovery using BP+ by showing equivalence with the column set forming a k-dimensional face of the convex hull. Perfect recovery of a Bag of Words (BoW) via BP+ relies on the existence of a hyperplane separating word embeddings in the document from the rest of the vocabulary. Pretrained vectors are better for sensing as words in the same document have similar embeddings, making them easier to separate out. SHP is more likely to be satisfied by these designs, as shown in FIG4. Comparisons to recovery using OMP/OMP+ reveal differences in sensing properties of embeddings like GloVe. The experiments show that compressed sensing framework is relevant even with non-random, pretrained word embeddings. Theoretical results suggest that simple tensor product sketch-based n-gram embeddings can approach BonG performance. Comparisons are made between different text representations. In this section, text representations including DisC and BonCs are compared on various tasks such as text classification using logistic regression with 2-regularization. Results are reproducible with provided code. Tasks include MR movie reviews, CR customer reviews, SUBJ subjectivity dataset, MPQA opinion polarity, TREC question classification, SST sentiment classification, and IMDB movie reviews. DisC is also tested on SICK relatedness, entailment tasks, and MRPC paraphrase detection. The curr_chunk discusses the use of logistic regression and ridge regression for tasks such as SICK entailment and MRPC paraphrase detection. It mentions the use of 1600-dimensional GloVe embeddings trained on the Amazon Product Corpus for evaluation. The DisC representation consistently performs well in the evaluations. The DisC representation performs well in evaluations, competing with skip-thoughts and CNN-LSTM on various tasks. It serves as a strong baseline, beating more complex approaches while requiring less time and resources. The model approximates BonC performance as dimension increases and isometry distortion decreases. In this paper, the connection between compressed sensing, learning, and natural language representation is explored. The study relates LSTM and BonG methods through word embeddings, introducing new document embeddings based on tensor product sketches. Classification performance is analyzed, with a generalization of compressed learning results and a bound on the loss of a low-dimensional LSTM classifier. Pretrained embeddings are shown to fit into the sparse recovery framework, efficiently preserving natural language information. The field of compressed sensing aims to recover high-dimensional sparse signals from few linear measurements using 1-norm minimization. Methods like Basis Pursuit Denoising and Dantzig Selector are used for noisy cases, but in noiseless scenarios, they reduce to Basis Pursuit. This approach efficiently preserves natural language information. The method recovers x from Ax using a greedy algorithm like matching pursuit or orthogonal matching pursuit. Recovery is guaranteed by the Restricted Isometry Property (RIP), which characterizes matrices A for signal recovery. Learning after compression is considered for information preservation. The hinge loss of a classifier trained on compressed data is bounded by that of the best linear classifier over the original samples. Theorem 4.2 extends this result to any convex Lipschitz loss function. RIP is a strong requirement for stable recovery of k-sparse vectors. The local nullspace property (NSP) is necessary to recover any vector x from a matrix A. The local restricted eigenvalue property (REP) is a condition that implies the nullspace property (NSP) for stable recovery of sparse vectors. Incoherence is a simple condition that can sometimes provide recovery guarantees, but word embeddings tend to have high coherence. The polytope condition enforces nonnegative NSP (NSP+), a weaker form of NSP. The polytope condition enforces nonnegative NSP (NSP+), a weaker form of NSP, which is equivalent to the polytope condition in Theorem A.2. This condition allows for stable recovery of sparse vectors. The polytope condition enforces nonnegative NSP (NSP+), which is equivalent to the polytope condition in Theorem A.2. Table 3 compares the performance of l2-regularized logit classifier over Bag-of-n-Grams (BonG) vectors with Bag-of-n-Cooccurrences (BonC) vectors. Table 4 compares element-wise product (DisC) and circular convolution for encoding local cooccurrences. The section compares alternative representations with the main evaluation in TAB1. Table 3 provides a numerical comparison between unordered n-grams and n-grams, showing similar performance. Table 4 explores circular convolution as a linear measurement of BonC vectors BID30. The construction involves computing a discrete Fourier transform. The LSTM update equation is derived by substituting parameters and assuming h0 = 0. The proof of Theorem 4.2 assumes a specific setting and defines a regularization function with a convex \u03bb-Lipschitz function. The optimal solution is obtained by taking first-order conditions, and the sub-gradient is bounded by \u03bb. The compressed domain also satisfies this result. The first-order optimal solution of \u0175 can be expressed as FORMULA1 with \u03b1 i satisfying |\u03b1 i | \u2264 \u03bbC m. Since 0 N \u2208 X, A is also (X, \u03b5)-RIP. The proof involves expanding \u0175 2 2, applying Lemma C.2, and using x 2 \u2264 R. Lemma C.3 states that \u0175 minimizes L S. By applying Lemma C.1 and Lemma C.2, the result is obtained. Substituting Equation 14 and applying Lemma C.2 yields the desired result. The proof involves bounding the loss term and the regularization term to imply the result. With probability 1 \u2212 \u03b3, the linear classifier minimizing L S is contained in a closed convex subset independent of S. Applying Lemma C.4 in the compressed domain yields the desired result. The proof involves showing that the matrix A satisfies the RIP condition when multiplying vectors x from a set of BonC vectors for documents of length at most T. The matrix A can be expressed as a matrix of embeddings. By applying a union bound, it is proven that A satisfies the RIP condition with high probability. The proof involves showing that the matrix A satisfies the RIP condition when multiplying vectors x from a set of BonC vectors for documents of length at most T. The matrix A can be expressed as a matrix of embeddings. By applying a union bound, it is proven that A satisfies the RIP condition with high probability. The square root of both sides of both inequalities completes the proof by defining a bounded orthonormal system (BOS) with constant B. The proof involves showing that the matrix A satisfies the RIP condition when multiplying vectors x from a set of BonC vectors for documents of length at most T. By applying a union bound, it is proven that A satisfies the RIP condition with high probability. The square root of both sides of both inequalities completes the proof by defining a bounded orthonormal system (BOS) with constant B. The set of functions associated with the p-grams in the vocabulary is a BOS with constant B = 1, and Lemma 4.1 is proven by applying Lemma D.1. The Supporting Hyperplane Property (SHP) characterizes when BP+ perfectly recovers a nonnegative signal. The proof involves showing that the matrix A satisfies the RIP condition when multiplying vectors x from a set of BonC vectors for documents of length at most T. By applying a union bound, it is proven that A satisfies the RIP condition with high probability. The square root of both sides of both inequalities completes the proof by defining a bounded orthonormal system (BOS) with constant B. The set of functions associated with the p-grams in the vocabulary is a BOS with constant B = 1, and Lemma 4.1 is proven by applying Lemma D.1. Corollary 5.1 shows that SHP is a weaker condition than NSP, and provides the proof for verifying SHP with a design matrix and a set of support indices. The proof involves showing that matrix A satisfies the RIP condition when multiplying vectors x from a set of BonC vectors for documents of length at most T. By applying a union bound, it is proven that A satisfies the RIP condition with high probability. Corollary 5.1 shows that SHP is a weaker condition than NSP, and provides the proof for verifying SHP with a design matrix and a set of support indices. Checking S-SHP allows us to determine if all nonnegative signals with index support S will be recovered by BP+ without running the optimization. The problem of sparse recovery can be solved using standard convex optimization algorithms. An implementation can be found at https://github.com/NLPrinceton/sparse_recovery. Efficiency of pretrained embeddings as sensing vectors was tested at d = 300 dimensions, with word2vec and GloVe vectors. 200 documents from each dataset were compressed and recovered. All embeddings had the same number of words for fairness. The efficiency of word embeddings as linear measurement vectors for Bag of Words signals was tested using embeddings trained on various corpora. Different embedding objectives and corpora showed similar effectiveness when normalized, with SN vectors being the most efficient without needing normalization for good performance. The efficiency of word embeddings as linear measurement vectors for Bag of Words signals was tested using embeddings trained on various corpora. Pretrained word embeddings are efficient sensing vectors for natural language BoW without needing normalization for good performance. A model-based justification is discussed to understand the relationship between BoW generation and word embeddings trained over words co-occurring in the same BoW. The model generates a document by setting a context vector and emitting words. The document vector is the sum of word embeddings. Empirical findings show that not all closest words to the context vector are emitted, as some words may have negative inner product with the context vector but can still be recovered. The model can recover words with negative inner product with the context vector, posing a challenge for theoretical arguments. Future work is needed to explain why embeddings from this model serve as efficient sensing matrices for natural language Bag of Words."
}