{
    "title": "r1gNni0qtm",
    "content": "Recurrent Neural Networks (RNNs) are efficient at solving sequential data problems, with some theories suggesting depth efficiency. This study extends the analysis to RNNs using different nonlinearities like ReLU, showing universality and depth efficiency benefits. Computational experiments validate the theoretical results, establishing RNNs as a top choice for processing sequential data. Recurrent Neural Networks (RNNs) excel in processing sequential data like text, audio, or video due to their depth efficiency. Recent research compares deep learning architectures, showing that ConvNets with product pooling and RNNs with multiplicative recurrent cells are exponentially more expressive than shallow networks. Extending this analysis to RNNs with rectifier nonlinearities poses a challenge. The challenge in analyzing RNNs with rectifier nonlinearities lies in the inability to apply standard tensor decomposition tools. To address this, generalized tensor decompositions are proposed to demonstrate universality and depth efficiency in such networks. Tensor methods have a successful history in machine learning, such as in improving face recognition accuracy and word embeddings models. These methods also enhance recommender systems for better accuracy and robustness. Tensor decompositions have been utilized in various applications of deep learning algorithms, such as representing filters in convolutional networks and compressing fully-connected layers in neural networks. These methods have also been used to improve the performance of recurrent networks in long-term forecasting and video classification tasks. Additionally, tensor decompositions have been instrumental in analyzing theoretical aspects of deep neural networks. Recent research has explored the connection between tensor decompositions and neural networks, showing that CP and Hierarchical Tucker decompositions correspond to shallow and convolutional networks respectively. TT decomposition can be represented as a recurrent network with multiplicative connections, providing a performance boost over standard RNN models. Theoretical results have been extended to analyze more practical deep neural networks, including CNNs with ReLU nonlinearities and dilated convolutions. In this paper, new tools are presented for analyzing theoretical properties of complex nonlinear RNNs with rectifier nonlinearities. The connection between tensor decompositions and multiplicative architectures is discussed, with a focus on networks with ReLU nonlinearities. The paper introduces a parametric feature map to preprocess data before feeding it into the network. The paper discusses the connection between tensor decompositions and multiplicative architectures in RNNs with rectifier nonlinearities. It introduces a parametric feature map for preprocessing data before inputting it into the network. The feature map involves an affine map followed by a nonlinear activation function. The score functions involve trainable weight and rank 1 feature tensors, utilizing the outer product operation in tensor calculus. Equation 2 is noted for its universal approximation property. The paper discusses tensor decompositions in neural networks, using the CP decomposition to compactly represent weight tensors. The CP-rank is the minimal value of R for the decomposition equation to exist. Outer products are used in equations to generalize the notation for later use. Tensor Train (TT) decomposition is another tensor decomposition method defined by Oseledets in 2011. It involves three-dimensional tensors G(t) and TT-cores with minimal TT-ranks. The score function for TT decomposition exhibits a recurrent structure similar to that of RNN, with defined hidden states r0r1. The score function for TT decomposition exhibits a recurrent structure similar to RNN, with defined hidden states allowing for a more compact form. The proof of Lemma 3.1 and main results were moved to Appendix A due to limited space. Using TT-cores, equation (9) can be rewritten in a more convenient index form, involving tensor contraction. By combining weights from G(t) and f\u03b8(\u00b7) into a single variable \u0398(t)G, a vector form is obtained, where G depends on the time step. Simplified hidden state equations can be derived by setting specific conditions, applicable to standard recurrent architectures. The TT decomposition score function resembles an RNN structure with defined hidden states. Equations can be simplified by introducing practical nonlinearities using a generalized operator. Associative and commutative binary operators can replace outer products to enhance the neural network structure. The generalized outer product operator allows for the creation of neural networks with various nonlinearities, such as rectifier, softplus, and simple RNNs. Different networks are analyzed based on their score functions and parameters. The generalized outer product operator enables the creation of neural networks with different nonlinearities. Parameters of the network involve matrices acting on input states to achieve the property of representing generalized shallow networks as RNNs of width 1. Generalized RNNs require an initial hidden state, which is initialized as the unit of the operator \u03be for theoretical and practical convenience. The introduction of generalized outer product allows for the investigation of RNNs with a wide range of nonlinear activation functions, particularly ReLU. While this change is practical, it complicates theoretical analysis as transitioning from networks to tensors is not straightforward. Previous analysis compared expressivity of networks by examining properties of weight tensors, enabling comprehensive evaluation of score functions. However, this method cannot be applied to generalized tensor networks. The introduction of generalized outer product complicates theoretical analysis for RNNs with nonlinear activation functions like ReLU. Transitioning to generalized tensor networks leads to loss of conformity with weight tensors, affecting properties like universality. The use of grid tensors is proposed for proper theoretical analysis. The introduction of grid tensors for generalized RNNs complicates theoretical analysis by considering a finite grid of template vectors for score function evaluation. The matrix F holds values of the representation function on selected templates, leading to a specific form of the grid tensor for shallow networks. The construction of the grid tensor for generalized RNNs is more complex but computable. The grid tensor for generalized RNNs can be computed recursively, similar to the hidden state in a single input sequence. The formulas are complex and moved to Appendix A. The analysis compares the expressive power of generalized RNNs and shallow networks with rectifier nonlinearity, assuming certain templates and an invertible feature matrix. Theorem 5.1 states that generalized RNNs with rectifier nonlinearity are universal, able to represent any tensor of order T and size m. This result applies to both generalized shallow networks as well. Theorem 5.1 shows that generalized RNNs with rectifier nonlinearity are universal, capable of representing any tensor of order T and size m. This result also applies to generalized shallow networks. Additionally, Lemma 5.1 states that the collection of grid tensors of generalized RNNs with any nonlinearity is closed under linear combinations, which also holds for generalized shallow networks. Lemma 5.2 demonstrates the existence of a generalized RNN with rectifier nonlinearities that satisfies a specific grid tensor condition. Theorem 5.1 proves that generalized RNNs with rectifier nonlinearity are universal for representing any tensor of order T and size m. This universality also extends to generalized shallow networks. Lemmas 5.1 and 5.2 demonstrate the closed collection of grid tensors for these networks with specific conditions. The networks with certain nonlinearities like max(x, y, 0) and xy are shown to be universal in representing any grid tensor. Expressivity of networks refers to their ability to represent functions compactly. ConvNets and RNNs with multiplicative nonlinearities are more expressive than shallow networks. Generalized RNNs with rectifier nonlinearity are only partially expressive. Theorem 5.2 states that for every value of R, there exists a generalized RNN with ranks \u2264 R and rectifier nonlinearity that is exponentially more efficient than shallow networks. This means that exponentially wide shallow networks are needed to realize the same grid tensor as the RNN. The proof involves constructing an example of such an RNN that measures pairwise similarity of input vectors. In Appendix A, a proof is provided for the property of expressivity in multiplicative RNNs. However, generalized RNNs with rectifier nonlinearities do not possess this property for every rank R. Numerical experiments show that the probability of obtaining CP-ranks of polynomial size becomes negligible with large T and R. All RNNs used in practice have shared weights to process sequences of arbitrary length. By imposing the constraint of shared weights, the property of universality is lost. In this section, the study investigates if theoretical findings on generalized tensor networks are supported by experimental data. The focus is on practical applications, particularly in problems like natural language processing typically solved by RNNs. Additionally, the experiment explores the low rank property of equivalent shallow networks for a subset of RNNs, with numerical estimates computed in various settings using computer vision datasets MNIST BID16 and CIFAR-10. The study uses computer vision datasets MNIST BID16 and CIFAR-10, along with the IMDB dataset for sentiment analysis. Test accuracy on IMDB dataset shows that a higher rank shallow network is needed to match the performance of a generalized RNN. Results on visual datasets are moved to Appendix B. Expressivity experiments involve generating generalized RNNs with different TT-rank values to estimate the rank of shallow networks needed for the same grid tensor realization. The study explores the rank of shallow networks needed to match the performance of generalized RNNs on various datasets. Results show that as the rank of generalized RNNs increases, exponentially wider shallow networks are required to implement the same functions. Numerical results for different scenarios are provided in Appendix B. In this paper, the connection between Recurrent Neural Networks and Tensor Train decomposition is explored, focusing on incorporating nonlinearities into network architectures. Theoretical analysis on rectifier nonlinearity is provided, with plans to extend the analysis to architectures like LSTMs in future work. Theoretical results aim to advance understanding of RNNs. The generalized outer product in eq. (16) can be replaced with the standard outer product without loss of generality, subsuming matrices C(t) into tensors G(t). The grid tensor of a generalized shallow network and a generalized RNN are defined in eq. (20) and eq. FORMULA0 respectively, with corresponding elements computed using specific formulas. The generalized RNN with grid tensors \u0393 A (X), \u0393 B (X), and arbitrary \u03be-nonlinearity can be represented by a generalized RNN with grid tensor \u0393 C (X) using specific weight settings. This network can be represented in a form of generalized RNN with unit ranks for any associative and commutative binary operator \u03be. The generalized RNN with grid tensors \u0393 A (X), \u0393 B (X), and arbitrary \u03be-nonlinearity can be represented by a generalized RNN with grid tensor \u0393 C (X) using specific weight settings. This network can be represented in a form of generalized RNN with unit ranks and \u03be-nonlinearity. The hidden states of generalized RNN have a specific form, and there exists an open set of generalized RNNs with rectifier nonlinearity that can be realized by a rank 1 shallow network. The weight settings for a generalized RNN result in a constant tensor \u0393(X) with entries of 2(MR)T-1. This property holds under small perturbations denoted collectively by \u03b5. The grid tensors obtained can be represented using rank 1 generalized shallow networks with specific weight settings. In this section, additional computational experiments are provided to analyze generalized RNNs using different \u03be-nonlinearities. Theoretical analysis of rectifier nonlinearity is presented, along with the performance of various nonlinearities on different datasets for classification. The experiments analyzed the impact of different nonlinearities on MNIST, CIFAR-10, and IMDB datasets for classification. Optimal nonlinearity choice significantly improved performance. Parameters were optimized using Adam with a learning rate of 10^-4 and batch size of 250. Expressivity experiments showed that rank 1 generalized shallow networks can achieve similar results as higher rank networks, but less likely for larger values of R. Distribution of lower bounds on the rank of generalized shallow networks equivalent to randomly generated generalized RNNs of ranks (M = 6, T = 6, \u03be(x, y) = max(x, y, 0) and \u03be(x, y) = x^2 + y^2)."
}