{
    "title": "rJeoKJ3NKr",
    "content": "Our proposed inference network architecture generalizes to unseen probabilistic queries by training a single network directly from data. This approach outperforms existing methods on benchmark datasets, providing fast and flexible inference. The proposed approach outperforms existing methods on benchmark datasets by learning the parameters of an undirected probabilistic graphical model with hidden variables using maximum likelihood. The intractability of the problem is twofold when hidden variables are present, making probabilistic queries not tractable, requiring approximations like belief propagation or variational inference. Approximations like belief propagation or variational inference are used in ML learning for directed and undirected models. In the latter case, learning is even harder. Amortized inference, such as variational autoencoders, addresses the computational intensity of iterative solutions for approximate inference. Variational autoencoders (VAEs) are faster than actual variational inference (VI) optimization but are limited to predefined queries. Belief propagation (BP) and VI can answer arbitrary queries but require more computation time. The goal of learning PGM parameters is to answer arbitrary probabilistic queries, allowing tasks like inpainting in images. This work suggests learning a system for arbitrary probabilistic queries without ML learning. The approach proposed in this work, called query training (QT), aims to train a neural network to answer arbitrary probabilistic queries without traditional machine learning learning methods. This method sidesteps the difficulties associated with the partition function and allows directed and undirected models to be equally usable. The process involves unrolling inference into a neural network, randomizing queries during training to find a consistent parameterization that generalizes to new queries. The density in undirected models can be expressed as a function with visible and hidden variables. Query training allows for learning parameters to answer arbitrary probabilistic queries, unlike traditional methods. This method aims to generalize to new queries by unrolling inference into a neural network and randomizing queries during training. In this work, the focus is on training a system to answer arbitrary probabilistic queries without relying on machine learning. By unrolling inference into a neural network and randomizing queries during training, the goal is to find a consistent parameterization that generalizes to new queries. This approach aims to put directed and undirected models on equal footing in terms of usability. The approach in this work focuses on query training (QT) in a graphical model with a single parameterization. A query is a binary vector that partitions visible variables into inputs and outputs. During training, random queries are used to estimate conditional probabilities without relying on machine learning. This method aims to generalize to new queries and equalize directed and undirected models in usability. The QT-NN computes cross-entropy between true and estimated outputs using random query masks. It processes input through N stages to estimate the sample. The network takes a sample and query mask as input, blocking access to output variables. The QT-NN is a flexible version of the encoder in a VAE, estimating variables based on input and query masks. The network minimizes loss through stochastic gradient descent, using cross-entropy to measure accuracy. QT-NN provides inference results for unobserved variables based on input and query masks. A feedforward function approximates arbitrary inference queries by unrolling parallel BP equations. Unary factors are created from available evidence and queries, with soft evidence incorporated as needed, resulting in an informative density vector. The QT-NN uses a unary vector of factors u to encode inputs and queries, enabling parallel BP equations for inference. Messages between layers are directional, with the input term u re-fed at each layer. The network outputs a belief for each variable obtained through a softmax in the last layer. The QT-NN uses a unary vector of factors u to encode inputs and queries, enabling parallel BP equations for inference. Messages between layers are directional, with the input term u re-fed at each layer. The network outputs a belief for each variable, obtained through a softmax in the last layer. The parameters \u03b8 relevant to the factor between variables i and j are represented by \u03b8 ij = \u03b8 ji, and the portion that only affects variable i is contained in \u03b8 i. All layers share the same parameters, and the functions f \u03b8 ij (\u00b7) are derived from \u03c6(x; \u03b8) using the BP equations. Parameter T is the \"temperature\" of the message passing, allowing for flexibility in the NN. The QT-NN uses a unary vector of factors u for parallel BP equations in inference. Query training is superior to pseudo-likelihood in handling hidden variables and high correlations in input data. It removes multiple variables to better leverage information sources. In the context of query training with neural networks, recent approaches like NVIL and AdVIL have been developed, with AdVIL being considered superior. Experiments using RBM compare QT with PCD, showing competitive results. AdVIL results are also presented, using the same datasets and preprocessing as the original paper. Random queries are generated for testing, and the normalized cross-entropy (NCE) metric is used for evaluation. The text discusses the advantages of sparse connectivity patterns in fully connected graphs for message passing. It compares different methods like PCD, AdVIL, and QT for inference queries, with QT-NN being simpler to compute. PCD-BP and AdVIL-BP are alternative methods using RBM weights. Training details for PCD, AdVIL, and QT are provided, with validation sets used for evaluation. QT-NN is trained in 10 layers using ADAM with a validation set for learning rate selection and early stopping. Results in Table 3 show QT-NN outperforms other methods for most datasets. Query training eliminates the need for estimating partition functions, produces faster and more accurate inference networks, and can solve arbitrary queries. The QT-NN is trained in 10 layers using ADAM with a validation set for learning rate selection and early stopping. Results show QT-NN outperforms other methods for most datasets by eliminating the need for estimating partition functions and producing faster and more accurate inference networks. The worry is that only a small fraction of the exponential number of potential queries is seen during training, which may lead to query overfitting. Training queries must come from the same distribution as test queries to avoid this issue. In future work, QT can be applied to more complex models like grid MRFs. Modifications may allow sample generation and other inference mechanisms. Hadamard product with q removes information not in query mask, replacing with 0 for uniform binary distribution. Network output is v and \u0125, representing inferred probabilities for visible and hidden units. \u0125 is inferred but not used during training. Computation of f w (x) is designed as specified above. The computation of f w (x) is designed to be numerically robust, starting with f MP w (x) for T = 0 and then correcting for positive temperatures."
}