{
    "title": "Byni8NLHf",
    "content": "The text discusses a method for topic modeling that involves generating word/word-pairs from documents, applying a TF-IDF algorithm for semantic filtering, and using k-means algorithm to merge word pairs with similar meanings. Experiments on the OMDb, Reuters Dataset, and 20NewsGroup Dataset show that our proposed data preprocessing improves topic accuracy by up to 12.99\\%. Comparing results with other topic models like Latent Dirichlet allocation and Restricted Boltzmann Machines. Adjusting the number of clusters and word pairs for different types of text documents is also discussed. In the big data era, text-based knowledge dominates the vast digital database on the Internet. Machine learning techniques in e-commerce utilize customer preferences to make personalized recommendations, such as suggesting related books or movies based on past purchases. Topic modeling algorithms analyze text descriptions to provide content-based recommendations, converting documents into topic vectors for similarity comparison. These algorithms are used in text-mining, preference recommendation, and computer vision applications. The traditional topic models like Latent Semantic Indexing (LSI), Probabilistic Latent Semantic Analysis (PLSA), and Latent Dirichlet Allocation (LDA) use different methods to extract latent topics from text data. These models are trained and tested on manually categorized online documents like IMDb movie reviews and Wikipedia articles. The paper explores building a topic model using explicit semantic analysis and investigates the use of semantic knowledge from language analysis or existing dictionaries like WordNet. The main contributions include redesigning a new topic model that combines two types of text features and applying numerical methods for data processing and feature extraction in topic modeling and information retrieval. The paper introduces a new topic model that combines text features and applies numerical methods for data processing. It compares various topic models like LDA, LSA, word2vec, and RBM, highlighting their pros and cons. The paper discusses topic modeling techniques such as LDA, LSA, and word2vec. LDA uses sparse Dirichlet prior distributions for document-topic and topic-word relationships, while LSA learns latent topics through matrix decomposition. Word2vec generates word vectors from large datasets. The training process for word2vec involves generating word-context pairs and training word embeddings using CBOW and skip-gram models. Extensions of word2vec include paragraph2vec and LDA2vec, which focus on learning vector representations for text pieces and document-wide feature vectors, respectively. The LDA2vec model combines word and document vectors in the same space, training them simultaneously. The BID18 RBM extracts latent semantic representations from documents using a bipartite graphic architecture. The RBM model outperforms LDA in log-probability and retrieval accuracy. A Deep Belief Network (DBN) was developed based on stacked RBMs. The current RBM model for topic modeling uses a Bag of Words approach. The Bag of Words approach is used for topic modeling, with a focus on word pairs and dependencies. Stanford natural language parser is utilized to extract word pairs from sentences. The relationship between words is represented by segments in each word pair. The Bag of Words approach for topic modeling focuses on word pairs and dependencies extracted using the Stanford natural language parser. The TF-IDF weight is a statistical measure used to evaluate the importance of words in a document collection. The first 10000 most frequent word pairs are kept in the word pair dictionary to avoid increasing the dictionary size and reducing performance. The TF-IDF algorithm evaluates word importance in a document collection by considering word frequency and corpus frequency. It helps filter out high-frequency but irrelevant words, like \"first\" and \"name\", from the training dataset. Proposing a two-step TF-IDF processing method to filter word pairs based on their TF-IDF scores. The method generates word pairs, applies word-level TF-IDF, and filters out pairs with scores below the threshold. Clustering semantically close word pairs is done to reduce the dictionary size. The text discusses using WordNet to determine cluster centroids for semantically close word pairs in a clustering approach. The method involves building a word level tree based on WordNet relations and selecting words closest to the root as centroids. The approach can be further divided into min-path clustering and max-path clustering based on the selected path. The text discusses using K-means clustering to group word embedding vectors into clusters. Despite initial doubts, experimental results show that K-means clustering is effective. The approach involves generating topic distributions for documents using RBM model and retrieving top N documents based on Euclidean distance. The method is evaluated on 3 datasets: OMDb, Reuters. The proposed method involves using K-means clustering to group word embedding vectors into clusters. The datasets used for evaluation are OMDb, Reuters, and 20NewsGroup, with each dataset divided into training, validation, and testing sets. The OMDb dataset contains movie descriptions, the Reuters dataset contains news articles, and the 20NewsGroup dataset contains documents. K-means clustering is applied to define classes for movies and news articles. The dataset consists of 7674 documents divided into 8 classes. The training, validation, and testing datasets contain different numbers of news articles. The evaluation method used is mean Average Precision (mAP) score, which considers the order of information retrieval results. Different mAP values are used to evaluate retrieval performance. The retrieval performance is evaluated using mean Average Precision (mAP) score, which considers the order of information retrieval results. Relevant documents are retrieved based on topic vectors and class labels. The mAP score is calculated by averaging the Average Precision (AveP) for each document. The experiment compares the performance of two RBM models based on input features. The experiment evaluates two RBM models with different input features on the OMDb dataset and Reuters dataset. The word/word pair combined model consistently outperforms the word-only model in terms of mAP scores, with the most significant improvements seen at feature numbers 11000 and 12000. The combined model consistently outperforms the word-only model in terms of mAP scores across different datasets. The most significant improvements are seen at specific feature numbers for each dataset, with improvements ranging from approximately 1% to over 10%. In the second experiment, different K values were tested to see how they affect the effectiveness of generated word pairs for topic modeling. Results showed that all K values outperformed the baseline model, with the most significant improvement seen at K = 100, ranging from 2.41% to 4.46% for mAP scores. In the experiment, larger K values led to better mAP scores for word/word pair combination models in both Reuters and 20NewsGroup datasets. Significant improvements were observed at K = 500 for Reuters dataset and K = 800 for 20NewsGroup dataset, with improvements ranging from 0.31% to 0.50%. The most significant improvements were seen at K = 1000 for both datasets, with improvements ranging from 2.82% to 3.33%. In the experiment, larger K values led to better mAP scores for word/word pair combination models in both Reuters and 20NewsGroup datasets. The combined model outperformed the baseline model when K was greater than 800, with the best results seen at K = 800 or K = 1000. For the OMDb dataset, the combined model consistently outperformed the baseline model due to the low mAP score of the baseline model. Different word pair generation algorithms were compared, with the \"semantic\" word pair generation method proposed in the paper showing promising results. The experiment compared different word pair generation algorithms, with the \"semantic\" method showing better mAP scores than the \"Non-K\" method. The K-means clustering in the semantic method helps avoid redundancies in the input space, leading to a slightly higher mAP score compared to the Non-K method. The N-gram word pair generation has lower mAP scores than the baseline due to introducing meaningless word pairs, acting as noise in the input. Including word pairs without semantic importance does not improve model accuracy. Proposed techniques optimize the original RBM model during dataset processing. The original RBM model was optimized by processing the dataset. Word pairs were extracted using a semantic dependency parser and filtered using TF-IDF processing. Kmeans clustering merged similar word pairs and improved topic prediction accuracy. The proposed word/word pair combined model showed significant improvements in mAP scores across different datasets."
}