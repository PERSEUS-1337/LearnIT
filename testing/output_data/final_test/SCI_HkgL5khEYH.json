{
    "title": "HkgL5khEYH",
    "content": "Variational Auto-encoders (VAEs) are deep generative latent variable models with a generative model capturing data distribution and an inference model inferring latent codes. Recent research highlights issues with traditional training methods leading to mismatches in learned latent codes and the prior distribution, affecting the generative model's ability to produce realistic data samples. In this paper, it is shown that VAE training issues arise from undesirable global optima. The generative model is unidentifiable, leading to various models with different properties, and bias in the VAE objective may favor poorly explaining models with easy-to-approximate posteriors. A new inference method, LiBI, addresses these problems by learning better generative and inference models compared to traditional methods on synthetic datasets. Generative latent variable models consist of a generative model capturing data distribution and an inference model inferring likely latent codes. Traditional training methods often result in generative models ignoring latent codes and failing to match the prior distribution, leading to unrealistic data generation. These issues stem from global optima of the VAE training objective. The global optima of VAE training often lead to undesirable solutions due to unidentifiability of generative models and bias in the VAE objective. A novel inference method, LiBI, addresses these issues by learning generative models that capture data distribution and inference models that better satisfy modeling assumptions. LiBI outperforms traditional methods on synthetic datasets. In the context of addressing issues with VAE training, a novel inference method called LiBI aims to learn generative models that capture data distribution and inference models that better satisfy modeling assumptions. This method outperforms traditional approaches on synthetic datasets by maximizing the variational lower bound (ELBO) and inferring latent codes efficiently. The VAE model compromises the generative model's quality by trading off between explaining data well and having easy-to-approximate posteriors. This can lead to models that fail to capture the data distribution and mismatch with the prior. An example model with specific parameters demonstrates this compromise. The VAE model compromises the generative model's quality by trading off between explaining data well and having easy-to-approximate posteriors. This can lead to models that fail to capture the data distribution and mismatch with the prior. An example model with specific parameters demonstrates this compromise, selecting a model with a simple posterior even when restricted to models that fit the data well. The VAE model compromises between data explanation and simple posteriors. An example model with fixed A and learned B shows that while all \u03b8 choices explain data equally well, the VAE objective favors \u03b8 with less correlation in posteriors. Figures illustrate the preference for high upper diagonal values in B and low lower diagonal values. The data to latent code mutual information for the selected \u03b8 is suboptimal. The VAE objective may select suboptimal models with uninformative latent codes, even when the true data generating model is informative. Joint training of inference and generative models can lead to unintended optima, especially in non-linear VAEs. Learning the inference model can bias the generative model selection, even with a rich variational family. The generative model is nonidentifiable under the MLE objective, leading to multiple global minima that maximize the data likelihood. The variational family can contain posteriors of multiple models, some of which may not meet desired criteria. The mean-field variational family can fully minimize the posterior matching objective. The text discusses how posterior collapse can occur at global optima of the VAE objective, even with improvements to the inference model or limitations on the generative model's capacity. Common issues with traditional VAE training stem from the non-identifiability of the likelihood and bias towards models with uninformative latent codes. The text proposes a novel inference method to address the bias of the VAE objective towards models with simple posteriors and the non-identifiability of the likelihood. It suggests decoupling the training of generative and inference models to avoid biasing effects and undesirable global optima of the likelihood. Task-specific constraints can be incorporated to ensure informative latent codes are necessary for the task. In the proposed approach, called Likelihood Before Inference (LiBI), constraints are added only to the generative model to improve task-specific modeling desiderata. This decoupling of training aims to address bias in VAE training towards simple posteriors and non-identifiability of the likelihood. In the LiBI approach, constraints are applied to the generative model to improve modeling desiderata. This involves computing joint maximum likelihood estimates for \u03b8 and z n, and learning \u03c6 to compute approximate posteriors q \u03c6 (z|x). The process is repeated to encourage the generative model to capture p(x) given p(z). In the LiBI approach, constraints are applied to the generative model to improve modeling desiderata by initializing h(x n ; \u03d5) = \u00b5(x n ; \u03c6) for intelligent random initialization. Step 3 improves the quality of the generative model, with a comparison to existing inference methods on synthetic data sets. In this paper, a novel training procedure called LiBI is proposed to address issues with VAE training. LiBI avoids undesirable optima while maintaining the tractability of traditional VAE inference. It is shown that LiBI can learn generative models that capture the data distribution and inference models with aggregated posterior matching the prior, outperforming traditional methods on synthetic datasets. Traditional methods struggle with issues such as posterior collapse and mismatch between aggregated posterior and prior in VAE literature. Posterior collapse occurs when the posterior equals the prior, yet the model can still generate samples from the data. Existing literature focuses on mitigating model collapse through modifying optimization procedures, choosing variational families, or adjusting the model structure. Posterior collapse in VAE literature occurs when the likelihood changes during training, leading the model to ignore the inference network's output. The mismatch between aggregated posterior and prior is when q(z) = p(z), despite expectations that they should match. In VAE literature, issues like posterior collapse and mismatch between aggregated posterior and prior can occur as global optima of the VAE objective. Existing methods address these issues by adjusting the prior flexibility or developing robust sampling methods from the latent space. Traditional VAE inference struggles to resolve issues like posterior collapse and mismatch between aggregated posterior and prior at global optima. Comparing traditional VAE inference with LiBI on a synthetic dataset, it is shown that traditional inference learns a generative model \u03b8 that approximates posteriors easily but fails to capture the data distribution. The generative process assumes p \u03b8 (x) = N 0, \u03c3 2 with \u03b8 controlling I(X; Z). The mutual information between x and z is computed, and the posterior p \u03b8 (z|x) is derived for this univariate example. The model involves a generative process with a diagonal matrix B and posterior p \u03b8 (z|x) as a Gaussian. The mean-field Gaussian variational family is used, but it does not include the true posterior. The best-fitting mean-field approximation can be computed using a diagonal matrix B and a full-covariance matrix \u03a3. The LiBI framework involves two steps: learning a high-quality likelihood for data generation and performing inference to learn latent codes. Various methods can be used for each step, such as GAN for Step 1 and MCMC sampling for Step 2. A tractable approximation to Step 1 is derived in Equation 31, using a single sample to make the corresponding data most likely. This approach is amenable to gradient-based optimization methods and can be enhanced to include task-specific constraints. The LiBI framework involves learning a high-quality likelihood for data generation and performing inference to learn latent codes. A problem arises with biased learning towards z n 's close to 0 in the MLE estimate. Non-identifiability in the MLE estimate is discussed, along with a solution. Consideration is given to alternative generative processes with equal data marginals and likelihoods. The issue is addressed by preferring alternate parameters Z, \u03b8 when c > 1 in the joint log-likelihood. The LiBI framework involves learning a high-quality likelihood for data generation and performing inference to learn latent codes. When c > 1, the joint log-likelihood prefers alternate parameters Z, \u03b8 to avoid biased learning towards z n 's close to 0 in the MLE estimate. This is achieved by constraining z n 's to be Gaussian using the Henze-Zirkler test for Gaussianity and by constraining the empirical mean and covariance of the z n 's to be that of the standard normal. The LiBI framework involves learning a high-quality likelihood for data generation and performing inference to learn latent codes. The framework encourages the likelihood to satisfy modeling assumptions, ensuring accurate reconstruction of x's given Gaussian z's. The LiBI Inference Method incorporates constraints as smooth penalties into the Lagrangian and defines a neural network parameterized by \u03d5 to return values for x n. The LiBI framework involves learning a high-quality likelihood for data generation and performing inference to learn latent codes. A neural network parameterized by \u03d5 returns the specific z n that generated x n. The process involves minimizing a loss function to optimize the parameters \u03b8 and \u03d5."
}