{
    "title": "H1x-3xSKDr",
    "content": "Batch normalization (BN) is commonly used to stabilize and speed up training in deep neural networks, reducing parameter updates needed for low training error. However, it also decreases robustness to adversarial perturbations and corruptions. Substituting weight decay for BN can eliminate the relationship between vulnerability and input dimension. The vulnerability observed is attributed to the tilting of decision boundaries along input dimensions of low variance, rather than gradient explosion from BN usage. Batch normalization (BN) is a standard component in deep neural networks that stabilizes training and reduces sensitivity to hyperparameters. However, it can decrease robustness to adversarial examples. Tuning the constant in BN can improve robustness at the expense of test accuracy. Various methods aim to enhance robustness, but effectively doing so remains a challenge. In this work, the impact of Batch Normalization (BN) on robustness in neural networks is considered, particularly in terms of robustness to common corruptions. Previous methods have aimed to enhance robustness, but effectively doing so remains a challenge. Recent analyses have shown that BN can cause exploding gradients and increased sensitivity to input perturbations as network depth increases. Batch Normalization (BN) can induce sensitivity in neural networks, even with clean test accuracy. Adversarial vulnerability can increase with input dimension, emphasizing the need for effective regularizers. BN was found to confound results in a study, but removing BN showed no inherent relationship between vulnerability and input dimension. BN modifies hidden layers' pre-activations in a neural network. Batch Normalization (BN) modifies the hidden layers' pre-activations of a neural network by subtracting the mean and dividing by the standard deviation, then scaling and shifting the result. This process introduces complications, especially when considering differences in mini-batches. Adding a single BN layer to models trained by gradient descent can lead to differences in representations of examples, potentially causing exploding gradients. Normalization of intermediate representations can also affect the ability to distinguish between definite and ambiguous instances in classification tasks. The last layer of a neural network typically decodes class label-homogeneous clusters using mean and variance information for classification. Adding a single BN layer to models trained by gradient descent can exacerbate exploding gradients, as shown by Yang et al. (2019). The adversarial vulnerability of linear classifiers is related to the tilting angle \u03b8 of the decision boundary, with BN affecting this angle in a simple linear model. Increasing model complexity does not mitigate this vulnerability. Normalization aligns the decision boundary with the Bayes solution but increases adversarial vulnerability. The batch-normalized model has a tilting angle of 66.7\u00b0 compared to 0\u00b0 for a linear model. The batch-normalized model has a tilting angle of 66.7\u00b0 compared to 0\u00b0 for a linear model. The dataset seen by the classifier includes class distribution means symmetric around zero. The Bayes-optimal solution for the binary classification task involves a linear classifier with weight vector and bias term. The effect of batch-normalizing the input to the classifier is analyzed in the simplest setting. The batch-normalized linear classifier has a tilting angle of 66.7\u00b0 compared to 0\u00b0 for a linear model. The dataset has class distribution means symmetric around zero. Batch-normalized gradient descent yields a tilted decision boundary w.r.t. the nearest-centroid classifier. The tilting angle \u03b8 of the batch-normalized decision boundary is reported for different training methods on MNIST 3 vs. 7. The decision boundary is determined by the angle between datasets before and after normalization. The order of magnitude of c relative to data variance affects the weight value. Simulations of the toy model are depicted in Figure 1. Constant learning rate GD converges to the max-margin solution, while Batch-normalized GD converges for arbitrary learning rates. The angle \u03b8 w.r.t. the nearest-centroid classifier is computed after training linear models on the MNIST 3 vs. 7 dataset. Results consistent with the boundary tilting theory are shown in Table 1, indicating that BN causes tilting unaffected by parameters \u03b3 and \u03b2. Post-normalization, there is no signal to \u03b3 and \u03b2 about the variances of the original dataset. Increasing the numerical stability constant c enhances robustness in terms of absolute test accuracy for additive white Gaussian noise on MNIST and CIFAR-10 datasets by 33% and 41% respectively, at the expense of standard accuracy. This effect is consistent with Labatie (2019), where high signal variance directions are dampened under BN, leading to a reduction in sensitivity along low signal directions. This preferential exploration of low signal directions reduces the signal-to-noise ratio and increases sensitivity to the input. We evaluate the robustness of convolutional networks with and without Batch Normalization (BN) on various datasets using white-box adversarial attacks. The test accuracy under input perturbations is measured using projected gradient descent (PGD) in \u221e -and 2 -norm variants. Different network architectures show varying levels of robustness against noise and common corruptions, demonstrating a considerable disparity in robustness. In evaluating convolutional networks with and without Batch Normalization (BN) on various datasets, standard meta-parameters were used to train models with and without BN. Different learning rate schemes were examined for SVHN and CIFAR-10 datasets. BN increased clean test accuracy but reduced accuracy for additive noise and PGD-\u221e attacks. The study compared convolutional networks with and without Batch Normalization (BN) on different datasets. BN improved clean test accuracy but decreased accuracy for additive noise and PGD attacks. Different learning rate schemes were tested for SVHN and CIFAR-10 datasets, with results summarized in Table 2. In the second \"large\" learning rate experiment, deeper batch-normalized models do not recover the robustness of shallower models, and higher learning rates do not improve performance. Results for deeper models can be found in Appendix E, while other learning rate schedules and robustness vs. training epoch curves are in Appendix K. Robustness is evaluated on a corruption benchmark with 19 types of distortions, and results for VGG variants and WideResNet on CIFAR-10-C are reported. The batch-normalized variant shows higher error rates for \"noise\" corruptions compared to other models. BN increases mCE by 1.9-2.0% for VGG models and 1.6% for WRN. Some corruptions have varying effects on different models, with BN improving accuracy for Contrast, Snow, and Spatter. This disparity in accuracy for different corruption categories is further examined in detail. The study found that modern CNNs trained on standard image datasets rely heavily on texture for object recognition. Batch-normalized models showed higher error rates for \"noise\" corruptions compared to other models, with BN improving accuracy for certain corruption categories. The clean test accuracy, additive Gaussian noise, and BIM perturbations were evaluated for both batch-normalized and unnormalized models trained for different epochs. The study found that modern CNNs rely heavily on texture for object recognition. Batch-normalized models showed higher error rates for noise corruptions, with BN improving accuracy for certain categories. BagNets trained on ImageNet with architecture discarding spatial information between patches. Top-5 accuracies for patch sizes {9, 17, 33} reduced to 1.25%, 5.09%, and 14.62% for AWGN. BN may exacerbate the tendency to use superficial features for classification. The study found that modern CNNs rely heavily on texture for object recognition. Batch-normalized models showed higher error rates for noise corruptions, with BN improving accuracy for certain categories. BN may exacerbate the tendency to use superficial features for classification. Results in Table 5 show that BN consistently reduces accuracy for PGD by 8.54% to 11.00%. Black-box transfer analysis in Appendix E.2 is consistent with white-box analysis. Batch size and depth affect the maximum trainable depth, with BN limiting it. Robustness decreases with increasing batch size. The study discusses the impact of batch size on the robustness of networks, showing a decrease in robustness with larger batch sizes. Batch-normalized networks exhibit a tension between clean accuracy and robustness, especially for depths with around 25 or fewer layers. Adversarial vulnerability scales as \u223c \u221a d under the He et al. (2015) initialization scheme. Experiments demonstrate that adversarial training through projected gradient descent can help recover independence between vulnerability and input dimension. The study shows that input dimension can be recovered through adversarial training with little trade-off in clean accuracy. Regularization with weight decay helps achieve this without loss scaling as predicted. Increasing image width leads to improved adversarial test accuracy ratios. The study demonstrates that increasing image width improves adversarial test accuracy ratios. Results in Appendix F show accuracy ratios for different image sizes compared to the original dataset. Using a two-hidden-layer ReLU MLP with batch norm reduces accuracy for noise and adversarial attacks. Robustness decreases as image size increases, with batch normalization leading to less robustness. The study found that increasing image size reduces robustness to noise and adversarial attacks, with batch normalization causing a decrease in robustness. Applying regularization constants to the model showed a decrease in accuracy for noisy and adversarial perturbations. The study examined the impact of batch normalization on model robustness at test time, showing a significant decrease in accuracy for noisy and adversarially perturbed inputs. Weight decay, on the other hand, increased accuracy for such inputs. Several studies have explored the impact of batch normalization on training processes. While batch normalization can accelerate and stabilize training in shallow networks, it can cause gradient explosion in very deep networks without skip connections. This difference in effectiveness is attributed to the smoothing effect on the optimization landscape in shallow networks, while very deep networks may become untrainable. In our work, a single batch-normalized layer induces severe adversarial vulnerability. Tracked statistics may contribute to this vulnerability, but weight decay's loss scaling mechanism also plays a role in reducing the generalization gap in batch-normalized networks. In batch-normalized networks, weight decay's loss scaling mechanism contributes to reducing the generalization gap, even though batch norm is not typically used on all layers. Input dimension scaling experiments show that the input dimension is irrelevant to adversarial vulnerability. Robust optimization is equivalent to parameter norm regularization for linear models, but maintaining the latter is considered a more efficient approach. There is no free lunch with batch norm when model robustness is a concern. The importance of identifying different regularization mechanisms in batch-normalized networks is highlighted. The hyperparameter 'c' in mini-batch variance plays a crucial role in robustness, acting as a threshold on input dimensions. Increasing 'c' affects clean test accuracy and vulnerability to noise and adversarial perturbations. In experiments with MNIST and CIFAR-10 datasets, increasing 'c' impacts clean accuracy and robustness to noise and adversarial attacks. For CIFAR-10, VGG models were trained with specific parameters over multiple seeds. Increasing 'c' or \u03bb can enhance robustness but may lead to decreased accuracies. BN is not used in these experiments, and higher 'c' values significantly improve noise robustness. In experiments with MNIST and CIFAR-10 datasets, increasing 'c' impacts clean accuracy and robustness to noise and adversarial attacks. The absolute accuracies are consistently higher without BN. Error bars indicate standard error of the mean over four and five random seeds for MNIST and CIFAR-10, respectively. The default setting starts in the bottom right corner and the initial trade-off between clean test accuracy and robustness is traced up and leftwards until the curves inflect. The pixel range was clipped to {\u00b11}, {\u00b12}, and {\u00b12} for SVHN, CIFAR-10, and ImageNet, respectively. During the evaluation of robustness with PGD, perturbation magnitudes were set to 0.03 for SVHN and CIFAR-10, and 0.01 for ImageNet. The PGD-2 parameter was set to \u221e \u221a d to reduce random error. No random start was used at test-time. A code snippet for the PGD-\u221e evaluation on SVHN was provided. When evaluating robustness with PGD, perturbation magnitudes were set to 0.03 for SVHN and CIFAR-10, and 0.01 for ImageNet. Using 40 iterations of PGD did not significantly improve accuracy compared to 20 iterations. The degradation in robustness from using BN may be due to BN increasing standard test accuracy. In some cases, increasing accuracy with Batch Normalization (BN) may align the decision boundary with the Bayes-optimal solution. While BN typically improves clean test accuracy slightly on common datasets, there are natural cases where it does not. For example, ResNets trained with Fixup initialization on CIFAR-10 showed higher clean test accuracy without BN. MNIST results also indicate comparable clean accuracy regardless of BN usage. The results of Tables 6 & 7 show consistent clean accuracy regardless of Batch Normalization (BN), but reveal significant differences in robustness. Evaluating on corruptions or perturbations not seen during training is deemed more informative. PGD adversarial training was not used in the main text to assess vulnerability. Accuracy for a small value was reported for brevity, but plotting accuracy vs. ensures accuracy reaches zero for larger values to address gradient masking issues. PGD-\u221e training reduces the BN-vulnerability gap when tested on PGD-\u221e. PGD-\u221e training reduces the BN-vulnerability gap when tested on PGD-\u221e perturbations. BN decreases mean test accuracy on MNIST-C by 11 \u00b1 2%, from 87.0 \u00b1 0.5% to 76 \u00b1 2%. Mu & Gilmer (2019) reported an 11.15% degradation in absolute test accuracy for PGD training. Brightness alterations decimate the performance of BN and PGD variants, while the baseline shows little degradation. The PGD trained batch-normalized model performs worse than the baseline for various corruptions, with 7 \u00b1 2% higher mCE. The MNIST-C corruption benchmark results show that the PGD BN model has double-digit percentage decreases in accuracy for \"Brightness\", \"Fog\", \"Impulse Noise\", \"Stripe\", and \"Zigzag\" corruptions compared to the baseline. The section provides supplementary explanations and results for the PGD BN model's performance on various corruptions. The accuracy of different models fluctuates with random seed, while the Per-img baseline remains consistent. Training details for models on the SVHN dataset are also mentioned. In an experiment using stochastic gradient descent with momentum, batch size of 128, and initial learning rate of 0.01, Batch Normalization (BN) improved clean test accuracy but reduced accuracy for noise and adversarial attacks. Training deeper VGG models on SVHN failed, so only results for models trainable with BN are reported. Fixup initialization was compared with WideResNet for reducing normalization layers in deep networks. The study compared WideResNet with Fixup to the default architecture with BN. Results showed higher clean test accuracy for WideResNet on CIFAR-10, but VGG8 outperformed both variants under noise. WRN had little generalization gap between noisy CIFAR-10 and 10.1. The Fixup variant improved accuracy for noisy CIFAR-10. The Fixup variant improves accuracy for various datasets and raises questions about the role of skip-connections and gradient masking in white-box perturbations on ImageNet. Unnormalized target models show higher accuracy compared to unnormalized sources. The study found that unnormalized networks have more stable features compared to batch-normalized networks. Pre-trained ImageNet models in PyTorch lack hyperparameter details, indicating the results are generalizable. Batch normalization limits trainable depth and decreases robustness with batch size, while unnormalized networks show no such relationship. The study found that unnormalized networks have stable features compared to batch-normalized networks, with no relationship between depth or batch size in unnormalized networks. Unnormalized networks take longer to converge but can achieve higher clean test accuracy and good robustness simultaneously. The study compared the stability of unnormalized networks to batch-normalized networks, finding no correlation between depth or batch size in unnormalized networks. Unnormalized networks take longer to converge but can achieve higher clean test accuracy and robustness simultaneously. The models were trained using fully-connected models with ReLU units on MNIST dataset for 10 and 40 epochs. The model is optimized through empirical risk minimization using stochastic gradient descent. The loss function equation is minimized by controlling the scaling of the loss function with a regularization term. The study empirically tests a model with a single linear layer and cross-entropy loss function on variants of MNIST dataset. The model trained on MNIST with increasing input dimensions showed increased adversarial vulnerability in terms of accuracy and loss. Adversarial damage, defined as the average loss increase after attack, was found to grow like \u221a d. The model was trained using SGD for 50 epochs with a constant learning rate and a mini-batch size of 128. Resizing MNIST to various resolutions with NEAREST interpolation also affected adversarial vulnerability. The study found that adversarial vulnerability in image widths was consistent except for width 112, which had slightly more damage. By using weight decay regularization with parameter \u03bb dependent on input dimension, input dimension invariant vulnerability was achieved with minimal impact on test accuracy. This approach outperformed PGD training by avoiding arbitrary hyperparameters and not prolonging training time. The study compared adversarial training with weight decay regularization to achieve robustness against unseen attacks and common corruptions. Models trained with weight decay showed 12% higher accuracy compared to batch norm on invariance-based MNIST examples. The focus was on defending against traditional perturbations, with a detailed comparison between adversarial training and weight decay provided. The study compared adversarial training with weight decay regularization to achieve robustness against attacks and corruptions. Weight decay reduces the generalization gap, even in batch-normalized networks. The loss scaling mechanism persists in settings where batch norm is not typically used. The study analyzed the impact of batch normalization on robustness against various corruptions. Results showed that for VGG8, batch norm had varying effects on different types of noise, with Gaussian noise having the largest impact. For VGG13, the accuracy gap due to batch norm increased significantly for Gaussian and Impulse noise at higher severity levels. Overall, batch norm's effect on robustness varied across different corruption types. The study analyzed the impact of batch normalization on robustness against various corruptions. Results showed that for VGG8, batch norm had varying effects on different types of noise, with Gaussian noise having the largest impact. For VGG13, the accuracy gap due to batch norm increased significantly for Gaussian and Impulse noise at higher severity levels. Overall, batch norm's effect on robustness varied across different corruption types. The generalization gaps for noise variants ranged from 0.2% to 2.9% for VGG13 and from 6.9% to 12.1% for WRN. JPEG compression had a generalization gap of 4.6%. The \"Adversarial Spheres\" dataset involves classifying points on two concentric spheres with different radii. The task involves attributing points to inner or outer spheres, focusing on n = 2 concentric circles. Batch normalization affects robustness and stability of training, especially sensitive to learning rate \u03b7. Using SGD instead of Adam, a finite dataset of 500 samples from N(0, I) projected onto circles is used. Unnormalized network achieves zero training error for \u03b7 up to 0.1, while batch-normalized network is untrainable at \u03b7 = 0.01. 10,000 test points are sampled for evaluation of robustness. To evaluate robustness, 10,000 test points are sampled from the same distribution for each class, with noise applied. Models trained to 100% accuracy with a smaller learning rate are evaluated. The batch-normalized model classifies 94.83% correctly, while the unnormalized net achieves 96.06%. Training a fully connected network on a binary classification problem shows that the batch-normalized model fails to train at a higher learning rate compared to the unnormalized model. By increasing the stability constant c, the tilting angle \u03b8 can be achieved without tracking, allowing for arbitrary batch sizes at test time. Testing BNGD in eval mode with different c values reveals insights into the vulnerability of batch normalization. Eliminating tracking and BN altogether reduces the boundary tilting angle \u03b8 significantly. Eliminating BN reduces \u03b8 by 25.2, per-image normalization further reduces \u03b8 by 31.3. Model 10(b) achieves 38.7% accuracy with BN, while baseline 10(c) achieves 66.1% without normalization. The experiment challenges the claim that tracking in BN is the main vulnerability source, suggesting per-dimension normalization plays a role. Using BN without tracked statistics at test-time requires large batch sizes. Increasing stability constant c can reduce vulnerability without tracking. Increasing the numerical stability constant c can improve tracked statistics at test time, allowing for interpolation between configurations with and without tracking. The weight matrices visualized show differences between using BN and not using BN, with BN not being class-aware leading to a closer resemblance to a digit \"3\". Gradient masking effects are considered to explain the differences observed. BN degrades the visual appearance of adversarial examples for strong unbounded attacks, while semantically relevant features are introduced for inputs classified with high confidence. Gradient masking is not a factor, as evidenced by common corruption benchmarks and unbounded attacks reaching 100% success. For MNIST experiments, a LetNet-5 CNN variant from the AdverTorch library is trained with different variants including L2 weight decay regularization and batch normalization. The models achieve similar clean test accuracy with different techniques. Two unbounded attack schemes are used to construct adversarial examples. The text discusses the construction of adversarial examples using two unbounded attack schemes: \"fooling images\" and the Carlini & Wagner method. The fooling images procedure involves starting with white noise and minimizing cross entropy loss to achieve high confidence predictions. The process involves updating the starting point iteratively and clipping pixel values. The approach is similar to Activation Maximization techniques for interpreting neural networks. The text discusses the construction of adversarial examples using fooling images and the Carlini & Wagner method. The fooling images procedure involves starting with white noise and minimizing cross entropy loss to achieve high confidence predictions, similar to Activation Maximization techniques. The Carlini & Wagner attack seeks to find the smallest perturbation to achieve misclassification. The models 12(a) and 12(b) are robust and somewhat interpretable without post-hoc regularization. The batch-normalized model confidently classifies images resembling natural digits, while baselines contain more class-relevant features. The fooling images in Figure 12(b) show no fully saturated confidence predictions and resemble matched filters. The text discusses the Carlini & Wagner method for crafting adversarial examples with specific parameters like a confidence parameter of k = 4 and a learning rate of 1e-2. Results show a three-fold improvement in perturbation norms for batch-normalized models compared to unnormalized models. Fooling images for SVHN dataset are visualized, along with misclassifications using a basic iterative gradient method. The text introduces a basic iterative gradient method (BIM-2) for a random sample of each class in Figure 15 using a simple CNN architecture. The model is trained on the SVHN dataset with a constant learning rate of 0.01 and a mini-batch size of 128. The loss is weighted by the inverse frequency of each class due to the dataset's imbalance. Despite similar test accuracy, fooling images for the unnormalized model show task relevant edges, while the batch-normalized model produces difficult to interpret images. The examples of grid 15(a) resemble the target class, while batch-normalized model images have a subtle textured effect. The model with a subtle textured effect preserves semantic cues of the source image. Using a high initial learning rate, annealed during training, achieves higher test accuracy. Li et al. (2019) introduce the concept of learning order, a predictive property of generalization ability. For CWL2, the confidence parameter k = 4 is set, with an \"arbitrary misclassification\" objective. Plot (b) shows perturbed images for the baseline L2 regularized model. The adversarial examples in the images show semantic features corresponding to the predicted class, with perturbations being more perceptible in the baseline L2 regularized model compared to the batch-normalized model. The perturbations in the batch-normalized model are classified with the same or higher confidence, but are not as noticeable and have lower 2-norms. Using a high initial learning rate can prevent the model from fitting high complexity patterns too early. Using a high initial learning rate can prevent the model from fitting high complexity patterns too early, allowing it to move on to more challenging patterns in a \"curriculum\" approach. An experiment on CIFAR-10 with different initial learning rates shows the interplay between learning rate, robustness, and vulnerability. A shallow VGG variant is trained over 150 epochs with standard meta-parameters and data augmentation, evaluating robustness measures every 10 epochs. The experiment on CIFAR-10 with different initial learning rates shows the interplay between learning rate, robustness, and vulnerability. The final test accuracies for different models and learning rates are provided, along with evaluations on variants of the test set for robustness. The unnormalized model demonstrates higher robustness than the batch-normalized model after around 70-80 epochs. After annealing the learning rate, the unnormalized model shows higher robustness compared to its batch-normalized counterpart in the \"small lr\" case. The impact of a large constant learning rate on mitigating BN vulnerability is explored in a subsequent experiment. Results suggest little difference in robustness. The impracticality of a large constant learning rate setting is highlighted, with potential clean test accuracy being left unrealized. The vulnerability is not absent but rather the baseline fails to reach its full potential. The unnormalized model shows higher robustness compared to the batch-normalized model in the \"small lr\" case. The accuracy of the unnormalized model on all test sets increases almost monotonically with prolonged training, while the batch-normalized model converges quickly then plateaus. Careful early stopping is more relevant to batch normalization. The unnormalized model demonstrates higher robustness compared to the batch-normalized model in the \"small lr\" case. The accuracy of the unnormalized model on all test sets increases steadily with prolonged training, while the batch-normalized model quickly converges and plateaus. Early stopping is more critical for batch normalization. The learning rate is annealed at epochs 60 and 120, with evaluations done every ten epochs on various test set variants. The light grey shading indicates where the unnormalized model outperforms the batch-normalized model. In a study to isolate the effect of annealing the learning rate for CIFAR-10, a large fixed learning rate of 0.1 was used. Results showed that the accuracy of the batch-normalized variant was either similar or slightly higher than the unnormalized variant. Increasing the weight decay penalty to 1e-3 led to instability in the batch-normalized case due to the already large learning rate. The comparison between the unnormalized and batch-normalized curves showed the impact of higher weight decay on the unnormalized model. The higher weight decay unnormalized model competes with and slightly outperforms the batch-normalized model in terms of robustness by epoch 130, as shown by the curves. Error bars represent the standard error over three random seeds."
}