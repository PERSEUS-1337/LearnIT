{
    "title": "BJJLHbb0-",
    "content": "Unsupervised anomaly detection in high-dimensional data is crucial for machine learning research and industrial applications. A Deep Autoencoding Gaussian Mixture Model (DAGMM) is proposed for this purpose, combining a deep autoencoder with a Gaussian Mixture Model for improved anomaly detection without decoupled model learning. DAGMM optimizes deep autoencoder and mixture model parameters simultaneously, using an estimation network for parameter learning. It balances reconstruction, density estimation, and regularization to improve anomaly detection without pre-training. Experimental results show up to 14% improvement in F1 score compared to state-of-the-art techniques. Anomaly detection is crucial in various fields like cybersecurity, system management, and medical care. Anomaly detection is crucial in fields like cybersecurity, system management, and medical care. Density estimation is at the core of anomaly detection, where anomalies are found in low probability density areas. Robust anomaly detection on high-dimensional data without human supervision is challenging. Two-step approaches are commonly used to address the curse of dimensionality, involving dimensionality reduction followed by density estimation in a low-dimensional space. Combining dimensionality reduction and density estimation is desirable for anomaly detection, but joint optimization is computationally difficult. Recent works have used deep networks for this purpose, but face limitations in preserving essential information, capacity of density estimation models, and fitting tasks. In this paper, the Deep Autoencoding Gaussian Mixture Model (DAGMM) is proposed as a deep learning framework for unsupervised anomaly detection. DAGMM preserves key information in a low-dimensional space by combining reduced dimensions from dimensionality reduction and induced reconstruction error. Anomalies differ from normal samples in the reduced dimensions. DAGMM utilizes a compression network for dimensionality reduction and a Gaussian Mixture Model for density estimation in anomaly detection. DAGMM addresses challenges in joint optimization of dimensionality reduction and density estimation for Gaussian Mixture Model learning by utilizing an estimation network to predict mixture membership and facilitate parameter estimation. This allows for simultaneous training of a dimensionality reduction component that aids in density estimation, making DAGMM suitable for end-to-end training. DAGMM is friendly to end-to-end training, allowing for joint optimization of dimensionality reduction and density estimation. It avoids the need for pre-training and demonstrates superior performance in anomaly detection tasks, with up to a 14% improvement in F1 score. The end-to-end trained DAGMM shows low reconstruction error compared to pre-trained models, outperforming baseline methods in anomaly detection. Various methods in unsupervised anomaly detection include reconstruction-based methods like PCA and kernel PCA. Recent works have proposed methods like Robust PCA and deep autoencoders to improve anomaly detection by analyzing reconstruction errors. However, these methods are limited as they only focus on one aspect of anomaly analysis. Anomalous samples may not always exhibit high reconstruction errors, making detection challenging, especially with complex structures or noisy data. Despite these challenges, there is still hope to detect such \"lurking\" anomalies. DAGMM considers both density estimation in a low-dimensional space and reconstruction error caused by dimensionality reduction for comprehensive anomaly detection. Traditional techniques use a two-step approach of dimensionality reduction followed by clustering analysis due to the curse of dimensionality in high-dimensional data. Recent works propose deep autoencoder based methods to jointly learn dimensionality reduction and clustering components, addressing the issue of key information loss during dimensionality reduction without guidance from clustering analysis. However, state-of-the-art methods are limited by oversimplified clustering models and pre-trained dimensionality reduction components. DAGMM explicitly addresses these issues with an estimation network evaluating sample density in the low-dimensional space for anomaly detection. DAGMM introduces an estimation network to evaluate sample density in a low-dimensional space, allowing for parameter estimation of GMM without traditional EM procedures. This approach enables end-to-end training and improves clustering analysis and density estimation quality. Unlike traditional one-class classification methods, DAGMM estimates data density in a jointly learned low-dimensional space for more robust anomaly detection, overcoming the curse of dimensionality. DAGMM focuses on unsupervised anomaly detection by jointly learning linear dimensionality reduction and Gaussian mixture modeling. Unlike traditional methods, DAGMM uses a deep autoencoder for non-linear dimensionality reduction and density estimation in a low-dimensional space. DAGMM is a deep unsupervised anomaly detection model that combines dimensionality reduction with Gaussian mixture modeling. It uses a deep autoencoder for non-linear dimensionality reduction and density estimation in a low-dimensional space. The model consists of a compression network and an estimation network, where the compression network reduces input samples' dimensionality and feeds the representations to the estimation network for likelihood prediction. The estimation network in DAGMM predicts the likelihood of input samples using a Gaussian Mixture Model. The compression network generates low-dimensional representations from deep autoencoder learning and reconstruction error features. The network computes the low-dimensional representation z for a sample x using encoding and decoding functions, with parameters \u03b8 e and \u03b8 d. The features derived from the reconstruction error can be multi-dimensional, considering various distance metrics. The compression network generates low-dimensional representations using deep autoencoder learning. The estimation network in DAGMM predicts the likelihood of input samples with a Gaussian Mixture Model, estimating parameters and evaluating likelihood for samples without alternating procedures. The estimation network in DAGMM predicts sample energy using a Gaussian Mixture Model. Parameters are estimated for the GMM components, and sample energy is inferred using these parameters. Anomalies can be detected by comparing sample energy to a threshold during testing. The objective function for DAGMM training is constructed based on the dataset of samples. The objective function for DAGMM training includes three components: the loss function for reconstruction error, modeling probabilities of observing input samples, and addressing the singularity problem similar to GMM. To address the singularity problem similar to GMM, DAGMM penalizes small values on the diagonal entries in covariance matrices. The meta parameters \u03bb1 and \u03bb2 are set to 0.1 and 0.005 respectively, yielding desirable results. The estimation network in DAGMM is used for membership prediction, similar to latent variable inference in probabilistic graphical models. Neural variational inference has been proposed to handle difficult latent variable inference problems using deep neural networks. The DAGMM framework addresses singularity issues by penalizing small values in covariance matrices. The estimation network predicts membership using neural variational inference, tightening the energy function bound by minimizing the negative evidence lower bound. This approach improves the estimation network's approximation of the true posterior. DAGMM utilizes a deep estimation network to parametrize a sample-dependent prior distribution, different from neural variational inference. It is a deep unsupervised version of adaptive mixture of experts combined with a deep autoencoder, employing end-to-end training instead of pre-training. Anomaly detection performance is challenging to improve in a well-trained deep autoencoder for subsequent density estimation tasks. The compression and estimation networks can enhance each other's performance through regularization and meaningful density estimations. The choice between pre-training and end-to-end training in DAGMM is discussed using a public benchmark dataset in Section 4.5. In Section 4.5, the effectiveness of DAGMM in unsupervised anomaly detection is demonstrated using benchmark datasets like KDDCUP, Thyroid, Arrhythmia, and KDDCUP-Rev. The KDDCUP dataset from the UCI repository is preprocessed to obtain a dataset of 120 dimensions with \"normal\" samples treated as anomalies. The Thyroid dataset from the ODDS repository has 3 classes, with the hyperfunction class considered as anomalies and the other two as normal. The hyperfunction class is treated as an anomaly class in the Arrhythmia dataset. In the KDDCUP-Rev dataset, samples labeled as \"attack\" are treated as anomalies with a minority group ratio of 0.2. Both traditional and deep learning methods are considered as baselines for anomaly detection. In anomaly detection, traditional and state-of-the-art deep learning methods are used as baselines. OC-SVM is a popular kernel-based method, while DSEBM-e leverages sample energy and DSEBM-r uses reconstruction error. DCN is a clustering algorithm that uses k-means to regulate autoencoder performance for anomaly detection. Anomaly detection methods like OC-SVM, DSEBM-e, DSEBM-r, and DCN are commonly used. In DAGMM variants, GMM-EN removes the reconstruction error component, while PAE eliminates the energy function, making it similar to a deep autoencoder. Sample energy and cluster centers are crucial for anomaly detection in DAGMM. In anomaly detection methods like OC-SVM, DSEBM-e, DSEBM-r, and DCN, DAGMM variants such as BID22, E2E-AE, PAE-GMM-EM, and PAE-GMM adopt different strategies for training deep autoencoders and GMMs to detect anomalies using sample reconstruction error or sample energy as criteria. The DAGMM variants, including DAGMM-p and DAGMM-NVI, use sample energy for anomaly detection. DAGMM-NVI incorporates neural variational inference and different objective functions. Reconstruction features like relative Euclidean distance and cosine similarity are considered in the experiments. In Appendix D, the importance of reconstruction features in DAGMM and how to select them are discussed. The network structures of DAGMM on different datasets are summarized: KDDCUP dataset uses a compression network with 3-dimensional input and a GMM with 4 mixture components for best performance. Thyroid dataset also uses a 3-dimensional input compression network with 2 mixture components for optimal performance. The DAGMM instances are implemented using a fully-connected layer with input neurons and output neurons activated by function f, along with a dropout layer with keep probability p during training. They are trained using the Adam algorithm with a learning rate of 0.0001. Different datasets have varying numbers of training epochs and batch sizes. The parameters \u03bb 1 and \u03bb 2 are set as 0.1 and 0.005, respectively. Baseline methods undergo an exhaustive search for optimal meta parameters. Anomaly detection performance is evaluated using average precision, recall, and F1 score. Based on the anomaly ratio from Table 1, the threshold is set to identify anomalies. DAGMM outperforms baseline methods in terms of F1 score on all datasets, especially on KDDCUP and KDDCUP-Rev. Training data is clean, with 50% used for training and the rest for testing. Anomaly class is considered positive, and precision, recall, and F1 score are defined accordingly. DAGMM outperforms existing methods in terms of F1 score on various datasets, with significant improvements on KDDCUP and KDDCUP-Rev. The curse of dimensionality limits OC-SVM's performance, while DSEBM works well on multiple datasets but is surpassed by DAGMM due to joint consideration of latent representation and reconstruction error in energy modeling. Pre-trained deep autoencoders may limit the performance of DCN, PAE-GMM, and DAGMM-p, as significant changes on reduced dimensions are challenging. GMM-EN struggles with density estimation without reconstruction constraints, and E2E-AE shows limitations in reducing anomalies. E2E-AE struggles with reducing reconstruction error compared to PAE and DAGMM on KDDCUP and Thyroid datasets. DAGMM and DAGMM-NVI show similar performance, with no significant improvement from neural variational inference. Appendix B provides the energy function learned by DAGMM on clean training data. Experiment results on contaminated training data are also presented, with 50% of data reserved for analysis. Anomaly detection results on contaminated training data from KDDCUP show decreased accuracy as contamination ratio increases. DAGMM maintains good accuracy with 5% contaminated data, while OC-SVM is more sensitive to contamination. Training with clean data is crucial for better detection accuracy. The DAGMM model achieves state-of-the-art accuracy in unsupervised anomaly detection by training with high-quality data. Figure 3 displays the low-dimensional representation learned by DAGMM, PAE, DAGMM-p, and DCN on the KDDCUP dataset, showcasing DAGMM's ability to separate anomalous samples effectively. In Figure 3a, DAGMM effectively separates anomalous samples from normal samples in the low-dimensional space, outperforming PAE, DAGMM-p, and DCN. Despite efforts to fine-tune pre-trained models, anomalies still overlap with normal samples in Figures 3b, 3c, and 3d. Pre-trained deep autoencoders may be suboptimal for density estimation tasks. The reconstruction error in trained DAGMM is comparable to pre-trained deep autoencoders. The experimental results show that DAGMM outperforms other models in separating anomalous samples from normal samples in the low-dimensional space. The compression network and estimation network mutually boost each other's performance during end-to-end training, leading to better compression and robust density estimation. Overall, DAGMM suggests a promising direction for density estimation and anomaly detection. In this paper, the Deep Autoencoding Gaussian Mixture Model (DAGMM) is proposed for unsupervised anomaly detection. DAGMM combines dimensionality reduction and density estimation through end-to-end training. It consists of a compression network and an estimation network, allowing for better compression and robust density estimation. The model outperforms others in separating anomalous samples from normal samples in a low-dimensional space, suggesting a promising direction for anomaly detection. The Deep Autoencoding Gaussian Mixture Model (DAGMM) is proposed for unsupervised anomaly detection, combining dimensionality reduction and density estimation through end-to-end training. DAGMM outperforms other techniques on benchmark datasets, showing up to a 14% improvement in the F1 score. It suggests a promising direction for anomaly detection in high-dimensional data. In the testing phase, OC-SVM requires setting parameter \u03bd during training. \u03bd represents anomaly ratio in training data but is challenging to determine when all training data are normal samples. An exhaustive search is conducted to find the optimal \u03bd for highest F1 score on different datasets. \u03bd values are set to 0.1, 0.02, 0.04, and 0.1 for KDDCUP, Thyroid, Arrhythmia, and KDDCUP-Rev, respectively. Reconstruction features are discussed, emphasizing their importance in a network security dataset where anomalies are network flows with spoofing. In the network security dataset, anomalies are network flows with spoofing attacks. Deep autoencoders are used to reduce dimensions from 20 to 1, making it easier to separate anomalies from normal samples. By analyzing the L2 reconstruction error, anomalies with similar representations to normal samples in the reduced space can be identified. In the network security dataset, anomalies are network flows with spoofing attacks. Deep autoencoders reduce dimensions to 1, aiding in separating anomalies from normal samples. By analyzing the L2 reconstruction error, anomalies similar to normal samples in the reduced space can be identified. This motivates the inclusion of reconstruction features into DAGMM for anomaly detection. Reconstruction feature selection guidelines include using continuous and differentiable error metrics with outputs in a range of small values for ease of training the estimation network. In this study, cosine similarity and relative Euclidean distance are selected based on these rules. In a case study, the evaluation of DAGMM includes a comparison with PAE-GMM on the KDDCUP dataset. Anomalies with low cosine similarity and high relative Euclidean distance are easily captured by both techniques, while more challenging anomalies are not as effectively identified. The model learned by DAGMM tends to assign lower cosine similarity to anomalies than PAE-GMM, making it easier to differentiate anomalies from normal samples. The objective function of DAGMM includes components from deep autoencoder, estimation network, and penalty for covariance matrices, with a coefficient ratio of 1 : \u03bb 1 : \u03bb 2. A large \u03bb 1 value minimizes the role of the deep autoencoder's loss function in optimization. In our exploration, we find that a ratio of 1 : 0.1 : 0.005 consistently delivers expected results across all datasets in the experiment. Varying the base of this ratio affects anomaly detection accuracy, with adjustments made to \u03bb 1 and \u03bb 2 when the base is set to 2. TAB6 displays the average precision, recall, and F 1 score after 20 runs of DAGMM on the KDDCUP dataset. DAGMM shows consistent performance on the KDDCUP dataset as the base varies from 1 to 9 with a step of 2. \u03bb 1 and \u03bb 2 remain stable despite changes in the base."
}