{
    "title": "Hyez1CVYvr",
    "content": "Deep neural networks have excelled in classification tasks, but struggle with detecting samples from new class distributions. A methodology is proposed to train neural networks to efficiently detect out-of-distribution examples without compromising classification accuracy. This approach, based on the Outlier Exposure technique, introduces a novel loss function that achieves state-of-the-art results in out-of-distribution detection for image and text classification tasks. This method is also suitable for training classification algorithms based on Maximum Likelihood methods. Most classification algorithms assume data from all class distributions are available during training. In an open world scenario, where new class distributions may appear during testing, classifiers need to detect out-of-distribution examples while maintaining high accuracy. Deep neural networks can predict out-of-distribution examples with high confidence, but this can indicate overfitting. The inability of neural networks to detect out-of-distribution examples is addressed by training them to make uncertain predictions for novel class distributions. This is achieved by minimizing the distance between the output distribution and a uniform distribution using a loss function based on KL divergence. This technique, known as Outlier Exposure, draws anomalies from diverse datasets to improve network calibration. The Outlier Exposure (OE) technique improves OOD detection by drawing anomalies from diverse datasets. It proposes a novel loss function with two regularization terms, outperforming previous work and achieving state-of-the-art results in OOD detection with OE on image and text classification tasks. Our proposed method combined with the Mahalanobis distance-based classifier outperforms the original method and achieves state-of-the-art results in OOD detection. Previous work used GAN frameworks to generate negative instances for training SVM classifiers to detect examples from unseen classes. Kliger & Fleishman utilized a multi-class GAN framework for simultaneous classification and novelty detection. Hendrycks & Gimpel proposed a baseline for detecting misclassified instances. Temperature scaling, small perturbations at the input, and GAN examples were used to improve novelty detection in neural networks. Lee et al. (2018a) forced lower confidence in predicting classes, while Liang et al. (2018) observed that these techniques can distinguish in-and out-of-distribution images. Hendrycks et al. (2019) used the technique of OE to substitute GAN samples with a real and diverse dataset, forcing lower confidence in predicting classes. Other works also focus on making uncertain predictions for out-of-distribution examples, with methods such as ensemble classifiers and theoretical guarantees for detecting OOD examples. Lee et al. (2018b) defined a confidence score using the Mahalanobis distance to efficiently detect abnormal test samples. In the context of detecting out-of-distribution examples, various methods have been proposed such as using Euclidean distance or KL divergence metric. Different approaches like Distance-Based Post-Training (DBPT) and minimizing the l1 norm between output distributions have shown success in machine learning tasks. The approach discussed involves minimizing the l1 norm between output distributions to penalize confident predictions of a neural network. Instead of penalizing confident predictions, the neural network is forced to make predictions for examples generated by D with an average confidence close to its training accuracy, ensuring calibration. The constrained optimization problem aims to minimize cross entropy loss while aligning prediction probabilities with training accuracy in deep neural networks. The constrained optimization problem aims to align prediction probabilities with training accuracy in deep neural networks by using two constraints. The first constraint ensures the DNN predicts known classes with confidence close to its training accuracy, while the second constraint forces high uncertainty for unseen classes by producing a uniform distribution at the output. Training a neural network for a few epochs can calculate the training accuracy needed for this process. The constrained optimization problem in deep neural networks involves aligning prediction probabilities with training accuracy using Lagrange multipliers to convert it into an unconstrained optimization problem. This modification simplifies the process by using a common Lagrange multiplier for multiple constraints, reducing the number of hyperparameters in the loss function. The objective function is optimized with respect to parameters and Lagrange multipliers to approximate the original problem in machine learning applications. In machine learning applications, the constrained optimization problem in deep neural networks is approximated by calculating values for \u03bb 1 and \u03bb 2 through a validation technique. The problem is converted into an unconstrained optimization problem, where the maximum prediction probability produced by softmax for each example drawn from the dataset may change at each training epoch, causing difficulties in achieving a uniform distribution at the output. In deep neural networks, difficulties arise in achieving a uniform distribution at the output due to changing prediction probabilities for different classes at each training epoch. To address this issue, all prediction probabilities can be simultaneously pushed towards a uniform distribution. Additionally, the loss function is modified to prevent negative values during training. The loss function in deep neural networks minimizes the squared distance between training accuracy and average confidence in predictions, as well as the l1 norm between uniform distribution and softmax output. This helps achieve a uniform distribution at the output. During experiments, it was observed that starting training with a high value of \u03bb1 can slow down the learning process. It is recommended to split the training into two stages, with the first stage focusing on training the DNN using only the cross entropy loss function until reaching a certain point. During the first stage, the DNN is trained using the cross entropy loss function until reaching the desired accuracy level A tr. Then, fine-tuning is done using a combined loss function. The effectiveness of this method is demonstrated in image and text classification tasks by comparing it with a previous OOD detection method. The detection method used is Maximum Softmax Probability (MSP) detectors, with evaluation metrics from Hendrycks et al. (2019). Performance metrics for OOD detection include False Positive Rate at N % True Positive Rate (FPRN) and Area Under the Receiver Operating Characteristic curve (AUROC). In OOD detection tasks, AUROC summarizes performance for varying threshold values, while AUPR is important for class-imbalance. Results of image classification experiments are shown in Table 1, with network architecture details similar to Hendrycks et al. (2019). For CIFAR 10 and CIFAR 100 experiments, 40-2 wide residual networks (WRNs) were used, trained for 100 epochs with a cosine learning rate, dropout rate of 0.3, and batch size of 128. Nesterov momentum and l2 weight regularization were also applied. The network was fine-tuned for 15 epochs for CIFAR 10 and 20 epochs for CIFAR 100. For SVHN experiments, 16-4 WRNs were trained with specific parameters and fine-tuned for 5 epochs. The 80 Million Tiny Images dataset was used during fine-tuning. The 80 Million Tiny Images dataset was utilized for fine-tuning in the OOD detection task. Additional image classification experiments were conducted to analyze the impact of each regularization term in the loss function. Results showed that combining two regularization terms improved OOD detection performance. The combination of two regularization terms in the loss function improved OOD detection performance and test accuracy of the DNN. Lee et al. (2018b) proposed a DBPT method for OOD detection using class-conditional Gaussian distribution for confidence score calculation. The authors proposed two techniques to distinguish in-and out-of-distribution examples: adding perturbation to input examples and using a feature ensemble method to calculate confidence scores based on Mahalanobis distance. The feature ensemble method extracts hidden features, computes class mean and covariances, and calculates weighted average scores using logistic regression. This post-training method can be combined with the Mahalanobis distance-based classifier for improved performance. The proposed classifier by Lee et al. (2018b) is a post-training method that can be combined with a new loss function. After fine-tuning a DNN with the proposed loss function, the Mahalanobis distance-based classifier was applied, achieving state-of-the-art results in out-of-distribution detection tasks. The method was tested on image classification tasks and compared against Lee et al. (2018b) with promising results. In these experiments, the OOD detection evaluation metrics used in Lee et al. (2018b) are adopted. This includes True Negative Rate at N % True Positive Rate (TNRN), Area Under the Receiver Operating Characteristic curve (AUROC), and Detection Accuracy (DAcc). The method's adaptability and effectiveness are demonstrated by following the experimental setup of Lee et al. (2018b) and training ResNet. In the experiments, the method's adaptability and effectiveness are demonstrated by training ResNet with 34 layers using various datasets. The hyper-parameters are tuned using a validation dataset, and the Mahalanobis distance-based confidence score is computed. Additional training details are provided in Appendix C. The hyper-parameters for tuning include noise magnitude and layer indexes for feature ensemble. The Mahalanobis distance-based classifier is combined with the proposed method by training ResNet with 34 layers and fine-tuning with a specific loss function using 80 Million Tiny Images. The batch sizes for data sampled from D in and D OE out were 128 and 256 respectively. The network was fine-tuned for 30 epochs for CIFAR-10, 20 epochs for CIFAR-100, and 5 epochs for SVHN. Hyperparameters \u03bb 1 and \u03bb 2 were chosen using a separate validation dataset. Results in Table 4 show the effectiveness of the method combined with the Mahalanobis distance-based classifier, outperforming the original method in all experiments. This validates the contribution of the technique in achieving state-of-the-art results in OOD detection with OE and in combination with DBPT methods. The Mahalanobis distance-based classifier, combined with our method, improves OOD detection in DNNs by extracting learned features and defining confidence scores. Our proposed method enhances classification and OOD detection simultaneously, with a loss function minimizing l1 norm between DNN output distribution and uniform distribution. Simulation results demonstrate the effectiveness of our approach in distinguishing in-and out-of-distribution data. The proposed method enhances OOD detection in DNNs by minimizing the l1 norm between the output distribution of the softmax layer and the uniform distribution. Experimental results show state-of-the-art performance in image and text classification tasks. Additionally, the method can be combined with DBPT methods like the Mahalanobis distance-based classifier for improved OOD detection. The SVHN dataset consists of 32x32 color images with 604,388 for training and 26,032 for testing. The CIFAR 10 dataset contains 60,000 32x32 color images with 50,000 for training and 10,000 for testing. CIFAR 100 has 20 superclasses with 5 classes each, totaling 100 classes and 60,000 images. Images are standardized before training. The dataset used for training/testing includes 80 Million Tiny Images, Textures, LSUN, Rademacher, and WikiText-2. Preprocessing steps were applied to make the datasets disjoint from each other. The curr_chunk discusses various datasets used in experiments, including SNLI, IMDB, Multi30K, WMT16, Yelp, and EWT. It also mentions NLP OOD example detection using the maximum softmax probability baseline detector after fine-tuning with OE. During fine-tuning with our proposed loss function, hyper-parameters were chosen using a separate validation dataset. Additional training details are provided in Table 7."
}