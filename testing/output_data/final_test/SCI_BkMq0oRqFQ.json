{
    "title": "BkMq0oRqFQ",
    "content": "Batch Normalization (BN) and its variants are widely used in deep learning to improve training of neural networks. The relationship between ordinary least squares and partial derivatives in back-propagation through BN is explained, recasting it as a least squares fit. This perspective, called \"gradient-least-squares,\" provides an accurate description of BN. Two adjustments to BN are proposed and evaluated. BN has been shown to enable faster convergence and lower loss in deep networks. This work examines the back-propagation of Batch Normalization (BN) and recasts it as a least squares fit, providing novel insights into the effectiveness of BN and alternative normalization approaches. It draws a connection between least squares and the gradient computation of BN, offering a new perspective on why BN is so effective. Our work presents a new perspective on Batch Normalization (BN) by viewing it as a least squares regression, providing insights into its effectiveness and alternative normalization approaches. We demonstrate the extensibility of this view by evaluating two variants of BN and provide empirical support on CIFAR-10. The back-propagation of BN in a neural network can be recast as least squares regression, decomposing gradients into explained and residual portions. The back-propagation of Batch Normalization in neural networks can be viewed as a least squares regression, separating gradients into explained and residual portions. Gradient-least-squares can offer insights for future neural network design and understanding. Figure 1 illustrates normalization with batch statistics and presents a novel identity on quantities computed during forward and back-propagation. Batch Normalization (BN) in neural networks can be viewed as a regression with batch moments. The normalization process marginalizes out batch and spatial dimensions, simplifying the computation. Ignoring certain parameters does not affect the main focus on Gaussian normalization. Back-propagation involves partial derivatives of mean (\u00b5) and standard deviation (\u03c3) with respect to each activation (x i). The normalized output is represented as DISPLAYFORM1, with \u00b5 and \u03c3 as functions of each x i. The least-squares problem involves fitting \u03b1 and \u03b2, where \u03b2 = Cov(z, g). When z are normalized activations and g are partial derivatives, Ez = 0 and Var(z) = 1. The moment statistics are defined by \u00b5 = N i=1x i N. Let L be a function dependent on the normalized activations z i. The function L depends on normalized activations z_i, with gradients satisfying a specific equation for all j. The proof involves deriving partial derivatives using the chain rule and total derivative rules. By rearranging the partial derivatives, we can satisfy the ordinary least squares framework. Substituting equations leads to recovering \u03b1 and \u03b2, concluding with rearranged equations. During back-propagation, the normalization function in BN controls output to have mean near 0 and variance near 1, normalized over the dataset. Activations are split into partitions for mean and variance computation. Most work on BN focuses on describing activation distributions. The normalization function in BN controls output to have mean near 0 and variance near 1 during back-propagation. Partitions are used for mean and variance computation, referred to as normalization partitions. Theorem 1 shows that BN incorporates least squares fitting into gradient computation, fitting gradients of activations with a single-variable with-intercept least squares model. Popular normalization techniques are recast into the gradient-least squares view for extensibility. Batch normalization (BN) refers to dimensions corresponding to items, height, width, and channels. In non-image applications, H and W are 1. BN marginalizes out items and spatial dimensions, but keeps statistics for each channel separate. Layer Normalization (LN) marginalizes out channels but computes separate statistics for each batch item. LN is used in large LSTM models and recurrent networks. The distinction between BN and LN lies in the data point partitions during back-propagation. Instance Normalization (IN) and Group Normalization (GN) are introduced as alternatives to Batch Normalization (BN) and Layer Normalization (LN) in the context of image processing tasks. IN emphasizes end-to-end training with derivatives passing through the moments, while GN aims to improve performance on image-related tasks with memory constraints on batch size. Both IN and GN have different normalization techniques compared to BN and LN, providing new options for optimizing neural network training. Group Normalization (GN) is introduced as an alternative to Batch Normalization (BN) and Layer Normalization (LN) for image processing tasks, focusing on improving performance with memory constraints on batch size. GN partitions channels into sub-groups for moment computations, with similarities to LN when the number of groups is one. Future normalization methods may pattern match with BN, LN, IN, or GN, with back-propagation formulated as a least-squares fit against normalized activations. Figure 2 illustrates normalization partitions for BN, LN, IN, and GN. Theorem 1 is agnostic to how activations are partitioned before normalization, equation 9 applies to any method that partitions and normalizes activations. L2 normalization of weights in WN is different from Gaussian normalization in BN, but WN can be seen as a least squares regression. Improved residual mappings in ResNets show BN coming first, constraining gradients to be decorrelated with normalized activations. Update to ResNet architecture emphasizes the importance of residual mappings. The update to the ResNet architecture improved residual mappings by moving BN operations early in the network, supported by gradient-least-squares perspective. BID6 provides empirical support for BN-early order in shake-shake regularization architectures. The surprise arises from viewing BN as only controlling activation distribution, while in gradient-least-squares perspective, the first layer of each residual mapping is crucial for gradient calculations. The branch constrains gradients in residual mappings to be zero-centered and decorrelated with activations. Gradient-least-squares in deep neural networks is seen as a regression problem. BN and LN normalize over different partitions of activations, leading to two-step regressions during back-propagation. The order of BN and LN layers affects the regression process. The second regression in the study aims to decorrelate partial gradients from activations to improve performance on CIFAR-10 relative to BN with batch size 128. Regularization is used to address the issue of gradient regressions failing on correlated data in small batches. The study aims to improve performance on CIFAR-10 relative to BN with batch size 128 by using regularization to decorrelate partial gradients from activations. Streaming estimates of past gradients are used to create virtual data in the regression, outperforming standard BN on small batches but not large batches. The normalization calculation inside SwN involves rescaling contributions to the batch mean for each normalization scheme. Batch Normalization (BN) is a key technique in training deep networks, with variations like Decorrelated Batch Normalization (DBN) and Spectral Normalization (SpN) being introduced. These methods aim to improve performance by decorrelating activations and using differentiable operations. The regression interpretations for DBN and SpN are still unresolved, but they show promise in enhancing training processes. Batch Normalization (BN) is a crucial technique in deep network training, with various benefits such as smoother loss landscapes and faster convergence on least squares loss. Recent studies have shown the advantages of placing BN early in residual mappings of ResNet, explaining why ResNet outperforms earlier deep neural network designs. Additionally, BN has been recast as a stochastic process, providing a novel approach to regularization by regressing partial derivatives against normalized activations. Batch Normalization (BN) is seen as a gradient regression calculation that decorrelates and zero-centers gradients with respect to normalized activations. Placing a LN layer before or after BN can be viewed as a two-step regression process. Empirical results show that BN and LN together outperform either individually, especially in addressing BN's performance degradation with small batch sizes. We propose two open approaches for investigating the empirical improvements in neural networks with Batch Normalization (BN) and small batch sizes. The first approach focuses on changes to gradient regression formulations, while the second approach explores relationships between gradients of activations. The goal is to search for a shared noisy component that arises from gradients in the same normalization partition. The text discusses the use of Batch Normalization in neural networks to remove systematic noise during back-propagation. It delves into the derivation of partial derivatives and the differentiation process, emphasizing the importance of shared weights and normalization partitions. In our work, we track exponential running estimates across batches, marginalizing dimensions into accumulators of shape C. The b subscript indicates running averages with momentum as a hyperparameter. We regularize gradient regression with virtual activations and gradients, appending virtual batch items for standard BN processing. Virtual data receives virtual gradients during back-propagation. Regularized gradient regression with virtual data z + , \u2202L \u2202z + and z \u2212 , \u2202L \u2202z \u2212 modifies gradients during back-propagation. Virtual data can be weighted with hyperparameters, leading to improvements in experiments. Performance of larger batches is not recovered, as seen in final validation performances. Overfitting is observed in baseline evaluation with identity, but not in accuracy. The base learning rate was increased by a factor of 1/64 compared to the baseline rate for runs with batch size 128, resulting in noticeable overfitting in terms of cross entropy but not accuracy."
}