{
    "title": "BkxAUjRqY7",
    "content": "An important question in task transfer learning is determining task transferability, measured by a novel metric called H-score. This metric estimates the performance of transferred representations from one task to another in classification problems, providing a clear operational meaning for task relatedness. Inspired by information theory, H-score can help select source tasks and devise efficient transfer learning policies. Transfer learning is a learning paradigm that exploits relatedness between different tasks to reduce the need for supervision. It involves initializing parameters from a previously trained model for a related task, rather than starting from scratch. This approach has been successfully applied in various tasks, including medical image analysis. Transfer learning is used for target tasks with limited labeled data, such as medical image analysis and structural damage recognition in buildings. The key question is transferability, determining when and to what extent a transfer may work. Traditionally, transferability is measured empirically using model loss or accuracy, but this study aims to estimate transferability more efficiently and accurately. In this study, the focus is on estimating transferability directly from training data using statistics and information theory. The method quantifies the transferability of feature representations across tasks by analyzing the error probability of using a feature for a learning task. The H-score is used as a metric to evaluate feature effectiveness and transferability among tasks. Task transferability is defined as the normalized H-score of the optimal source feature for the target task. The paper introduces a transferability metric based on statistics and information theory. It can be computed efficiently from input data and is consistent with empirical measurements. Theoretical results are presented in Sections 2-4, with experiments on image classification and scene understanding tasks in Section 5. Related works are reviewed in Section 6. In this paper, notations and concepts in Euclidean information theory and statistics are introduced. Random variables, values, alphabets, and probability distributions are represented by X, x, X, and P X respectively. Tasks are defined as tuples (X, Y, P XY) where X is training features, Y is training labels, and P XY is the joint probability. Transferability is defined using local information geometry concepts. The paper introduces notations and concepts in Euclidean information theory and statistics. It discusses binary hypothesis testing problems and error exponents, focusing on the asymptotic error probability. The error exponent is characterized as the squared length of a projection, with the log likelihood ratio playing a key role. The paper assumes small values and introduces the concept of MatrixB as the Divergence Transition. In this paper, MatrixB is defined as the Divergence Transition Matrix (DTM) of a joint probability. The singular values of B satisfy certain conditions, and functions are defined for each i = 1, . . . , K \u2212 1. The maximal HGR correlation problem is studied, finding the K strongest, independent modes in P XY efficiently using the ACE algorithm. It is shown that f * and g * are the universal minimum error probability features for all possible inference tasks. Performance metrics for feature representation in learning tasks are presented in this section. Performance metrics for feature representation in learning tasks are evaluated by finding a k-dimensional functional representation that is most discriminative for classification. An analytical approach based on the H-Score is used to measure the effectiveness of the feature function in predicting labels. The H-Score considers inter-class variance and feature redundancy to determine the discriminative and efficient features for learning tasks. The H-Score is a key metric for evaluating the effectiveness of feature representation in learning tasks. It provides insight into the discriminative and efficient nature of features for predicting labels, with a deeper operational meaning related to error probability in hypothesis testing. The H-Score is proportional to the error exponent of the decision region based on the feature representation, particularly when the features are zero-mean with identity covariance. Incorporating normalization into the error exponent computation results in Definition 2. The proof for Theorem 1 uses H(f) = B \u039e 2 F, where B is the DTM matrix and \u039e is composed of information vectors \u03be i. This allows for an upper bound on the H-score of a learning task. The H-score is used to measure task transfer learning effectiveness and solve the source task selection problem. In a neural network setting, knowledge transfer from a source task to a target task involves copying parameters from the first N layers of the trained source model to the target model. Transferability is quantified by the error probability feature of the target task, denoted as T(S, T). This approach helps solve the source task selection problem. The error exponent of transferring from source task S to target task T via feature representation fS is quantified by T(S, T) = r, where 0 \u2264 T(S, T) \u2264 1. Fine-tuning in task transfer learning involves adding free layers before the target classifier, optimized for the target label. For transferability to hold exactly, fine-tuning layers should consist of linear transformations. The H-score is equivalent to the log-loss of the linear transfer model under the local assumption. The transferability metric can be used to compare task transferability with fine-tuning. Computing H-score from sample data takes O(mk^2) time. The ACE algorithm can efficiently solve the HGR-Maximum Correlation problem for discrete variables. For continuous variables, f opt can be obtained through a different formulation of the HGR maximal correlation problem. BID26 reformulated the maximal HGR correlation objective to eliminate whitening constraints, using neural network layers to model functions f and g. The sample complexity of ACE is only 1/k of estimating P Y,X directly, making transferability computable with fewer samples. Maximizing the objective with respect to zero-mean function g results in the H-score definition. The computation of H T (f opt) can be skipped in certain cases, such as when selecting the source task with the largest transferability to the target task. The H-score is related to mutual information, with H(f(x)) \u2264 2I(X; Y). Figure 3 compares the optimal H-score. The optimal H-score of a synthesized task increases with the feature dimension k, reaching an upper bound when k \u2265 6. H-score is consistent with mutual information for large feature dimensions. H-score is easier to compute than mutual information for continuous input variables. In this section, the transferability metric is validated through experiments on real image data, covering object classification and non-classification tasks in computer vision. The metric is compared with the performance of transferring features from ImageNet 1000-class classification to Cifar 100-class classification using a similar network. Cifar-100 is considered more challenging due to its smaller sample size and lower resolution images, making it suitable for transfer learning. The transfer network used a pretrained ResNet-50 model for the target task, training on 20,000 images from Cifar-100 split into training and testing sets. H-score was validated, showing a linear decrease in log-loss and increase in accuracy as H-score increased. The relationship between H-score and log-loss was not affected by target sample size. The experiment demonstrated that target sample size does not affect the relationship between H-score and log-loss. Transfer performance is better when an upper layer of the source network is transferred, indicating similarity between tasks. The transferability metric was tested for selecting the best target task for a given source task, showing promising results. In an experiment using the Taskonomy dataset, transferability was compared to task affinity for 3D scene understanding tasks. The dataset contains 4,000,000 images with annotations for 26 computer vision tasks. 20,000 images were randomly sampled for training, and eight tasks were selected for analysis. In an experiment using the Taskonomy dataset, 20,000 images were randomly sampled for training. Eight tasks were chosen, including classifications and pixel-to-pixel tasks. The ground truths for pixel-to-pixel tasks like Edges and Depth were clustered into a palette of 16 colors for H-score computation. The experiment was conducted on a workstation with 3.40 GHz \u00d78 CPU and 16 GB memory, with each pairwise H-score computation finishing in less than an hour. Source tasks were ranked based on their H-scores for a given target task. In an experiment using the Taskonomy dataset, source tasks were ranked based on their H-scores for a given target task. The results show that the top two transferable source tasks are identical for both methods, with some differences in lower rankings, especially in 3D pixel-to-pixel tasks. The experiment on the Taskonomy dataset ranked source tasks based on their H-scores for target tasks, showing similarities in the top transferable tasks. The correlation between transferability and affinity was positive, indicating task relatedness. Agglomerative clustering was used to group tasks based on their H-scores, revealing relationships between 2D and 3D tasks. In the experiment on the Taskonomy dataset, source tasks were ranked based on their H-scores for target tasks, showing similarities in transferable tasks. Higher order transfer involves combining features from multiple tasks for better performance, with feature concatenation being a common method. The ranking results of all combinations of source task pairs for each target task are shown in Figure 8. The exception for 3D Occlusion Edges and Depth is examined in Figure 9 through visualizing pixel-by-pixel H-scores. The exception in Figure 9 visualizes pixel-by-pixel H-scores for first and second order transfers to Depth using a heatmap. Combining tasks with different transferability patterns has a significant improvement on the target task's performance. Transfer learning can be categorized into domain adaptation and task transfer learning, with our focus on the latter. Our paper focuses on transfer learning, specifically task transfer learning, which involves comparing transfer accuracy between different layers in neural networks for image classification and NLP tasks. Studies have also looked at task relatedness through approaches like task generation and theoretical results on transferability. In transfer learning, task transfer accuracy is compared between different neural network layers for image classification and NLP tasks. Task relatedness can be estimated from data using various approaches such as feature subset selection or feature weight learning. Universal feature selection aims to find the most informative features when the exact inference problem is unknown. In this paper, H-score is introduced as an information theoretic approach to evaluate feature performance in multi-task transfer learning. The transferability metric measures the efficiency of transferring features across tasks, with successful predictions shown in ImageNet-1000 to Cifar-100 transfer. The metric is also applied to various computer vision tasks using the Taskonomy dataset, with plans to extend to non-classification tasks in future works. The plan is to extend theoretical results to non-classification tasks and relax local assumptions on conditional distributions. Investigating higher order transferability properties and developing scalable algorithms. Using transferability information to design better task hierarchies for transfer learning. The optimal test for empirical distribution of samples can be stated in terms of information-theoretic quantities. The binary hypothesis testing problem is illustrated with probability density functions for different distributions. The curr_chunk discusses the probability of type I and type II errors in hypothesis testing, along with error exponents and information vectors. It also mentions using local information geometry to connect error exponents. The overall probability of error is defined, and references are made to Cover & BID6 for more background information on error exponents. The curr_chunk discusses the error exponent in hypothesis testing using local information geometry. It presents Lemmas 1 and 2, connecting the error exponent to feature functions. A k-dimensional generalization of Lemma 2 is also provided. The curr_chunk discusses the HGR maximal correlation as a generalization of Pearson's correlation coefficient to capture non-linear dependence between random variables. It highlights properties and solutions for different feature dimensions. The curr_chunk discusses an efficient algorithm, ACE, for computing the joint probability in real applications. It involves initializing functions and iteratively updating conditional distributions. The algorithm has variations for improved efficiency and supports continuous X with large feature dimensions. An alternative formulation supporting continuous X and large feature dimension k has been proposed. The ACE algorithm converges in exponential time, with precise sampling complexity for k = 1. The algorithm computes true maximal correlation functions and estimations from sampled training data, showing exponential convergence using Sanov's Theorem. The proof considers a 1-dimensional feature function f : X \u2192 R, with lemma DISPLAYFORM0 stating \u03be(x) = P X (x)f (x). Theorem 3 discusses properties of conditional expectations of f (x) given P X|Y =0 , P X|Y =1 \u2208 N X (P 0,X ). Lemmas 4 and 2 are applied to derive equations for the conditional expectations. The proof involves deriving equations for conditional expectations in a multi-dimensional case. It also discusses the cross-entropy loss in softmax regression for minimizing the joint empirical distribution. Using information geometry, it can be shown that minimizing the log loss is closely connected to the modal decomposition of B. The classification performance is measured with B \u2212 \u03a8\u03a6 T 2 F, where (f, \u03b8) are associated with (\u03a8, \u03a6). In the context of estimating transferability, the goal is to find the optimal weight \u03a8 * that minimizes the log-loss by taking the derivative of the Objective function. The close-form solution for the log loss involves the H-score term. The log loss is negatively linearly related to H-score, as demonstrated experimentally using synthesized tasks. The relationship is linear with a constant offset, and under the local assumption, the DTM matrix of X and Y is defined.\u03c6 and \u03a6 X|Y are used to show this relationship. The mutual information is expressed in terms of information vector \u03c6 X|Y. Results from Zamir et al. (2018) compare H-score and affinity score for pairwise transfer. Classification task results are shown in FIG1 and Depth results in 13. Affinity and transferability agree on top tasks despite different value ranges. Quantization of pixel-to-pixel task labels balances computational complexity and information loss. Too much loss leads to bad approximation, while little loss requires larger clusters and costs. Figure FORMULA3 shows information retention after quantization. After quantization, cluster centroids were used to recover ground truth image pixel-by-pixel. Setting cluster number N = 16 strikes a balance between recoverability and computation cost. Edge detection results on a sample image showed that N = 5 resulted in lost edges in the ground truth image."
}