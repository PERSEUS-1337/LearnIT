{
    "title": "SklZVQtLLr",
    "content": "The description of neural computations in neuroscience involves two competing views: a classical single-cell view and a more recent population view. The interaction between cell classes and low-dimensional trajectories in shaping neural computations is not well understood. To address this, machine-learning tools are used to train RNNs and analyze network dynamics. Low-rank, mixture of Gaussian RNNs are introduced to control the dimensionality of neural dynamics. Using back-propagation, we determine the minimum rank and number of cell classes needed to implement neuroscience tasks of increasing complexity. Mean-field theory is then used to reverse-engineer the solutions and identify the roles of dimensionality and cell classes in neural computations. The rank determines the phase-space for dynamics, while multiple cell classes allow networks to switch between different dynamics. This approach has implications for neuroscience experiments and the development of explainable AI. Reverse-engineering RNNs on neuroscience tasks has provided insights into cognitive processes, focusing on neural dynamics and connectivity. A new approach using low-rank networks leads to analytically tractable RNNs with defined dimensionality and cell classes, revealing their roles in neural computation. The dynamics of recurrent networks are defined by inputs and a scalar readout for the network's output. This approach determines the minimum rank and cell classes needed for complex neuroscience tasks, with implications for explainable AI development. The dynamics of recurrent networks are described by inputs and a scalar readout modeling the network's output. The network's inputs and output directions are defined by column vectors, while recurrent connectivity is given by a rank-K matrix. Previous studies developed a mean-field theory for the dynamics of internal variables in networks with joint Gaussian distribution of connectivity and input vectors. The dynamics can be described by internal variables \u03ba1 and \u03ba2, with functional connectivities determined by a structural component and an activity-dependent term. This framework is extended to describe networks in the present work. In this work, the framework is extended to networks with neurons assigned to different populations, each described by connectivity vectors drawn from mixtures of Gaussians. Cell classes are defined by connectivity profiles, and mean-field theory is used to determine functional connectivities. A mixture of Gaussians network model is applied to classical neuroscience tasks, with minimal-rank connectivity and cell classes. Training a RNN is done in a supervised manner using BPTT and the ADAM algorithm to optimize connectivity matrices. In this study, the focus is on training networks with various rank values to find the minimal rank K min for a solution. Mean-field theory is used to analyze low-rank RNNs and extract statistical features of trained vectors. By reconstructing rank-K min RNNs with connectivity vectors described by a P-mixture of Gaussians, the study identifies principles regarding dimensionality and cell roles. Our approach identified two general principles for the roles of dimensionality and cell classes in building networks that perform multiple tasks. For a perceptual integration task, a network with rank K = 1 and P = 1 population can integrate input signals to make positive/negative decisions. The internal variable \u03ba integrates the input signal before converging to fixed-points, similar to drift-diffusion models for perceptual integration tasks. In a parametric working memory task, a network with rank K = 2 and P = 1 population is needed to report the difference between two stimuli. The dynamics of the network show how two internal variables \u03ba 1 and \u03ba 2 implement the required computational variables for the task. The rank of the network determines the dimensionality of the phase space and the number of internal variables available for computation. In a context-dependent perceptual integration task, a network with a single population is not able to integrate signals based on a contextual cue. However, a rank-1 network with two populations can successfully switch between integrated signals. Having two populations allows the network to switch between different operations by reconfiguring the internal variable's dynamical landscape in a context-dependent manner. Contextual inputs modulate population gains, controlling functional connectivities. Multiple populations enable flexible switching between dynamics, implementing various operations on internal variables. Increasing dimensionality allows parallel processing of inputs, while increasing the number of populations enhances task flexibility. Increasing the number of populations allows for selective modulation of functional connectivities, enhancing flexibility in controlling internal variables. Networks can perform multiple tasks in parallel by combining rank-1 matrices solving different tasks simultaneously. Additionally, constructing networks with multiple input streams and a single internal variable enables solving complex tasks. This work provides an abstract description of computations involving parallel processing and task flexibility. In this work, an abstract description of computations in recurrent neural networks is provided, focusing on dimensionality and cell classes. The understanding gained from simple tasks has been used to build networks solving multiple tasks, with implications for reverse-engineering networks trained on real-world tasks."
}