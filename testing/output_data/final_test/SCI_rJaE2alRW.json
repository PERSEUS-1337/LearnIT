{
    "title": "rJaE2alRW",
    "content": "The Significance-Offset Convolutional Neural Network is a deep architecture for regression of multivariate asynchronous time series, inspired by AR models and gating mechanisms in RNNs. It uses an AR-like weighting system learned through a convolutional network, achieving promising results on various datasets. The code for numerical experiments and implementation is available. Time series forecasting focuses on modeling future values based on past observations using conditional probability distribution. This problem has been approached by econometrics and machine learning communities independently. This paper examines the use of convolutional neural networks (CNNs) in modeling the conditional mean for autoregression in multivariate and noisy financial time series data. The code for experiments and implementation will be shared online for reproducibility. Financial time series in the financial data have low signal-to-noise ratio and heavy-tailed distributions, making market returns predictability a challenge. Different sources observe the same signal asynchronously, each with biases and noise that need to be recovered. The original signal for recovery is from different sources observing the same signal asynchronously, with possible lead-lag relationships. Traditional econometric models may not be sufficient due to changing factors over time, motivating the coupling of linear models with deep neural networks for learning nonlinear relationships. Quotes from four market participants for the same CDS show bid and ask prices throughout the day. The spread between the best sell and buy offers is also displayed. The SignificanceOffset Convolutional Neural Network is proposed as an extension of standard autoregressive models for time series forecasting. Inspired by successful gating systems in recurrent neural networks, it competes with multilayer CNN and LSTM models. Literature in time series forecasting has a history in econometrics, focusing on explaining variables rather than improving prediction power. Recent research has focused on explaining variables in time series forecasting rather than improving prediction power. Models often 'over-fit' on financial time series, leading to unstable parameters and poor out-of-sample performance. Gaussian processes are commonly used for forecasting irregularly sampled time series. There is a trend towards combining machine learning and econometrics models, such as the Gaussian Copula Process Volatility model. This paper combines AR models and neural networks, following the success of deep neural networks in various fields like computer vision. The existing literature in various fields like computer vision, audio signal processing, and natural language processing has been surpassed. However, applying sequence modeling and adversarial training techniques from other domains to financial time series forecasting is challenging due to the stochastic nature of financial processes. Deep learning for time series forecasting, especially in the financial domain, is still limited despite growing interest in using neural networks for predictions. In the financial domain, interest in using neural networks for predictions is not new. Recent papers have explored different architectures like 4-layer perceptrons and WaveNet for modeling price change distributions. However, promising results or innovative architectures have not been publicly published yet. This paper investigates the capabilities of standard architectures like CNN, Residual Network, and multi-layer LSTM on artificial and real financial time series data. Gating mechanisms for neural networks, essential in training recurrent architectures, can be expressed as a function with a sigmoid nonlinearity controlling the output flow. Compositions of these functions lead to popular recurrent architectures like LSTM and GRU. Similar concepts have been used in highway networks for successful training of deeper architectures. The proposed gating system aims to weight different candidate predictors, similar to softmax gating in MuFuRU. This approach involves weighting outputs of intermediate layers in neural networks, similar to attention networks used in tasks like image captioning and machine translation. The method involves weighting separate inputs based on learned functions modeled as multi-layer CNNs, without using recurrent connections. The proposed method involves using multi-layer CNNs instead of projections on learned directions, without recurrent layers. Gaussian processes are useful for handling asynchronous data but are not suitable for financial datasets with fat-tailed distributions. Predicting autoregressive time series like AR(2) can involve highly nonlinear functions when sampled irregularly. When dealing with asynchronous sampling in autoregressive time series, estimating parameters becomes more complex due to the lack of closed-form functions. Multivariate time series observed separately and asynchronously add further difficulty in assigning appropriate weights to past values. Aligning such series at fixed frequencies may result in information loss or dataset enlargement. When dealing with asynchronous sampling in autoregressive time series, aligning separate dimensions as a single one with dimension and duration indicators can help avoid information loss or dataset enlargement. This approach, at the core of the proposed architecture, stores consecutive observations together as a single value series with indicator features for series identification and durations between observations. This data representation is suitable for prediction using models like LSTM. The proposed model combines autoregressive approach with neural network to assign meaningful data-dependent weights to memorized observations. This allows for flexibility in situations where observed values may be biased, by using a neural network to calculate the weights. The model proposed combines autoregressive approach with neural network to assign data-dependent weights to memorized observations, allowing flexibility in biased situations. It formalizes the idea of predicting conditional future values of a subset of elements in a multivariate time series using a fully convolutional network with specific activation functions and matrix operations. The proposed SOCNN architecture combines autoregressive approach with a convolutional network to assign data-dependent weights to memorized observations. It predicts conditional future values of elements in a multivariate time series, preserving the time dimension up to the top layer. The network includes offset and significance networks, with the last convolutional layer having filters equal to the output dimension. The proposed SOCNN architecture combines autoregressive approach with a convolutional network to assign data-dependent weights to memorized observations for predicting future values in a multivariate time series. The separation of temporal dependence, local significance of observations, and predictors provides interpretability of the fitted functions and weights. The significance network assigns data-dependent weights to regressors in an autoregressive manner. The SOCNN architecture combines autoregressive approach with a convolutional network to assign data-dependent weights to memorized observations for predicting future values in a multivariate time series. The significance network learns high-level features indicating the relative importance of past observations, considering time and duration between observations. The function L2 error is a natural loss function for estimators of expected value. The offset network output can be seen as separate predictors of changes between observations and the target variable. An auxiliary loss function is considered equal to the mean squared error of intermediate predictions. The model is evaluated on financial, artificially generated, and household electric power consumption datasets. The impact of positive values of \u03b1 on model training and performance is discussed. The performance of SOCNNs is compared with simple CNN, single-and multi-layer LSTM, and 25-layer ResNet using a household electric power consumption dataset. The impact of network components and training process details are discussed. The network architecture is tested on artificially generated multivariate time series datasets. The final series is composed of a univariate series, durations between random times, and indicators of the 'available source'. The household electricity dataset contains measurements of 7 quantities related to electricity consumption recorded every minute for 47 months. The aim is to focus on asynchronous time-series by altering it to contain only one feature per observation with durations ranging from 1 to 7 minutes. The proposed model aims to predict features in incoming non-anonymous quotes from the credit default swap market. The dataset consists of 2.1 million quotes from 28 sources, each with 31 features. The model predicts the next quoted price based on the last 60 quotes from one of the fifteen most active market participants. The proposed model focuses on constructing artificial asynchronous time series datasets for reproducible research. The networks outperform benchmarks on asynchronous, electricity, and quotes datasets, while almost matching results on synchronous datasets. The significance network plays a crucial role in performance, with the offset network's depth having minimal impact. The proposed SOCNN model showed lower variance in test and validation errors, especially in the early training stages and for the quotes dataset. The small positive auxiliary weight helped stabilize test error, while higher weights improved performance on asynchronous datasets. Comparing SOCNN to other networks, it reacted better to additional noise in input terms, resulting in better results. The SOCNN model is the most robust among compared networks, less prone to overfitting, and reacts better to additional noise in input terms. The significance of noisy observations increases with noise magnitude, showing that noisy observations are not discarded by SOCNN. The proposed weighting mechanism coupled with convolutional networks creates a new neural network architecture for time series prediction, specifically designed for regression tasks on asynchronous signals with high noise levels. This approach has shown success in forecasting financial and artificially generated asynchronous time series, outperforming popular convolutional and recurrent networks. The model can be extended by adding intermediate weighting layers and exploring different convolutional kernels in the offset sub-network. Further empirical studies are needed to test the performance on other real-life datasets. The proposed architecture for time series prediction involves a new neural network design for regression tasks on asynchronous signals with high noise levels. The model has shown success in forecasting financial and artificially generated time series data, outperforming popular networks. There is a need for common econometric datasets benchmark and time series regression. The proposed architecture for time series prediction involves a new neural network design for regression tasks on asynchronous signals with high noise levels. To evaluate the robustness of the networks, noise terms are added to part of the input series and evaluated on datapoints. The procedure involves selecting observations, adding noise terms, and evaluating trained models on the dataset separately. The proposed model for time series prediction involves a new neural network design for regression tasks on asynchronous signals with high noise levels. To evaluate the model, a grid search is performed on hyperparameters, including the offset sub-network's depth and the auxiliary weight alpha. The model's performance is compared with CNN, ResNet, multi-layer LSTM networks, and linear (VAR) model. The benchmark networks were designed to have a comparable number of parameters. The neural network model for time series prediction used an activation function with a leak rate of .1 in all layers except the top ones, where linear activation was applied. The CNN had the same number of layers, stride, and kernel size structure. Max pooling with a pool size of 2 was applied every two convolutional layers. The network hyperparameters used in comparison are shown in TAB3. The training and validation sets were randomly sampled from the first 80% of timesteps, with a 3 to 1 ratio. The remaining 20% of data was used as a test set. Adam optimizer was used for training, with a batch size of 128 for artificial data and 256 for quotes dataset. Batch normalization was applied between each convolution and activation. Training samples were shuffled at the beginning of each epoch to prevent overfitting. At the beginning of each epoch, training samples were shuffled to prevent overfitting. Dropout and early stopping were applied. Weights were initialized following a normalized uniform procedure. Experiments were conducted using Tensorflow and Keras. Artificial data models were optimized using a NVIDIA GPU, while the quotes dataset used an Intel CPU. A multivariate time series with noisy observations of an autoregressive signal was simulated. The series were constructed using specific parameters. Synchronous and asynchronous artificial series were simulated, with different durations between observations. The curr_chunk discusses the creation of synchronous and asynchronous time series datasets with 7 features. The asynchronous dataset was created by deterministic time step sampling and random feature sampling. The curr_chunk describes random feature sampling in a 10-dimensional vector dataset with 7 indicator features. The sub-sampled dataset is 40% of the original dataset's length, and the schedule of sampled timesteps and available features is provided in the supplementary material. The activations of significance and offset sub-networks are shown for three input series from the network trained on the electricity dataset. Each row displays activations corresponding to past values of a single feature, with 25 out of 60 past values included in the input data for 3 separate datapoints. Note the log scale on the left graph."
}