{
    "title": "Hkc-TeZ0W",
    "content": "Our hierarchical model efficiently places computational graphs on heterogeneous hardware devices, learning to assign operations to groups and allocate them to devices without human intervention. Experiments show superior performance in optimizing TensorFlow graphs with over 80,000 operations compared to human placements. Our method achieves significant runtime reductions of up to 60.6% per training step for models like Neural Machine Translation, surpassing previous state-of-the-art placement methods. Deep neural networks have been effectively used in various practical applications, leading to increased demand for computational resources. To address this demand, a common approach is to use a distributed environment with CPUs and GPUs, where operations of neural networks are explicitly placed on specific computing devices for model and data parallelism. Automated solutions for efficiently distributing computation in neural networks are becoming more important due to the growing diversity of hardware devices and trends towards automated neural architecture search. Device placement involves learning to partition a graph across available devices, with traditional graph partitioning methods serving as a baseline. Traditional graph partitioning methods are used as a baseline for automated device placement in neural networks. Experiments with Scotch, an open-source library, aimed to balance computational load and minimize communication cost. However, the approach yielded disappointing results due to the non-stationarity of the cost function. The focus is on a distributed environment using shared CPUs and GPUs, where cost-based models provide a strong baseline for memory optimizations. In this paper, a new approach is proposed for automated device placement in neural networks. Traditional graph partitioning methods serve as a baseline for memory optimizations in dynamic cost environments. A recent method (referred to as ColocRL) utilizes a recurrent neural network policy network to optimize computation speed in computational graphs. However, this approach is limited to small graphs and requires manual partitioning by human experts. The paper proposes a two-level hierarchical network for automated device placement in neural networks, eliminating the need for manual grouping. The first model, the Grouper, groups operations of the graph based on context, while the second model, the Placer, assigns devices to these groups. The network is trained using reinforcement learning to optimize computation speed and feasibility. Our method for automated device placement in neural networks is end-to-end and does not require manual grouping of operations. It effectively handles large graphs and outperforms default placements in models like Inception-V3, ResNet, Language Modeling, and Neural Machine Translation. The approach learns the tradeoff between computation and communication in hardware, showcasing superior results compared to other placement algorithms. The Hierarchical Planner method optimizes device placement in neural networks to reduce training time per iteration by 60.6%. It consists of a Grouper and a Placer sub-network trained jointly to speed up neural network graph training. Policy gradients are used to train the Hierarchical Planner due to the non-differentiable reward (runtime) in the problem. The Hierarchical Planner uses policy gradients to train the Grouper and Placer sub-networks. The Grouper groups operations and generates embeddings, while the Placer assigns device placements using a sequence-to-sequence model with LSTM and attention mechanism. The Grouper generates operation embeddings with 3 vectors: operation type embedding, output sizes and number of outputs vector, and adjacency information vector. The operation type embedding is of size 20 with a vocabulary of 200 TF operations. The output sizes vector is limited to 6 edges and 4 elements each, while the adjacency vector contains incoming and outgoing edges index with a maximum of 12 edges. To generate input for the Placer, groups are filled with index of incoming and outgoing operations, padded with -1 if edges are less than 6. Group embedding is created by concatenating vectors for operation type count, total output shapes count, and group adjacency info. Placer's RNN encoder reads group embeddings to produce hidden states. The Placer's decoder RNN predicts one device per time step based on the input group embeddings. Each device has its own trainable embedding and is fed as input to the next decoder time step. The decoder uses an attention mechanism to attend over the encoder states and samples one device per step from the Placer's softmax during training. Temperature and a tanh constant are used to make the activations less steep and allow for exploration in the model. The policy gradient method is used to train the Hierarchical Planner for improving decisions over time. The cost function is optimized by maximizing the expectation of the reward for device placement. Parameters of the Grouper and Placer are optimized to determine group assignment and device placement probabilities. The policy is trained using the policy gradient method to optimize the cost function for device placement. The Grouper assigns operations to groups independently, while the Placer conditions group placement on those already placed. A baseline is subtracted from the reward to reduce variance. Distributed training is implemented with a shared parameter server among controllers. In experiments, 4 controllers communicate with 16 workers, each worker executing placements on GPUs. Each controller is hosted on a single GPU, totaling 36 GPUs used. Workers run placements in parallel, and controllers compute gradients using measured runtimes. Each controller maintains a separate baseline to reduce runtime variance. In Section 3, the Hierarchical Planner is applied to machine learning models in computer vision and natural language processing. Results are compared to heuristic and RL-based graph optimization baselines, showing the ability to find performant placements. The approach is evaluated on four deep neural networks, including Inception-V3, demonstrating the hierarchical architecture's effectiveness in learning better placements. The network consists of multiple blocks with convolutional and pooling layers, executed sequentially. ResNet is a popular model for image classification using residual connections. RNNLM is a Recurrent Neural Network Language Model with LSTM cells organized in a grid structure. The Neural Machine Translation model with attention mechanism has a similar architecture to RNNLM but is more computationally expensive due to its many hidden states. To decrease training time, BID28 and BID35 propose placing each LSTM layer, attention, and softmax layer on separate devices. Our Hierarchical Planner finds better placements than previous methods. Three versions of the NMT model were evaluated with batch size 64: a 2-layer encoder-decoder, a 4-layer version, and an 8-layer version. In a study comparing deep RL methods, the model architectures, hyperparameters, and input data were kept consistent with previous state-of-the-art methods. Evaluations were done on various models including a 152-layer ResNet with ImageNet data and more complex NMT models with 4 and 8 layers. Baselines such as CPU Only, GPU Only, and Scotch placements were compared to placements found by their approach. TensorFlow automatically places operations not implemented on GPU on CPU. Scotch static mapper BID24 is used for graph mapping, considering computational cost, data flow, and device capacities. Scotch optimizer is used for MinCut, focusing on balancing computation and minimizing interdevice communication on GPUs. Human experts hand-craft placements for different models, optimizing for model parallelism. ColocRL method is also utilized in the placement process. The ColocRL method uses policy gradient to train a recurrent neural network for placing colocation groups on devices. Rewards are based on the negative square root of runtime for training steps in TensorFlow models, with invalid placements receiving a large negative reward. Runtimes are measured in seconds, and the reward calculation involves running predicted placements for 10 steps and using the median value of the last 5 steps. The experiments were conducted using TensorFlow r1.3 on machines with 1 Intel Haswell 2300 CPU and up to 8 Nvidia Tesla K40 GPUs. The Policy Network architecture includes a Grouper with a hidden size of 64 and a Placer with an LSTM hidden size of 256. The encoder of the sequence-to-sequence model consists of two layers of LSTM forming a bi-LSTM, while the decoder uses a uni-directional LSTM. The Grouper's softmax output size is set to 256, and various group sizes were experimented with (64 to 1024). In our experiments, the Placer's softmax output size is set to 256. The number of unrolled steps in the Placer is equal to the number of groups. Noise was added to the logits of both the Grouper and the Placer networks for the first 500 policy training steps to encourage exploration. The policy is updated only for valid placements after the initial 500 steps to prevent convergence to rewards associated with invalid placements. Comparisons with Graph Partitioning Heuristics are reported in Table 1. The Hierarchical Planner's performance is compared with graph partitioning heuristics in Table 1. The model learns to use a single GPU for ResNet and RNNLM, while distributing Inception-V3 across 2 GPUs for a 16.3% reduction in runtime. The NMT model is placed across multiple GPUs for efficient processing. The Hierarchical Planner outperforms human experts in GPU allocation for NMT models with 2 and 4 layers, achieving a 60.6% and 53.7% improvement, respectively. Visualization of the placement for NMT (4-layer) is provided. However, for NMT (8-layer), the automated method is slightly slower than human experts but still useful. Results for Scotch and MinCut were worse than human expert baselines. A comparison with ColocRL would require using the same software. ColocRL requires running models with the same software and hardware. Our approach shows a 60.6% improvement over best heuristics for NMT (2-layer), compared to 19.0% for ColocRL. Results for NMT (4-layer) and NMT (8-layer) were not reported for ColocRL. ColocRL makes the assumption that certain operations must be colocated, unlike our method. The Hierarchical Planner's placements show high granularity and infeasible parallelism for prior methods. The Hierarchical Planner's placements show high granularity and infeasible parallelism for prior methods, with a placement for NMT (4-layer) resulting in a 53.7% faster runtime per training iteration compared to hand-crafted placements. The generated placement is non-trivial and highly parallelized, distributing all unrolled steps of LSTM, attention, and softmax layers across multiple GPUs. This level of parallelism is impossible for methods like ColocRL, which forces all operations within an unrolled LSTM step to be on the same device. The automated placement method enables efficient grouping of operations on different devices, outperforming previous methods like ColocRL. The policy search space is large, with 5 devices and 46,600 operations. Through training, the Grouper learns to partition the computational graph, ultimately using only a small subset of groups. This approach treats device placement as a sequential decision-making task. The Hierarchical Planner optimizes device placement for a TensorFlow graph by training a new policy for each model. Results show a minimal 7% difference in placement speed after 1000 iterations of policy updates, taking up to three hours for the largest benchmark. The reward calculation involves running the model according to the predicted placement for 5 training steps. The policy is a lightweight network trained on a single GPU to optimize model placement on multiple devices. Actual runtime measurements are used to calculate the reward, with worker nodes running the input model for predicted placements. Even with limited hardware, good results can be achieved, as shown by training the policy on a 4-layer NMT benchmark model on 5 devices. The policy can be trained efficiently with just one worker, reducing policy training time. While more workers can further decrease training time, reasonable times can still be achieved with only one worker. For example, it takes less than 2.5 hours for the policy to achieve a placement with a training step time of 1.94 seconds. This overhead is justified when training NMT models for hundreds of thousands of steps. The NMT model needs to run for approximately 562500 steps, with a reduced runtime per step saving 265 GPU-hours. Training the Hierarchical Planner first and then using its placement for the target model is the current approach, but interleaving their training could improve efficiency. The Hierarchical Planner was compared against a simpler model called the Simple Planner, which independently predicts operation placements. The Simple Planner independently predicts operation placements in the input model, finding placements within 20% of the Hierarchical Planner for Inception-V3. It successfully places RNNLM and ResNet benchmarks on a single GPU but struggles with larger models like NMT with 4 or 8 layers. The Hierarchical Planner breaks the problem into grouping and placing sub-tasks, enabling it to scale to larger models and condition placement on previously placed operations. The Hierarchical Planner breaks the problem into grouping and placing sub-tasks, enabling it to scale to larger models and condition placement on previously placed operations. The Grouper significantly improves the performance of the Hierarchical Planner by learning groupings rather than using random ones. The paper presents a hierarchical method for efficiently placing operations of a computational graph onto devices using learned groupings. The approach utilizes a policy gradient method to optimize parameters and can scale to graphs with over 80,000 operations. The method surpasses placements by human experts and previous deep RL methods, achieving up to 60.6% better performance on tasks like image classification and language modeling."
}