{
    "title": "Sye7qoC5FQ",
    "content": "The goal of network representation learning is to learn low-dimensional node embeddings that capture the graph structure and are useful for downstream tasks. A study on the robustness of these methods to adversarial attacks is lacking. The first adversarial vulnerability analysis on methods based on random walks shows efficient perturbations that poison the network structure, impacting both embeddings and downstream tasks. These attacks are transferable and successful even with restrictions. Unsupervised node embedding approaches are gaining popularity for achieving state-of-the-art performance on network learning tasks. The goal of network representation learning is to embed nodes in a low-dimensional space to capture the graph structure. Techniques based on random walks are successful for tasks like link prediction and node classification. However, the robustness of these methods to adversarial attacks has not been studied. Traditional learning methods can be easily fooled by data perturbations, but the impact on node embeddings is still unknown. The robustness of node embeddings to adversarial perturbations in graph data has not been addressed. Adversaries are common in domains using graph embeddings, making it crucial to assess if node embedding approaches can be easily manipulated. Our work differs from existing studies by focusing on plain graph data and exploring the potential cascading effects of perturbations on interconnected nodes. Our work focuses on manipulating the interaction/dependency structure of plain graph data, such as adding fake relations in social networks or fake reviews in recommendation engines. Unlike traditional node embedding methods, our approach deals with unsupervised and transductive training, making it challenging to handle poisoning attacks where the model is learned after the attack. This necessitates a different approach from classical gradient-based methods designed for continuous data. We propose a principled strategy for adversarial attacks on unsupervised node embeddings, efficiently solving a challenging bi-level optimization problem associated with the poisoning attack. Our experiments show that attacks learned based on one model successfully fool other models as well, shedding light on an efficient algorithm for finding adversarial perturbations in a discrete and combinatorial graph domain. Node embeddings are vulnerable to adversarial attacks, requiring more robustness for practical use. The study focuses on attacking unsupervised node embedding methods, showing how minor changes can significantly impact the quality of embeddings. This highlights the need for further research to enhance the resilience of node embeddings against adversarial perturbations. Unsupervised methods are often used in practice for their flexibility in solving various tasks. Adversarial attacks on machine learning models, particularly deep neural networks, have been shown to be effective with small perturbations. Recent works have demonstrated adversarial examples in different domains. Attacks can be categorized based on their goals, knowledge, and capabilities, with poisoning attacks targeting training data and evasion attacks targeting test data. Poisoning attacks are less studied compared to evasion attacks. Poisoning attacks are less studied compared to evasion attacks. Recent works have analyzed the robustness of semi-supervised graph classification methods to adversarial attacks. One approach linearizes a graph convolutional network to calculate the change in class probabilities for edge perturbations. Another approach uses reinforcement learning to select relevant edge flips for targeted attacks in semi-supervised graph classification tasks. Our work focuses on general attacks on unsupervised node embeddings, manipulating graphs for graph clustering. Previous works have optimized graph structure to manipulate information spread, user opinions, shortest paths, page rank scores, and other metrics. There are also poisoning attacks on multi-task relationship learning, but they still deal with the classical scenario of i.i.d. instances within each task. In this paper, the focus is on poisoning attacks on the graph structure, where the attacker can manipulate the graph by adding or removing edges within a budget. The goal is to damage the embeddings produced by existing models by perturbing the graph structure. The approach mainly involves random walks and extends to spectral approaches. The graph is represented as G = (V, E) with V as the set of nodes, E as the set of edges, and A as the adjacency matrix. The goal of network representation learning is to find a. The adjacency matrix |V|\u00d7|V| is used in network representation learning to find low-dimensional embeddings for each node. DeepWalk and node2vec learn embeddings based on biased random walks, maximizing the probability of observing a node's neighborhood. An attacker can modify entries in the adjacency matrix, denoted as \u00c2, within a fixed budget, affecting the graph structure. The attacker aims to degrade the quality of learned embeddings in network representation learning by modifying entries in the adjacency matrix within a fixed budget. The goal is to maximize the loss of the model under attack, impacting tasks like node classification or link prediction that rely on the embeddings as features. The attacker aims to degrade learned embeddings in network representation learning by modifying the adjacency matrix within a fixed budget. The optimization problem involves minimizing loss while the attacker tries to maximize it, leading to a complex bi-level optimization problem. Generating random walks for the skip-gram model is a crucial step, but flipping edges in the graph changes the training corpus, making gradient-based methods ineffective. Recent results suggest that RW-based node embedding approaches implicitly factorize the Pointwise Mutual Information. DeepWalk is a popular Random Walk (RW) based node embedding approach that factorizes the Pointwise Mutual Information (PMI) matrix. It utilizes Singular Value Decomposition to obtain the embedding Z*. To address sparsity in the matrix, the elementwise maximum is introduced. DeepWalk utilizes the Shifted Positive PMI (PPMI) approach and introduces the elementwise maximum to optimize the embedding Z*. The loss function for DeepWalk is based on the singular values of the matrix M(A), transforming the bi-level problem into a single-level optimization problem. However, maximizing the loss function is challenging due to the discrete nature of the problem and the need for a gradient-based approach. To efficiently calculate the gradient for DeepWalk, eigenvalue perturbation theory is used to approximate the loss without recomputing the SVD of the adjacency matrix. This allows for an efficient gradient-based optimization approach, where small perturbations result in a good approximation of the loss. The gradient-based optimization approach for DeepWalk efficiently computes the loss without recomputing the eigenvalue decomposition of the adjacency matrix. However, the method is limited when considering discrete flips and requires a dense instantiation of the adjacency matrix, which is infeasible for large graphs. This motivates the need for a more advanced sparse closed-form approach to compute the change in loss given a set of flipped edges. Lemma 2 states that the matrix S can be expressed as S = U(\u039b)UT, where U and \u039b contain the eigenvectors and eigenvalues solving the generalized eigen-problem Au = \u03bbDu. The spectrum of S (and M) can be obtained from the generalized spectrum of A. This formulation is key for an efficient approximation, especially when dealing with large graphs and edge flips. Theorem 2 provides an efficient approximation for the change in singular values of matrix M due to an edge flip. It involves the generalized eigenvalue \u03bb y of A solving \u03bb y A = \u03bb y D u y, where A and D are updated after the edge flip. This approach allows for leveraging results from eigenvalue perturbation theory to approximate the spectrum change efficiently. Theorem 2 presents an efficient method to approximate the change in singular values of matrix M resulting from an edge flip. By utilizing eigenvalue perturbation theory, the spectrum change can be efficiently approximated without the need to recompute eigenvalues. This approach simplifies the evaluation of different edge flips and allows for bounding the change in singular values of the resulting matrix S. The current chunk discusses using a surrogate loss L DW3 to analyze the spectrum of matrix M, as there are no tools available for direct analysis. The surrogate loss is effective in attacking node embeddings and the skip-gram model. Methods based on spectral embedding factorize the graph Laplacian and are connected to RW based approaches. The goal is to maximize L DW3 by performing edge flips to reduce complexity. The current chunk discusses forming a candidate set by randomly sampling flips and computing their impact on the loss via L DW3. The runtime complexity of the approach is O(N \u00b7|E|+C \u00b7N log N ). The approach is easily parallelizable, and it is suboptimal to maximize the overall loss via L DW * when attacking a specific node or downstream task. The current chunk discusses defining a target-specific loss depending on t's embedding for edge flips, focusing on changes in eigenvectors rather than eigen/singular values. Two cases are studied: misclassifying a target node and manipulating node pair similarity. Surrogate embeddings Z * are introduced to efficiently compute eigenvector changes, showing no significant impact on downstream tasks. The performance on downstream tasks can be improved by approximating generalized eigenvectors in closed-form after performing edge flips. The computation can be made more efficient by pre-computing certain terms and parallelizing the process for each eigenvector. After pre-computing certain terms and parallelizing the computation for each eigenvector, the optimal embeddingZ * (A ) can be efficiently computed for any edge flip. The targeted attack aims to enforce misclassification of the target node t for node classification, using pre-trained classifier C on clean embeddingZ * . The loss is calculated based on the classification margin m(t) = p t,c(t) \u2212 max c =c(t) p t,c. The targeted attack aims to decrease the similarity between nodes with an edge and increase similarity between nodes without an edge by modifying the graph. This is achieved by selecting top flips with smallest margin and averaging over randomly trained classifiers. Another approach is to treat this as a tri-level optimization problem, targeting link prediction. The targeted attack aims to decrease similarity between nodes with an edge and increase similarity between nodes without an edge by modifying the graph through flips based on average precision scores. This work is the first to consider adversarial attacks on node embeddings with no known baselines, comparing with strong baselines such as randomly flipping edges, removing edges based on eigencentrality, and removing edges based on degree centrality. The study evaluates different attacks on node embeddings, including gradient-based, closed-form, link prediction, and node classification attacks. The experiments aim to assess the quality of approximations, damage to embedding quality, and success of attacks under restrictions. The downstream node classification task serves as a proxy for evaluating embedding quality post-attack. The study evaluates attacks on node embeddings, focusing on approximations and damage to embedding quality. The closed-form strategy outperforms gradient-based methods in approximating loss. Investigating edge removal and addition effects on sparse graphs is also explored. The study evaluates attacks on node embeddings, focusing on approximations and damage to embedding quality. Investigating edge removal and addition effects on sparse graphs is also explored. To ensure non-singleton nodes, candidate edges are randomly sampled and top edges are selected based on a scoring function. An alternative add-by-remove strategy is implemented, showing better performance empirically. The graph is undirected, with edges flipped accordingly. Strategies achieve higher loss compared to baselines when removing edges. Node classification task is used to evaluate quality, with B deg being the strongest. Our strategies outperform baselines in attacking node embeddings, causing significant damage on Cora and PolBlogs datasets. Restricted attacks are evaluated by randomly choosing a percentage of nodes to restrict, showing increasing damage as restrictions are imposed. The attack strategies outperform baselines in damaging node embeddings on Cora and PolBlogs datasets. Even with restrictions on nodes, the attack is still successful, causing damage across different node degrees and centrality distributions. The findings highlight the need for a principled method like ours, as intuitive heuristics cannot identify adversarial edges. Our method successfully attacks high-degree nodes, with 77.89% nodes misclassified on average. Low-degree nodes are easier to misclassify, while high-degree nodes are more robust. The experiments confirm that attacks hinder downstream tasks, with significant damage caused by flipping edges. Transferability of attacks between models is important, as demonstrated by the perturbed graph used for learning node embeddings. The experiments show that adversarial attacks on node embeddings have a significant negative impact on node classification and link prediction. The attacks generalize across different models, causing more damage compared to baseline transferability. This highlights the vulnerability of node embeddings to efficient adversarial computations. Attacks on node embeddings can have a significant negative impact on node classification and link prediction. Adversarial edges are transferable across different models, showcasing the vulnerability of node embeddings to efficient adversarial computations. The spectral embedding is obtained through eigen-decomposition of the graph Laplacian, with different Laplacian definitions such as unnormalized Laplacian and normalized random walk Laplacian. Future work includes modeling attacker knowledge, attacking other network representation learning methods, and developing defenses against such attacks. The unnormalized Laplacian and normalized random walk Laplacian can be attacked with a single strategy due to their shared eigenvalues. Flipping a single edge in the graph Laplacian results in a change in eigenvalues, which can be estimated through the generalized eigen-problem. The eigenvalues of the unnormalized Laplacian and normalized random walk Laplacian can be estimated by flipping a single edge in the graph Laplacian, using the theory of eigenvalue perturbation. The loss of spectral embedding after flipping an edge can be approximated by summing over the smallest eigenvalues. The analysis shows that spectral embedding and random walk approaches are similar. The change in adjacency matrix and degree matrix after a single edge flip can be approximated. The upper bound on singular values is determined, and the tightness of the bound is visualized. Transferability of the proposed attack and baseline attacks is examined. In this study, the transferability of the strongest baseline, B eig, in removing edges is examined. Results show that compared to the proposed attack, the baseline causes less damage. However, it can still significantly impact GCN when removing 250 edges on Cora. Further investigation into this counterintuitive finding is planned for future work."
}