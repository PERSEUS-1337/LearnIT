{
    "title": "rJNwDjAqYX",
    "content": "Reinforcement learning algorithms rely on engineered rewards from the environment that are intrinsic to the agent, as annotating environments with hand-designed rewards is challenging and not scalable. Curiosity-driven learning without extrinsic rewards was studied across 54 benchmark environments, showing good performance and alignment with hand-designed rewards. Different feature spaces for prediction error were explored, with learned features showing better generalization. Limitations of prediction-based rewards in stochastic setups were also demonstrated. Game-play videos and code are available for further reference. Reinforcement learning (RL) involves training agents with a reward function aligned with the task. Dense intrinsic rewards like \"curiosity\" and \"visitation counts\" supplement extrinsic rewards, aiding in training success. Intrinsic rewards guide agents to explore environments efficiently, bridging gaps between sparse extrinsic rewards. In scenarios with no extrinsic reward, intrinsic motivation like curiosity drives exploration. Pre-training agents with intrinsic rewards allows faster learning when fine-tuned to novel tasks. However, there has been no systematic study on learning solely with intrinsic rewards. In this paper, a large-scale empirical study of agents driven purely by intrinsic rewards is conducted across diverse simulated environments using a dynamics-based curiosity model. The central idea is to represent intrinsic reward as the error in predicting the consequence of the agent's action given its current state. Agents are shown to make progress using only curiosity without any extrinsic reward or end-of-episode signal. Further details, including video results, code, and models, can be found at the provided link. The study explores dynamics-based curiosity in various environments, emphasizing the importance of choosing a compact embedding space for stable online training. The goal is to predict future states accurately in high-dimensional raw observation spaces, such as images, to improve learning performance. Encoding observations via a random network is a simple yet effective technique for modeling curiosity in reinforcement learning benchmarks. This approach shows good performance in training environments, but learned features generalize better to novel games. The paper presents a large-scale study on curiosity-driven exploration in various environments, including Atari games, Super Mario Bros., Unity 3D navigation, multi-player Pong, and Roboschool. It investigates different feature spaces for learning dynamics-based curiosity and analyzes limitations of direct prediction-error based curiosity formulation. The study demonstrates how agents can reward themselves without actual progress in certain environments. In a 3D navigation task, an agent is incentivized with a reward based on the informativeness of transitions. The reward is calculated using a network to embed observations, a forward dynamics network, and prediction error. This encourages the agent to explore areas with high prediction error, leading to more complex dynamics. In a 3D navigation task, an agent is rewarded based on transition informativeness. The agent explores areas with high prediction error, indicating complex dynamics. Dynamics-based curiosity is effective in areas with less agent time or complex dynamics. Mean-squared error is used as surprisal in the curiosity formulation. A good feature space for dynamics-driven curiosity should be compact and filter out irrelevant aspects of the observation space. In this work, the qualities of a good feature space are proposed: compactness, sufficiency, and stability. The efficacy of various feature-learning methods is systematically investigated. The efficacy of feature-learning methods such as Pixels, Random Features (RF), and Variational Autoencoders (VAE) is investigated. Pixels are simple but may be tricky due to high-dimensional observation space. RF features are stable and can be compact, while VAEs fit latent variable generative models using variational inference. The method involves using variational inference to approximate the posterior distribution of latent variable z given observed data x. An inference network q(z|x) is used to model this posterior, generating mean and variance vectors for a Gaussian distribution. These vectors serve as features for an embedding network \u03c6, providing a low-dimensional summary of the observation. The features may contain noise and evolve during training. Inverse Dynamics Features (IDF) predict actions based on state transitions, using a neural network \u03c6 to embed states. The goal is to capture environment aspects relevant to the task. The features learned should correspond to aspects under the agent's control, but may not capture important aspects beyond immediate influence. Learned features are unstable and may require pre-training for stability. Random data collection may lead to biased results. The features learned by the agent may not capture all important aspects, leading to bias. To address nonstationarity, critical choices were made in the learning algorithm, focusing on reducing instability and ensuring consistency across environments. The PPO algorithm was chosen for its robustness and minimal hyper-parameter tuning requirements. Reward normalization was implemented to address the non-stationary nature of the reward function. Normalization techniques were applied to address non-stationarity in the reward function. This included scaling rewards, normalizing advantages, and observations, as well as increasing the number of parallel actors for stability. Increasing the number of parallel actors affects batch-size. Normalizing features is important for combining intrinsic and extrinsic rewards. Using batch-normalization in the feature embedding network helps ensure consistent scaling of intrinsic rewards. Removing the 'done' signal is crucial to prevent leaking information about the reward function. This can make some Atari games too simple if not addressed. The evaluation curves show that a purely curiosity-driven agent can gather rewards in Atari games without using extrinsic rewards. Curiosity models trained on pixels do not work well, while VAE features perform similarly or worse than random and inverse dynamics features. Inverse dynamics-trained features perform better than random features in 55% of Atari games. An interesting outcome of analyzing Atari games is that random features for modeling curiosity are a strong baseline, likely effective in half of the games. In the context of pure exploration, avoiding death is only necessary if it is \"boring\", as it just transitions the agent. Removing 'done' separates exploration gains from the death signal, with agents avoiding death to prevent returning to familiar areas. This subtlety has been overlooked in previous experiments without extrinsic rewards. In a large-scale analysis, a purely curiosity-driven agent explores 54 diverse environments without extrinsic rewards, including Atari games, Super Mario Bros., Roboschool scenarios, Two-player Pong, and Unity mazes. The goal is to investigate various questions using both policy and embedding networks working directly from pixels. The code and videos of the agent in action are available on the website. The large-scale analysis explores diverse environments without extrinsic rewards, including Atari games. The goal is to investigate questions about the behaviors of purely curiosity-driven agents and the effect of different feature-learning methods. Extrinsic rewards are only used for evaluation, not training. The analysis focuses on purely curiosity-driven agents in Atari games, where extrinsic rewards are only used for evaluation, not training. The agent learns to obtain external rewards without seeing any during training, as seen in the game 'Breakout' where exploring further leads to collecting points as a by-product. The agent in Atari games relies on curiosity to explore and avoid predictable death resets. Curiosity rewards can be sufficient without external rewards, suggesting game designers may guide agents through tasks using curiosity alone. However, a curious agent may perform worse than a random agent if the extrinsic reward is not correlated with exploration. In Atari games, the agent's exploration can be hindered when extrinsic rewards are not correlated with exploration. Different feature learning methods are compared, showing that encoding pixels into features performs better than training dynamics on raw pixels. Random features also perform well due to their stability. The study compared different feature learning methods in Atari games, finding that random features work well due to their stability. IDF features outperform random features for generalization tasks. VAE method was somewhat unstable, so RF and IDF were chosen for further experiments. Curious agents using IDF collected more game rewards than random agents in 75% of Atari games. In 75% of Atari games, an RF-curious agent outperforms a random agent in collecting game rewards. IDF features perform better than RF in 55% of the games. Super Mario Bros was studied using different feature-learning methods, with a purely curious agent showing significant improvement in passing game levels. Further details can be found in the appendix. Increasing the batch-size by using 1024 parallel environment threads in training a purely curious agent on Mario Bros results in better performance, allowing the agent to explore more of the game, discover secret rooms, and defeat bosses. The graph shows a significant improvement compared to training with 128 parallel environment threads. The large-scale experiment focused on training a purely curiosity-driven agent in a modified Pong environment with one paddle and two balls. The agent learned to intercept and strike the balls in a challenging continuous action space. The performance improved as the base RL algorithm (PPO) training progressed, with the policy and embedding networks trained on pixel observations. The environment was more difficult than typical games, but the agent adapted well. The experiment involved training a curiosity-driven agent in various environments, including a modified Pong game with one paddle and two balls. The agent learned to interact with the environment by striking the balls and exploring different actions. The Ant environment with controllable joints was also used, where a walking-like behavior emerged from curiosity-driven training. The agent's interaction with the environment was captured in result videos. In a two-player Pong game, both sides are controlled by curiosity-driven agents, leading to longer rallies over time without the need for external rewards. In a two-player Pong game, curiosity-driven agents play without a teacher, leading to long rallies that eventually crash the Atari emulator. The purely curious agent can efficiently explore and learn skills like game playing and walking behavior. Reward-free learning allows for generalization to novel environments, as shown by pre-training the agent in Mario Bros Level 1-1 and testing its ability to generalize to new levels. In Mario Bros, training on one level and fine-tuning on another shows strong transfer with curiosity-only reward. Transfer performance is weaker when transitioning from day to night levels due to color palette shift. IDF-learned features transfer in both cases, while random features only transfer in the first scenario. Overall, the results suggest that learned features generalize better to novel levels compared to random features. Curiosity-driven skills help agents explore efficiently in new environments without extrinsic rewards. However, dense rewards are more effective for specific tasks, although designing them can be challenging. In this section, the evaluation focuses on how curiosity can aid an agent in tasks with sparse or terminal rewards. The study involves 3D navigation in a maze with 9 rooms and a sparse terminal reward. Agents are compared based on training with extrinsic reward only or with both extrinsic and intrinsic rewards. Classic RL struggles in scenarios with only terminal rewards, making curiosity a valuable addition for task performance. In experiments comparing extrinsic (classic RL) versus extrinsic+intrinsic reward training, the latter typically led to better performance in Atari games with sparse rewards. Curiosity bonus improved performance in 4 out of 5 games. The study combined extrinsic and intrinsic rewards without tuning, leaving the optimal combination for future exploration. The text discusses combining extrinsic and intrinsic rewards for future research directions in intrinsic motivation. It mentions using prediction error, uncertainty, or improvement to drive agents to explore challenging environments. Little work has been done in the pure exploration setting without external rewards, with some related approaches using forward dynamics models of feature spaces. Intrinsic rewards can be generated using smoothed state visitation counts, with count-based methods showing strong results in combination with extrinsic rewards. The choice between count-based and dynamics-based approaches is still unclear, but the focus in this paper is on dynamics-based bonuses due to their scalability and parallelizability. Previous experiments did not yield success in scaling up existing count-based implementations for large-scale studies. In evolutionary computing, learning without extrinsic rewards has been extensively studied, known as 'novelty search'. Novelty is defined as the distance to the nearest neighbor among previous events. Solutions can be more interesting when not solely optimizing for fitness. Other exploration methods include utilizing uncertainty about value function estimates or perturbations of the policy. Previous work on intrinsic motivation approaches is reviewed in BID35 and BID24 BID23. Alternative exploration methods involve utilizing an adversarial game. Alternative methods of exploration include BID42 using an adversarial game between agents, BID8 optimizing empowerment, and BID5 focusing on diversity for learning skills without rewards. The surprising effectiveness of random features is highlighted, with literature on random projections and neural networks. Random features work well for simpler problems, but feature learning is better for complex problems. Preliminary evidence suggests that learned features may be beneficial for dynamics-based exploration. Our exploration has shown that agents trained with curiosity rewards can generalize well in novel levels of Mario Bros. They exhibit useful behaviors in various environments, such as playing Atari games without rewards, crossing multiple Mario levels, and displaying unique behaviors in different scenarios. However, not all environments align exploration with extrinsic rewards. The limitation of prediction error-based curiosity is also noted. The handling of stochastic dynamics in prediction error-based curiosity can lead to seeking out transitions with high entropy. An experiment was conducted to illustrate this issue by adding a noisy TV to the environment. The noisy-TV problem was empirically validated in a maze scenario. The presence of a noisy TV in the environment significantly slows down learning for the agents in the maze. Despite this, with enough time, agents can occasionally converge to consistently receiving the extrinsic reward. It is crucial for future work to address the issue of stochasticity efficiently. The study presents a scalable approach that can learn complex behaviors without the need for a reward function or end-of-episode signal. Random features perform well, but learned features also show promise. The study demonstrates that random features perform well, but learned features show better generalization. The goal is to leverage unlabeled environments to enhance task performance, with future work focusing on transfer from unlabeled to labeled environments. Training code and environments are available on their website for further exploration. The study explores the performance of curiosity-driven agents on 48 Atari games using Inverse Dynamics and Random features. Results on combining intrinsic and extrinsic rewards on sparse reward games are also included, showing that tuning the combination is crucial for optimal performance. In multiple environments, curiosity-driven agents outperform random agents, although there are cases where the agent's behavior seems random or minimizes returns. Results show that for most of the training process, Random Features (RF) perform better than random agents in 67% of environments, while Inverse Dynamics Features (IDF) perform better in 71% of environments. The mean rewards after 100 million frames for different games are compared in Table 2. The study compares the performance of agents trained with intrinsic plus extrinsic reward versus extrinsic reward only. The results show that combining extrinsic with intrinsic rewards can lead to more effective exploration and better ultimate scores. The focus is not on how to optimally combine the rewards, but rather on the benefits of larger batches for improved performance."
}