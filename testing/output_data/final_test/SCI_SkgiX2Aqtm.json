{
    "title": "SkgiX2Aqtm",
    "content": "We introduce a new class of likelihood-based auto encoders called Pseudo Invertible Encoders for information compression from high dimensional data. The Gaussian Pseudo Invertible Encoder outperforms WAE and VAE in generating sharp images on MNIST dataset. Emphasizing the importance of invertible compression, especially when it's difficult to determine what information is important. Compression of images for person ID in a small company requires less resolution than person ID at an airport. Invertible information compression allows for undoing compression if needed for future purposes. Recent advances in classification models show that deep learning architectures can achieve state-of-the-art performance without information loss. i-RevNet models implement modifications to achieve invertibility and performance similar to standard models, challenging the belief that information loss is necessary for good classification performance. Flow-based generating models like BID0, BID11, and BID6 use bijective mappings to transform input data distributions. Auto-Encoders, especially Variational Auto Encoders (VAE) and Wasserstein Auto Encoders (WAE), are effective in reducing data while retaining essential information for tasks. These methods emphasize stable training and good results in reconstruction and generation. The Pseudo Invertible Encoder combines bijective mappings with restriction and extension of mappings to dependent sub-manifolds. It introduces a new class of likelihood-based Auto-Encoders and demonstrates the properties of Gaussian Pseudo Invertible Encoder in manifold learning. Comparisons with WAE and VAE on MNIST show better image sharpness. RevNet proposes a Reversible Residual Network to address memory consumption bottlenecks in growing networks. The RevNet is a Reversible Residual Network that replaces residual blocks with coupling layers to reconstruct layer activations. It can accommodate non-invertible components for more efficient training. i-RevNet adds a downsampling operator to circumvent non-invertible modules. The model is only invertible up to the last layer and does not allow dimensionality reduction. Auto-Encoders were introduced as an unsupervised learning algorithm for dimensionality reduction by compressing input data. Wasserstein Auto Encoders use Optimal Transport for training, while our model does not require pixel-level optimization and only performs encoding at training time. The approach for obtaining dimensionality reduction invertible mappings is based on restricting mappings to low-dimensional manifolds and extending inverse mappings with constraints. This involves using a residual manifold to match dimensionalities of hidden and initial spaces, utilizing a pair of extended functions G and G^-1. The approach for obtaining invertible dimensionality reduction mappings involves finding the invertible transformation G with constraints expressed by R. By focusing on a parametric family of functions F invertible on R D, we select F \u03b8 with parameters \u03b8 satisfying certain constraints. The obtained function G is Pseudo Invertible Endocer (PIE), chosen as a maximizer of the log likelihood of observed data. The joint distribution for Dirac's delta function is derived considering constraint 3. A sequence of Gaussians converges to the delta function. The log likelihood is optimized using Gradient Descent with a Standard Gaussian prior distribution. To address limitations in real-value bijectives, a composition of basic bijectives is used for dimensionality reduction mapping. The log likelihood is expressed in terms of Jacobians of the PIE functions. The model, referred to as PIE, utilizes PIE blocks and a Normalizing Flow with multi-scale architecture. The architecture achieves semantic compression and consists of convolutional blocks followed by linear blocks. The convolutional PIE blocks include coupling layers and 1\u00d71 convolutions for invertible downsampling of the image. The PIE model utilizes convolutional and linear blocks with coupling layers and 1\u00d71 convolutions for invertible downsampling of the image. At the end of the convolutional PIE block, variables are split with one part projected to the residual manifold R. Linear PIE blocks are constructed similarly but without downsampling. Affine coupling layers are used to enhance model flexibility. The modified version of the model introduced in BID1 utilizes neural networks to scale and bias input data x, with invertibility not required for these functions. The Jacobian of coupling layers is used to calculate the log determinant, and affine couplings operate on non-intersecting parts of the tensor. Different channel permutation mechanisms are proposed to capture correlations between channels and features. Invertible 1\u00d71 convolutions are shown to outperform fixed permutations. In BID6, invertible 1\u00d71 convolutions outperform fixed permutations. Householder Matrices are used to parametrize invertible linear mappings. Inverse of the obtained orthogonal matrix is its transpose, simplifying computation. Invertible downsampling reduces spatial size and increases channel number. Checkerboard patterns are used for downsampling. The Split block in the discussed experiment reduces data dimensionality by splitting variables into two parts, z and r. z is processed further while r is constrained. A Gaussian PIE was trained on the MNIST dataset with convolutional and linear blocks. The experiment involved reducing data dimensionality in the last layer by 50% of the input size using three linear blocks in PIE. Affine transformations utilized Householder reflections, with optimization done using the Adam optimizer. The model decreased dimensions from 784 to 10, showcasing PIE's ability to learn with different constraints. Lower constraint values resulted in better reconstruction, while too low values produced fuzzy images. Sampling from a narrowed distribution improved image accuracy. The experiment demonstrated that tightening the constraint by decreasing the value increases the accuracy of reconstructed images. Lower constraint values improved image reconstruction, while too low values resulted in fuzzy images. Sampling from a narrowed distribution enhanced image accuracy. In an experiment comparing image sharpness, the Wasserstein distance function was utilized. The Laplace filter was used as an edge detector, with variance of activations computed over 10000 sampled images. Results showed that PIE outperformed VAE and WAE in generating sharp images, even surpassing the sharpness of original MNIST dataset images. This was attributed to the checkerboard pattern in the downsampling layer of the PIE convolutional block. The new class of Auto Encoders, Pseudo Invertible Encoder, utilizes a checkerboard pattern in the downsampling layer to capture intrinsic data properties and reconstruct sharper images. The model learns manifold structure and generates sharp images, bridging the gap between Auto Encoders and Normalizing Flows."
}