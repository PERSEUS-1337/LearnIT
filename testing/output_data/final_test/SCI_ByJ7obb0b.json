{
    "title": "ByJ7obb0b",
    "content": "In this paper, the authors demonstrate how feedforward deep networks have a low-rank derivative structure that allows for the use of second-order information without the need for approximations. They implement Cubic Regularization (CR) on a deep network with stochastic gradient descent, showing success in escaping plateau regions of the objective function. This approach requires less computational cost compared to traditional gradient descent methods. Gradient-based optimization methods use derivative information to determine search directions for minimizing an objective function. The steepest descent method converges slowly in ill-conditioned systems, while Newton's method, which uses second-order derivative information, converges quadratically but is computationally expensive and does not scale well. Gradient-based optimization methods like steepest descent and Newton's method have limitations in terms of scalability and computational cost. Various approaches, such as conjugate gradient and quasi-Newton methods, aim to combine the strengths of different methods while addressing their weaknesses. Deep learning presents unique challenges that can be addressed using gradient-based optimization techniques. Deep learning problems are large, nonconvex, and stochastic due to mini-batch sampling. Analytical derivative information can be calculated through back-propagation. Researchers have developed training methods to overcome these challenges. Training methods for deep neural networks aim to address challenges such as nonconvexity and stochasticity. Layer-wise pretraining and transfer learning help initialize weights effectively. Optimization algorithms like Adam and Adagrad adjust learning rates based on history. Nesterov momentum leverages history in stochastic gradient descent. These methods indirectly utilize second-order information but are limited by mini-batch stochasticity. Some researchers have used second-order information to improve training, often approximating the Hessian. Methods like L-BFGS estimate the Hessian but face challenges due to noise from sampling techniques. Subsampling and mini-batch reuse are strategies to address these challenges, updating the Hessian approximation less frequently and reusing minibatches to calculate gradient differences. In deep learning, second-order methods can be used to improve training by approximating the Hessian. These methods have a higher computational cost but can be made more efficient by using smaller mini-batches for Hessian calculations. The bias in Hessian approximations can affect the performance of quasi-Newton methods. Some approaches avoid Hessian approximations altogether and use finite differences instead. This paper demonstrates that the derivatives of feedforward deep learning networks exhibit a low-rank, outer product structure, allowing for manipulation of second-order derivative information. The low-rank structure of second-order derivatives in deep learning allows for efficient manipulation without approximation. Cubic Regularization (CR) is implemented to utilize Hessian information for calculating learning rates in training deep networks, improving the ability to exit plateau regions. Analytically calculating second-order derivatives is uncommon due to scalability constraints, but the low-rank structure enables this approach. In deep learning, the low-rank structure of second-order derivatives allows for efficient manipulation without approximation. This structure enables the exact calculation, storage, and use of second-order derivatives in an efficient manner by only dealing with the components needed to represent the full Hessians. This approach involves some extra calculations but has comparable storage costs to gradient calculations. The low-rank structure is illustrated for a feedforward network with arbitrary activation functions, showcasing the efficiency in dealing with derivatives. The low-rank structure of second-order derivatives in deep learning allows for efficient manipulation without approximation, enabling exact calculation, storage, and use of second-order derivatives in an efficient manner. This structure is illustrated for a feedforward network with arbitrary activation functions, showing efficiency in dealing with derivatives. The derivative formulae for feedforward networks exhibit a low-rank structure that is independent of the objective function or softmax usage, and is present in convolutional and recurrent layers as well. Index notation with the summation convention is extensively used in calculations for scalar, vector, and matrix representations. In differential geometry, index notation is used in deep networks with slight adaptations for better structure. The Kronecker delta is employed as the identity matrix in index notation. The summation convention can simplify expressions with Kronecker deltas. A generic feedforward network with ReLU activation functions and categorical cross-entropy is considered. The first derivatives for the deep network are calculated on a per-sample basis, preserving the generality of the expression. The low-rank structure of the network is shown to be independent of the objective function and the use of softmax. Any smooth function of p j can be used instead of softmax as long as Equation 13 holds. The quantity \u03b7 (n,k),j i needs to be stored for k = 1, 2, ..., n \u2212 1. The low-rank structure of the network is shown to be independent of the objective function and the use of softmax. The quantity \u03b7 (n,k),j i needs to be stored for k = 1, 2, ..., n \u2212 1 for second derivative calculations. The second-order objective function derivatives are then calculated using repeated use of certain formulas. Calculating second derivatives in neural networks involves evaluating the Hessian and storing values for derivative calculations. The number of matrices to store increases with the number of hidden layers, but it is manageable in practice. In neural networks, calculating second derivatives involves evaluating the Hessian. The derivatives retain a low-rank structure, making it possible to calculate and discard values without storing them. However, manipulating the entire Hessian may not be computationally feasible due to the rank increase with mini-batch sampling. The low-rank properties of the Hessian exist on a per-sample basis, providing computational savings when calculating scalar or vector quantities. Including bias terms would increase the gradient size but not change the overall low-rank structure. Calculating third derivatives may be unwieldy and memory-intensive. Cubic Regularization (CR) is a trust region method that uses a cubic model of the objective function at the j-th iteration. The cubic term allows for utilizing information in the Hessian without requiring convexity, but it involves computationally expensive operations like calculating the Hessian and solving for optimal values. CR is not commonly used in deep learning due to these computational challenges. In this paper, the approach is to use Cubic Regularization (CR) as a metamethod to calculate a learning rate for a given search direction in training algorithms. By fixing the search direction, the algorithm calculates a cubic function of the learning rate at each iteration, solving for the optimal value. This method aims to overcome the computational challenges of expensive operations in deep learning. The approach in this paper uses Cubic Regularization (CR) to calculate a learning rate for a search direction in training algorithms. When assuming g is a descent direction, \u03b1 is guaranteed to be real and positive. Two approaches for calculating \u03b1 involve sampling in mini-batch training, but the second approach is preferred due to less sampling noise. The paper introduces Cubic Regularization (CR) for calculating learning rates in training algorithms. Three options were considered for determining the step direction, with the third option being the most effective. Computational tests were conducted using deep feedforward networks on MNIST and CIFAR-10 datasets. The paper introduces Cubic Regularization (CR) for calculating learning rates in training algorithms, focusing on optimization iteration rather than wall clock time. Implementation details can be found in Appendix A, with an example of CR applied on top of SGD showing little to no benefit. The average learning rate with CR was around 0.05. The learning rate plot with a moving average of 100 (shown in green) suggests 0.02 was a good choice. The optimization process did not encounter plateaus, but CR could be beneficial in such cases. Weight initialization on a plateau showed the algorithm's ability to adjust learning rates. Applying Cubic Regularization (CR) to Stochastic Gradient Descent (SGD) on the CIFAR-10 Dataset resulted in a significant increase in learning rate on plateaus, enabling larger steps in flat spaces. This behavior was similar to results seen on MNIST, prompting consideration for other training algorithms like Adagrad BID4. When applied to other training algorithms like Adagrad BID4 and Adadelta on MNIST, Cubic Regularization (CR) did not show significant differences when algorithms performed well. However, on plateaus, CR increased the learning rate, helping algorithms exit plateaus faster. Adagrad and Adadelta already had adaptive learning rate behavior, so the increases from CR were smaller. With Adadelta, the learning rate dropped steadily as it exited plateaus but jumped again around iteration 1200. The CR technique, when applied to Adagrad and Adadelta, can supplement existing optimizers by helping algorithms exit plateaus faster. It is most useful when optimization is stuck on a plateau before convergence, allowing for determination of convergence or being stuck in a flat region. Applying CR may lead to advancements in calculating search direction and step length, but this would require a separate algorithm. In experiments, applying CR to Adagrad and Adadelta showed improvements in escaping plateaus. However, using CR with Adam did not provide gains as with other methods. Adam's adaptivity surpasses Adagrad and Adadelta, making it better at escaping plateaus. Overlaying an additional learning rate on Adam may interfere with its variable-specific learning rate. Analyzing each algorithm's update scheme with CR calculations could reveal interference and ways to improve both algorithms. Adaptation of CR to calculate layer-specific learning rates could be beneficial in future work. Calculating variable-specific and layer-specific learning rates with CR can address computational cost concerns. Variable-specific learning rates involve rescaling each variable's step by the corresponding diagonal entry in the Hessian, while layer-specific learning rates involve rescaling the step of each variable in a layer by a measure of the block diagonal component of the Hessian. The storage cost for second-order calculations is relatively inexpensive, but the number of operations increases. The number of matrix multiplications in Equation 9 scales quadratically with the number of layers. Na\u00efve matrix multiplication cost scales cubically with matrix size. CR requires specifying \u03c3, while SGD requires an initial learning rate and decay scheme. Optimization performance with CR is relatively implementation-dependent. In this paper, the authors demonstrate that feedforward networks exhibit a low-rank derivative structure, allowing for efficient representation of the Hessian. This structure enables the extraction of higher-order derivative information at a low computational cost. The choice of \u03c3 in CR has a stronger impact on the variability of the learning rate than on its magnitude, with little effect in very curved spaces and bounding step length in flat spaces. The authors demonstrate that feedforward networks have a low-rank derivative structure, allowing for efficient extraction of higher-order derivative information at low computational cost. The CR method uses second-order derivative information to calculate a learning rate, showing comparable optimization performance to SGD but with an adaptive learning rate that prevents stagnation on plateaus. CR requires less problem-specific knowledge and can be incorporated into existing methods, with potential for further work on exploiting the low-rank derivative structure. There is room for further work on exploiting the low-rank derivative structure to enable CR to calculate search directions and step sizes efficiently. Initialization near the origin improved objective function values, and networks were initialized with random weights within specified bounds. Mini-batch size was set to 32, and TensorFlow BID0 was used for implementation. Learning rates varied with network size, chosen to be large and reasonable for optimization. For optimization, learning rates were chosen to be large and reasonable but not optimal. An exponential decay with a rate of 0.95 per 100 iterations was used. The \u03c3 value and initial learning rate for each network were specified. The deep network definition included ReLU activation functions in hidden layers, a softmax at the output layer, and categorical cross-entropy as the objective function. The deep network's first derivatives are calculated without summation over j. Intermediate quantities are defined to simplify the derivation process. Second derivative calculations involve considering intermediate quantities. Convolutional and recurrent layers maintain the low-rank derivative structure of fully connected feedforward layers. Total derivatives are calculated via back-propagation in larger networks. A convolutional layer is defined. A convolutional layer can be defined as a structure with input xij, vertical stride \u03c3, horizontal stride \u03c4, activation function A, and output vst. Simplifying expressions with z makes it easier to see the low-rank structure in derivatives. The conditional form of expressions relates to w where t indicates the recursion loop. This structure is similar to hidden layers in feedforward networks. The convolutional layer structure is similar to the hidden layers of a feedforward network when weights are identical at each layer."
}