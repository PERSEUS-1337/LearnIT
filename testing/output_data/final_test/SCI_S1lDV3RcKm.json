{
    "title": "S1lDV3RcKm",
    "content": "Generative adversarial networks (GANs) are effective for modeling complex distributions and have shown impressive results on various tasks. A GAN-based framework for learning from incomplete high-dimensional data is proposed, which includes a complete data generator and a mask generator for missing data. An adversarially trained imputer is used to impute missing data. Experiments show the framework's effectiveness under the assumption of missing completely at random. GANs are powerful for learning complex distributions and are different from likelihood-based methods. GANs are implicit probabilistic models that represent a probability distribution through a generator trained adversarially with a discriminator. They have been successful in generating photorealistic images and are useful for tasks like image inpainting. However, training GANs requires access to a large amount of fully-observed data, which can be challenging in real-world applications with missing data. The generative process for incompletely observed data involves a complete data vector x and a binary mask m determining which entries in x are revealed. The observed and missing elements are denoted as x obs and x mis respectively. Unknown parameters for data distribution (\u03b8) and mask distribution (\u03c6) are estimated by maximizing the marginal likelihood. The missing data mechanism is characterized by independence relations between complete data x and masks m, categorized as MCAR, MAR, or NMAR. Most incomplete data work assumes MCAR or MAR for efficient estimation of parameters \u03b8 using variational lower bounds or the EM algorithm. The paper introduces a GAN-based framework for learning high-dimensional data distributions with incomplete observations. It utilizes an auxiliary GAN to model missingness with masks, filling missing entries with a constant value. The generator is trained to make the masked data indistinguishable from real incomplete data. This framework builds on the AmbientGAN concept to distinguish corrupted real samples from corrupted generated samples. The proposed framework utilizes an auxiliary GAN to model missingness in high-dimensional data distributions. It distinguishes corrupted real samples from corrupted generated samples under various corruption processes. The generator is trained to effectively learn complex data distributions from incomplete data, generating high-quality imputations. The proposed MisGAN framework utilizes a masking operator to fill in missing entries with a constant value, improving the efficiency of implementation. It distinguishes between corrupted real and generated samples to learn complex data distributions and generate high-quality imputations. The MisGAN framework uses element-wise multiplication and a mask generator to model missing data. It focuses on the missing completely at random (MCAR) case and utilizes two generator-discriminator pairs for masks and data. The framework follows the Wasserstein GAN formulation for loss functions. The proposed MisGAN framework utilizes Wasserstein GAN formulation for optimizing generators and discriminators to model missing data. It includes a mask generator and alternates between optimizing discriminators and generators. The coefficient \u03b1 is introduced to improve performance by encouraging generated masks to match real masks and complete samples to match real data distributions. The MisGAN framework utilizes a mask generator and a data discriminator to model missing data. The discriminator D x takes masked samples as input, allowing for the use of existing architectures without the need for customized modules. The masks are binary-valued, and the output of the mask generator G m is relaxed to [0, 1] n for gradient-based training. The discriminator D x in MisGAN is unaware of the masked data generation process. The MisGAN framework uses a discriminator that is unaware of the masked data generation process. The theoretical analysis and experiments support the effectiveness of the framework in recovering the complete data distribution, regardless of the filled-in value or the location of missing values. In a simplified scenario with n-dimensional data, the location of missing values can impact data distribution recovery. A left transition matrix is constructed based on filled-in values from a finite set P. The MisGAN framework effectively recovers complete data distribution regardless of missing value location or filled-in value. In the presence of missing data, MisGAN solves a linear system to estimate the unknown true data distribution. The framework recovers data distribution regardless of missing value location or filled-in value. The masking scheme in MisGAN solves a linear system to estimate the true data distribution, regardless of missing value location. The transition matrices have the same null space, implying uniqueness in solving the linear system. The choice of filled-in value does not affect the recoverability of the true data distribution. The masking scheme in MisGAN solves a linear system to estimate the true data distribution, with full rank T q,\u03c40. The non-negativity constraint leads to a unique solution when the data distribution is sparse, with specific conditions for uniqueness outlined. The sparsity assumption is common in scenarios like natural images on low-dimensional manifolds. High missing rates increase the likelihood of non-unique solutions due to a larger null space of T q,\u03c4. Bruckstein et al. (2008) provided a sufficient condition for sparsity in non-negative solutions to underdetermined systems. The non-negativity constraint in MisGAN's masking scheme ensures a unique solution for sparse data distributions. However, even considering the location of missing values does not make the missing data problem less ill-posed, as stated in Corollary 2. This corollary highlights that without a unique non-negative solution to the linear system, the true data distribution cannot be uniquely recovered. In practice, incorporating application-specific prior knowledge into the model can help regularize the missing data problem. Techniques like convolutional networks for natural images and decoder-based deep generative models like GANs with low dimensional latent codes can enforce sparsity constraints. The training objective of MisGAN for missing data is justified by Theorem 2, which discusses mask distribution and two induced distributions. Theorem 2 states that two distributions induce the same distribution for f \u03c4 (x, m) if they have the same marginals for all masks with p \u03c6 (m) > 0. Missing data imputation is addressed by equipping MisGAN with an imputer G i and discriminator D i, which jointly learn the data generating process and imputer objectives. The MisGAN model utilizes a complete data generator Gx with \u03b2 = 0.1 to match distributions of imputed and real data. The imputer G i can be trained independently with a pre-trained data generator Gx to target different missing distributions. The architecture for MisGAN imputation is shown in Figure 2. The MisGAN model utilizes an imputer G i that can be trained independently with a pre-trained data generator Gx. G i generates imputed results with the same dimensionality as its input, x m + \u03c9 m, using a deep neural network. The masking ensures that observed parts of x remain unchanged in the output. Noise injected to G i scales with the number of missing dimensions. Various properties of MisGAN on the MNIST dataset are assessed, showing how it behaves under different missing patterns and architectures. MisGAN is evaluated on three datasets: MNIST, CIFAR-10, and CelebA, with data rescaled to [0, 1]. Three types of missing data distributions are considered: Square observation where all pixels are missing except for certain parts. The missing data distribution types considered are square observation, dropout, and variable-size rectangular observation. Evaluation is done using the Fr\u00e9chet Inception Distance (FID) metric, with a LeNet model used for MNIST dataset instead of Inception network. The study evaluates MisGAN using the MNIST dataset, considering two architectures: convolutional networks and fully connected networks. The model uses the FID metric for evaluation, with different architectures like ConvMisGAN and fully connected MisGAN. FC-MisGAN and Conv-MisGAN are trained using an improved procedure for Wasserstein GAN with gradient penalty. MisGAN is compared to ConvAC, a baseline model capable of learning from incomplete data. ConvAC is similar to sum-product networks with a compositional structure like deep convolutional networks. Results show the generated data. Results show that Conv-MisGAN outperforms FC-MisGAN in generating visually better samples with missing data. Additionally, MisGAN generally outperforms ConvAC, which tends to produce samples with aliasing artifacts. In terms of FIDs, Conv-MisGAN and FC-MisGAN have similar performance under square observation, but differ under independent dropout. FC-MisGAN degrades significantly under independent dropout with high missing rates compared to Conv-MisGAN, leading to discontinuity artifacts in generated samples. The missing data problem is ill-posed due to less overlapping co-occurrence among pixels, making it challenging to determine pixel correlations across quadrants without additional assumptions. The importance of incorporating prior knowledge into the model is highlighted in Conv-MisGAN, which shows less severe discontinuity artifacts compared to FC-MisGAN. The mask discriminator in MisGAN is crucial for learning the correct distribution robustly, as demonstrated in failure scenarios with AmbientGAN. Rescaling pixel values does not prevent failures in AmbientGAN, while MisGAN avoids such issues. MisGAN avoids learning degenerate solutions by explicitly modeling the mask distribution, leading to successful imputation results with a variety of outputs. Training the imputer network together with the data generator without the mask generator/discriminator fails due to the ill-posed learning problem without agreement on the mask distribution. In this section, MisGAN is evaluated on three datasets: MNIST, CIFAR-10, and CelebA for missing data imputation. The performance is compared to baseline methods including zero/mean imputation, matrix factorization, and GAIN. GAIN is an imputation model that outperforms many state-of-the-art methods by completing missing data using an imputer network trained adversarially with a discriminator. MisGAN uses convolutional generators and discriminators for imputation, outperforming other methods in all cases, especially under high missing rates. GAIN training was found to be unstable for block missingness, with an optimal number of training epochs for effective imputation behavior. MisGAN, a highly flexible framework for learning GAN data generators in the presence of missing data, outperforms GAIN in stability across all scenarios. It can be extended to capture MAR and NMAR mechanisms, but further investigation is needed for learnability due to dependence between the transition matrix and data distribution. The modified architecture NMAR showed similar results to the original MisGAN, suggesting extra dependencies may not affect learnability. Formal evaluation of this framework is left for future work. Theorems 1 and 2 define sets of feature values and masks, with probability distributions on masks. Support of a distribution is defined as the set of masks with non-zero probabilities. The support of a distribution q in D M is denoted by S q \u2282 M. The set M \u03c4,v represents masks consistent with v, where q(m) > 0 and v m = \u03c4m. The vector T q,\u03c4 x determines the marginals {x ([v] for any \u03c4 \u2208 P, q \u2208 D M, and x \u2208 R I. The proof involves solving for x ([v] m ) in terms of T q,\u03c4 x for m \u2208 M \u03c4,v = \u2205 using induction on the size of M \u03c4,v. The conclusion holds if no v 0 exists with |M \u03c4,v0 | = k + 1. T q,\u03c4 x determines x([v 0 ] m ) for m = 0 to k. Theorem 1 follows from Proposition 1 and Proposition 2, showing MisGAN learns the distribution p(x obs , m). The distribution of f \u03c4 (x, m) under the optimally learned missingness q = p(m) is represented by T q,\u03c4 x. Theorem 2 restates Proposition 1 and Proposition 2, even when \u03c4 / \u2208 P. Corollary 2 is derived by adding a novel symbol \u03c8 / \u2208 P to the set of feature values. By choosing \u03c4 = \u03c8 for the masking operator, \u03c8 in a masked sample indicates a missing entry. The generative model for missing data involves solving the linear system T q,\u03c8 p x = T q,\u03c8 p * x, ensuring p x is non-negative and zero for all s \u2208 I \\ I. Theorem 1 states that if the solution to the original problem is not unique, the non-negative solution exists. Theorem 1 states that if the solution to the original problem FORMULA7 is not unique, then the non-negative solution to the augmented linear system with an extra constraint on I \\ I with \u03c4 = \u03c8 is also not unique. Root mean square error (RMSE) is a common metric for evaluating missing data imputation performance, but in complex systems, the conditional distribution p(x mis |x obs ) can be highly multimodal. The ground truth of missing values may not correspond to the global mode of p(x mis |x obs ), leading to large errors in metrics like RMSE. The FID is computed between completed data and fully-observed data as an evaluation metric to assess model imputation performance. Comparing FID and RMSE on MNIST shows inconsistencies in rankings at different missing rates. MisGAN outperforms GAIN and matrix factorization in FID despite having worse RMSE results. MisGAN produces the best completion results despite higher RMSE compared to other models. The architecture of Conv-MisGAN follows DCGAN with a 128-dimensional latent code. The imputer network for MisGAN is trained using U-Net implementation from CycleGAN and pix2pix. MisGAN is trained for 300 epochs on all datasets, with the imputer trained for 1000 epochs on MNIST and CIFAR-10. ConvAC and GAIN were trained for different numbers of epochs on various datasets. ConvAC was trained for 1000 epochs using the Adam optimizer with a learning rate of 10 for MNIST, while GAIN was trained for 1000 epochs for CIFAR-10 and 300 epochs for CelebA. FIDs were analyzed during training to select the best model for comparison with MisGAN in terms of imputation results. The GAIN model is trained for 1000 epochs on CIFAR-10 and 300 epochs on CelebA, adapted from the original code released by the authors."
}