{
    "title": "SkxwBgpmDE",
    "content": "Training labels can vary in quality, from trusted experts to weak sources like crowd-sourcing. This leads to a trade-off between quality and quantity in learning. \"Fidelity-weighted learning\" (FWL) is introduced as a way to train deep neural networks using weakly-labeled data by considering label quality. The success of deep neural networks relies heavily on labeled data, which can be limited. Weak annotators, such as crowd-sourced data or biased classifiers, can provide additional weak supervision. However, it is unclear how these weakly-labeled data can improve training for the desired task. Fidelity-Weighted Learning (FWL) is a Bayesian semi-supervised approach that leverages a small amount of data with true labels to generate a larger training set with confidence-weighted weakly-labeled samples. This method modulates the fine-tuning process based on the fidelity of each weak sample, addressing the issue of inaccuracies introduced by weak annotators. Fidelity-Weighted Learning (FWL) utilizes a small set of true labeled data to create a larger training set with confidence-weighted weakly-labeled samples. The method adjusts the fine-tuning process based on the fidelity of each weak sample, controlling the use of weak supervision based on sample proximity to observed data. The weak annotator provides weak labels for training samples, creating a dataset D w. A neural network student is pre-trained on this weak data, while a Bayesian function approximator teacher is fitted to observations from the true function. The student is then fine-tuned on labels generated by the teacher, considering confidence levels. The setup involves trainable and non-trainable components, with three phases in the training process. The student is trained on weak labels from a weak annotator to learn data representation and pretrain. The student function consists of two parts: one learns data representation and the other performs the prediction task. The teacher is trained on strong data represented in terms of the student's representation to generate a soft dataset with sample, predicted label, and confidence. A Gaussian process is used as the teacher to capture label uncertainty. The Gaussian process teacher captures label uncertainty using the student's representation. The GP is trained on samples from Ds to learn posterior mean and covariance. Soft dataset Dsw is created using the GP for input samples from Dw \u222a Ds with predicted labels and uncertainties. The Gaussian process teacher captures label uncertainty using the student's representation. Soft dataset Dsw is created using the GP for input samples from Dw \u222a Ds with predicted labels and uncertainties. The student network's weights are fine-tuned on the soft dataset, modulated by teacher-confidence. The student network is fine-tuned using samples from the soft dataset Dsw, where the uncertainty of each sample is mapped to a confidence value. This confidence value determines the step size for each iteration of stochastic gradient descent. The teacher's uncertainty is used to modulate the magnitude of parameter updates, with high confidence leading to larger step sizes for updating parameters. Data points where the teacher is not confident result in down-weighted training steps for the student, keeping the student function as trained on weak data in Step 1. The student can be trained using different methods: Full Supervision Only, Weak Supervision Only, Weak Supervision + Oversampled Strong Supervision, Weak Supervision + Fine Tuning, and Weakly Labeled Data + Teacher Labeled Data. These methods involve training on either strong or weak labeled data, with variations in oversampling, fine-tuning, and confidence considerations. The student network parameters are updated using a total learning rate \u03b7 t, which is a combination of the usual learning rate \u03b7 1 and a function \u03b7 2 based on label uncertainty. The function \u03b7 2 modulates the learning rate based on the quality of the current sample, with \u03b7 2 (x t) = exp[\u2212\u03b2\u03a3(x t)]. The proposed method involves using a function \u03b7 2 to adjust the learning rate based on the reliability of the data point's soft label. This is done by multiplying the loss of each example in a mini-batch by its fidelity score and averaging these fidelity-weighted losses to calculate the batch gradient. The hyper-parameter \u03b2 controls the influence of weak and strong data on the training process, allowing for a trade-off between bias and variance. In Appendix A, the method is applied to a one-dimensional toy problem for illustration. In this section, FWL is applied to a document ranking task using a state-of-the-art pairwise neural ranker architecture. The goal is to learn a function that maps each data sample to a scalar output value indicating the probability of one document being ranked higher than another with respect to a query. The size of publicly available datasets with query-document relevance judgments is limited, making this task challenging. The student uses a neural network architecture to learn embeddings for query/document pairs and predict ranking probabilities. The teacher is a clustered GP algorithm, and the weak annotator is BM25. The study introduces an unsupervised method for scoring query-document pairs based on matched terms statistics. Results show that FWL significantly boosts performance in document ranking tasks, with NN W outperforming NN S when trained on weak annotations. The study shows that NN W performs better than NN S in document ranking tasks. Alternating between strong and weak data during training brings some improvement, but fine-tuning NN W using labels generated by the teacher leads to the best results. Using FWL to include the estimated label quality from the teacher shows a significant performance boost. The study compares the performance of different weak annotators in the FWL setup on the Robust04 dataset. Four weak annotators, including BM25+RM3, were used to provide weak supervision for unlabelled data. The quality of the weak annotator significantly impacts the performance of FWL, as shown by their mean average precision (MAP) on the test data. Training neural networks using large amounts of weakly annotated data is an attractive approach when true labels are scarce. Fidelity-weighted learning (FWL) is a new student-teacher framework for semi-supervised learning with weakly labeled data. FWL improves training speed and outperforms other semi-supervised methods, as demonstrated in document ranking experiments. In FWL, a one-dimensional toy problem is used to illustrate the steps. The true function ft(x) = sin(x) is approximated by a weak annotator function fw(x) = 2sinc(x). Gaussian process regression is applied with a specific kernel to obtain a good estimate of ft(.) using both strong observations and weak annotations. During fine-tuning, a feed-forward network with 3 layers and 128 neurons per layer is used. Adam optimizer with an initial learning rate of 0.001 is employed. Two experiments are conducted: 1. Neural network trained on weak data and fine-tuned on strong data. 2. Teacher-student framework using FWL approach, which considers label confidence for better approximation of the true function. In FIG4, FWL improves approximation of true function by considering label confidence. Experiment repeated 10 times. Average RMSE on test points for Algorithm 1 Clustered Gaussian processes outlined. Training process detailed with K-means clustering and Gaussian processes for each cluster. Teacher-student framework used for evaluating soft labels and uncertainty. The teacher-student framework utilizes trained teacher Tc i to evaluate soft labels and uncertainty for samples from Dsw to compute \u03b72(xt) for step 3 of FWL. Multiple GPs {GPc i } are suggested to explore the data space effectively, as using a single GP led to poor performance. Sparse Gaussian Process in GPflow is scalable with inducing points and stochastic methods, optimizing a variational lower bound for large datasets. The clustered GP algorithm aims to make large dataset processing more efficient by optimizing the location of inducing points. The algorithm involves choosing the number of clusters based on the dataset size and utilizes a student model proposed in BID3. The student model consists of three components for learning data representations. The function \u03c8 is defined as a compositionality function that maps terms in a query and document to dense real value vectors. The weighting function assigns weights to each term in the vocabulary, simulating the effect of IDF. The compositionality function projects embedding-weighting pairs to a representation, improving ranking performance. The function improves ranking performance by simulating IDF effects using word2vec embeddings and IDF weighting. It includes a fully connected feed-forward network with hidden layers and a softmax for predictions. Cross entropy loss is employed, and a Gaussian Process is used as a teacher for training. The teacher model uses sparse variational GP regression with a specific kernel. The length scale and noise level are empirically determined for the Matern3/2 kernels. The constant value in the kernel accounts for noise in labels, distinct from weak label noise. The weak annotator used in the clustered GP algorithm for ranking tasks is BM25. Two standard TREC collections, Robust04 and ClueWeb, are used for ad-hoc retrieval tasks. Spam documents were filtered out using the Waterloo spam scorer. The text discusses the process of creating query sets for information retrieval tasks using human-labeled judgments and weak labels from AOL query logs. Pre-processing steps were applied to filter out irrelevant queries containing URL substrings. The text discusses the process of creating query sets for information retrieval tasks using human-labeled judgments and weak labels from AOL query logs. Pre-processing steps were applied to filter out irrelevant queries containing URL substrings. The dataset collected 6.15 million queries for Robust04 and 6.87 million queries for ClueWeb. The weakly labeled training set Dw was prepared by taking the top 1,000 retrieved documents using BM25 for each query from the training query set Q. The evaluation of the model was done through 3-fold cross-validation, with hyper-parameters tuned using batched GP bandits. The student model for experiments had hidden layers of sizes {64,128,256,512} and used ReLU as a non-linear activation function. The initial learning rate and dropout parameter were chosen from {10^-3, 10^-5} and {0.0, 0.2, 0.5} respectively. Embedding sizes of {300,500} were considered with a batch size of 128. The Adam optimizer was used for training with dropout as a regularization technique. During inference, the top 2,000 retrieved documents using BM25 were re-ranked using the trained models. The Indri 5 implementation of BM25 with default parameters was used."
}