{
    "title": "Hye5NQYU8r",
    "content": "The presence of place and grid-cells aid navigation in animals and humans, forming an internal representation of the external world. While deep neural networks show properties similar to place and concept cells, they lack grid-cell like firing patterns, limiting path integration and feature transformation. This suggests a limitation in current deep learning practices for analogical reasoning and memory retrieval tasks. In the rat brain, cells in the hippocampal region, specifically \"place cells,\" signal the animal's location in space, supporting the idea of the hippocampus operating like a cognitive map. The discovery of place cells and grid cells in animals aids navigation by forming an internal representation of the external world. Deep neural networks rely on learned feature spaces for relational reasoning and analogical problem-solving. The network navigates concept spaces to apply desired feature transformations for correct solutions. In this work, the focus is on investigating the firing properties of neurons in convolutional neural networks (CNN) during classification tasks. The ability of the network to solve analogical problems depends on grid cell-like properties among DNN neurons. The study aims to understand activation patterns in the final and pre-final layers of the CNN while it identifies hand-written images. After training a CNN to classify hand-written digits, the study investigated the activation properties of neurons in the network's concept space. The final layer neurons exhibited place-cell like activity, with a preference for one cluster. This behavior was expected given the training paradigm. The study extended the analysis to observe the firing pattern of the previous layer in a CNN trained to classify hand-written digits. The pre-final layer had 50 neurons, but none exhibited grid-cell like firing properties. Principal Component Analysis (PCA) revealed 11 PCs that explained the network's behavior. The trained CNN would struggle with analogical problems due to its simplicity compared to deeper models trained on more complex datasets."
}