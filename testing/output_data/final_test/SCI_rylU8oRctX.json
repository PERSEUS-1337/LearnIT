{
    "title": "rylU8oRctX",
    "content": "In this paper, three algorithmic approaches for dealing with limited data access are evaluated. One successful approach is using external data in classification tasks, while consistency regularization, specifically virtual adversarial training (VAT), has shown strong results in semi-supervised learning benchmarks. Consistency regularization aims to maintain output stability when input or network perturbations occur. Generative adversarial networks (GANs) are used to create additional data for improving the generalization capability of classification networks. The use of unlabeled data is explored for enhancing performance in GANs and Virtual Adversarial Training (VAT). Deep neural networks have shown great performance in tasks like speech or image recognition, but often require large datasets. Collecting data for real-world applications can be expensive and result in unbalanced or incorrectly labeled data. Recent approaches, such as few-shot learning, aim to achieve good performance with limited examples by learning a task-specific similarity measure. This approach has shown impressive results on datasets like Omniglot. A task-specific similarity measure is learned to embed inputs before comparison, with semi-supervised learning achieving strong results in image classification tasks. Generative models can create additional labeled or unlabeled samples to leverage information, with GAN-based frameworks performing best when generating poor quality images for better generalization. Another approach uses generative models to learn feature representations instead of generating data. The main focus is on consistency regularization in dealing with limited data for image and sound recognition tasks. Various approaches are compared in terms of their behavior with different amounts of labeled and unlabeled data. The aim is to improve accuracy by leveraging both types of data. Transfer learning is a useful method for dealing with limited data in image and sound recognition tasks. It involves transferring knowledge from a base model trained on a similar problem to a target model, with weights fine-tuned afterwards. While it requires a similar dataset for the base model training, it often yields better results than consistency regularization methods. The study found that transferring from ImageNet to CIFAR-10 yielded better results than consistency regularization methods. However, transferring to SVHN did not produce convincing results, limiting the generalization of the approach. The focus was on generative models, consistency regularization, and using external data during classification. Only one representative algorithm for each approach was compared, with the use of external data after training being a common technique in few-shot learning problems. The paper discusses using a convolutional siamese neural network for learning embeddings of inputs and computing similarities. It utilizes a support set for classifying test images into categories, with the network returning the category with the maximum similarity. Consistency regularization aims to increase the network's robustness against input perturbations by minimizing the distance between the original and perturbed inputs. This method can sample inputs from both labeled and unlabeled datasets. Regularization methods like VAT, inspired by adversarial training, involve adding perturbations to inputs and applying consistency regularization to minimize the distance between clean and perturbed predictions. By combining VAT with entropy minimization, performance can be further increased. Hyperparameters need to be tuned for each task to optimize the perturbation process. Generative models like GANs are used to increase model accuracy and robustness in a semi-unsupervised manner. The training method involves a generator network G producing realistic samples and a discriminator network D distinguishing between real and fake samples. The output of D consists of K + 1 categories, with one additional extra category. The classifier is trained on a specific number of categories, with an extra category for samples generated by D. The loss function includes supervised and unsupervised parts, with feature matching used to improve performance. Three experiments are conducted using the MNIST dataset. In this paper, three experiments are conducted using the UrbanSound8k dataset, which consists of 8732 sound clips representing different urban noise classes. The sound clips are preprocessed into log-scaled mel-spectrograms with 128 components. Training and evaluation are done using random 3-second snippets, resulting in an input size of 128 \u00d7 128. No external unlabeled data is used in the first experiment. In the experiments, the size of labeled data is varied to compare three methods. Another experiment explores how unlabeled data can compensate for labeled data. The last experiment considers class distribution mismatch. Generative models and consistency regularization allow the use of external unlabeled data. All methods are compared to a standard model with specific architectures for different datasets. The training utilized ReLU nonlinearities, Adam optimizer, batch normalization, dropout, and max-pooling. L2 regularization was employed for generalization. Models had similar computational power with convolutional and fully connected layers. Hyperparameters were manually tuned through gridsearch. Test accuracy was calculated using a separate test dataset. The test accuracy was calculated using separate test datasets containing 500 samples per category for MNIST and 200 samples per category for UrbanSound8k. Experiments were conducted in PyTorch framework with varied amounts of labeled data and no use of unlabeled external data. Training approaches included baseline, VAT, GAN, and siamese neural network, each repeated eight times. Mean accuracies and standard deviations were calculated. Results for MNIST dataset varied labeled data per category on a logarithmic scale between 0 and 200 with 31 steps. Using 200 labeled samples per category, the baseline network achieves 95% accuracy. With just one labeled sample per class, the baseline network reaches around 35%, a significant improvement over random guessing. Siamese neural networks show even better performance in the low data regime, reaching around 45% accuracy with just one labeled sample. However, the advantage of using siamese networks diminishes when more than 10 labeled examples per class are used. VAT outperforms GAN for up to 20 labeled samples per category. VAT has a higher benefit compared to GAN for up to 20 labeled samples per category, showing little improvement for higher numbers of labeled samples. Results are similar on the UrbanSound8k dataset. The siamese network improves with one labeled sample but performs worse than the baseline with more data. VAT and GAN show accuracy benefits for higher amounts of labeled data but do not further improve accuracy for more than 100 samples, with a decline in accuracy for 200 labeled samples. Adversarial training can decrease accuracy compared to baseline for 200 labeled samples, showing a trade-off between accuracy and robustness. VAT achieves better results with less unlabeled data than GAN for low labeled data, while GANs perform better with a moderate amount of labeled examples and many unlabeled examples. The results for the UrbanSound8k dataset show similar trends to the MNIST dataset, with high benefits seen when labeled data is low and unlabeled data is high. VAT requires similar amounts of labeled and unlabeled data for improvement, achieving better results with less unlabeled data when labeled data is limited. In this experiment, the investigation compares the results for VAT and GAN, focusing on mismatched samples in datasets MNIST and UrbanSound8k. The aim is to train a neural network with six outputs to classify categories [0, 6], while mismatched examples belong to categories [7, 9]. The accuracy improvement is dependent on the amount of unlabeled samples, with a fixed number of five labeled examples per category. The total number of unlabeled examples varies between 30, 120, and 600, showing a strong influence on the results. The experiment involved varying the number of unlabeled examples between 30, 120, and 600, with mismatches ranging from 0-100%. The distribution of examples across categories was kept equal, with neural networks trained and their accuracies calculated. Results showed higher accuracy with six classes compared to previous experiments. The experiment on the MNIST dataset showed that increasing the amount of unlabeled data improved accuracy, with GAN and VAT methods performing similarly. However, VAT performed worse than baseline with high class mismatch, while GAN showed a linear correlation between accuracy and class mismatch. Overall, increasing unlabeled data improved robustness against class mismatch. Adding unlabeled data can improve accuracy and robustness against class mismatch, with VAT performing better at low mismatch levels. There is little correlation between class mismatch and accuracy overall, except for GAN with 30 or 120 unlabeled samples. Surprisingly, adding samples not belonging to target classes can enhance overall accuracy, which is beneficial for training on rare or hard-to-obtain samples. However, the impact on performance needs to be evaluated. In this paper, three methods for dealing with little data have been compared. Siamese neural networks are recommended when labeled data is scarce. When additional unlabeled data is available, VAT outperforms GAN for low data amounts, while GAN is preferred for moderate or high amounts. Testing is necessary for individual cases as results may vary. Surprising results were obtained in the class mismatch experiment, showing that adding non-target class samples can improve accuracy. In the class mismatch experiment, adding samples not belonging to target classes may not necessarily reduce accuracy. The relationship between these samples/classes and the target ones heavily influences accuracy. It is questioned if datasets performing well in transfer learning tasks are suitable for semi-supervised learning. Combining methods like VAT in GAN or GAN with siamese neural networks could yield interesting results."
}