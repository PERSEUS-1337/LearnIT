{
    "title": "SJgdnAVKDH",
    "content": "Self-training is a semi-supervised method that augments labeled data with model predictions on unlabeled data. It has been studied in classification tasks but its application in sequence generation, like machine translation, is not well understood. This study shows that self-training can be effective in sequence generation by adding noise to hidden states as a regularizer. Injecting noise into the input space further enhances this mechanism, resulting in a \"noisy\" version of self-training. Empirical study on utilizing unlabeled data to improve performance in deep neural networks across machine translation and text summarization tasks. Noisy self-training effectively enhances baseline performance by injecting noise into the input space. Semi-supervised learning methods aim to leverage both labeled and unlabeled data for better results. Back-translation is an example of a semi-supervised approach that utilizes monolingual data to enhance machine translation. In this work, the authors revisit the self-training method, where a base model labels unannotated data to augment the training set. A \"student\" model is then trained with this new set to improve performance. This method, originally designed for classification problems, may be effective in natural language processing tasks when a good fraction of predictions on unlabeled samples are correct. Self-training has been applied successfully in natural language processing tasks such as word sense disambiguation and parsing. However, its effectiveness in language generation applications, like machine translation, remains unclear due to the discrepancy between hypotheses and ground-truth targets. Some gains have been reported, but it is still unknown what makes self-training work in these cases. In this paper, the authors aim to investigate the effectiveness of self-training in sequence generation tasks like machine translation and text summarization. They evaluate self-training on a small-scale machine translation task and observe significant performance gains over the supervised baseline. Additionally, they conduct an ablation analysis to understand the key factors contributing to its success, finding that the decoding method for generating pseudo targets plays a role in the improvement. The authors found that perturbing hidden states and injecting noise into the input are crucial for preventing self-training from getting stuck in local optima. This additional noise helps propagate labels and correct incorrect predictions, leading to significant gains in machine translation and text summarization tasks. Classic self-training involves starting from a base model trained with labeled data and iteratively incorporating predictions on unlabeled data to update the model using a subset of pseudo parallel data. Key factors include selecting the subset based on confidence scores and combining real and pseudo data either jointly or separately. In self-training, the model is trained on pseudo parallel data first and then fine-tuned on real data. Separate training with the whole pseudo parallel dataset produces better performance for neural sequence generation. The unsupervised loss from unlabeled instances is defined using empirical data distribution and conditional distribution by the model. Self-training in the context of neural sequence generation involves training on pseudo parallel data before fine-tuning on real data. It connects to entropy regularization and aims for low-density separation between classes. A machine translation task on WMT 2014 is used to analyze its effectiveness, with ablation analysis to understand performance factors. The study analyzes the effectiveness of self-training in neural sequence generation using a machine translation task on the WMT 2014 dataset. They work with a subset called WMT100K, train with the Base Transformer architecture, and use the fairseq toolkit for experiments and implementation. The study evaluates self-training in neural sequence generation for machine translation using the fairseq toolkit. Results are reported in terms of case-sensitive tokenized BLEU scores. Green bars in Figure 1 represent the application of self-training for three iterations, including pseudo-training (PT) and fine-tuning (FT) steps. Surprisingly, even training on its own predictions in the first iteration improves BLEU, with further enhancement through fine-tuning. Test BLEU continues to improve over the first three iterations until convergence. The test BLEU improves over the first three iterations, outperforming the initial baseline by 3 points. The behavior is unexpected as no new information is added during the process. The addition of pseudo-parallel data may change the training trajectory towards a better local optimum. Continuing training from the baseline model yields a 1.9 BLEU point improvement, comparable to initializing from random. This contradicts the assumption that parameter \u03b8 should not significantly change, prompting further investigation into the reasons for this unexpected performance boost. The study explores the impact of decoding strategy on performance gains in sequence generation tasks. Results show that using beam search for decoding unlabeled data contributes to performance improvements, but the full explanation remains unclear despite a gain of 1.4 BLEU points. The study investigates the impact of decoding strategy on performance gains in sequence generation tasks. Despite a gain of 1.4 BLEU points over the baseline, the hypothesis does not fully explain the observed results. Removing dropout during pseudo training leads to a drop in performance during beam search decoding, indicating the importance of dropout in improving results. The study explores the impact of decoding strategy on performance gains in sequence generation tasks. Table 2 shows that beam-search decoding contributes partially to performance gains, while dropout accounts for most of it. The investigation delves into why dropout, meant to avoid overfitting, brings advantages over the baseline model. One hypothesis is that noise enforces local smoothness, mapping semantically similar inputs to similar targets. The study examines the impact of decoding strategy on performance in sequence generation tasks. Self-training is explored as a regularization method, smoothing the data space with additional monolingual data. A toy task of summing two integers is used to verify the hypothesis. 10000 possible data points are randomly sampled for training, validation, and testing. In a study on sequence generation tasks, self-training is used as a regularization method to smooth the data space with additional monolingual data. A toy task involving summing two integers is employed to test this hypothesis. The study involves 10000 data points for training, validation, and testing, with specific details provided in Appendix A.1. The impact of self-training on smoothness and test errors is compared between baseline and self-training pseudo-training methods. In a study on sequence generation tasks, self-training is used as a regularization method to smooth the data space with additional monolingual data. One way to decrease smoothness is by increasing the dropout probability in the pseudo-training step, but a large dropout makes the model too unstable and slow at converging. Therefore, a simple model-agnostic perturbation process called noisy self-training (noisy ST) is considered. This involves perturbing the input during the pseudo-training step to modify the model. In noisy self-training (noisy ST), the input is perturbed by shuffling two integers to improve model learning of the commutative law. The perturbation enhances smoothness and symmetry of the output space, reducing test errors. The shuffling perturbation significantly improves smoothness metrics and symmetry of predictions, leading to reduced errors. The smoothing effect in noisy self-training can lead to a \"self-correcting\" behavior, improving model performance. However, it may also worsen predictions in some cases. Fine-tuning benefits from smoothing and consistently enhances baseline performance across datasets. Two perturbation functions are applied: synthetic noise and paraphrase, with noise levels studied for influence. The study compares the performance of Noisy ST (NST) with supervised and normal ST, showing NST outperforms by 6 BLEU points. Synthetic noise is preferred over paraphrasing due to simplicity. Ablation analysis of noisy ST without dropout improves baseline by 2.3 BLEU points, and combining with dropout adds another 1.4 BLEU improvement. The study examines the effectiveness of noisy self-training in various sequence generation tasks and resource settings. Experiments on machine translation and text summarization datasets show promising results under both high-resource and low-resource conditions. The model is trained from scratch in each iteration for optimal performance. Back-translation is also used as a reference point in some scenarios. In machine translation, back-translation is compared to self-training methods like noisy self-training, which leverages target monolingual data. The effectiveness of back-translation varies based on the availability of in-domain target monolingual data. The study implements back-translation by translating target data back to the source and training real and pseudo parallel data jointly. Testing is done on high-resource (WMT14 English-German) and low-resource (FloRes English-Nepali) translation benchmarks. For the low-resource machine translation dataset FloRes English-Nepali, noisy self-training was evaluated with 560K training pairs and a weak supervised system. They used the Big Transformer architecture and sampled 5M English sentences for noisy self-training. The results are presented in Table 4. Noisy self-training outperforms baselines by a large margin in both datasets, even improving weak baselines. Domain mismatch effect is demonstrated with English monolingual data being more effective for English-origin sentences. Noisy self-training beats back-translation on WMT100K and English-origin test set of FloRes. Noisy self-training outperforms baselines on Gigaword summarization dataset with 3.8M training sentences. Different settings are evaluated, including using 100K or 640K training examples and mining in-domain monolingual data. ROUGE scores are reported in Table 5, showing consistent improvement with noisy self-training. Noisy self-training consistently outperforms baselines in all settings on the Gigaword summarization dataset. It approaches the performance of state-of-the-art systems with much larger pretraining datasets. The study focuses on the WMT English-German dataset to analyze the impact of parallel dataset size, monolingual dataset size, and noise level on noisy self-training results. The study analyzes the impact of data size on noisy self-training results using a small LSTM model, Base Transformer, and Big Transformer. Noisy self-training consistently improves performance, with larger gains seen for intermediate parallel dataset sizes. Monolingual data sizes ranging from 100K to 3.8M samples are also explored, showing performance improvements. Results on WMT100K data show that noisy self-training outperforms self-training, with performance improving as monolingual data size increases. However, larger noise levels may not always be better, as they can eventually destroy input information. The study used 100K parallel data and 3.8M monolingual data, varying word blanking probabilities in synthetic noise. In this section, the study explores the impact of noise levels on performance, showing sensitivity to noise levels and the effectiveness of intermediate values. The process involves training the model on noisy source data first, then finetuning on clean parallel data. Two variations in the \"pseudo-training\" step are discussed: training with fake target predicted by the model or training with real target paired with noisy source. The study compares the performance of training with fake target and real target paired with noisy source data. Results show that using fake target predicted by the model is crucial for successful noisy self-training when parallel data size is small. The study discusses the challenges of noisy self-training with small parallel data size. Using a fake target predicted by the model is essential for successful training, as the real target paired with noisy source data can make learning even harder. This issue is more pronounced when the parallel data size is small, making it difficult for the model to fit the real target even with clean source data. Co-training methods like democratic co-training and tri-training involve training multiple models on the same data feature set, with some models acting as teachers for others. Recent approaches involve perturbing the input or feature space for data augmentation. These techniques have shown success in classification tasks but their impact on language generation tasks is not well understood. Recent work explores self-training for neural sequence generation, highlighting its effectiveness in improving generalization, especially with limited labeled data. The injection of noise during self-training is crucial for success, leading to the development of a new approach called noisy self-training. Experiments in machine translation and text summarization show the effectiveness of this method in various resource settings. The experiments optimize with Adam using specific parameters and are based on fairseq. Different datasets and batch sizes are used, with validation done on the validation set. Self-training and noisy self-training involve specific steps and dataset preprocessing. The model architecture for the toy sum dataset is a single-layer LSTM with word embedding size 32, hidden state size 32, and dropout rate 0.3. The WMT10K baseline model uses a single layer LSTM with word embeddings size 256, hidden state size 256, and dropout rate 0.3. Adam optimizer with learning rate 0.0005 is used for training on the WMT100K dataset. Training includes 30K update steps for the baseline model and (300K pseudo-training + 100K fine-tuning) update steps for self-training. Validation curves are shown to confirm model convergence. The validation curve of the baseline model shows overfitting, with the model checkpoint selected at the lowest point. Different learning rates (0.0002, 0.0005, 0.001) resulted in BLEU scores of 15.0, 15.6, and 15.5 respectively. Self-training methods like joint training and noisy self-training are compared on the WMT100K dataset. The filtering process improves joint training but still lags behind separate-training methods by over 1.5 BLEU points. Using all data in separate training produces comparable results. The model at pseudo-training step smooths the space and fine-tuning step greatly reduces errors in each iteration. The fine-tuning step greatly reduces errors in each iteration. The numbers from 0 to 96 are listed in increments of 2."
}