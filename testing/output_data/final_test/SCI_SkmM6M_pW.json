{
    "title": "SkmM6M_pW",
    "content": "The first deep neural network architecture for Egocentric Spatial Memory (ESM) models navigation in a spatially extended environment. It updates global maps based on local observations using a recurrent neural network and external memory for encoding latent representations of visited places. This enhances mapping accuracy for embodied agents navigating to goal locations. The Egocentric Spatial Memory (ESM) network is crucial for embodied agents to navigate to goal locations. Experiments show its effectiveness in 3D mazes compared to other algorithms. The functionality of navigation cells in the brain is also hypothesized. The model highlights the importance of individual modules and efficient communication among them, aiming to advance research in collaboration and communication. Egocentric spatial memory (ESM) is essential for constructing spatial maps and reaching goal locations in navigation tasks. Neurophysiological studies have identified specialized cells in mammalian brains for processing spatial information, including head-direction cells (HDC), border and boundary vector cells (BVC), place cells (PC), and grid cells (GC). These cells have specific functionalities related to orientation and firing behavior. The Egocentric Spatial Memory Network (ESMN) is a computational architecture inspired by neurophysiological discoveries, aimed at endowing artificial agents with memory capabilities for navigating complex environments. It integrates specialized cells like head-direction cells (HDC), border and boundary vector cells (BVC), place cells (PC), and grid cells (GC) to enable mammals to reach goal locations. The Egocentric Spatial Memory Network (ESMN) integrates different navigation cells into a deep neural network for accurate 2D global map construction from egocentric views. It encapsulates four cell types with neural network modules in one architecture, allowing the agent to take egomotion actions and fuse observations over time to create local maps. ESMN aligns spatial information by estimating egomotion and transforming past maps using a spatial transformer network. It also includes a spatial memory for integrating local maps into global maps. Our novel deep learning architecture, the Egocentric Spatial Memory Network (ESMN), integrates local maps into global maps and stores discriminative representations of visited places. The loop closure component compares current observations with external memory to correct the global map. This neuroscience-inspired AI model bridges computer science and computational neuroscience, outperforming competitive baselines and state-of-the-art visual SLAMs. The ESMN's exceptional map construction performance advances robotics and computer science, with potential applications in path planning for robots. In our work, we propose a neuroplausible navigation system called ESMN with four integrated cell types for potential engineering applications like robot path planning. We evaluate ESMN in 3D maze environments, demonstrating skills in positional inference, free space prediction, loop closure classification, and map correction. We analyze each module and their functional mappings with the four cell types, as well as conduct ablation studies and compare with state-of-the-art SLAM methods. In our work, we propose a neuroplausible navigation system called ESMN with four integrated cell types for potential engineering applications like robot path planning. We conduct ablation studies, compare with state-of-the-art SLAM algorithms, and show the efficacy of our integrated framework on unifying the four modules. There is a rich literature on computational models of egocentric spatial memory (ESM) primarily in cognitive science and AI, focusing on related works in machine learning and robotics. Reward-based learning is frequently observed in spatial memory experiments, with reinforcement learning commonly used to formalize it. Deep Q-networks have been employed in navigation tasks to maximize rewards while LSTM is used to represent spatial memory. In contrast to previous works on spatial memory and navigation, our ESM network considers action uncertainty and predicts the agent's pose based on egocentric views. We also introduce loop closure and map correction for long-term exploration, addressing challenges in reinforcement learning and SLAM algorithms. The ESM model proposes a unified deep neural network for visual SLAM, addressing the limitations of SLAM using monocular cameras. It aims to improve positional inference, intrinsic representation of places, and space geometry during 2D map construction, inspired by neurophysiological findings. The Egocentric Spatial Memory Network (ESMN) addresses the problem of modeling object-to-self representation in a global map using an RGB camera. The agent predicts egomotion and takes macro-actions in a 2D grid world without other sensors. The Egocentric Spatial Memory Network (ESMN) learns a global map in egocentric coordinates based on visual input pairs. The global map is a 2D probabilistic grid map representing the agent's belief of free space in artificial 3D maze environments. The ESMN architecture is proposed to tackle this problem, illustrated in FIG0 and detailed in Figure 2. The Egocentric Spatial Memory Network (ESMN) consists of four modules: Head Direction Unit (HDU), Boundary Vector Unit (BVU), Place Unit (PU), and Grid Unit (GU). PU encodes visited places, GU integrates local maps from BVU, and detects loop closure to eliminate errors. The framework systematically combines different navigation modules inspired by biological advantages. The ESMN model includes modules like HDU, BVU, PU, and GU, inspired by biological systems. The algorithm uses deep networks to extract features for recognizing visual scenes, improving map construction and correction. The agent relies on RGB camera images for spatial reasoning and takes actions to explore and predict poses. The HDU module predicts the agent's actions based on input from two camera views. The ESMN model utilizes modules like HDU, BVU, PU, and GU to improve spatial reasoning and map construction. The HDU module predicts actions based on input from two camera views, with a focus on reducing action space dimensionality through classification. The ESMN integrates egocentric views into a 2D representation using a recurrent neural network, similar to BID10. BVU serves as a local mapper for short-term free space representation maintenance. Future exploration could involve using recurrent neural networks for positional inference. The BVU module in the ESMN model updates the local space representation for short-term free space maintenance by encoding geometric information and transforming it into an egocentric top-down view using a 2D-CNN and de-convolutional layers. It utilizes an update rule with a spatial transformer network to estimate egomotion and transform the previous space representation to the current egocentric coordinate. The BVU module in the ESMN model updates local space representation for short-term free space maintenance by encoding geometric information and transforming it into an egocentric top-down view. The sampling kernel performs bilinear interpolation to transform the previous space representation into the current egocentric coordinate. Loop closure is essential for efficient exploration and spatial reasoning in navigation tasks, with ESMN detecting loop closure by encoding discriminative representations of specific places and creating training targets based on past observations. The PU module in the ESMN model creates training targets for loop closure detection by comparing current and past observations using a triplet loss function. The loop closure label is determined based on the mean squared error between embeddings, with additional criteria based on proximity to previously visited locations. The implementation of a binary mask in the egocentric global map helps in detecting loop closures accurately by comparing positions far from the most recent visited locations. This reduces false alarms and improves search speed. Additionally, a memory system is integrated into the local mapping framework for long-term storage of location representations. The addressable memory in the memory system is indexed by spatial coordinates, with memory vectors at each location. The reading and writing heads are fixed in the center of the memory, and past spatial information is transformed based on egomotion. The reading vector is formulated mathematically, covering a memory patch of specified width and height. The writing mechanism is simplified for GU, using a specific equation. The maze layouts vary in geometries, textures, and lighting conditions, with different levels of lighting stimulation. The maze layouts vary in geometries, textures, and lighting conditions. Each maze is stimulated with normal, weak, and strong lighting conditions. Maze 5_S and 5_W refer to Maze 5 with strong and weak lighting conditions respectively. Maze 1 is adopted from Willow Garage (2011) and Maze 8 is inspired by a radial arm maze used for spatial memory testing in rats. Digits are pasted on walls for loop closure classification tasks. Multiple writing and reading heads are used to achieve various objectives related to memory and global map construction in egocentric coordinates. The global map correction process involves using memory vectors for loop closure classification and merging places together to eliminate discrepancies. To preserve topological structure and connectivity, a stack of 3D convolution and de-convolution layers is used with inputs from local maps predicted at anchor and recalled places. The training process for ESMN involves perturbing ground truth egomotions, generating synthetic global maps, and minimizing regression loss between predicted and ground truth maps. ESMN is trained end-to-end using stochastic gradient descent with specific parameters and Adam Optimizer BID18. Ground truths provided include local map, egomotion, and loop closure classification label. Modules are first trained separately for faster convergence before fine-tuning in ESMN. The input frame size is 3 \u00d7 64 \u00d7 64, normalized RGB images, batch size of 10, and a discriminative representation of dimension 128. The size of the local space representation is 32 \u00d7 32, covering an area of 7.68 meters \u00d7 7.68 meters in the physical world. The global map is 500 \u00d7 500 in size. Parameters \u03bb and \u03b1 are set to 0.5 and 0.01 respectively. Experiments are conducted on eight 3D mazes in Gazebo for robotic simulation BID19. The mazes have complex geometry and various textures. Different illumination conditions are used for training: strong, normal, and weak. ESMN is trained to prevent over-fitting in a single-modality environment. Our ESMN demonstrates consistent performance across different lighting conditions in the test set, showcasing its ability to generalize in various environments. For loop closure classification tasks, unique features are created on walls to represent specific pathways. The agent navigates through mazes with obstacle avoidance, using ground truths obtained from a virtual 2D laser scanner. Training and validation data are collected from maze 1 to 5, while testing data are from maze 6 to 8. Egomotions are divided into rotation and translation classes for simulation evaluation and individual module training. The integrated framework is trained by evaluating experiments and individual modules separately using fully supervised signals. The Head Direction Unit (HDU) is decoded to predict head direction in world coordinates by accumulating estimated egomotions. Results show that the predicted head directions closely match the ground truth poses. Egomotion classification errors are computed from 2 classes. The egomotion classification errors are computed from 2 classes of rotation and 4 classes of translation under different illumination conditions. There is a slight degradation in rotation + translation compared to rotation alone, possibly due to the observable change in egocentric camera views. Quantitative analysis of predicted local maps using MSE, Correlation, and Mutual Information shows a decrease in MSE and an increase in correlation and MI as the agent explores the environment, validating the effectiveness of ESMN. The ESMN accurately estimates proximity to physical obstacles and accumulates beliefs of free space representations based on egomotions. A novel memory unit (GU) stores discriminative representations of visited places in egocentric coordinates, interacting with a Place Unit (PU) for loop closure classification. Grid cells (GC) are identified as principal inputs to place cells (PC) in the hippocampus. GU constantly reads and writes memory vectors in the proposed ESMN. The proposed ESMN architecture includes a Grid Unit (GU) that interacts with a Place Unit (PU) for loop closure classification. An ablation study on global map performance in Maze 6 shows the effectiveness of the model. ESMN accurately detects loop closure and performs map correction after discrepancies are eliminated. The ESMN architecture includes a Grid Unit (GU) and a Place Unit (PU) for loop closure classification. Map correction is performed to eliminate discrepancies in long-term mapping, improving the accuracy of predicted global maps. An ablation analysis studies the necessity of egomotion estimation from HDU using a feed-forward 3D-CNN. Sustainable mapping over long durations is achieved by using Long Short Term Memory in the baseline model. The study introduces a new architecture, LSTM Direct, which predicts global maps using camera views as inputs. An ablated model without PU and GU (HDU + BVU) is compared with the integrated architecture (HDU + BVU + PU + GU), showing superior results in terms of MSE, correlation, and MI metrics. The significant improvement suggests the importance of estimating loop closure and map correction for accurate mapping. The study highlights the importance of estimating egomotion for better map construction and integrating local maps for more flexible and efficient computation. The performance of the LSTM Direct baseline drops significantly when constructing global maps for longer durations, confirming the advantages of using GU for long-lasting memory. The ablated model without PU and GU is compared with the integrated architecture, showing superior results in terms of MSE, correlation, and MI metrics. The significance of loop closure and map correction for accurate mapping is emphasized. Our proposed architecture with all four modules enabled at t = 448 and t = 1580 validates the necessity of these steps to eliminate errors during long-term mapping. Comparing with state-of-the-art monocular visual SLAM methods, our ESMN significantly outperforms in terms of MSE, Cor, and MI. ORBslam shows high false positives in loop closure detection, possibly due to limitations of local feature descriptors, unlike our approach. Our ESMN can robustly detect loop closure and correct maps over time, addressing limitations of local feature descriptors. SLAMs often struggle to track feature descriptors during rotations, requiring manual parameter adjustments. The proposed deep neural network architecture integrates navigation cell types for accurate pose estimation and mapping. Our model accurately estimates agent pose and creates 2D spatial representations of environments in egocentric coordinates. It integrates egocentric views to build belief about free space and uses external spatial memory for loop closure detection. Evaluation experiments show our model outperforms competitors and state-of-the-art SLAM algorithms. Ablation study confirms the importance of individual modules and communication efficiency within the architecture. The model consists of four units: Head Direction Unit (HDU), Boundary Vector Unit (BVU), Place Unit (PU), and Grid Unit (GU). The GU utilizes a location-based addressing mechanism with fixed reading and writing heads in the center of external memory. A 2D binary masking map is used for memory operations in GU. The Grid Unit (GU) in the model reads and writes discriminative spatial representations from the global map using element-wise multiplication with binary masking maps. The predicted local maps over time steps in Maze 6 are shown in Figure 9, depicting the agent's position and orientation. The predicted local maps in Maze 6 show the agent's position and orientation using various spatial convolution and normalization techniques. The black areas represent unknown regions, while red indicates higher belief in free space. The Boundary Vector Unit (BVU) architecture consists of multiple layers of spatial convolution, normalization, and activation functions. Example results of predicted local maps in Maze 7 are shown, with frames displaying the agent's position and orientation. The top row shows the camera view, the second row shows ground truth with red arrows, and the third row shows the corresponding top-view. The Boundary Vector Unit (BVU) architecture includes spatial convolution, normalization, and activation functions. Results of predicted local maps in Maze 7 are displayed, with frames showing the agent's position and orientation. The top row shows the camera view, the second row shows ground truth with red arrows, and the third row shows the corresponding top-view accumulative belief of the predicted local maps. The architecture of the Head Direction Unit (HDU) includes spatial convolution, normalization, and activation functions. It consists of multiple layers such as ReLU, Linear, and CrossEntropyCriterion, with a final output of 6 classes."
}