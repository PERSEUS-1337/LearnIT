{
    "title": "SJxiHnCqKQ",
    "content": "Crafting adversarial examples on discrete inputs like text sequences is fundamentally different from generating such examples for continuous inputs like images. This paper introduces a novel algorithm called MCTSBug, which uses Monte Carlo tree search (MCTS) to find important words to perturb and perform homoglyph attacks on text. The algorithm is black-box and highly effective in fooling deep learning classifiers. MCTSBug can deceive deep learning classifiers with a 95% success rate on large-scale datasets by perturbing a few characters. It outperforms gradient-based methods and generates imperceptible adversarial perturbations. This raises concerns about the robustness of deep learning systems in safety-sensitive applications. Recent literature has defined perturbations as vector \u2206x for classifiers in safety-sensitive applications. Deep learning has shown success in natural language processing tasks like sentiment analysis and machine translation. While there is extensive research on adversarial examples for image classification, less attention has been given to generating adversarial examples in texts. Some recent studies have focused on defining adversarial perturbations on deep RNN-based text classifiers. Generating adversarial sequences in text involves projecting a vector to the nearest word vector in the embedding space, potentially replacing words with irrelevant ones. Different strategies have been proposed to create semantically meaningful adversarial sequences, such as using saliency maps and greedy scoring methods. These techniques aim to deceive deep classifiers by making minimal modifications to the original text. Crafting adversarial examples on discrete text sequences involves making minimal modifications to deceive deep classifiers. Unlike continuous inputs like images, defining the distance between two discrete sequences is challenging. The perturbations are visible, and empirical effectiveness needs improvement. Crafting adversarial examples on discrete text sequences is challenging due to the difficulty in defining the distance between two sequences. Possible choices include using deep learning NLP models with embedding layers, linguistic knowledge, or other strategies. However, current embedding models may not provide accurate nearest neighbors among words. The paper discusses the challenges of crafting adversarial examples on text sequences, focusing on the edit distance between texts. It also explores the black-box setup in fooling deep learning classifiers, using Google's Perspective API BID19 as an example. This API allows easy access to a text classification system that predicts toxicity. The paper discusses the challenges of crafting adversarial examples on text sequences, focusing on the black-box scenario where the model's structure and parameters are not accessible. Unlike previous methods relying on gradient evidence, this approach assumes attackers cannot access the target model's inner workings. A search strategy based on Monte Carlo tree search (MCTS) is designed to find the most important words to perturb in the vast search space of possible changes. The paper introduces MCTSBug, an algorithm that generates adversarial text sequences to deceive deep-learning classifiers by perturbing key words using homoglyph characters. This black-box approach aims to uncover vulnerabilities in deep learning models for discrete inputs. Our novel algorithm, MCTSBug, can deceive deep RNN models with a 95% success rate on text classification tasks. It works in a black-box setting, uses simple character-level transformations, and generates almost imperceptible perturbations. A deep learning classifier maps input space to label space. Adversarial examples involve perturbation vectors to create adversarial samples. The strength of the adversary determines permissible transformations. Different methods for finding adversarial examples include untargeted and targeted approaches. Recent studies focus on measuring the maximum change in any. Recent studies focus on adversarial examples for image classification, creating imperceptible modifications to pixel values through optimization procedures. Adversaries can be categorized based on the maximum change they can make to each feature, the Euclidean distance between x and x, and the number of feature variables they can alter. Additionally, adversaries can be classified as blackbox or white box based on their knowledge levels. In the context of adversarial examples for image classification, adversaries can be categorized as blackbox or white box based on their knowledge levels. In the black box setting, the adversary can only query the target classifier without knowing the model details or feature representations. In the white box setting, the adversary has access to the model, parameters, and feature set. Studies have extended adversarial examples to the black box setting, showing it is possible to create adversarial samples. One study by BID28 demonstrated creating adversarial samples to reduce classification accuracy without knowledge of model structure or parameters in image tasks. BID6 and BID16 also focused on generating black-box adversarial samples using different methods. BID17 utilized prior information and bandit optimization for gradient estimation. A new method called MCTSBug was designed to generate adversarial modifications on text sequences without gradient guidance, treating perturbation search as a two-stage task. The text discusses the process of creating imperceivable modifications to important words in a text input to disrupt a deep learning model's output classification. This is achieved through a two-stage task involving determining important words and making slight modifications using homoglyph characters. The modifications create challenges for the deep learning model while appearing unchanged to humans. The approach involves a search process to identify important words and efficiently modify them to impact the model's classification of the entire text sequence. Homoglyph attacks involve making small changes to words using characters with identical shapes, which can impact a deep learning model's classification of text sequences. This type of attack poses a significant cybersecurity threat, especially when attackers use homoglyphs to spoof domain names of well-known websites. Computer systems may treat visually similar characters from different origins as distinct, leading to potential misclassification by learning-based classifiers. Classifiers handle words through a dictionary to represent a finite set of possible words. The size of the NLP dictionary is much smaller than the space created by all possible characters. By creating visually similar but mis-spelled versions of important words, we can convert them to \"unknown\" in the dictionary. This strategy can force deep learning models to make wrong classifications, as changing one character to its homoglyph pair can make a word unrecognizable. This process neutralizes the word for the deep learning system. When generating adversarial text sequences, a token transformer is needed to perturb the text. Adversarial image samples often use gradient direction for perturbation, but applying this directly to text samples is problematic. One issue is that the gradient on the original text sample may lead to loss of information in the word, requiring modification of characters to their homoglyph pairs. The process of neutralizing a word for a deep learning system involves modifying a random character to its homoglyph pair. Generating perturbations for text samples in deep learning classifiers is challenging. BID30 proposed creating perturbations based on the gradient of the word embedding layer, but this method assumes continuity between words which may not always hold true. BID32 used linguistic rules to transform words, but this approach can be complex and time-consuming. Controlling the distance of perturbations in the discrete space is also difficult. Generating perturbations for text samples in deep learning classifiers is challenging. While some methods exist, such as using linguistic rules, they can be complex and time-consuming. Homoglyph attacks can neutralize words for classifiers, highlighting the need for efficient ways to find important words. Monte Carlo Tree Search, a method combining Monte Carlo and Bandit, has been successful in various applications, including game AI. The Upper Confidence Tree framework, including the UCB algorithm, has shown significant improvements in decision-making processes. Monte Carlo Tree Search (MCTS) is a solving algorithm used in various applications, such as game AI and feature selection. It requires a value function that can be calculated through querying a deep learning neural network. The adversarial perturbation generation problem is formalized as a reinforcement learning problem, where words are changed using a homoglyph transformer. The process involves selecting words to add to the current set until reaching a predefined limitation. The state space S is the powerset of word set X, with action space A equivalent to set X. A transition function p : S \u00d7 A \u2192 S removes the word action a from current state s. A policy \u03c0(s) is a probability distribution over possible actions. MCTS algorithm creates a search tree where nodes represent states, with edges representing actions connecting parent and child states. The goal is to find the optimal terminal state s T that maximizes a score evaluating adversarial samples. The goal of MCTS is to find the optimal terminal state by maximizing a predefined score function. The score function can be based on choices like cross-entropy loss, but this may not always reflect the success of an adversarial sample. For targeted attacks, the score function is defined based on the difference between the maximum predicted probability on any class except the original predicted class and the predicted probability of the target class. The targeted attack in MCTS involves using a modified UCT-1 algorithm for node selection based on the Upper Confidence Bound criterion. The algorithm includes four steps: Selection, Expansion, Simulation, and Backpropagation. Unlike traditional optimization methods, MCTS does not require gradients, allowing for flexibility in function choice. MCTSBug uses MCTS to search for the most appropriate set of words to change, then modifies the words using homoglyphs. One character in one word can neutralize the set of words, and the effect on the prediction is correctly simulated in the MCTS value. The MCTSBug algorithm uses MCTS to find the best words to modify using homoglyphs. It simulates the effect on predictions accurately. Adversarial samples are generated until success or search limit is reached. Previous methods using saliency or leave-one-out score are less effective. MCTSBug models word selection as a combinatorial optimization problem and uses Monte Carlo Tree Search for better results. The MCTSBug algorithm utilizes Monte Carlo Tree Search to find optimal words to modify using homoglyphs, focusing on global optimum compared to greedy strategies. It evaluates the effectiveness through experiments on deep learning models for text classification, assessing the validity of adversarial samples, impact on classifier accuracy, transferability between models, and robustness to configuration parameters. Three baselines were implemented for comparison, with MCTSBug showing superior performance. The MCTSBug algorithm uses Monte Carlo Tree Search to modify words with homoglyphs for optimal results. It compares four methods including random selection and gradient method based on model knowledge. The importance of security for IT managers is highlighted, with security sliding between the third and fourth most important focus for companies. The importance of security for IT managers has been sliding between the third and fourth most important focus for companies. Original Message about adding a song by Dave Matthews Band to a collection, with one song being particularly incredible on a DVD worth buying. The curr_chunk discusses the value of a DVD featuring a song by Dave Matthews Band, specifically highlighting a remarkable performance by backup singers. The DVD is recommended for purchase due to this standout song. The curr_chunk discusses the use of DBPedia Dataset and MCTSBug in generating adversarial samples. These methods are used to find the most important tokens in a text by evaluating the importance of each word and performing a greedy selection. The curr_chunk discusses using MCTSBug to find important tokens in text. Details of datasets, experimental setups, and results are in the appendix. Figures and tables show behavior under attacks. Adversarial training using MCTSBug on AG's News dataset was ineffective. The curr_chunk evaluates the effectiveness of MCTSBug in generating adversarial samples with minimal perturbation. It compares the attack success rates on 7 datasets, showing that MCTSBug outperforms baseline methods by successfully flipping predictions with a high success rate. The curr_chunk discusses the performance of different baselines in generating adversarial samples, highlighting the limitations of the Random and Gradient baselines. It explains that the Gradient baseline's poor performance is due to its inability to accurately estimate function value changes with large perturbations and varying input features. The curr_chunk discusses the targeted attack success rates of MCTSBug on various datasets, showing a 70%-90% success rate on the Basic-LSTM model. The paper introduces MCTSBug as a framework for generating imperceptible adversarial text sequences in a black-box setting. The novel framework MCTSBug generates imperceptible adversarial text sequences using homoglyph attack and MCTS to fool deep RNN-based text classifiers. The method successfully deceives classifiers across seven benchmarks by changing one character to its homoglyph pair, which is often classified as \"unknown\" by models. This highlights the limited coverage of NLP training datasets in representing the vast space of possible NLP letters. Attacks on text classifiers, such as the Good Word attack, aim to evade machine learning classifiers by adding positive words to spam messages. However, these attacks may not work well on modern deep learning models. In 2013, the concept of adversarial samples was introduced, showing that small changes in images can fool deep learning classifiers. The study focused on generating adversarial sequences on text to fool deep learning classifiers. They applied a white-box adversarial attack called projected Fast Gradient Sign Method to modify input text until it is misclassified. The algorithm picks a word, generates a perturbation using gradients, and maps it to the nearest word in the embedding space. The study focused on generating adversarial sequences on text to fool deep learning classifiers using white-box attacks. Different methods were used, including heuristic rules, word frequency analysis, and gradient access to modify input tokens and create adversarial samples. These methods operate in the word embedding space and rely on gradients to guide the modification process. The study explored generating adversarial text sequences to deceive deep learning classifiers using white-box attacks. Previous methods relied on gradients to modify input tokens in the word embedding space. In contrast, a new method proposed by BID12 utilizes greedy scoring strategies to rank words and apply character-level transformations to minimize perturbation distance. However, the visibility of perturbations and empirical effectiveness require improvement. Unlike previous approaches, the new method does not require knowledge of the target model's structure, parameters, or gradients. The study compared our method with previous approaches for generating adversarial text samples. Recent studies have also focused on creating adversarial examples for different NLP tasks like seq2seq machine translation and reading comprehension. BID7 attacks seq2seq models using projected gradient input to produce different outputs. BID18 generates samples to attack Question Answering models by feeding misleading information. Machine learning models process text by discretizing it into tokens, with words being the smallest unit for input. Word embeddings are mappings that project words in a dictionary to unique vectors in a vector space, making them more suitable for use in machine learning models. These models include a special entry for out-of-vocabulary words. Homoglyphs have been a security issue for years, with hackers using them for attacks. In 2000, someone created a website 'bl00mberg.com' using homoglyphs. In 2000, a website 'bl00mberg.com' was created using homoglyphs to release fake financial news and deceive investors. This issue has persisted over the years, with domain names now able to include Unicode characters. Name spoofing attacks are also seen in computer viruses, mimicking system processes. Despite advancements in web systems, this issue still occurs in popular browsers like Chrome and Firefox. Recent studies focus on adversarial examples for image classification, creating imperceptible modifications to pixel values. Recent studies have shown that DNN models are vulnerable to adversarial perturbations, with different strategies like L-BFGS algorithm and saliency maps being used to generate adversarial examples. Carlini et al. have proposed attacking methods with optimization techniques to create adversarial images with smaller perturbations. In recent studies, it has been shown that DNN models can be vulnerable to adversarial samples, which can reduce classification accuracy in a black-box manner without knowledge of the model structure or parameters. Adversarial examples have been extended to domains like speech recognition and speech-to-text. Experimental setup includes using seven large-scale text datasets for NLP tasks and testing on state-of-the-art RNN models like Basic-LSTM with random embedding and two LSTM layers. The text discusses different LSTM models for classification, including Basic-LSTM, Bi-LSTM, and their variations with GloVe embeddings. The models are evaluated without adversarial samples in Table 3. The text discusses the performance of deep-learning models without adversarial samples, achieving state-of-the-art performance on various datasets. The attacking methods were implemented using PyTorch 0.3.0 on a server machine with 4 Titan X GPU cards. Evaluation was based on the successful rate of the attack on generated adversarial sequences, with lower accuracy indicating more effective attacks. The attack successful rate on generated adversarial samples was shown on a certain dataset, with the X axis representing the number of modified characters and the Y axis showing the attack successful rate. The text discusses the robustness of the MCTSBug algorithm to parameter changes and the transferability of adversarial samples generated using it. Transferability was evaluated on four LSTMs with different embeddings. The experiment demonstrates that adversarial sequences can significantly reduce the accuracy of the target model from 90% to 30-60%. Adversarial samples can be transferred successfully to different models, showing the effectiveness of the method across multiple models. The DeepWordBug attack performs differently on various classes of inputs, indicating potential bias in the target model. The experiment shows that adversarial samples can greatly decrease the accuracy of the target model. Adversarial samples are difficult to filter using probability distribution. The MCTS attack algorithm can be applied to different types of inputs. An example is generating adversarial samples on the MNIST dataset using the L0 norm. By performing at most 20 actions, we successfully flip 37 samples out of 100 tested pixels, viewed as a combinatorial optimization problem. Samples are shown in FIG2."
}