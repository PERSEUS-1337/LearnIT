{
    "title": "SJecKyhEKr",
    "content": "Deterministic models are easier to interpret than stochastic ones but struggle to fully explain observational data due to nature's unpredictability. To address this, process noise is added to make deterministic models behave stochastically. However, this can lead to failures in the simulator, resulting in wasted computation and impacting downstream inference tasks. Performing inference in a space can be seen as rejection sampling, using a conditional normalizing flow to reduce simulator crashes and improve computational efficiency. Deterministic simulators are often made stochastic by adding noise to account for uncertainty, leading to better inferences in practice. The paper discusses the challenges of adding noise to input states in simulations, which can lead to failures and reduced performance. The authors propose a technique to minimize this failure rate. The paper introduces a technique to minimize failure rates when sampling from brittle simulators by learning state-dependent density over perturbations. This approach improves sample efficiency and increases the fidelity of inference results on various examples. The simulator is denoted as a deterministic function f : X \u2192 {X , \u22a5}, where \u22a5 signifies failure. The paper proposes a method to reduce failure rates in sampling from simulators by learning state-dependent density over perturbations. This approach enhances sample efficiency and improves the accuracy of inference results. The simulator is represented as a deterministic function f : X \u2192 {X , \u22a5}, where \u22a5 indicates failure. Stochastic perturbations denoted as z t \u2208 X are added to the state, and a rejection sampler is defined by iteratively sampling from a proposal distribution and evaluating f until successful iteration occurs. The paper proposes a method to reduce failure rates in sampling from simulators by learning a proposal distribution over perturbations. This approach aims to improve sample efficiency and inference accuracy by defining a rejection sampler with an acceptance rule targeting the current state. The goal is to minimize the distance between joint distributions specified over accepted states using different distributions. The paper introduces a method to enhance sampling efficiency by learning a proposal distribution over perturbations. It focuses on minimizing the distance between joint distributions specified over accepted states using different distributions, optimizing the acceptance rate through stochastic gradients. The flow is trained to minimize KL divergence between the implicitly specified distribution over accepted samples and the learned proposal distribution, retaining the proposal distribution's shape in various state space regions. The flow retains the shape of the proposal distribution in regions of state space to prevent collapse to trivial perturbations. By exploiting change of variables, optimization of the proposal distribution is possible without computing the derivative of the rejection rate. The generation of training data and learning of the autoregressive flow is computationally intensive, but the technique aims to maximize computational efficiency when deployed. Sampling from the flow is efficient, taking only seconds to iterate. The training procedure for the conditional masked autoregressive flow is computationally intensive but aims to maximize efficiency when deployed. Sampling from the flow is accelerated using GPUs and scales favorably in the number of samples produced. The structure of the flow includes single-layer MADE blocks with batch normalization layers, and conditioning is introduced using the current state vector. The conditional masked autoregressive flow is trained using a computationally intensive procedure to maximize efficiency. Conditioning is introduced by using the current state vector as input to a hypernetwork. The flow is implemented in PyTorch and optimized using ADAM. The results for the annulus problem show that the flow effectively learns the conditional distribution over accepted perturbations. The band shows effective flow learning with low rejection rates and variance reduction in evidence across multiple Monte Carlo sweeps. Inference results with lower variance can be achieved using a fixed sample budget. The example involves inferring a constant speed circular orbit model with a misspecified constant velocity forward motion model. A failure constraint limits the distance change from the origin, reflecting large allowable covariances in brittle simulators. The experiment demonstrates that by learning q \u03c6 as a proposal in a particle filter, rejection rates can be reduced to less than 4% compared to 75% under p. This leads to lower variance evidence approximations and increased fidelity of inference, confirmed by a paired t-test score of < 0.0001. By applying the method to the MuJoCo robotics simulator, a hard limit is placed on object overlap to address model misspecification. Gaussian perturbations are added to the state, resulting in reduced rejection rates and improved inference fidelity. The experiment shows that learning a conditional autoregressive flow significantly decreases rejections, leading to better hypothesis testing outcomes. The experiment showed that using a conditional autoregressive flow reduced rejection rates, improving hypothesis testing outcomes. The method addressed simulator failures caused by perturbing input states, leading to better inference fidelity. The curr_chunk discusses the use of rejection samplers and conditional autoregressive flow to improve simulation platforms by reducing variance in inference results. This method addresses issues caused by naively perturbing input states, with practical applications in scientific communities. The Kalman filter is a deterministic transition model perturbed with additive Gaussian noise, allowing for representation of variability in real-world systems. Complex systems require deterministic simulators with stochastic perturbations and numerical methods. Examples include stochastic Hodgkin Huxley models in neural dynamics and computational finance analysis of asset prices. The text discusses the challenges of ensuring robustness in complex simulators, with studies on sensitivity of earth science models to input parameters and alternative approaches to handling simulator failure. The methods lack clear probabilistic interpretation in time series models. Autoregressive flows (AFs) are a flexible class of density estimators that define a trainable density to approximate the target distribution by transforming samples through learned warpings. Masked autoencoder for distribution estimation (MADE) facilitates GPU-based parallelization in AFs. Multiple MADE blocks are used in masked autoregressive flows (MAF) to overcome challenges in high-dimensional scaling. The aim of this work is to develop a flexible proposal for perturbations that minimizes mass on perturbations causing simulator failures. This reduces computational costs and increases sample size. Deterministic models are considered, expressed as simulators iterating the state. The simulator iterates the state using a function f, which can return \u22a5 for invalid inputs. The function f is defined as f: X \u2192 {X, \u22a5}, with X A being valid inputs and X R being invalid inputs. The algorithm derived from this only requires f to be one-to-one in the accepted region. Stochastic perturbations denoted as z t \u2208 X are applied to induce a distribution over states. The iterated state is calculated as x t \u2190 f(x t\u22121 + z t). The simulator iterates the state using a function f, which can return \u22a5 for invalid inputs. The iterated state is calculated as x t \u2190 f(x t\u22121 + z t). The random variable A t \u2208 {0, 1} denotes whether the perturbation is accepted. The naive approach to sampling from the perturbed system involves repeatedly sampling from the proposal distribution and evaluating f until successful. This incurs wasted computation as failed iterations are discarded. The objective is to derive a more efficient sampling mechanism. Algorithm 1 defines a rejection sampler targeting successfully iterated states. The function f iterates states, with A t indicating perturbation acceptance. Algorithm 1 defines a rejection sampler targeting successful iterations, illustrated in Figure 3. The target distribution is denoted as p(x t |x t\u22121 ) = p(x t |x t\u22121 , A t = 1). The simulator acts as a rejection sampler, with a proposal distribution over z t. The proposal distribution over z t is shown in blue, while the target distribution p(z t ) is implicitly defined as p(z t ) = 1/Mp p(z t )I [f (z t ) = \u22a5]. Sampling from p constructs a rejection sampler with proposal p(z t ) and acceptance ratio p, targeting p(x t |x t\u22121 ). The functional form of p is unavailable, and the density cannot be evaluated for any input value. The distribution over accepted perturbations, denoted p(z t |x t\u22121), is conditioned on acceptance under the chosen simulator. The change of variables rule can be applied to relate this to p(x t |x t\u22121). Accepting perturbations that exit successfully shifts mass from regions where the simulator does not exit to regions where it does. In a rejection sampler, the probability of accepting a proposed sample is proportional to the ratio between the target distribution and the proposal. The acceptance probability in a rejection sampler is determined by the ratio of the target distribution to the proposal distribution. If the simulator fails, the target density is zero, and the sample should be rejected. The sample should be accepted with a certain probability in the accepted region, regardless of the choice of M. The acceptance rule in a rejection sampler is simplified to I [f (x t\u22121 + z t ) = \u22a5], without needing to evaluate M p, M, or p. This probabilistic interpretation allows us to define a proposal distribution q \u03c6 that maximizes acceptance rate while maintaining the same joint distribution as the original model. The simulator defines a proposal distribution q \u03c6 over accepted samples to minimize the distance between joint distributions specified by p and q \u03c6 using KL divergence. Samples are generated by directly sampling from the model, and stochastic gradient descent is used for minimization. The proposal distribution q \u03c6 is defined over accepted samples to minimize the KL divergence between joint distributions specified by p and q \u03c6. By applying a change of variables and canceling Jacobian terms, the KL term can be minimized by setting q \u03c6 (z t |x t\u22121 ) equal to p(z t |x t\u22121 ). This distribution is defined after rejection sampling as denoted by M q \u03c6 as the acceptance rate under q. The proposal distribution q \u03c6 aims to minimize the KL divergence between joint distributions p and q \u03c6 by setting q \u03c6 (z t |x t\u22121 ) equal to p(z t |x t\u22121 ). The acceptance rate under q is denoted as M q \u03c6, and optimizing q \u03c6 (z t |x t\u22121 ) involves maximizing this quantity using stochastic gradients. This allows learning the distribution over accepted x t values. The distribution over accepted x t values can be learned by learning the distribution over z t without calculating the Jacobian or inverse of the transformation defined by f. Density estimation on accepted samples from a rejection sampler can minimize wasted computation and target the same joint distribution, while retaining interpretability by utilizing the simulator. Additional results and experimental details for the annulus and MuJoCo experiment are included, along with two additional experiments involving balls bouncing in a square enclosure. The simulator perturbs the position and velocity of balls with Gaussian noise, leading to potential overlaps or intersections. Invalid configurations are rejected, conditioning on the state of both balls simultaneously. Results are displayed in Figure 4, with rejection rates plotted in Figure 4c based on the position of the first ball. The autoregressive flow reduces rejection rates in the simulation by proposing perturbations jointly, eliminating rejection in the permissible space. The trained proposal significantly decreases rejection compared to the a-priori specified proposal, showcasing the effectiveness of using q \u03c6. The autoregressive flow reduces rejection rates in the simulation by proposing perturbations jointly, achieving a strong level of statistical significance. Applying the algorithm to the WormSim simulator for Caenorhabditis elegans shows an increase in failure rate with perturbation scale. The autoregressive flow reduces rejection rates in simulations by proposing perturbations jointly, achieving statistical significance. The rejection rate during training is approximately 53%, with potential for further reduction through larger flows with regularized parameters. The learned proposal distribution q \u03c6 for the bouncing balls experiment shows the permissible region and invalid region induced by fixed ball positions. The flow deflects away from disallowed regions, with failure defined as radius change > 0.03. Variances of SMC sweep computed using 100 random traces and 50 sweeps per trace. \"Tosser\" configuration in MuJoCo used with overlap limit of 0.005. Integration time of 0.002 observed. Only x-y coordinates are considered. The capsule's position and velocity are perturbed with Gaussian noise in the MuJoCo simulation. Initial position and velocity have priors with specific standard deviations. The normalizing flow takes the capsule's state and actuator state as inputs, with time as control. The normalizing flow model is not perturbed under the time-varying control dynamics. Variances of the SMC sweep are computed by generating 50 random traces and performing 20 SMC sweeps per trace with 100 particles to compute the evidence."
}