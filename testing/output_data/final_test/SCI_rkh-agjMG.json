{
    "title": "rkh-agjMG",
    "content": "Prospection is crucial for humans in creating new task plans, but it has not been thoroughly explored in robotics. Predicting multiple task outcomes involves capturing task semantics and variability in the world state. This study proposes a method for learning a model that encodes task planning representations, combining machine learning for task semantics and task planning techniques for generalization in new environments. The approach involves training a neural net to predict likely outcomes from high-level actions in a given world, enabling the robot to generate understandable task plans and anticipate environmental changes many steps ahead. This method is demonstrated in a stacking task where a robot selects colored blocks in a cluttered environment. The algorithm generates realistic image and pose predictions in a cluttered environment with colored blocks and obstacles for task performance. It also demonstrates results on a simple navigation task. Humans excel at problem-solving by envisioning the consequences of their actions, a key aspect of intelligence. This ability is crucial for tasks like stacking colored blocks in a specific pattern using formal planning languages. However, defining goal conditions and action effects for robots to execute such tasks is a complex and time-consuming process. Deep generative models like conditional GANS and multiple-hypothesis models enable the generation of realistic future scenes. Recent work in robotics focuses on structured prediction, with approaches like SE3-nets predicting object motion masks and trajectories to intermediate goals. However, current methods mainly focus on short-term predictions. Recent work in robotics has focused on short-term predictions using deep policy learning for robotic tasks. One-shot imitation learning has also been explored for general-purpose block manipulation tasks, but relies on human expert solutions and lacks prospective future planning for reliable performance in new environments. The proposed model aims to learn high-level task structures to address these limitations. The proposed model aims to learn high-level task structures for robotic tasks, generating interpretable task plans by predicting movement goals. This approach is effective for solving complex problems involving spatial reasoning in robotics. Recent works have explored integrating planning and learning in uncertain worlds, such as DeSPOT. Approaches like QMDP-nets embed learning into planners using neural networks, like value iteration networks for navigation tasks. Other techniques like the Strategic Attentive Writer (STRAW) focus on sequence prediction for planning actions. Monte Carlo Tree Search (MCTS) has been used with learned action policies, but without a predictive model of the future. In this work, the problem of learning representations for high-level predictive models for task planning is examined. Various approaches have been proposed, such as fitting predictive models to predict the true state of an occluded world and using unsupervised learning of visual models to push objects around in a plane. The options framework and policy sketches are also discussed as methods for thinking about MDPs and curriculum learning. One method involves using curriculum learning with \"policy sketches\" and FeUdal networks, where a \"manager\" network sets goals for lower level \"worker\" networks. Generative models must handle a stochastic world and can generate multiple realistic predictions using methods like Generative Adversarial Networks (GANs). Adversarial methods and custom loss functions have been proposed for tasks like imitation learning and synthesizing diverse images. Deep autoencoders and spatial softmax representations have also been explored for reinforcement learning. Recently, various approaches have been proposed for reinforcement learning tasks. BID11 introduced a deep predictive network with a stochastic policy for manipulation tasks. BID29 developed a deep multimodal embedding for task adaptation across different interfaces. BID13 presented DARLA, a DisentAngled Representation Learning Agent for generalization to new environments. Planning problems with continuous states and controls are defined, incorporating observed information and high-level task actions. In the scene, high-level actions describe the task structure with a hidden world state encoding task information and sensor input truth. The goal is to learn models for generating intelligent task plans using a goal prediction function mapping observations to hidden states and actions. This function consists of an encoder mapping observations to hidden states and a decoder mapping hidden states to observations and actions. The encoder function maps observations to the hidden state, while the decoder function maps the hidden state to observations and actions. Different world state transformation functions can be learned, and the hidden state contains all necessary information about the world. Additional functions for planning include the expected reward-to-go and the policy over high-level actions. These functions can be learned from task demonstrations to project features into the latent world state for planning purposes. Our method involves predicting task executions and their consequences using a dataset of mixed execution failures and successes. Semantic labels are provided for different actions to create meaningful intermediate goals for prediction. The model takes high-level actions and state observations as input, with an encoder using convolutional blocks to process the data. The neural network architecture includes convolutional blocks, spatial soft-argmax layers, and Transform blocks. Two models are considered: a simple encoder-decoder network and a U-net, similar to BID15's model for image generation. The nonlinear transformation function maps hidden representations to latent representations. The neural network architecture includes convolutional blocks, spatial soft-argmax layers, and Transform blocks. Two models are considered: a simple encoder-decoder network and a U-net, similar to BID15's model for image generation. The nonlinear transformation function maps hidden representations to latent representations. The hidden world representation h is mapped to one of the m latent representations hj representing possible goals for the robot. Multiple hypotheses loss is used to train the system to predict several possible future states. Random noise is concatenated to the hidden state to represent variability in goal poses. The transformation function T j (h, z) is used to represent minor variability between actions by applying random noise vector z. Multiple hypothesis learning is explored to predict different possible future worlds, using a modified version of the MHP loss function. The predictor model predicts N H = 4 to N H = 8 different future states, each expressed as a weighted sum of different outputs. The observed state x is classified into three classes for the manipulator robot, and may include GPS position and camera view for a mobile robot. The loss function is expressed with weights for world state and low-level action prediction. Mean absolute error is used to compute the loss, encouraging accurate predictions with a normalizing effect. High-level action predictions are handled differently using an embedding for action description. In experiments, N H = 4 hypotheses were used with an exploration probability \u03bb = 0.05. Two functions, V (h) and p(a|h), were learned to estimate the value of a world and the probability of taking an action. The value function V (h) is trained using the full data set and operates on the current hidden state h. The value function operates on the current hidden state to predict future rewards. Action prior is learned on successful training data only to guide exploration in tree search. The method was applied to navigation and block-stacking tasks using a simulated robot. Data collection involves generating random world configurations and robot poses. In a navigation task, a robot is initialized in a random pose and trained using control laws and termination conditions to generate motions. Training involved positive and negative examples using Keras BID7 and Tensorflow on an NVidia Tesla K80 GPU. The robot's high-level actions include investigating objects in a construction site environment. Training took 12-18 hours depending on parameters used. The robot state is represented by a 6DOF pose, along with a 64x64 RGB image for an aerial view. Data was collected using a Gazebo simulation for navigation to four targets. The network was trained to predict goals accurately, and different network architectures were analyzed for pose error prediction. The ability to predict goals for task planning was tested in a more complex environment. In a block stacking task, the robot needed to stack colored blocks while avoiding obstacles. Training was done with 7500 trials, 3152 successful. Predictions were made using different dropout levels for training the decoder network. The robot's state was represented by its 6DOF end effector pose. The robot's state was represented by its 6DOF end effector pose and gripper state. A 64 \u00d7 64 RGB image of the scene was provided. Results showed successful predictions in block-stacking tasks, with better accuracy in predicting successes due to high variability in failures. An example of a task failure was shown when the robot attempted to pick up a red block but was interfered by a blue block. The robot experienced a task failure when trying to pick up a red block due to interference from a blue block. Varying levels of dropout affected model quality during training, with a dropout level of 0.125 - 0.25 providing the best results for predicting future scenarios. Using dropout in transforms or the decoder blurred results, impacting accuracy. Random seeds at each time step introduced noise to capture uncertainty. The dropout levels of 0.125 - 0.25 provided the best results for predicting future scenarios. Representing randomness through discrete choices and additional noise improved model performance, generating cleaner predictions. However, the impact of capturing randomness on predictive task models remains inconclusive. Adding a 32-D noise vector improved model performance by reducing validation loss and pose error. The randomness introduced by this noise vector led to better generalization and test performance, capturing small local variances. This addition was found to be unimportant for overall details but beneficial for enhancing model accuracy. The addition of a 32-D noise vector improved model performance by reducing validation loss and pose error, leading to better generalization and test performance. However, predicting a mean and covariance for each transform resulted in worse performance and the collapse of separate modes. Comparing models with and without skip connections showed lower image quality without skip connections, with an image-pixel error of 5.01 \u00d7 10 \u22124 and an average pose error of 9.61 \u00d7 10 \u22125. Total weighted validation loss was 0.012. The addition of skip connections in the model significantly improved image quality and pose accuracy. The hidden state only needs to encode changing aspects of the image, resulting in higher quality images and more accurate end effector poses. The model allows for interpretable task plans for robot execution with minimal labeling and can be adapted to various domains. In this work, the model allows for interpretable task plans for robot execution with minimal labeling and can be adapted to various domains. There are avenues for improvement in future work, such as automatically detecting change-points and expanding the method into a full planning algorithm using predicted value and action priors operating on sensor data."
}