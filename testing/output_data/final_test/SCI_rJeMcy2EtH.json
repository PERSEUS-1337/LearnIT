{
    "title": "rJeMcy2EtH",
    "content": "We introduce two approaches for efficient Bayesian inference in stochastic simulators with nested stochastic sub-procedures. These simulators are used in various sciences as generative models, but inferring from them is challenging due to the inability to evaluate their density directly. Our two-step approach approximates conditional densities of sub-procedures before using MCMC methods on the full program. This process simplifies dealing with lower-dimensional sub-procedures separately. Our approach simplifies Bayesian inference in stochastic simulators with nested sub-procedures by approximating conditional densities before using MCMC methods on the full program. This allows for isolating lower-dimensional sub-procedures without restricting the overall problem dimensionality. The utility of our approach is demonstrated on a simple simulator, commonly used in various scientific and industrial fields. Simulators are probabilistic generative models that can be used for Bayesian inference by constraining internal variables or outputs. This allows for running the simulator in reverse to determine parameter values. Recent advances in probabilistic programming systems have made it easier to encode and reason about simulators. Performing inference in simulators with nested stochastic sub-procedures (NSSPs) is challenging due to the inability to evaluate the density of their outputs without a normalizing constant. This hinders the evaluation of the overall simulator's normalized density, making common inference methods like Markov chain Monte Carlo ineffective. Performing inference in simulators with nested stochastic sub-procedures (NSSPs) is challenging due to the inability to evaluate the density of their outputs without a normalizing constant. This hinders the evaluation of the overall simulator's normalized density, making common inference methods like Markov chain Monte Carlo ineffective. Two new approaches are introduced to address this issue, focusing on approximating the individual NSSPs' conditional density and evaluating the unnormalized density of the NSSP in isolation. Training a regressor to approximate the normalizing constant of nested stochastic sub-procedures (NSSPs) allows collapsing them into the outer program. This approximation enables the use of conventional inference methods like MCMC and variational inference, scaling to higher dimensional simulators more effectively. This advancement opens the door to tractably running inference for complex problems. Our two approaches involve gradient-based learning of neural-network-based approximations for nested stochastic sub-procedures (NSSPs). These approximations can be reused for different datasets and configurations of the outer simulator, amortising the cost of running multiple inferences. The approaches are suitable for automation and can serve as PPS inference engines. Nested stochastic sub-procedures (NSSPs) are defined over valid program traces, where each random draw is represented by a density function. NSSPs cannot be directly evaluated exactly, but samples can be drawn from them or they correspond to the normalized density of a nested probabilistic program. These approximations can be reused for different datasets and configurations, making them suitable for automation and serving as probabilistic programming system (PPS) inference engines. The normalized density of a nested probabilistic program, represented by j |\u03c6 j, can be approximated by drawing samples from a separate inference procedure. Our inference schemes target simulators with sampling procedures, denoted by P in a j (x j |\u03c6 j). We replace these terms with approximations based on the prior program, allowing for the construction of an evaluable approximate target. The text discusses a method to construct an approximate target density by replacing terms in a probabilistic program with approximate surrogates. These surrogates are parametrized by deep neural networks and trained to minimize the Kullback-Leibler divergence. Training is done using stochastic gradient descent with gradient estimates. The text discusses using gradient descent to update NSSPs in a probabilistic program. If NSSPs satisfy [Case A], samples can be drawn easily. If NSSPs only satisfy [Case B], nested inference is needed to generate samples. Each nested inference problem is isolated and easier than running inference on the overall program. The text discusses using gradient descent to update NSSPs in a probabilistic program. Each nested inference problem is isolated and simpler than the overall inference problem. Sampling from P pr is used to generate input-output pairs for each NSSP, which is then separately trained. If NSSPs satisfy [Case B], they have a known unnormalised density on internal variables with unknown normalizing constants. Learning regressors to predict these constants can collapse NSSPs into the outer program for overall program evaluation. In the context of using gradient descent to update NSSPs in a probabilistic program, the text discusses approximating the normalization constant I in a j (\u03c6 j ) by introducing regressors R r (\u03c6 j ; \u03c4 r ). By learning these regressors, the NSSPs can be collapsed into the outer program for overall evaluation, allowing for inference in a pre-image space to ensure samples are from the desired posterior. Learning the regressors R r (\u03c6 j ; \u03c4 r ) is done by running the program forward to gather pairs {\u03c6 j ,\u00ce r (\u03c6 j )} for each NSSP, using them as a training dataset for a neural network regressor. This ensures R r (\u03c6 j ; \u03c4 r ) \u2192 I r (\u03c6 j ) \u2200\u03c6 j as training pairs tend to infinity. A 60-d nested Gaussian example is used in this section for validation against ground truth values. Figure 2 demonstrates results for Method 1. Figure 2 demonstrates promising results for Method 1, showing accurate inference for most marginal distributions. Issues with neural network stability are being investigated. These results suggest effective inference in higher dimensions than importance sampling. The variational objective is constructed using proposal parameters \u03ba = {\u03b7 a j ; a j \u2208 S r }. Gradients for stochastic gradient ascent are defined for each subproblem r \u2208 S r. Samples are extracted during training from each forward run. During training, samples are extracted from each forward run to train each NSSP separately. The regressors are trained using the L2-norm between the regressor and the approximations of the marginal. Parameters are learned to minimize this objective, resulting in accurate capture of the inputs if the neural network has sufficient capacity. The objective is defined by running Ppr forward and randomly selecting inputs passed to NSSP if called more than once. During training, samples are extracted from each forward run to train each NSSP separately by minimizing the L2-norm between the regressor and the approximations of the marginal. The approach outlined in Method 2 can be improved upon in the case where nested sub-procedures are rejection samplers, ensuring unbiased estimates for the target. During training, samples are extracted to train each NSSP separately by minimizing the L2-norm between the regressor and the approximations of the marginal. To ensure unbiased estimates for the target, one can improve upon the approach by directly estimating 1/I(\u03c6) and using this as the basis for the regressor. This circumvents the conundrum of generating both strictly positive and unbiased estimates for I(\u03c6). The method involves using unbiased estimates to construct an approximate objective for developing pseudo-marginal samplers. An example with a high-dimensional multivariate Gaussian model is used to demonstrate the validity of the methodology. Sequential sampling from a Markov process is done, and the correct ground truth is analytically calculated. Histograms comparing predicted and ground truth values are provided. Histograms comparing predicted and ground truth values for all 60 dimensions are provided in Figure 2."
}