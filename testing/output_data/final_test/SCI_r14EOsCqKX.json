{
    "title": "r14EOsCqKX",
    "content": "The efficacy of heuristics in improving deep learning models, such as learning rate schedules, knowledge distillation, skip connections, and normalization layers, has been beneficial. Empirical analysis through methods like mode connectivity and canonical correlation analysis (CCA) helps understand the training dynamics and why these heuristics succeed. Specifically, knowledge distillation and learning rate heuristics like (cosine) restarts and warmup are explored using mode connectivity. The introduction of heuristics like normalization layers, residual connections, and learning rate strategies has greatly accelerated progress in Deep Learning. Despite their simplicity and efficacy, the reasons why some of these heuristics work are still relatively unknown. In our work, we use mode connectivity and SVCCA to analyze deep networks and investigate strategies like cosine learning rate decay, learning rate warmup, and knowledge distillation. These strategies aim to accelerate training by adjusting learning rates. The strategy of training deep networks involves reductions and restarts of learning rates to escape spurious local minima. SGDR improves convergence by reducing the number of iterations needed for convergence. Learning rate warmup is important for training deep networks with large or dynamic batch sizes, involving increasing the learning rate followed by decreasing it using various schemes like step-decay or exponential decay. This strategy induces stability in the initial phase of training with large learning rates and has been used in training ResNets and Transformer networks at scale. Knowledge distillation (KD) involves training a smaller student model to mimic a larger teacher model, resulting in improved performance. The effectiveness of this strategy is discussed in sections 3, 4, and 5. Additionally, mode connectivity (MC) is a recent observation that shows the possibility of connecting different architectures at scale. The authors introduce mode connectivity (MC), a concept showing the ability to connect different local minima of deep networks via a piecewise-linear curve. They propose an algorithm to locate such a curve, which can help in understanding loss surfaces. Additionally, Raghu et al. (2017) suggest using CCA to analyze network activations, proving its computational advantages over alternatives. The authors demonstrate the computational benefits of using CCA for representational analysis in deep learning. They use mode connectivity to enhance understanding of cosine annealing, learning rate warmup, and knowledge distillation. Their experiments challenge common beliefs about cosine annealing and show that learning rate warmup primarily affects weight changes in deeper layers. Knowledge distillation primarily shares latent knowledge in deeper layers. The mode connectivity framework introduces a simple curve connecting optima in the loss function, challenging the belief that minima are isolated. It connects modes in the parameter space and validates accuracy on different curves. The mode connectivity framework introduces curves connecting optima in the loss function, challenging the belief that minima are isolated. Different curves connect modes in the parameter space with variations in hyperparameters and optimization techniques. The framework connects optima in the loss function by using different initializations and training schemes to establish robustness for analysis. Experimentation includes various strategies like optimizers, data augmentation, and hyperparameters to explore the limits of the approach. The study explores connecting optima in the loss landscape using different training schemes and initializations. Various strategies such as batch size, optimizer choice, and learning rate are tested to understand their impact on model convergence. The study explores connecting optima in the loss landscape using different training schemes and initializations. Different modes (B, C, D, E, F) are tested with various hyper-parameters while keeping other settings the same as mode G. The mode connectivity algorithm is applied to pairs of modes resulting in curves GA, GB, GC, GD, GE, and GF. Validation accuracy for models on each connecting curve is shown during different epochs of the training procedure. The study explores connecting optima in the loss landscape using different training schemes and initializations. Model parameters on the curve are given by specific equations. In a few epochs of the curve training, a high accuracy connecting curve is found for each pair of modes. The existence of these curves is supported by t-SNE plots in the appendix. To analyze the dynamics of SGD and SGDR, Canonical correlation analysis (CCA) is used along with interpolation of the loss surface between parameters at different epochs. Raghu et al. (2017) proposed coupling CCA with pre-processing steps like SVD or DFT to design a similarity metric for comparing neural net layers of different sizes or networks. The scalar outputs of neurons for input data can be stacked to create a matrix representing the output of a layer. Neurons and data points are used to create a matrix representing layer outputs in neural networks. Comparing layers using activations instead of weights and biases is important for interpretability. SVCCA involves dimensionality reduction and CCA to compare activation sets of different layers. The process continues with orthogonality constraints until a correlation threshold is met. The process of comparing neural network representations involves using SVCCA for dimensionality reduction and CCA to compare activation sets of different layers. The correlation values are computed until a threshold is met, and the average correlation is used as a measure of similarity between the layers. DFT pre-processing is suggested for convolutional layers with a large number of neurons to make the computation more efficient. SGDR was introduced as a modification to the common linear or step-wise decay of learning rates. SGDR is a learning rate strategy that decays rates along a cosine curve and restarts them at the initial value. It simulates warm restarts after a certain number of epochs, increasing the period for rate variation. SGDR is claimed to outperform other schedulers, especially for dealing with multi-modal functions and avoiding local optima. BID18 propose an ensembling strategy using iterates before restarts to escape local minima. Empirically investigate the effect of restarts on model activations using the CCA framework. Train VGG-16 network on CIFAR-10 dataset with SGDR. For experiments on the CIFAR-10 dataset, SGDR was used with T0 = 10 epochs, Tmult = 2, \u03b7max = 0.05, and \u03b7min = 10^-6. VGG training was also done with SGD and a step decay learning rate scheme. Mode connectivity algorithm was used to analyze the loss landscape on the optimization path of SGDR. Training loss for models along the line segment and curve found through mode connectivity was compared. In experiments on the CIFAR-10 dataset, SGDR was used with specific parameters. The mode connectivity algorithm was applied to analyze the loss landscape during optimization. Training loss patterns were compared between SGDR and SGD, showing differences in trajectory. SGDR's trajectory differs from SGD's, with experiments showing convergence to different regions. Intermediate points on the hyperplane reveal insights into the trajectory, suggesting SGDR does not converge to local minima. Mode connectivity path corresponds to lower training loss compared to SGDR's convergence points. The trajectory of SGDR differs from SGD, converging to different regions. Models on the connecting curve between iterates w 70 and w 150 show overfitting and poor generalization. Averaging iterates with cyclic or constant learning rates can improve generalization. CCA similarity plots for model pairs at epochs 10 and 150, and epochs 150 and 155, are presented in FIG0. The activations of shallower layers are more similar between partially and fully trained networks in standard SGD and SGDR training. Learning rate warmup is a common heuristic for training deep neural nets. The learning dynamics of SGD depend on the batch size and learning rate ratio. In large batch training of deep convolutional neural networks, increasing batch size requires a corresponding increase in learning rate. However, significant batch size increases may not support a proportional learning rate increase due to loss function curvature. Warmup is used to prevent training instability when using large learning rates. The importance of the warmup phase in the learning rate schedule is highlighted in large batch training. In large batch training of deep convolutional neural networks, increasing batch size requires a corresponding increase in learning rate. Warmup is used to prevent training instability when using large learning rates. We train a VGG-11 architecture on the CIFAR-10 dataset using SGD with momentum of 0.9. Learning rate for small batch (batch-size of 100) is 0.05, and for large batch (batch-size of 5000) is 2.5. Warmup increases the learning rate from 0 to 2.5 over the first 200 iterations. Learning rate is scaled down by a factor of 10 at epochs 60, 120, and 150. Figure 4 shows the learning rate and validation accuracy for different setups. Figure 4 (c) shows the similarity for different layers in the setups. An increase in similarity is observed for the last few layers in the large batch setup. The increase in similarity for the last few layers in the large batch setup with warmup suggests stability in the fully-connected stack. Training without warmup but freezing the fully-connected stack initially validates this. Results for ResNet-18 and ResNet-32 also support this claim, demonstrating its generality. Knowledge distillation involves training a \"student\" model using the output probability distribution of a \"teacher\" model to transfer dark knowledge and improve performance. The study investigates if this knowledge transfer is limited to certain parts of the network by measuring representational similarity between layers. The study explores knowledge distillation by training a student model with a teacher model's output distribution to improve performance. Using a VGG-16 model as the teacher and a shallow convolutional network as the student, the distillation training achieves a validation accuracy of 85.18%, compared to 83.01% with standard training. Layer-wise representations are compared with the teacher network. The study compares layer-wise representations of a student network with a teacher network, showing that knowledge distillation mainly impacts the deeper layers. This aligns with previous findings on fine-tuning or transfer learning, suggesting that training higher layers is crucial. The study investigates three heuristics in deep learning: cosine annealing, learning rate warmup, and knowledge distillation using landscape analysis tools. The analysis suggests that the reasons for the success of cosine annealing are not supported in practice, and learning rate warmup helps prevent overfitting. The investigation explores the effectiveness of cosine annealing, learning rate warmup, and knowledge distillation in deep learning. It suggests that cosine annealing's success is not practical, learning rate warmup prevents training instability in deeper layers, and knowledge transfer from teacher to student occurs mainly in deeper layers. New heuristics are proposed to improve the training process, such as training only portions of the student network instead of the whole network. Results on SGDR support the use of averaging schemes, and self-distillation shows improved performance through multiple generations of knowledge distillation. The study suggests training only subsets of the model can reduce computational costs for future generations. Freezing weights instead of using learning rate warmup can maintain training performance with less computation. The learning rate is initialized to 0.05 and scaled down at specific epochs. Data augmentation is done through random cropping of input images. Validation Loss, Training Accuracy, and Training Loss are shown in Figures 7, 8, and 9. In Section 3, experiments on the trajectory of SGDR are presented using the plane defined by points w70 and w30. The visualization in FIG0 shows the connectivity of 6 pairs, confirming overfitting tendencies. In Section 3, experiments on the trajectory of SGDR are presented using the plane defined by points w70 and w30. The Training loss and Validation loss surface are plotted for another plane defined by SGDR's iterates w30, w70, and their connection w30-70. The VGG-16 architecture used does not include Batch Normalization, so VGG-16 is trained with Batch Normalization using SGDR to verify observations. Batch Normalization statistics are computed for the network at the test stage. Other training parameters remain the same as discussed in Section 3. Figure 13(a) displays the training loss for models on the line segment and MC curve connecting iterates from SGDR. Higher training loss is observed for models on the line segment, indicating SGDR finds paths crossing a barrier in the loss landscape. CCA similarity plots compare model pairs at different epochs, showing correlations between layers. This contrasts with typical SGD decay and illustrates the immediate impact of restarting on the model. In their work, Raghu et al. (2017) found that restarting training does not greatly impact the model, especially in the shallower layers. Comparing epochs before and after a restart also supports this hypothesis. Additionally, holding the FC stack frozen can induce stability during training with large batches and learning rates. This was demonstrated on the VGG-11 network. In experiments with ResNet architectures 18 and 32, the learning rate was adjusted for better training results. Results were consistent across different configurations, but further research is needed to confirm these findings on larger datasets like ImageNet."
}