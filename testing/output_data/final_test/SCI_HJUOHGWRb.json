{
    "title": "HJUOHGWRb",
    "content": "Contextual Explanation Networks (CENs) are deep networks that generate parameters for context-specific probabilistic graphical models used for prediction and explanations. Unlike existing tools, CENs learn to predict and explain jointly, providing valid instance-specific explanations with no extra computational cost. This approach acts as a regularization and improves performance in low-resource settings. Results show that CENs are competitive in image and text classification tasks, offering additional insights behind each prediction. Model interpretability is crucial in machine learning due to the increasing use of complex predictive algorithms. While high performance may indicate predictive capabilities, perturbation analysis reveals vulnerabilities in black-box models. To ensure trust in machine learning systems, explanations for decisions are essential. Restricting models to human-intelligible ones is a potential solution, but post-hoc explanations for complex models can also be effective. Contextual explanation networks (CENs) are introduced as deep neural networks that generate parameters for probabilistic graphical models, serving as explanations and used for predictions while encoding prior knowledge. Data typically includes low-level unstructured features and high-level human-interpretable features for interpretability. Contextual Explanation Networks (CENs) use deep networks to process low-level representations and construct context-specific probabilistic models for high-level features. The explanation mechanism is integral to CENs, with models trained to predict and explain jointly. An example is diagnosing heart arrhythmia risk, where diverse causes like smoking and diabetes can have varying effects in different contexts. Data includes raw text medical notes as context and specific attributes like high blood pressure and diabetes. The paper introduces Contextual Explanation Networks (CENs) as probabilistic models that use deep networks to process low-level representations and construct context-specific hypotheses for high-level features. The main contributions include formally defining CENs, deriving learning algorithms, and showing that CENs generate explanations faster than other methods. Contextual Explanation Networks (CENs) use deep networks to quickly generate explanations, detect noisy features, and improve model diagnostics and prediction accuracy. CENs are implemented by extending established deep architectures for image and text data, as well as designing new architectures for survival analysis. Experiments show that explanations act as a regularizer and enhance sample efficiency. The combination of deep networks with graphical models has been extensively explored, with recent work focusing on task-agnostic representations discovered by deep networks. The goal of this body of work is to design structured probabilistic models that combine deep learning flexibility. CENs integrate neural networks into graphical models, preserving interpretability compared to previous deep graphical models. CENs preserve simplicity in contextual models by shifting complexity into the process of conditioning on the context. They operate similarly to meta-learning, where a meta-model learns to generate task-specific models. This approach has been successful in zero-shot and few-shot learning, cold-start recommendations, and other scenarios. CENs generate interpretable models by incorporating domain knowledge and using attention mechanisms for improved interpretability. They handle complex context representations, unlike previous models that rely on discrete variables. This approach has shown promise in solving complex tasks like program induction for algebraic word problems. CENs focus on generating interpretable models by incorporating domain knowledge and using attention mechanisms for improved interpretability. They differ from previous methods by learning to provide explanations along with predictions, rather than constructing them post-hoc. Various visualization techniques and explanations by example are also used for interpretability. The framework discussed encompasses personalized models that partition input space and fit local sub-models. It addresses learning from data with context, attributes, and targets, aiming to predict targets from attributes and context. Contextual explanation networks are models that provide explanations by relating interpretable variables to targets. Contextual explanation networks (CENs) provide explanations by relating interpretable variables to targets, allowing for modeling conditional probabilities in a black-box fashion while keeping explanations simple and interpretable. CENs assume each explanation is context-specific, with a conditional probability defining the validity of an explanation in a given context. This approach allows for making predictions and interpreting them by inferring the posterior probability of an explanation being valid in a specific context. In practice, p w (\u03b8 | C) is represented with a neural network that encodes context into the parameter space of explanations. Different ways to construct an encoder are considered, such as deterministic encoding using a delta-function to collapse the conditional distribution. An example of a CEN architecture is shown in FIG11, where the context is transformed into an attention vector for constructing a contextual hypothesis. The contextual hypothesis is constructed using a dictionary of sparse atoms. MoE utilizes an attention mechanism to combine predictions from each model in the dictionary. The deterministic encoding of \u03b8 depends on C, leading to a tractable conditional log-likelihood. However, deterministic encoding lacks constraints on generated explanations, which can result in unstable, overfitted local models. To address this, constraints are imposed to ensure human-interpretable and globally meaningful explanations. To address issues with local explanations, a global dictionary of sparse atoms is introduced. The encoder generates context-specific explanations using soft attention over the dictionary. The model is trained by learning weights and the dictionary. Extending the representation to mixtures of delta-distributions allows for the recovery of mixtures of experts. Contextual Variational Autoencoders recover mixtures of experts using a dictionary of experts and an encoder. The log-likelihood is the same as for MoE, with p w (k | C) represented as soft attention over the dictionary for combining expert predictions. Learning is done by optimizing the log-likelihood or via EM. To infer an explanation, the posterior is computed. Modeling p(Y | X, C) avoids representing the joint distribution, providing better regularization in low-resource settings and a coherent Bayesian framework. The curr_chunk discusses a Bayesian framework for regularization in low-resource settings using a variational autoencoder with explanation parameters. The encoder and decoder are represented by q w (\u03b8 | C) and p u (C | \u03b8) respectively, utilizing a global learnable dictionary. The generative process and evidence lower bound (ELBO) are outlined, with a logistic normal distribution over the simplex for q w (\u03b8 | C). In this section, the relationship between CEN-generated and LIME-generated post-hoc explanations is analyzed. LIME constructs explanations as local linear approximations of the decision boundary of a model in the neighborhood of a given point. The quality of the linear model as an approximation to the original model is measured using L(f, \u03b8, \u03c0 x,c ), with L2 and L1 losses typically used. The neighborhood is defined by a distribution concentrated around the point of interest, allowing for comparison of explanations produced by CEN and LIME. The relationship between CEN-generated and LIME-generated explanations is analyzed by comparing their local linear approximations of the decision boundary. The quality of the approximation is measured using L2 and L1 losses, with the neighborhood defined by a distribution concentrated around the point of interest. The local approximations obtained by solving (6) recover the original CEN-generated explanations when the context encoder is deterministic and the space of explanations is linear. The loss function defined as DISPLAYFORM1 concentrates around \u03b8 as DISPLAYFORM2, ensuring high probability of recovering \u03b8. Equivalence between CEN and LIME explanations is established, but LIME may produce inconsistent explanations in certain conditions. CENs are more effective for structured data compared to simple classification. CENs allow for representing p(Y | X, \u03b8) using graphical models, particularly valuable for survival time prediction tasks in medical settings. The setup involves designing CENs with CRF-based explanations for sequentially structured outputs, where targets are sequences of binary variables indicating event occurrence or censoring. CENs use CRF-based explanations for sequential outputs in survival time prediction tasks. Targets are represented with latent variables, and the model ensures only valid sequences are assigned non-zero probability. Pairwise potentials between targets ensure certain configurations are improbable. The joint log-likelihood of the data consists of two parts: sum over non-censored instances and sum over censored instances. CENs use CRF-based explanations for sequential outputs in survival time prediction tasks, focusing on different data modalities like image, text, and time-series. They analyze the impact of explanations on predictive model performance and efficiency in learning and prediction processes. The curr_chunk discusses the impact of noisy interpretable features on explanations generated by LIME and how CEN can help detect and avoid such situations. It also explores the insights gained from visualizing and inspecting explanations. Details on model performance on classification tasks are provided in Table 1. The curr_chunk discusses the use of classical datasets like MNIST and CIFAR10 for supervised learning, as well as a text dataset for sentiment classification of IMDB reviews. High-level features are extracted from images using downscaled versions and HOG descriptors, while word sequences are used for sentiment analysis. The study also includes poverty prediction for household clusters in Uganda using satellite imagery and survey data. The study focuses on poverty prediction in Uganda using satellite imagery and survey data. Each household cluster is represented by satellite images and categorical variables. A pre-trained VGG-F network is used to compute embeddings for binary classification of poor and not poor households. The dataset is small, so the VGG-F model is kept frozen to prevent overfitting. Linear regression and deep nets are used as baselines, with different architectures for MNIST, CIFAR10, and IMDB datasets. The study compares different deep architectures for poverty prediction in Uganda using satellite imagery and survey data. Models include VGG-F, bi-directional LSTM, and multi-layer perceptron. Three types of models are used: CENs with deterministic encoding, mixture of experts, and VCENs. CENs are trained to generate predictions and explanations using a global dictionary. Sparsity regularization is applied to the dictionary. Adding a small sparsity penalty on the dictionary helps avoid overfitting for large dictionary sizes in CENs. The explanation layer in CENs affects the optimization problem geometry, leading to faster convergence. Explanations act as a regularizer, improving sample efficiency in model training. The results show that contextual explanation networks outperform sparse linear models on the Satellite dataset with only 500 training points. Training an MLP on both satellite image features and survey variables does not achieve the same results as contextual explanation networks. The quality of linear explanations depends on the selection of interpretable features, especially when features are noisy or incomplete. The experiment investigates the effect of noisy features on explanations provided by LIME and CEN. LIME successfully overfits explanations to noisy features, while CEN's performance worsens with increasing noise levels. The experiment shows that while LIME can overfit explanations to noisy features, CEN's performance degrades with increasing noise levels. This highlights a drawback of post-hoc explanations, as they can mislead when based on poor or incomplete features. After training CEN with a dictionary of size 32, the encoder tends to select different explanations (M1 and M2) for urban and rural household clusters in Uganda based on satellite images. Explanations weigh various categorical features differently, such as water source reliability and house construction materials. CEN tends to pick M1 for urban areas and M2 for rural areas, as visualized on the map. The CEN model selects different explanations (M1 and M2) for urban and rural household clusters in Uganda based on satellite images. It correlates with high nightlight intensity in major city areas. The model's high performance allows for drawing conclusions about factors influencing classification of households as poor. Additionally, CENs are applied to survival analysis to predict event occurrence time and assess risk. In survival analysis, CENs with deep nets are used as encoders and CRF-structured explanations. Two datasets, SUPPORT2 and PhysioNet 2012 challenge, are utilized for ICU patient analysis. The data is preprocessed and CRF explanations are generated for influential features like dementia and days in hospital. The data from SUPPORT2 had 9105 patient records with 73 variables, while PhysioNet had 4000 patient records with 37-dimensional time-series data. Features were encoded and missing values filled with -1. Training used 7105 records, validation 1000, and testing 1000. The survival timeline was capped at 3 years and converted into 156 intervals of 7 days each. The patient data from the ICU was resampled and mean-aggregated at 30 min intervals, with missing values filled with 0. The context, C, was based on the resampled time-series, while attributes, X, used the last available measurements. The survival timeline was limited to 60 days and split into 60 intervals. Different models, including Aalen and Cox models, were used as baselines. CRFs were combined with neural encoders in models like MLP-CRF and LSTM-CRF, which have shown success in natural language applications. The CRF layer assigns weights to latent features. The study used CENs with CRF-based explanations for survival analysis, with parameters assigned to latent features. Two metrics specific to survival analysis were used: accuracy in predicting survival at different time points and relative absolute error. Results for all models are provided in TAB2. Our implementation of CRF baseline in the study reproduced and slightly improved performance reported previously. MLP-CRF and LSTM-CRF models enhance CRFs but lose interpretability. In contrast, CENs outperform neural CRF models in certain metrics and provide explanations for risk prediction for each patient over time. Visualizing explanation weights for influential features in CENs helps understand patient-specific dynamics in survival rate predictions. In this paper, contextual explanation networks (CENs) are introduced as models that predict by generating and leveraging context-specific explanations. CENs are defined as probabilistic models that can provide explanations during the prediction process, offering benefits such as regularization and consistency. Post-hoc explanations can be misleading in certain cases, highlighting the importance of integrating explanation into the prediction process. The contextual explanation networks (CENs) introduced in this paper offer benefits such as strong regularization, consistency, and the ability to generate explanations with no computational overhead. However, limitations include the uninterpretable process of conditioning on context and the need for a more hierarchically structured space of explanations. Future work could explore ideas like context selection or rationale generation to improve interpretability. The contextual explanation networks (CENs) proposed in this paper aim to improve prediction capabilities, model diagnostics, pattern discovery, and data analysis in high-stakes applications. CENs represent the predictive distribution and the generative process behind the data, exploring whether they can represent any conditional distribution when the class of explanations is limited. CENs are viewed as a mixture of predictors, with the effectiveness depending on the richness of the mixing distribution. The mixing distribution in contextual explanation networks (CENs) should be rich to approximate any smooth density. However, limitations in the distribution and the separation of inputs into subsets (C, X) may impact model interpretability and predictive power. The mixing distribution in contextual explanation networks (CENs) should be rich to approximate any smooth density. Limitations in the distribution and the separation of inputs into subsets (C, X) may impact model interpretability and predictive power. To address this, p(\u03b8 | C) fully factorizes over the dimensions of \u03b8, and hypotheses, p(Y | X, \u03b8), factorize according to an underlying graph. This factorization also applies to explanations according to G, where subsets of Y variables and corresponding Markov blankets are denoted by \u03b1 and MB(\u03b1) respectively. All encoding distributions in the paper fully factorize over the dimensions of \u03b8, including delta functions, mixtures, and neural net parametrized encoders. The contextual explanation networks (CENs) aim to approximate any smooth density with a rich mixing distribution. The decision boundary of CEN can be approximated by constructing approximations of an arbitrary predictor in the locality of a specified point. This involves solving an optimization problem to measure the quality of the approximation and ensure human-interpretability of the selected local hypotheses. When the predictor is defined by a CEN, the problem can be posed in terms of approximations and explanations represented by \u03b8. Theorem 1 provides a concentration result for linear hypotheses in the context of contextual explanation networks (CENs). It answers whether \u03b8 can recover \u03b8 under certain conditions. The analysis considers log-linear explanations for binary classification without bias terms in linear models. The loss function is defined for a C-Lipschitz vector-valued function with a zero-mean distribution. The optimization problem simplifies to least squares linear regression without bias terms. The solution can be written in closed form, with \u03b8 as a random variable. Concentration bounds on \u03b8 are obtained using continuity properties and random matrix theory. The concentration bounds on \u03b8 are obtained using continuity properties and random matrix theory. Further, let \u03c0x and \u03c0c concentrate such that p\u03c0x(x\u2212x>t)<\u03b5x(t) and p\u03c0c(c\u2212c>t)<\u03b5c(t), where \u03b5x(t) and \u03b5c(t) both go to 0 as t \u2192 \u221e. The proof involves bounds from the convexity of the norm, spectral norms of matrices, Lipschitz properties, and concentration of variables. The concentration bounds on \u03b8 are derived using continuity properties and random matrix theory. The matrix Chernoff inequality is utilized, and as \u03c4 \u2192 \u221e and t \u2192 \u221e, terms on the right-hand side vanish, leading to \u03b8 concentrating around \u03b8. Sampling more points around (x, c) can make the first term negligibly small. With additional assumptions on the sampling distribution, precise convergence rates can be derived. In the local approximation setup, precise convergence rates can be derived by controlling the assumptions on the sampling distribution. This differs from linear regression with random design where assumptions on the design matrix may be unrealistic. Concentration analysis for a more general case with a convex loss function and decomposable regularizer is possible but more complex and unnecessary for the current purpose. The prediction in MoE involves using each of the K experts to compute the predictive distribution. To explain a prediction in a contextual Mixture of Experts (MoE) model, we can analyze the posterior weights assigned to each expert. If a single expert has a high weight, it can be considered as the explanation. However, in general, weights may be spread out, making it challenging to find a local explanation. To learn contextual MoE, we can optimize the conditional log-likelihood or use expectation maximization (EM) procedure. This involves computing posteriors at each iteration. In contextual variational autoencoders, the iterative procedure guarantees convergence to a local optimum. The evidence is expressed through expected conditional likelihood, context reconstruction error, and KL-based regularization term. Optimization of the ELBO is done using first-order methods with Monte Carlo sampling. Reparametrization is straightforward when the encoder has a Gaussian distribution. In survival analysis, encoders output probability distributions over a dictionary for better performance and faster convergence. Sampling from the encoder results in logistic normal distributed samples that are re-parametrized. The Dirichlet distribution is used for the prior, and the KL-based regularization term is estimated stochastically. The goal is to predict the occurrence time of future events, such as patient death or customer turnover. The survival data in survival analysis involves censored instances where event times are not observed. The survival function, S(t), gives the probability of an event not happening up to time t. The hazard function, \u03bb(t), represents the instantaneous rate of failure. Proportional hazard models assume \u03bb is a function of instance features. Cox's model assumes \u03bb \u03b8 (t; x) := \u03bb 0 (t) exp(x \u03b8). Aalen's model is another approach for survival modeling. Survival analysis involves censored instances where event times are not observed. Cox's proportional hazard model assumes \u03bb \u03b8 (t; x) := \u03bb 0 (t) exp(x \u03b8). Aalen's model is a time-varying extension with \u03bb \u03b8 (t; x) := x \u03b8(t). Survival analysis can be approached as a multi-task classification problem. BID29 proposed a model called sequence of dependent regressors for survival time prediction. This model uses LSTM to generate \u03b8 t for constructing the log-likelihood for CRF. The model proposed by BID29 uses LSTM to generate \u03b8 t for constructing the log-likelihood for CRF, which involves censored instances in survival analysis. The model optimizes an objective function with regularization terms and log-likelihood for both censored and non-censored inputs. Additionally, context variables are considered in generating \u03b8 t for each time. In our experiments, CENs take context variables as inputs and generate \u03b8 t for each time step using a recurrent encoder. The architectures for CENs considered in our experiments are detailed in FIG8. Each \u03b8 t is generated using a constrained deterministic encoder with a global dictionary of size 16. The CEN-CRF architectures are trainable end-to-end, and the objective is optimized using a stochastic gradient method. The objective function is constructed for each mini-batch based on censored and non-censored instances. The experimental setups for CENs included details on architectures, training procedures, and qualitative results on MNIST and IMDB datasets. MNIST dataset was split into training, validation, and testing points, trained for 100 epochs with Adam optimizer. HOG representations were computed using 3x3 blocks. For CIFAR10 experiments, input images were normalized and rescaled representations were standardized. The data for IMDB dataset was split into train, validation, and test sets. Models were trained with Adam optimizer and randomly initialized. Pre-trained VGG-16 network was used for feature extraction from satellite imagery. VCEN model utilized dictionary-based encoding and logistic normal distribution for output. The inference network used a normal distribution for output, trained with Adam optimizer. Minimal pre-processing was done on medical data, with missing values denoted by negative entries. PhysioNet time series data was resampled at regular intervals, creating missing values. Additional qualitative results were discussed for CENs on MNIST and IMDB data. Figures 9a, 9b, and 9c show qualitative results for CENs on MNIST data, illustrating predictions made by CEN-pxl. The charts display true labels, input images, explanations for top 3 classes, and attention vectors from a global dictionary. Different patterns in the data are captured by elements in the dictionary, aiding CEN in making predictions. The CEN model uses a global dictionary for prediction, with sharp attention for correct predictions and dispersed attention for mistakes. High attention entropy indicates ambiguous examples. Test weights assigned to topics show bi-modal behavior in sentiment prediction. The CEN model shows bi-modal behavior in sentiment prediction, with topics contributing either positively, negatively, or neutrally. Genre topics have negligible contributions, indicating no biases towards any specific genre. Acting-related topics are bi-modal, while CEN assigns highly negative weight to the topic related to \"bad\". The CEN model assigns weights to different topics in sentiment prediction, with no bias towards specific genres. The model shows bi-modal behavior in sentiment prediction, with topics contributing positively, negatively, or neutrally. Acting-related topics are bi-modal, with highly negative weight assigned to \"bad\". The model actively uses biases picked up from data for prediction, as shown in the dictionary of explanation patterns. The CEN model assigns weights to different topics in sentiment prediction, showing bi-modal behavior. Atoms 5 and 11 in the dictionary have inverse weights for topics related to family and violence, which are negatively correlated. These patterns are used by CEN to predict sentiment based on the review context."
}