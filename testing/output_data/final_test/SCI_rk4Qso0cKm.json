{
    "title": "rk4Qso0cKm",
    "content": "The algorithm presented aims to train a robust neural network against adversarial attacks by incorporating randomness in a more optimal way than simply adding noise to all layers. The algorithm proposed involves using Bayesian Neural Networks to learn model distributions efficiently and formulate a mini-max problem to train a robust neural network against adversarial attacks. Experimental results show significant accuracy improvement compared to previous methods under strong attacks. The vulnerability of deep neural networks to adversarial attacks is a significant concern, as carefully crafted perturbations can lead to incorrect predictions. These imperceptible perturbations, known as adversarial attacks, pose security risks for applications requiring strong reliability. Various algorithms have been developed to defend against such attacks and improve network robustness, with some showing promising results on medium-sized data. In response to the vulnerability of deep neural networks to adversarial attacks, various defense algorithms have been developed. Two effective approaches on medium-sized data involve adversarial training and adding stochastic components to neural networks. The recent algorithm proposed in BID25 is recognized as a successful defense, while the Random Self-Ensemble (RSE) approach achieves similar performance to Madry's adversarial training. A new defense algorithm called Adv-BNN combines adversarial training and Bayesian network to enhance robustness. The paper introduces a novel approach called Adv-BNN, which combines Bayesian Neural Network techniques with adversarial training to improve robustness against attacks. The method assumes stochastic weights in the network and utilizes a mini-max formulation for training. Testing on CIFAR10, STL10, and ImageNet143 datasets shows significant enhancements over previous defense methods. The neural network is parameterized by weights w \u2208 R d, denoted by f (x; w), where x \u2208 R p is the input example and y is the label. The training/testing dataset is D tr/te with size N tr/te. The loss function is f (x i ; w), y i, where i is the index of the data point. Adversarial perturbation is denoted as \u03be \u2208 R p, and the adversarial example is generated by x adv = x o + \u03be under norm constraint BID25. The paper focuses on attacks under norm constraint and considers the cross-entropy loss in experiments. The Hadamard product is denoted as . In this section, we summarize related works on adversarial attack and defense. Attack algorithms generate adversarial examples based on the gradient of loss function with respect to the inputs. For example, FGSM perturbs an example by the sign of gradient and uses a step size to control the norm of perturbation. C&W attack formally poses attack as an optimization problem and applies a gradient-based iterative solver to get an adversarial example. PGD attack is commonly used to benchmark defense algorithms due to its effectiveness. The goal of PGD attack is to find adversarial examples in a \u03b3-ball. Starting from x0 = xo, PGD attack conducts projected gradient descent iteratively to update the adversarial example, with a direct control of distortion by changing \u03b3. In random neural networks, attackers seek a universal distortion \u03be to cheat a majority of realizations of the random weights by maximizing the loss expectation. Defense methods against attacks on random neural networks involve maximizing loss expectation by updating model weights through multi-step SGD updates. Various defense techniques have been proposed, including denoiser-based methods and randomized image preprocessing. Adversarial training is a key defense strategy, involving training neural networks on adversarial examples until loss convergence. Incorporating adversarial training in Bayesian neural networks for better robustness. Another approach involves adding a \"noise layer\" to input features with Gaussian noise to increase deep neural network robustness. This method can generate an infinite number of models without additional memory cost. The algorithm introduces a noise layer in Bayesian neural networks to improve robustness. It differs from existing methods by adding noise to weights, not input features, and incorporates adversarial training for enhanced performance. The goal is to estimate the distribution of hidden variables in the model, but exact solutions are often intractable due to high-dimensional integrals. To speed up inference in high-dimensional Bayesian neural networks, two approaches are commonly used: sampling efficiently using methods like Stochastic Gradient Langevin Dynamics (SGLD) or approximating the true posterior with a parametric distribution. However, both methods have limitations, leaving high-dimensional Bayesian inference as an open problem. Stochastic Gradient Langevin Dynamics (SGLD) updates with Gaussian noise are easy to implement but inefficient for fast inference due to correlated samples. Variational inference is efficient with approximated posterior q \u03b8 (w), but assuming a fully factorized Gaussian distribution can lead to large deviations from the true posterior. The simplicity and efficiency of using a family of distributions in design are preferred, despite potential large deviations from the true posterior in convolutional neural networks. Techniques in deep learning often incorporate Bayesian inference concepts, such as Dropout as a regularization tool. Variational dropout extends this idea by learning optimal dropout rates for data, eliminating the need for manual hyper-parameter tuning. Our Adv-BNN combines adversarial training with Bayesian neural networks to enhance model protection. The Bayesian neural network assumes a fully factorizable joint distribution and utilizes normal distributions for posterior q \u00b5i,si (w i ). The prior distribution is simple. The prior distribution for the Bayesian neural network is a Gaussian N(0, s^2_0 I d\u00d7d). Variational inference involves adapting robust optimization to the evidence lower bound (ELBO) during training. The ELBO can be written as an alternative to directly maximizing it. The alternative objective in robust optimization for Bayesian neural networks involves maximizing a specific objective combined with a prior distribution. This approach includes both classification and regression tasks by considering the network output on adversarial samples. The training process differs from standard BNN training by taking expectations over adversarial examples. The training process for robust optimization in Bayesian neural networks involves maximizing a specific objective combined with a prior distribution. Expectations are taken over adversarial examples instead of natural examples, with a randomized PGD attack used to find x adv at each iteration. The KL term is calculated exactly, while the second term is approximated by sampling. The Bayes by Backprop algorithm BID2 is adopted for updating \u00b5 and s. The randomness is decoupled from model parameters to generate unbiased gradient estimators. A new layer called RandLayer is designed for easier integration into the deep learning framework. The new layer RandLayer is designed for easier integration into deep learning frameworks. The local reparameterization trick is used to sample activations instead of weights, reducing variance. The Bayes by Backprop method is found efficient in experiments. We rewrite equations for SGD iterations and sample new weights in each forward propagation for unbiased stochastic gradients. Our Adv-BNN method introduces a factor \u03b1 to adjust regularization for small datasets or large models. By training with min-max robust optimization, noise in the model is canceled out through multiple iterations, improving performance without degradation. The Adv-BNN method introduces a factor \u03b1 for regularization adjustment. It explores the potential benefits of combining randomized network and adversarial training to control local Lipschitz constants for better robustness. The connection between randomized network and local Lipschitz regularization has been derived in previous research. Adversarial training is connected to local Lipschitz regularization by controlling the gradient of the loss over the input. This relationship helps in identifying adversarial examples and improving model robustness. The training method simplifies to controlling the local Lipschitz value on the training set. The adversarial training can be simplified to Lipschitz regularization, but the local Lipschitz value remains large on the test set for complex datasets like CIFAR-10. To address this drawback, combining the randomness model with adversarial training shows a significant improvement in robustness. The performance of robust Bayesian neural networks (Adv-BNN) is tested against strong baselines on various datasets, inspired by adversarial training and BNN methods. The text discusses the improvement in adversarial robustness through the combination of randomness and robust optimization. It compares the method with another defense algorithm and includes hyper-parameters for reproducibility. Experiments were conducted on challenging datasets like STL-10 and ImageNet-143. The dataset used for the experiment contains 18,073 training and 7,105 testing images, all 64\u00d764 pixels. The study compares accuracy under the white box \u221e -PGD attack, adjusting PGD for models with stochastic components. Results are presented in Fig. 2 and Tab. 1, showing accuracy under \u221e -PGD attack on CIFAR-10, STL-10, and ImageNet-143 datasets. When combined with adversarial training, BNN significantly increases testing accuracy by approximately 10% across various datasets. The overhead of Adv-BNN is minimal, only doubling the parameter space. Modifying existing network architectures into BNN is straightforward. The Adv-BNN model enhances accuracy by 10% with minimal parameter increase. Converting architectures to BNN is simple. Transfer attack is studied to assess vulnerability to adversarial examples between different models. In an experiment studying transfer attacks between different models, the affinity between models was analyzed using an affinity matrix. Results showed that some models were more similar and robust to black box attacks than others. In practice, sample efficiency is crucial for accurate prediction. Only 10\u223c20 forward operations are needed for robust results, independent of adversarial distortion. Our algorithm is suitable for large-scale scenarios. Additionally, 20 steps of PGD iterations are sufficient for finding adversarial examples. In testing adversarial defense methods, increasing PGD-steps from 20 to 100 shows decreased effectiveness. Even with 1000 iterations, the accuracy remains stable, indicating minimal benefit for the adversary. Choosing 10-20 ensemble members for prediction is sufficient, and increasing PGD-steps beyond 20 is unnecessary. Combining Bayesian neural networks with adversarial training enhances robustness against attacks. Combining Bayesian neural networks with adversarial training significantly increases robustness against attacks. The method relies on controlled local Lipschitz value for robust classification. Training the BNN with adversarial examples boosts robustness, but the optimal defense solution remains an open problem. The algorithm for white box attack on random networks is derived following guidelines of attacking networks with \"obfuscated gradients\". The Adv-BNN model utilizes majority voting for prediction and implements white-box attacks by maximizing loss on the ground truth label. The RandLayer seamlessly integrates into deep learning frameworks like PyTorch for easy implementation of forward & backward propagation."
}