{
    "title": "BJvVbCJCb",
    "content": "We propose a neural clustering model that learns latent features and clustering without a predefined number of clusters. Using a supervised approach, we agglomerate features towards cluster centroids. The model shows competitive results on text and image data, including MNIST. Results are also provided for fashion-MNIST, 20 newsgroups dataset, and a Twitter dataset. Clustering is a fundamental problem in unsupervised learning, grouping items into clusters based on similarity. Cluster analysis is crucial for grouping items based on similarity, with the ability to differentiate items in a feature space. Various models have been developed, including Gaussian mixture models, DBSCAN, k-means, and hierarchical models. While the cluster analysis community focuses on unsupervised learning of cluster membership, the deep learning community has a history of unsupervised representation learning with models like variational autoencoders. In this paper, the authors propose using noise as targets for agglomerative clustering (NATAC). They sample points in feature space as noise targets and match them with latent features. By progressively removing targets during training, they agglomerate latent features around fewer target centroids. To address training instability, an auxiliary loss is added to prevent model collapse and improve representations. The model's performance across different modalities is explored. Our method, unlike previous approaches, does not require a predefined number of clusters. It utilizes noise as targets for agglomerative clustering and progressively removes targets during training to agglomerate latent features around fewer centroids. An auxiliary loss is added to prevent model collapse and improve representations, addressing training instability. Our method utilizes noise as targets for agglomerative clustering without the need for a predefined number of clusters. The approach, known as \"Noise as Targets\" (NAT), aims to extract useful features by mapping raw representations of objects to points on the unit sphere. This mapping minimizes the distances between the mapped points and the target points, improving representations and preventing model collapse. The NAT approach aims to map raw inputs to a latent space, covering the space well and placing similar inputs near similar targets. The encoder function learns to minimize L2 loss between representations and targets sampled from the unit sphere. Inputs are processed in batches with permuted targets to minimize batch-wise loss. The Hungarian Method is used in the NAT approach to compute optimal assignments of latent representations and targets, pushing similar inputs to similar targets. The model learns to map inputs to latent space closely matching noise targets, achieving unsupervised feature learning. This method performs comparably to state-of-the-art unsupervised representation learning methods, treating targets as cluster centroids for one-to-one assignment with latent representations. The paper proposes a new neural clustering method that involves progressively deleting targets and re-assigning representations to nearby targets while maintaining model stability. This approach allows for many-to-one matchings and cluster formation by agglomerating similar representations. The paper introduces a neural clustering method that involves deleting and re-assigning representations to nearby targets to form clusters while maintaining model stability. This approach allows for many-to-one matchings and encourages similar examples to be assigned to the same target. The paper introduces a neural clustering method that involves an auxiliary objective to prevent model collapse and inform clustering. The auxiliary objective is a reconstruction loss that encourages clustering on similar pixel values. The paper introduces a neural clustering method with an auxiliary reconstruction loss to prevent model collapse and inform clustering. The NATAC model is trained using minibatches, where examples are paired with random targets and matched using the Hungarian method to minimize distances. The delete-and-copy policy is then applied to optimize the assignment of examples to targets. The NATAC model uses a delete-and-copy policy with an auxiliary loss to optimize example-target assignments. Training stops when the number of unique targets no longer decreases. Initial training prioritizes the auxiliary objective for improved performance. The NATAC model utilizes a delete-and-copy policy with an auxiliary loss to optimize example-target assignments. Training transitions through warm-up, transition, and clustering stages to improve model stability and performance. NMI scores from varying latent space dimensionality show improved performance compared to baseline models. Visualizations of latent representations are available in the TensorFlow embedding projector. In experiments, optimal models have latent space dimensions between 4 and 12. Larger dimensions lead to model collapse, while smaller dimensions result in information bottleneck. For example, a 2-dimensional latent space was too small for reliable tweet reconstruction. In our experiments, we evaluated models on four datasets - MNIST, Fashion-MNIST, 20 Newsgroups, and a Twitter dataset. The key evaluation metric used was the normalized mutual information (NMI) score, which measures information gained from cluster membership. Models were trained on concatenated train and validation sets and evaluated on the test set. The study evaluated models on MNIST dataset using NMI score to measure cluster membership information. The models were trained with auxiliary reconstruction loss and small convolutional architectures for encoder and decoder networks. The experiment aimed to analyze the model's ability to learn cluster membership and latent representations. The study evaluated models on MNIST dataset using NMI score to measure cluster membership information. Results show that the model's performance is best when d = 10, indicating the impact of latent space dimensionality. The model outperforms NATAC-k and AE-k in clustering MNIST examples and achieves competitive results compared to other methods. The model learns representations suitable for k-means clustering. The model outperforms NATAC-k and AE-k in clustering MNIST examples by learning representations suitable for k-means clustering. It successfully finds centroids representing different digits, including \"dead centroids\" with few examples not representing digits. The model differentiates between digits with different slopes, indicating distinct latent representations. Fashion-MNIST, introduced in BID35, is a dataset with ten types of clothes, considered more challenging than MNIST. The model's performance on Fashion-MNIST is comparable to MNIST, with some differences in representation learning for k-means clustering. The model still outperforms NATAC-k and AE-k, except when d = 12. The model's representation learning for k-means clustering on Fashion-MNIST differs from MNIST, with some garments being difficult to separate. For example, \"pullovers\" and \"shirts\" cluster together, while \"sandals\" split into flip-flops and high-heeled shoes. The model also identifies \"dead clusters\" with no discernible garment. Further visualizations can be found in the appendix. The 20 Newsgroups dataset consists of 18,846 documents categorized into twenty news categories. A 60:40 temporal train-test split is used, resulting in test set documents different from the train set. NMI is calculated on the news categories. Various clustering models are compared, with the best performing NATAC model evaluated on the entire MNIST dataset. The authors used precision as the evaluation metric for their models, with many top-performing methods assuming uniform class distribution and a set number of clusters. They employed a two-layer fully connected network with hidden layer sizes of 256 and ReLU nonlinearities. Each article was represented as an L2 normalized TF-IDF vector of the 5000 most common words in the train set. The best performing NATAC model achieved an NMI of 0.479 on the entire 20 Newsgroups dataset. The authors evaluated their models using precision as the metric, achieving an NMI of 0.479 on the 20 Newsgroups dataset. Comparisons were made with other methods, including spherical k-means clustering. The NATAC model performed competitively with non-neural methods in terms of NMI on the dataset. The authors built a dataset of 38,309 ASCII-only tweets with 647 different hashtags for testing. They used a character-based Sequence-to-Sequence autoencoder with a bidirectional GRU encoder and achieved an NMI of 0.479 on the Twitter dataset. The authors compared their approach to using spherical k-means along with NATAC-k and AE-k baselines, showing that NATAC-k outperforms NATAC and AE-k models. The latent mapping learned by NATAC models improves on a vanilla autoencoder, but the centroid assignment of a trained NATAC model is less effective than using k-means. Neural models outperform spherical k-means, which is more competitive with neural methods in this experiment than in the 20 Newsgroups experiments. The sensitivity of the NATAC framework to changes in hyperparameters is explored using the model and dataset from the 20 Newsgroups experiments. The study explores the impact of changing hyperparameters on the performance of the NATAC training method using the 20 Newsgroups dataset. The method shows robustness to varying hyperparameter values but extreme changes negatively affect performance. Training 50 NATAC models with the same hyperparameters reveals small variation in NMI and the number of converged clusters. The NMI varies with a small standard deviation of 0.007 regardless of the number of clusters in the NATAC model. The variance in the number of converged clusters is mainly due to dead centroids. The value of \u03b1 is gradually increased from 0 to 1 over several epochs during training. After 100 epochs of pre-training, the model does not benefit significantly from further pretraining. After 100 epochs of pre-training, the model does not significantly benefit from further pretraining. Longer pre-training periods lead to more clusters in the model. Varying the coefficient \u03bb in NATAC models, similar to \u03b1, starts with a small value for warm-up training and gradually increases. In experiments with 20 Newsgroups, \u03bb initial is set to 10^-4 and \u03bb final to 10^-2. The transition in \u03bb occurs simultaneously with the change in \u03b1 during training. The final NMI of NATAC models varies with different values for \u03bb initial and \u03bb final. Larger \u03bb initial values lead to higher NMI scores and more clusters. This novel neural clustering method does not rely on a predefined number of clusters and performs well across modalities. NATAC shows competitive performance across modalities, outperforming baselines on Fashion-MNIST and text datasets. It requires tuning hyperparameters like latent space dimensionality and warm-up training length. Future work includes exploring semi-supervised settings and different agglomerative policies. The latent space can be considered beyond a unit normalized hypersphere. Automatic control of coefficients to eliminate manual hyperparameter setting could be explored. Clustering across different feature spaces may improve representation learning. Experimentation with polar coordinates in Fashion-MNIST dataset showed poorer performance compared to euclidean geometry. Not L2 normalizing the encoder network output was also tested for better latent space representation. The model aimed to improve the latent space representation by incorporating the geometry of noise targets, but this led to the noise targets collapsing. Different modalities used varying hyperparameters, with a large batch size of 100 and a transition phase lasting 100 epochs. The value of \u03bb was adjusted to ensure the NAT loss was 1% of the total loss, with \u03b1 incrementally increased during the transition phase. The model used a batch size of 100 and a transition phase lasting 100 epochs. The value of \u03bb was adjusted to ensure the NAT loss was 1% of the total loss, with \u03b1 incrementally increased during the transition phase. The hyperparameters for each experiment included a warm-up period, a transition period, and specific optimizer settings. Architecture details for the encoder and decoder used in the MNIST experiments were also provided. The model utilized a batch size of 100 and a transition phase lasting 100 epochs. The value of \u03bb was adjusted to ensure the NAT loss was 1% of the total loss, with \u03b1 incrementally increased during the transition phase. The hyperparameters included a warm-up period, a transition period, and specific optimizer settings. Architecture details for the encoder and decoder used in the MNIST experiments were also provided."
}