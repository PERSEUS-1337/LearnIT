{
    "title": "B1guLAVFDB",
    "content": "The paper discusses the importance of understanding deep neural networks and focuses on span recovery as a key primitive. It shows that partial recovery of the row span of the innermost weight matrix is possible in multi-layered networks with ReLU activation functions. Our algorithm can recover vectors in the row span of $\\mathbf{A}$ using poly$(n)$ non-adaptive queries to $M(x) even with differentiable activation functions. Full span recovery is achievable, especially for reasonably wide networks, both random and trained on MNIST data. Additionally, we show how span recovery can be used as an attack to induce misclassification in neural networks. In the given framework, we aim to learn properties of an unknown function f : R n \u2192 R by only accessing the value f (x) for different inputs x. This approach is applicable in various contexts like blackbox optimization, PAC learning, adversarial attacks, and structure recovery. Specifically, we focus on the scenario where f (x) is a neural network with a latent low-dimensional structure M (x) = \u03c3(Ax), where A is a rank k matrix and \u03c3 is a neural network. In the context of learning properties of an unknown function f, we aim to recover the row-span of the weight matrix A, which is a rank k matrix. This span recovery is essential for capturing the relevant subspace of the input to f and is used for effective dimension reduction or designing adversarial attacks. By learning Span(A), we essentially capture the behavior of f on the projection of x onto the row-span of A. The kernel of matrix A can be manipulated to deceive the function, leading to incorrect behavior on perturbed inputs. Various works have approached this issue from an approximation-theoretic perspective, aiming to approximate the function well within a bounded domain. Cohen et al. (2012) proposed an adaptive algorithm for approximating the function when A is a rank 1 matrix. Tyagi & Cevher (2014) and Fornasier et al. (2012) extended this to rank k matrices, providing algorithms with polynomial sample complexity. In this paper, it is proven that span recovery for deep neural networks with high precision can be efficiently achieved with polynomial function evaluations, even for networks with many layers and scalar outputs. Specifically, for deep networks with ReLU activation functions, a subspace of dimension at least k/2 can be recovered with polynomial non-adaptive queries. This is done by utilizing volume bounding techniques and combinatorial analysis of the sign patterns of the ReLU. The gradient matrix of a ReLU network allows for partial span recovery with polynomial function evaluations. A non-adaptive algorithm can recover a subspace of dimension at least k/2 with O(kn log k) queries, even in networks with bottleneck layers. This showcases the ability of deep learning to capture more information than linear models. The algorithm introduced in deep learning allows for more information capture by the model. It is non-adaptive, allowing for pre-chosen evaluation points and successful span recovery. The focus is on proving statements under minimal assumptions, requiring mild assumptions on weight matrices, specifically on orthant probabilities and sign patterns of matrices. The algorithm in deep learning allows for successful span recovery with minimal assumptions on weight matrices, focusing on non-degeneracy conditions and full span recovery for thresholded networks with differentiable activation functions. The algorithm provides an approximation of the true subspace Span(A) with exponentially small distance. The algorithm in deep learning enables successful span recovery for thresholded networks with differentiable activation functions, under certain assumptions. It utilizes a novel gradient-estimation scheme to recover the gradient of M(x) and the span of A efficiently. The algorithm enables successful span recovery for networks with differentiable activation functions. Full recovery is not always possible for small network layer sizes, but realistic architectures allow for full span recovery. Input obfuscation attacks can be applied after successful span recovery to cause misclassifications. Input obfuscation attacks can cause misclassifications by tricking the network into classifying noise as normal inputs with high confidence. This attack injects noise in the null space of A to obfuscate the input without changing the network output. Unlike traditional adversarial attacks, this method aims to change the input significantly without affecting the network's output. The curr_chunk discusses the indicator vector for the nonzero coordinates of x, singular values of a matrix A, condition number of A, identity matrix I, orthogonal projection matrix P, multivariate Gaussian distribution N, and gradient information for function f(x) = g(Ax). It emphasizes the importance of understanding the span of the gradient matrix for span recovery. The curr_chunk discusses recovering the full span of A using gradient information, particularly for neural networks like deep ReLU networks. It highlights the importance of understanding the rank and singular values of the gradient matrix for span recovery. The curr_chunk introduces assumptions for partial span recovery in neural networks like deep ReLU networks. It focuses on the orthant probabilities of the distribution Ag, which follows a multi-variate Gaussian distribution with covariance matrix AA T. The assumption is that the probability of a random vector x lying in a certain orthant of R k is close to uniform. This assumption can be applied to a larger class of distributions. The assumptions for partial span recovery in neural networks involve conditions on weight matrices and non-zero probabilities. Randomly initialized networks satisfy these conditions, even with non-identically distributed entries. The algorithm for recovery in neural networks involves computing gradients using Gaussian vectors and returning the subspace spanned by these gradients. The procedure requires showing the possibility of computing gradients via finite differences with oracle queries to the network. This is demonstrated by proving the existence of gradients for Gaussian vectors and all points within a bounded radius. The algorithm for recovery in neural networks involves computing gradients using Gaussian vectors. It shows that gradients lie in the row-span of A, and can be computed with O(n) queries to the network and in poly(n) runtime. The procedure demonstrates the existence of gradients for Gaussian vectors within a bounded radius. The algorithm for recovery in neural networks involves computing gradients using Gaussian vectors. The gradients lie in the row-span of A, and can be computed with O(n) queries to the network and in poly(n) runtime. The procedure demonstrates the existence of gradients for Gaussian vectors within a bounded radius, showing that Z has rank at least k/2. The algorithm for recovery in neural networks involves computing gradients using Gaussian vectors within a bounded radius. The set of possible sign patterns in the row span of matrix V is much smaller than 2^k, leading to an increase in rank when appending a gradient with a uniform sign pattern. The algorithm in Figure 1 makes O(kn log(k/\u03b4)/\u03b3) queries to M(x) and returns a subspace V of dimension at least k/2 with probability 1 \u2212 \u03b4. The algorithm for recovery in neural networks involves computing gradients using Gaussian vectors within a bounded radius. Even with a binary threshold function at the end, full span recovery of the weight matrix A can be achieved in the networks considered, albeit with an approximation to the subspace due to the non-linearity of the functions used. Our algorithm involves building a subspace V \u2282 R n that approximates the span of A. We recover vectors close to Span(A) but orthogonal to V by querying M for inputs M((I n \u2212 P V )x). The activation functions are assumed to be continuous, twice differentiable, and Lipschitz. The network is non-zero with bounded probability for subspaces of dimension less than k, gradients are not arbitrarily small near the boundary, and conditions for differentiability and bounded derivatives are met. The algorithm involves building a subspace approximating the span of A, querying M for inputs to find vectors orthogonal to V, and ensuring non-zero network evaluation even when projected away from any k-dimensional subspace. The algorithm for recovering the span of networks with differentiable activation functions and 0/1 thresholding involves ensuring non-zero network evaluation at threshold points where gradients are not vanishingly small. The running time is polynomial in log(1/\u03b7), allowing for exponentially small gradient sizes. The condition number of weight matrices and the algorithm's analysis are also discussed. The algorithm for approximate span recovery involves finding a vector x within a subspace V such that \u03c3 V (x) is close but bounded away from the boundary. This is achieved by generating perturbations at the point x from Proposition 4.1. The running time and query complexity of the algorithm are polynomial in N, with a precision parameter \u03b4 affecting the approximation of Span(A). The algorithm involves generating perturbations at a point x in subspace V to estimate the directional derivative \u2207ui \u03c3(x). By carefully bounding gradients and Hessians near x, a linear system is set up to solve for the gradient \u2207\u03c3(x). The algorithm involves generating perturbations at a point x in subspace V to estimate the directional derivative \u2207ui \u03c3(x). By bounding gradients and Hessians near x, a linear system is set up to solve for the gradient \u2207\u03c3(x). Lemma 4.2 states that with probability 1 \u2212 2 \u2212N/n 2 , a vector v \u2208 R n can be found in expected poly(N ) time satisfying certain conditions. Theorem 4.3 shows that Algorithm 2 runs in poly(N ) time with queries to M (x), following specific steps outlined in the text. The algorithm involves generating perturbations at a point x in subspace V to estimate the directional derivative. By bounding gradients and Hessians near x, a linear system is set up to solve for the gradient. The algorithm returns a subspace V \u2282 R n of dimension k such that for any x \u2208 V, we have P Span(A) x 2 \u2265 (1 \u2212 ) x 2. Recovery deteriorates as width decreases and depth increases in neural networks. Gradients are calculated analytically via auto-differentiation at sample points distributed according to a standard Gaussian for feedforward, fully-connected networks with ReLU units. In experiments with decreasing layer sizes for varying sample complexity, the rank of the resulting gradient matrix is computed. Recovery algorithms GradientsRandom, GradientsRandomAda, and GradientsM-NIST are used for training on the MNIST dataset with a size 10 vector output. Gradients are calculated via perturbations at random points for a random network. GradientsRandom, GradientsRandomAda, and GradientsMNIST are used for training on the MNIST dataset with a size 10 vector output. Experimental outcomes are similar across all scenarios, with span recovery deteriorating in networks with small widths and multiple layers as depth increases. This holds true for networks randomly initialized with Gaussian weights or trained on MNIST, using adaptive or non-adaptive recovery algorithms. Full span recovery is challenging in small networks with widths less than 10 on MNIST, achieving below 80% accuracy. However, for more realistic networks with moderate or high widths, full span recovery is easier, indicating vulnerability to attacks. Experimental results show quick full span recovery of the first-layer weight matrix with rank 80 in all cases with less than 100 samples on MNIST. Full span recovery is achieved almost immediately in all cases, with less than 100 samples, on the MNIST dataset. Span recovery algorithms are demonstrated as an attack to fool neural networks by converting images into noisy versions without changing the output, leading to adversarial attacks. Theorem 5.58 of Vershynin (2010) states that if entries A are drawn i.i.d. from a sub-Gaussian isotropic distribution, then the rows of A are isotropic. By conditioning on rows having norm \u221an, the theorem conditions hold for a large re-scaling constant. Setting n > \u2126(k^3), it follows that \u03ba(A) < (1 + 1/(100k)), which holds if A has orthogonal rows. Ag is distributed as a multi-variate Gaussian with covariance A^T A, and the pdf is x^T x for an identity covariance Gaussian N(0, I_k). The pdf of an identity covariance Gaussian is denoted by x^T x. A lower bound for p(x)/p(x) is established for x with x^2 \u2264 16k. By spherical symmetry of Gaussians, the sign pattern of Ag is shown to be non-zero with probability 1. The second part of Assumption 2 is demonstrated by showing that Pr[M(x) = 0] \u2264 1 - \u03b3\u03b4/100 for entries drawn independently from a continuous symmetric distribution. The text discusses the distribution of values in a neural network, showing that certain conditions lead to non-zero results with high probability. An algorithm is presented that can compute gradients efficiently with a high success rate. The ReLU function is mentioned, along with the differentiability of the network within a certain range. The text discusses the distribution of values in a neural network, showing that certain conditions lead to non-zero results with high probability. An algorithm is presented that can compute gradients efficiently with a high success rate. The ReLU function is mentioned, along with the differentiability of the network within a certain range. If \u2207M (g) exists, there is an > 0 such that M is differentiable on B (g). With good probability, if g \u223c N (0, I n ), then M (g) is continuous in the ball B (g) = {x \u2208 R n | x \u2212 g 2 < } for some computed value. Conditioning on a sign pattern for each layer results in a linear network, leading to a linear program with constraints and variables. The text discusses constraints and variables in a neural network, focusing on the feasible polytope P representing input points satisfying activation patterns. A non-linear constraint x 2 \u2264 (nd) 10c is introduced, with a feasible region B. The Lesbegue measure V (P * ) of the region P is bounded, showing that P * \u2282 P. Rotation of P by y \u2192 y 2 \u00b7 e 1 \u2208 R n does not change the volume of the feasible region. The text discusses constraints and variables in a neural network, focusing on the feasible polytope P representing input points satisfying activation patterns. Rotation of P by y \u2192 y 2 \u00b7 e 1 \u2208 R n does not change the volume of the feasible region. The resulting region is contained in a Eucledian box with bounded Lesbegue measure. The weight matrices are assumed to have polynomially many bits specified. The text discusses constraints and variables in a neural network, focusing on the feasible polytope P representing input points satisfying activation patterns. Rotation of P by y \u2192 y 2 \u00b7 e 1 \u2208 R n does not change the volume of the feasible region. The resulting region is contained in a Eucledian box with bounded Lesbegue measure. The weight matrices are assumed to have polynomially many bits specified. Note that each matrix is at most 2 n C for some constant C, leading to the probability that a multivariate Gaussian g \u2208 N (0, I n ) is \u03b7 close to the boundary for any discontinuity in M (x) being at most \u03b72. The text discusses the feasibility of perturbing points in a neural network without hitting boundaries. By ensuring that the network is differentiable in a ball around a point, it is shown that perturbing the point by a vector within a certain norm does not lead to hitting boundaries. The weight matrices are assumed to have polynomially many bits specified, leading to the conclusion that the network is differentiable in the ball as needed. The text discusses the differentiability of a ReLU network in a ball around a point, showing that perturbing the point within a certain norm does not hit boundaries. By setting up a linear system, the directional derivative can be solved exactly in polynomial time. This ensures that perturbing points in the neural network does not lead to hitting boundaries. The text discusses the differentiability of a ReLU network in a ball around a point, showing that perturbing the point within a certain norm does not hit boundaries. By setting up a linear system, the directional derivative can be solved exactly in polynomial time. This ensures that perturbing points in the neural network does not lead to hitting boundaries. The number of sign patterns with at most k/2 non-zero entries is bounded, and an algorithm is presented that returns a subspace of dimension at least k/2 with high probability. The text discusses the differentiability of a ReLU network in a ball around a point, showing that perturbing the point within a certain norm does not hit boundaries. By setting up a linear system, the directional derivative can be solved exactly in polynomial time. This ensures that perturbing points in the neural network does not lead to hitting boundaries. The number of sign patterns with at most k/2 non-zero entries is bounded, and an algorithm is presented that returns a subspace of dimension at least k/2 with high probability. Massart (2000) discusses the probability of certain events occurring, and perturbing x by a certain amount results in a positive change in \u03c3. The text discusses the differentiability of a ReLU network in a ball around a point, showing that perturbing the point within a certain norm does not hit boundaries. By setting up a linear system, the directional derivative can be solved exactly in polynomial time. The number of sign patterns with at most k/2 non-zero entries is bounded, and an algorithm is presented that returns a subspace of dimension at least k/2 with high probability. The last paragraph delves into mathematical calculations involving the gradient of the function and probabilities of certain events occurring. The text discusses perturbing a point within a certain norm in a ReLU network, ensuring the gradient remains within bounds. By setting up a linear system, the directional derivative can be solved exactly. The number of sign patterns with at most k/2 non-zero entries is bounded, and an algorithm returns a subspace of dimension at least k/2 with high probability. Mathematical calculations involving the gradient and probabilities are also explored. The text discusses perturbing a point within a certain norm in a ReLU network to ensure the gradient remains within bounds. An algorithm returns a subspace of dimension at least k/2 with high probability, with mathematical calculations involving the gradient and probabilities. The process involves iteratively applying Lemma 4.2 to construct a subspace V \u2282 R^n of dimension k, ensuring linear independence of vectors v_i and satisfying certain properties for each i \u2208 [k]. The text discusses constructing a subspace V in R^n of dimension k with linearly independent vectors v_i. By perturbing a point within a certain norm in a ReLU network, the gradient can be kept within bounds. The algorithm ensures the gradient remains within bounds by iteratively applying Lemma 4.2."
}