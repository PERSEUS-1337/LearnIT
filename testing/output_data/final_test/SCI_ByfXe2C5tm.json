{
    "title": "ByfXe2C5tm",
    "content": "Symbolic logic enables interpretable rule-based reasoning systems that can be enhanced with prior knowledge. Combining neural networks and logic programming in natural language question answering involves using a Prolog prover with pretrained sentence encoders and fine-tuning via Evolution Strategies for multi-hop reasoning. The goal is to enable rule-based reasoning on natural language by applying multi-hop reasoning. The system outperforms strong baselines in question answering tasks by inducing domain-specific rules from training data. For example, combining statements like \"Socrates was born in Athens\" and \"Athens belongs to Greece\" requires multi-hop reasoning to deduce the answer to a question like \"Where was Socrates born?\" Recent work addresses challenges in rule-based reasoning on natural language by leveraging deep learning methods capable of handling linguistic variability. However, the black-box nature of neural networks makes interpretation difficult. In contrast, logic programming languages like Prolog use symbolic rules for interpretable reasoning, allowing users to incorporate external knowledge easily. NLPROLOG is a system that combines symbolic reasoning and rule-learning with pretrained sentence representations for rule-based multi-hop reasoning on natural language input. It integrates global and local interpretation, allows for easy integration of prior knowledge, and does not require transforming natural language text into formal logic. The method includes an external non-differentiable theorem prover that considers similarities between symbols, achieved by modifying a Prolog interpreter to support weak-unification. The NLPROLOG system combines symbolic reasoning with pretrained sentence representations for rule-based multi-hop reasoning on natural language input. It utilizes sentence encoders initialized with pretrained embeddings and fine-tunes them for a question answering task using Evolution Strategies as a gradient estimator. This approach allows for learning domain-specific logic rules from natural language data and enables easy parallelization of training. The NLPROLOG system integrates Prolog-like reasoning with pretrained sentence embeddings and an external logic prover, utilizing weak unification and Gradual Rule Learning (GRL) to learn First-Order Logic (FOL) rules from entailment. The system is evaluated on Question Answering (QA) datasets, showing comparable performance to state-of-the-art neural QA models and enabling the creation of an ensemble model that outperforms individual models. This work focuses on multi-hop reasoning for QA, combining logic and distributed representations. Multi-hop reasoning in neural QA models involves iteratively updating a query embedding by integrating information from context sentences using attention mechanisms. Differentiable memory structures are used to track entities for temporal reasoning, achieving state-of-the-art results in reasoning-focused QA tasks. Dynamic Memory Network and Neural Turing Machine are examples of models utilizing differentiable memory structures for multi-hop question answering. The use of differentiable memory structures in neural QA models improves performance on multi-hop reasoning tasks. Incorporating coreference information enhances the model's ability to handle complex QA problems. However, interpreting the reasoning steps and imposing strong priors on the procedures remain challenging. Evaluation on natural language data sets is needed to assess real-world performance. Investigating the combination of formal logic and distributed representations is a potential direction for future research. The combination of formal logic and distributed representations has a long tradition and has been applied to tasks like Recognizing Textual Entailment and Semantic Textual Similarity. Neural multi-hop reasoning models have been extensively studied in Knowledge Base Completion tasks. The construction of a densely connected KB from text for QA tasks is challenging due to natural language ambiguity. The Natural Theorem Prover (NTP) approach is closely related to our research. Our approach involves using a non-differentiable prover and Evolution Strategies to scale theorem proving for question answering, unlike the Natural Theorem Prover (NTP) which struggles with scalability due to exponential growth of possible proofs. Another approach by BID0 uses Natural Logic to search a large KB for a single statement, which differs from our method of learning rules combining multiple facts. Our approach simplifies the preprocessing step by using Open Information Extraction (Open IE) textual patterns instead of transforming natural language to logic. This differs from traditional systems like Watson and COGEX, which require a transformation to logical form. The OPENQA system utilizes a mixture of handwritten and automatically obtained operators to parse, paraphrase, and rewrite queries for large-scale question answering on knowledge bases with Open IE triples. Our method focuses on combining multiple facts by learning logical rules using a custom Prolog engine. Prolog programs consist of rules in the form of Horn clauses, with the central procedure being unification. Fact atoms are rules with a body size of zero, and we disregard negation and disjunction in our approach. The central procedure of Prolog is unification, which aims to find variable substitutions making two atoms syntactically equal. Prolog's proof algorithm, backward-chaining, starts from a goal atom and generates subgoals by applying suitable rules. These rules are unified with the goal atom, and the resulting variable substitutions are applied to create new subgoals. For example, applying the rule country(X, Y) \u21d0 born_in(Y, X) to the goal country(Greece, Socrates) yields the subgoal born_in(Socrates, Greece), which is then processed similarly. The central procedure of Prolog is unification, which finds variable substitutions making two atoms syntactically equal. Backward-chaining generates subgoals by applying rules, resulting in a set of rule applications and variable substitutions called proof. The number of possible proofs is exponential in the number of predicate and entity symbols. Applying Prolog to natural language question answering requires accounting for semantic similarities and ambiguities. NLPROLOG combines inference based on weak unification and distributed representations to allow reasoning on natural language statements. The statements are transformed into triples using Open IE, and symbols are embedded into a vector space for estimation. The symbols in NLPROLOG are embedded into a vector space to estimate similarities, which are used for proof and training. The process is illustrated in FIG1, showing the interplay of components. An encoder function is used to embed symbols, with cosine similarity being the preferred metric. In NLPROLOG, symbols are embedded into a vector space for estimating similarities used in proof and training. An encoder function populates embeddings with pretrained sentence vectors. A separate lookup table is introduced for predicate symbols of rules and goals to account for different semantics. Fine-tuning the similarity function is proposed by updating symbol embeddings. Evolution Strategies with Adam are used for optimization due to the non-differentiable proof search step. Evolution Strategies with Adam are employed to estimate weight updates for NLPROLOG. The model is trained to determine if a Prolog program entails the truth of a candidate triple, assigning high probabilities to true triples and low probabilities to false ones. The reward is modeled as J(\u03b8) = yp(c|R; \u03b8), where y \u2208 {\u22121, 1} is the gold label. The reward in NLPROLOG is modeled as J(\u03b8) = yp(c|R; \u03b8), where y \u2208 {\u22121, 1} is the gold label. To estimate p(c|R; \u03b8), exhaustive search for proofs up to depth D is conducted, yielding proofs with success scores. Rule templates are used for Inductive Logic Programming to learn rules from training data, allowing NLPROLOG to model relations between predicates efficiently. In NLPROLOG, rule templates are used for Inductive Logic Programming to model relations between predicates efficiently. The experiments showed that proofs involving rule templates often result in a drop in proof success score, leading to rule embeddings remaining close to their initialization. The Gradual Rule Learning (GRL) algorithm addresses the drop in proof success score caused by rule templates in NLPROLOG. GRL segments training into phases based on rule complexity, ensuring better estimation of p(t|F ; \u03b8) in each phase. The Gradual Rule Learning (GRL) algorithm segments training into phases based on rule complexity to estimate p(t|F ; \u03b8) more accurately. The method is evaluated on QA datasets BABI-1K-STATIC and subsets of WIKIHOP BID44, focusing on predicates like publisher, developer, and country for true multi-hop reasoning. Training and validation question counts are provided for each predicate. Scores are reported for the validation set due to the unavailability of the test set in WIKIHOP. The study focuses on multi-hop reasoning using the GRL algorithm on QA datasets like WIKIHOP. It involves queries with query entities, candidate entities, and support documents transformed into natural language triples. Baseline models BIDAF and FASTQA are used for comparison. The implementation is done using the JACK QA framework. The study utilizes the JACK QA framework to train separate models for selected predicates of WIKIHOP. Modified versions are evaluated to improve performance by transforming predicted answers and candidates into vectors. The experiments on WIKIHOP are more significant due to natural linguistic variability, while BABI-1K-STATIC tasks are used for studying NLPROLOG in a controlled environment. The study uses the JACK QA framework to train models on selected predicates of WIKIHOP. Tasks in BABI-1K-STATIC do not require reasoning about a changing world state, making them suitable for NLPROLOG. The tasks are transformed into triples and used as input for NLPROLOG. Tasks QA4 and QA15 require entity output, while QA17 and QA18 are binary classification tasks. The study does not compare results systematically with other methods on individual BABI tasks. The study utilizes BABI-1K-STATIC for ablation experiments, noting that NLPROLOG performs similarly or better than other methods except on QA4. A modified reward function is used for questions with answer candidates, inspired by Bayesian Personalized Ranking. The reward function is adjusted based on the aggregation function used. Results for selected predicates of WIKIHOP are presented in Table 1. The study presents results for selected predicates of WIKIHOP in Table 1. NLPROLOG is consistently outperformed by neural QA models on every predicate, with a lack of diverse rules attributed to the multi-hop nature of the data. NLPROLOG struggles to find meaningful rules for the predicates developer and publisher, resulting in very few proofs involving rules compared to the predicate country. NLPROLOG struggles with multi-hop reasoning for predicates developer and publisher, but performs well on country. NLPROLOG outperforms other models on problems requiring multi-hop reasoning. If NLPROLOG's prediction doesn't use rules, it relies on nearest neighbor search. The prediction method involves a nearest neighbor search among fact triple embeddings. A system combines neural QA models FASTQA and BIDAF with NLPROLOG for better predictive accuracy. The system utilizes NLPROLOG for multi-hop reasoning when necessary and neural QA models for pattern matching in other cases. Results are shown in Table 1 for different predicates on the development set of WIKIHOP. Ensembling a neural QA model with NLPROLOG improved predictive accuracy for different predicates on the WIKIHOP development set. Errors in NLPROLOG were mainly due to issues with fact extraction by OPENIE, leading to unanswerable queries for certain predicates. The predictive accuracy of FASTQA and NLPROLOG was evaluated on a test set of the country predicate. Removing all documents mentioning the query entity resulted in an increase in FASTQA accuracy by approximately 1% and a decrease in NLPROLOG accuracy by approximately 11%. NLPROLOG achieved an accuracy of 51.61% on hard problems that cannot be solved with a simple heuristic, compared to 46.77% by FASTQA. The neural models and NLPROLOG complement each other well, with neural models compensating for missing information and NLPROLOG being less affected by spurious correlations. Experiments on BABI-1K-STATIC show the impact of the GRL training procedure, with different phases affecting the results. The error analysis in Fig. 4 highlights crucial missing information due to an OpenIE tool error. The curr_chunk discusses errors in the OpenIE tool affecting information extraction in experiments. Different error types like A(mbiguous), I(nfo), R(ule), and E(mbedding) are identified. Manual rule templates are experimented with, showing varying accuracy scores on BABI tasks. The impact of rule templates on NLPROLOG performance is evaluated on bAbI-1k-static. The curr_chunk discusses the effectiveness of rule templates in improving results for different problems in GRL. Phase 1 of GRL may not contribute to all problems, but it strongly improves results for QA15 and QA18. Manual rule templates show that training with ES is helpful even when sufficient rules are provided. The model without rules can solve over 90% of problems in QA18. The curr_chunk discusses the development of NLPROLOG, a system for rule-based reasoning on natural language input. It highlights the importance of optimizing rule templates as a hyperparameter for solving different problems. Example proof trees generated by NLPROLOG are shown, demonstrating the application of transitive rules. Noise introduced by the Open IE process is noted, along with the ability of NLPROLOG to learn domain-specific rules from training data. NLPROLOG combines a symbolic prover with pretrained sentence embeddings and is trained with Evolution Strategies. It can learn domain-specific rules and outperforms strong baselines in QA tasks. The system's expressiveness could be extended by incorporating different symbolic provers, such as one for temporal logic, to model temporal dynamics in natural language. Future improvements in symbolic provers, Open IE systems, and pretrained embeddings are also of interest. NLPROLOG aims to enhance its performance by incorporating improvements in symbolic provers, Open IE systems, and pretrained sentence representations. Evaluating the proposed method on tasks like knowledge graph reasoning without the noise from Open IE step would be beneficial. Additionally, studying NLPROLOG's behavior with multiple WIKIHOP query predicates is of interest. The weak unification algorithm in Spyrolog without occurs check has a worst-case complexity that is exponential in the depth of the proof search. NLPROLOG only attempts to unify symbols with a similarity greater than a user-defined threshold \u03bb to keep things tractable. The success score S is used to adjust \u03bb during the search step for a statement q. The optimization of proof search in Spyrolog involves adjusting the threshold \u03bb based on the success score S to prune the search tree effectively. Training on BABI-1K focuses on predicate symbols, while WIKIHOP embeddings are initialized using WIKI-UNIGRAMS model. All experiments use the same set of rule templates. The experiments involved using rule templates with specific forms and a similarity threshold of 0.3. Optimization steps evaluated perturbations on training problems using GRL with three phases. The final phase for predicates publisher and developer used 1,000 mini-batches. Rule usage was encouraged using aggregation functions, switching to product in the last phases of GRL. Figure 4 shows a Venn diagram of error sets per predicate."
}