{
    "title": "Hk5elxbRW",
    "content": "The top-$k$ error is a key measure in machine learning and computer vision, often evaluated using deep neural networks trained with cross-entropy loss. While cross-entropy is optimal with infinite data, using a loss function tailored for top-$k$ classification can enhance performance with limited and noisy data. Our empirical evidence suggests that smooth loss functions with non-sparse gradients are essential for deep neural networks. We introduce a family of smoothed loss functions for top-$k$ optimization in deep learning, with cross-entropy being a special case. Evaluating these functions is computationally challenging, but we provide an algorithm with a time complexity of $\\mathcal{O}(k n)$ using polynomial algebra and a divide-and-conquer approach. Additionally, we propose a novel approximation for fast and stable algorithms on GPUs. Comparing cross-entropy with margin-based losses in various noise and data size scenarios, we find that our loss functions perform well, particularly for $k=5$. Our investigation shows that the loss function for top-k optimization is more robust to noise and overfitting than cross-entropy. In machine learning, label confusion can arise from various factors, leading to the need for models to predict the k most likely labels. Learning such models is a longstanding task in machine learning. In machine learning, models trained with cross-entropy have shown exemplary capacity to approximate data distribution accurately, especially in correctly labeled large datasets. Deep convolutional neural networks trained with cross-entropy have achieved remarkable success on the top-5 error in the ImageNet challenge. Despite not being tailored for top-5 error minimization, cross-entropy is top-k calibrated for any k, making it effective in large data settings. However, in cases with limited data, learning large models with cross-entropy can lead to overfitting. To address overfitting on incomplete or noisy labels, a new family of top-k classification loss functions for deep neural networks is introduced. Inspired by multi-class SVMs, this loss function creates a margin between the correct top-k predictions. Examples of label confusion in images from the ImageNet validation set highlight the need to predict more than a single label per image. The need to predict multiple labels per image is motivated by the margin between correct and incorrect predictions. Traditional top-k loss functions perform poorly with deep neural networks due to lack of smoothness in derivatives. To address this, the loss is smoothed with a temperature parameter, increasing time complexity. An algorithm with O(kn) time complexity is presented, providing insights for numerical stability in the forward pass and a novel approximation for instabilities in the backward pass. Our investigation introduces a novel approximation for the backward pass, showing that the top-k loss outperforms cross-entropy with noisy labels or limited data. Previous work on top-k loss functions focused on shallow models, with approaches like convex surrogates and truncated re-weighted top-k loss. The performance difference decreases with correctly labeled data, aligning with theoretical results. The authors study top-k loss functions derived from cross-entropy and hinge losses, proving that cross-entropy is top-k calibrated for any k. This property explains why cross-entropy performs well on top-5 error in large datasets. The experiments are conducted on linear models or fine-tuned pre-trained deep networks, but the goal is to design loss functions for training deep neural networks from random initialization. Smoothing is a useful technique in optimization. In related work, smoothing techniques have been used to improve optimization speed and generalization in various algorithms, including binary SVMs, structured prediction, and deep neural networks. Different approaches such as introducing temperature parameters, Moreau-Yosida regularization, modifying activation functions, and adding entropy terms to the loss function have been proposed to achieve smoother optimization processes. These techniques aim to enhance the convergence speed and performance of the algorithms. In this work, smoothing techniques are shown to be necessary for neural networks to perform well with the loss function. The focus is on top-k error minimization and multi-class SVM background. The prediction is based on the index with the highest score, and the classification loss incurs a binary penalty. The classification loss incurs a binary penalty by comparing the prediction to the ground truth label. A surrogate loss is used as an upper bound on the loss for optimization. The multi-class SVM loss is suggested as an upper bound on the loss. The surrogate loss is zero if the ground truth score is higher than all other scores by a margin of at least one. The value of 1 as a margin is arbitrary and can be changed to \u03b1 for any \u03b1 > 0. Proposition 8 in Appendix D.2 shows how \u03b1 and the quadratic regularization hyper-parameter choices are interchangeable. The framework is generalized to top-k classification, where k \u2208 {1, ..., n \u2212 1}. The notation s [p] refers to the p-th largest element of s, and s \\p to the vector (s 1 , ..., s p\u22121 , s p+1 , ..., s n ) \u2208 R n\u22121. Y (k) denotes the set of k-tuples with k distinct elements of Y. Bold font is used for a tuple \u0233 \u2208 Y (k) to distinguish it from a single label \u0233 \u2208 Y. Prediction is based on the top-k largest scores. The loss function for top-k classification introduces a surrogate loss to create a margin between the ground truth and the k-th largest score. This surrogate loss is challenging to optimize due to its non-smooth nature and weak derivatives with at most two non-zero elements. The loss function for top-k classification introduces a challenging surrogate loss with weak derivatives having at most two non-zero elements. Coupling this with a deep neural network often leads to poor performance. The optimization difficulty explains the lack of exploration of SVM losses in deep learning. A proposed smoothing technique aims to address these issues and experimental evidence suggests improved performance in practice. The text introduces notation for top-k classification and discusses a smoothing technique with a temperature parameter to improve loss function performance. The smooth loss function has properties that make it advantageous over its non-smooth counterpart. The smooth loss function L k,\u03c4 has key properties such as being infinitely differentiable with non-sparse gradients. It collapses to maximizations as \u03c4 approaches 0, and is an upper bound on \u039b k. It serves as a valid surrogate loss for minimizing \u039b k and is a generalization of cross-entropy for different values of k and temperature. Influence of temperature \u03c4 on training a DenseNet 40-12 on CIFAR-100 is examined. High temperatures lead to non-sparse gradients, aiding model training. Accuracy is good with \u03c4 \u2265 0.01, but too small \u03c4 hinders convergence. Smoothness alone is not enough for effective training. The experimental evidence suggests using L k,\u03c4 over l k for training neural networks, despite L k,\u03c4 being computationally expensive due to summations over large cardinalities. To address this challenge, reframing the problem to reveal exploitable structure is proposed. The elementary symmetric polynomials \u03c3 i (e) are defined for a vector e \u2208 R n and i \u2208 {1, .., n}. They are used to re-write L k,\u03c4 for efficient computation in training deep neural networks. Existing algorithms for evaluating these polynomials are designed for CPU with double floating point precision. The text discusses the efficient computation of elementary symmetric polynomials \u03c3 i (e) for training deep neural networks with single floating point precision and GPU parallelization. The approach connects \u03c3 i (e) to a polynomial expansion to compute the coefficients \u03b1 n\u2212k for obtaining the value of \u03c3 k (e). The text discusses the efficient computation of elementary symmetric polynomials \u03c3 i (e) for training deep neural networks with single floating point precision and GPU parallelization. A divide-and-conquer approach is used to compute the expansion of P by merging two branches of the recursion. The method computes all coefficients (\u03c3 i ) 1\u2264i\u2264n instead of just the required (\u03c3 i ) k\u22121\u2264i\u2264k. By leveraging the relationship \u03c3 i (e) = \u03c3 n (e)\u03c3 n\u2212i (1/e) and Vieta's formula, the computation of \u03c3 k (e) only requires the first k coefficients of the polynomial Q. The algorithm efficiently computes \u03c3 k (e) by utilizing the first k coefficients of Q, resulting in a time complexity of O(kn). With O(log(n)) levels of recursion that can be parallelized, the algorithm scales well on GPUs. Algorithm 1 initializes polynomials and merges branches through parallel polynomial multiplications. Adjusting coefficients in step 10 can achieve a time complexity of O(n log(k) 2) with Fast Fourier Transform for polynomial multiplications. The algorithm efficiently computes \u03c3 k (e) by utilizing the first k coefficients of Q, resulting in a time complexity of O(kn). Algorithm 1 initializes polynomials and merges branches through parallel polynomial multiplications. Adjusting coefficients in step 10 can achieve a time complexity of O(n log(k) 2) with Fast Fourier Transform for polynomial multiplications. Obtaining numerical stability in single floating point precision requires special attention, described in Appendices B.2.1 and B.3.3. The backward pass in Appendix B.3.3 optimizes memory allocation by utilizing results from the forward pass, reducing operations and speeding up the process significantly. The derivative notation is introduced, and equation FORMULA5 explains the relationship between derivatives. The backward pass optimizes memory allocation by utilizing results from the forward pass. The derivative notation is introduced, and a recursive relationship for computing derivatives is explained. The backward pass optimizes memory allocation by utilizing results from the forward pass. The computation can be stabilized by an approximation with significantly smaller overhead, avoiding numerical instabilities. Theoretical results suggest that Cross-Entropy (CE) is an optimal classifier in the limit of infinite data, but label noise complicates data distribution estimation with finite samples. The behavior of CE and L k,\u03c4 is explored with varying label noise and training data. In this experiment, the impact of label noise on CE and L 5,1 is investigated using the CIFAR-100 dataset. The dataset contains 60,000 RGB images with 50,000 samples for training-validation and 10,000 for testing. The study explores the behavior of CE and L k,\u03c4 with varying label noise and training data sizes. The experiments are conducted using Pytorch and Nvidia Titan Xp cards. In this experiment with the CIFAR-100 dataset, 50,000 samples are used for training-validation and 10,000 for testing. There are 20 \"coarse\" classes, each with 5 \"fine\" labels. Images are preprocessed before being input to the network with data augmentation techniques applied. Label noise is introduced by replacing fine labels with others from the same coarse class with a certain probability. The study evaluates loss functions using DenseNet 40-40 architecture on CIFAR-100 dataset. Label noise is introduced, and CE shows overfitting, benefiting from early stopping. Validation set of 5,000 images is used to monitor accuracy, with results averaged over three runs. The study compares the performance of L 5,1 and CE loss functions on a test set with noisy labels. L 5,1 outperforms CE in top-5 accuracy with noisy labels, while CE performs better in top-1 accuracy without noise. L 5,1 also shows better top-1 error with noise, indicating its preference when labels are only informative about top-5 predictions. The ImageNet dataset used in the study contains ambiguity and noise in labels, allowing for exploration due to its large number of training samples. The study uses subsets of ImageNet dataset with various sizes for training and validation. Images are resized to 256 pixels, cropped to 224x224, and normalized. Random crops and flips are used during training, while a ten-crop procedure is used during testing. Subset sizes range from 64k to 1.23M images, each containing all 1,000 classes with a balanced number of images per class. The study uses subsets of the ImageNet dataset with various sizes for training and validation. The largest subset has a slight imbalance similar to the full ImageNet dataset. A ResNet-18 model is trained using Stochastic Gradient Descent with a batch size of 256 for 120 epochs. Nesterov momentum of 0.9 is used, with the learning rate divided by ten at epochs 30, 60, and 90. Training on the whole dataset takes about a day and a half. Using multiple crops in training requires a probability distribution over labels for each crop, which is then averaged to compute the final prediction. The standard method involves using a softmax activation for top-1 predictions, but for top-5 predictions, the probability of a label being part of the top-5 should be marginalized over all combinations. This can be computed with algorithms to evaluate \u03c3 k and its derivative. The results in Appendix C show that using multiple crops in training improves top-5 error rates by at least 0.2% for all loss functions. A new family of loss functions, including L 5,0.1, outperforms cross-entropy (CE) in small data settings. However, as the dataset size increases, CE performs slightly better in the full data setting. Non-sparsity is crucial for loss functions to work well with deep neural networks, and efficient algorithms have been developed to compute the smooth loss. In this section, efficient algorithms are presented to compute the smooth loss and its gradient, showing that the smooth top-5 loss function is more robust to noise and overfitting than cross-entropy with limited training data. The insight of smoothing the surrogate loss function aids in training deep neural networks and can be applied to other loss functions, such as smoothed SVM losses for structured prediction problems. The loss function is defined with parameters \u03c4 > 0 and k \u2208 {1, ..., n \u2212 1}, and results are derived with a specific loss function formulation. The text discusses the convergence of a specific function and its upper bound in relation to a loss function with parameters \u03c4 > 0 and k \u2208 {1, ..., n \u2212 1}. The results are derived using a specific loss function formulation. The text discusses the convergence of a specific function and its upper bound in relation to a loss function with parameters \u03c4 > 0 and k \u2208 {1, ..., n \u2212 1}. Suppose k = 1. Let s \u2208 R n and y \u2208 Y. We introduce y * = argmax y\u2208Y {\u2206 1 (\u0233, y) + s\u0233}. Now suppose k \u2265 2. We construct an example (s, y) such that L k,\u03c4 (s, y) < l k (s, y) by setting y = 1 and manipulating variables \u03b1 and \u03b2. The text discusses proving the existence of x > 0 to conclude a proof by continuity. It involves a monotonically increasing function and an upper bound on prediction loss. The proof includes introducing random variables and sets, and replacing values based on probability distributions. The text introduces a scoring function and sets of scores, with probabilities and set cardinalities derived. It concludes with Proposition 4 stating L k,\u03c4 is an upper bound on prediction loss \u039b k. Loss \u039b k is discussed, with cases where \u039b k (s, y) = 0 and \u039b k (s, y) = 1 explored. The text introduces Z y and T k as sets, and discusses the time complexity of obtaining coefficients of polynomials. The complexity of computing the coefficients of polynomials using a divide-and-conquer algorithm is discussed. To ensure numerical stability, all computations are maintained in the log space. The \"log-sum-exp\" trick is used to ensure stability in computing exp(x1) + exp(x2). The backward recursion of Algorithm 2 becomes unstable under certain conditions. A p-th order approximation to the gradient is defined to reduce errors. The recursion in FORMULA7 can become unstable for large elements in e. The ratio DISPLAYFORM5 decreases quickly with p, leading to stable computation of equation FORMULA5. However, it is challenging to control the approximation error at runtime. The instability of \u03b4 k,i is addressed by using a p-th order approximation. This heuristic has been found to work well in practice, changing the complexity of the forward pass to O((k + p)n). The Summation Algorithm (SA) is an alternative to the Divide-and-Conquer (DC) algorithm for evaluating elementary symmetric polynomials, with the inner loop being parallelizable. The Summation Algorithm (SA) and Divide-and-Conquer (DC) algorithm are compared for evaluating elementary symmetric polynomials. The execution time of both algorithms is compared on a GPU using different parameters. The speed of the forward pass and backward pass using Automatic Differentiation (AD) and a Custom Algorithm (CA) is also evaluated. The Custom Algorithm (CA) and Divide and Conquer (DC) algorithm are compared for evaluating symmetric polynomials. DC offers nearly logarithmic scaling with n due to parallelization, while SA scales linearly with n. DC has O(log(n)) levels of recursion with parallel operations, showing near-logarithmic scaling with n. The advantage of Algorithm 2 over automatic differentiation is demonstrated in runtimes, especially in the use case of ImageNet. In the context of evaluating symmetric polynomials, the numerical stability of the algorithms is investigated, focusing on the stability of the algorithm in the machine learning context. Overflow is a critical issue as it can cause the weights of the learning model to diverge. Compensation algorithms are not considered for improving stability, and the assumption of stability in the algorithm is not verified in the use case discussed. Numerical stability is tested in a machine learning context by evaluating the weights of a learning model when an overflow occurs. Using a random mini-batch of images from ImageNet, the stability of the SA and DC algorithms is compared for different temperature parameters. DC proves to be more stable than SA in the log-space, remaining stable in single floating point precision until a certain threshold. The probability of selecting a label for the top-k prediction is considered by marginalizing over all k-tuples containing the label. The unnormalized probability for selecting a label in the top-k prediction can be computed and rescaled by a temperature parameter. Normalization is done by dividing the unnormalized probabilities by their sum. Insights on the choice of temperature parameter are discussed, with low values suggested for sound learning objectives. However, optimization with a high temperature value can be challenging and may fail in practice. Optimization can be challenging with a high temperature value, leading to potential failure. It is important to use a low temperature that allows for satisfactory optimization while still preserving a sensible trade-off between regularizer and loss. Adjusting the regularization hyperparameters accordingly is crucial for achieving this balance. The choice of temperature parameter can impact the scale of the loss function, influencing the speed of convergence and training/validation accuracies. Optimization at high temperature values can be challenging, requiring adjustments to regularization hyperparameters for a balance between regularizer and loss. The choice of temperature parameter impacts the loss function scale, affecting convergence speed and accuracies. An annealing scheme is typically used in continuation methods to enhance approximation quality, but setting a fixed temperature value can yield similar performance without the need for extensive fine-tuning. During experimental investigation, a low fixed temperature hyper-parameter is set for model learning on the training data. Other hyper-parameters like quadratic regularization and learning rate are adjusted through cross-validation on the validation set. The optimal temperature value is largely independent of neural network architecture but influenced by k and n values. Proposition 8 suggests fixing \u03b1 to 1 for deep networks trained with l k,\u03c4, simplifying the quadratic regularization process. In experiments, a fixed temperature is used for model learning, while quadratic regularization and learning rate are adjusted through cross-validation. Fixing \u03b1 to 1 simplifies the regularization process for deep networks trained with l k,\u03c4. Top-1 Error results are compared for different margin values on the validation set. The study compares the impact of margin parameters on top-1 and top-5 performance. Results show that increasing the margin parameter leads to improved accuracy. Additionally, the standard deviation of scores on CIFAR-100 is detailed for completeness."
}