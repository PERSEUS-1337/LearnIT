{
    "title": "BJxAHgSYDB",
    "content": "Automated machine learning methods, like hyperparameter and neural architecture optimization, are computationally expensive due to training numerous model configurations. A new method is introduced in this work to save computational budget by terminating poor configurations early on. This approach treats the task as a ranking and transfer learning problem, optimizing a pairwise ranking loss and leveraging learning curves from other datasets. The model effectively ranks learning curves without needing many or lengthy curves, accelerating neural architecture search by up to 100 times without significant performance degradation. Additional experiments analyze ranking quality, model component influence, and predictive capabilities. The text discusses the importance of early termination in optimizing neural architectures and hyperparameters. It highlights the use of learning curves to predict model performance and the limitations of current extrapolation techniques. The text discusses the limitations of current extrapolation methods for predicting model performance based on learning curves. It introduces a transfer learning technique that utilizes learning curves from other problems to address the need for sample learning curves. The approach considers the varying accuracy ranges in different datasets and uses a ranking model to predict model performance. The text introduces a ranking model to predict model performance by comparing it against the best solution so far. It also considers the model's network architecture for reliable predictions on short learning curves. The method is tested on five image classification datasets and shown to accelerate neural architecture search. Ablation studies are conducted to better understand the model's behavior. Prior work on learning curve prediction involves extrapolating partial learning curves using basic functions. The text discusses various methods for predicting model performance based on learning curves. Different approaches include using parametric basic functions, Bayesian models, extrapolating learning curves, and using support vector machines for prediction. Each method has its unique way of forecasting model accuracy without prior knowledge of the learning curve. Different methods for predicting model performance based on learning curves have been discussed. Domhan et al. (2015) can forecast without prior learning curve knowledge but requires more epochs for accuracy. Chandrashekaran & Lane (2017) need to see a few learning curves to predict accurately, while Klein et al. (2017) and Baker et al. (2018) require many full-length learning curves for useful forecasts. Pointwise ranking methods are used to determine if the current learning curve leads to better results. Pairwise ranking methods are more efficient than pointwise ranking methods. A pairwise ranking loss is considered for this application for the first time. Bandit-based methods like Successive Halving and Hyperband leverage early termination. BOHB extends Hyperband by using Bayesian optimization to select settings. These methods aim to terminate less promising training processes early. Automated machine learning methods that terminate less promising training processes early, such as DARTS, are crucial for accelerating hyperparameter optimization. Neural Architecture Search, a computationally intensive subproblem, is a major challenge in this field, with parameter sharing methods like BOHB being commonly used. The last extensions of the idea involve a continuous relaxation of the search space for learning shared parameters and architecture through gradient descent. This method is a strong alternative to early termination methods but is not applicable to general machine learning or arbitrary architecture search spaces. Learning curve refers to the performance function with increasing iterations of an iterative learning algorithm, with the final learning curve representing the entire training process. LCRankNet is a new method proposed to predict the probability that a model is better than another directly, based on model characteristics and the partial learning curve. It eliminates the need for a two-step process used in existing methods for early termination in training models. LCRankNet predicts the probability of one model being better than another based on model characteristics and partial learning curves. It uses a logistic function and minimizes cross-entropy loss to determine parameters. The model's prediction depends on the outcome of a function that takes a model representation as input. LCRankNet predicts model performance based on characteristics and learning curves. The model representation includes partial learning curve, architecture description, and dataset ID. A neural network processes the representation parts, which are concatenated and fed to a fully connected layer. The architecture includes special layers for processing different parts. The learning curve prediction model uses convolutional layers with different kernel sizes followed by global max pooling. The architecture is based on NASNet search space with two cells and five blocks each. An LSTM generates the architecture embedding from encoded choices. The model learns from learning curves of other datasets. The model uses data set embeddings to learn from learning curves of other datasets. The architecture component is regularized to avoid instabilities during training by using an autoregressive model. The attention mechanism is also utilized to facilitate the process. The model utilizes the attention mechanism to train the layers jointly using Adam. The hyperparameter \u03b4 controls the trade-off between precision and recall. Setting \u03b4 = 0.45 terminates runs early if the predicted probability of improvement is below 45%. The meta-knowledge creation and model analysis for Neural Architecture Search acceleration are discussed. The study focuses on accelerating Neural Architecture Search by comparing methods on various datasets like CIFAR-10, CIFAR-100, Fashion-MNIST, Quickdraw, and SVHN. A subset of the Quickdraw dataset is used with 100 classes for training and 100 for testing. Meta-knowledge is created by randomly selecting 200 architectures per dataset from the NASNet search space, totaling 1,000 unique architectures trained on different datasets. The study involves training different architectures on various datasets using specific training methods and image preprocessing techniques. Experiments are conducted in a leave-one-data-set-out cross-validation approach to analyze learning curve rankings and prediction methods. In an experiment, 50 random learning curves are chosen as a test set, with five random learning curves used for training in each repetition. Different learning curve prediction methods rank architectures based on partial learning curves. The correlation between true and predicted rankings is measured using Spearman's rank correlation coefficient. The method LCRankNet consistently outperforms existing methods, especially when there are limited or very short partial learning curves available. This is attributed to considering the network architecture in the prediction process. The impact of different learning curve prediction methods is analyzed in detail. Chandrashekaran & Lane (2017) shows good results, Baker et al. (2018) has high variance, and Domhan et al. (2015) requires a minimum learning curve length. Using the last seen value for ranking learning curves is efficient (Klein et al., 2017). In this experiment, learning curve predictors are used to search for network architectures by accelerating a random search in the NASNet search space. The random search samples 200 models and trains each for 100 epochs to find the best model. Successive Halving and Hyperband algorithms are followed to determine the best model after iterating over all 200 architectures. The goal is to minimize. The experiment uses learning curve predictors to accelerate a random search in the NASNet search space. Domhan et al. (2015) 's method is noted for underestimating performance with short learning curves. Results show all methods accelerate the search with little regret, defined as the difference in model accuracy between random search and the methods. Our method outperforms others in terms of time efficiency, with results confirming better performance compared to previous studies. While some cases show slightly higher regret, the time saved outweighs this drawback. Visualizations of random search for SVHN reveal similar behavior and accuracy among models, making it challenging to determine model discards efficiently. Successive Halving and Hyperband are exceptions to this trend due to fixed termination criteria, leading to more efficient runtime. The algorithm fixes the number of terminated runs and their discard time, regardless of the learning curve properties. A comparison with DARTS shows our method's efficiency, achieving a 2.99% classification error on CIFAR-10 in just 20 GPU hours, outperforming DARTS. However, LCRankNet does not always perform perfectly, with cases of regret and longer search times. In this section, we analyze the decisions made by LCRankNet, providing examples of correct and incorrect decisions. LCRankNet assigns higher probabilities based on meta-knowledge, with cases where early stopping could have been applied sooner. The model's behavior is illustrated through four example decisions in Figure 5. LCRankNet makes decisions based on meta-knowledge, with examples of correct and incorrect decisions shown. The learning curve indicates that m is better than m max until a certain point, where the probability decreases and training stops early. The behavior of LCRankNet is reasonable, even in difficult situations where it is hard to predict which curve will perform better. Deciding whether one model will be better than another based on a partial learning curve is challenging. The learning curve of m is better than m max until a certain point, where the difference in final accuracy is minimal. Despite initial appearances, m eventually outperforms m max, highlighting the difficulty in predicting model performance based on early learning curve trends. The analysis compares different variants of a learning curve ranker, including those with and without metadata, architecture description, or learning curve consideration. The configuration trained with pairwise ranking loss outperforms the one trained with pointwise ranking loss. Metadata is found to be essential for the model due to the need for sufficient data, leading to higher variance in variants without it. Even considering only the learning curve shows benefits from additional data. The variant that only considers the learning curve benefits from additional meta-knowledge, showing similar behavior regardless of the data set. Using meta-knowledge, both components achieve good results independently, with orthogonal characteristics. The architecture-only variant performs well for short learning curves, while the learning curve-only variant improves significantly with curve length. Combining both methods leads to further improved results. Comparing pointwise and pairwise ranking losses, the latter outperforms in optimizing model parameters directly for the task. LCRankNet is a method that uses a pairwise ranking loss to optimize model parameters directly for the task at hand. It can automatically terminate unpromising model configurations early by considering learning curves from other data sets. In experiments, LCRankNet outperformed three alternatives and achieved the fastest results in optimizing network architectures, being up to 100 times faster. LCRankNet is 100 times faster in obtaining results without compromising accuracy, showcasing its efficiency in optimizing network architectures. The method also provides insights into its components and predictions for better understanding of design choices and functionalities."
}