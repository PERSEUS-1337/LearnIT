{
    "title": "Bkgr2kHKDH",
    "content": "The methodology presented optimizes neural network architecture, pruning, and quantization policies together in an end-to-end manner. A quantization-aware accuracy predictor is trained to assist in selecting the best fit. Experiments on ImageNet show that this approach maintains 75.1% accuracy while saving 2.2\u00d7 BitOps compared to an 8-bit model. The methodology optimizes neural network architecture, pruning, and quantization policies together. Experiments show 75.1% accuracy with 2.2\u00d7 BitOps savings compared to an 8-bit model. Achieves similar accuracy as MobileNetV2+HAQ with 2\u00d7/1.3\u00d7 latency/energy savings. Outperforms separate optimizations using ProxylessNAS+AMC+HAQ by 2.3% accuracy while reducing GPU hours and CO2 emissions. Deep learning applications like autonomous driving and robotics require efficient network architecture design to meet performance constraints. Model compression techniques such as pruning and quantization can reduce costs, but hyper-parameter tuning is crucial for optimal results. The complexity of tuning increases exponentially when considering all stages together, necessitating innovative solutions. Recent works have applied AutoML techniques like Neural Architecture Search (NAS) to automate model design, outperforming human-designed models. Researchers also use reinforcement learning for model compression through automated pruning and quantization. However, optimizing these factors separately can lead to sub-optimal results and require significant time and energy. A joint approach is needed to address these challenges efficiently. EMS is proposed as an end-to-end design method to optimize deep learning models for specific hardware platforms, addressing challenges in search time and energy consumption. The approach combines pruning and quantization in a joint search space, derived from one-shot NAS, to achieve optimal results efficiently. EMS is an end-to-end design method derived from one-shot NAS, reorganizing the traditional pipeline into \"architecture search + mixed-precision search\". It includes coarse-grained architecture search and fine-grained channel search, as well as finding the optimal mixed-precision quantization policy. The approach focuses on search efficiency by training a flexible super network for joint search over architecture and channel number, and optimizing the trade-off between accuracy and resource consumption in mixed-precision search. Our PredictorTransfer Technique improves sample efficiency by transferring a quantization-aware accuracy predictor from a full-precision predictor. This allows for fast joint search over model architecture, channel number, and mixed-precision quantization, enabling efficient deployment on new hardware. Our method, EMS, integrates NAS, pruning, and quantization into a unified solution. It achieves a 2.2\u00d7 reduction in BitOps compared to the 8-bit version while maintaining 75.1% accuracy with ResNet34. Our models outperform ProxylessNAS+AMC+HAQ by 2.3% accuracy under the same latency constraints, reducing GPU hours and CO2 emissions significantly. The quantization-aware accuracy predictor's dataset collection NN architecture enables efficient model search with the supernet. Various methods like network pruning and quantization accelerate model inference. Neural Architecture Search has evolved to reduce search time using different approaches. Neural Architecture Search has evolved to reduce search time by jointly training models and leveraging network weights. One-shot methods and performance predictors are used to optimize model efficiency. Works like SPOS, ChamNet, AMC, HAQ, and EMS focus on different aspects of model optimization, with EMS standing out for directly searching mixed-precision architecture. Pruning techniques have also shown progress in model optimization. Progresses in pruning techniques have evolved from fine-grained pruning to channel-level pruning, aiming to accelerate inference on general-purpose hardware by reducing the search space based on importance scores. Recent researches leverage AutoML techniques to automate the exploration process of model optimization and surpass human design. Quantization techniques, such as weight grouping, binarization, and regularization, are essential for deploying models on hardware platforms like FPGAs and mobile phones. Recent researches use AutoML techniques to automate model optimization. Quantization methods like weight grouping, binarization, and regularization are crucial for deploying models on hardware platforms. Jacob et al. (2018) utilized 8-bit integers for weights and activations on mobile devices. HAQ proposed leveraging AutoML to determine bit-width for mixed-precision quantized models, showing a strong correlation between network architecture and quantization. Multi-stage optimization involves searching for the best neural network architecture on the target dataset in the first stage and pruning in the second stage. Recent researches utilize AutoML techniques for automating model optimization, including quantization methods like weight grouping and binarization. However, the separation of stages in optimization can lead to sub-optimal solutions and time-consuming evaluations. An end-to-end design methodology involves training an accuracy predictor for full precision NN, incrementally training for quantized NN, and using evolutionary search to find specialized NN architecture. This approach aims to address the challenges of automatic design in a more efficient manner. Evolutionary search is used to find a specialized neural network architecture that fits hardware constraints. Joint optimization combines NAS, pruning, and quantization to find a balance among these configurations. The approach involves training a super network, building a quantization-aware accuracy predictor, and using a latency/energy lookup table for resource-constrained evolution search. This method aims to tackle the joint optimization problem efficiently. The joint optimization problem in resource-constrained evolution search involves tackling a quadratic search space for architecture configuration and quantization policy. Unlike previous methods, the predictor in this approach balances full precision and mixed-precision data, leading to more stable accuracy statistics across different backbone networks like ResNet and MobileNet. The overall framework of our end-to-end design includes a super network with fine-grained channels, an accuracy predictor, and evolution search optimizing architecture, pruning, and quantization. Neural architecture search aims to find a good sub-network from a large search space efficiently. Recent one-shot based NAS uses a large, multi-branch network to extract sub-networks for evaluating accuracy directly. In this paper, a super network with various choices for layers is used for neural architecture search. The super network supports different kernel sizes, channel numbers, and depths, creating a search space of over 10^35 sub-networks. To efficiently conduct architecture search, the super network must meet specific properties, such as direct evaluation of sub-network performance without re-training. The super network supports an extensive search space with fine-grained channel numbers and different operators. To address the challenge of accuracy approximation in a large search space, the progressive shrinking algorithm is adopted for training the super network. The progressive shrinking algorithm is used to train the super network by distilling smaller sub-networks from a full sub-network. This approach reduces variance during training and ensures competitive accuracy without re-training. Additionally, a quantization-aware accuracy predictor is proposed to predict accuracy based on architecture configurations and quantization policies, reducing the cost of designs in various deployment scenarios. During the search, predicted accuracy is used instead of measured accuracy. The predicted accuracy acc = P (arch, prune, quantize) is used instead of the measured accuracy. The predictor P takes the encoding of the network architecture, pruning strategy, and quantization policy as input. The network architecture is encoded block by block, with choices of kernel sizes, channel numbers, and bitwidths. A 3-layer feed-forward neural network is used as the accuracy predictor with each embedding dim equaling to 400. Our predictor-based method for architecture-pruning-quantization does not require frequent evaluation on the target dataset during the search phase. Integration with various search methods is possible at a low cost. The challenge lies in collecting a dataset for training the predictor for quantized models due to the time-consuming nature of collecting quantized model accuracy data. Training a quantization-aware accuracy predictor for quantized models is challenging due to the complexity of architecture design and quantization policy. Traditional training methods may result in a significant performance drop. A pre-trained full-precision predictor can be fine-tuned with an additional input head to transfer to quantized models. Collecting a dataset for training the predictor for quantized models is time-consuming and costly, requiring about 0.2 GPU hours per data point. To increase sample efficiency and address data scarcity, a predictor-transfer technique is proposed for quantized models. A pre-trained predictor on full-precision models is fine-tuned and transferred to quantized models, utilizing a smaller quantized accuracy dataset for short-term fine-tuning. The quantization-aware accuracy predictor incorporates quantization bits into the input embedding and is further fine-tuned using pre-trained weights. The quantization-aware accuracy predictor is fine-tuned using pre-trained full-precision predictor weights for increased efficiency. The optimal network architecture and quantization policy vary for different hardware, so direct optimization based on measured latency and energy on the target hardware is utilized. Approximating latency and energy of the model by summing up each layer's values helps in evaluating candidate policies without costly hardware testing. To optimize resource-constrained model search, a lookup table is used to calculate latency and energy for each layer under different configurations. Evolution-based architecture search is adopted, and the evaluation process is replaced with a quantization-aware accuracy predictor. This reduces the cost from N model inferences to one predictor inference, with resource constraints verified using a latency/energy lookup table. Our method efficiently verifies resource constraints using a latency/energy lookup table, eliminating candidates exceeding constraints. It outperforms state-of-the-art models for hardware with fixed quantization or mixed precision, reducing search time significantly. CO2 emissions and cloud compute costs are negligible. Data preparation involves generating two types of data for quantization-aware accuracy prediction, using the ImageNet-100 dataset to speed up the process. Our method efficiently verifies resource constraints using a latency/energy lookup table, outperforming state-of-the-art models for hardware with fixed quantization or mixed precision. Data preparation involves generating two types of data for quantization-aware accuracy prediction, using the ImageNet-100 dataset. For evolutionary architecture search, we set the population size to be 100 and choose Top-25 candidates to produce the next generation. The mutation rate is 0.1, and we follow specific quantization policies for weights and activations. Our method quantizes weights and activations with specific policies to minimize KL-divergence. Experiments on ImageNet dataset show improved latency and energy consumption compared to state-of-the-art models. Performance is compared with models from He et al., 2018 and Cai et al., 2019b, as well as some SOTA fixed models. The platform used for measuring resource consumption is BitFusion, employing a 2D systolic array of Fusion Units. The model outperforms fixed precision models by more than 10% accuracy under strict constraints and 5% compared to HAQ. Performance is compared with models from AMC and HAQ under latency constraints. Our end-to-end designed model achieves better accuracy than sequentially designed models, with a 0.5% accuracy increase and 2.2x BitOps saving. It consistently outperforms state-of-the-art models under different efficiency constraints, with notable improvements in accuracy and performance. The model also incurs significantly lower costs for cloud computing and CO2 emissions compared to other works. Our end-to-end designed models consistently outperform both mixed-precision and fixed precision SOTA models under certain constraints, showing significant improvement in ImageNet top1 accuracy from the MobileNetV2 baseline. The predictor-transfer technique improves pairwise accuracy, achieving 85% accuracy using less than 3k data. Using predictor-transfer technique, we can achieve 85% pairwise accuracy with less than 3k data points, compared to needing at least 4k data without this technique. The accuracy consistently improves due to the mutual information preserved between architecture and quantization policy. Our joint optimization outperforms multi-stage optimized models, showing better accuracy (74.1% vs 71.8%) under the same latency/energy constraint. Additionally, our model improves over 2% accuracy (from 71.5% to 73.9%) compared to a searched model under a tight BitOps constraint. Our proposed EMS method achieves the same accuracy as ResNet34 full precision model while consuming half the BitOps. The predictor-transfer technique shows higher and faster pairwise accuracy convergence compared to training from scratch, especially with limited data. Our method for architecting mixed-precision models directly searches for optimal architecture without multi-stage optimization, using a predictor-based approach to save GPU hours and reduce CO2 emissions. We also propose a predictor-transfer technique to address data collection expenses, showing the necessity of joint optimization for improved performance compared to state-of-the-art models."
}