{
    "title": "HkxGaeHKvB",
    "content": "NAMSG is an adaptive first-order algorithm for training neural networks. It efficiently computes gradients at remote observation points to adjust step sizes for faster convergence. The method utilizes AMSGRAD and provides convergence analysis for convex and nonconvex problems. It offers a data-dependent regret bound for convex settings and achieves O(log(T)) regret bound for strongly convex functions. Experimental results show NAMSG performs well compared to other adaptive methods. Efficient optimization methods are crucial for training deep neural networks with large datasets. First-order optimization methods, like NAMSG, are popular for their ease of implementation and low computation overheads. Compared to second-order methods, they are more effective in handling gradients. NAMSG compares favorably to popular adaptive methods like ADAM, NADAM, and AMSGRAD. Momentum methods, such as HB Polyak (1964), are crucial for improving the performance of SGD by amplifying steps in low-curvature eigen-directions of the Hessian through accumulation. Nesterov's Accelerated Gradient (NAG) is a momentum-based method that suppresses steps along high curvature eigen-directions to prevent oscillations. However, these approaches are approximations derived for exact gradients and lack a full study on gradient noise. Among variants of SGD methods, adaptive methods like ADAGRAD, RMSPROP, ADADELTA, ADAM, and NADAM have been successful in scaling gradient elementwise by averaging past gradients. ADAGRAD is suitable for sparse gradients but suffers from rapid step size decay with nonconvex loss functions. Other adaptive methods address this issue by using exponential moving averages. In response to the limitations of ADAM and other adaptive methods, AMSGRAD was proposed as a solution to improve convergence in training neural networks. Building upon this, a new method called NAMSG combines CNAG and AMSGRAD to efficiently compute stochastic gradients for faster convergence in a stochastic setting. By adjusting learning rates for eigen-directions with different curvatures and selecting observation distances, NAMSG aims to expedite convergence in training neural networks. The NAMSG scheme introduces a method that combines CNAG and AMSGRAD to efficiently compute stochastic gradients for faster convergence in training neural networks. It adjusts learning rates for eigen-directions with different curvatures and selects observation distances to expedite convergence. The convergence properties are analyzed by modeling the training process as a dynamic system, revealing the benefits of remote gradient observations and providing a strategy to select the observation factor without grid search. Regret bounds are introduced in the convex setting and further improved for strongly convex functions, with experiments demonstrating the efficiency of NAMSG in real problems. In the context of machine learning, the minimization problem of a stochastic function is considered, where x represents the model parameters and \u03be is a random datum containing an input-output pair. The feasible set of points F is defined with bounded diameter D\u221e and gradient constraints. The projection operation \u03a0F,A(y) is defined for A \u2208 Sd+ and y \u2208 Rd. In machine learning, the optimization problem involves minimizing the empirical risk on a training set using a configurable NAG method. This method utilizes updating directions and observation points to compute gradients and make predictions for forthcoming updates. By substituting the gradient with the observation gradient in the HB update, the original form of CNAG is obtained with configurable coefficients \u03b1 t , \u03b2 t , \u03b7 t. To simplify the method, x t is approximated by\u1e8b t + \u03b7 t\u22121 \u03b1 t\u22121 m t\u22121, resulting in a concise form of CNAG with observation factor \u00b5 t = \u03b7 t (1 \u2212 \u03b2 t )/\u03b2 t. Practical computation of CNAG requires only 3 scalar vector multiplications and 3 vector additions per iteration. The relation between CNAG and ASGD is studied to guide the selection of momentum coefficient. ASGD improves on SGD in various regimes by taking long and short steps and an appropriate average. ASGD requires 3 hyper-parameters: short step \u03b1, long step \u03ba, and statistical advantage \u03be. These parameters vary widely and are difficult to estimate, limiting the application of ASGD. The appendix introduces CNAG as a more efficient form of ASGD, with a momentum coefficient \u03b2 close to 1 for narrowing the gap between step sizes on eigen-directions. NAMSG combines CNAG with AMSGRAD's nonincreasing preconditioner and projects the parameter vector x into the feasible set F, requiring low computation overheads compared to AMSGRAD. In Algorithm 1, the observation factor \u00b5 t can be configured to accelerate convergence in local stochastic quadratic optimization. The projection can be omitted in implementation to save computation, especially with weight decay for regularization. Default values and a practical strategy for setting the observation factor without grid search are provided based on the analysis of convergence rates for both convex and non-convex problems. In local stochastic quadratic optimization, the gradient observation is noisy. The optimization process of NAMSG can be simplified by ignoring projections and approximating the update division by solving a preconditioned problem. Solving the quadratic problem by NAMSG is equivalent to solving the preconditioned problem by CNAG. The algorithm accelerates convergence in optimization with configurable observation factors. In local stochastic quadratic optimization, the algorithm NAMSG simplifies the optimization process by solving a preconditioned problem. The coefficients and updates are defined using eigenvectors and eigenvalues of the Hessian matrix. The algorithm recommends a momentum factor of \u03b2 = 0.999 for faster convergence. Based on the analysis in Section 2, a momentum factor of \u03b2 = 0.999 is recommended. Figure 1 shows the gain factor and standard deviation limit of CNAG. Proper observation factor \u00b5 improves convergence rate and accelerates divergence in nonconvex problems. A small curvature \u03bb converges slower compared to large curvatures, but using a large \u00b5 can alleviate this issue. For fast convergence speed, \u00b5 = 0.1 is recommended, while \u00b5 = 0.2 improves generalization at the cost of more iterations. In NAMSG, experiments show that a \u03b2 2 close to 1 leads to variation. Based on the analysis in Section 2, a momentum factor of \u03b2 = 0.999 is recommended for NAMSG experiments. A large \u03b2 and proper \u00b5 ensure a large convergence domain, while a small \u00b5 and scaling \u03b1 proportional to argmin \u03c4 g fac improve convergence rate for nonconvex settings. Training with a large step size \u03b1 is beneficial for both the convergence of tiny positive \u03bb and the divergence of tiny negative \u03bb. Selecting a smaller \u00b5 and scaling \u03b1 proportional to argmin \u03c4 g fac generally improves convergence rate. The proposed hyper-parameter policy OBSB aims to improve convergence rate by selecting an optimal initial \u03b1 through grid search. A data-dependent regret bound of NAMSG is provided for convex settings, with further enhancements for strongly convex functions. The algorithm's convergence property is evaluated by regret, defined as the sum of differences between online predictions and fixed point parameters. The algorithm NAMSG aims to converge to optimal parameters on average by ensuring a nonincreasing step size through positive definiteness of \u0393 t. The regret bounds of NAMSG and AMSGRAD have a similar form, but NAMSG has lower coefficients when \u03b2 1 and \u03b3 are close to 1. Corollary 1 shows that NAMSG outperforms previous methods for strongly convex functions. In Corollary 1, NAMSG shows superior performance compared to previous methods for strongly convex functions. It achieves a regret bound of O(log(T)) with O(1/t) step size under certain assumptions. The proof of theorems is provided in the appendix, and numerical experiments suggest that piecewise constant \u03b1 t and constant \u03b2 1t lead to fast convergence. Experiments comparing NAMSG with other optimization methods are presented in this section. In comparison to SGD with momentum, CNAG, ADAM, NADAM, AMSGRAD, NAMSG, and OBSB are evaluated for logistic regression and neural networks on the MNIST dataset using MXNET. Logistic regression experiments use a minibatch size of 256, with hyperparameters chosen by grid search for all methods except NAMSG and OBSB. In the experiment, NAMSG and OBSB are evaluated for a multiclass classification problem on MNIST using a simple CNN architecture. OBSB performs the best with respect to train loss, while NAMSG converges faster than other methods. The test accuracy fluctuates after initial epochs, indicating possible overfitting. Both methods achieve fast convergence in the convex setting. The experiment involves training Resnet-20 on the CIFAR-10 dataset with NAMSG and OBSB, showing efficient performance in non-convex problems. The network architecture includes fully-connected layers with BN and ReLU, achieving low train loss and fast convergence. The Resnet-20 model is trained on CIFAR-10 using various optimization algorithms like SGD, ADAM, NADAM, CNAG, AMSGRAD, NAMSG, and OBSB. The network architecture includes 3x3 convolutions, 18 layers with different feature map sizes, batch normalization, ReLU activation, and global average pooling. Training runs for 75 epochs with hyper-parameter adjustments during training. In experiments with different optimization algorithms on the Resnet-20 model trained on CIFAR-10, OBSB converges the fastest for both fastest training speed and best generalization. NAMSG is also faster than other methods and achieves good test accuracy. CNAG shows significant acceleration compared to SGD, ADAM, NADAM, and AMSGRAD. CNAG achieves the highest test accuracy among different optimization algorithms tested on the Resnet-20 model trained on CIFAR-10. OBSB, NAMSG, and SGD obtain similar final test accuracy, outperforming ADAM, NADAM, and AMSGRAD. NAMSG and OBSB converge faster with low computational overheads compared to other popular adaptive methods. The NAMSG method efficiently computes gradients at remote observation points, scales update vectors, and guarantees convergence in convex settings with a data-dependent regret bound. It outperforms ADAM, NADAM, and AMSGRAD in numerical experiments. The NAMSG method efficiently computes gradients at remote observation points, scales update vectors, and guarantees convergence in convex settings with a data-dependent regret bound. It outperforms ADAM, NADAM, and AMSGRAD in numerical experiments. For simplicity, denote the regret bound due to the convexity of the objective function. Various inequalities are derived based on definitions and assumptions, leading to the completion of the proof with the use of Lemmas. The proof utilizes Lemma A2 from Reddi et al. (2018) to establish regret bounds for a strongly convex objective function. Algorithm A1 ASGD is presented with input parameters and iteration steps for parameter updates. The term Q3 is bounded to complete the analysis. The proof establishes regret bounds for a strongly convex objective function using Algorithm ASGD. ASGD maintains two iterates: descent iterate x_t and a running average x_t. The update of Algorithm ASGD is rewritten and a variable transform is defined. The update equation for Algorithm ASGD is modified to reduce computational overheads by using adjustable coefficients. The momentum coefficient for CNAG is derived, and the comparison shows CNAG reduces costs compared to ASGD. Constant hyperparameters are used in experiments for ADAM, NADAM, and AMSGRAD. For AMSGRAD, SGD, CNAG, NAMSG, and OBSB, hyperparameters are selected through grid search. Different sets of hyperparameters are used for each algorithm. OBSB runs grid search for 5 epochs on MNIST and 20 epochs on CIFAR10. Convergence rates are computed every 2 epochs on MNIST and every 10 epochs on CIFAR10. Adjustments to \u03b1 and \u00b5 are made for faster convergence when needed. The experiments were conducted using NAMSG on a workstation with an Intel Xeon E5-2680 v3 CPU and a NVIDIA K40 GPU. The source code and hyperparameters for NAMSG can be downloaded from GitHub. The simulation environment used was MXNET, and the datasets MNIST and CIFAR-10 can be downloaded from their respective sources."
}