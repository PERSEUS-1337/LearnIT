{
    "title": "H1xsSjC9Ym",
    "content": "Recent work has shown that deep reinforcement-learning agents can learn to follow language-like instructions from infrequent environment rewards. A new framework is presented where instruction-conditional RL agents are trained using rewards obtained from reward models trained from expert examples. This approach separates the representation of instructions from how they are executed, allowing agents to be accurately rewarded for completing tasks not present in the expert data. In a grid world, agents can learn commands involving blocks and spatial relations without new expert examples. Deep RL shows promise in following language-like instructions in 2D and 3D worlds. Rewarding agents for task completion requires an interpreter for instruction language evaluation. The interpreter for instruction language evaluation in a grid world must have full knowledge of the semantics of the instruction language to determine when reward should be granted to the agent. Implementing such an interpreter can be feasible but typically requires significant engineering efforts. The complexity of natural language poses challenges for implementing reward systems in large environments. Can we learn instruction-conditional reward functions to train policies? Inverse Reinforcement Learning methods have shown success in learning reward functions from expert data. This paper explores adapting these mechanisms to learn language understanding. The paper introduces a framework called AGILE for training an instruction-conditional reward model using expert examples of completed instructions alongside a policy to maximize the reward. AGILE relies on RL objectives with a learned reward model instead of environment reward. AGILE-trained policies outperform those trained with environment reward in learning speed and performance. The method is comparable to true-reward A3C agents with unsupervised reward prediction objectives. AGILE is a framework for learning to model reward for instructions and training a policy from that reward model. The agent trained within AGILE can construct colored orientation-invariant arrangements of blocks as instructed without the need to implement the reward function. The reward model in AGILE can generalize beyond its training examples, allowing the policy to adapt to changes in the environment. In AGILE, the reward model is trained to define a meaningful reward function for the policy by predicting if a state is a goal state for a given instruction. Positive and negative examples are sampled from a policy-dependent data stream, rather than a static dataset, to adjust the policy's parameters for maximizing total reward. The policy is trained to maximize return R \u03c0 (\u03b8) while the reward model minimizes cross-entropy loss L D (\u03c6). The Iverson Bracket maps truth to 1 and falsehood to 0. \u03b3 is the discount factor. State trajectories are denoted by (c, s 1:\u221e ) \u223c G \u03c0 \u03b8. B is a replay buffer for (c, s) pairs from T-step episodes. D \u03c6 (c, s) is the probability of a positive label, indicating a state is more likely to be a goal state for instruction c. The policy is trained to maximize return while the reward model minimizes cross-entropy loss. The approach involves using a reward model to distinguish between instruction-goal state pairs and instruction-state pairs. The policy acts based on the instruction and is trained using the reward from the reward model. The approach involves using a reward model to distinguish between instruction-goal state pairs and instruction-state pairs. A heuristic is proposed to identify false negatives by ranking examples in B based on the reward model's output and discarding potential false negatives. This helps rectify the limitation where the policy's performance drops as more objective goal-states are treated as negative examples. The AGILE approach involves using a reward model to distinguish between instruction-goal state pairs and instruction-state pairs. A heuristic is proposed to identify false negatives by ranking examples in B based on the reward model's output and discarding potential false negatives. The anticipated negative rate \u03c1 is a hyperparameter that should be tuned on a task-specific basis for optimal performance. The reward model D \u03c6 and the policy \u03c0 \u03b8 learn two related but distinct aspects of an instruction: the reward model focuses on recognizing the goal-states. The AGILE approach involves using a reward model to distinguish between instruction-goal state pairs and instruction-state pairs. It proposes reusing a reward model trained in AGILE as a reward function for training or fine-tuning policies, inspired by Generative Adversarial Imitation Learning (GAIL). AGILE focuses on recognizing goal-states, while GAIL trains both a reward function and a policy to maximize the modelled reward. AGILE differs from GAIL in several key aspects. AGILE allows a single agent to learn multiple skills, the reward model observes only states, and the policy's reward is based on thresholded probability instead of log-probability. This change is made to address issues with goal-specifications and intermediate states. AGILE's performance benefits from dropping the logarithm from GAIL-style rewards and using probability D \u03c6 (c, s t ) as the reward. Experimentation in the GridLU grid world environment shows similar performance levels to discretized AGILE rewards. Models in this environment receive a 56x56 RGB image of the world state. Two types of models are tested for processing instructions: Neural Module Networks and a generic model. Neural Module Networks (NMN) are used to process structured expressions in instructions, while a generic model encodes unstructured instructions with an LSTM. NMN is an architecture for grounded language processing where a tree of neural modules is constructed based on the language input, with visual input fed to leaf modules and outputs sent to parent modules. The structure of the instructions is mimicked in the tree of modules, with each module performing a convolution followed by a shared weight process. Neural Module Networks (NMN) use convolution and Feature-Wise Linear Modulation (FiLM) to process structured expressions in instructions. The NMN's output undergoes max-pooling and is fed through a 1-layer MLP to produce action probabilities or the reward model's output. NMN is effective when the language structure is known, but a basic structure-agnostic architecture is also explored using FiLM to condition a convnet on an instruction representation from an LSTM. The convnet is conditioned on an instruction representation from an LSTM. The network outputs are produced using the output of the 5th layer of the convnet. Two architectures are referred to as FiLM-NMN and FiLM-LSTM. FiLM-NMN is the default model unless specified otherwise. Training the policy networks within AGILE used the Asynchronous Advantage Actor-Critic (A3C) with ground-truth reward. Any alternative training mechanism using reward could be used for fair comparison. The policy trained within AGILE, referred to as AGILE-A3C, used A3C with hyperparameters \u03b3=0.99 and \u03bb=0. Episodes were 30 steps long, but the agent was trained on 15-step advantage estimation rollouts. Each experiment was repeated 5 times, with success defined by task-specific criteria. The success rate was the main performance metric, using the NMN-based policy and reward model unless specified otherwise. GridLU-Relations is an adaptation of the SHAPES visual question answering dataset where blocks can be freely moved to induce spatial relationships like above or right of. The task involves five spatial relationships and a formal grammar to generate instructions like 'put a red circle north from a blue square'. The GridLU-Relations task involves spatial relationships and formal grammar to generate instructions like 'put a red circle north from a blue square'. There are 990 possible instructions in this task, with underspecified and somewhat ambiguous instructions. The GridLU-Relations task involves spatial relationships and formal grammar to generate instructions. The commands are underspecified, and the AGILE-A3C agent needs to infer specific meanings from goal-state examples. The binary ground-truth reward is used as the success criterion for evaluating AGILE. The AGILE-A3C agent outperformed the standard A3C agent in learning tasks due to the modeled rewards being easier to learn initially and becoming more sparse as the reward model improves. The A3C algorithm showed improvement with the auxiliary task of reward prediction, reinforcing the association between instructions and states. The AGILE-A3C agent significantly improved A3C performance by predicting rewards based on states prior to non-zero rewards. AGILE-A3C achieved nearly perfect performance, inducing a reward function from a limited set of examples. The best results were obtained with an anticipated negative rate of \u03c1 = 25%, maintaining stable performance over 100-200 million steps. In experiments, the AGILE-A3C agent achieved 60% performance with 8000 examples, but optimal performance required over 100,000 examples. Generalization to unseen instructions was tested by holding out 10% as a test set, showing that the trained model performed equally well on both training and test instructions. AGILE with Structure-Agnostic Models achieved a high 97.5% success rate with \u03c1 = 25% and trained almost as fast as an RL-RP agent with the same architecture. The reward model accuracy peaked at 99.5% with \u03c1 = 25%, but deteriorated with larger values of \u03c1, confirming the importance of this hyperparameter. The false negative rate was consistently low during training. During training, the false negative rate remained consistently low. The reward model initially had higher false positives (20-50% depending on \u03c1) but decreased for smaller \u03c1 values. Early false positives may aid in the policy's training by providing a curriculum. The AGILE framework was tested for robustness to changes in environment dynamics. The AGILE framework was tested for robustness to changes in environment dynamics by modifying the physics of the world. The success rate of the policy dropped when red square objects were made immovable, but increased to 69.3% after fine-tuning the policy using the reward model. This experiment suggests that the AGILE reward model learns useful linguistic knowledge for policy adaptation. The AGILE framework aims to avoid programming the reward function by using the AGILE-A3C model, which performs comparably to A3C without directly using the reward function. A task called GridLU-Arrangements was developed to model this setting explicitly, with multiple viable goal-states associated with each instruction. Training data was generated by producing random instantiations of goal-state classes as positive examples for the reward model. In GridLU-Arrangements, 36 instructions correspond to 390 million goal-states. AGILE was trained on only 100,000 goal-states for optimal performance. No reward function was written, and evaluations were done manually. Half of the episodes started with four square blocks of the same color in random positions, with instructions sampled uniformly. The AGILE agent was trained on 100,000 goal-states in GridLU-Arrangements, with instructions sampled uniformly. In half of the episodes, blocks of different colors were positioned randomly. The agent achieved correct arrangements in 58% of episodes, with failures mostly in episodes involving eight blocks. The AGILE agent achieved correct arrangements in 58% of episodes, despite being impeded by randomly positioned non-target-color blocks. This demonstrates the potential of AGILE for teaching agents to follow semantically vague instructions. Various approaches to teaching agents to follow instructions have been explored, such as reinforcement learning with programmed reward functions. Various approaches have been explored to teach agents to follow instructions, including training semantic parsers to produce formal representations of queries and directly predicting actions from demonstrations. The assumption that the agent and demonstrator share the same actuation model may not always hold true. Tellex et al. (2011) developed a log-linear model to map instruction constituents to their corresponding objects, places, or state sequences. Their approach requires a structured representation of the environment and intermediate supervision. Our work falls under apprenticeship learning, focusing on tasks learned from demonstrations and feedback. Many approaches to apprenticeship learning are variants of inverse reinforcement learning (IRL), such as the GAIL algorithm. AGILE, closely related to the GAIL algorithm from the IRL family, is a framework for training instruction-conditional RL. Unlike previous IRL-style methods, AGILE learns reward models for instructions directly from pixels. It differs from other apprenticeship learning methods by using goal-states only to train the reward module, which generalizes to new environment configurations or instructions. AGILE is a framework for training instruction-conditional RL agents using rewards from learned models, jointly trained from data provided by experts and the agent being trained. This approach opens up new possibilities for training language-aware agents in real-world and simulated environments. Acquiring data through human annotation may be more viable than defining reward functions programmatically. Programming rewards for instruction-following can be as challenging as interpreting language directly. Our experiments show that policies trained in the AGILE framework perform well and learn quickly, even without ground-truth reward. The reward model in AGILE helps the policy start learning by making false positive decisions early on. This suggests that the reward model in AGILE learns a form of shaped reward, which can be useful in cases where true reward is absent or sparse. Further investigation into these cases is left for future work. Our experiments in the AGILE framework show that policies can learn quickly without ground-truth reward. Mitigating false negatives can improve reward model accuracy, leading to robust training comparable to RL with reward prediction. AGILE separates learning \"what should be done\" and \"how it should be done\" into two model components, with the \"what\" knowledge generalizing better to new environments. Fine-tuning with a frozen reward model allows the policy to regain capability in new settings. While there is a gap between experimental tasks and real-world scenarios, our results are an encouraging first step. In more complex environments, AGILE shows promising results for learning without ground-truth reward. Future work could explore applying AGILE to realistic settings, addressing issues like perspective differences and body part visibility. AGILE also performs well with natural language, suggesting potential for future applications. AGILE shows promising results for learning without ground-truth reward in complex environments. Future work could explore applying AGILE to realistic settings, addressing issues like perspective differences and body part visibility. AGILE also performs well with natural language instructions, indicating potential for future applications. The policy and discriminator were trained concurrently using RMSProp and A3C. A baseline predictor was used to predict discounted return. Different hyperparameters were used for the policy and discriminator. A designated worker trained the discriminator while others trained the policy. The replay buffer B was used for discriminator training, giving the same performance as using (c, s) pairs from the discriminator worker. Regularizing the discriminator by clipping weights matrices columns to have L2 norm <= 1 was crucial. Policy rewards were linearly rescaled to [0; 0.1]. RL with reward prediction fetched batches from the replay buffer for extra gradient computation. Hyperparameters for GridLU-Relations and GridLU-Arrangements tasks were specified. The GridLU world is a 5x5 gridworld with blocks of different shapes and colors. An agent can carry a block, change color when carrying, and interact with blocks by picking up or dropping them. Other actions include moving in different directions. The agent is like a cursor and can be moved around the grid. The GridLU agent acts as a cursor in a 5x5 gridworld, interacting with blocks by picking up or dropping them. The world is represented as a color image with neural networks taking this image as input. AGILE policies were trained with datasets of different sizes for 5 \u00b7 10^8 steps, reporting the maximum success rate achieved. The agent was trained for 100M time steps, selecting the checkpoint that best fooled the discriminator based on internal rewards. The AGILE-trained agent in the GridLU agent requires 8000 to 130000 examples to understand the instruction language. There are 990 unique instructions that can be generated, with different expansions for <obj>, <go to instruction>, and <bring to instruction>. The instance generator for the AGILE-trained agent in the GridLU environment generates objects and places them randomly in a 5x5 grid. It does not sample instructions uniformly but generates the environment and instruction simultaneously. Additionally, a distractor object is included in the environment. The instance generator for the AGILE-trained agent in the GridLU environment generates objects and places them randomly in a 5x5 grid. It does not sample instructions uniformly but generates the environment and instruction simultaneously. Two 'sanity checks' are imposed to discard instances that fail. The number of different initial arrangements of three objects is at least 2300, leading to a total number of task instances being approximately 1.7 x 10^7. Training on GridLU-Relations involved comparing predictions of the discriminator with the ground-truth reward checker. In the GridLU-Arrangements task, instructions were categorized based on arrangement and color specifications. The number of distinct goal-states was computed and neural architectures were detailed, using specific notations for operations. ReLU was used as the nonlinearity in all layers except for LSTM. The FiLM-NMN discriminator D \u03c6 processes a 56x56 RGB image through a stem convnet with specific convolutional layers. It consists of modules that take left-hand and right-hand inputs to perform computations using FiLM coefficients and weight tensors. The FiLM-NMN discriminator D \u03c6 uses modules with left-hand and right-hand inputs to process instructions for different tasks, such as color, shape, and relations. These modules are connected based on the instructions given, and the output is used for further processing. The computation involves using the same set of weights for all modules, with a total of 12 modules including one for go-to instructions. The output of the discriminator is then computed based on the instructions provided. The FiLM-NMN discriminator uses modules with left-hand and right-hand inputs to process instructions for tasks like color, shape, and relations. The output is computed by max-pooling the FiLM-NMN output and feeding it to an MLP with a hidden layer of 100 units. The policy network is similar to the discriminator network but outputs softmax probabilities for 5 actions and uses an additional convolutional layer to combine outputs. The FiLM-LSTM models use an LSTM with 100 hidden units to predict FiLM biases and multipliers. The structure-agnostic models utilize an LSTM with 100 hidden units to predict FiLM biases and multipliers for a 5 layer convnet. FiLM coefficients for each layer are computed based on specific equations, with different characteristics for each layer including filter sizes, output features, and padding strategies. Layer 5 is connected to layer 3 through a residual connection. The convnet output h5 is max-pooled and used in an MLP with 100 hidden units to generate predictions. The baseline predictor and reward prediction pathways utilize the max-pooled result hmaxpool for input. The reward prediction pathway produces probabilities of positive or zero rewards in AGILE."
}