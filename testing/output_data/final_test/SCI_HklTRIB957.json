{
    "title": "HklTRIB957",
    "content": "The file size of trained neural network models is a bottleneck when deploying them on mobile devices or transmitting them over limited channels. A codec for compression is proposed to address this issue. A codec based on transform coding for neural networks achieves compression factors between 7.9\u20139.3 with only a 1%\u20132% decrease in accuracy for image classification. The need for large training data, long training times, and computational complexity are challenges in deep learning. Memory footprint of saved neural networks is a challenge for implementations on mobile or embedded devices with limited storage and transmission capabilities. In this paper, a complete codec pipeline is proposed for compressing neural networks efficiently. The codec relies on transform coding for weights and clustering-based compression for biases and normalizations. It provides high coding efficiency, minimal impact on network accuracy, and is applicable to existing models without the need for retraining. Several related works in the literature focus on techniques like quantization. The tensorflow framework offers a quantization method to convert floating-point weights to 8-bit fixed-point weights. Our proposed methods show significant coding gains beyond quantization. Unlike Deep Compression, our approach aims at transparent compression of existing network models without retraining or modifying the architecture. Transparent coding and coding modified content are distinct problems. The SqueezeNet network architecture aims to reduce the number of weights by up to 7.4 times. It uses a 2D DCT followed by quantization for encoding convolutional filters, known as transform coding. The quantizer's bit depth can be adjusted for specific applications, typically using 5-6 bit/coefficient. The weights of dense layers and 1 \u00d7 1 convolutions are arranged block-wise for transform coding. K-means clustering is used for coding biases and normalizations, with code books generated. Clustering reduces distortion compared to uniform quantization, leading to higher network accuracy for a given number of bits. The accuracy of the network is higher with a given number of quantizer steps, but the compression factor is smaller due to a more uniformly distributed occurrence of code book indices. Clustering is used for network parameters sensitive to distortion caused by uniform quantization. Processed data is entropy coded layer-wise and stored with meta data including network architecture and filter details. The evaluation of compression algorithms for image classification networks involves analyzing the architecture, filter dimensions, block arrangements, scaling factors, quantizer details, and code books for clustering. Performance is measured using Top-5 accuracy and compression efficiency is assessed using the compression factor. The networks are encoded, decoded, and used for image classification using the ILSVRC-2012 validation set. The study evaluates compression algorithms for image classification networks using the ILSVRC-2012 validation set. Three subsets of technology are tested: quantization only, clustering algorithm on all parameters, and complete pipeline with transform coding and clustering. Results show that the complete pipeline outperforms subsets, with compression factors of ten or higher observed without significant accuracy loss. AlexNet stands out due to its high number of weights. AlexNet has a high number of weights, with over 90% located in the first dense layer. Pruning and retraining are needed to address this issue. Transform coding and clustering do not provide additional benefits beyond quantization. Compression factors of over five can be achieved without significant accuracy loss, even for networks like SqueezeNet. The codec achieves compression factors of 7.9 and 9.3 for 1% and 2% accuracy decreases, respectively. Computational complexity is analyzed by measuring encoder and decoder run times using unoptimized Python code. In this paper, a codec based on transform coding and clustering is proposed for compressing neural networks. The codec achieves low-complexity and efficient compression, with negligible impact on network performance. Encoder and decoder run times are measured at 29s and 7.6s, respectively."
}