{
    "title": "HkNGsseC-",
    "content": "Expressive efficiency is the relationship between two architectures A and B, where B can replicate any function of A, but A can perform functions that B cannot without significantly increasing in size. Deep networks are exponentially more efficient than shallow networks. This study extends expressive efficiency to network connectivity, focusing on the impact of \"overlaps\" in the convolutional process. To analyze network design, we focus on Convolutional Arithmetic Circuits (ConvACs) as a surrogate for ConvNets. Our results show that denser connectivity and overlapping local receptive fields lead to exponential increases in expressive capacity. Modern architectures already exhibit this increase without fully-connected layers. Deep networks are exponentially more expressive than shallow networks, known as \"Depth Efficiency.\" The Depth Efficiency attribute in network design brings exponential increase in expressive power through polynomial changes in the model, such as adding more layers. Modern networks consist of various architectural features like connectivity schemes, convolution filter sizes, pooling geometry, and activation functions. Whether these attributes relate to expressive efficiency like depth remains an open question. In network design, expressive efficiency is defined by the ability of one architecture to realize functions of another architecture with certain size constraints. The type of efficiency is determined by the function relating the sizes of the architectures. In this paper, the efficiency associated with the architectural attribute of convolutions is studied, specifically focusing on the size of convolutional filters and their proportion to the stride. Network architectures are classified as non-overlapping when the local receptive field size equals the stride, leading to completely separated sets of pixels for each pair of neurons in the same layer. Conversely, overlapping architectures have a stride smaller than the receptive field, with the degree of overlap determined by the total receptive field and stride. The total receptive field and stride in overlapping architectures can grow faster than in non-overlapping ones. Non-overlapping networks have theoretical merits and better convergence guarantees, but they are uncommon in practice. The question arises as to why non-overlapping architectures are not widely used despite their advantages. Recent architectures have been using smaller receptive fields and a mix of overlapping and non-overlapping layers. The trend is towards non-overlapping layers playing a larger role, leading to the question of why slightly overlapping architectures are sufficient for most tasks. A theoretical framework called Convolutional Arithmetic Circuits (ConvACs) is being used to study ConvNets, showing that linear activations and product pooling can be effective alternatives to non-linear activations and pooling layers. Recent architectures have been utilizing smaller receptive fields and a combination of overlapping and non-overlapping layers. A theoretical framework called Convolutional Arithmetic Circuits (ConvACs) has been used to study ConvNets, showing that linear activations and product pooling can be effective alternatives to non-linear activations and pooling layers. Overlapping ConvACs are proven to be exponentially more efficient than non-overlapping ones, with their expressive capacity directly related to the degree of overlap. Experimental results on the CIFAR10 dataset support these theoretical findings. In this section, a class of convolutional networks called Overlapping Convolutional Arithmetic Circuits (Overlapping ConvACs) is introduced. These networks share architectural features with standard ConvNets but include overlooked aspects such as any number of layers and unrestricted receptive fields and strides. The model is described for inputs with two spatial dimensions, like color images, and square-shaped convolutional filters. The Generalized Convolutional (GC) layer is defined as a fusion of a 1x1 linear operation with a pooling function. The Generalized Convolutional Network replaces pooling layers with convolutions with stride greater than 1. Input and output tensors have width, height, and depth parameters, with each spatial location corresponding to a window slice of the input tensor. A Generalized Convolutional Network (GCN) uses local receptive fields and pooling functions to define the operation of each layer. The network consists of multiple GC layers with specified parameters such as receptive field size, stride, and pooling function. The Generalized Convolutional Network (GCN) utilizes local receptive fields, strides, and pooling functions to define each layer's operation. The output of the last layer typically consists of a vector representing the score function for each class, with inference performed by selecting the class with the highest score. The first layer is considered a low-level feature representation shared across tasks and datasets within the same domain, treated as a separate fixed \"zeroth\" convolutional layer known as the representation layer. The Generalized Convolutional Network (GCN) uses local receptive fields, strides, and pooling functions to define each layer's operation. The output tensor of this layer is a function h y (x 1 , . . . , x N ). The network can be a common all-convolutional network with ReLU activations or a Convolutional Arithmetic Circuit (ConvAC) with product pooling functions. ConvACs are circuits containing product and sum operations, introduced by BID5. The Convolutional Arithmetic Circuit (ConvAC) framework, introduced by BID5, is typically described using tensor decompositions. When all layers in ConvAC are non-overlapping, the formulation coincides with a sequence of 1x1 convolutions and product pooling layers. However, if some layers overlap, it leads to Overlapping ConvACs. This model inherits traits from modern ConvNets, forms a universal hypotheses space, and allows for mathematical analysis based on measure theory. The Convolutional Arithmetic Circuit (ConvAC) framework, introduced by BID5, allows for mathematical analysis based on measure theory and tensor analysis. Generalized tensor decompositions can transfer theoretical results to standard ConvNets with ReLU activations. Empirically, ConvACs work well in various practical settings, such as optimal classification with missing data and compressed networks. Additionally, overlapping layers in ConvACs do not diminish their representational power, as networks with different architectures can realize the same functions. The Convolutional Arithmetic Circuit (ConvAC) framework allows for mathematical analysis based on measure theory and tensor analysis. Generalized tensor decompositions can transfer theoretical results to standard ConvNets with ReLU activations. Empirically, ConvACs work well in various practical settings. Using smaller local receptive fields, a proposition states that two GC Networks with a product pooling function can be equivalent if the architecture of one can be derived from the other through specific modifications. The local receptive fields of network A can be effectively shrunk to match those of network B, allowing for the realization of the identity function. Overlapping architectures are just as expressive as nonoverlapping ones, satisfying the efficiency property. Overlapping networks can lead to an increase in expressive capacity, sometimes resulting in an exponential gain. The methods for analyzing the expressive efficiency of overlapping ConvACs are described in this section. It covers the background on tensor analysis needed to understand the analysis, focusing on tensor matricization and rearranging entries to a matrix shape. The assumption of equal dimensions simplifies the analysis. The matricization of A with respect to the partition P \u00b7 \u222a Q, denoted by A P,Q, is the M |P| -by-M |Q| matrix holding the entries of A. The broader definition of a \"neuron\" is any scalar value in the output array of a network layer. The discussion focuses on comparing the efficiency of non-overlapping ConvACs to overlapping ConvACs for a fixed set of M representation functions. The discussion compares the efficiency of non-overlapping ConvACs to overlapping ConvACs. Overlapping ConvACs can realize additional functions outside the subspace of non-overlapping ConvACs. To compare both architectures, an auxiliary objective is needed. Grid tensors are used to compare architectures, specifically the grid tensor defined by the output of a ConvAC. Assuming linearly independent fixed representation functions, template vectors can be found for non-overlapping ConvACs to represent all possible grid tensors over these templates. The discussion focuses on comparing the efficiency of non-overlapping ConvACs to overlapping ConvACs using grid tensors. Template vectors are chosen for non-overlapping ConvACs to represent all possible grid tensors over these templates, ensuring F is non-singular. By examining the ranks of matricized grid tensors, the expressive efficiency of network architectures A and B can be compared. An upper-bound on the rank of A(h(A)) P,Q is sought, while showing that rank A(h(B)) P,Q can be significantly greater. The efficiency of non-overlapping ConvACs is compared to overlapping ConvACs using grid tensors. By examining matrix ranks, we can compare the expressive efficiency of network architectures A and B. The lemma connects upper-bounds for non-overlapping ConvACs with grid tensors induced by them. The efficiency of overlapping ConvACs is analyzed by examining the relationship between overlapping degree and rank. It is shown that overlapping ConvACs can achieve exponentially large ranks, proving their exponential efficiency compared to non-overlapping ConvACs. Measures of overlapping degree are defined, and main results are presented in sec. 4.2. Additional results related to \"Pooling Geometry\" are discussed in app. B. To analyze the efficiency of overlapping architectures, the measurement of the overlapping degree is rigorously formulated by defining the total receptive field and total stride. Proposition 1 shows that architectures may have a smaller effective total receptive field under certain parameter settings. The \u03b1-minimal total receptive field is then defined as the smallest effective total receptive field larger than \u03b1. The exact definitions are provided, highlighting the global statistics of the architecture. The total receptive field in networks is crucial for understanding their efficiency. Non-overlapping networks have a total receptive field equal to the total stride until the spatial dimension collapses to 1\u00d71. In contrast, overlapping networks can have a much faster growth in total receptive field. This difference means that non-overlapping networks are effectively shallower than overlapping networks. The main result presented is Theorem 1, which analyzes networks based on the point at which their total receptive field is sufficiently large. The main result presented is Theorem 1, which analyzes ConvAC networks with a fixed representation layer and multiple GC layers. The theorem states that for almost all choices of parameters, except a null set, a specific equality holds true for template vectors that are non-singular. The construction of an example in ConvAC networks with a fixed representation layer and multiple GC layers shows that a function cannot be approximated by a non-overlapping architecture if certain conditions are met. The key lies in designing filters that create a mostly diagonal matrix of rank D. The construction of ConvAC networks involves designing filters to create a mostly diagonal matrix of rank D, resulting in a product of entries that can be represented as a Kronecker product. This leads to an exponential form and can be extended to multiple layers with small local receptive fields. See appendices for definitions, lemmas, and proofs. The complexity of designing ConvAC networks involves creating filters to form a diagonal matrix of rank D, leading to an exponential form that can be extended to multiple layers with small local receptive fields. The lower bound for non-overlapping ConvAC architectures depends on the first layer where the total receptive field exceeds a quarter of the input, making it infeasible for architectures like GoogLeNet. After the spatial dimension collapses to 1\u00d71, the total receptive field and stride equal the width H of the representation layer. This implies that the next to last layer must have at least half the channels of the target network for one non-overlapping network to realize another. The first GC layer having a local receptive field R greater than a quarter of its input applies to any arbitrary sequence of layers. Assuming the stride S is less than H/2, a non-overlapping architecture satisfying this lower bound can represent any possible grid. The lower bound for representing any possible grid tensor is achieved by increasing the receptive field size in the architecture. Using small local receptive fields and pooling layers is a common practice to avoid resource-intensive large convolutions. The network structure is illustrated in FIG2. The network architecture comprises a sequence of GC blocks with varying local receptive field sizes and strides. The lower bound for this network is determined by the total receptive field and stride of each layer. The first layer that meets the conditions of the theorem is identified to simplify the general lower bound. The lower bound for the network architecture with varying local receptive field sizes and strides is determined by the total receptive field and stride of each layer. For typical values of M = 64, B = 5, and H \u2265 20, the lower bound is at least 64 20, showing an exponential separation from the non-overlapping case. As B grows, the bound approaches the result for large local receptive fields. When H grows, the lower bound is dominated by the local receptive fields. Additionally, a lower bound can be derived for a network following VGG style architecture with multiple convolutional layers before each \"pooling\" layer. Theoretical results from sec. 4.2 are confirmed in practice, showing tasks that require highly expressive overlapping architectures. Non-overlapping architectures would need to grow significantly to match the performance. The study confirms theoretical results by demonstrating the power of overlapping architectures in achieving high performance levels. By replacing convolutional layers with max-pooling layers and using the same number of channels, the network shows improved performance with 5 blocks and a final dense layer for classification. In the fifth \"conv-pool\" block, there is a final dense layer with 10 outputs and softmax activations. Two types of data augmentation schemes are used for training on the CIFAR-10 dataset: spatial augmentations involving random translations and horizontal flipping, and color augmentations involving hue, saturation, and luminance shifts. Data augmentation is employed to increase the difficulty of the dataset and prevent overfitting by small networks. Both spatial and color augmentation schemes are separately tested. The training process involves testing spatial and color augmentation schemes to enhance results. Training is done for 300 epochs using ADAM with standard hyper-parameters. Results show that higher receptive field values require fewer channels for the same accuracy. The source code for reproducing the experiments can be found at https://github.com/HUJI-Deep/OverlapsAndExpressiveness. The study found that networks with more overlapping layers require exponentially more channels to achieve the same performance as networks with fewer overlaps. There is a significant difference in the number of parameters between overlapping and non-overlapping architectures, with overlapping networks requiring significantly more parameters. Surprisingly, all overlapping networks achieve similar training accuracy with the same number of parameters, suggesting that even a small amount of overlapping can provide the benefits of overlapping networks. The study found that increasing the amount of overlapping in networks does not affect performance in terms of expressivity. The depth efficiency conjecture suggests depth is important in deep networks' success, but other attributes like overlapping receptive fields may also play a significant role. The study highlights that denser connectivity in neural networks leads to increased expressivity, independent of depth. It also explains why non-overlapping networks are rarely used in practice due to their limited expressivity compared to overlapping networks. The study shows that denser connectivity in neural networks increases expressivity, even in common modern architectures without fully-connected layers. The overlapping degree of networks is low but already in the exponential regime of expressivity, making them sufficiently expressive for practical needs. Further increasing overlapping degree has insignificant effects on performance, a conjecture to be investigated in the future. The role of receptive fields in neural networks has been studied in few works, showing that classification accuracy can decline with decreased overlaps. Using very large local receptive fields does not significantly improve performance compared to the increase in computational resources. While previous works focused on learning receptive fields from data, this analysis lays the groundwork for guiding architecture design by quantifying expressivity. Additionally, studying the effective total receptive field of different layers can provide insights for architecture design. The effective total receptive field in neural networks grows during training, suggesting that weights should be initialized with a large initial receptive field. Trained networks tend to maximize their effective receptive field, enhancing their expressive capacity. Overlapping architectures show an advantage over non-overlapping ones, as demonstrated theoretically and empirically. Our theoretical analysis is based on ConvACs framework extended to overlapping configurations. Previous studies suggest results can be transferred to standard ConvNets. Moving to overlapping architectures removes depth cap at log 2 (input size). The ConvAC architecture introduced by BID5 is like a regular ConvNet but uses linear activations and product pooling layers instead of non-linear activations. Each point in the input space is represented as a sequence of vectors, typically for images. The first layer applies representation functions to local patches, resulting in feature maps. The ConvAC architecture, introduced by BID5, uses linear activations and product pooling layers instead of non-linear activations in a regular ConvNet. It includes hidden layers with convolutional and pooling operations, leading to Y network outputs represented by score functions. The ConvAC architecture, introduced by BID5, utilizes linear activations and product pooling layers in Y network outputs to classify X into classes based on score functions. The coefficients tensor A y is a multi-dimensional array specified by N indices, with entries given by polynomials in the network's conv weights. Overlaps in functions represented by ConvACs lead to inefficiency in non-overlapping ConvAC implementations, requiring a different pooling geometry as shown in BID4. A ConvAC with a different pooling geometry may implement some functions more efficiently than a standard ConvAC, but the reverse is also true. Overlapping operations are not simply equivalent to a ConvAC with different pooling geometry. However, a ConvAC using overlaps can efficiently implement any function that a non-overlapping ConvAC with standard pooling can. Pooling can be more efficient in some cases. Overlapping architectures can be exponentially more efficient than non-overlapping ConvACs. Theorem 2 shows that overlapping architectures can induce grid tensors with exponential efficiency. The text discusses the efficiency of overlapping architectures in inducing grid tensors with exponential efficiency, compared to non-overlapping ConvACs. The parameters of the layers are either \"unshared\" or \"shared\", leading to different results in terms of matricized rank. The proof involves constructing separate networks for each possible matricization, demonstrating the efficiency with respect to the Lebesgue measure. The proof shows that the lower bound holds almost everywhere for all possible pooling geometries with respect to the Lebesgue measure. While pooling geometry alone may be less expressive than overlapping networks, the combination of overlaps and alternative pooling geometries is still unknown. The choice of pooling geometry can provide an inductive bias that may aid in solving specific tasks. In this section, the proofs for the theorems and claims in the article are presented, along with the preliminaries needed to understand them. It starts with an introduction to tensor analysis and relevant results linking tensors to ConvACs. Basic definitions and operations related to tensors are discussed, including the tensor product and rank-1 tensors. The text discusses tensor analysis, including the concept of tensor matricization and rank-1 decomposition. Matricization of a tensor with respect to a partition is explained, and the Kronecker Product is introduced as a result of applying the matricization operator. The Kronecker Product is defined for tensors A and B, resulting in a matrix A B. It has the property that rank (A B) = rank (A) \u00b7 rank (B). To compute rank (A P,Q), it is decomposed into a Kronecker product of matrices. If a linear transform F with non-singular matrices is invertible, the matrix rank of A P,Q equals the matrix rank of F(A) P,Q. The concept of grid tensors is defined for functions in ConvACs, with different network architectures corresponding to known tensor decompositions. Shallow networks correspond to rank-1 decompositions, while deep networks decompose the coefficients tensor A y. In BID4, the matrix rank of the coefficients tensors A y serves as a bound for the size of networks decomposing A y. For non-overlapping ConvAC and \"low-high\" partition P = {1, . . . , N /2}, Q = { N /2 + 1, . . . , N }, the rank of the matricization A y P,Q is a lower-bound on the number of channels in the next to last layer of any network decomposing A. In the case of square inputs, indices are represented by pairs (j, i) denoting spatial locations of patches x (j,i). The matrix rank of coefficients tensors A y in BID4 serves as a bound for network size. For non-overlapping ConvAC, the rank of matricization A y P,Q is a lower-bound on channels in the next to last layer. Grid tensors are analyzed for ConvACs using template vectors. If representation functions are linearly independent and continuous, template vectors can be chosen for a non-singular F. The matrix rank of coefficients tensors A y in BID4 serves as a bound for network size. Template vectors can be chosen for a non-singular F, leading to a lower bound on the size of non-overlapping ConvACs. The proof of lemma 1 and lemma 2 is provided, along with the relationship between the grid tensor and coefficients tensor matricization rank. The text discusses the matrix rank of coefficients tensors in BID4, providing bounds for network size. It also introduces lemma 3 regarding matrices with polynomial functions. Additionally, it simplifies notations for the GC layer with product pooling function. The parameters of the GC layer are represented, and an equality between input X and output Y is established. In the context of discussing the matrix rank of coefficients tensors in BID4 and introducing lemma 3, the text explores the \"unshared\" case where different weights and biases are considered for each spatial location in a GC layer. The proposition and theorem proofs are based on parameters shared across spatial locations and the possibility of realizing a function with different weights and biases for each location. The text discusses setting specific values for coordinates in a function realized by a GC layer with local receptive fields and strides. It proves that for certain parameters, the function can be the identity function. The proof involves showing that the grid tensor induced by the GC network has a rank satisfying a specific equation. The proof involves constructing an example to achieve a desired matricization rank for a ConvAC with specific parameters in the GC layers. The proof involves constructing an example to achieve a desired matricization rank for a ConvAC with specific parameters in the GC layers. The following layer has a local receptive field large enough to match spatial locations in the input, resulting in an exponential form of the rank as given in the theorem. The proof involves constructing an example to achieve a desired matricization rank for a ConvAC with specific parameters in the GC layers. The rank is exponential as given in the theorem. The parameters of the GC layers are denoted, and there exists an assignment to \u03b1 and other parameters such that a specific equation is satisfied. The proof is symmetric for different partitions, and the entry of the induced grid is computed. The entry A(h) of the induced grid tensor is computed for arbitrary indices, with parameters set for layers following the first GC layer. The second GC layer parameters are set for summing along the channels axis and taking products over non-overlapping local receptive fields. The second GC layer parameters involve summing along the channels axis and taking products over non-overlapping local receptive fields of size R. This process is further detailed by reducing equations to a product of matrices and deriving specific conditions for different cases. The text discusses how the expression for f(u, v) depends on specific indices in the context of matrices and matrix rank. It also explores conditions where f(u, v) is influenced by certain indices and how it affects the overall claim. The text delves into the relationship between specific indices and the expression for f(u, v) within matrices and matrix rank, ultimately impacting the overall claim. The text discusses the relationship between specific indices and the expression for f(u, v) within matrices and matrix rank, impacting the overall claim. The proof involves finding the first layer with a total receptive field greater than H/2 and estimating its total stride. Claim 5 helps identify the first B \u00d7 B layer with a receptive field larger than H/2, and when combined with claim 7, the limits and special case for B \u2264 H/5 are determined."
}