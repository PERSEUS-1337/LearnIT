{
    "title": "S1ef6JBtPr",
    "content": "In this paper, the problem of multi-agent reinforcement learning is approached by formulating it as performing inference in a graphical model. A practical off-policy maximum-entropy actor-critic algorithm called Multi-agent Soft Actor-Critic (MA-SAC) is derived for approximate inference using variational inference. MA-SAC is effective in both cooperative and competitive settings, outperforming other methods in experiments. Our work introduces MA-SAC, a multi-agent RL algorithm derived from a probabilistic framework, showing superior performance in various scenarios. It offers a unified perspective on maximum-entropy algorithms in multi-agent settings, contrasting with traditional RL focused on single-agent learning in static environments. In multi-agent settings, stability issues can arise if agents are independently trained using standard single-agent RL methods. Treating multiple agents as a single centralized meta-agent for training becomes infeasible due to the exponential growth of the action space. Executing the centralized policy may not always be possible due to geographic separation, communication overhead, and other challenges. Designing a reward function for competitive agents in a centralized meta-agent setup is very challenging, making it generally impractical. In multi-agent scenarios, traditional single-agent RL methods struggle to handle cooperative and competitive interactions. New RL algorithms are needed to address this challenge. The problem is framed as probabilistic inference in a graphical model, extending from single-agent to multi-agent setups. In multi-agent scenarios, traditional single-agent RL methods struggle to handle cooperative and competitive interactions. New RL algorithms are needed to address this challenge. The environment is modeled using separate but related Markov Decision Processes for each agent, leading to the development of Multi-agent Soft Actor-Critic (MA-SAC) algorithm, which follows the centralized training, decentralized execution paradigm. The Multi-agent Soft Actor-Critic (MA-SAC) algorithm involves each agent learning its own policy while being aware of other agents. MA-SAC avoids issues of independent training and centralized policies by setting a tunable temperature parameter. This modification improves the inference procedure and leads to better performance compared to the Multi-agent Deep Deterministic Policy Gradients algorithm (MADDPG). The MA-SAC algorithm outperforms MADDPG on cooperative and competitive tasks by presenting a probabilistic view of multi-agent reinforcement learning. It derives an off-policy maximum entropy actor-critic algorithm and demonstrates its effectiveness in practice. The framework allows for the derivation of maximum-entropy variants of other reinforcement learning algorithms in the multi-agent setting. In recent years, RL has advanced in game playing and robotics. Researchers have attempted training all agents independently in a multi-agent setup using single-agent RL algorithms, but this approach has limitations. Foerster et al. proposed a modification to independent Q-learning to stabilize training. Agents require additional information to deal with non-stationarity, which can be provided through multi-agent communication or centralized training of decentralized policies. Agents in communication-based approaches use an emergent language to jointly execute tasks, requiring ongoing communication post-training. Centralized training and decentralized execution allow agents to make independent decisions based on local observations. Some approaches support cooperative agents only, while others also accommodate competitive agents. MA-SAC follows the centralized training and decentralized execution paradigm, supporting both cooperative and competitive agents. It is a maximum-entropy based algorithm derived from single-agent RL algorithms like soft-Q-learning and soft-actor-critic. This approach provides a different perspective on the control problem, leading to more stable and easier to train algorithms. MA-SAC is a maximum-entropy algorithm derived from single-agent RL algorithms. It can be applied in cooperative environments and outperforms MADDPG on several tasks. The algorithm is similar to MADDPG when the temperature parameter is set to zero. In Iqbal & Sha (2019), an improvement in scalability of MADDPG using an attention-based mechanism in the critic is discussed. They provide a unified probabilistic framework for multi-agent RL and derive MA-SAC from it. A Markov Decision Process (MDP) models an environment with a single trainable agent, while a Markov Game (MG) supports multiple agents in the environment. In multi-agent reinforcement learning, each agent has its own action space and reward function. The transition probability function considers actions taken by all agents, and agents may only observe a part of the environment. Agents strive for optimal behavior by modulating their policies, which do not encode any notion of optimality. In multi-agent reinforcement learning, agents modulate their policies to achieve optimal behavior by considering their local observations. Each agent follows its own policy, taking into account the actions of other agents in the environment. In multi-agent reinforcement learning, agents adjust their policies to achieve optimal behavior based on the actions of other agents. The concept of a best response policy becomes crucial in this scenario, where each agent aims to maximize its rewards while considering the policies of others. Probabilistic graphical models are used to formalize the best response strategy, treating all other agents as part of the environment. Each agent's perspective is modeled as a separate Markov Decision Process (MDP), with policies encoded into the environment. In multi-agent reinforcement learning, agents adjust their policies to achieve optimal behavior based on the actions of other agents. The environment dynamics are non-stationary as other agents' policies change. Standard single-agent RL algorithms cannot be used independently to train each agent. All variables, including actions of other agents and environment state, are part of the environment from the perspective of agent i. Centralized training of agents allows them to observe actions taken by all other agents, aiding in learning by making the environment appear stationary. Despite learning in a centralized manner, the policies are decentralized to avoid communication overhead. MDPs do not inherently define optimality, so additional optimality variables are added to the graphical model for each agent. The optimality variables indicate the optimality of actions taken by each agent. The optimal policy is found by conditioning on these variables. However, this policy is not stationary, decentralized, or practical to execute simultaneously by all agents. It is risk-seeking and may lead to bad outcomes for high rewards infrequently. To address the risk-seeking behavior of policies that may lead to bad outcomes, structured variational inference is used to approximate the posterior distribution over trajectories. The posterior remains faithful to environment dynamics to avoid risk-seeking behavior, allowing for easy recovery of optimal policies. The policy of each agent is parameterized by \u03b8 j, and the posterior over trajectories is approximated using a specific distribution. The approximate posterior in structured variational inference uses the same environment dynamics model as the Markov game and assumes agents act independently. Only \u03b8 i can be adapted to approximate the true posterior, while other policy parameters remain constant. The ELBO expression is derived using the joint distribution and the approximate posterior. The prior distribution over actions is assumed to be uniform. The MA-SAC maximizes ELBO i for all agents in an actor-critic framework by defining the function Q i (s (t) , a (t) ) in the Markov game. The problem is cast in an actor-critic framework to address high variance in gradient estimates when directly optimizing ELBO i over \u03b8 i for all agents. The MA-SAC framework maximizes ELBO i for all agents in an actor-critic setup by defining Q i (s (t) , a (t) ) in the Markov game. Q i uses actions of all agents at time t as input and is parameterized using \u03c6 i \u2208 R d Q. The function is optimized by minimizing the error using a replay buffer D. One-step lookahead is used to compute Q i (s (t) , a (t) ) under the current parameters \u03c6 i. The lookahead term Q i (s (t+1) , a (t+1) ) is computed using current parameters \u03c6 i. Q i estimates are used to find \u03b8 i maximizing ELBO in equation 5. Centralized training setup is needed to sample from policies of all agents for E Qi computation. The objective is to optimize the policy of agent i for choosing the correct action at time t. The objective is to optimize the policy of agent i for choosing the correct action at time t by approximating expectations using samples and computing gradients using RE-INFORCE or reparameterization. MA-SAC and MADDPG algorithms are used, with a temperature parameter \u03b1 i for entropy and reward maximization. The discount factor \u03b3 must also be incorporated in the optimization process. The discount factor \u03b3 is crucial in the probabilistic framework, representing the possibility of an agent dying and entering an absorbing state with zero rewards. It is multiplied to the expectation term in equation 7 to incorporate discounted rewards. Neural networks are used to parameterize agent policies and Q functions, with twin sub-networks for Q functions to improve learning stability. Multi-agent environments proposed by Lowe et al. are utilized in the experiments. In experiments, multi-agent environments from Lowe et al. (2017) are used, where agents observe the environment state, take actions, and can communicate with each other. Agents receive rewards based on their actions, potentially different for each agent. In experiments, multi-agent environments from Lowe et al. (2017) are used, where agents observe the environment state, take actions, and can communicate with each other. Agents receive rewards based on their actions, potentially different for each agent. The environments used in the experiments can be cooperative or competitive, with different reward structures. One environment involves cooperative navigation with n agents and n landmarks, where agents aim to cover all landmarks simultaneously. Another environment is cooperative communication with 3 landmarks, where an agent is rewarded based on different colored landmarks. In a multi-agent environment, agents communicate to achieve goals. One landmark is targeted based on color, with a speaker knowing the color and guiding a listener. In another scenario, cooperating agents chase an adversary for rewards. The environment involves physical deception with one adversary and two agents. In a physical deception scenario, two cooperating agents and one adversary compete to reach a target landmark. The cooperating agents know the target landmark, while the adversary does not. Rewards and penalties are based on proximity to the target landmark. The agents must visit both landmarks to confuse the adversary in a mixed cooperative-competitive setting. Q functions and agent policies are parameterized using neural networks. In a physical deception scenario, agents and an adversary compete to reach a target landmark. Rewards are based on proximity to the target. Neural networks parameterize Q functions and agent policies. Fully connected neural networks with two hidden layers of 128 units each are used, along with ReLU activation function. Gumbel-Softmax is used for discrete action sampling. Target networks are updated with each gradient step. Training is done for 15000 episodes, each of length 25, with a replay buffer of size 10^6. In physical deception scenarios, agents and an adversary compete to reach a target landmark, with rewards based on proximity. Q functions and agent policies are parameterized by neural networks. Fully connected neural networks with two hidden layers of 128 units each are used, along with ReLU activation function. Gumbel-Softmax is used for discrete action sampling, and target networks are updated with each gradient step. Training is conducted for 15000 episodes, each of length 25, with a replay buffer of size 10^6. In subsequent experiments, a replay buffer of size 10^6 is populated in a round-robin fashion, with a randomly sampled batch of 1024 examples used for each gradient step. The value of \u03b1 i is empirically tuned separately for each environment, with specific values determined through grid search. A consistent discount value of 0.95 is used across all experiments. MADDPG and MA-SAC controlled agents are compared in different environments, with rewards averaged over 100 independent episodes. The rewards were averaged over 100 independent episodes and policies were converted into deterministic policies before testing. The process was repeated 5 times with different random seeds. Cooperative navigation and communication tasks had both agents controlled by the same algorithm, while predator-prey and physical deception tasks had agents controlled by different algorithms. MA-SAC performed at least as well as MADDPG on all tasks and outperformed it on most tasks. The temperature parameter \u03b1 i played a crucial role in MA-SAC's performance. The parameter \u03b1 i is crucial for MA-SAC performance, influencing policy entropy and rewards. Tuning \u03b1 i is essential for optimal performance, with high values leading to random behavior and low values risking local optima. MA-SAC produces stochastic policies for easier training, and experimenting with automatic tuning techniques is left for future work. Training with a stochastic policy can lead to a more robust policy, similar to training with an ensemble of deterministic policies. Converting stochastic policies to deterministic ones can improve performance. Centralized training scalability challenges arise with an increasing number of agents, requiring parameterization of the Q network for efficiency. Various approaches have been proposed to address this issue. In multi-agent reinforcement learning, alternative parameterizations of Q i can be integrated with MA-SAC. Centralized training can be achieved without access to other agents' policies by training local approximations to opponents' policies. The problem is framed as probabilistic inference in a graphical model, leading to an off-policy maximum entropy actor-critic algorithm based on centralized training and decentralized execution. The proposed algorithm, based on centralized training and decentralized execution, outperforms MADDPG on cooperative and competitive tasks. Various ideas for parameterizing Q-functions can be integrated with MA-SAC to improve scalability. The framework can also derive maximum-entropy variants of other RL algorithms for multi-agent settings, leaving room for future exploration."
}