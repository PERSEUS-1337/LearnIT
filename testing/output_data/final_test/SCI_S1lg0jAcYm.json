{
    "title": "S1lg0jAcYm",
    "content": "To backpropagate gradients through stochastic binary layers, the ARM estimator is proposed for adaptive variance reduction in Monte Carlo integration. It combines variable augmentation, REINFORCE, and reparameterization to merge two expectations using common random numbers. The ARM estimator shows superior performance in auto-encoding variational inference and maximum likelihood estimation for discrete latent variable models with one or multiple stochastic binary layers. Python code for reproducible research is available. Recent interest in estimating parameters to maximize the expectation of a function of a random variable, with applications in variational inference and Bayesian modeling. The reparameterization trick is a common technique used for computing gradients in these scenarios. The reparameterization trick is commonly used to compute gradients for maximizing the expectation of a function of a random variable. However, this trick may not be applicable to discrete random variables, limiting its use in practice. To address the high variance issue in computing gradients, introducing a baseline can help reduce the variance of the method. The reparameterization trick is commonly used to compute gradients for maximizing the expectation of a function of a random variable. Introducing a baseline (control variate) can help reduce the variance of methods like REINFORCE, REBAR, and RELAX, which aim to produce low-variance and unbiased gradient estimators. However, estimating baseline parameters can increase computation and introduce potential conflicts between minimizing sample variance and maximizing the expectation objective. Introducing a baseline can reduce variance in methods like REINFORCE, REBAR, and RELAX. However, estimating baseline parameters can conflict with maximizing the expectation objective. A novel gradient estimator called ARM is proposed for binary latent variables, which is unbiased, low-variance, and simple to implement. It involves rewriting the expectation with respect to Bernoulli random variables as one with respect to augmented exponential random variables. The ARM estimator is derived in the augmented space using strategies like sharing common random numbers and antithetic sampling. It improves the REINFORCE estimator by introducing an optimal baseline function for variance reduction. The ARM estimator utilizes the function f and random noises for variance reduction without adding extra parameters. It outperforms REBAR and RELAX by minimizing sample variance in gradient estimates. Experimental results demonstrate its unbiasedness, low variance, fast convergence, and effectiveness in predicting outcomes for discrete latent variable models. Python code for reproducible research is available. The ARM estimator uses the sigmoid function and indicator function for variance reduction in gradient estimates. Theorem 1 presents the gradient of expectations for binary random variables, with a multivariate generalization of the univariate AR gradient. Python code for reproducible research is provided at the given GitHub link. The ARM estimator utilizes the sigmoid and indicator functions for variance reduction in gradient estimates. It is shown that the ARM estimator has smaller worst-case variance than REINFORCE for the univariate case. The derivation of the univariate AR gradient involves adding an antithetic sampling step or subtracting the AR estimator by a baseline function with specific properties. The ARM estimator is defined for the objective function with Monte Carlo samples, showing improved performance compared to the AR estimator. The ARM estimator, denoted as g AR 2K, uses DISPLAYFORM2 and DISPLAYFORM3. The optimal anti-symmetric baseline function for variance reduction is Equation FORMULA0. The optimal baseline function g AR (u) = f (1 [u<\u03c3(\u03c6)] )(1 \u2212 2u) minimizes the variance of Monte Carlo integration. The optimal anti-symmetric baseline function for the AR estimator leads to lower estimation variance than any constant baseline function. Stochastic neural networks can represent complex distributions and resist overfitting, but training can be challenging with stochastic discrete layers. Training stochastic neural networks, especially with stochastic discrete layers, can be challenging. The ARM estimator can be applied for gradient backpropagation in stochastic binary networks for both auto-encoding variational inference and maximum likelihood estimation. The gradient of the ELBO with respect to the hidden layers can be expressed and estimated using a single Monte Carlo sample. The proof of Proposition 6 in Appendix C discusses the computation complexity of vanilla REINFORCE for stochastic hidden layers. The ARM estimator's complexity depends on the relative cost of evaluating the function f and gradient backpropagation. Proposition 7 extends these findings to stochastic binary networks, providing expressions for gradient calculations. The ARM estimator is compared to other unbiased estimators like REINFORCE and RELAX in optimizing \u03c6 to maximize a function. The closer p0 is to 0.5, the more challenging the optimization becomes. The ARM gradient can be expressed as DISPLAYFORM3 with a single random sample for Monte Carlo integration. The REINFORCE and AR estimators have high variance gradients, leading to potential divergence if the stepsize is too large. In contrast, RELAX and ARM show lower estimation variance. The ARM estimator closely matches the true probability with univariate estimation. The optimal solution for \u03c3(\u03c6) is 1 when p0 is less than 0.5. The trace plots show true/estimated gradients and Bernoulli probability parameters updated via gradient ascent. Gradient variances for p0 = 0.49 are estimated using Monte Carlo samples, with distinct behavior from true gradients. The univariate ARM estimator approximates the true gradient behavior well for learning with gradient ascent. It has the lowest estimation variance compared to other estimators with the same number of Monte Carlo samples. Sample mean, standard deviation, and signal-to-noise ratio are plotted against the value of \u03c6 for each estimator in Figure 5. The ARM estimator outperforms REINFORCE and RELAX in gradient estimation, showing high signal-to-noise ratios regardless of the optimization problem. The ARM estimator provides high signal-to-noise ratios for discrete VAE optimization, outperforming other stochastic gradient estimators like REINFORCE and RELAX. In discrete VAE optimization, different network architectures like \"Nonlinear,\" \"Linear,\" and \"Linear two layers\" are considered using various binarization methods such as MNIST-static and MNIST-threshold. The ARM estimator shows superior performance compared to REINFORCE and RELAX in optimizing discrete VAEs. In discrete VAE optimization, different network architectures are considered using various binarization methods like MNIST-static and MNIST-threshold. The training process involves maximizing a single-Monte-Carlo-sample ELBO with Adam BID13 and selecting the learning rate from a predefined range. Batch sizes are set for different datasets, and test negative log-likelihoods are summarized in TAB2 for MNIST-static. Additional results are provided in TAB4 of the Appendix, along with trace plots of training and validation negative ELBOs on different datasets. In discrete VAE optimization, different network architectures are considered using various binarization methods like MNIST-static and MNIST-threshold. The training process involves maximizing a single-Monte-Carlo-sample ELBO with Adam BID13 and selecting the learning rate from a predefined range. Batch sizes are set for different datasets, and test negative log-likelihoods are summarized in TAB2 for MNIST-static. Additional results are provided in TAB4 of the Appendix, along with trace plots of training and validation negative ELBOs on different datasets. In Figure 2, REBAR and RELAX show signs of overfitting on MNIST-static and Omniglot using the \"Nonlinear\" architecture, while ARM demonstrates fast convergence without overfitting, providing state-of-the-art performance with low computational cost. ARM achieves significantly lower test negative log-likelihoods with low computational cost compared to vanilla REINFORCE on MNIST-static. It shows better performance and robustness on different network architectures, resisting overfitting and demonstrating better generalization ability. REBAR and RELAX exhibit severe overfitting on MNIST-static and OMNIGLOT with the \"Nonlinear\" network architecture. ARM outperforms REBAR and RELAX in terms of lower overfitting and faster computation time. Trace plots show ARM has higher variance but better generalization ability. The hypothesis is that REBAR and RELAX may favor suboptimal solutions with lower gradient variance, leading to overfitting. ARM shows lower variance and better convergence, outperforming REBAR and RELAX in terms of overfitting and computation time. The conditional distribution is approximated using a stochastic binary network with two hidden layers. Training is done with Adam optimizer, learning rate of 10^-4, mini-batch size of 100, and 2000 epochs. The accuracy of conditional density estimation is evaluated by estimating the negative log-likelihood over the test set. The ARM estimator achieves the lowest test negative log-likelihood in training a discrete latent variable model with stochastic binary layers. It outperforms other biased and unbiased gradient estimators on similarly structured networks. The ARM estimator, without relying on extra parameters for variance reduction, efficiently computes gradients for Bernoulli distributions using correlated binary latent variables. It leads to fast convergence and low test negative log-likelihoods in training stochastic binary feedforward neural networks. Extensions include generalizing to multivariate categorical latent variables and applying to reinforcement learning with discrete action spaces. The ARM estimator efficiently computes gradients for Bernoulli distributions using correlated binary latent variables, leading to fast convergence in training stochastic binary feedforward neural networks. It can be extended to multivariate categorical latent variables and applied to reinforcement learning with discrete action spaces. The exponential distribution is denoted as t \u223c Exp(\u03bb), with probability density function p(t | \u03bb) = \u03bbe^(-\u03bbt), mean E[t] = \u03bb^(-1), and variance var[t] = \u03bb^(-2). The Bernoulli random variable can be reparameterized using augmented exponential random variables. The gradient computation is facilitated by reparameterizing the expectation with respect to two augmented exponential random variables. This reparameterization allows for the expression of the gradient using REINFORCE and another reparameterization. The gradient can be expressed using reparameterization with augmented exponential random variables, leading to the Augment-REINFORCE (AR) estimator. Swapping the indices of the exponential random variables allows for an equivalent expression of the gradient. The merging of terms inside the expectation is motivated by their potential positive correlation. The merging of terms in the gradient estimator is motivated by their potential positive correlation, leading to the Augment-REINFORCE-merge (ARM) estimator. This new approach controls estimation variance by sharing exponential random variables for Monte Carlo integration. Further optimization can be done by taking a weighted average of the terms. The ARM gradient estimator controls estimation variance by sharing exponential random variables for Monte Carlo integration. The variance of the ARM gradient is bounded by 1/25 times the square of the difference in function values, with a worst-case variance smaller than that of REINFORCE. The proof for K = 1 automatically follows for K > 1, showing that the estimation variance of ARM is lower than that of REINFORCE when the function is always positive or negative. The variance of the ARM gradient estimator is lower than that of REINFORCE when the function is always positive or negative. To maximize variance reduction, constrained optimization is equivalent to a Lagrangian problem. For univariate AR gradient, estimators are computed for different values of \u03c6 and p0. Theoretical gradient standard deviations and signal-to-noise ratios are also calculated. Figure 8 shows randomly selected example results. Figure 8 displays example results of predicting the lower half of a MNIST digit using a binary stochastic network with two binary linear stochastic hidden layers. Notable variations between random draws are highlighted in red squares."
}