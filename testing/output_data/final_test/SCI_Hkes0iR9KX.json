{
    "title": "Hkes0iR9KX",
    "content": "Most existing Graph Neural Networks (GNNs) are extensions of Convolutional Neural Networks (CNNs) to graphs, involving message passing between nodes and global feature pooling. However, in datasets with unlabeled nodes, this approach may fail in learning simple graph classification tasks. To address this, a two-step method is proposed: first, a graph embedding algorithm generates continuous feature vectors for each node, then GNN is applied for improved performance. The GNN is applied to the point-cloud representation of the graph provided by the embedding method, learning to infer the topological structure encoded in the spatial distribution of embedded vectors. The approach is extended to graph clustering and a new architecture is proposed, utilizing spatial representation for a graph pooling algorithm. The problem of graph down-sampling is transformed into a column sampling problem, improving state-of-the-art results for several benchmark datasets. The adjacency matrix of a graph shows local connectivity, allowing for local feature aggregation in Graph Neural Networks (GNNs) through message passing between nodes. This approach significantly improves results for various datasets, such as the PTC dataset, by over 22%. The goal is to design neural network architectures that enable deep networks to directly work with graphs, leveraging the success of deep learning methods in machine learning applications. In this paper, the limitations of current Graph Neural Network (GNN) architectures are addressed. GNNs may fail to infer the topological structure of a graph if it is not labeled or if the feature vectors do not carry relevant information. Additionally, GNNs struggle with hierarchical feature learning due to the lack of tensor representation in graphs. To overcome these shortcomings, a spatial representation of the graph is provided to the GNN in the proposed approach. The proposed approach enhances Graph Neural Networks by providing a spatial representation of the graph, improving topological structure inference and introducing a new graph pooling method. The proposed approach advances the state-of-the-art results for 5 benchmark datasets by merging closest pairs of nodes in the spatial domain and utilizing geometrical representation to infer graph structure. Bold-face letters denote matrices and vectors, with specific notation for elements and sets. The proposed approach advances graph analysis by merging nodes in the spatial domain and using geometrical representation. It translates the problem into a point-cloud BID30 analysis. The deep network is given the geometrical information to infer graph structure. The proposed method advances graph analysis by merging nodes in the spatial domain and using geometrical representation. A graph embedding method aims to find a continuous feature vector for each node of the graph, encoding the topological structure in the spatial distribution of the feature vectors. DeepWalk and Node2Vec generalize word embedding advances to graphs, with DeepWalk using local information from random walks to learn node representations. Graph kernels are tools for measuring similarity between graphs, allowing kernel machines like SVM to work directly on graphs. They decompose graphs into substructures such as walks, subgraphs, paths, and sub-trees to compute similarity. However, graph kernels are computationally expensive due to the need to compute similarities between every pair of graphs in the training set. The complexity of training a kernel based method scales with the square of the training data size. Computing the kernel matrix elements can be computationally expensive, especially for graph kernels like the shortest path graph kernel. The proposed method has linear complexity with the number of nodes and training data size, outperforming other graph kernels. Another issue with graph kernels is that the features used for classification are independent of the dataset. Graph Neural Networks have gained interest for working with graphs, adopting convolutional network structures. Unlike images, there is no specific order of neighboring nodes in graphs, so feature vectors are uniformly aggregated. The feature matrix X is updated as X \u21d0 f(AXW), where W is the weights matrix for all nodes and f(\u00b7) is a non-linear function. This operation can be repeated k-times to receive \"messages\" from nodes within a certain distance. The lack of tensor representation in graphs makes pooling challenging in Graph Neural Networks. A recent architecture proposes sampling nodes and applying a one-dimensional CNN, but this approach may not capture the graph's structure effectively. A soft graph pooling method introduced in another study aims to address this issue. In BID37, a soft graph pooling method was proposed to map nodes with similar adjacency vectors to the same cluster in the pooled graph. However, this method may not be suitable for large sparsely connected graphs as orthogonal adjacency vectors can exist within the same cluster. The method also fails to preserve the sparsity of the adjacency matrix. In contrast, BID7 and BID11 combined graph clustering algorithms with GNN for graph downsizing without assuming pre-clustered graphs, dynamically pooling based on learned feature vectors. The CNN uses spatial order to aggregate local feature vectors with different weight matrices. In GNNs, the feature vector of a node is updated based on neighboring nodes using a weight matrix. If the graph is not labeled, feature aggregation is done accordingly. In GNNs, feature aggregation is based on the degree of the node, with information propagated around the graph. However, this approach does not learn the topological structure of the graph. In GNNs, feature aggregation is done using aggregate functions like element-wise max or mean. The inability of GNNs to distinguish between different clusters in a graph is highlighted, showing their limitations in solving simple graph classification problems. The embedding step in deep learning language modeling methods projects words with similar meaning to close data points in a continuous space. GNNs lack this important embedding layer, making it difficult for the network to understand node differences and similarities in a graph. In text data, each sentence can be represented as a sequence of nodes connected to each other, with a fixed dictionary of words. The GNN lacks a fixed embedding layer like deep networks for text data. A two-step method is used in this paper: graph embedding to represent the graph in a continuous space, followed by a neural network analyzing the spatial representation to perform tasks. This approach translates the graph analysis problem into a point-cloud analysis problem. The lack of a pooling function is another motivation for this work. The lack of a pooling function in existing GNNs motivates the development of a new graph pooling method. Inspired by deep networks analyzing point-cloud and text data, an embedding step is used to provide a geometrical representation of the graph. This step transforms the graph into a point-cloud in the embedding space, where nodes with similar structural roles are represented by close points. The discussion in Section 3 highlights the use of a graph embedding method to independently solve the embedding problem for each graph. DeepWalk is utilized as the graph embedding algorithm, with the input to the deep network being the concatenation of embedding vectors and node attributes. The network includes an initial feature transformation stage with fully connected layers, Batch Normalization, and an element-wise operation. The network architecture includes a linear transformation, Batch Normalization, and an element-wise non-linear function. Local Features Aggregation combines node feature vectors with neighboring nodes. Graph Pooling is a challenge in extending CNN architecture to graphs, as accurately representing the topological structure is difficult. The proposed pooling function aims to preserve the spatial distribution of feature vectors in graph nodes. It dynamically down-samples the graph based on the feature vectors' distribution, unlike predefined methods in CNNs. After k steps of pooling, the graph size is reduced to n/2^k. The final aggregator uses the element-wise max function. The proposed pooling function down-sizes the graph to n/2^k nodes using the element-wise max function. Edge attributes can be used for feature aggregation in the network architecture. The network includes multi-layer perceptrons with batch normalization and dropout layers. Defining a metric for preserving the graph's topological structure is challenging, but measuring accuracy for a subset of nodes is more straightforward. The proposed method focuses on sampling informative data points in graph down-sampling by considering the spatial distribution of feature vectors to preserve the topological structure. The algorithm computes distances between data points, identifies closest pairs, and merges nodes accordingly. The algorithm focuses on down-sampling graphs by merging nodes based on spatial distribution of feature vectors. Closest pairs of nodes are identified and merged, down-sizing the graph by a factor of 2 in each pooling layer. If the graph has an odd number of nodes, the remaining node is merged with itself. The algorithm down-samples graphs by merging nodes based on spatial distribution of feature vectors. Closest pairs of nodes are identified and merged, reducing the graph size by half in each pooling layer. If the graph has an odd number of nodes, the remaining node is merged with itself. The down-sampled graph is transformed into a point-cloud for graph clustering, inspired by deep neural network architectures for point-cloud segmentation. In BID30, a graph clustering architecture is proposed in which the global feature vector is concatenated with node feature vectors to understand each node's role in the graph structure. The last layer classifies each node independently, with multiple correct clustering labels for each graph. The approach is tested on synthesized graphs to demonstrate the limitations of conventional GNNs in clustering unlabeled graphs. The proposed approach involves assigning each node in a graph a distinct data point in continuous space and representing close nodes with close data points. Inspired by weighted feature aggregation in CNNs, the weight matrix assigned to neighboring nodes is conditioned on the difference between their feature vectors. A side network is used to generate the weight matrix for the main network, similar to a previous idea in BID37. In BID37, a proposal was made to condition weight matrices on edge attributes, but edge attributes may not always specify the relation between neighboring nodes. The conventional GNNs can struggle to infer the graph's structure, as shown in experiments with synthetic data. The proposed approach is then applied to various graph classification benchmarks, outperforming kernel-based methods and recent deep learning approaches in classification accuracy. DeepWalk BID29 is used for graph representation, and training is done using the Adam method in Pytorch with a dropout layer included. In the proposed method, a dropout layer with dropout rate 0.6 is used after the last two dense layers. Feature vectors of nodes are concatenated with their embedding vectors, and the self covariance matrix of each node is computed. Concatenating the vectorized form of the self covariance matrices with node feature vectors can improve classification accuracy for some datasets. The proposed method uses a dropout layer with a dropout rate of 0.6 after the last two dense layers. It concatenates feature vectors of nodes with their embedding vectors and computes the self covariance matrix of each node. This technique can enhance classification accuracy for certain datasets. In a separate analysis, three graph tasks with synthetic data demonstrate that GNNs without an embedding stage may struggle to infer graph structure. The study compares the performance of the proposed method, a network from BID38, and a simple GNN with 4 steps of message passing and an aggregate function. The generated graphs consist of 3 to 6 clusters, each with 30 to 60 nodes connected to 5 other nodes within the same cluster. Additionally, nodes from different clusters are connected in a specific pattern. The classifiers are trained to detect if the clusters in a graph form a loop. The classifiers were trained to detect if clusters in a graph form a loop. The accuracy of the proposed approach, DGCNN, and simple GNN were 0.99, 0.55, and 0.53, respectively. Another experiment trained the networks to estimate the number of clusters, with accuracies of 1, 0.35, and 0.33 for the proposed approach, DGCNN, and simple GNN. The study highlights that CNNs extended to graphs can fail to infer graph structure without graph embedding. The proposed approach in the experiment focuses on clustering nodes in graphs into three clusters. A comparison is made between the proposed network and a deep learning based approach. The proposed approach achieves an average clustering accuracy of 0.99, while the comparison method only reaches 0.34. The failure of the comparison method is attributed to the lack of labeled nodes and ineffective message passing. The performance of the proposed approach (GEO-DEEP) is further evaluated on established datasets using 10-fold cross validation. The proposed approach (GEO-DEEP) achieved high accuracy in clustering nodes in graphs, outperforming other methods in the experiment. It was compared with graph kernels and deep learning based methods on multiple datasets, showing superior performance on most datasets. The approach showed significant improvement in accuracy, especially on the PTC dataset. The proposed approach achieved high accuracy in clustering nodes in graphs, outperforming other methods. The size of most graphs in the datasets is small, and the approach without pooling layers achieves close classification accuracy. The variances of accuracy with real datasets are reported. GNNs may not infer the structure of graphs effectively, but the proposed approach analyzes a spatial representation of the graph. The proposed approach extends CNN architecture to graphs with a graph pooling method, merging nodes in the spatial domain. It outperforms existing graph classification methods, advancing state-of-the-art results by over 22% on the PTC dataset."
}