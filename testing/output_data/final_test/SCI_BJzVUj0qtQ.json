{
    "title": "BJzVUj0qtQ",
    "content": "Deep neural networks are vulnerable to adversarial examples, which can mislead classifiers with imperceptible perturbations. Adversarial examples have good transferability, enabling black-box attacks in real-world applications. Many methods have been proposed to improve robustness against adversarial attacks, with some defenses shown to be effective. This paper introduces the attention shift phenomenon, hindering transferability of adversarial examples to defense models. An attention-invariant attack method is proposed to generate more transferable adversarial examples, validated through experiments on the ImageNet dataset. Recent progress in machine learning and deep neural networks has led to substantial improvements in various pattern recognition tasks. However, deep neural networks are highly vulnerable to adversarial examples, which are maliciously generated to produce unreasonable predictions. These examples have raised concerns in security-sensitive applications like self-driving cars and healthcare. The proposed method demonstrates the insecurity of defense techniques by fooling eight state-of-the-art defenses at an 82% success rate on average. Adversarial attacks on deep neural networks have gained attention for evaluating model robustness and improving defenses. Various methods like fast gradient sign and basic iterative methods are used for generating adversarial examples. These examples can transfer between models, leading to practical black-box attacks and security concerns. Extensive research is focused on developing robust models to defend against such attacks. Research focuses on building robust models to defend against adversarial attacks, including training with adversarial examples, image denoising/transformation, leveraging generative models, and theoretically-certified defenses. Non-certified defenses have shown robustness but can be easily circumvented by new attacks. Some defenses demonstrate an attention shift phenomenon compared to normally trained models. The defense models BID27, BID29, and BID8 focus on different regions compared to normally trained models, affecting the transferability of adversarial examples. BID8 claims resistance to transferable adversarial examples, making black-box attacks challenging. The attention shift phenomenon in defense models hinders transferability, as shown by different discriminative regions in their predictions compared to normally trained models. This shift is caused by training under different data distributions. The attention shift in defense models, caused by training under different data distributions, reduces the transferability of adversarial examples. To counter this, an attention-invariant attack method is proposed, generating adversarial examples less sensitive to the model's attentive region. This method aims to fool black-box models with defense mechanisms based on attention shift. The proposed attention-invariant attack method aims to improve success rates of black-box attacks against defense models by convolving gradients with a pre-defined kernel. Extensive experiments show that this method can evade state-of-the-art defenses with an average success rate of 82%, highlighting the insecurity of current defense mechanisms against adversarial examples. In image classification tasks, black-box adversaries pose a challenge by attacking models without access to architecture or parameters. Various methods aim to improve transferability for powerful black-box attacks, including adaptive query-based approaches that turn black-box attacks into white-box attacks. Recent methods use queries to estimate gradients for attacking models. In this paper, transferable adversarial examples are used for black-box attacks, aiming to defend against adversarial attacks on deep learning models. Various methods have been proposed to increase model robustness, including detecting adversarial examples and non-certified defenses that demonstrate resistance against transferable attacks. In this paper, the focus is on generating transferable adversarial examples against defenses that resist such attacks. The goal is to create adversarial examples that are visually indistinguishable from real examples but fool the classifier. The L\u221e norm is used for measurement, and the objective is to maximize the loss function of the classifier. The constrained optimization problem in generating transferable adversarial examples for black-box attacks involves calculating the gradient of the loss function with respect to the input. Various methods have been proposed to solve this optimization problem, including linearizing the loss function and performing one-step updates. The Basic Iterative Method (BIM) extends FGSM by iteratively applying gradient updates with a small step size \u03b1 to generate powerful white-box attacks. BIM restricts adversarial examples within the L \u221e norm bound by clipping x adv t or setting \u03b1 = /T. Integrating a momentum term into the attack method can improve transferability. The Iterative Fast Gradient Sign Method BID30 incorporates random transformations to calculate gradients for adversarial examples. It aims to find minimal perturbations to measure model robustness but is less effective for black-box attacks. Some defenses like BID27, BID29, and BID8 are shown to be robust against these attacks. The attention shift phenomenon may hinder the transferability of adversarial examples to defenses like BID27, BID29, and BID8. Adversarial examples generated for one model may not transfer well to another model with attention shift, as the discriminative regions used by the defenses differ from normally trained models. To address this, an attention-invariant attack method is proposed to reduce the impact of attention shift. The proposed method uses shifted images to optimize an adversarial example, making the perturbations less sensitive to the white-box model's attentive regions. This approach improves transferability to other models but requires calculating gradients for multiple images, which can be reduced by sampling a small number of shifted images. The proposed method uses shifted images to optimize an adversarial example, making perturbations less sensitive to the model's attentive regions. By calculating gradients for only one image under a mild assumption of shift-invariance in convolutional neural networks, the gradient for the unchanged image can be averaged with all shifted gradients, reducing the need to calculate gradients for multiple images. This approach improves transferability to other models. The kernel matrix W is generated from a two-dimensional Gaussian function to optimize adversarial perturbations. The method can be integrated into various gradient-based attack methods. The gradient of the loss function is calculated and convolved with the pre-defined kernel W to generate new adversarial examples. In this section, experimental results demonstrate the effectiveness of the proposed method in improving the transferability of adversarial examples to defense models. The experiments were conducted on an ImageNet-compatible dataset with eight robust defense models tested against black-box attacks. These models include Inc-v3 ens3, Inc-v3 ens4, IncRes-v2 ens BID27, HGD, R&P, and others. The NIPS 2017 defense competition included various submissions and models such as Inception v3, Inception v4, Inception ResNet v2, and ResNet v2-152. Adversarial examples were generated using methods like FGSM, MI-FGSM, and DIM. Basic iterative method and Carlini & Wagner's method were not included due to their limitations in generating transferable adversarial examples. The attacks combined with the attention-invariant method are denoted as A-FGSM, A-MI-FGSM, and A-DIM. Hyper-parameters include a maximum perturbation of 16, 10 iterations, step size of \u03b1 = 1.6, and a transformation probability of 0.7. Adversarial attacks were performed on Inception v3, Inception v4, Inception ResNet v2, and ResNet v2-152 using FGSM, MI-FGSM, DIM, and their extensions with the attention-invariant attack method. The proposed attention-invariant attack method, including A-FGSM, A-MI-FGSM, and A-DIM, is used to generate adversarial examples for attacking defense models. Success rates of black-box attacks are reported, showing significant improvements with the attention-invariant based attacks compared to baseline attacks. The success rates consistently outperform baseline attacks by 5% to 30%, with the A-DIM method showing particularly strong performance against the IncRes-v2 model. The combination of A-DIM and DIM was used to attack the IncRes-v2 model, resulting in 60% success rates against defenses, highlighting their vulnerability. The effectiveness of the proposed method was validated, showing smoother adversarial perturbations compared to baseline methods. In this section, adversarial examples are generated for an ensemble of models to improve transferability. Attacks are conducted using FGSM, A-FGSM, MI-FGSM, DIM, and A-DIM against eight defenses, showing improved success rates over baseline attacks. The success rates of adversarial examples generated by A-DIM can fool state-of-the-art defenses at an 82% success rate on average. Current defenses are shown to be ineffective for real-world applications. The size of the kernel W is crucial for improving success rates of black-box attacks, with different kernel sizes affecting the attacks on the Inc-v3 model. In Fig. 3, success rates against five defense models are shown. The success rate increases initially and stabilizes after a kernel size of 15 \u00d7 15. Adversarial images generated for the Inc-v3 model using A-FGSM with different kernel sizes are displayed in Fig. 4. A new attention-invariant attack method is proposed to generate more transferable adversarial examples. The method involves optimizing an adversarial image using shifted images and convolving the gradient with a pre-defined kernel. Experiments validate the effectiveness of the proposed method. The proposed attention-invariant attack method, A-DIM, successfully fools eight state-of-the-art defenses at an 82% success rate on average. This highlights the vulnerability of current defenses and raises security concerns for developing more robust deep learning models. The method is also effective for white-box and black-box attacks against normally trained models, generating adversarial examples for various models using different attack methods. The attention-invariant based attacks using A-DIM successfully deceive normally trained models with a kernel size of 7x7. Results in TAB5, Table 6, and Table 7 show that these attacks outperform baseline attacks in most cases."
}