{
    "title": "HJxEhREKDH",
    "content": "We study the convergence of gradient descent (GD) and stochastic gradient descent (SGD) for training deep linear ResNets with fixed linear transformations at input and output layers. Both GD and SGD with zero initialization on hidden weights can converge to the global minimum of the training loss. The condition for convergence is sharper by a factor of $O(\\kappa L)$ compared to standard deep linear networks. Additionally, global convergence of SGD for training deep linear ResNets is established for the first time. The global convergence of SGD for training deep linear ResNets is proven with a linear convergence rate when the global minimum is $0$. Theoretical understanding of SGD and GD for deep neural networks remains limited, but studying deep linear networks can provide insights into the learning process and architecture choices in deep learning. Deep linear networks have been extensively studied for their optimization landscape analysis and convergence guarantees for optimization algorithms. Research has shown that critical points of deep linear networks with square loss function are either global minimums or saddle points, with depth helping to accelerate optimization. Recent research has focused on the optimization trajectory of gradient descent (GD) for training deep linear networks, proving global convergence rates under certain assumptions. Identity parameterizations in deep linear networks have also been considered, leading to the development of deep linear ResNets with small norm solutions for deep residual networks with large depth. Recent research has shown that for deep linear networks, gradient descent (GD) can converge to the global minimum under certain conditions. Bartlett et al. studied the convergence rate of GD for deep linear ResNets with identity initialization, while Arora et al. demonstrated that GD can converge with random initialization schemes. In this paper, the global convergence of both GD and SGD for training deep linear ResNets without any condition on the training data is established. It is proven that under certain conditions on the input and output linear transformations, GD and SGD can converge to the global minimum of the training loss function, especially with appropriate Gaussian random linear transformations. The proof establishes global convergence of GD and SGD for training deep linear ResNets by deriving conditions on input and output transformations. This allows for finding global minima with zero initialization on hidden weights. The use of appropriate Gaussian random linear transformations further enhances convergence. Using Gaussian random linear transformations, GD can converge to the global minimum with an -error within Op\u03ba logp1{ qq iterations, where k, r are the output dimension and rank of training data matrix X, and \u03ba denotes the condition number of the covariance matrix of the training data. The condition on neural network width is independent of depth L and is better by a factor of OpL\u03baq. SGD for training deep linear ResNets can converge to the global minimum with an -error within r O`\u03ba 2 \u00b41 logp1{ with appropriate neural network width. SGD can converge to the global minimum with an -error within r O`\u03ba 2 \u00b41 logp1{ q\u00a8n{B\u02d8iterations for training deep linear networks. Linear transformations map inputs to hidden layers and outputs. Bounds hold with high probability when transformations are randomly generated by Gaussian distributions. If the input transformation copies inputs to the first d components of the first hidden layer and the output transformation takes the first k components of the last hidden layer, there is no guarantee of convergence. However, a simple deterministic choice of input and output transformations ensures convergence in wide enough networks. This condition is weaker than that for Gaussian random transformations and improves on convergence guarantees for linear networks. A significant amount of work has focused on optimizing neural networks with nonlinear activation functions. The training loss landscape of these networks is considered highly nonconvex and nonsmooth, making it challenging to characterize the optimization trajectory and convergence performance of gradient descent and stochastic gradient descent. Previous studies have shown that wide enough neural networks can learn certain functions in polynomial time, but they only considered training some of the network weights. Additionally, research has explored the convergence of gradient descent for training shallow networks under specific conditions. Previous research has focused on optimizing neural networks with nonlinear activation functions, exploring the convergence of gradient descent for training shallow networks under specific conditions. Recent studies have shown that wide enough neural networks can achieve nearly zero expected error with commonly-used Gaussian random initialization. Gaussian random initialization can lead to almost zero expected error in neural networks. Research has extended these results to multi-layer over-parameterized ReLU networks, showing that training in a \"lazy training\" regime is influenced more by parameter scaling than over-paramterization. Further studies have improved convergence rates and conditions for both shallow and deep networks. Training deep neural networks with over-parameterization conditions has been extensively studied. Various researchers have shown global convergence for deep ReLU ResNets, with some conditions on data separation and network width. Deep residual networks are preferred due to their convergence properties, even with minimal assumptions on the training data. In the context of training deep neural networks with over-parameterization conditions, the analysis focuses on linear networks and provides insights that can be applied to understand optimization in nonlinear cases. Mathematical notations are used to denote scalars, vectors, and matrices, as well as norms and distributions. The study aims to achieve stronger results for linear networks, independent of sample size n, with milder conditions on network width. In this work, deep linear ResNets are considered, defined by input x, output f, scaling parameter \u03c4, weight matrices A and B, and hidden layer weight matrices W1,...,WL. The analysis focuses on linear networks in the context of training deep neural networks with over-parameterization conditions, aiming for stronger results independent of sample size n and milder conditions on network width. The formulation of ResNets in the paper allows wider hidden layers by choosing appropriate dimensions for matrices A and B. The training loss is defined using square loss for input data matrix X and output label matrix Y. Algorithms are considered that only train the weights for hidden layers while fixing the input and output. Algorithms focus on training hidden layer weights while keeping input and output weights fixed. Zero initialization is adopted for hidden weights, satisfying balancedness condition. The convergence result is implied by Theorem 3.1, with specific settings chosen to match previous work. Global convergence is established if a certain condition is met. The setting considered in Bartlett et al. (2019) may not guarantee global convergence due to the size of LpW p0q q\u00b4LpW\u02daq. To address this, we propose using Gaussian random input and output transformations for A and B. This choice characterizes the singular values of A and B, as well as the training loss at initialization. Based on Theorem 3.1 and Proposition 3.3, Corollary 3.4 shows that global convergence can be achieved by GD if the neural network is wide enough. With Gaussian random input and output transformations, if the network width satisfies certain conditions, GD can achieve a training loss within a specified number of iterations. Theorem 3.6 establishes the global convergence of SGD for training deep linear ResNets. With specific conditions on input and output weight matrices, step size, and maximum iteration number, SGD can achieve convergence with high probability. Theorem 3.6 proves SGD's global convergence for training deep linear ResNets under specific conditions. Combining Theorem 3.6 and Proposition 3.3 shows that a wide neural network allows SGD to achieve global convergence. Corollary 3.7 provides conditions on network width and iteration complexity for SGD to find a point with training loss at most LpW\u02daq` within a certain number of iterations. The condition on the neural network width for SGD is worse compared to GD, with higher iteration complexity. Stronger conditions are needed for SGD due to its trajectory uncertainty. In the special case of LpW\u02daq \" 0, SGD can achieve a linear convergence rate to the global minimum. Boosting probability to 1\u00b4\u03b4 is possible by running multiple copies of SGD independently. In this section, we discuss different linear transformations at input and output layers and their impact on convergence performance. Specifically, we focus on the conditions for GD convergence, where input and output weight matrices A and B play a crucial role. Gaussian random transformations have been shown to satisfy this condition, ensuring global convergence for deep linear ResNets. Linear transformations A and B play a crucial role in ensuring convergence performance for deep linear ResNets. Gaussian random transformations satisfy the condition for GD convergence. Other transformations, such as identity transformations, are also discussed. By considering specific settings for A and B, it is shown that certain conditions must be met for convergence to occur. In specific cases, the bound for global convergence may not hold for deep linear ResNets. Gaussian random initialization on hidden weights is used, while identity transformations can also satisfy convergence conditions. An example is provided where matrices A and B are constructed with a parameter \u03b1 specified later. In specific cases, deep linear ResNets may not guarantee global convergence. Gaussian random initialization on hidden weights is used, and identity transformations can also satisfy convergence conditions. Matrices A and B are constructed with a parameter \u03b1 specified later, ensuring global convergence with sufficiently large \u03b1. Experiments on synthetic data validate the theory, including comparisons between input/output transformations and training deep linear ResNets. In experiments on synthetic data, 10-hidden-layer linear ResNets were tested with different input and output transformations. Results showed that gradient descent with modified identity or random initialization led to convergence, while identity initialization got stuck. This validates the theory that deep linear ResNets may not always converge globally. In experiments on synthetic data, 10-hidden-layer linear ResNets were tested with different input and output transformations. Results showed that gradient descent with modified identity or random initialization led to convergence, while identity initialization got stuck. This validates the theory that deep linear ResNets may not always converge globally. Additionally, the convergence performances of linear ResNets were compared with standard deep linear networks, showing similar results for different layer choices. In experiments on synthetic data, 10-hidden-layer linear ResNets were tested with different input and output transformations. Results showed that gradient descent with modified identity or random initialization led to convergence, while identity initialization got stuck. This validates the theory that deep linear ResNets may not always converge globally. Additionally, the convergence performances of linear ResNets were compared with standard deep linear networks, showing similar results for different layer choices. The paper proved the global convergence of GD and SGD for training deep linear ResNets with square loss, under certain conditions on the transformations. The text discusses the convergence of gradient descent and stochastic gradient descent for training deep linear ResNets under certain conditions. It includes proofs and lemmas related to the gradient bounds and smoothness properties of the training loss function. The proof of Theorem 3.1 involves using shorthand notations for weight matrices and proving the theorem by induction on the update number s. The proof shows that the maximum norm of the weight updates is bounded by 0.5 under certain conditions. The proof of Theorem 3.1 involves induction on the update number s, showing that weight updates are bounded by 0.5. The proof of Proposition 3.3 establishes bounds on singular values and initial training loss separately. Bounds on the singular values and initial training loss are established separately. The singular values of matrices A and B are bounded by 0.9 and 1.1, and the initial training loss is also bounded. The proof of the bounds on the initial training loss involves applying zero initialization on hidden layers and utilizing Young's inequality. By setting specific conditions and probabilities, the proof is completed, leading to the establishment of the bounds on the initial training loss. The proof of Corollary 3.4 establishes conditions for achieving optimal training loss, while the proof of Theorem 3.6 demonstrates convergence guarantees based on specific conditions and probabilities. The bounds on initial training loss are established through zero initialization on hidden layers and the use of Young's inequality. The proof establishes conditions for achieving optimal training loss by proving convergence guarantees based on specific conditions and probabilities. The theorem is proven by induction on the update number, showing that there is an iterate achieving training loss within 1 of optimal. The proof uses induction to show that the inductive hypothesis holds for s \" t. It is proven that certain inequalities hold, leading to the conclusion that the training loss is within 1 of optimal. The proof uses induction to show that the inductive hypothesis holds for s \" t. By verifying certain inequalities, it is concluded that the training loss is within 1 of optimal. The step size is set accordingly to ensure this outcome. The proof uses induction to show that the training loss is within 1 of optimal. Inequalities are verified to ensure this outcome, with the step size adjusted accordingly. The martingale inequality is used to prove the relationship between LpW ptq q and LpW p0q q. The text discusses the use of inequalities and martingale theory to show the relationship between training loss and optimal loss. Jensen's inequality is applied to derive a super-martingale, and Azuma's inequality is used to bound the martingale difference. The text discusses the application of Azuma's inequality to show that with high probability, one of the iterates of SGD can achieve training. This completes the proof of the inductive step, showing the relationship between training loss and optimal loss. With probability at least 2/3\u03b4, one of the iterates of SGD can achieve training loss within 1 of optimal. The proof is completed by setting neural network width and step size parameters. The convergence guarantee is established on the last iterate of SGD, proven by induction on the update parameter t. The proof of convergence in expectation and high-probability results for SGD involves applying martingale inequality and Azuma's inequality. By setting neural network parameters, SGD can achieve training loss close to optimal with high probability. The proof involves applying martingale inequality and Azuma's inequality to show convergence in expectation and high-probability results for SGD. It also includes a lemma on positive definite matrices and a gradient lower bound proof. The proof involves applying martingale inequality and Azuma's inequality to show convergence in expectation and high-probability results for SGD. It also includes a lemma on positive definite matrices and a gradient upper bound proof. The stochastic gradient upper bound is derived by defining sets of training data points and using inequalities based on the choice of parameters. The text discusses the transformation of terms V and r in a mathematical context, focusing on the effects of replacing layers. It also delves into bounding the second term using inequalities and Jensen's inequality. The last inequality is derived from Jensen's inequality and the substitution of certain terms."
}