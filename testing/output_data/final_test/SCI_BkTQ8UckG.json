{
    "title": "BkTQ8UckG",
    "content": "We introduce a new technique for learning visual-semantic embeddings for cross-modal retrieval, incorporating hard negatives and ranking loss functions. By modifying common loss functions, fine-tuning, and using augmented data, we achieve significant improvements in retrieval performance. Our approach, VSE++, surpasses state-of-the-art methods by 8.8% in caption retrieval and 11.3% in image retrieval on MS-COCO and Flickr30K datasets. Joint embeddings facilitate various tasks in image, video, and language understanding. In this paper, the focus is on learning visual-semantic embeddings for tasks like image-caption retrieval, visual question-answering, and image synthesis from text. The embeddings involve mappings from different domains into a common vector space, representing the underlying structure of the domains. Visual question-answering involves describing an image with captions and finding the nearest caption in response to a question. Image synthesis from text involves inverting the mapping from a joint visual-semantic embedding to the image space. The focus is on visual-semantic embeddings for cross-modal retrieval, measuring performance by recall at K. Retrieval assesses joint embeddings for image and language data quality. The problem involves ranking with the correct target closer to the query in the corpus. The formulation is related to learning to rank problems and max-margin structured prediction. The paper introduces a novel loss function, augmented data, and fine-tuning to improve caption retrieval performance. They outperform previous results on benchmark datasets by almost 9% and emphasize the importance of a powerful image encoder and fine-tuning. The model, VSE++, complements other recent articles proposing new model architectures or similarity functions for cross-modal retrieval. The BID21 model uses an attention mechanism on both image and caption to compute similarity, while BID10 utilizes a multi-modal context-modulated attention mechanism. The proposed loss function and triplet sampling can be applied to similar approaches for image-caption retrieval, aiming to maximize recall at K. Positive pairs are denoted as (i n , c n ) and negative pairs as (i n , c m =n ). The proposed approach defines a similarity function for image-caption retrieval, ranking captions based on similarity scores. It differs from other methods by using a joint embedding space. Feature-based representations are computed from images and captions to improve retrieval accuracy. The approach defines a similarity function for image-caption retrieval using a joint embedding space. Model parameters are used for mappings to obtain initial image and caption representations, with training involving the minimization of empirical loss over training data. Recent approaches to joint visual-semantic embeddings have utilized a triplet ranking loss for image retrieval. The loss function (i, c) is used for a single training exemplar, with a hinge-based, triplet ranking loss with margin \u03b1. The loss assigns a higher value to the hardest negative sample, c, in the query i. The loss function for joint visual-semantic embeddings involves a hinge-based triplet ranking loss with margin \u03b1. It assigns higher value to the hardest negative sample in the query. The loss is proportional to the expected loss over sets of negative samples, with computational efficiency achieved by summing over negatives within a mini-batch. Another approach is a pairwise hinge loss where positive pairs are encouraged to be within a radius \u03c11, while negative pairs should be no closer than \u03c12 > \u03c11. The loss function for joint visual-semantic embeddings involves a hinge-based triplet ranking loss with margin \u03b1, focusing on hard negatives for training. Canonical Correlation Analysis can be used to preserve correlation between text and images in the joint embedding. The loss function for joint visual-semantic embeddings involves a hinge-based triplet ranking loss with margin \u03b1, focusing on hard negatives for training. The loss comprises two terms, one with i and one with c as queries, specified in terms of the hardest negatives, c and i. Referred to as Max of Hinges (MH) loss, it is superior to Sum of Hinges (SH) loss when multiple negatives with small violations dominate. An example illustrates how moving a hard negative might require a significant change to the mapping. The SH loss may create local minima due to new negative samples dominating the loss, pushing the model back. To address this, the MH loss focuses on the hardest negatives, improving computational efficiency by finding them in mini-batches. This approach increases the likelihood of obtaining harder negatives and makes the loss more robust to label errors in training data. In Appendix A, the probability of sampling hard negatives is analyzed. Experiments are conducted with VSE++, comparing it to a baseline formulation VSE0 with SH loss. Two image encoders, VGG19 and ResNet152, are used with image features extracted from FC7. The image embedding dimensionality is 4096 for VGG19 and 2048 for ResNet152. Images are resized to 256 \u00d7 256 and either a single center crop of size 224 \u00d7 224 or the mean of feature vectors for 10 crops is used for training. For training, different crop methods are used such as one center crop (1C), 10 crops (10C), and random crops (RC) with VGG19 model. A GRU with dimensionality set to 1024 is used for caption encoding. Normalization of both caption and image embeddings is done in VSE++, unlike in VSE0 where only caption embedding is normalized. In experiments, not normalizing image embedding helped VSE0 baseline, while VSE++ was not affected. Evaluation done on Microsoft COCO and Flickr30K datasets. MS-COCO has 82,783 training images, 5000 validation, and 5000 test images. rV set contains 30,504 images. Results reported using both training sets. Each image has 5 captions. Results reported by averaging over 5 folds of 1K test images or testing on full 5K test. The models are trained using the Adam optimizer BID12 for at most 30 epochs. Fine-tuned models start with a learning rate of 0.0002 for 15 epochs, then decrease to 0.00002 for another 15 epochs. Margin is set to 0.2 for most experiments with a mini-batch size of 128. Over-fitting is addressed by selecting the best performing model snapshot based on recalls on the validation set. The best snapshot is selected based on recalls on the validation set. Results on the MS-COCO dataset are presented in Table 1. Ablation studies for the baseline VSE0 show improvements with VSE++ using ResNet152 and fine-tuning the image encoder. Significant gains are seen with the MH loss function. Comparing VSE++ to the current state-of-the-art on MS-COCO shows improvements in caption and image retrieval. In comparison to sm-LSTM, VSE++ shows an 11.3% improvement in image retrieval. Training with additional data like RC or RC+rV results in a 5.9% and 5.1% increase in caption retrieval for VSE++. Fine-tuning the VGG19 image encoder widens the performance gap between VSE0 and VSE++ to 6.1%. Using ResNet152 instead of VGG19 also improves results. The performance gap between VSE0 and VSE++ widens to 6.1% when fine-tuning the VGG19 image encoder. Using ResNet152 instead further improves results, with a gap of 5.6%. The best result is achieved with ResNet152 and fine-tuning the image encoder, resulting in an 8.6% performance gap. Over-fitting is observed with VSE++ when trained with pre-computed features of 1C due to limited training data. The model selection is based on performance with the validation set to prevent over-fitting. Training with RC data avoids over-fitting. The MH loss leads to improvements in performance on Flickr30K. The MH loss improves performance across datasets and models, taking longer to 'warm-up' during training compared to the SH loss. Curriculum learning did not enhance training with the MH loss. The MH loss improves performance compared to the SH loss, especially in terms of recall rates on the Flickr30K dataset when training with RC. The effective sample size for searching negatives impacts performance, with smaller sets potentially being more robust to label errors. The MH loss leads to higher recall rates, with the optimal negative set size around 128. For negative sets smaller than the mini-batch size, 128, negatives are randomly sampled from the mini-batch. For larger negative sets, the model performs slightly below VSE0. The MH loss has a higher probability of containing hard negatives, leading to longer training times for large negative sets. Performance dropped with a negative set size of 512 due to dataset size and outlier sampling. Despite performance drops with larger batch sizes, it still outperforms the SH loss. The MH loss can complement sophisticated model architectures or similarity functions. It was applied to order-embeddings BID27 with an asymmetric similarity function. The MH loss was used in training with a negative set size of 512, leading to longer training times. Despite performance drops with larger batch sizes, it outperformed the SH loss. The MH loss was applied to order-embeddings BID27 with an asymmetric similarity function. The training settings for Order0 included a learning rate of 0.001 for 15 epochs and 0.0001 for another 15 epochs, with a margin of 0.05. The absolute value of embeddings was taken before computing the similarity function for Order0. Test images and captions were evaluated using VSE0 and VSE++ (ResNet)-finetune. The MH loss was found to improve caption retrieval compared to the SH loss. Results showed a 4.5% improvement from Order0 to Order++ in R@1 for caption retrieval. The MH loss has the potential to enhance various loss functions in retrieval and ranking tasks. The study focused on learning visual-semantic embeddings for image-caption retrieval using a new loss based on violations from hard negatives. Experiments were conducted on MS-COCO and Flickr30K datasets. Our proposed loss function significantly improves performance on MS-COCO and Flickr30K datasets, guiding a more powerful image encoder, ResNet152. The VSE++ model achieves state-of-the-art performance on MS-COCO and near-best results on Flickr30K. This loss function can enhance training for more sophisticated models using a similar ranking loss."
}