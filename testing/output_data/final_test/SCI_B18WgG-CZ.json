{
    "title": "B18WgG-CZ",
    "content": "Recent advancements in natural language processing (NLP) have been driven by distributed vector representations of words trained on large amounts of text. While these representations are commonly used for individual words, learning representations of sequences like sentences remains a challenge. Recent research has explored various unsupervised and supervised learning techniques to create fixed-length sentence representations. A new multi-task learning framework is proposed in this work to combine different training objectives for more effective sentence representations. Transfer learning in natural language processing (NLP) has shown significant improvements by sharing a single recurrent sentence encoder across tasks. This approach has led to consistent enhancements in transfer learning and low-resource settings, utilizing general-purpose representations. Pretrained word embeddings are commonly used in NLP systems, benefiting tasks like reading comprehension and sequence labeling. Many neural NLP systems use pretrained word embeddings but learn task-specific representations from scratch. In low-resource settings, general-purpose sentence representations can be beneficial. Recent work has focused on learning these representations, but there is no clear consensus on the best training objective. Understanding the inductive biases of neural models is crucial for representation learning progress. Neural machine translation systems seem to capture morphology and syntactic properties, while sequence-to-sequence parsers also play a role. Our work focuses on building representations that encode various aspects of a sentence, utilizing a one-to-many multi-task learning framework. By training on multiple weakly related tasks, we aim to learn sentence representations that capture different inductive biases present in tasks like skip-thoughts, machine translation, and natural language inference. The primary contribution of our work is to combine diverse sentence-representation learning objectives into a single multi-task framework. This approach aims to capture various characteristics of a sentence that could be useful across different tasks. This is the first large-scale reusable sentence representation model obtained by combining a set of training objectives with a high level of diversity, including multi-lingual NMT, natural language inference, and constituency parsing. The study explores diverse sentence representation learning objectives, such as multi-lingual NMT, natural language inference, constituency parsing, and skip-thought vectors. Extensive experimentation shows that these representations lead to improved performance on various tasks. The models achieve comparable performance to those trained from scratch using only 6% of the training data on the Quora duplicate question dataset. Advances in learning distributed representations of words combined with deep learning allow for complex composition functions of word embeddings using neural networks. Skip-thought vectors extend skip-gram models to learn reusable sentence representations from weakly labeled data, but training can be time-consuming. Faster alternatives like sequential denoising autoencoders are being considered. Recent advancements in learning distributed representations of words have led to the exploration of faster alternatives such as sequential denoising autoencoders and shallow log-linear models. Simple word embedding averages have been shown to be comparable to more complex models like skip-thoughts. A completely supervised approach to learning sentence representations from natural language inference data has outperformed previous methods on transfer learning benchmarks. Representations learned by state-of-the-art large-scale NMT systems have also been shown to generalize well to other tasks, although the use of an attention mechanism hinders the learning of a fixed-length vector representation of a sentence. Recent advancements in learning distributed representations of words have led to the exploration of faster alternatives such as sequential denoising autoencoders and shallow log-linear models. A bi-attentive classification network is presented to compose information from all hidden states for improvements over a model trained from scratch. Discourse-based objectives can also be leveraged for learning good sentence representations. The work is compared to a many-to-many sequence-to-sequence model trained on various tasks, with key differences in attention mechanism usage and task objectives. The curr_chunk discusses the use of multi-task models for sequence-to-sequence problems, with a focus on encoding different information signals. It also mentions the success of \"universal\" multi-task models in computer vision. The encoder-decoder models are highlighted, where the encoder produces a fixed-length vector representation of the input for the decoder to generate the output. The encoder produces a fixed-length vector representation of the input, which the decoder uses to generate an output. The decoder is auto-regressive and breaks down the joint probability of outputs into conditional probabilities. Encoders and decoders are parameterized as RNN variants like LSTMs or GRUs. An attention mechanism allows the decoder to condition on every hidden state of the encoder RNN. This work does not employ an attention mechanism, resulting in a single, fixed-length sentence representation. The encoder produces a fixed-length vector representation of the input, which the decoder uses to generate an output. To address the vanishing gradient issue, the decoder is conditioned on the encoder hidden representation. A bidirectional GRU is used for the encoder and a unidirectional conditional GRU for the decoder. The encoder representation is provided as conditioning information to avoid information attenuation. Multi-task sequence-to-sequence learning models are explored for NMT, including one-to-many, many-to-one, and many-to-many architectures. In this work, a one-to-many model is considered for combining inductive biases from different training objectives. A bidirectional GRU encodes input sentences from various tasks into a compressed summary, which conditions a task-specific GRU to generate the output sentence. Multi-task training is motivated by theoretical insights showing that learning multiple related tasks jointly leads to good generalization and inductive biases learned from multiple tasks are beneficial for learning new tasks. Skip-thought vectors are an extension of skip-gram word embedding models to sentences, trained on the BookCorpus dataset to predict the next and previous sentences simultaneously. The encoder for the current sentence and decoders for the previous and next sentences are parameterized separately as RNNs. Additionally, training skip-thoughts by predicting only the next sentence from the current one has shown comparable results. In BID44, it is shown that predicting the next sentence alone yields similar performance. NMT can be seen as a sequence-to-sequence learning problem using a parallel corpus of English-German and English-French sentence pairs. Constituency Parsing demonstrates the viability of a sequence-to-sequence approach, where the encoder takes the sentence and the decoder produces its linearized parse tree. Training is done on weakly labeled parses and gold parses. Natural Language Inference (NLI) is a 3-way classification problem where the goal is to classify the relationship between a premise and a hypothesis as entailment, contradiction, or neutral. A shared recurrent sentence encoder is used to encode both the premise and hypothesis into fixed length vectors, which are then fed into an MLP for classification. This approach differs from sequence-to-sequence learning used in other tasks. The authors train on 1 million sentence pairs from SNLI, MultiNLI corpora using different training ratios for each task. They perform \u03b1 i * N parameter updates on task i before selecting a new task randomly. In contrast, a simpler approach is taken where a new sequence-to-sequence task is trained on after every parameter update sampled uniformly. The authors train on 1 million sentence pairs from SNLI, MultiNLI corpora using different training ratios for each task. They perform parameter updates on task i before selecting a new task randomly. A new sequence-to-sequence task is trained on after every parameter update sampled uniformly. An NLI minibatch is interspersed after every ten parameter updates on sequence-to-sequence tasks. Model details can be found in section 7 in the Appendix. The authors evaluate the quality of learned word representations and transfer learning performance on low-resource tasks using fixed-length sentence representations. Results show training logistic regression on various supervised transfer tasks, with comparisons to approaches trained from scratch. Refer to their paper for detailed task descriptions. Adding more tasks and increasing hidden units in the GRU improves transfer performance. The model shows gains in sentiment classification tasks and outperforms Infersent on TREC and paraphrase identification tasks. Constituency parsing also enhances sentence performance. Incorporating constituency parsing improves performance on sentence relatedness and entailment tasks. Training an MLP on fixed sentence representations outperforms complex supervised approaches, even with a small fraction of the training data. Learning word embeddings from scratch yields competitive results compared to pretrained methods. In TAB4, learned word embeddings are competitive with popular methods like GloVe, word2vec, and fasttext on benchmarks. Sentence representations are probed for syntactic properties and characteristics, showing improvements with multi-lingual NMT and parsing. The sentence representations outperform skip-thoughts and are comparable to Infersent for image-caption retrieval. The text discusses the performance of transfer models on various tasks, including MRPC and STSB. It also mentions the correlation between cosine similarities and semantic textual similarity benchmarks. Additionally, it includes qualitative analysis through visualizations and nearest neighbor exploration of learned representations. The text discusses the clustering of sentences labeled by category and the performance of transfer models on various tasks. It also includes a multi-task framework for learning fixed-length sentence representations. The text introduces a multi-task framework for learning fixed-length sentence representations, incorporating diverse training signals. The framework includes tasks like multi-lingual NMT, constituency parsing, skip-thought vectors, and natural language inference. Results show competitive performance compared to previous methods, with good word embeddings produced. Evaluation includes probing for sentence characteristics and syntactic properties. The text introduces a multi-task framework for learning fixed-length sentence representations, incorporating diverse training signals such as multi-lingual NMT, constituency parsing, skip-thought vectors, and natural language inference. The framework aims to understand and interpret the inductive biases of the models and explore the application of generative models to language. Architectural specifics and training details of the shared encoder using GRU are presented, with experiments on different types of GRUs. The text introduces a multi-task framework for learning fixed-length sentence representations using 2 layer bidirectional GRUs. Each decoder has separate word embeddings, conditional GRUs, and fully connected layers. The encoder's last hidden state is used as the initial hidden state of the decoder. Models use word embeddings of 512 dimensions and GRUs with 1500 or 2048 hidden units. Training is done with minibatches of 48 examples using the Adam optimizer with a learning rate of 0.002 on an Nvidia Tesla P100-SXM2-16GB GPU. The researchers trained their models for 7 days on an Nvidia Tesla P100-SXM2-16GB GPU, taking advantage of advancements in GPU hardware and software. They did not tune architectural details and hyperparameters due to a lack of clear criteria. They experimented with different approaches for generating sentence representations and selected the one with better performance on the validation set. Max-pooling was found to work best on sentiment tasks like MR and CR. The researchers experimented with different approaches for generating sentence representations and found that max-pooling works best on sentiment tasks like MR and CR. They also employed vocabulary expansion by training a linear regression to map from pre-trained word embeddings to their model's word embeddings. The specifics of their multi-task ablations are described in the experiments section. The representation h x is a concatenation of final hidden vectors from bidirectional GRUs with 1500-dimensional hidden vectors. Evaluation is done on various text classification benchmarks using logistic regression classifier with 10-fold cross validation for tuning. The tasks include sentiment classification, question type classification, subjectivity/objectivity classification, and opinion polarity. The evaluation metric for various text classification tasks includes classification accuracy, F1 score, and Pearson correlation. Tasks involve paraphrase identification, relatedness scoring, and entailment classification using different datasets such as MRPC and SICK. The evaluation focuses on measuring the similarity between sentences using cosine similarity and textual similarity benchmarks like STS. Image-caption retrieval is a ranking task using MSCOCO dataset with pre-trained ResNet for feature extraction. Evaluation criteria include Recall@K. Additionally, evaluation is done on Quora duplicate question dataset to identify duplicate question pairs. The task involves identifying duplicate question pairs using a binary classification problem. A 4 layer MLP with 1024 hidden units is trained for this purpose. Evaluation is based on classification accuracy. Additionally, low-resource settings are created by reducing training examples. Six different classification tasks are considered to predict sentence characteristics and syntactic properties. The sentence characteristic tasks involve classifying sentence lengths into 8 ranges and determining if a word is contained in a sentence. The order task determines the position of two words in a sentence. These tasks are performed using a random subset of the 1-billion-word dataset. The syntactic properties tasks focus on passive and tense characteristics. The passive and tense tasks involve binary classification to determine sentence characteristics. The top syntactic sequence task is a 20-way classification with different dataset splits. Evaluation of sentence representations on semantic textual similarity benchmarks is reported in Table 7."
}