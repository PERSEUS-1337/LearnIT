{
    "title": "HJgdVWPTv4",
    "content": "Sequential decision problems in real-world applications often require real-time solutions with limited computational resources. Width-based lookaheads have shown superior performance in classical planning and Atari games with tight budgets. This study explores the application of width-based lookaheads to Stochastic Shortest Paths (SSP) problems, addressing challenges and proposing a cost-to-go estimation method. Experimental results demonstrate the algorithm's effectiveness in SSP benchmarks, outperforming other rollout algorithms like UCT and RTDP. Model-based lookahead algorithms offer autonomous solutions to a wide range of sequential decision-making problems. This paper focuses on Stochastic Shortest Path (SSP) problems and the use of width-based planning algorithms to prioritize exploration of new areas. Lookaheads with heuristics estimating costs-to-go have shown success in real-time decision-making with limited computational budgets. The study investigates width-based planners for Stochastic Shortest Path (SSP) problems, comparing breadth-first search and depth-first search algorithms. A novel width-based algorithm is proposed, which estimates costs-to-go by simulating a base policy. Experimental results show favorable comparisons to existing algorithms. The focus is on decision-making under stochastic uncertainty over a finite number of stages. The study focuses on decision-making under stochastic uncertainty over a finite number of stages, characterizing a dynamic system with state, control, and random disturbance elements. Control is constrained to a subset dependent on the current state, and random disturbance is characterized by a probability distribution. The class of policies considered involves mapping states to controls. The study focuses on decision-making under stochastic uncertainty over a finite number of stages, characterizing a dynamic system with state, control, and random disturbance elements. Control is constrained to a subset dependent on the current state, and random disturbance is characterized by a probability distribution. Policies map states to controls, with admissible policies minimizing expected costs starting from an initial state x0. Optimal policies minimize this cost, with the optimal cost J* depending on x0 and being equal to J\u03c0*. The optimal cost function J* assigns costs to initial states x0 in Stochastic Shortest Path (SSP) problems. In Stochastic Shortest Path (SSP) problems, the goal is to minimize a cost function with a termination state t where the system remains at no further cost. Optimal control policies ensure termination with probability 1 within a certain number of stages. Proper policies guarantee this condition is met, while improper policies do not. In Stochastic Shortest Path (SSP) problems, optimal control policies aim to minimize a cost function with a termination state t. Proper policies ensure termination with probability 1 within a certain number of stages, while improper policies do not. The RTDP BID2 algorithm uses a rollout strategy with a d-step lookahead approach to approximate the optimal cost-to-go from the current state xk. The algorithm selects controls that minimize a cost function, with Jk+1 approximating the optimal cost-to-go J*k+1. The approximation can be a base heuristic, problem-specific, domain-independent, or learned from a simulator. Alternatively, Jk+1 can approximate the cost-to-go of a suboptimal policy \u03c0, with estimates obtained through simulation. The rollout policy combines a lookahead strategy with a base policy to estimate the cost-to-go. Q-factors of state and control pairs are computed and minimized to determine the rollout control. Assumptions are made to ensure viability of lookaheads with d > 1, including the ability to simulate the system under the base policy. The system can be simulated under the base policy to generate sample trajectories and costs. Real-time constraints make it challenging to calculate rollout control due to exponential complexity. To avoid lookahead size blowup, recursion is cut based on structural width criteria, resulting in a selective strategy. The selective strategy for Monte-Carlo Tree Search algorithms like UCT focuses on expanding novel states with unique features, independent of the objective function used. Width-based Search is used to prioritize state expansion based on novel valuations. Rollout IW (RIW) is an integration of IW(1) as a depth-first rollout algorithm, ensuring novel states are reached through the shortest path. Novelty is redefined by considering a state novel if it reaches at least one value of a state variable through a shortest path. State features are defined to mimic the breadth-first search strategy. Rollout IW (RIW) is a depth-first rollout algorithm that ensures novel states are reached through the shortest path. Novelty is determined by reaching at least one value of a state variable through a shortest path, mimicking a breadth-first search strategy. The search strategy involves setting the best upper bound on the shortest path to reach each value of a state variable from the root state, with the horizon denoting the maximum search depth allowed. To prove a state as novel, it must be shown that either the state is new in the lookahead tree or contains at least one state variable value with a shortest path equal to the current state's depth. Rollout IW (RIW) ensures novel states are reached through the shortest path by marking non-novel states as solved. The algorithm terminates when the root state is labeled as solved. Non-novel states are treated as terminals with a cost-to-go of 0, potentially biasing towards non-novel states. To address this, estimating upper-bounds on the cost-to-go is crucial for RIW over SSPs. Width-based algorithms have been successful in various domains but perform poorly on SSP problems. Lookaheads prioritize non-novel states over reaching goals, and depth-first lookaheads ignore useful information. A simple SSP problem involves navigating to a goal with minimal actions. IW(1) and RIW(1) algorithms, with sufficient budget, can solve this problem efficiently. Both IW(1) and RIW(1) algorithms prioritize non-novel states over reaching goals, leading to suboptimal paths. RIW(1) may randomly reach the goal, but trajectories are pruned due to non-novel states. The algorithm RIW introduces the concept of pruning trajectories with non-novel states in the context of deterministic transition functions. It evaluates the novelty of a state based on a set of features and shortest path distances. The maximum number of novel states is O(|F|), where F is the set of features. The labeling of nodes limits the number of rollouts from the initial state to O(|F| \u00d7 b), where b is the maximum number of applicable control variables. The algorithm RIW introduces pruning trajectories with non-novel states based on features and shortest path distances. The labeling of nodes limits rollouts to O(|F| \u00d7 b), where b is the maximum number of applicable control variables. The lookahead tree in stochastic shortest path problems is a relaxation of the original SSP, allowing one possible outcome of a control input. Backpropagating labels using \u03bb-labeling terminates after at most DISPLAYFORM1, reconciling width over classical planning problems with SSPs. The algorithm RIW introduces pruning trajectories based on features and shortest path distances. Rollout IW(1) with \u03bb-labeling guarantees reaching every width 1 terminal state in polynomial time. The algorithm RIW introduces pruning trajectories based on features and shortest path distances, terminating when the computational budget is exhausted. Cost-to-go approximations involve running Monte Carlo simulations of a suboptimal base policy, but this method has drawbacks such as overestimating costs. The new methods added to the MCTS family combine lookaheads with stochastic simulations of policies to balance computational efficiency with performance. The RIW-RW method uses a random walk as the base policy, assigning equal probabilities to controls. The RIW-RW method combines lookaheads with stochastic simulations using a random walk policy. A rolling horizon is set for the rollout algorithm, with simulations and lookahead unrolling interrupted if the computational budget is exhausted. Rollout algorithms have a cost improvement property, preventing extremely long trajectories that consume the computational budget. The second width-based MCTS method utilizes stochastic enumeration (SE) techniques to estimate upper bounds on costs-to-go for stochastic rollout algorithms with a large depth lookahead. Inspired by Knuth's algorithm, SE methods estimate the total cost of a tree by tracking two quantities, C and D, representing the total cost and the number of nodes at each level respectively. The algorithm starts at a root vertex u with D = 1, updates D to be D = |S(u)|D, and randomly chooses a successor vertex v from S(u). The estimate C is updated as C = C + c(u, v)D based on the edge cost. This process continues until a vertex v with no successors is selected. Knuth's C quantity relates to the worst-case cost-to-go J(x) k of a rollout algorithm with a lookahead strategy. Lookaheads create trees over states connected by controls, with edge costs corresponding to realizations of a random variable. Knuth's algorithm estimates the random variable g(x, u, w) and successors S(v) of a vertex, corresponding to admissible controls U(x). The estimator's variance can be exponential due to uneven cost distribution in the tree. In experiments, Knuth's algorithm is used with stopping conditions to enforce a computational budget. Evaluation is done on GridWorld domains, aiming to move from an initial grid position to a goal. In GridWorld, the goal is to move from an initial position to a goal position in a grid. There are 4 available actions: move up, down, left, or right. Actions have a cost of 1, except reaching the goal which has a cost of 0. The complexity can be scaled through grid size and goal locations. Extensions include stationary goals, moving goals, obstacles, and partial observability. Each instance has a d0 x d1 grid, with the state being the agent's current location. The transition function is formalized with goal states and agent's position changes for each action. The agent's position changes for each action in GridWorld, with stationary goals positioned in the middle of the grid. Obstacles are included as grid cells where the agent cannot move, creating a stochastic Canadian Traveller Problem instance. In the GridWorld problem, obstacles are present in certain grid cells, affecting the agent's movement. The agent can only observe obstacles when in a neighboring cell, and once observed, the status remains unchanged. John Langford designed two MDP problems related to Reinforcement Learning. Langford designed two MDP problems for Reinforcement Learning in GridWorld with a moving goal. The first problem, Antishaping, uses reward shaping to encourage actions away from the goal state. The state space is controlled by the number of states value, N. The goal state can be achieved by continuously selecting a specific action. The goal state in Antishaping is to reach T k = {N \u2212 1} by selecting u k = 0 continuously. The cost of transitions is 0.25 divided by N -x k+1, except when x k+1 = N \u2212 1 where the cost is 0. Langford's Combolock problem addresses the issue of random exploration leading back to the start state. The transition function in Combolock is DISPLAYFORM6, with the goal state T k = {N \u2212 1}. The cost of transitions in Combolock is 1, except for the transition to the terminal state N \u2212 1 where the cost is 0. Reinforcement Learning algorithms like Q-Learning may struggle to solve these domains. Langford claims that algorithms like Q-Learning struggle in certain domains, while the E3 family of algorithms, which focus on exploring a maximum number of states, perform well. Various algorithms and their performance metrics are listed. In the GridWorld domain, the depth-first width-based rollout algorithm, RIW(1), is evaluated with and without base policies. The features for planning are defined as F = {(a, i, d) | a \u2208 D(x i )} for GridWorld and F = {(i, d) | i \u2208 N )} for Antishaping and Combolock.\u03bb = 1 is used for labels back-propagation, with no significant changes observed with \u03bb = \u221e. Two additional rollout algorithms are considered: RTDP BID2 and UCT BID14. The exploration parameter for UCT is set to 1.0. The maximum depth of a simulated trajectory is H - l, where l is the depth at which the trajectory began and H is the horizon value. A single simulated trajectory is used for cost-to-go approximation. Manhattan distance heuristic is used for GridWorld domains with obstacles. Results for GridWorld problems with obstacles provide a lower bound on the cost-to-go. Different methods are evaluated at varying levels of complexity by changing the number of states and using different simulator budgets. The results show performance metrics for algorithms like Man., Rnd., UCT, and RIW at different simulator budgets (100, 1000, 10000). Results are presented for the Atari-2600 game Skiing, a SSP problem using the OpenAI gym's interface. The slalom mode requires skiing through gates as fast as possible, with time penalties for missed gates. The reward values are based on total time taken plus penalties. Environment settings include frame skip of 5 and sticky actions. Evaluation is done with a simulator budget of 100 and partial caching. The experiment involves using a simulator budget of 100 and partial caching for the Atari-2600 game Skiing. The maximum episode length is 18,000 frames with a frame skip of 5, equivalent to 3,600 actions. A heuristic cost-to-go estimate is used, based on missed gates and time penalties. Pixel values from the current gray scaled screen at full resolution are used as features for the RIW(1) algorithms. The experiment used a simulator budget of 100 and partial caching for the Atari-2600 game Skiing, with a maximum episode length of 18,000 frames. Pixel values from the current gray scaled screen were used as features. Different lookahead algorithms were evaluated, including H NA, H Rnd, and H Man. Results showed that off-the-shelf stochastic estimation techniques did not benefit MCTS algorithms significantly. The results of different lookahead algorithms on GridWorld domains with stationary goal, moving goals, and obstacles were discussed by BID19. Results for a 100x100 grid were omitted due to insufficient simulator budgets. In the stationary goal GridWorld domain, RIW(1) benefits from using H Rnd with larger simulator budgets, while performance remains constant with H NA. RIW(1) statistically outperforms other algorithms on different domains with the largest budget and H Rnd. In GridWorld with moving goals, RIW(1) dominates performance for the largest budget. Results for GridWorld with a stationary goal and obstacles show RIW(1) outperforming all methods on 10x10 and 20x20 domains with the largest budget. Algorithms using H Man as the base heuristic show dominance over H Rnd in various scenarios, with H Man outperforming H Rnd for the larger 50x50 grid in UCT. RIW(1) with H Rnd is statistically dominant on smaller 10 state domains, while H Rnd with RIW(1) shows improved performance across all domain settings. RIW(1) with H Heu outperforms all other methods, including H NA and Antishaping, across various domain settings. In the Skiing Atari-2600 game, RIW(1) using H Heu dominates all other methods and shows similar performance to the DQN algorithm after 100 million frames of training. RIW(1) achieves its performance in Table 7 with only 1.8 million frames, demonstrating its efficiency. Bertsekas (2017) considers AlphaGo Zero to be state-of-the-art in MCTS algorithms, combining reasoning over confidence intervals and classic simulation of base policies. The algorithm introduced in the curr_chunk achieves super-human performance in the game of Go by using supervised learning algorithms to obtain parametric representations of costs-to-go. Unlike AlphaZero, it uses a CNN to automatically extract features describing spatial relations between game pieces. The algorithm also prioritizes controls based on estimated win probabilities and simulates opponent strategy via self-play to generate successor states. Unlike AlphaZero, the algorithm's simulators are static and remain unchanged. The BID11 algorithm integrates rollout with a deep neural network to guide search and extract state features. It does not use neural networks to approximate costs-to-go like AlphaZero. Policies learned after 40 million interactions with the simulator show significant performance improvement over the original rollout algorithm. MCTS approaches combine lookaheads, cost-to-go approximations, and statistical tests to focus sampling efforts, which the width-based methods in this paper also do. The width-based methods in this paper provide alternatives to existing MCTS approaches, with active research ongoing to understand their impact. Designing MCTS algorithms requires following strict protocols to avoid ad-hoc solutions and relying on diverse benchmarks to highlight limitations and improve state-of-the-art methods."
}