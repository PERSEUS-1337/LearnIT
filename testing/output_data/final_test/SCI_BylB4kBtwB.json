{
    "title": "BylB4kBtwB",
    "content": "Recent advances have enabled the creation of deep complex-valued neural networks, yet the full potential of complex intermediate computations remains unexplored. A novel mechanism for signal extraction in the frequency domain is proposed, demonstrated through audio source separation in the Fourier domain. This mechanism combines a complex-valued convolutional version of Feature-Wise Linear Modulation (FiLM) with a signal averaging operation. An explicit amplitude and phase-aware loss function is introduced, which considers the complex-valued components of the spectrogram. The effectiveness of this phase-aware loss is compared to others using the Wall Street Journal Dataset, showcasing its efficacy. Operating in the complex-valued frequency domain, deep complex-valued networks outperform real-valued counterparts with fewer parameters. The proposed mechanism improves performance and demonstrates the regularizing effect of deep complex-valued networks. While complex-valued neural networks have been studied for a long time, deep complex-valued models have only recently gained momentum. Complex-valued representations in deep learning offer advantages for certain types of data, especially those in the frequency domain. Using short-time Fourier transforms can reduce the temporal dimension of the signal representation, benefiting the training of recurrent neural networks. Using short-time Fourier transforms (STFT) on raw signals is computationally efficient and beneficial for training recurrent neural networks (RNNs) and convolutional neural networks (CNNs) on long sequences. Recent work has focused on designing deep complex-valued neural networks to address numerical problems during training. The text discusses the use of complex-valued representations for frequency domain signals to avoid numerical problems during training. A new signal separation mechanism using a complex-valued convolutional version of Feature-wise Linear Modulation is presented, along with a signal averaging operation to increase robustness to noise and interference. The text introduces a new magnitude and phase-aware loss for signal extraction, focusing on reducing interference and noise correlation. The proposed method is tested in audio source separation to retrieve distinct signals from input mixes, showing regularization effects. Leveraging the Convolution Theorem for information retrieval has been previously done using holographic reduced representations in machine learning. Holographic Reduced Representations (HRRs) are used in associative memories to store key-value data. Retrieval of a value can be done by convolving the data with the key or applying an inner product. Fast Fourier Transform (FFT) can be applied to keys and data for elementwise multiplication, providing a less expensive circular convolution in the time domain. Danihelka et al. (2016) used associative memories to enhance LSTMs' capacity and robustness by applying independent permutations on the memory to create decorrelated noise in multiple copies. In order to enhance the signal-to-noise ratio, a complex multiplication is performed between the key and permuted copies, followed by signal averaging. Danihelka et al. (2016) did not rely on FFTs for frequency domain conversion, instead using complex-valued multiplication. Interest in Fourier domain representations has been growing in the machine learning community, with Bruna et al. (2013) introducing graph Fourier Transform for convolutions on graphs. In the context of Convolutional Neural Networks (CNNs), spectral pooling introduced by Rippel et al. (2015) allows pooling in the frequency domain, maintaining spatial dimensionality and retaining more information. Parametrization of convolution filters in the Fourier domain by Rippel et al. (2015) leads to faster convergence during training. Arjovsky et al. (2016) designed an RNN with a unitary hidden transition matrix. In the context of recurrent neural networks (RNN), unitary hidden transition matrices are utilized to mitigate vanishing and exploding gradients. Various unitary transformations such as diagonal matrices, permutations, rotations, and the Discrete Fourier Transform are employed. Different approaches like converting input to the frequency domain using Short Time Fourier Transform (STFT) and utilizing Fourier Recurrent Unit (FRU) have been proposed to address gradient issues and enhance expressivity. Speech separation has been extensively studied in audio processing literature. Recently, deep learning techniques have gained interest in solving this problem. Hershey et al. (2015) introduced a deep clustering approach for speech separation, where high-dimensional embeddings of mixture signals are learned and used for separation. Chen et al. (2016) extended this with the deep attractor network. In speech separation, deep learning techniques like deep clustering and deep attractor networks are used to learn high-dimensional embeddings for separating mixture signals. Different approaches integrate phase information and reconstruct time-domain signals for better clustering of time-frequency points dominated by different speakers. In speech separation, deep learning techniques integrate phase information to reconstruct time-domain signals for better clustering of time-frequency points dominated by different speakers. Our proposed framework provides an end-to-end solution for signal retrieval in the complex-valued frequency domain, processing both spectrogram magnitude and phase information. This differs from previous methods that only work on magnitude representation with heuristic phase reconstruction. The text discusses various approaches to speech separation, with some focusing on time-domain signal processing while others work on complex-valued spectral input. The ConvTasNet architecture, utilizing temporal convolutional networks, has shown promising results in audio speech separation. The text discusses the motivation for using FiLM in signal extraction to increase signal to noise ratio by leveraging the convolution theorem. The text discusses using FiLM for signal extraction by leveraging the convolution theorem and Fourier transform. It explains how to retrieve spectral information of clean signals and assumes an impulse response for speech separation. It also introduces a stochastic process for noise components in signal estimation. The text discusses signal extraction using FiLM, focusing on signal quality measured by SNR. It explains signal averaging to increase SNR and introduces a weighted cosine similarity for SDR optimization. The cosine similarity loss is defined in the real-valued domain, incorporating phase implicitly. In speech enhancement, a weighted cos time loss is used to differentiate between speech and noise signals based on target energies. In speech enhancement, a loss function is suggested that considers both magnitude and phase by computing the inner product in the complex plane. This inner product in the frequency domain is shift-invariant and is calculated as a weighted average of the cross correlation in the time domain. The complex inner product between two signals is determined by their similarity in magnitude and phase. The complex inner product between two signals is scale and time invariant, with a loss function that penalizes amplitude and phase mismatches. The real part is responsible for amplitude similarity, while the imaginary part is responsible for phase matching. The similarity loss function, CSimLoss, includes penalty constants for amplitude and phase mismatches. The architecture used for speech separation relies on the U-Net architecture and complex-valued building blocks. The penalty constants for amplitude and phase mismatches are \u03bb real and \u03bb imag, with \u03bb real fixed at 1. The optimal value found for \u03bb imag is 10^4 to ensure phase matching values align with amplitude matching values in the loss function. Results are presented in Table 2 and Table 3 for CSimLoss with \u03bb imag = 10^4. The proposed signal extraction mechanism improves baseline model performance by adding residual connections and replacing batch normalization with layer normalization. Complex LayerNorm outperformed BatchNorm due to model learning instabilities. The implementation of the extraction mechanism in audio source separation is discussed, utilizing simple complex residual blocks in U-Net architecture. Residual networks and identity connections have influenced image segmentation, with U-Nets incorporating these elements. The U-Net block architecture includes downsampling and upsampling blocks that double or halve feature maps, respectively. Complex normalization, CReLU, and convolution layers are applied successively. Downsampling uses a 1x1 kernel with a 2x2 stride, while upsampling employs bilinear interpolation. Residual blocks are used before and after doubling/halving blocks for memory constraints and improved performance. The FiLM approach applies affine transformations to convolutional feature maps based on the question embedding. Multiple transformations of the input spectrogram are created using FiLM, with parameters determined from the U-Net output. A complex mask is generated for the original spectrogram and each FiLM-transformed spectrogram using a conditioned ResNet. Each spectrogram is then multiplied by its corresponding complex mask, resulting in multiple candidates. The output of the last upsampling block in the U-Net is used to generate scaling and shift matrices for the input mix spectrogram. These matrices operate on the input mix through a complex convolution layer, creating multiple representations of the input mix for estimating clean speech. The complex representations of the input mix are multiplied by complex masks generated through a series of convolution layers and residual blocks. This process aims to separate speech from multiple speakers by focusing on specific patterns in the representation, increasing separation capability and reducing interference. Each mask corresponds to a specific input transformation and serves as a feature of the speaker embedding. The complex masking procedure aims to identify speakers by generating masks through convolution layers and residual blocks. Results of experiments and architecture variants are detailed in the appendix. Parameters such as the number of residual blocks and feature maps are explored to improve separation capability. The complex masking procedure generates masks through convolution layers and residual blocks to identify speakers. The number of parameters, input mixture transformations, and mask dropout rate are key factors. Dropping out masks reduces noise correlation and, along with signal averaging, regularizes the retrieval mechanism. Complex-valued baseline models outperform real-valued counterparts in the results. The real and complex U-Nets have the same architecture, but the complex models outperform the real ones by a significant margin. The complex networks have an advantage in input, inference, and output, making them superior despite differences in size or depth. The extraction mechanism improves the quality of the retrieved signal without adding many parameters. The extraction mechanism significantly enhances signal quality with minimal parameter increase. Spectral-domain losses outperform time-domain ones, with CSimLoss achieving the best SDR result. Comparison to ConvTasNet shows the effectiveness of the proposed extraction mechanism. In this paper, the authors compare their model to ConvTasNet, a speech separation system that achieved state-of-the-art results. They retrained ConvTasNet using a standard setup and obtained 12.1 SDR, compared to their model's 11.3 SDR. The authors introduced a new complex-valued extraction mechanism for signal retrieval in the Fourier domain, focusing on audio source separation and proposing a phase-aware loss function. The authors proposed a new phase-aware loss for audio source separation, improving over other frequency and time-domain losses. Their method aims to cancel differences in amplitude and phase between signals x and y. This could lead to new research directions in signal retrieval. The authors introduced a phase-aware loss for audio source separation to address differences in amplitude and phase between signals x and y. Resolving the system of equations ensures magnitude and phase alignment between the reference and estimation. Working with a null reference vector y can lead to infinite choices for the estimated signal x, prompting the use of a cosine similarity-based function to learn from noisy-only data. Complex layer normalization whitens 2D vectors by left-multiplying the 0-centered data by the inverse square root of the covariance matrix. Unlike batch normalization, it computes mean and covariance statistics over layer features instead of batch instances, avoiding the need to estimate batch statistics during training. This approach is more suitable for speech data, which is sparse in both time and frequency domains. The text discusses the limitations of batch statistics in speech data processing and advocates for the use of intra-sample normalization techniques like Layer Normalization. Batch normalization is deemed more appropriate for dense natural images. Intra-sample normalization like CLN is recommended for speech signals to ensure robust data normalization when dealing with a large number of feature maps. The speech mixtures used in training are generated from two-speaker mixtures selected randomly from the Wall Street Journal WSJ0 training set. The training set, si_tr_s, consists of mixed signals with varying SNR levels. A validation set of 10 hours and a test set of 5 hours were also generated using different speakers from the WSJ0 development set. STFT parameters include a Hann window size of 256 and a hop length of 128. Experiments with the Wall Street Journal dataset were conducted using models trained with backpropagation and Stochastic Gradient Descent with Nesterov momentum. Learning rate schedule from Trabelsi et al. (2017) was used for training. During training, a learning rate schedule from Trabelsi et al. (2017) was followed. The model was warmed up with a constant learning rate of 0.01 for the first 10 epochs, then increased to 0.1 from epoch 10 to 100. Learning rates were annealed by a factor of 10 at epochs 120 and 150, with training ending at epoch 200. Models were trained with batch sizes of 40 and 24 to fit GPU memory, using the Permutation Invariant Training criterion (PIT) to consider all possible assignments between target signals and estimated clean speeches. During training, the Permutation Invariant Training criterion (PIT) is used to minimize loss by considering the output-target assignment with the minimal training loss. This helps address the label permutation problem caused by randomly ordering target speakers. Inference assumes the model produces output without permuting speeches, as changing output-to-speaker assignment can decrease Signal to Noise Ratio (SNR) and Signal to Distortion Ratio (SDR) by interfering with speakers' speeches. The best configuration for the model involves using a complex standard initialization for most layers, with a unitary initialization for specific convolutional layers. This approach allows for generating FiLM parameters and masks efficiently. In Tables 2 and 3, different architectures with varying numbers of mixture transformations are tested. The addition of mixture transformations does not significantly increase the number of parameters in the model. The baselines in Table 2 do not include the proposed masking method and loss, and are similar to the U-Net in Figure 1 without certain components. Real and complex U-Nets output masks that are complex multiplied with the mix to infer clean speech. Table 2 presents speech separation experiments using different real and complex-valued model variants with varying numbers of residual blocks and feature maps. The number of parameters is expressed in millions, and the effective number of feature maps for a complex feature map is double the reported number due to real and imaginary parts. The study compared real and complex-valued model variants in speech separation experiments. Complex models outperformed real models, even with advantages in capacity and depth. The focus will now be on transformations and losses suitable for complex models. Wider and deeper models improved separation quality, and increasing input transformations had a positive impact. The study found that increasing input transformations positively impacted the task of separating audio sources, with the best results obtained using losses computed in the spectral domain. Local ensembling was shown to be beneficial for speech separation, and performing dropout on masks could help with regularization for retrieval and separation mechanisms. The study explored the impact of different dropout rates on input transformations for speech separation using spectral loss functions. Wider and deeper models were tested with varying numbers of input mixture transformations, resulting in improved SDR scores. In experiments with wider and deeper models for speech separation, it was found that wider models may overfit without input transformations. However, when 10 input transformations were introduced, SDR scores improved significantly. Ensembling with mixtures of 5 and 15 transformations acts as a regularizer, improving SDR scores. However, increasing the number of input transformations may lead to overfitting. Models with multiple input transformations outperform those without, achieving higher SDR scores on average. Dropping out speech candidates further regularizes the wider model. The experiments showed that a dropout rate of 0.1 resulted in the best SDR score improvement from 11.05 to 11.34. Using CSimLoss outperformed L2 freq, with wider models not exceeding a 10.91 dB threshold with L2 freq. The highest SDR score with L2 freq was 10.93 for a narrower model with 15 input transformations. Figure 4 and Figure 5 show validation curves of models with the highest SDRs using L2 spectral loss or CSimLoss, with and without dropout on input mixture transformations. The curves correspond to models with 15 input transformations."
}