{
    "title": "HydnA1WCb",
    "content": "We propose a novel architecture called Gaussian prototypical networks for k-shot classification on the Omniglot dataset. This model extends prototypical networks by incorporating a confidence region estimate as a Gaussian covariance matrix, allowing for a direction and class dependent distance metric in the embedding space. Results show improved performance over vanilla prototypical networks with equivalent parameters, achieving state-of-the-art results in 1-shot and 5-shot classification on the Omniglot dataset. Our experiments suggest that Gaussian prototypical networks may perform better in noisy datasets, common in real-world applications. Few-shot learning aims to replicate humans' ability to recognize new object categories with minimal examples. Deep learning models excel with abundant data but require slow, iterative training in a supervised regime. Few-shot learning requires fast adaptation to new data, with k-shot classification learning unseen classes using k labeled examples. Non-parametric models like k-nearest neighbors do not overfit but depend on distance metric choice. Architectures combining parametric and non-parametric models have been successful in k-shot classification. A novel architecture based on prototypical networks is developed and tested on the Omniglot dataset. The Gaussian prototypical network is a model that maps images into embedding vectors and estimates image quality. It predicts a confidence region around the embedding vector using a Gaussian covariance matrix, learning a direction and class-dependent distance metric in the embedding space. This model is shown to be a preferred way of using additional trainable parameters compared to increasing the dimensionality of vanilla prototypical networks. The study explores the advantages of allowing models to express confidence in individual data points, especially in noisy datasets. Results show performance consistent with state-of-the-art in 1-shot and 5-shot classification on the Omniglot dataset. The paper discusses related work, introduces methods, and presents an episodic training scheme. The paper discusses the advantages of non-parametric models like k-nearest neighbors for few-shot classifiers, emphasizing the importance of choosing the right distance metric. Different approaches, such as metric embedding and matching networks, have shown promising results in improving classification accuracy. The method discussed in the curr_chunk focuses on training a few-shot classifier by sub-sampling classes and examples in each mini-batch. Meta-learning, particularly using an LSTM to predict updates, has shown high accuracies on datasets like Omniglot. A task-agnostic meta-learner based on temporal convolutions has been proposed, outperforming other approaches in few-shot learning. Combinations of parametric and non-parametric methods have been successful in this area. Our approach in few-shot learning focuses on image classification without using meta-learning. Building on a model from BID16, we map images into embedding vectors and predict confidence with a learned covariance matrix. This enriches the embedding space for clustering images under a direction and class-dependent metric for classification. The Gaussian prototypical network diagram illustrates this process. The Gaussian prototypical network extends the prototypical networks by predicting embedding vectors and confidence regions using a Gaussian covariance matrix. It involves mapping images into embedding vectors, defining class prototypes, and classifying query images based on distances modified by total covariance. The Gaussian prototypical network uses class prototypes defined by support image embeddings and query image embeddings' proximity for classification. Encoder architectures for vanilla and Gaussian networks are similar, with differences in interpreting encoder outputs and constructing the embedding space metric. The encoder for vanilla networks transforms images into high-dimensional vectors, while the Gaussian network predicts embedding vectors and confidence regions using a Gaussian covariance matrix. The Gaussian prototypical network uses trainable weights \u03b8 for the encoder output, which includes an embedding vector x and a real vector s raw relevant to the covariance matrix \u03a3. Three variants are explored: a) Radius covariance estimate with D S = 1, b) Diagonal covariance estimate with D S = D, where \u03c3 is calculated from the raw encoder output s raw. The covariance matrix in the Gaussian prototypical network is represented as \u03a3 = diag ( \u03c3), where \u03c3 is calculated from the raw encoder output s raw. Two encoder architectures were used: a small one for validation and a big one to test increased model capacity. Four methods were explored to translate the raw covariance matrix output into an actual covariance matrix. The raw encoder output is transformed into a covariance matrix using different methods: a) using softplus function, ensuring S > 1, b) using sigmoid function, ensuring 1 < S < 2, and c) using a modified sigmoid function, resulting in 1 < S < 5. These approaches were beneficial for training the models. The prototypical model uses a training regime where a subset of classes is chosen, support and query examples are selected, and class prototypes are defined in the embedding space based on encoded embeddings. The distances between query examples and class prototypes are used for classification. The Gaussian prototypical network uses query examples and class prototypes to classify and calculate loss. It estimates the covariance of each embedding point and learns a class and direction-dependent distance metric in the embedding space. The distance from a class prototype to a query point is calculated using centroid and class covariance matrix. The optimal combination of Gaussians centered on individual points is used to create a class prototype in the Gaussian prototypical network. Linear Euclidean distances are preferred for constructing the loss function, with a variance proposed for calculating the class covariance matrix. The algorithm for this process is detailed in Algorithm 1. The algorithm described in Algorithm 1 weights by 1/\u03c3 2 and estimates model accuracy on the test set by classifying it for different numbers of support points N s = k. The accuracies are aggregated and the k-shot classification accuracy is determined. Test results for the 5 highest training accuracies are considered to ensure impartiality, and error bounds on accuracies are obtained. Models are evaluated in 5-way and 20-way test classification using the Omniglot dataset. The Omniglot dataset used for test classification included 1623 character classes from 50 alphabets, with 964 unique character classes in the training set and 659 in the test set. Data augmentation was done by rotating characters to increase the number of classes. No validation set was used, and the best model was chosen based on training accuracies alone. The Omniglot dataset used for test classification included 1623 character classes from 50 alphabets, with data augmentation by rotating characters to increase classes. This resulted in 77,120 images in the training set and 52,720 in the test set. Rotational augmentation introduced degeneracies for symmetric characters, making 100% accuracy unattainable. In a series of few-shot learning experiments on the Omniglot dataset, different aspects of Gaussian prototypical networks were explored, including embedding space dimensionalities, covariance matrix generation methods, and encoder capacities. A comparison with vanilla prototypical networks revealed the advantages of the Gaussian variant in utilizing additional trainable parameters. The best performance on Omniglot was achieved by predicting a single number per embedding point using the radius method. Various factors such as encoder size, distance metrics, degrees of freedom in the covariance matrix, and network dimensionality were investigated. In experiments on the Omniglot dataset, Gaussian prototypical networks were explored, including embedding space dimensions and covariance matrix generation methods. The models were trained with 60 classes and tested with 20 classes, achieving good results. The Adam optimizer with a learning rate of 2 \u00d7 10 \u22123 was used, and models were implemented in TensorFlow on a single NVidia K80 GPU in Google Cloud. Training time was less than a day, and down-sampling the input dataset improved performance. The mini-batch in the experiment had 1 support point per class, with 19 query points. Using encoder outputs as covariance estimates was found to be more beneficial than adding parameters for additional embedding dimensions. The best model was trained on an undamaged dataset for 220 epochs before continuing training with down-sampled images. The study continued training with down-sampled images, varying the sizes for different epochs. Purposeful damage to the dataset improved covariance estimate usage and (k > 1)-shot results. Models performed well in 1-shot and 5-shot classification on the Omniglot dataset, nearing perfect performance in 5-shot 5-way classification. The need for a more complex dataset for further few-shot learning algorithms was highlighted. The study validated the assumption that the Gaussian prototypical network outperforms the vanilla version by predicting covariances of individual embedded images. Results show high performance in 1-shot and 5-shot classification on the Omniglot dataset, approaching perfect accuracy in 5-shot 5-way classification. The study compared the performance of different networks in predicting covariances of embedded images. The network showed the ability to down-weight data points when trained on partially down-sampled data, leading to broader distribution of covariance estimates. In this paper, Gaussian prototypical networks were proposed for few-shot classification, outperforming vanilla prototypical networks on the Omniglot dataset. The study found that estimating a single real number on top of an embedding vector works better than estimating a diagonal or full covariance matrix. The results suggest that lower quality datasets may benefit from a more complex covariance matrix estimate. The study introduced Gaussian prototypical networks for few-shot classification on the Omniglot dataset, showing superior performance compared to vanilla prototypical networks. Training in the 1-shot regime yielded the best results, with close to perfect performance in 5-way classification. By down-sampling fractions of the training dataset, better accuracies were achieved, allowing the network to fully utilize covariance estimates. The ability to learn the embedding was highlighted as crucial for improved classification. The study demonstrated the effectiveness of Gaussian prototypical networks for few-shot classification on the Omniglot dataset, outperforming vanilla prototypical networks. By down-sampling parts of the training data, the network could better utilize covariance estimates, especially beneficial for poorer-quality datasets. This was supported by experiments with down-sampling Omniglot, showing the importance of down-weighting certain data points for accurate classification."
}