{
    "title": "rkMlSnAqYX",
    "content": "Recognizing the relationship between two texts is crucial for natural language understanding (NLU). Neural network models used for NLU tasks often face biases in training datasets, affecting their performance. A proposed framework utilizes adversarial learning to create bias-free representations, particularly in Natural Language Inference (NLI) scenarios. The adversarially-trained models show robustness against dataset-specific biases, making them more reliable for new NLI datasets. Natural Language Inference (NLI) is used to assess a model's understanding of the relationship between two texts. Recent studies have shown biases in NLI datasets that allow hypothesis-only models to perform well without truly learning the text relationship. This is evident in popular datasets like SNLI, where annotation artifacts enable models to outperform baseline expectations. The existence of annotation artifacts in NLI datasets hinders progress in deep learning research for NLU. Trusting top models' performance is challenging when relationships can be inferred without looking at the premise. Current solutions like constructing new datasets, filtering easy examples, and compiling adversarial examples are limited in addressing the issue of generalization in models despite dataset biases. The proposed architectures aim to reduce dataset-specific biases in NLI models by using adversarial learning during classification tasks. Experimental results show that these architectures generate more robust sentence representations and perform better on different datasets compared to non-adversarial models. The study explores methods to ignore hypothesis-only biases when training NLI models, finding annotation artifacts in multiple NLU datasets. Previous research has identified biases in NLI tasks, allowing models to perform well by focusing on syntactic clues alone. The proposed architectures aim to reduce dataset-specific biases in NLI models through adversarial learning during classification tasks. Biases in NLU datasets were found due to faulty dataset-construction methods. Various NLU datasets exhibit biases, such as high performance in story cloze completion without considering the story context. Stylistic features in datasets like ROC stories can predict the correct ending. In reading comprehension, systems performed well by only using the final sentence or ignoring the passage. Visual question answering studies showed non-trivial performance by using only the question text due to biases. Neural networks are sensitive to adversarial examples in various tasks like machine vision, NLP, and reading comprehension. Training models on data with adversarial examples may not generalize well. Domain-adversarial neural networks aim to increase robustness to domain change. Our method learns to ignore latent annotation artifacts without requiring direct supervision. Our goal is to create robust representations that transfer well to other datasets, unlike previous work that focused on removing biases from text representations. Recent studies have used adversarial learning in NLI tasks to generate non-logical examples and improve model robustness. Our approach does not aim to remove specific attributes but rather aims for better transferability to different datasets. Our approach uses a GAN-style framework to train robust NLI models without external resources. We focus on removing biases that benefit hypothesis-only models. The baseline NLI architecture includes an encoder and classifier, with proposed adversarial architectures for further improvement. The model includes premise and hypothesis encoders, a classifier, and addresses biases in datasets to improve robustness in Natural Language Inference (NLI) models. The design of two models for robust NLI, a single-classifier model and a double-classifier model, aims to remove biases in hypothesis representations through adversarial learning. The double-classifier model includes an adversarial classifier that maps the hypothesis representation to an output, emphasizing the interaction between the NLI classifier and the hypothesis classifier. The NLI classifier is trained to minimize a specific objective function, with gradients back-propagated from the classifier into the premise and hypothesis encoders. The double-classifier model for robust NLI includes an adversarial hypothesis classifier that aims to remove biases in hypothesis representations. Gradients are back-propagated from the classifier into the premise and hypothesis encoders, while the hypothesis classifier is trained to minimize cross-entropy loss with reversed gradients to discourage learning patterns useful for classification. The interplay between the NLI classifier and the hypothesis classifier is controlled by hyper-parameters. The double-classifier model includes hyper-parameters \u03bb Loss and \u03bb Enc to control adversarial loss and weight of adversarial update. The model has a potential limitation where biases may still be encoded in a way accessible to the normal classifier but not to the adversary. In neural cryptography, Alice and Bob communicate while Eve eavesdrops. Abadi & Andersen's experiments showed that encoded communication could still be eavesdropped on. To prevent biases from being hidden in the representation, a new architecture combines NLI and adversarial classifiers into a single network. The single-classifier model in neural cryptography combines NLI and adversarial classifiers into a single network to prevent hidden biases. It includes premise and hypothesis encoders and one classifier, g, which acts as both a normal NLI classifier and an adversarial hypothesis classifier. This setup aims to reduce the risk of false impressions and learning hidden biases in hypothesis representations. The model operates in two modes: normal mode where premise-hypothesis pairs are fed through encoders and classifier, and adversarial mode where the premise is replaced with a random one to predict entailment decisions. The single-classifier model in neural cryptography combines NLI and adversarial classifiers into a single network to prevent hidden biases. It includes premise and hypothesis encoders and one classifier, g, which acts as both a normal NLI classifier and an adversarial hypothesis classifier. In the adversarial mode, gradients are reversed going into the hypothesis encoder using GRL \u03bb to scale it by \u03bb Enc. This simplifies the architecture by avoiding the need for two classifiers and reduces the risk of failed adversarial learning. The proposed architecture combines NLI and adversarial classifiers into a single network to prevent hidden biases. It includes premise and hypothesis encoders and a single classifier. The training regime involves choosing random examples with a hyper-parameter \u03bb Rand. The final loss function combines two operation modes with a random variable z \u223c Bernoulli(\u03bb Rand). To evaluate the model's performance on NLI datasets, 11 datasets are used, including SNLI and MNLI. The Multi-genre Natural Language Inference (MNLI) BID58 dataset was created by humans generating hypotheses for given premises. Adversarial models were trained on SNLI to demonstrate robustness, and evaluation was done on various datasets including SNLI-hard. Other NLI datasets considered were human-judged datasets like Scitail, ADD-ONE-RTE, JOCI, MPE, and SICK, which used automatic methods for pairing context and hypothesis sentences. The text discusses the evaluation of semantic phenomena using datasets recast from existing NLU datasets into NLI. A mapping from model predictions to target dataset labels is defined, and InferSent's method is adopted for learning sentence representations. The chosen model utilizes a bidirectional LSTM network and a one hidden layer neural network for combining premise and hypothesis representations. The study evaluates different architectures for NLI models, focusing on the performance of a double-classifier model compared to a non-adversarial baseline model. Results show that the double-classifier outperforms the baseline in most cases, with larger gains compared to the single-classifier model. The methodology can be applied to other models as well, following the InferSent training regime. The study compares the performance of double-classifier and single-classifier models in NLI tasks. Results show that the double-classifier model outperforms the baseline, with minimal decrease in performance on SNLI even with large improvements on target datasets. In contrast, the single-classifier model shows a significant drop in performance on SNLI when there are large improvements on target datasets. The study compares double-classifier and single-classifier models in NLI tasks. Adversarial architectures show improved results on bias-free GLUE diagnostic test set. However, no improvements are seen on SNLI hard subset BID21, suggesting different biases exist in target NLI datasets compared to SNLI. The study compares double-classifier and single-classifier models in NLI tasks, showing improved results on bias-free GLUE diagnostic test set. Adversarial models perform worse on MNLI compared to SPR, with slight improvements on datasets with biases. Strengthening the adversary leads to the double-classifier outperforming the baseline on SCITAIL. Our study evaluates the robustness of adversarial learning in NLI tasks across multiple datasets by fine-tuning on target datasets with varying amounts of training data. Four models were updated on MNLI and SICK datasets, showing consistent label spaces with SNLI. Our study evaluates the robustness of adversarial learning in NLI tasks across multiple datasets by fine-tuning on target datasets with varying amounts of training data. MNLI and SICK have the same label spaces as SNLI. Adversarial pre-training shows little gain in MNLI but is better in most data regimes in SICK. Stronger adversary may improve the quality of transferred representations. The study evaluates the impact of stronger adversaries on NLI tasks across different datasets. Results show that while performance on SNLI test sets decreases with a stronger adversary, many other datasets benefit from it. Single-classifier models experienced significant drops in quality even with basic configurations. Hidden biases in the representation of hypothesis encoder f H were discovered through experiments with a trained adversarial model. The study found that even with basic configurations, single-classifier models showed decreased quality when faced with stronger adversaries. This raises concerns about the presence of hidden biases in learned hypothesis representations. The study revealed hidden biases in hypothesis representations through experiments with an adversarial model. Results on SNLI's dev set showed that retraining a classifier on frozen hypothesis representations improved performance, indicating biases still exist. Even a frozen random encoder captured biases, performing well above the majority baseline. This suggests that word embeddings contain significant information that can propagate through a random encoder. The study found biases in hypothesis representations using an adversarial model. Results on SNLI's dev set showed that retraining a classifier on frozen hypothesis representations improved performance, indicating biases still exist. Word embeddings contain significant information that can propagate through a random encoder, as shown by others. The adversarial models make predictions less impacted by biases, especially with certain contradiction words. The top 5 examples in the training set show that the adversarial model predicts contradiction less frequently than the baseline on examples with these words. The study presented a solution for combating biases in NLI datasets using adversarial learning. Two architectures were designed to discourage biases in hypothesis representations, resulting in more unbiased models. Empirical evaluation showed improved performance compared to non-adversarial baselines on various datasets. The methodology developed in this work can be extended to deal with biases in other NLU tasks, such as Reading Comprehension, story cloze completion, and Visual Question Answering. Adversarial architectures can help remove biases from the latent representation in models relying on encoding biased elements. The study considered a range of datasets and highlighted the importance of investigating biases in the broader research community. The study explores biases in NLU tasks like Reading Comprehension and Visual Question Answering. They use the InferSent training regime with SGD and an initial learning rate of 0.1. MNLI is used, but gold labels for the test set are not publicly available. They remove 10K examples from the training set for the development set. Results show performance on SNLI with adversarial training and transferring representations to new datasets. The study examines biases in NLU tasks like Reading Comprehension and Visual Question Answering, using the InferSent training regime with SGD. Results show performance on SNLI with adversarial training and transferring representations to new datasets. The hypothesis-only model trained on SNLI performs below the majority baseline when tested on other datasets, indicating different biases in each dataset. The study analyzes biases in NLU tasks using the InferSent training regime with SGD. Results show differences in biases between datasets, with the hypothesis-only model trained on SNLI performing below the majority baseline on other datasets. Cross-validation results with different hyper-parameter settings are provided, showing performance degradation with increased random premises. The double classifier results are more stable, and the top 20 indicator words in SNLI are shown in TAB5."
}