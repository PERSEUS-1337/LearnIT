{
    "title": "B1gF56VYPH",
    "content": "Recent advances in deep learning have shown promising results in low-level vision tasks, but single-image-based view synthesis remains a challenge. A novel network architecture called monster-net has been proposed for stereoscopic view synthesis at arbitrary camera positions along the X-axis, using adaptive kernels with globally and locally adaptive dilations. This architecture efficiently handles global camera shifts and local 3D geometries to create natural-looking 3D panned views from 2D input images. Extensive experiments were conducted on datasets like KITTI and CityScapes. Our monster-net outperforms the state-of-the-art method in RMSE, PSNR, and SSIM metrics on datasets like KITTI and CityScapes. The \"t-shaped\" kernel provides more reliable disparity information for unsupervised monocular depth estimation, confirming the effectiveness of our method. Recent advances in deep learning have significantly improved novel view synthesis tasks, which involve generating new views from different camera positions. This technology has various applications in robotics, navigation, virtual and augmented reality, and cinematography. Generating stereo images from a single input view is particularly challenging but essential for 3D visualization. The demand for stereoscopic content has increased due to the affordability and availability of VR/AR equipment. Previous works like Deep3D have focused on supervised right-view generation, while our proposed Deep 3D Pan pipeline offers a new approach. Our proposed Deep 3D Pan pipeline allows for generating new views at arbitrary camera positions with better quality by using adaptive convolutions. This model adjusts the baseline for different 3D sensations and inter-pupillary distances. The \"monster-net\" can produce left and right view images from a single input, defining \"pan\" as camera movement parallel to the center view. In the context of 3D modeling, camera movement is parallel to the center view plane. The Deep 3D Pan pipeline generates new views at arbitrary camera positions with adaptive convolutions, showing superior performance on challenging stereo datasets. Previous deep learning-based approaches have surpassed classical techniques for both multiple-image and single-image input novel view synthesis. Previous deep learning-based approaches have utilized optical flow guided image warping and adaptive convolutions to generate novel views. Optical flow guided image warping has been used to indirectly train CNNs for flow estimation, but not for synthesizing new views. Some methods have implemented flow-guided warping for unsupervised training or to regularize supervised methods. Im et al. (2019) used plane sweep at the feature level to create a cost volume for multi-view stereo depth. The curr_chunk discusses the implementation of plane sweep at the feature level for multi-view stereo depth estimation, comparing it to kernel estimation or adaptive convolutions used in previous works by Flynn et al. (2016) and Xie et al. (2016). These approaches involve blending multiple inputs to generate synthetic views. The curr_chunk discusses various approaches to adaptive convolutions in the context of video interpolation and multiplane image representation. These methods include using shifted versions of input images, adaptive separable convolutions, geometric-aware networks, and affine transformations. In the context of video interpolation and multiplane image representation, different approaches to adaptive convolutions are discussed. These methods involve using multiple input images to reduce complexity and improve synthesis accuracy. The focus is on single-image based stereoscopic view synthesis, which is a challenging task requiring understanding of 3D geometry, handling occlusions, ambiguities, and non-Lambertian surfaces. The Deep 3D Zoom Net by Gonzalez & Kim (2019a) is highlighted for its ability to estimate a selection volume for blending multiple upscaled versions of an input image. The curr_chunk discusses adaptive convolutions in DeepStereo and Deep3D, focusing on the drawbacks of a 1D horizontal-only constant-dilation kernel. The kernel's inefficiency in sampling positions opposite to camera movement and further away from maximum disparity values is highlighted. The drawbacks of geometric-aware networks in stereo vision include limited occlusion handling capabilities and complexity due to the need for multiple sub-networks and pre-processing steps. The networks predict deformable kernel structures but only one shape is shared for all output pixels, leading to limited performance. The Deep 3D Zoom Net uses adaptive kernel dilation for generating 3D-zoomed output images, while SepConv in video interpolation employs a sequential NxN adaptive kernel with limited receptive fields. Recent works have focused on improving stereoscopic view synthesis by enhancing loss functions for training CNNs. Zhang et al. (2019) introduced a multi-scale adversarial correlation matching (MS-ACM) loss to penalize structures and ignore noise, aiming to improve image quality. However, it is suggested that using widely accepted l1 and perceptual losses for image reconstruction may be more beneficial than complex loss functions in addressing issues with stereoscopic view synthesis. Our proposed dilation adaptive \"t-shaped\" convolutions incorporate global and local information of the input scene into the synthesis of each output pixel value by learning specific kernels and dilation values. The \"t\" shape allows the network to fill in gaps due to shifted camera positions using neighboring pixel information, generating 3D panned versions along the X-axis. Our proposed global dilation filter, including cross-shaped and \"t-shaped\" kernels, allows for adjusting 3D panning along the X-axis in image synthesis. The filter parameters are defined for different directions, enabling adaptive dilation based on the desired 3D effect. The proposed global dilation filter adjusts 3D panning based on the input image and scene geometry. It uses \"t-shaped\" kernels to capture more information for synthesis, with longer wings for left or right camera panning directions. The \"t-shaped\" kernels are more efficient than symmetric shapes for camera panning synthesis, capturing disparity and occlusion information with fewer parameters. The \"t-shaped\" kernel approach efficiently captures disparity and occlusion information for camera panning synthesis, generating a natural and appropriate disparity map. The network utilizes surrounding information to fill in gaps in the \"t-shaped\" kernel for disparity and occlusion maps, showing occlusions due to camera shifts. Disparity amounts vary based on camera distance and scene geometries. The \"t-shaped\" kernel introduced considers variable disparitites in synthesizing new views globally and locally adaptively. It allows for effective accounting of camera shift and local image geometry changes, enhancing accuracy in novel view synthesis. The global dilation value (g d) is determined by the pixel distance between consecutive kernel sampling positions, which is calculated based on the pan amount (P a) applied to the input center image divided by the total number of filter parameters in the longer \"t-shaped\" kernel wing. During training, the pan amount for leftward or rightward panning is determined based on the closest objects to the camera. The KITTI dataset is assumed to have a maximum object disparity of 153 pixels. Global dilation considers global camera shift, while a locally adaptive mechanism blends multiple images at different dilations to synthesize new views with variable disparity. The \"monster-net\" is an end-to-end trainable CNN that blends filtered images using a \"t-shaped\" kernel with N different dilations. The blending weights control local dilation per-pixel and are learned via a convolutional neural network. The output image value at a pixel location is calculated based on the global dilations, with a view synthesis network called the \"t-net\" and a resolution restoration block called the \"sr-block\". The \"t-net\" estimates global dilation kernel parameters and local dilation weights for synthesizing detailed image structures of a new view image. It utilizes an auto-encoder with skip connections to have large receptive fields and efficiently fuse global information. The \"t-net\" estimates values for global dilation kernel parameters and local dilation weights to synthesize detailed image structures in a new view image. It utilizes an auto-encoder with skip connections for better feature extraction and efficient fusion of global and local information. The t-net has two output branches, one yielding horizontal and vertical parameter maps, and the other generating blending weight maps for local adaptive dilation. The t-net generates novel views by estimating global dilation kernel parameters and local dilation weights. It uses a 1-channel constant feature map to account for varying pan direction and occlusions. A super resolution block estimates a low-resolution view and applies deep learning techniques to bring it to high resolution. Our proposed pipeline introduces a stereo-SR method that utilizes LR stereo pairs to generate super-resolved images. This technique involves progressively shifting the right-view and processing it with a CNN to enhance the left-view at a fixed stride. Our proposed Deep 3D Pan pipeline utilizes a maximum disparity prior to dynamically set the shifting stride for processing LR stereo pairs. The high-resolution center view is progressively shifted and downscaled by x2 to maintain high frequency information. The sr-block module effectively operates on LR dimensions without performance degradation. The LR input view is processed using a shifted-downscaled center view stack, where the stride can be any real number. The stack undergoes Conv-ReLU layers and upscaling to the target resolution, with the final output being an RGB image. Nearest interpolation is used to avoid checkerboard artifacts. The effectiveness of the \"t-shaped\"-dilation-adaptive kernel was demonstrated through experiments on KITTI2012, KITTI2015, and CityScapes datasets, as well as the VICLAB STEREO dataset focusing on indoor scenes. The monster-net formulation allows training on multiple stereo datasets simultaneously, avoiding over-fitting on a single camera baseline. The Deep 3D Pan is a novel approach in this context. The Deep 3D Pan pipeline is the first method designed for stereoscopic view synthesis trained on multiple datasets concurrently. It is compared against Deep3D and a modified version of SepConv for fair comparison. The monster-net is also compared with Deep3D-B, a larger version of Deep3D. The Deep3D model is compared against SepConv-D, with Deep3D-B and SepConv-D trained using a combination of l1 and perceptual loss. The quality of the embedded disparity in the \"t-shaped\" kernel is also compared with state-of-the-art models for monocular depth estimation. For more implementation details, refer to the appendix A-4. Our method introduces a disparity refinement sub-network using prior information from a \"t-shaped\" kernel and a special post-processing step for generating sharp and consistent disparity maps. Performance comparison with previous works shows our monster-net outperforms Deep3D baseline by 0.7dB in PSNR, 2.0 in RMSE, and 0.03 in SSIM on full resolution images. Our method outperforms the Deep3D baseline by 0.7dB in PSNR, 2.0 in RMSE, and 0.03 in SSIM. The qualitative results show superior image quality compared to Deep3D and SepConv. Our model generalizes better on different datasets and incorporating an indoor dataset further improves performance. The monster-net, trained on K+CS+VL datasets, achieved a mean PSNR of 21.78 dB. Visualizations and ablation studies proving design efficacy can be found in the appendices. The addition of a disparity refinement sub-network helped the monster-net outperform state-of-the-art models for unsupervised monocular depth estimation. Our monster-net with disparity refinement outperforms supervised and unsupervised methods for monocular disparity estimation. It utilizes an adaptive \"t-shaped\" kernel for Deep 3D Pan, showing superior performance for right-view generation on KITTI and CityScapes datasets. The monster-net also demonstrates good generalization capabilities and produces high-quality synthetic panned images with no discontinuities. The monster-net utilizes a \"t-shaped\" kernel for Deep 3D Pan, generating novel views with varying pan amounts. The network produces different maps for different pan magnitudes, handling occlusions in the output image. The pan amount serves as prior knowledge to the network, affecting the generated disparity maps and occluded content in the 3D panned image. The disparity refinement network architecture in Figure 10 has two input branches: one with the center image disparity prior and RGB image, and the other with the output panned view disparity prior and generated panned view. This architecture improves stereo matching performance by increasing the receptive field size in the horizontal axis. The disparity refinement network architecture improves stereo matching performance by increasing the receptive field size in the horizontal axis. The network utilizes ambiguity learning to account for occlusions and complex regions, training with loss functions to produce refined disparity maps similar to primitive ones. The disparity refinement block in the network improves stereo matching by minimizing a primitive disparity loss and incorporating ambiguity learning to blend disparities from the first and second forward passes. This approach differs from naive post-processing methods and can be trained end-to-end with the network. Our novel post-processing step involves running the forward pass of the monster-net with disparity refinement block for the first and second pass, using ambiguity masks to combine disparity maps. The KITTI dataset is utilized for evaluation. The KITTI dataset consists of mid-resolution stereo images used for evaluation, with 29,000 stereo pairs from 33 scenes. The KITTI2015 dataset is reserved for validation, containing 400 images and sparse disparity ground truths obtained from LIDAR. CityScapes is a higher resolution stereo dataset with 24,500 stereo pairs used for training and validation. Our Deep 3D Pan pipeline is the first method trained on multiple baseline datasets for stereoscopic view synthesis and unsupervised monocular depth estimation. Unlike previous work, we only require knowledge of the dataset baseline to train on multiple datasets. The Deep 3D Pan pipeline is trained on multiple baseline datasets for stereoscopic view synthesis and unsupervised monocular depth estimation. The training process involves multiplying the baseline by a relative factor, using the Adam optimizer with specific parameters, and applying various data augmentations. The vertical flip augmentation was found to hinder learning and was therefore avoided during training. During training of the monster-net model, images were sampled with a 50% chance for rightward or leftward panning. Random resolution degradation was applied to the input to improve results by making the network more sensitive to edges and structures. Tricks like adding noise to input have been used in previous works to make networks invariant to noise and more sensitive to structures. Left or right view can be used as input during training, with specific parameters set for each. When training the monster-net model, the pan amount is set based on the input view used, with specific kernel configurations for different models like Deep3D and SepConv. The monster-net is trained using a combination of l1 loss and perceptual loss to measure the distance between generated and ground truth images in a deep feature space. The perceptual loss function utilizes the mean square error of the output of the first three max-pooling layers from the pre-trained VGG19 network. A constant \u03b1p = 0.01 is introduced to balance the contributions of l1 and perceptual losses. The total loss function is applied to both low-resolution and super-resolved images to yield the total loss function Lpan. Visualizations on the CityScapes datasets for the monster-net trained on KITTI and KITT + CityScapes are shown in Figure 13. The Deep3D baseline over-fits to KITTI dataset but performs poorly on CityScapes. Our monster-net excels on CityScapes when trained with or without it, showing improvements over Deep3D. Utilizing perceptual loss enhances sharpness and structure in results on CityScapes. Sharper results with clear edges and structures are achieved using perceptual loss, improving generalization on indoor datasets. Comparison between networks trained on different datasets shows better performance with marginal decrease on KITTI dataset and significant quality improvement on VICLAB STEREO dataset. The method allows for 3D panning beyond the baseline, as demonstrated in Figure 15. The monster-net with adaptive dilations allows for 3D panning beyond the baseline, generating new views with consistent structures and no discontinuities. The adaptive \"t-shaped\" kernel with globally and locally adaptive dilations contributes to this, producing sharper results compared to fixed dilation kernels. Our pipeline benefits from training on multiple datasets simultaneously, such as KITTI and CityScapes, improving geometry reconstruction by exposing the network to a variety of objects at different resolutions. The addition of a super resolution block enhances sharpness in images, and utilizing perceptual loss further improves quality. Our monter-net with refinement block, utilizing perceptual loss, outperforms state-of-the-art methods for unsupervised monocular depth estimation. The addition of ambiguity learning further improves prediction accuracy on the KITTI2015 dataset. Our monster-net with disparity refinement and special post-processing shows significant improvement in accuracy metrics compared to state-of-the-art methods. The qualitative comparison against previous methods and ground truth disparity demonstrates reliable disparity maps, especially on thin structures and image borders. Additionally, our method excels in detecting far away objects, as shown in Table 3 for KITTI2015 metrics. The best performing models are highlighted in bold based on a 1 threshold, representing the percentage of disparity values with a relative error less than 0.25."
}