{
    "title": "HkezXnA9YX",
    "content": "Numerous models for grounded language understanding have been proposed, including generic models and modular models. A comparison was made on their systematic generalization abilities using a synthetic VQA test. Modular models showed more systematic generalization, with sensitivity to module layout. End-to-end methods often learned inappropriate layouts. Our results suggest that systematic generalization in language understanding may require explicit regularizers or priors in addition to modularity. Neural network models dominate natural language processing tasks like machine translation and text generation, excelling in benchmarks but struggling to generalize beyond specific training data distributions. These models tend to rely on statistical regularities present in datasets, making it challenging for them to make accurate predictions in real-world scenarios. Neural networks struggle with generalization beyond specific training data distributions, lacking systematicity in how they generalize. Adding modularity and structure to their design, like in Neural Module Networks, can improve their generalization capabilities. Neural Module Networks (NMNs) use several modules to process input, similar to a computer program. However, their adoption is hindered by the need for domain knowledge in creating and connecting modules. NMNs' performance is often matched by traditional neural models. This study investigates the impact of modularity and structure on NMNs' generalization abilities compared to generic models. In this case study, the focus is on visual question answering (VQA) in its binary form, where the answer is \"yes\" or \"no\". A good VQA model should be able to reason about all possible object combinations, even if trained on a small subset. To address this, a new synthetic dataset called Spatial Queries On Object Pairs (SQOOP) is introduced to test spatial relational reasoning abilities. The study introduces a new dataset called Spatial Queries On Object Pairs (SQOOP) for testing spatial relational reasoning abilities in visual question answering. It is found that Neural Module Networks (NMNs) generalize better than other neural models when layout and parametrization are appropriately chosen. Using a tree layout is crucial for solving the hardest version of the dataset. Experimenting with end-to-end methods for NMNs shows promising results. Our experiments challenge the intuition of researchers in the field and aim to improve systematic generalization of neural approaches to language understanding. The study is conducted on the SQOOP dataset, a minimalistic VQA task designed to test the model's ability to interpret unseen combinations of known relation and object words. The dataset involves queries about known objects and relations, ranging from common to extremely rare instances. SQOOP is a VQA task that tests compositional reasoning skills by asking yes-no questions about spatial relations between objects in images. Each image contains 5 randomly chosen objects represented by letters and digits, with relations like LEFT OF and ABOVE. The dataset aims to improve systematic generalization of neural approaches to language understanding. The dataset SQOOP contains 5 objects with spatial relations like LEFT OF and ABOVE, resulting in 5040 unique questions. Training sets are created with unique questions to test models' ability to answer questions about all possible object pairs after being trained on a subset. The #rhs/lhs parameter is used to sample different objects for each object pair to ensure solvability. The dataset SQOOP contains 5 objects with spatial relations like LEFT OF and ABOVE, resulting in 5040 unique questions. Training sets are created with unique questions to test models' ability to answer questions about all possible object pairs after being trained on a subset. The #rhs/lhs parameter is used to sample different objects for each object pair to ensure solvability. Different versions of the dataset with varying #rhs/lhs values are generated for training and testing, with lower values being harder for generalization. Training sets contain 1 million examples to prevent overfitting on training images. Various VQA models have been proposed in the literature. Several VQA models have been proposed in the literature, with two main trends identified. Some models like FiLM and Relation Networks are generic and task-agnostic, while others like Neural Module Networks require task-specific knowledge. The models process images through a CNN stem, followed by task-specific computations conditioned on the question, and a fully-connected classifier for prediction. The models considered in this paper for VQA include CNN+LSTM, FiLM, Relation Network (RelNet), and Memory-Attention-Control (MAC) network. These models differ in how the computation h q x = model(h x , q) is performed. CNN+LSTM, FiLM, and RelNet encode the question q into a fixed-size representation h q using a unidirectional LSTM network. RelNet uses a network g applied to pairs of feature columns of h x concatenated with the question representation h q. FiLM networks use convolutional FiLM blocks applied to h x. FiLM block is a residual block where a FiLM layer is inserted after the 2nd convolutional layer, conditioned on the question via scaling and shifting parameters. The output undergoes an extra convolution and max-pooling to produce h q x. MAC network produces h q x using a Memory-Attention-Composition cell conditioned on the question. NMN constructs a question-specific network for question answering. The NMN approach to question answering involves constructing a question-specific network using trainable neural modules. These modules are connected in a computation graph based on layout and parametrization decisions. Different module types perform various computations, with a trend towards using more homogeneous modules. In this work, the focus is on using homogeneous modules in Neural Module Networks (NMNs) for question answering. The study simplifies the NMNs by using only two types of modules: unary and binary, with a single binary module type retained when only one input is available. Despite these simplifications, NMNs outperform other models in terms of generalization. The layout decision is simplified by using exactly three modules, making the connection between modules the main focus. In the follow-up works, aspects of computation in Neural Module Networks (NMNs) are predicted by learnable mechanisms to reduce background knowledge needed. The End-to-End NMN paradigm predicts layout with a seq2seq model and computes module parametrization using a soft attention mechanism. The NMN is constructed by applying a generic neural module with shared parameters, question-specific parametrization, and inputs. The Neural Module Networks (NMNs) use modules connected and conditioned on a question q. Two choices for the generic neural module are the Find module and the Residual module. The equations for these modules involve convolution weights and biases. The Find and Residual modules in Neural Module Networks differ in how parameters are dependent on question words. The Find module outputs a feature tensor, unlike in BID12 where it only outputs an attention map. Different architectures are experimented with, such as NMN-Chain where modules form a sequential chain. In NMN-Chain and NMN-Tree, attention vectors are hardcoded with different module connectivities. Stochastic N2NMN treats layout as a stochastic variable with tree or chain values, calculating output probabilities by marginalizing layout. The Attention N2NMN model uses the N2NMN method for learning parametrization and is structured similarly to NMN-Tree. It aims to understand models capable of systematic generalization and induce successful architectural decisions. All models share the same stem architecture with convolution layers, batch normalization, and max pooling. The input is a 64x64x3 image. The NMN-Tree model shows strong systematic generalization in solving various dataset versions, including the most challenging split. Generic models like Conv+LSTM and RelNet struggle with SQOOP questions, while MAC and FiLM perform well with almost perfect accuracy. The MAC and FiLM models struggle to generalize on splits with lower #rhs/lhs ratios in the SQOOP task, showing error rates of 13.67% and 34.73% respectively on the hardest split. Even with an 18 #rhs/lhs ratio, some runs result in a 2% error rate. These results are considered a failure to pass the SQOOP test for both models, with MAC showing occasional strong generalization performance on #rhs/lhs=1. In successful MAC models, specific units focus on the right question words, with a high number of MAC units (12) being helpful for convergence. Ablation studies on MAC models did not significantly reduce the performance gap with NMN-Tree. The NMN-Tree model differs from generic models by not using a language encoder and instead being built from modules parametrized by question words. It is structured to allow modules to locate objects and reason about object locations independently. To determine the key difference for superior generalization, performance of NMN-Tree, NMN-Chain, and NMNChain-Shortcut models are compared. The NMN-Tree model, unlike generic models, does not use a language encoder and is built from modules parametrized by question words. Results show that using a tree layout is crucial for generalization, with NMN-Chain models performing poorly. The NMN-Chain model struggles with generalization, especially on the easiest dataset splits, in contrast to the NMN-Tree model which performs nearly perfectly. Adding shortcut connections in NMN-Chain-Shortcut significantly improves generalization performance. Comparing test set responses of NMN-Chain models trained on different dataset splits reveals insights into its poor generalization. The NMN-Chain model shows poor generalization, with little agreement between predictions of different runs. The NMN-Tree model demonstrates strong generalization but requires prior knowledge for successful layout and parametrization. Layout induction experiments are conducted using the Stochastic N2NMN model to reduce the need for prior knowledge. The Attention N2NMN models show that high attention quality is associated with accuracy, especially for #rhs/lhs=1 splits. Initial probability p 0 (tree) = 0.1 is considered significant due to the challenging datasets. Experiments on different splits reveal that layout induction success depends on various factors. The initialization of models with different p(tree) values significantly influences their convergence behavior. Models initialized with p(tree) = 0.1 struggle to converge to a tree, while those initialized with p(tree) = 0.9 consistently stay in a regime with a high p(tree). Intermediate settings of p(tree) = 0.5 show varied behaviors for different modules. Notably, N2NMN with Residual modules remains spurious with p(tree) = 0.5 \u00b1 0.08, while N2NMN with Find modules always converges to a tree. Surprisingly, Stochastic N2NMNs with Residual modules trained with p(tree) = 0.5 and #rhs/lhs=1 achieve low test error despite not resolving layout uncertainty during training. The Attention N2NMN model experiments with fixed tree-like layouts and varying attention weights. Results show it fails at #rhs/lhs=1 but improves at #rhs/lhs=2, with 9 out of 10 runs performing well. Variance in performance is high, with one run showing a 26% error rate. The Attention N2NMN model shows high variance in performance, with one run having a 26% error rate. All 10 runs on the split with 18 rhs/lhs generalize flawlessly. Modules 1 and 2 focus on different object words while module 3 focuses on the relation word. An attention quality metric \u03ba is defined to understand the relationship between successful layout induction and generalization. The scatterplot in Figure 5 shows that high generalization is strongly associated with higher \u03ba values, indicating the importance of different modules focusing on different object words. Despite some cases of spurious attention, Attention N2NMN performs well compared to MAC, especially on the split with 2 rhs/lhs. Recent research has shown that Attention N2NMNs outperforms MAC in generalization, especially on the split with 2 rhs/lhs. This suggests that Attention N2NMNs retains the strong generalization potential of NMNs with hard-coded parametrization. The debate on whether connectionist models can account for systematicity in cognition continues. Recent research has highlighted the lack of systematic generalization in modern seq2seq models, which also affects generic VQA models. Adding explicit modularity and structure to deep learning models shows potential for improving systematicity. Neural networks often struggle with generalization in downstream language tasks, relying on dataset-specific biases for answers. Recent research has shown that state-of-the-art natural language entailment and reading comprehension systems can be influenced by statistical biases and unrelated information. The use of synthetic datasets like CLEVR and ShapeWorld aims to improve grounded language understanding by testing models on unknown combinations of known words. SQOOP focuses on minimizing the difference between training and test images to evaluate a model's ability to interpret new scenarios. The ShapeWorld dataset by BID22 is a synthetic VQA platform that tests generalization, but not SQOOP-style relational reasoning. BID4 studied generalization to rare objects, but did not consider as many models as we do. Our key paradigm is Neural Module Networks (NMN), introduced by BID1, which allows answering longer questions than those in training data. Neural Module Networks (NMN) enable answering longer questions than in training data. BID12 and BID19 developed end-to-end NMNs, requiring thousands of ground-truth layouts for pretraining. BID13 attempted to soften layout decisions but achieved lower performance on the CLEVR task. BID9 successfully induced layouts on CLEVR using a heterogeneous NMN with a scene graph input. Our results support the use of modular neural architectures for language understanding, showing that a modular model with general purpose residual blocks outperforms specialized architectures designed for visual reasoning. The layout of modules in the model significantly impacts its performance, with a tree-like structure showing stronger generalization than a typical chain of layers. This study provides clear empirical evidence in favor of modular and structured networks, highlighting the importance of design in achieving high performance. Our findings suggest that end-to-end and/or soft versions of modular models may not be sufficient for strong generalization. In settings where strong generalization is needed, end-to-end methods often converge to less compositional solutions. This is evident in our experiments on layout and parametrization induction, even with strong initialization sensitivity. Merely replacing hard-coded components with learnable counterparts may not be enough, and research on regularizers or priors to guide learning towards more systematic solutions may be necessary. Our parametrization induction results on the #rhs/lhs=2 split show that a weaker nudge towards systematicity may suffice for end-to-end NMNs. While our investigation was on a synthetic dataset, we believe our findings are most relevant in real-world language understanding. Constructing bias-free synthetic datasets that require understanding the entirety of the language is easier than collecting real-world datasets that do not permit dataset-specific solutions. Approaches that can generalize strongly from imperfect and biased data will likely be required. Our experiments simulate scenarios where strong generalization from imperfect data is needed. Findings aim to inform language understanding researchers on factors facilitating generalization. A correlation between \u03ba and error rate is observed. Hard-coded MAC model variations show similarities to NMN-Tree but perform less successfully. The NMN with the Residual module can answer test questions with a low error rate of 1.64 \u00b1 1.79%. When connected in a tree, modules of spurious models generalize well, but poorly when connected as a chain. The output distribution is a mixture of mostly correct and mostly random predictions, with test accuracies around 99% and 60% respectively. The Residual module with p(tree) \u2248 0.5 has lower confidence in predictions compared to sharp tree models, with a high log loss of 0.27 \u00b1 0.04. The structure induction progress is visualized in FIG4, showing p(tree) saturating to 1.0 for #rhs/lhs=18 and remaining around 0.5 for #rhs/lhs=1."
}