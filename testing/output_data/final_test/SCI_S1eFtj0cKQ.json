{
    "title": "S1eFtj0cKQ",
    "content": "This paper evaluates and compares generative models for Continual Learning on sequential image generation tasks. Various strategies like rehearsal, regularization, generative replay, and fine-tuning are considered. The original GAN performs best, with generative replay outperforming other methods. Training generative models sequentially on CIFAR10 remains a challenge. In Continual Learning, the challenge of retaining previously learned information when data distribution changes is addressed. Generative models in CL scenarios are the focus of this paper, with previous work mainly concentrating on classification tasks. Traditional approaches include regularization, rehearsal, and architectural strategies. Discriminative and generative models differ in their strategies for overcoming catastrophic forgetting. Generative models in Continual Learning differ from discriminative models in architecture and learning objectives. Methods for discriminative models may not directly apply to generative models. Continual Learning strategies for generative models can be used to train discriminative models. A comparative study of generative models with different CL strategies was conducted, sequentially learning generation tasks on MNIST, Fashion MNIST, and CIFAR10 datasets. Generative models in Continual Learning involve training on new classes without forgetting previous tasks. Various models like VAEs, GANs, CVAE, CGAN, WGANs, and WGAN-GP are evaluated on tasks using MNIST, Fashion MNIST, and CIFAR10 datasets. Different CL approaches like finetuning, rehearsal, regularization, and generative replay are compared for maintaining knowledge from previous tasks. In Continual Learning, various generative models are evaluated using different CL approaches to maintain knowledge from previous tasks. Evaluation is done using quantitative metrics and visualization, discussing data availability and scalability of CL strategies. The study compares generative models in a CL setting and highlights success/failure modes of model-CL approach combinations. Related work and the experimental setup are described before presenting results and discussion, concluding the study. In Continual Learning, generative models are evaluated using different approaches to maintain knowledge from previous tasks. The naive method of fine-tuning leads to catastrophic forgetting, while other methods like rehearsal and regularization aim to overcome this issue. Rehearsal involves keeping samples from previous tasks, but scalability and data availability can be limitations. Regularization constrains weight updates to maintain knowledge from previous tasks. Elastic Weight Consolidation (EWC) is a standard method for regularization in Continual Learning, estimating weights' importance to maintain past knowledge. Other methods include distillation, dynamic architecture, and generative replay for preserving knowledge from previous tasks. Generative replay, also known as pseudo-rehearsal, is a method where a generative model produces samples from previous tasks. Continual Learning strategies for discriminative models are not directly applicable to generative models due to different learning objectives and architectures. CL for generative models is less explored compared to discriminative models. Previous work successfully applied Elastic Weight Consolidation (EWC) on the generator of Conditional-GANs (CGANs) to prevent catastrophic forgetting. Generative replay involves a generative model producing samples from previous tasks. Different methods like Variational Continual Learning (VCL) and student-teacher approaches have been experimented on VAEs for Continual Learning. VCL adapts variational inference to a continual setting, while the student-teacher method involves the student learning the current task from the teacher. VASE is another method explored for Continual Learning. Memory replay methods like VASE, VAEs, and Memory Replay GANs have been developed for Continual Learning. These methods allocate spare capacity for new knowledge while protecting previous learning from forgetting. Generative replay methods involve generating samples from past tasks and have been used in both adversarial and variational frameworks. Replay alignment transfers previous knowledge to the current generator, but can lead to mode collapse. Focus is on finding the best generative model and Continual Learning strategy. Empirical evaluation is key to identifying the most suitable combination. Various CL strategies are compared across different generative models to determine the optimal approach. Evaluation metrics play a crucial role in this process. In the context of evaluating generative models in Continual Learning, metrics are crucial. While classification accuracy is commonly used in discriminative settings, for generative models, the Fr\u00e9chet Inception Distance (FID) is a commonly used metric. FID compares the statistics of generated samples to real samples, addressing shortcomings of other metrics like Inception Score (IS). The Fr\u00e9chet Inception Distance (FID) compares the activation distributions of real and generated samples from two multivariate Gaussians. It measures the similarity between the samples and can be adapted with different classifiers for model comparison. Another approach involves training a classifier with labeled generated samples to evaluate the generator's fitting capacity on real data. The Fitting Capacity method evaluates generative models based on complex data characteristics, not just feature distribution. It involves annotating samples using conditional or unconditional models, as well as using an expert network for labeling. In addition to the commonly used FID metric, the Fitting Capacity evaluates samples based on a classification criterion. This approach complements quantitative metrics in evaluating generative models. In evaluating generative models, while quantitative metrics like FID are important, qualitative evaluation through visualizing samples remains informative. However, visual inspection alone may not be sufficient for comparing well-performing models. Experimental setup includes tasks using MNIST, Fashion MNIST, and CI-FAR10 datasets, with evaluations done at the end of each task using 6 different generative models including GANs and VAEs. The Fitting Capacity method is used to evaluate models based on complex data characteristics. In evaluating generative models, qualitative evaluation through visualizing samples is important. Experimental setup includes tasks using MNIST, Fashion MNIST, and CIFAR10 datasets, with evaluations done at the end of each task using different generative models including GANs and VAEs. Strategies for Continual Learning of generative models are compared to baselines, with experiments conducted on multiple seeds and epochs per tasks using Adam for optimization. For Continual Learning strategies, a vanilla Rehearsal method is used where a fixed number of samples per task are added to the training set of the generative model. The dataset is balanced by copying saved samples to ensure each class has the same number of samples. Selecting 10 samples per class is shown to provide satisfactory validation accuracy. Fitting Capacity is compared between original accuracy with 10 samples per task and final capacity, with higher capacity indicating prevention of catastrophic forgetting. Equal Fitting Capacity refers to the balance between overfitting and failure to memorize samples. Different methods like EWC and Generative Replay are used for Continual Learning strategies. Generative replay involves using a previous generative model to sample from learned distributions while a current model learns the current distribution. The evolution of metrics through tasks is shown in the reported figures. The evolution of metrics through tasks is displayed in figures, with a focus on Fitting Capacity results. Generative Replay outperforms other approaches in the adversarial framework, while the Rehearsal approach excels in the variational framework. However, the Rehearsal approach is not suitable for CGAN and WGAN-GP. The Rehearsal approach is effective but inadequate for CGAN and WGAN-GP. Fitting Capacity is lower than a classifier trained on 10 samples per class. EWC struggles with catastrophic forgetting and performs similarly to naive Fine-tuning. Task definition strongly influences results, with EWC showing success with conditional models like CGANs. The Fisher Matrix struggles to understand the influence of the class-index input vector c in non-conditional models, leading to forgetting of previous tasks. This results in the generator only being able to generate samples of one class. Even with multiple classes initially, the Fisher matrix fails to protect the class from the second task in subsequent tasks. Adversarial methods show significant performance differences. Adversarial methods outperform variational methods in continual learning. GANs produce higher quality samples, as seen in FIG7. GAN with Generative Replay is equivalent to a memory of 100 samples per class. Catastrophic forgetting is visualized in Fig.4, showing the performance evolution for each class. VAE decreases performance in some classes with Generative Replay, while GAN does not. In comparison to GANs, VAE models show decreased performance in certain classes, while GANs remain stable. GANs benefit from sample quality and stability, while VAE samples are noted to be more blurry. However, in a Rehearsal setting, VAE models are effective and stable due to their learning objective and probabilistic hidden variables. Fine-tuning and EWC show higher fitting capacity for unconditional models. The capacity of Fine-tuning and EWC in TAB0 is higher than expected for unconditional models. An expert annotation before computing the Fitting Capacity can lead to artificially increased labels variability. The Fitting capacity of conditional generative models is comparable to Continual Learning classification. The best performance is with CGAN: 94.7% on MNIST and 75.44% on Fashion MNIST. In an experiment comparing CL methods on MNIST and Fashion MNIST, Generative Replay and Rehearsal were tested on CIFAR10. WGAN-GP was selected for its satisfying performance in avoiding forgetting of previously seen categories. Results from the experiment on CIFAR10 show that all methods fail to generate images that allow for learning a classifier that performs well on real CIFAR10 test data. Naive Fine-tuning forgets previous tasks, while Rehearsal does not yield satisfactory results. The FID score improves with each new task, but visualization reveals that the generator suffers from mode collapse. Generative Replay fails in sequential task scenarios as the generator suffers from mode collapse, resulting in blurry and indistinguishable images. Training the model separately on each task yields visually satisfactory but quantitatively subpar results. This highlights the challenge of training generative models on sequential tasks. Training generative models on sequential task scenarios does not translate to successful training on all data or categories, with state-of-the-art models struggling on real-life image datasets like CIFAR10. Designing a continual learning strategy for such datasets remains a challenge, with specific characteristics relevant to continual learning strategies discussed. Rehearsal violates data availability assumptions by recording samples, leading to a high risk of overfitting when only a few samples represent a task. EWC and Generative Replay respect this assumption, with EWC not requiring computational overload during training but needing to compute the Fisher information matrix and store its values. The memory cost of EWC for saving past information is twice the model size, which can be expensive compared to rehearsal methods. Rehearsal and Generative Replay provide more samples for learning but increase training costs. A new metric BID32 evaluates CL for generative models, favoring rehearsal methods in experiments. In experiments on Continual Learning (CL) settings, generative models combined with Generative Replay effectively prevent catastrophic forgetting on MNIST and Fashion MNIST datasets. The method uses the generator as a memory to maintain past knowledge. Generative models can learn continually with various methods but do not perform well on CIFAR10 due to generation errors accumulating. Approaches have limitations with discrete task boundaries set by the user. Future work includes investigating automatic task boundary detection and experimenting with smoother transitions between tasks. The implementation used for WGAN-GP samples on CIFAR10 includes classes from 0 to 9 such as planes, cars, birds, cats, deers, dogs, frogs, horses, ships, and trucks. Generation errors increase as tasks progress, leading to blurry images in the final task."
}