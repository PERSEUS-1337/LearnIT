{
    "title": "Hko85plCW",
    "content": "Sequence-to-sequence models with soft attention have been successfully applied to various problems, but their decoding process is inefficient for real-time applications. To address this, Monotonic Chunkwise Attention (MoChA) splits the input sequence into chunks for more efficient computation. Models using MoChA can be trained effectively and enable online, linear-time decoding. In tasks like online speech recognition, MoChA achieves state-of-the-art results and matches offline soft attention models. In document summarization experiments, MoChA outperforms baseline monotonic attention models. Sequence-to-sequence models with soft attention mechanism use encoder and decoder recurrent neural networks to process input sequences, allowing the model to bridge long input-output time lags. The attention mechanism enables the decoder to refer back to the encoder's hidden states, providing valuable insight into the input-output alignment. Soft attention in sequence-to-sequence models allows the model to condition on any input sequence entry, but has a quadratic time and space cost, limiting its use on very long sequences. It also requires waiting for the input sequence to be processed before producing output, making it unsuitable for real-time sequence transduction problems. Monotonic input-output alignment can mitigate these issues. The alignment between input and output sequences should be monotonic to mitigate issues in real-world problems like speech recognition and synthesis. An attention mechanism introduced enforces this alignment for linear-time decoding. MoChA utilizes a hard monotonic attention mechanism to choose the endpoint of the chunk over which it attends, with chunk boundaries and soft attention weighting. The paper introduces \"Monotonic Chunkwise Attention\" (MoChA), a novel attention mechanism that combines the benefits of hard monotonic attention with soft alignments. MoChA allows for soft attention over small memory chunks preceding the hard attention endpoint, improving model performance compared to standard soft attention mechanisms. MoChA is a new attention mechanism that combines hard monotonic attention with soft alignments, improving model performance. It provides a 20% relative improvement over monotonic attention on document summarization tasks. The mechanism can be easily applied to existing sequence-to-sequence models with standard backpropagation, requiring only a modest increase in parameters and computational cost. The MoChA attention mechanism combines hard monotonic attention with soft alignments, improving model performance by 20% on document summarization tasks. It can be easily integrated into existing sequence-to-sequence models with standard backpropagation, requiring only a modest increase in parameters and computational cost. In the sequence-to-sequence framework, the context vector c i is crucial for the decoder to access information from the encoder hidden state sequence h. Using an attention mechanism has become standard practice to compute c i, as simply setting it to the final encoder hidden state led to degraded performance with long sequences. The most commonly used attention mechanism involves producing an unnormalized scalar \"energy\" value for each memory entry at each output timestep. In the sequence-to-sequence framework, the context vector c i is crucial for the decoder to access information from the encoder hidden state sequence h. An attention mechanism is commonly used to compute c i by producing energy values for each memory entry and normalizing them using the softmax function. This allows for a weighted average of h to be computed as the context vector. However, this form of attention is not suitable for real-time sequence transduction as it requires observing the entire input sequence before producing any output. The attention mechanism in sequence-to-sequence models computes context vectors by producing energy values for memory entries and normalizing them with softmax. However, soft attention has issues with time and space complexity. BID25 proposed a hard monotonic attention mechanism where selection probabilities are computed for memory entries starting from the previous attended index. A discrete attend/don't attend decision is made based on these probabilities. The hard monotonic attention mechanism proposed by BID25 involves making a discrete attend/don't attend decision for memory entries based on selection probabilities computed from the previous attended index. This process allows for online sequence transduction and efficient memory usage. The hard monotonic attention mechanism by BID25 involves computing the context vector c i as a weighted sum of memory entries based on selection probabilities. This process enables online sequence transduction and efficient memory utilization. The proposed solution involves using \"soft\" monotonic attention for training and hard monotonic attention at test time. To encourage discreteness, Gaussian noise is added to the logistic sigmoid function's activations to learn effectively binary values. Sampling is avoided at test-time in favor of simple thresholding. The proposed solution involves using \"soft\" monotonic attention for training and hard monotonic attention at test time. A modified energy function was used to address optimization issues when switching from softmax to logistic sigmoid. Hard monotonic attention imposes constraints on the model, allowing only single entry attention and strict monotonic alignment. The proposed solution utilizes soft monotonic attention for training and hard monotonic attention at test time. It involves a modified energy function to address optimization issues when switching from softmax to logistic sigmoid, allowing strict monotonic alignment with single entry attention. The proposed MoChA attention mechanism allows for soft attention over small memory chunks, improving input-output alignment while maintaining linear-time complexity benefits. This addresses performance degradation observed with hard monotonic attention, offering a solution for smoother input-output alignment during training. At test time, MoChA allows for soft attention over memory chunks, enabling nonmonotonic alignments and reordering of memory entries. This increases runtime complexity by a constant factor w but still allows for online decoding with a modest increase in parameters. During training, MoChA uses a decoding algorithm that involves computing the expected value of c i based on its induced probability distribution \u03b2 i,j. This probability distribution is calculated by summing over possible positions where attention could have stopped scanning the memory, and is scaled by the monotonic attention probability \u03b1 i,k. The total number of model parameters only increases by about 1% when adding the second attention energy function ChunkEnergy(\u00b7). During training, MoChA computes the expected value of c i using its probability distribution \u03b2 i,j, which is scaled by the monotonic attention probability \u03b1 i,k. An efficient algorithm is used to compute \u03b2 i,j in parallel for a sequence x = {x 1 , . . . , x T }. This involves convolving x with a sequence of 1s and truncating appropriately. Equations (20) to (23) handle the computation of the attention probability distribution, MoChA's probability distribution, and the expected value of the context vector c i. The novel attention mechanism in MoChA allows for soft attention over memory chunks with adaptive locations, resulting in efficient training and linear-time decoding. MoChA was tested on speech recognition and document summarization tasks, showcasing its effectiveness in inducing monotonic alignment and handling non-monotonic alignments. The limitations of our model were highlighted, emphasizing that only the attention mechanism was changed while keeping all other aspects the same. This approach allowed for isolating the performance difference caused by switching to MoChA. The best-case performance of MoChA may be underestimated due to potential benefits from different hyperparameter settings, which is left for future exploration. In experiments, different hyperparameter settings were tuned for MoChA, with specific values chosen for speech recognition and summarization tasks. Small window sizes were found to significantly improve performance over hard monotonic attention. Metrics were reported on the test set at the training step of best performance on a validation set. The goal of online speech recognition on the Wall Street Journal corpus is to produce the sequence of words spoken in a recorded speech utterance using RNN-based models. The model ingests the spoken utterance as a mel-filterbank spectrogram and consists of convolution layers, convolutional LSTM layers, and unidirectional LSTM layers in the encoder. The decoder, a single unidirectional LSTM, uses either MoChA or a standard soft attention mechanism to produce distributions over character and word-delimiter tokens. Performance is measured in terms of word error rate (WER) after segmenting characters output by the model. The performance of MoChA in online speech recognition on the Wall Street Journal corpus surpassed the state-of-the-art by 20% relative. MoChA-based models showed slightly higher variance across trials compared to soft attention, resulting in a lower best WER but a slightly higher mean WER. This marks the first time an online attention mechanism matched the performance of MoChA. The online attention mechanism MoChA matched the performance of standard soft attention for the first time. Attention alignments for an example from the WSJ validation set showed similar results across all attention mechanisms. MoChA utilized the opportunity to produce a soft attention distribution over each length-2 chunk, with a small value of w = 2 being sufficient for gains. Additional experiments confirmed the benefits of MoChA, with a modest increase in parameter count for the use of a second independent attention energy function. The ChunkEnergy function incurs a modest increase in parameter count, but re-training with different energy functions did not significantly affect performance. MoChA allows for a larger input window for context vectors, showing potential benefits. The study explored increasing the temporal receptive field of the convolutional front-end to improve context vector production. However, the performance difference was not significant. Further experiments demonstrated the advantages of using MoChA for online speech recognition. In a different task of sentence summarization, BID25 achieved slightly lower performance with hard monotonic attention compared to a soft attention baseline. The study focused on document summarization using hard monotonic attention, specifically on the CNN/Daily Mail corpus. The goal was to generate a sequence of \"highlight\" sentences from a news article, using a pointer-generator network as the baseline model. The potential benefits of online and linear-time attention were also highlighted for real-world scenarios with long bodies of text that need to be summarized as they are being created. The model's encoder uses a single bidirectional LSTM layer, while the decoder employs a unidirectional LSTM with an attention mechanism. A copy mechanism is included to interpolate between softmax output and word IDs weighted by attention distribution. Testing with different attention mechanisms showed that hard monotonic attention degraded performance, but MoChA effectively reduced the gap between monotonic and soft attention. The model uses soft attention over adaptively set chunks, avoiding marginalization over chunkwise end-of-sequence tokens. This differs from the Neural Transducer model, which pre-segments input into fixed-sized chunks for sequence-to-sequence transduction. The model uses soft attention over adaptively set chunks, while BID7 uses a large chunk size of 150, making it unsuitable for online settings. Other non-attentive sequence transduction models include connectionist temporal classification, RNN transducer, segment-to-segment neural transduction, and segmental RNN. These models do not directly condition the decoder on the input sequence. The decoder does not directly condition on the input sequence and decoding is done via a dynamic program. Attention-based models perform best in speech recognition experiments. Some works have considered hard monotonic alignments using reinforcement learning or separately-computed target alignments. MoChA is proposed as an attention mechanism. MoChA is an attention mechanism that performs soft attention over adaptively located chunks of the input sequence. It allows for online and linear-time decoding, facilitating local input-output reorderings. MoChA achieved state-of-the-art performance in online speech recognition and outperformed a hard monotonic attention-based model in document summarization. Future work includes applying MoChA to problems with monotonic alignments like speech synthesis and morphological inflection, and exploring adaptive variations in chunk size. An example implementation of MoChA online is provided for further research. In section 3, experiments were conducted using TensorFlow for online speech recognition. Speech utterances were represented as mel-scaled spectrograms with 80 coefficients, delta, and delta-delta coefficients. The model included convolutional and LSTM layers, followed by additional LSTM layers with a hidden state size of 256. The model used for online speech recognition included a decoder with a 256-dimensional output, batch normalization, and ReLU nonlinearity. The attention mechanism had a hidden dimensionality of 128, and the network was trained using the Adam optimizer with a specific learning rate schedule. The model for online speech recognition included a decoder with a 256-dimensional output, batch normalization, and ReLU nonlinearity. Inputs were fed into the network in batches of 8 utterances using standard teacher forcing. Various techniques such as localized label smoothing, gradient clipping, variational weight noise, and L2 weight decay were applied during training. At test time, a beam search with rank pruning was used. Pointer-generator of BID26 was re-implemented for summarization using one-hot vectors. The model for online speech recognition included a decoder with a 256-dimensional output, batch normalization, and ReLU nonlinearity. Inputs were provided as one-hot vectors representing ID in a 50,000 word vocabulary, mapped to a 512-dimensional learned embedding. The encoder had a single bidirectional LSTM layer with 512 hidden units, and the decoder had a single unidirectional LSTM layer with 1024 hidden units. Attention mechanisms had a hidden dimensionality of 1024. Output words were embedded into a learned 1024-dimensional embedding and concatenated with the context vector before being fed back into the decoder. Training used the Adam optimizer with specific parameters and a batch size of 64. Input sequences were truncated to a maximum length of 400 words. During evaluation, a beam search with rank pruning was used, and a synthetic benchmark was conducted to compare the speed of MoChA versus standard soft attention. The speedup observed can be considered an upper bound due to other network components potentially dominating computational costs. In a synthetic benchmark comparing MoChA and soft attention, speedup was observed with MoChA having linear time complexity and increasing complexity with parameter w. Soft attention showed quadratic time complexity, resulting in a larger speedup as input and output sequence lengths increased. In a synthetic benchmark, MoChA showed linear time complexity and increased complexity with parameter w. For T, U = 10 and w = 8, MoChA and soft attention had similar speeds when the chunk spanned the entire memory. Speedups from MoChA are most significant for large T and U with relatively small w. The MAtChA approach, Monotonic Adaptive Chunkwise Attention, was introduced as an alternative to fixed chunk sizes in memory regions. However, it did not outperform MoChA in various tasks despite increased memory and computational requirements. The test-time decoding process of MAtChA is similar to MoChA, with a slight variation in setting the chunk start location. The MAtChA approach assigns attention to memory entries based on specific equations involving probabilities and previous timesteps. It differs from MoChA in chunk sizes and decoding processes. The MAtChA approach utilizes a monotonic attention mechanism to compute the context vector c i for training models with backpropagation. The attention distribution is computed in parallel, but the quadratic number of terms needed for each output poses a challenge. The MAtChA algorithm requires a quadratic number of terms for each output timestep/memory entry combination, leading to high memory usage during decoding. Despite hopes for superior performance, MAtChA did not outperform MoChA in experiments. Therefore, it is not recommended for use in its current form, but future work may explore combining MoChA and MAtChA for potential benefits."
}