{
    "title": "rkxmPgrKwB",
    "content": "Neural network training relies on the loss landscape's structure, including local minima, saddle points, flat plateaus, and loss barriers. The permutation symmetry of neurons in each layer of a deep neural network leads to multiple equivalent global minima and critical points between them. Paths between global minima pass through 'permutation points' where weight vectors of two neurons collide and interchange, contributing to the landscape's flatness. In neural network training, the permutation symmetry of neurons creates flat plateaus at permutation points, allowing all permutations of neurons in a layer to have the same loss value. Higher-order permutation points exist in the loss landscapes, with a much larger number than equivalent global minima. Numerical demonstrations show continuous paths between minima in different network configurations. This geometric approach provides insights into weight-space symmetries and critical points. The loss landscape in neural network optimization is influenced by symmetries, with numerous studies exploring its properties both numerically and theoretically. Permutation symmetries in multilayer networks lead to equivalent configurations, impacting the structure of the loss landscape. Global minima in weight space must adhere to these symmetries, creating a unique landscape for optimization. Weight-space symmetry in neural networks leads to multiple equivalent partner minima, slowing down training dynamics near singular regions. Optimization paths may approach these regions, affecting training for deep neural networks. This property provides insights into observations on neural network landscapes. Observations on neural network landscapes reveal the presence of weight-space symmetries affecting training dynamics. The Hessian of the loss function shows flat regions with almost-zero eigenvalues. Exponentially growing saddles and numerous connected high-dimensional plateaus are also identified in the landscape. The existence of multiple saddles compared to global minima is proven due to weight-space symmetries in neural networks. Additionally, a novel low-loss path finding approach is proposed. The text discusses weight-space symmetries in neural networks and proposes a low-loss path finding algorithm to find barriers between partner minima. It explores continuous paths connecting equivalent global minima by merging weight vectors of neurons in a specific way, allowing for permutations of neuron indices at no extra cost. Constant-loss permutations are possible due to high-dimensional plateaus in the landscape. Permutations of neuron indices are possible in neural networks due to high-dimensional plateaus of critical points. The theory extends to higher-order saddles and provides lower bounds for the number of permutation points. A path-finding algorithm connects global minima via a permutation point, with numerical confirmation of first-order permutation saddles. The work also characterizes permutation points as critical points connected by paths at equal loss, and demonstrates the method in multilayer neural networks trained on MNIST. The landscape structure of multilayer neural networks trained on MNIST is studied, showing that critical points are mostly saddles except for the global minimum. Deep linear networks exhibit sharp transitions at plateaus edges, similar to deep nonlinear networks. Research suggests that local minima in nonlinear networks lie below a certain loss value. Recent studies prove that almost all local minima are global minima for multilayer networks under certain assumptions. Recent research focuses on the bottom of the landscape in neural networks, studying global minima and low-loss barriers between them. Studies have shown the existence of low-loss paths connecting global minima in wide two-layer networks. Different methods have been used to connect independent minima and find that the barrier diminishes with increasing width and depth in various network architectures. Training dynamics in the landscape have also been explored, showing that gradient descent can navigate the landscape effectively. For general loss functions, gradient descent converges to local minima if saddles have negative eigenvalues. Overparametrized neural networks converge to global minima near initialization, suggesting convex-like behavior. Training dynamics for finite size networks and soft-committee machines remain open questions. Initial learning dynamics in soft-committee machines are slowed by hidden neuron correlation. Training dynamics slow near singular regions due to weight-space symmetry. Saddle points in the landscape are numerous. In this study, the authors demonstrate the presence of a significant number of permutation points in deep neural networks, which are critical points with multiple flat directions. They show that these points may be linked to the observed plateaus during training. Unlike previous research on shallow networks, they investigate deep networks with multiple hidden layers and identify multi-dimensional equal-loss plateaus. Additionally, they establish a lower bound on the number of permutation points and develop a method to construct paths between global minima. The study explores permutation points in deep neural networks, utilizing a new algorithm to construct paths between global minima. It focuses on multilayer neural networks with nonlinear activation functions and defines permutation sets based on neuron permutations within layers. The study investigates permutation points in deep neural networks, using a novel algorithm to connect global minima. It centers on multilayer neural networks with nonlinear activation functions and defines permutation sets based on neuron permutations within layers. In this section, a novel method is presented to find a low-loss path between partner minima in deep neural networks. The method involves duplicating a neuron in a layer and studying permutation points to connect global minima. The goal is to find continuous low-loss paths between minima in the network landscape. The method involves finding a low-loss path between partner minima in deep neural networks by merging parameter vectors and output weight vectors at a 'permutation point' and completing the path using symmetry. The path is defined by a distance function and constraints on parameter configurations at different stages. The method involves moving from a quarter-way configuration to a configuration at half-way where output weights of related neurons are equal, without changing network output or loss. This is achieved by adjusting output weights continuously until they are equal for all neurons at a certain layer. The configuration at half-way is a permutation point, and the path continues by interchanging neuron indices and walking backwards until reaching the partner minimum. This path is defined algorithmically using reparametrization and unit-length vectors. The method involves adjusting parameters and performing gradient descent until convergence, with all parameters including \u03d1 and e changing. The process is repeated until reaching a specific point where output weights are equal without altering the network function. The path between two minima may contain a saddle point, potentially at the permutation point. Previous research has explored critical points in neural network landscapes induced by hierarchical structures. In two-layer neural networks, hierarchical structures induce critical points in the landscape of the loss function. Permutation points in neural networks with multiple outputs and layers are studied, showing that they lie in equal-loss subspaces of critical points. Continuous transformations can be performed between neuron indices in a layer starting from permutation points. The relevant parametrization of the path in neural networks is the distance d. Theorem 1 shows that setting derivatives to zero at critical points leads to zero derivatives in corresponding configurations. Different loss outcomes can occur at permutation points depending on the pair of neurons chosen for merging. The quadratic loss function decreases logarithmically as the distance between neurons to be merged decreases. The distance between neurons to be merged was decreased in logarithmically spaced steps to 1/10 of the initial value. Training data was generated by sampling input points from a normal distribution and computing labels using a teacher network with hidden neurons and an output layer. Plateaus of different dimensions enable the exchange of indices in the network. Multiple plateaus on different loss levels correspond to different local minima. The neurons are dropped at a permutation point, allowing for the exchange of indices in the hidden layer. Different plateaus with varying loss levels enable this permutation. The lowest-cost permutation can connect all global minima caused by arbitrary permutations of neurons in layer k. Higher-order permutation points involve mapping a minimum of a multilayer network to a configuration with additional neurons per layer by replicating some neurons. Higher-order permutation points involve replicating neurons in a multilayer network to create a new configuration where parameter vectors and output weight vectors are the same for certain neurons in a layer. This allows for a generalization of Proposition 1 to K-th order permutation points. The number of permutation points reducing to the same configuration can be counted combinatorially. The text discusses the counting of permutation points in neural networks, considering the replication of parameter vectors and neurons. It provides a lower bound on critical points with higher loss values than global minima. The ratio of K-th order permutation points to global minima is analyzed for different neuron counts per layer. The number of permutation points is shown to be significantly higher than global minima under certain conditions. The text discusses the counting of permutation points in neural networks, showing that every permutation point lies within a distinct but connected subspace of critical points. It also mentions the existence of high-dimensional equal-loss subspaces of critical points in neural networks with different neuron counts per layer. Neural network landscapes exhibit numerous high-dimensional plateaus of critical points and non-critical points at various loss values. Paths between global minima were constructed in a fully connected three-layer network using a student-teacher setting. The study involved adjusting parameters and decreasing distance between neuron parameter vectors in a neural network. The student was trained on a regression task with mean-squared error loss until convergence. The loss increased monotonically towards a permutation point, indicating it is a saddle point, not a minimum. The study observed that the permutation point in the loss landscape of a multi-layer network is a saddle, not a minimum. The barrier height decreased with the number of hidden neurons per layer, as shown by theoretical and empirical results. The experiment involved merging parameter vectors of neurons in a student network trained on MNIST with a teacher network. The mean squared loss between teacher and student output increased monotonically until the permutation point, which represented the loss barrier. In this study, the loss landscape of neural networks was explored, revealing that permutation points are critical points connected by equal-loss paths. Weight-space symmetry induces saddles and plateaus in the landscape, with the barrier height decreasing with the number of hidden neurons. The MNIST classification accuracy remains stable at permutation points, highlighting the unique structure of the loss landscape. The study explored the loss landscape of neural networks, revealing the importance of permutation points connected by low-loss paths. Empirical validation on a multilayer network trained on MNIST confirmed the reachability of permutation points. The loss at permutation points decreased with network size, aligning with previous findings. The algorithm for finding low-loss paths through permutation points was presented. The study focused on the importance of low-loss paths through permutation points in neural networks. By merging neurons and minimizing gradients, a local minimum at permutation points is achieved. The path-finding problem is reformulated using Lagrange multipliers to minimize distance while keeping loss minimal. This approach aims to approximate the ideal path through a sequence of configurations. The study emphasizes the significance of low-loss paths through permutation points in neural networks. By merging neurons and minimizing gradients, a local minimum at permutation points is attained. The path-finding problem is redefined using Lagrange multipliers to minimize distance while maintaining minimal loss, approximating the ideal path through a sequence of configurations. The Lagrangian formulation shows that the half-way configuration is a critical point, with both L(\u03b8) and \u03bbd l equaling zero at this point. The derivative with respect to an output in layer k + 1 remains the same after merging neurons in layer k and scaling their output weight vectors. The proposition extends Theorem 1 in (Fukumizu and Amari, 2000) for multiple output and multiple-layer neural networks. Neurons with identical parameter vectors in a layer implement the same function. Changing output weights defines a hyperplane of critical points in the next layer. Continuous transformations at fixed loss lead to decreasing output weights of a neuron to zero. The procedure allows for exchanging neurons in a neural network by adjusting output and input weights while maintaining a constant sum of weights. This enables smooth transformations between neurons in the same layer, ultimately sharing output weights between neurons. The procedure enables exchanging neurons in a neural network by adjusting weights while maintaining a constant sum. This allows smooth transformations between neurons in the same layer, sharing output weights. The proof involves constructing permutations starting at points where parameter vectors merge, resulting in different losses for each permutation. The procedure involves rescaling output weights and mapping to critical points in the neural network landscape. Parameter vectors are denoted for neural networks with varying numbers of neurons per layer. An unordered partition of neurons is considered, allowing for smooth transformations between neurons while maintaining constant sums. The procedure involves rescaling output weights and mapping to critical points in the neural network landscape. For neural networks with varying numbers of neurons per layer, an unordered partition of neurons allows for smooth transformations while maintaining constant sums. The outgoing weights of replicated neurons are the only free variables, constrained by equations defining dimensional hyperplanes. Intersecting these hyperplanes results in an equal-loss hyperplane. There are multiple 2nd order permutation points at layer k, with three ways to have distinct vectors out of n k in the case K = 3. The procedure involves rescaling output weights and mapping to critical points in the neural network landscape. For neural networks with varying numbers of neurons per layer, an unordered partition of neurons allows for smooth transformations while maintaining constant sums. The outgoing weights of replicated neurons are the only free variables, constrained by equations defining dimensional hyperplanes. Intersecting these hyperplanes results in an equal-loss hyperplane. Multiple 3rd order permutation points at layer k are considered, with different ways to replicate parameter vectors. The lower bound for general K is determined by partitioning n k into l positive integers without order, resulting in duplicated parameter vectors and unique parameter vectors. This leads to a lower bound of T(K, n k) due to the permutations in different layers. The factorial of an integer n can be approximated using Stirling's formula, which is accurate even for small n. As n approaches infinity, Stirling's formula can be applied to n-K and n-2K. The number of equivalent K-th order permutation points is at least nk! and each permutation point corresponds to a distinct high-dimensional subspace of critical points in the neural network landscape. The number of distinct subspaces in layer k is equivalent to the number of related permutation points."
}