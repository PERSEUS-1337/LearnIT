{
    "title": "S1l_ZlrFvS",
    "content": "In this paper, a new explanation approach is introduced for image similarity models, focusing on models that measure the similarity of two inputs rather than classification. The proposed method combines a saliency map with an attribute to explain the match, providing additional information not captured by saliency maps alone. The approach shows improved performance on attribute recognition and generalizes well across diverse datasets like Polyvore Outfits and Animals with Attributes. The curr_chunk discusses the application of feature embedding to measure similarity between images and text in artificial intelligence tasks like fashion compatibility scoring, image retrieval, and zero-shot recognition. Understanding similarity models can help improve anomaly detection, promote diversity in fashion recommendations, and enhance user trust in model predictions. The curr_chunk discusses explaining similarity models using saliency maps, which differ from classification models. Existing methods focus on explaining classification models, but for similarity models, the interaction between images determines important features. Saliency alone may not fully explain (dis)similarity. The curr_chunk discusses the importance of considering attributes and salient parts in explaining similarity models, rather than relying solely on saliency maps. The key factor in determining the explanation is the images themselves, not the image domain. The curr_chunk introduces Salient Attributes for Network Explanation (SANE), a method that generates a saliency map and attribute explanation to explain a model's similarity score. Unlike existing methods focusing on image classification, SANE explores explanations for image similarity models, providing a more human-interpretable explanation than saliency alone. It can explain any network architecture by measuring changes to a similarity score with different inputs. SANE predicts important attributes for similarity scores in image pairs using an attribute predictor, a prior on attribute suitability, and a saliency map generator. Different similarity models may highlight different attributes for the same image pair. The method aims to explain similarity scores by identifying key attributes in each image. The SANE method uses attribute predictor and saliency map generator to identify key attributes for similarity scores in image pairs. It encourages overlap between similarity saliency and attribute activation during training, resulting in improved attribute explanations and recognition performance. The curr_chunk discusses evaluating saliency map generation methods for image analysis, focusing on approaches that do not rely on specific model architectures. These methods identify important regions by perturbing input images and measuring changes in output class scores. Additionally, similarity models use a learned embedding space to analyze relationships between images based on proximity. The explanation system for embedding models must consider how distances between embedded points change when perturbing input images. The curr_chunk discusses strategies for adapting saliency methods for image analysis, focusing on manipulating single or both input images to explain model behavior. The paper introduces a novel explanation approach combining saliency maps and attributes, validated with metrics linking explanations to model performance. It improves attribute recognition and provides more informative explanations compared to prior work. Saliency methods are categorized into \"white box\" and \"black box\" approaches, with the former assuming access to internal neural network components. During training, saliency maps are matched with ground truth attribute activation maps, and at test time, attribute explanations are ranked based on their alignment with saliency maps. The image similarity model is assumed to be pretrained and fixed in all experiments. Most methods generate saliency maps using backpropagation. In contrast to \"white box\" approaches, \"black box\" methods for saliency map generation do not require knowledge of internal model details. These methods perturb the input image in a predefined way to measure its effect on the model output. Three \"black box\" and one \"white box\" methods were adapted and compared for saliency map generation. \"Black box\" approaches include Sliding Window and Randomized Input Sampling for Explanations (RISE), which mask image regions to measure their impact on the class score. LIME first obtains a super-pixel representation of an image and estimates the importance of randomly deleted super-pixel regions using Lasso. Researchers have explored methods of producing text-based explanations instead of saliency maps, including justifying a model's answer in visual question answering tasks, rationalizing self-driving vehicle behavior, and describing category selection. These methods provide qualitative results but lack quantitative evaluation on explanation accuracy. Several works exist which learn attribute explanations to identify important concepts or justify a model's decision by pointing to evidence. Kim et al. (2018a) learns a concept activation vector that separates examples with an attribute against examples without it, scoring the sensitivity of attributes based on directional derivative changes. These methods explain categorical predictions rather than similarity. Our method is the first to use attribute explanations in the image similarity setting. It consists of three components: the attribute explanation model, the saliency map generator, and an attribute explanation suitability prior. We train a CNN to produce attribute predictions, while keeping the image similarity model fixed. At test time, a saliency map is recovered for the match from the query image in a pair. The method involves using attribute explanations to rank each attribute's ability to explain image similarity in pairs of images. Saliency maps are generated for query and reference images, with binary attribute annotations used to determine ground truth attributes. If no annotations are available, an attribute discovery method can be used. The method involves using attribute explanations to identify which attributes contribute the most to image similarity scores. Attribute activation maps are created to highlight prominent regions in images, with a focus on matching ground truth attributes' activation maps with saliency maps for regularization. The method uses attribute activation maps to match ground truth attributes with saliency maps for regularization. Attribute confidence scores are obtained through a global average pooling layer and softmax, with training using a Huber loss to encourage sparsity in predictions. The loss function operates on attributes, not attribute activation maps, and scales a binary label vector by the number of ground truth attributes present in the image. The method utilizes saliency maps during training to identify attributes that explain image similarity predictions. Saliency maps are matched with attribute activation maps for regularization, focusing on regions important for similarity predictions. This approach aims to find attributes that best describe regions of high importance in similarity predictions. The method uses saliency maps to identify relevant attributes for explaining image similarity predictions. A loss function is defined to compare attribute activation maps with saliency maps, along with a classification loss. Saliency maps are generated by manipulating input images to measure their impact on similarity scores, helping determine the significance of image regions. The challenge is determining the best way of manipulating the input image to discover important regions. Four saliency methods are compared: Sliding Window, LIME, \"white box\" Mask, and RISE. These methods measure the effect of manipulating the image on the prediction of a specific object class. In the context of comparing saliency methods for image manipulation, the focus is on determining the most effective approach for identifying important regions in images. The comparison involves different methods such as Sliding Window, LIME, \"white box\" Mask, and RISE, which assess the impact of image manipulation on predicting object classes. However, when comparing two images, the approach extends to considering multiple reference images. The goal is to measure the effect of manipulating images on the similarity score between a query image and a reference image, with the option to manipulate both images or just the query image. Manipulating both images requires multiple forward passes through the image similarity model, making it costly unless the number of reference image manipulations is significantly less than query image manipulations. In experiments, setting the number of reference image manipulations much lower than query image manipulations is sufficient for obtaining an accurate saliency map for the query image. In comparing saliency methods for image manipulation, the focus is on identifying important regions in images. The approach involves evaluating different methods like Sliding Window, LIME, \"white box\" Mask, and RISE to assess the impact of image manipulation on predicting object classes. When comparing two images, the goal is to measure the effect of manipulating images on similarity scores. One method involves manipulating the query image alone while keeping a fixed reference image. The saliency maps produced by these methods are evaluated, and the usefulness of attributes in explaining predictions made by a similarity model is taken into account. The usefulness of attributes in explaining predictions made by a similarity model is determined by learning concept activation vectors (CAVs) over the image similarity embedding. CAVs identify which attributes positively affect the model's predictions, and the sensitivity of each concept to the model's predictions is measured by the TCAV score. This creates a single attribute ranking over the entire image similarity embedding. The study evaluates attribute explanations using a weighted combination of TCAV scores, attribute confidence, and similarity between attribute activation and saliency maps. Two datasets are used to demonstrate the approach's generalizability: Polyvore Outfits with fashion product images and Animals with Attributes 2 with natural images of animal classes. The study evaluates attribute explanations using TCAV scores, attribute confidence, and similarity between attribute activation and saliency maps. Animal classes annotated with 85 attributes are split into 40 for training and 10 for testing. Image similarity models are used for the datasets, including a type-aware embedding model for Polyvore Outfits and a ResNet model with triplet loss for AwA. Cosine similarity is used to compare image pairs' features. The study evaluates saliency map generation methods by comparing image feature representations using cosine similarity. Saliency maps are evaluated using insertion and deletion metrics to measure performance changes as pixels are added or removed. Results are compared using area under the curve (AUC) for both metrics. Results show that RISE performed best on most metrics for saliency map generation methods, except for LIME which did better on the deletion metric for AwA. LIME learns which super-pixels contribute to a similarity score, allowing for segmentation of animal parts. Mask, although not as effective, produces compact regions of salient pixels. Better performance was generally achieved when the reference image was fixed and only the query image was manipulated. The study highlights the importance of careful manipulation of both query and reference images due to noisy similarity scores. SANE's performance was evaluated using mean average precision (mAP) for attribute recognition. Additional metrics were used to assess attribute explanations using (query, reference) image pairs, similar to saliency map evaluation. Attributes were selected to explain similarity scores, and the impact of deleting these attributes on the score was measured. The text discusses measuring the effect of inserting or removing attributes on similarity scores. It explains how scores are calculated and normalized, using representative images to compare attributes. For example, to remove the attribute \"striped,\" a similar non-striped image is found. The text compares attribute recognition models on the Polyvore Outfits dataset, focusing on similarity scores between images with and without specific attributes. Three baseline approaches are provided, including a random baseline and a modified version of FashionSearchNet. Performance is evaluated in Table 2. The appendix provides details on the models. Results in Table 2 show that the model combining saliency maps and TCAV scores outperforms other models in attribute explanation metrics. It demonstrates that altering attributes predicted by SANE affects similarity scores significantly. TCAV performs well in insertion but poorly in deletion tasks. Other models, like SANE, are trained to reason about existing attributes in images, unlike TCAV. Using a bias towards globally informative attributes (TCAV scores) is more useful for insertion tasks. Training SANE to produce explanations leads to a 2% improvement on the attribute recognition task. SANE outperforms FashionSearchNet in localizing important image regions. A user study showed that 56% of subjects found the explanations helpful in understanding the model's behavior. Most users found the saliency maps and attributes useful on the AwA dataset and Polyvore Outfits, with 87% and 79% respectively. Qualitative examples in Figure 3 show that explanations pass important sanity checks, with attributes well-correlated with important pixels in the saliency map. The different explanations are ideal in different circumstances, helping to build trust in the model's behavior. The explanation model utilizes information from image pairs and saliency maps to provide sensible explanations. Removing the predicted attribute can significantly affect similarity scores, demonstrating the causal role of predicted attributes in similarity. In this paper, SANE is introduced as a method to explain an image similarity model's behavior by identifying important attributes and using saliency maps. Human confirmation of the usefulness of these explanations could help build trust in the model. Integrating the saliency generator and attribute explanation model closely is suggested for improved performance. Additional details on candidate saliency map generation methods are provided, categorized into input manipulation and optimization-based approaches. The paper introduces SANE to explain image similarity models by identifying important attributes using saliency maps. Different methods for generating saliency maps are discussed, including input manipulation and optimization-based approaches. A comparison of these methods and their runtime is provided in the paper. The paper introduces SANE to explain image similarity models by identifying important attributes using saliency maps. Different methods for generating saliency maps are discussed, including input manipulation and optimization-based approaches. A sliding window approach and a Monte Carlo method are used to manipulate the input image and generate saliency maps. Saliency maps are generated using random binary masks that are upsampled and cropped to manipulate input images. This method outperforms using a sliding window approach. The paper also discusses combining input manipulation with an optimization procedure to learn saliency maps directly. In Section A.1, saliency maps are generated by manipulating images through superpixel segmentation and perturbation operators. LIME uses random deletion of superpixels and Lasso for importance estimation, while Mask learns a low-resolution saliency map with Gaussian noise and image blurring. The perturbation operators defined include adding Gaussian noise and image blurring. A total-variation norm and L1 regularization are used to promote sparsity and avoid artifacts when learning the mask. This approach removes the reliance on superpixels and converges faster than LIME, although it is slower in practice. One advantage is the ability to learn the salience map for both query and reference images jointly. Sliding Window technique uses occlusion windows, while RISE randomly samples 2,000 masks for datasets. In addition to perturbation operators like Gaussian noise and image blurring, a total-variation norm and L1 regularization are used for sparsity. The approach removes reliance on superpixels and converges faster than LIME. It involves learning salience maps for query and reference images jointly. The method uses a sliding window technique with occlusion windows and randomly samples 2,000 masks for datasets. The approach involves learning salience maps for query and reference images jointly using a 50-layer ResNet base image encoder. The model is trained for attribute classification and image retrieval, with a fixed-reference RISE selected as the saliency map generator. Training is done for 300 epochs using Adam with a learning rate of 5e \u22124. After training the attribute model for 300 epochs using Adam with a learning rate of 5e \u22124, the best performing model was selected based on mAP on the validation set. Qualitative examples from the SANE model on Polyvore Outfits and AwA datasets are shown in Figures 7 and 8. An example of attribute deletion process is illustrated in Figure 9, where the most similar image without the input attribute is searched for in a database. Image similarity is computed over the attribute space to maintain predictions while varying the target attribute. For Polyvore Outfits, only images of the same type are considered to avoid bias towards a single attribute model. In Figure 9, the attribute deletion process is demonstrated to evaluate the impact of removing attributes on images. The predictions from different attribute models are averaged to avoid bias. The images show how certain attributes can significantly alter the appearance of images, while others have minimal effect. The process helps determine the importance of attributes as explanations in heldout data. The likelihood of each attribute in the AwA dataset being identified as the best attribute for an image pair on held-out data is shown in Figure 10. The ground truth bias for the attribute detection task is demonstrated according to metrics for the dataset. This prior could change for a different image similarity model, depending on its bias towards certain attributes like colors. The method proposed involves discovering attributes useful for model explanations by analyzing saliency maps of similar reference images to a query image. This is done by obtaining similar images using k-NN, generating saliency maps for each reference image, and identifying common saliency peaks as potential shared attributes. The method involves analyzing saliency maps of reference images to a query image to discover attributes. It selects top N reference images with peaks in a common location, generates saliency maps, crops patches around peak regions, upsamples patches, and clusters them to identify attributes. Evaluation compares this approach to random clustering and clustering based on embeddings. The saliency-based attribute discovery method outperforms unsupervised methods for Polyvore Outfits data but is surpassed by full-frame clustering for the AwA dataset. Full-frame clustering considers the background more, leading to better results for AwA. However, discovered attributes may be noisier due to the focus on background patches. Further investigation is needed to improve attribute discovery for explanations."
}