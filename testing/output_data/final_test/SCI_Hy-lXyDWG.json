{
    "title": "Hy-lXyDWG",
    "content": "Deep convolutional neural network (DCNN) based supervised learning is commonly used for large-scale image classification. Retraining these networks for new data is time and energy-intensive. A proposed methodology involves incrementally growing a DCNN to learn new classes while sharing part of the base network, inspired by transfer learning techniques. The updated network uses previously learned convolutional layers and adds new convolutional kernels for improved classification accuracy in recognition applications. The proposed scheme for incremental learning in recognition applications achieves comparable classification accuracy to regular incremental learning approaches. Deep Convolutional Neural Networks (DCNNs) have shown human-like performance in various cognitive applications, but face challenges with computational complexity and training time. Training DCNNs requires state-of-the-art accelerators like GPUs, limiting their usage to clouds and servers. Pre-training on large datasets like ImageNet is common practice. Incremental learning is crucial for Deep Convolutional Neural Networks (DCNNs) to adapt to new data continuously. Retraining large networks with both old and new data is not practical, so incremental learning updates the model with new data batches to ensure continuous learning. This approach overcomes the static nature of DCNNs trained on large datasets like ImageNet, allowing for flexibility in real-world scenarios. This paper focuses on incremental learning of deep convolutional neural network (DCNN) for image classification task, addressing the challenges of efficiently incorporating new knowledge without starting from scratch and increasing network capacity in an efficient manner. Prior works on incremental learning of neural networks have focused on learning new classes from fewer samples using transfer learning techniques. A Bayesian transfer learning method was proposed to avoid learning new categories from scratch, achieving zero-shot learning. The challenge lies in applying incremental learning on DCNN, which consists of both feature extractor and classifier in one architecture. Ensemble of classifiers was utilized by generating multiple hypotheses using carefully tailored distributions. BID8 utilized ensemble of modified convolutional neural networks as classifiers by generating multiple hypotheses. BID13 and BID8 improved existing classifiers by combining new hypotheses from newly available examples without compromising classification performance on old data. BID19 proposed a training algorithm that grows a network incrementally and hierarchically, grouping classes according to similarities and self-organizing them into different levels of hierarchy. New networks are cloned from existing ones, inherit learned features, fully retrained, and connected to the base network. The challenge with this method is the increase in hierarchical levels as new classes are added. Our work aims to grow a DCNN to accommodate new classes by network sharing, without forgetting old classes. We reuse previously learned convolutional layers as fixed feature extractors and add new trainable convolutional layers for learning new classes. The network's error resilience allows it to adapt to frozen parameters. In this work, Gabor filters are used as convolutional kernels along with regular trainable kernels to smoothly learn new classes without convergence issues. Sharing initial convolutional layers helps reduce training energy and maintain output accuracy. The study also quantifies energy consumption, training time, and memory storage savings associated with different amounts of network sharing. The study focuses on the importance of network sharing from a hardware perspective. Key contributions include proposing sharing of convolutional layers to reduce computational complexity, developing a methodology for optimal sharing to balance accuracy and energy consumption, creating an energy model to quantify energy usage during training, and demonstrating the efficiency of the methodology across different network structures and datasets. Incremental learning is a continuous process of learning new information gradually, also known as lifelong learning or evolutionary learning. It aims to mimic the ability of biological brains to learn new events without forgetting old ones. However, exact sequential learning may not work well in artificial neural networks due to fixed architectures and training algorithms. Incremental learning aims to address the stability-plasticity dilemma in neural networks by developing an algorithm that can grow the network, accommodate new classes with minimal overhead, and preserve previously acquired knowledge without suffering from catastrophic forgetting. The proposed incremental learning model aims to preserve previously acquired knowledge without catastrophic forgetting. The methodology involves sharing convolutional layers as fixed feature extractors to learn new classes using only newly available data. Increasing network capacity is achieved by widening the final softmax layer. Increasing network capacity by widening the final softmax layer to accommodate new classes may result in a small increment in learning capacity. This approach involves training only a small percentage of trainable parameters with new examples to prevent forgetting previously learned classes, but may not yield good inference accuracy for the new classes. To investigate the idea of widening the network for better learning capacity, a CNN with multiple convolutional layers was trained for 10 classes and then incrementally added 26 more classes. Each time new classes were added, two feature maps were included in the last convolutional layer to enhance performance. The study focused on incrementing and retraining the last few layers of a CNN to prevent overfitting when adding new classes. Specifically, 2 feature maps were added in the last convolutional layer for each addition of 5-6 classes. The new parameters were initialized using random numbers similar to the learned weights, and only the 8 kernels corresponding to the new feature maps and connections to the new output neurons were trained with the new examples. The rest of the parameters were shared with the base network to retain previously learned classes. The study focused on incrementing and retraining the last few layers of a CNN to prevent overfitting when adding new classes. The new network becomes the base network for the next 5 classes to be added, with only the 8 kernels corresponding to the new feature maps and connections to the new output neurons being trained with the new examples. The accuracy degradation due to 'partial network sharing' is negligible compared to the network 'without partial sharing'. The study addressed scalability issues in adding new classes to a CNN by retraining the final convolutional layer. This approach aims to prevent overfitting and convergence issues when learning new classes. The methodology involves cloning and retraining the final convolutional layer for each new set of classes, while sharing initial convolutional layers with the base network. The study addressed scalability issues in adding new classes to a CNN by retraining the final convolutional layer. A new set of classes shares the initial convolutional layers with the base network, with a separate final convolutional layer and subsequent layers. Cloning the final layers prevents initialization issues and maximizes feature transfer. A deep CNN was implemented for the CIFAR10 dataset using MatConvNet, containing three MlpConv blocks. The study implemented a deep CNN for the CIFAR10 dataset using MatConvNet BID18. The network had three MlpConv BID7 blocks, each with a convolutional layer, two fully connected layers, and a max-pooling layer. The base network was trained for 4 classes, then 6 more classes were added in two sets of 3. The last MlpConv BID7 block was cloned and retrained for the new classes, while the initial two MlpConv blocks were shared from the base network to minimize learning overhead. The accuracy degradation due to partial network sharing was negligible when training for additional class sets. When training for additional class sets, the accuracy of the updated network of 10 classes suffers a 1.8% degradation. Freezing parameters in a pre-trained convolutional network is a common practice for knowledge transfer, but previous methods only allowed classification of new datasets. Our proposed methodology allows for classification of both old and new learned classes together, a novel approach not explored before. In a large DCNN with many convolutional layers, retraining the final convolutional layer may not be enough. To address this, a ResNet BID3 approach was implemented for object recognition using CIFAR100 dataset. The network was trained with 50 classes initially, then additional classes were added in increments. Each time new classes were added, the last convolutional layer was cloned and retrained with new examples. This method allows for classification of both old and new classes together, a unique approach not previously explored. The study implemented a ResNet BID3 approach for object recognition using the CIFAR100 dataset. The network was initially trained with 50 classes and additional classes were incrementally added. The last convolutional layer was cloned and retrained with new examples for the new set of classes. Comparing the accuracies achieved by this method with a network trained without sharing any learning parameter showed an 8-12% accuracy degradation. The updated network accuracy for all 100 classes also showed about a 10% accuracy degradation. To mitigate this loss, network sharing was gradually reduced, leading to improved inference accuracy for both separate networks and the updated network. The study implemented a ResNet BID3 approach for object recognition using the CIFAR100 dataset. When sharing around 80% of learning parameters in the convolutional layers, accuracy within 1% of baseline was achieved. Classification accuracy for CIFAR100 with ResNet101 can reach 74% with all training samples applied together. However, for incremental learning, where all training samples are not available together, high accuracy is not achievable even without network sharing. Classification accuracy degrades drastically beyond 80% sharing. This indicates that 80% sharing is optimal for this network structure and application. The study implemented a ResNet BID3 approach for object recognition using the CIFAR100 dataset. Sharing around 80% of learning parameters in the convolutional layers resulted in achieving accuracy within 1% of baseline. For incremental learning, an optimal sharing configuration was determined through an Accuracy vs Sharing curve analysis. This configuration is used for learning any new set of classes, with only the high-layers corresponding to the new set of classes being retrained. The study implemented a ResNet BID3 approach for object recognition using the CIFAR100 dataset. Around 80% of learning parameters in the convolutional layers were shared, resulting in accuracy within 1% of baseline. For incremental learning, an optimal sharing configuration was determined through an Accuracy vs Sharing curve analysis. Only the high-layers corresponding to the new set of classes are retrained to prevent catastrophic forgetting. The circuit to system-level simulation framework was used to analyze the training scheme on DCNNs, including a computational energy model based on MAC operations. Energy consumption statistics were obtained by implementing multiplier and adder units in Verilog and mapping them to IBM 45nm technology. Storage requirements and memory access energy for the network were computed based on various factors. The memory structure in the proposed system was modeled using CACTI BID10 in 45nm technology library to estimate energy consumption. Deep learning toolboxes like Palm, MatConvNet BID18, and PyTorch BID12 were used for algorithm modifications and classification accuracy evaluation of DCNNs. Experiments were conducted using 4 NVIDIA Titan XP GPUs, with independent validation datasets. Benchmark details are listed in TAB3. In this section, results are presented showing the accuracy, energy efficiency, reduction in training time, storage requirements, and memory access achieved by the proposed design. Energy efficiency is achieved by eliminating a large portion of gradient computation and weight update operations during DCNN training. The normalized energy consumption per iteration for incremental training with and without sharing convolutional layers is illustrated in FIG4. The accuracies reported in this work are obtained using test datasets. The optimal sharing configuration for CIFAR100 in ResNet101 is 80%, resulting in 1.89x computation energy saving while learning new classes. The energy consumption is mainly dependent on the number of new classes, with only the output layer connections varying. Memory access energy is discussed separately. The proposed approach achieves significant savings in computation time by sharing convolutional layers, resulting in 2.3-2.6x reduction in training time per iteration for CIFAR100 in ResNet101 BID3. Convergence becomes faster due to inheriting features from the base model. Additionally, there is a 66-99% reduction in storage requirement. The proposed approach achieves significant savings in computation time by sharing convolutional layers, resulting in 2.3-2.6x reduction in training time per iteration for CIFAR100 in ResNet101 BID3. A large part of the training energy is spent on memory read/write operations for synapses. Proposed partial network sharing based training also provides 32-49% savings in memory access energy for CIFAR100 in ResNet101. The ImageNet BID1 (ILSVRC2012) dataset is challenging for object recognition/classification with 1.2 million labeled images. ResNet18 and ResNet34 BID3 were trained on scaled ImageNet2012 dataset, achieving top 1% and top 5% classification accuracy. In regular training, ResNet18 and ResNet34 achieved high classification accuracy. The dataset was divided into 3 sets for incremental learning. With our method, ResNet18 shared 1.5% of parameters and maintained accuracy. ResNet34 shared up to 38% of parameters, resulting in a 1.7x improvement in computation. In incremental learning, sharing 38% of learning parameters led to a 1.7x improvement in computation energy. Training time per iteration was reduced by 55% and memory access by 19%. The efficacy of incremental learning depends on the quality of the base network, with larger networks allowing more parameters to be shared without performance loss. The updated network performance closely matches the regular network on ImageNet due to the base network learning sufficient features from a large number of classes and examples. In this paper, an incremental training methodology for DCNNs is introduced, utilizing partial network sharing to accommodate new data without retraining the entire network. Increasing network depth leads to a higher percentage of parameter sharing while maintaining performance. The performance of DCNNs relies on having a representative training set, which can be time-consuming to acquire in practical applications. The proposed incremental training methodology for DCNNs allows for accommodating new data without retraining the entire network. It utilizes partial network sharing to preserve existing knowledge and improve computational efficiency. Results show significant improvements in real-world recognition applications."
}