{
    "title": "rJegl2C9K7",
    "content": "In recent years, Convolutional Neural Networks (CNNs) have become deeper, leading to a need for model acceleration. Prior work focused on transferring knowledge from a deep network (teacher) to a shallow one (student), but faced challenges with task-dependent knowledge and lack of effective training schemes. This study emphasizes the importance of features as crucial knowledge for the student network, proposing an efficient learning strategy to mimic features stage by stage. Extensive experiments validate the significance of features in improving performance. The proposed approach in the study significantly narrows down the gap between student and teacher networks by emphasizing the importance of features and outperforming state-of-the-art methods. Convolutional Neural Networks (CNNs) have made significant advancements in computer vision tasks like image classification, object detection, and semantic segmentation. However, the increasing depth of CNN architectures has led to high computational costs. Recent work has introduced lightweight models to reduce computing costs for mobile devices, but these models often sacrifice performance. To address this trade-off, knowledge distillation is used to train shallow networks to mimic deep ones, balancing efficiency and capability. Knowledge distillation is used to transfer information from a deep model to a shallow model for faster learning without losing accuracy. The main challenges include determining what knowledge to transfer and how to do so effectively. Previous methods have faced issues with sensitivity to tasks and datasets, requiring careful adjustment of hyper-parameters to balance learning from the teacher and original data. The challenges in knowledge distillation include inconsistencies between learning from the teacher model and ground-truth, as well as difficulties in transferring various types of knowledge accurately. Previous methods have not addressed the gap in learning abilities between student and teacher models. In this paper, a task independent knowledge transfer approach is proposed to address the gap in learning abilities between student and teacher models. The approach involves training the student to mimic features from the teacher stage by stage, isolating the knowledge contained in the teacher model from the ground-truth information. The student learns knowledge by mimicking the output features of the teacher in the first phase and is then trained with a task-dependent objective function based on these features in the second phase. This allows the student to focus on acquiring information from only one source in each phase, facilitating knowledge transfer. The method proposed involves training the student to mimic features from the teacher stage by stage, isolating the knowledge contained in the teacher model. The student learns knowledge by mimicking the output features of the teacher in sequential parts, allowing for more accurate transfer of information. The transfer process is divided into different stages, with only a sub-network trained at one time to alleviate the difficulty of mimicking the final features directly. The method involves training the student to mimic features from the teacher stage by stage, narrowing the gap in learning powers. Contributions include demonstrating the effectiveness of mimicking features, presenting a stage-by-stage training strategy, and surpassing state-of-the-art methods in performance and stability. Previous attempts in literature include model compression through network pruning to balance performance and storage capacity. In this work, model acceleration techniques such as network pruning, quantization, low-rank approximation, and knowledge transfer are discussed. Knowledge transfer involves training a shallow student model to learn from a deep teacher model, with the concept of Knowledge Distillation (KD) introduced for compression. FitNets, introduced as a solution to the limitations of Knowledge Distillation (KD), involve training a student model with an intermediate layer hint from the teacher model. This two-stage training approach provides better initialization for the student network. In our framework, stages are trained separately to prevent knowledge vanishing and speed up the mimicking process. Various types of knowledge transfer methods have been proposed, such as spatial attention maps and Maximum Mean Discrepancy (MMD) for feature map learning. However, existing methods do not effectively treat knowledge transfer. In our framework, we propose transferring knowledge from the network level instead of the task level to address issues like poor stability, limitations in migration to other tasks, and low performance. We introduce a stage-by-stage training strategy to enhance learning abilities between student and teacher networks. Our approach focuses on transferring knowledge from the network level to improve stability and performance. Unlike other methods like progressive block-wise training, our strategy is more efficient and accurate. Other approaches using reinforcement learning and adversarial networks for model compression still face challenges in network design and parameter updates. Our method involves the student mimicking features from the teacher in stages. Deep CNN models consist of feature extraction and application parts. The first part interprets an image as a high-dimensional feature, while the second part uses this representation for specific tasks like classification. The classification model uses a fully-connected layer and softmax function to predict probabilities for different classes. Similarly, detection models like RetinaNet BID17 use a backbone structure to extract feature maps and predict bounding boxes. The only difference between student and teacher models is their ability to extract features from images, as they share the same structure in using these features. If the student can produce identical features as the teacher, it should achieve similar results. When transferring knowledge from teacher to student, focus on feature extraction rather than the entire network. The student gains information from the teacher without needing labeled data, making it a generic solution. The CNN model makes inference through feature extraction and final stage prediction steps. The model can be trained with an objective function. When transferring knowledge from teacher to student, focus on feature extraction rather than the entire network. The student learns from the teacher using a shallow student model to mimic the output feature from the deep teacher model in two phases: feature learning and task adaption. In the task adaption phase, the student learns to apply features for different tasks by minimizing Eq.(2). Parameters \u0398 S E are fixed to prevent knowledge loss, only updating \u0398 S F. Teacher guidance is no longer needed as student and teacher have similar learning abilities. The method focuses on transferring knowledge from teacher to student and training the student to apply features for specific tasks, ensuring the student learns similar features as the teacher. In the task adaption phase, the student learns to apply features for different tasks by minimizing Eq.(2). Parameters \u0398 S E are fixed to prevent knowledge loss, only updating \u0398 S F. The method focuses on transferring knowledge from teacher to student and training the student to apply features for specific tasks, ensuring the student learns similar features as the teacher. To achieve this, the models are broken down into multiple stages, making it easier for the student to mimic the output of the teacher in each stage. In the task adaption phase, the student learns to apply features for different tasks by minimizing Eq.(2). Parameters \u0398 S E are fixed to prevent knowledge loss, only updating \u0398 S F. The method focuses on transferring knowledge from teacher to student and training the student to apply features for specific tasks, ensuring the student learns similar features as the teacher. The student feature encoder is divided into K stages, and the student model is trained stage by stage with DISPLAYFORM3. Parameters in previous stages are fixed to prevent transferred knowledge from vanishing. Only a subset of parameters is updated at each stage, maintaining computational efficiency. Breaking down teacher and student models is done using down-sampling layers as breakpoints. In Section 4, the proposed method evaluates performance through experiments on various datasets and tasks. Basic settings are introduced in Section 4.1, comparative experiments in Section 4.2, efficiency of feature transfer scheme in Section 4.3, and exploration on classification and detection tasks in Section 4.4. Evaluation is done on CIFAR-100, ImageNet, and COCO datasets. Among these, CIFAR-100 and ImageNet are used for classification, while COCO is for detection. BID5 serves as the teacher model, and ResNet-18 as the student model. In experiments, ResNet-18 is used as the student model for detection tasks with RetinaNet. Teacher models ResNet-101 BID16 and ResNet-50 are employed. Comparison experiments with knowledge transfer methods like KD, FitNets, AT, and NST are conducted. Transfer loss is calculated using 4 outputs of each residual block in classification and 4 feature maps in detection tasks. The importance of features and training a model with well-learned features is demonstrated through experiments on ImageNet for classification. The network can be divided into two parts: feature extraction before the fc layer and the softmax-activated fc layer as the classifier. Training the entire network with task-related objective function, then re-initializing and training only the fc layer can achieve similar results as end-to-end training. Experimenting with different parts of the model shows no real distinction in where to break it up, supporting the feasibility of stage-by-stage feature learning. The study demonstrates the effectiveness of training a student model to mimic the output features of a teacher model. By transferring knowledge through multiple stages, competitive results can be achieved even when the features are fixed. This approach proves to be more efficient than learning from scratch, as shown in experiments conducted on CIFAR-100 with different numbers of stages. The study applies experiments on CIFAR-100 with different numbers of stages using ResNet-18 as the student model and ResNet-34 as the teacher model. The network is divided into 4 blocks with down-sampling layers. Four independent student models are trained with 1 to 4 stages, followed by fine-tuning the classifier layer. Each stage involves end-to-end feature learning, with increasing accuracy compared to the baseline student model. Training with 4 stages improves performance by 4.7% in top1 accuracy compared to the baseline model. Adding more stages narrows the gap between student and teacher, making knowledge transfer easier. This approach outperforms other knowledge transfer methods on tasks like image classification and object detection. Our method demonstrates superior performance in knowledge transfer for classification tasks using CIFAR-100 and ImageNet datasets. The stage-by-stage feature transfer scheme outperforms other methods in accuracy, showing that the student model effectively learns from the teacher model. Our method achieves the best results on the ImageNet dataset, outperforming other methods that show inconsistencies across different datasets. Our approach focuses on transferring knowledge between networks independently from tasks, demonstrating stronger stability compared to existing methods sensitive to hyper-parameters. Our method outperforms baseline models and other methods in detection tasks, showing a 1.0% increase in mean Average Precision (mAP). Previous work is not as stable as ours, consistent with our approach of transferring knowledge independently from tasks. Our method divides the process into feature learning and final stages, trained independently with different losses to avoid confusion in the student model. Mimicking loss and target loss may not always align, highlighting the need for a teacher model to guide the student. This work introduces a stage-by-stage knowledge transfer approach to train a student model to mimic the output features of a teacher network gradually. The method focuses on the information within the model, offering a generic solution for model acceleration. The progressive training strategy reduces learning difficulties for the student in each stage, leading to improved performance on various tasks with strong stability."
}