{
    "title": "BJgWl3A5YX",
    "content": "Reinforcement learning (RL) is a powerful paradigm for deriving complex behaviors from simple reward signals in various environments. When applying RL to continuous control agents in simulated physics environments, the physical body of biological organisms and their controlling brains co-evolve, expanding the range of actuator/controller configurations. Intelligence is not solely in the agent's mind but also in the design of their body. The proposed method combines RL with an evolutionary procedure to uncover strong agents with optimized body and policy. Using the Shapley value, individual components' fair contributions are evaluated, considering synergies. Evaluation in a Robo-Sumo task environment shows significant performance improvement over baselines focusing solely on policy optimization. Reinforcement Learning (RL) uses a reward signal to derive complex agent policies, with recent progress in deep neural networks leading to strong results in various fields. Algorithms designed for stationary environments face challenges in non-stationary environments when multiple learning agents interact. Various approaches have been proposed for continuous control in physics simulator environments. Biological organisms' physical bodies constantly change, requiring joint optimization of the controlling brain and body for exploring a larger space of configurations. The interaction of evolution with learning by individual animals can lead to superior performance. Individual learning can enhance evolution at the species level, known as the \"Baldwin Effect.\" In learning agents, the physical shape of the body plays a dual role in effectively exerting forces and simplifying control learning. Our contribution proposes a method for uncovering strong agents by combining a body and policy, showing that intelligence resides in both the mind and body design. This contrasts with the traditional paradigm that views the body as a fixed part of the environment. The technique combines RL with an evolutionary procedure to identify body changes that contribute to agent performance. The method combines RL with an evolutionary procedure to identify body changes that contribute to agent performance in a 3D environment similar to the Robo-Sumo task. The results show that the proposed methods generate superior agents, outperforming baselines focusing on optimizing agent policy alone. EVCs have a genetically defined morphology and control system co-evolved for locomotion tasks. Using a curriculum approach can enable increasingly complex behavior. Embodied cognition suggests that morphological changes can impact behavior, leading to pressure to converge on a body design early in evolution. Giving the controller time to adapt to morphological changes can mitigate this interdependence. Learning to control an evolving body can speed up body evolution, with the extent of learning decreasing over time. Baldwinian evolution involves learning only in the evaluation phase, while Lamarkian evolution passes learning results to offspring. Using RL algorithms can achieve complex behaviors in continuous control tasks with fixed morphology. The experiments evaluate evolving the bodies of learning agents using Population Based Training (PBT) in a physically simulated environment. Multi-agent reinforcement learning is applied in partially-observable Markov games, where agents learn behavior policies from past interactions. Agents observe an egocentric view of positions and velocities in the physics simulator state. Our agents in the physically simulated environment have actuated hinges at the \"knee\" and \"hip\" of every limb. They learn policies independently to maximize long term utility. The analysis of body changes' importance uses cooperative game theory to quantify individual components' impact and synergies between them. The impact of individual players in a team is estimated using cooperative game theory. A player's marginal contribution in a team or permutation is defined as the change in performance resulting from their exclusion. Permutations of players are considered, with predecessors of a player in a permutation denoted as those occurring before them. The marginal contribution of a player in a permutation is the change in performance between the player and their predecessors. The Shapley value is a fair allocation of rewards to individual players in a team, reflecting their contributions to the team's success. It considers synergies between agents and is the marginal contribution of a player averaged across all player permutations. The method involves optimizing agents' policies and physical structures through competition and beneficial changes. It combines reinforcement learning to optimize controllers and an evolutionary procedure to optimize bodies. The approach outputs high-performing agents with improved bodies and fitted controllers. POEM is a method that maintains a population of RL agents and improves their controllers and physical bodies through an evolutionary process. It involves contests between agents to gather data for controller improvement and body evolution. POEM retains two sub-populations of agents for optimizing agent policies and improving agent bodies and controllers over time. POEM uses a variant of PBT to improve model parameters and learner hyper-parameters in continuous policy agents. It evolves agent bodies in the body-improving population and employs SVG for continuous control RL agents, incorporating off-policy Retrace-corrections for learning the action-value function. POEM uses an evolutionary procedure with policy learners to evolve agent bodies and learner parameters, adapting PBT. Agents play against each other in contests, with Elo ratings used to measure performance and drive evolution. The procedure maintains two subpopulations with an asymmetry in body types, affecting outcomes for the same action. POEM uses random match-making for agents to encounter each other in contests, evolving body parameters for body-improving agents. The environment is non-stationary due to evolving opponents and changing body configurations. The PBT procedure optimizes agent configuration by adjusting policy parameters, learner hyperparameters, and body shape based on agents' performance ratings. Agents are evaluated for evolution eligibility using specific criteria. Evolution eligibility criteria are used to avoid early conversion of the agent population. Agents undergo a warm-up period before being considered for evolution. Selection procedures determine if eligible agents will modify their parameters based on ratings in contests against other agents. After a warm-up period, agents compete in contests to compute win probabilities. If below a threshold, agents undergo inheritance and mutation. Inheritance adjusts parameters to be more like the opponent's, while mutation modifies parameters randomly within bounds. The experiments involve contests between agents using the MuJoCo physics simulator. A quadruped body, referred to as the \"ant body\", is used in the Robo-Sumo task where robots must tip their opponent over or force them out of the arena. The \"ant body\" used in the Robo-Sumo task is composed of a root sphere and 8 capsules connected via hinges, each actuated. The morphology is represented as a directed graph-based genotype, with nodes representing physical components and edges describing relationships between them. Each node describes the shape of a 3D rigid body and the limits of the hinge joint attaching it to its parent. Edges contain parameters for positioning, orienting, and enabling body symmetry. The genotype for the ant body consists of three nodes and three edges, defining the spherical torso, upper and lower legs. The body structure is determined by 25 parameters. Two sub-populations, body-fixed and body-improving, each with 64 agents, are used in the experiments. All actuators are available to the RL algorithm as actions. The experiment involves two sub-populations, one with agents having the same body configuration but different policy parameters, and the other with agents having different bodies. Both sub-populations start with random policy parameters and the same hyperparameters. The body-fixed agents share a standard body configuration, while the body-improving agents have varied body configurations sampled around the original body. The experiment involves two sub-populations, one with agents having the same body configuration but different policy parameters, and the other with agents having different bodies. Each sub-population has 64 agents. POEM matches agents for contests randomly, with body-fixed agents adapting to best match body-improving agents. The controller for body-improving agents must be robust to changes in the physical body. Agent performance is measured by Elo scores, with body-improving agents outperforming body-fixed agents in one run. In a study comparing body-improving agents to body-fixed agents, it was found that body-improving agents consistently outperformed body-fixed agents in over 70% of runs. This difference was statistically significant at the p < 0.001 level. POEM can find the configuration of strong agents by changing physical traits, typically outperforming agents with the original body. Evolved bodies are wider, higher, and heavier with a lower center of gravity. Body parameters converge early in training, possibly to a sub-optimal body. Using Body-Improving Controllers in the Original Body uncovers good combinations of body and controller. The study shows that using Body-Improving Controllers in the Original Body can lead to better performance, with controllers from the body-improving population outperforming those from the body-fixed population in 64% of runs. This demonstrates that POEM can find strong controllers even for the original body, highlighting its usefulness in situations where physical modifications are not possible. The study demonstrates that Body-Improving Controllers can enhance performance, even without modifying the physical body of agents. It explores the impact of changes to the body components on performance, considering synergies between them using the Shapley value in a cooperative game setting. In a cooperative game with 25 players representing body parts, the value of a subset of body changes is defined based on evolved and unevolved body configurations. A hybrid body is created with some parameters from the original body and some from the evolved body, evaluated using an evolved controller. The value of body changes in a cooperative game is determined by the win probability of an agent with a specific body and controller against a baseline agent. Shapley values are used to measure the relative importance of body changes, showing that certain parameters have a higher impact on agent performance than others. In a cooperative game, the impact of body changes on agent performance is measured using Shapley values. High Shapley components result in a significant performance boost compared to low ones. By ranking body parameters based on their Shapley value, incremental changes are made to the body-fixed body. This process generates 26 body variants, with the highest Shapley-valued parameters having the most impact on performance. The Shapley ranking of body parameters significantly impacts agent performance, outperforming other baseline heuristics. A framework combining continuous control RL agents with an evolutionary procedure for modifying agent bodies can produce stronger agents. Questions remain regarding modifying neural network architecture, using game theoretic methods in the evolutionary process, and ensuring diversity in agents' bodies for improved performance. In a study by Darrell Whitley, V Scott Gordon, and Keith Mathias, the structure of the ant body is described in detail, with parameters governing its shape. The ant body is represented as a graph-based genotype with mutable parameters for nodes and edges, except for Shape and Reflection. This analysis is based on independent multi-agent RL for physics. The analysis is based on independent multi-agent RL in a physics simulator environment, using multiple learners in a partially-observable Markov game. Agents receive shaping rewards based on their interactions, encouraging appropriate behavior policy learning in a partially observable Markov game with finite state sets and observation functions. In a physics simulator environment, agents interact in a partially-observable Markov game, receiving individual rewards and learning policies to maximize long-term utility. The true world state is held by a physics simulator, with agents receiving partial observations and taking actions to shape their behavior. The physics simulator in the environment holds the true world state, while agents receive partial observations in an egocentric view. Agents observe variables such as 3D positions of body parts, distances to pitch corners, and actuate hinges with continuous control signals. The POEM framework uses population-based training to evolve agents. The POEM framework utilizes population-based training (PBT) to evolve agents. The procedure involves measuring fitness, eligibility for evolution, selection, inheritance, and mutation. Elo ranking is used for fitness, with an iterative Elo rating update algorithm. Evolutionary events involve pairwise comparisons between agents for parameter updates. Eligibility criteria include processing a certain number of frames for learning. The POEM framework utilizes population-based training (PBT) to evolve agents, involving fitness measurement, eligibility for evolution, selection, inheritance, and mutation. Pairwise comparisons between agents are used for parameter updates based on Elo ratings and win-rate thresholds. The POEM framework uses PBT to evolve agents through fitness measurement, selection, inheritance, and mutation. When the fitness of a donor agent is lower than a threshold, the recipient agent is updated to be more similar through an inheritance procedure copying the donor's parameters and a mutation procedure applying random mutations within specified bounds. The Shapley value of the game is computed to determine the fair contribution of each body change, considering interdependence and synergies between components. An example is given with three possible changes to the body, showing that impact cannot be solely based on individual performance increases. The Shapley value determines the fair contribution of body changes by considering their interdependence and synergies. For example, applying changes a and b together only increases the win-rate to 56%, while adding c in combination with a or b improves performance by 5%. The Shapley value examines the average marginal contribution of components in achieving a win-rate of 61% when all three changes are applied. The Shapley value determines fair contribution of body changes by considering interdependence and synergies. It analyzes individual body changes using the average marginal contribution of components in all permutations. This computation is based on a hypothetical example and is intractable for two reasons. The Shapley value determines fair contribution of body changes by considering interdependence and synergies. It analyzes individual body changes using the average marginal contribution of components in all permutations. However, computing the Shapley value exactly is intractable due to the large number of body components. To overcome this, an approximation method is used by generating episodes where agent b(S) competes against the baseline agent to estimate its win-rate. To estimate the win-rate of agent b(S), we simulate episodes where it competes against a baseline agent and use the proportion of wins as an estimate. The Shapley value is then computed using an approximation method that samples component permutations."
}