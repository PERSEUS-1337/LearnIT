{
    "title": "SyNvti09KQ",
    "content": "The text discusses how autonomic nervous system responses can influence decision-making and how a novel approach to reinforcement learning using peripheral pulse measurements can improve learning efficiency in a simulated driving environment. The sympathetic nervous system (SNS) responds to dangerous situations, aiding in rapid decision-making and protecting from danger. It regulates various bodily functions and triggers the \"fight or flight\" response. These anticipatory responses help prepare for action and influence our appraisal of situations. Emotions influence human learning, planning, and decision-making. Intrinsic motivation is driven by feelings, while extrinsic motivation involves explicit goals. Driving is an example where both types of motivation are used, leading to physiological changes like heightened arousal. This paper presents a reinforcement learning framework that incorporates reward functions based on physiological responses to the environment, such as stress levels in a driving task. The goal is to determine the effectiveness of a reward function with intrinsic and extrinsic components in a reinforcement learning setting. The challenges of applying RL in real-world tasks include the need for large amounts of training data and the high cost of failure cases, such as in autonomous driving where rewards are sparse and bad actions can lead to catastrophic states. Humans also consider feedback from the body's nervous system for action selection, with increased arousal signaling danger or goal failure. Incorporating these visceral signals in an RL agent could reduce sample complexity and improve safety during explorations. Our work focuses on building intrinsic reward mechanisms trained on signals correlated with human affective responses, aiming to improve safety during explorations. We use implicit responses from the driver for training, which differs from explicit instructions commonly used in imitation learning. This approach addresses the structural credit assignment problem in RL by giving the agent intuition about new situations based on human feedback. The proposed method aims to reduce sparsity in reward signals, making learning more practical in a large parameter space. Experiments show that this approach can decrease the number of epochs needed for learning. By training a classifier using human physiological responses, the challenges of traditional search-based structured prediction can be simplified. The core contribution is a novel learning approach where the reward function is augmented with a model learned from the human nervous system. The proposed method aims to reduce sparsity in reward signals by incorporating a model learned from human nervous system responses. This model can improve safety and efficiency in learning processes. The idea is to use physiological responses as intrinsic rewards for artificially intelligent systems, enhancing learning efficiency and reducing catastrophic failures during training. When threats trigger a \"fight or flight\" response, it leads to physiological changes like increased heart rate, perspiration, and release of adrenaline and cortisol. These responses play a role in emotions, decision-making, and can lead to better decisions than purely rational thinking. While automatic responses can have errors, they are crucial for safety. Combining intrinsic emotional rewards with logical evaluation of threats is important for decision-making. In the context of decision-making, a combination of intrinsic emotional rewards and extrinsic rational rewards is crucial for optimal results. Reinforcement learning involves an agent interacting with the environment through actions and rewards, aiming to maximize future rewards. Algorithms like Deep Q-Networks learn policies to approximate optimal Q-functions, but applying RL techniques to real-world scenarios like autonomous driving is challenging due to high sample complexity. Reward shaping is a method to address the high sample complexity in driving tasks, where identifying the responsible action for success or failure is challenging. Sparse episodic reward signals in RL agents can be contrasted with continuous physiological responses like sympathetic nervous system (SNS) reactions in humans. Using SNS responses, such as changes in blood volume in the skin, can provide more informative feedback for drivers. Training a reward signal based on physiological signals capturing SNS responses is proposed as a solution. Using a reward signal trained on physiological responses like sympathetic nervous system activity can provide continuous and dense feedback for drivers. This approach allows for better credit assignment as SNS responses are linked to actions leading to success or failure. An example shows how a driver's pulse wave changes before a collision, indicating anticipatory response activation. The driver's anticipatory response is activated before a collision occurs. A recent study used facial expressions to train machine learning systems for image generation, leading to positive results. Our proposal suggests a reward function with both extrinsic and intrinsic components, aiming to predict human physiological responses to reduce stress and anxiety. In an RL framework based on a DQN, a modified reward function is proposed that combines the original reward with human physiological responses. This new reward is a convex combination controlled by a weighting parameter \u03bb, balancing task completion and intrinsic motivation. For example, in autonomous driving, the original reward could be velocity while the additional component reflects physiological responses. The system aims to minimize physiological arousal responses while completing tasks. An eight-layer CNN is used to estimate pulse amplitude from driver's physiological responses in a virtual environment. Challenges include building a predictive model of sympathetic nervous system responses due to the lack of realistic ground truth data. The focus is on modeling physiological responses in autonomous driving scenarios. In this work, high-fidelity simulations are used to collect physiological responses of humans for training a deep neural network to predict sympathetic nervous system (SNS) responses. The photoplethysmographic (PPG) signal captures blood volume changes in the skin periphery, reflecting emotional states like fear or anxiety. This phenomenon has been utilized in affective computing applications for emotional response analysis in various studies. The peripheral pulse can be measured unobtrusively, making it suitable for different applications. The pulse signal is used to capture nervous system responses in a scalable way. An artificial network is trained to mimic pulse amplitude variations based on visual input from the driver's perspective. A convolutional neural network is trained to mimic physiological responses based on input images collected in a simulated environment. The experiments were conducted in AirSim with an autonomous car equipped with an RGB camera. The autonomous car in a maze had an RGB camera for the agent to learn a policy mapping camera input to controls. The maze had walls and ramps, designed to be challenging. A CNN predicted normalized pulse amplitude from driver's physiological response using downscaled grayscale frames as input. The network architecture included a dense layer with 128 hidden units before the final layer with linear activation units. Four experienced participants drove in a maze, with frames ordered by predicted pulse amplitude indicating SNS response levels. The PPG signal was recorded from the index finger of the non-dominant hand using a Shimmer3 1 GSR+ with an optical pulse sensor at 51.6Hz. A peak detection algorithm was used to recover systolic peaks, which were normalized. Participants reported experiencing stress during a driving task in a virtual environment. The frames and pulse amplitude measures were used to train a CNN for further analysis. The CNN was trained using pulse amplitude measures from the PPG signal. The trained CNN, known as the visceral machine, provided rewards in the RL framework. Experiments were conducted to assess the efficiency of using predicted physiological responses in tasks related to autonomous driving. Three tasks were considered: maintaining high velocity, traveling long distances without mishaps, and driving towards a goal. Dense rewards were provided for velocity and distance tasks, while goal-directed task rewards were sparse and episodic. In experiments related to autonomous driving, a trained CNN known as the visceral machine provided rewards in the RL framework. Three tasks were considered: maintaining high velocity, traveling long distances without mishaps, and driving towards a goal. The goal-directed task had sparse and episodic rewards. Five models were trained independently for participants and combined. Using balanced visceral rewards with extrinsic rewards led to better learning rates compared to vanilla DQN or DQN with only the visceral component. Training examples were taken from the first 75% of frames, with the last 25% used for testing. Max pooling was inserted between layers 2 and 3. The CNN model used a batch size of 128 examples and included max pooling between specific layers to prevent overfitting. A dropout layer was added to address overfitting with a rate of 0.5. The models were trained for 50 epochs, resulting in a training RMSE loss under 0.1. Testing RMSE ranged from 0.10 to 0.19, significantly lower than random prediction. The CNN associated different rewards to situations, illustrated in Fig 4 with examples of predicted pulse amplitudes. The study predicted pulse amplitudes based on stress response to obstacles using a trained CNN in a DQN framework. Different values of \u03bb were tested to control the weight of visceral reward. Results showed mean extrinsic reward per episode over training time, with \u03bb=1 representing vanilla DQN execution. The performance generalized across participants, with similar data obtained from different models. The study tested different values of \u03bb to control the weight of visceral reward in a DQN framework. Results showed improved learning rates when \u03bb was non-zero or not equal to 1, with optimal \u03bb varying across tasks. Using visceral reward components led to longer episodes compared to vanilla DQN, indicating increased caution against collisions. The non-sparse rewards with visceral components contributed effectively to learning. The study tested different values of \u03bb in a DQN framework to control the weight of visceral reward. Optimal behavior was observed with mid-range values of \u03bb (e.g. 0.25), leading to improved learning rates and mission accomplishment. Lower \u03bb values promoted risk-averse behavior, while higher values required longer training periods. Longer episodes indicated increased caution against collisions when using visceral reward components in learning. The study tested different values of \u03bb in a DQN framework to control the weight of visceral reward. Optimal behavior was observed with mid-range values of \u03bb (e.g. 0.25), leading to improved learning rates and mission accomplishment. Lower \u03bb values promoted risk-averse behavior, while higher values required longer training periods. Longer episodes indicated increased caution against collisions when using visceral reward components in learning. The experiments also introduced a time-varying intrinsic reward that decays over time, with the intrinsic reward contributing less than 2% of the total reward by episode 95. The question of whether the CNN is predicting the SNS is raised. The study compared a proposed architecture with an agent using a distance-based reward component. Results showed that the agent using the CNN for the reward performed better. The CNN was found to capture more information than simple distance measures. The trained CNN captures context around driving in confined spaces, using peripheral physiological responses for reinforcement learning. Rewards based on sympathetic nervous system responses lead to efficient training. The trained CNN captures context around driving in confined spaces using peripheral physiological responses for reinforcement learning. Emotions play a role in decision-making but can also negatively impact decisions. Future work will focus on balancing intrinsic and extrinsic rewards and incorporating multiple intrinsic drives. The study used peripheral blood volume pulse prediction as an indicator of high arousal situations."
}