{
    "title": "HyI5ro0pW",
    "content": "Artificial neural networks have revolutionized data science and artificial intelligence, but they can become cumbersome with complex learning problems. To address this issue, a modified version of the fully connected layer called a block diagonal inner product layer is proposed. These layers have weight matrices that are block diagonal, creating densely connected neuron groups. This concept is an extension of group convolutional layers applied to fully connected layers. Block diagonal inner product layers can be achieved by initializing a block diagonal weight matrix or pruning off diagonal block entries. This method reduces network storage and speeds up run time without significantly affecting testing accuracy, offering a new approach to enhance network computation efficiency. Our work focuses on reducing memory requirements and improving computational demand in neural networks, specifically targeting inner product layers. While larger networks achieve higher accuracy, our approach aims to decrease network memory footprint without compromising computational efficiency. This addresses the challenge of balancing storage and computational resources in neural network architectures. One popular technique to condense neural networks is pruning, but it can slow down network training and execution. Block diagonal inner product layers offer a structured approach to speeding up runtime without sacrificing accuracy. Block diagonal inner product layers can be implemented by initializing a purely block diagonal weight matrix or by focusing pruning efforts off the diagonal blocks of a fully connected layer. These methods can reduce gradient computation time, improve accuracy, and support network robustness. Pruning can be used as a mapping to more convenient architectures, potentially speeding up training. Converting a fully connected layer into smaller inner product learners can boost the layer's performance and bring artificial neural networks closer to the architecture of biological mammalian brains. Weight pruning methods can create sparse networks that require less storage space. Iterative pruning using penalty methods involves adding a mask to disregard pruned parameters, reducing floating point operations but not the number performed. Further compression can be achieved through quantization and Huffman coding. After pruning, the average time for forward propagation is reported. After weight pruning to create sparse networks, further compression techniques like quantization and Huffman coding can be applied. Node pruning can speed up training and execution time, but drastic pruning may harm network accuracy, requiring additional weight fine-tuning. Other approaches include low rank approximation, parameter sharing, distillation, stale gradient updates, and lower precision techniques. These methods can be applied to any of the pruning techniques mentioned. Recent advancements in network architecture reduction have led to the development of structured parameter matrices with low displacement rank for high compression rates and fast evaluation. While BID36 focuses on toeplitz-related transforms for fully connected layers, block sparsity offers compression with parameter flexibility. Group lasso techniques, such as node pruning for convolutional filters, have also contributed to compression methods. Group lasso regularization is applied to entire filters in CNN architectures, while group convolutions and block diagonal inner product layers decrease the number of weights and connections. Decoupling channels in images is natural, transforming fully connected layers into smaller neuron groups for speedup. Neuron groups can boost convolutional layers. Methods like group lasso and group convolutions can be used to condense and speed up layers. Two methods for implementing block diagonal inner product layers are considered: initializing with a block diagonal weight matrix or iteratively pruning entries off diagonal blocks. Method 2 involves dense, iterative pruning, and block diagonal phases using weight decay with L1-norm. During the iterative pruning phase of Method 2 for implementing block diagonal inner product layers, weight matrix is pushed towards block diagonal by adding \u03b1 to the loss function, where \u03b1 is a tuning parameter. Pruning frequency and regularization are hyperparameters to consider. Masking out updates for pruned entries is more efficient than maintaining sparse format. Once pruning is complete, reformatting the weight matrix to have condensed blocks adjacent in memory maximizes speedup. Batched smaller dense calculations for the blocks can be done using cuBLAS strided batched multiplication. More pruning iterations may increase training time but can improve accuracy and reduce overfitting. The goal is to reduce memory storage of the inner product. Our goal is to reduce memory storage of inner product layers while maintaining or reducing execution time with minimal loss in accuracy. Experiments were conducted on MNIST, CIFAR10, and ImageNet datasets using Caffe BID17. Training was done with batched gradient descent and cross-entropy loss function. Forward time per inner product layer was measured, comparing times to sparse matrix runtime. The runtime comparison of sparse matrix multiplication with random entries in CSR format using cuSPARSE BID29 is reported. The forward and backward time for matrix products in gradient descent training with purely block diagonal layers is also discussed. Speedup in matrix multiplication with an n \u00d7 n weight matrix and batch size 100 for purely block diagonal weight matrices is shown in the figures. The overhead of performing cuBLAS strided batched multiplication increases as the number of blocks grows. Specialized batched multiplications for small matrices can achieve up to 6 fold speedup. BID44 suggests dividing channels into subgroups to improve information flow between group convolutions. This approach may require moving entries in memory or additional processing. Pruning to achieve block diagonal structure addresses the issue of performance impact from small matrix products. After pruning, learned weights provide a more complete picture with preserved information flow. An alternative to fixed channel shuffle is random shuffle of whole blocks. Results will be compared using these methods on the MNIST dataset. The LeNet-5 framework was used on the MNIST dataset with experiments conducted using various methods. The inner product layers were initialized using the Xavier weight filler, and results for the block diagonal method 1 without pruning were shown in FIG1. The (b1, b2)-BD architecture was discussed in terms of nonzero weights across both inner product layers. In experiments with the LeNet-5 framework on the MNIST dataset, a speedup of \u2265 1.4\u00d7 was achieved for b1 \u2264 50 with 8000 nonzero entries. Sparse format times are slow until less than 50 nonzero entries. Accuracy slightly declines as nonzero entries decrease, with FC achieving 99.11% accuracy. Without pruning, (100, 10)-BD has 98.52% accuracy. Traditional iterative pruning with L2 regularization resulted in 98.55% accuracy, but forward multiplication was over 8 times slower. Implementing (100, 10)-BD method 2 with pruning using 15 dense iterations and 350 pruning iterations resulted in a final accuracy of 98.65%. However, there was no significant benefit observed in using pruning over initializing pure block diagonal inner product layers for the MNIST dataset. Toeplitz (3) in BID36 achieved an error rate of 2.09% with a single hidden layer net with 1000 hidden nodes on MNIST, providing a 63.32 fold compression. On the other hand, a net with one hidden layer, 980 hidden nodes using (49, 1)-BD on MNIST achieved a 29.43 fold compression and an error rate of 4.37% with pruning method 2. In experiments on the MNIST dataset, (49, 1)-BD achieved a 29.43 fold compression and 4.37% error rate using method 2 with pruning. The speedup was 1.53 for forward only and 1.04 for forward and backward runtime combined. Hyperparameters like learning rate and weight decay were unchanged, with manual tuning for new hyperparameters introduced by method 2. Additionally, experiments were conducted on the CIFAR10 dataset using Krizhevsky's cuda-convnet, which consists of convolutional and fully connected layers. The inner product layers ip1 and ip2 have specific weight matrices. The weights are initialized using a Gaussian filler with standard deviation 0.1. Training batch size is 100, and hyperparameters from Caffe's \"quick\" model are used. The block diagonal method 1 without pruning shows speedup in the ip1 layer. Sparse format performs poorly until there are less than 50 nonzero entries. Accuracy declines as the blocks increase. The inner product layers ip1 and ip2 have weight matrices initialized with a Gaussian filler. Training batch size is 100, and hyperparameters from Caffe's \"quick\" model are used. Sparse format performs poorly until there are less than 50 nonzero entries. FIG2 shows accuracy decline as blocks increase. (64, 2)-BD achieves 76.29% accuracy, while traditional iterative pruning gives 75.18% accuracy with slower computation. Implementing (64, 2)-BD method 2 with pruning results in 74.81% accuracy, a 35.97 fold compression with only 1.5% accuracy drop. Total forward runtime of ip1 and ip2 in (64, 2)-BD is 1.6 times faster than in FC. FC achieved comparable speed with sparse format by using traditional iterative pruning to leave 37 and 40 nonzero entries in the final inner product layers, resulting in an accuracy of 73.01%. Implementing block diagonal layers with pruning yielded similar accuracy and memory condensation to traditional iterative pruning but with faster execution time. Node pruning until ip1 had only 2 outputs, resulting in a final accuracy of 59.67%. The (64,2)-BD net had a final accuracy 15.14% higher with pruning, achieving 76.29% accuracy on CIFAR10. The (64,2)-BD net achieved an accuracy of 72.49% on an independent test set and 75.63% on the training set without pruning. With pruning, the accuracy on the independent test set was 74.81% and on the training set was 76.85%. Both block diagonal methods reduce overfitting, with pruning showing slightly more improvement. Shuffling of ip2 layer blocks during training did not significantly affect final accuracy. Fixed sub-block shuffle on the (2, 2)-BD architecture did not improve accuracy. These methods may be beneficial for networks with stacked block inner product layers or sparse convolutions but did not show benefits in this case. In experiments with the AlexNet framework on the ImageNet dataset, methods involving group lasso, group convolution, and block substructure pruning were tested. Memory constraints on the Bridges Supercomputer prevented running ImageNet experiments with AlexNet. The AlexNet architecture includes five convolutional layers and three inner product layers with specific weight matrices. The study explores the use of block diagonal inner product layers to reduce network size and training time without compromising performance. Traditional pruning methods can slow down training and execution time due to inefficient sparse computation. The block diagonal approach addresses this by focusing on dense regions along the diagonal, leading to faster training. Another method combines structured pruning with block diagonal layers for improved accuracy. In experiments, method 2 outperformed the block diagonal method for CIFAR10, offering higher accuracy at the cost of slightly longer training time. Block diagonal methods provide flexibility and enable training of larger network architectures by converting fully connected layers into smaller inner product learners. This reduces GPU memory constraints and allows for additional speedup. Dependency between layers is a bottleneck in network parallelization, but structured sparsity like block diagonal layers can alleviate this issue. Software optimized for weight matrices with organized sparse form can further enhance speed and efficiency. Software optimized for weight matrices with organized sparse form, such as blocks, can provide up to 6 fold speedup in specialized batched matrix multiplication. Hardware is evolving to better support sparse operations, especially for neuromorphic systems with a 2-D structure that are inefficient for large dense matrix calculations."
}