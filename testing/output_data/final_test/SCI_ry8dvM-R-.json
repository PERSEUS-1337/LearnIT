{
    "title": "ry8dvM-R-",
    "content": "Multi-task learning with neural networks aims to improve performance by leveraging commonalities in tasks but often faces task interference. To address this, a routing network paradigm is introduced, consisting of a router and function blocks that dynamically compose different neural networks for each input. The model is trained using collaborative multi-agent reinforcement learning. Our model combines a router and function blocks for joint training, outperforming cross-stitch networks and shared-layer baselines on multi-task datasets like MNIST, mini-imagenet, and CIFAR-100. The routing networks show improved accuracy and faster convergence, with constant per-task training cost compared to linear scaling in cross-stitch networks. In CIFAR-100, we achieve cross-stitch performance levels with an 85% reduction in training time. Multi-task learning (MTL) involves learning multiple tasks simultaneously by leveraging domain-specific information in training signals. A routing network architecture is proposed, consisting of a router and function blocks that recursively process inputs. The router can decide to take a PASS action if fewer iterations are needed. This approach outperforms cross-stitch networks and shared-layer baselines on multi-task datasets like MNIST, mini-imagenet, and CIFAR-100, showing improved accuracy and faster convergence with constant per-task training cost. The proposed routing network architecture allows dynamic self-organization in response to input, sharing function blocks for different tasks when positive transfer is possible. It can condition its decision on various factors like current activation, task label, depth, and historical decisions for improved compression and re-use of existing functions. The function blocks can be simple neural network layers or whole networks. The proposed routing network architecture allows dynamic self-organization by adding neural network layers or whole networks as function blocks for different tasks. Reinforcement learning is used to train the routers, with a separate RL agent for each task learning its own policy for routing instances through the function blocks. The study introduces three image classification datasets for Multi-Task Learning (MTL): a multi-task MNIST dataset, a Mini-imagenet data split, and CIFAR-100. Extensive experiments show improved accuracy compared to cross-stitch networks and joint training with layer sharing. Traditional MTL deep learning involves hand-designing neural network architectures to balance task-specific and shared parameters. In routing networks, a dynamic compositional model adjusts its structure for each task, unlike traditional deep convolutional networks. The approach is compared to techniques for selective transfer learning and mixtures of experts architectures. The gating network in mixtures of experts models selects appropriate weightings for expert network outputs. The proposed end-to-end differentiable model for expert networks allows for a soft mixture decision, but lacks important effects like exploration vs. exploitation tradeoffs. While mixtures of experts have been explored in transfer learning, they are not scaled for a large number of tasks. Additionally, dynamic representations in single and multi-task models struggle to scale to deep models with many parameters. Routing networks can create dynamic network architectures for convnets by routing some layers, extending research on automated architecture search. This technique applies to multitask learning, allowing the construction of general architectures without manual parameter selection. Our approach explores multi-task learning using a multi-agent reinforcement learning algorithm and a recursive decision process. It differs from existing work on continual learning and automated architecture search. A routing network, known as BID11, utilizes evolutionary algorithms instead of RL. It consists of a router and function blocks, with routing being the process of selecting and applying function blocks to an input vector. The router selects from function block choices based on the input instance (v, t) where v is a representation vector and t is a task identifier. The BID11 routing network uses evolutionary algorithms for routing, selecting function blocks based on input (v, t) to produce a classification\u0177. Algorithm 1 details the routing procedure, taking input vector v, task label t, and maximum recursion depth n. The algorithm iterates n times, selecting function blocks and applying them to produce an output vector. A special PASS action skips to the next iteration. The router decision function maps the current representation, task label, and depth to the next function block index. The algorithm iterates n times, selecting function blocks and applying them to produce an output vector. A routing network can be represented by adding copies of its layers as function blocks. The router decision function maps the current representation, task label, and depth to the next function block index during training. The algorithm iterates n times, selecting function blocks and applying them to produce an output vector. A routing network can be represented by adding copies of its layers as function blocks. The router decision function maps the current representation, task label, and depth to the next function block index during training. The network applies Algorithm 1 to sample s, storing a trace T = (S, A, R, r final). The final reward r final is determined based on the correctness of the network's prediction \u0177. Loss L(\u0177, y) is computed between prediction \u0177 and ground truth y, and backpropagation is used to train the parameters. Routing is viewed as an RL problem with states as triples (v, t, i) and actions as function block choices. The router makes decisions on actions (including PASS) for function blocks. The state is updated accordingly, and the router policy is trained using various RL algorithms. Instances are routed through the network to make predictions, with states, actions, and rewards recorded. Final rewards depend on the prediction accuracy. The router in the network makes decisions on actions for function blocks, which are then trained using SGD/backprop. Gradients are computed for specific function blocks, and the router is trained using an RL algorithm. The routing network uses immediate action rewards and a final reward based on network performance. For classification problems, the final reward is set to +1 for correct predictions and -1 for incorrect ones. In this paper, the focus is on experimenting with immediate rewards to encourage the router in a network to use fewer function blocks. Two rewards were tested, based on historical data: the average number of times a block was chosen and the average historical probability of the router choosing that block. No significant difference was found between the two approaches, so the average probability reward was used. In our experiments, we found no significant difference between using average probability as a reward and evaluated the effect of \u03c1 on performance. Task-based routing involves routing and dispatching agents trained using single-agent and multi-agent RL strategies. FIG2 shows variations in routing decision approaches. In experiments, task-based routing involves single-agent and multi-agent RL strategies. Multi-agent approach includes a dispatching agent for assigning inputs to agents. MARL algorithm Weighted Policy Learner (WPL) is used with tabular and approximator representations of the policy. The approximator representation for routing involves using either one MLP for depth or a vector of MLPs for each decision/depth. Q-Learning and Policy Gradient algorithms are applicable with tabular and approximation function policy representations. REINFORCE BID35 is used to train both representations. Implementing router decision policy with multiple agents turns the routing problem into a stochastic game, where incompatible agents compete for blocks to train. In a multi-agent setting, incompatible agents compete for blocks to train, while compatible agents can benefit from sharing function blocks. The environment for routing networks is non-stationary due to training function blocks and router policy, making training more challenging than in a single-agent setting. Single-agent policy gradient methods like REINFORCE are less suited for this changing environment and may suffer from degraded performance. The weighted policy learner (WPL) algorithm is a multi-agent reinforcement learning algorithm designed to address the challenges of training in a non-stationary environment. It uses a policy gradient approach to update the router policy, dampen oscillations, and push agents to converge more quickly towards a Nash equilibrium strategy. The WPL algorithm moves away from Nash equilibrium strategy and increases it when approaching one. Historical average return for each action is initialized to 0. The function simplex-projection projects policy values to a valid probability distribution. The states in the trace are not used. The WPL-Update algorithm is defined for the tabular setting and future work includes adapting it for function approximators. Training of router and function blocks is performed independently after computing the loss. The training of router and function blocks is done independently after computing the loss. Experiments were conducted with adding gradients from router choices to function blocks, but no advantage was found. Three datasets were used for experimentation: MNIST-MTL, Mini-Imagenet (MIN-MTL), and CIFAR-100 (CIFAR-MTL). In MNIST-MTL, tasks involve differentiating instances of a class from non-instances. MIN-MTL is a smaller version of ImageNet for easier training. For Mini-ImageNet, 50 labels are randomly chosen to create tasks from 10 subsets of 5 labels each. Each label has 800 training instances and 50 testing instances, totaling 4k training and 250 testing instances per task. CIFAR-100 has coarse and fine labels, with one task for each of the 20 coarse labels and 500 instances for each corresponding fine label. There are 20 tasks with 2.5k instances each for training and 500 for testing. All results are reported on the test set and averaged over 3 runs. The datasets challenge learning in different ways. The experiments are conducted on different datasets with varying levels of task coherence and interference. The model used is a convnet architecture with 4 convolutional layers and 3 fully connected layers. The experiments involve a convnet architecture with 4 convolutional layers and 3 fully connected layers. Neural net approximators are used for router agents, with a parameter sweep conducted to optimize learning rate and \u03c1 value. SGD is chosen over Adam for better performance, with a special \"PASS\" action allowing agents to skip layers during training. In the experiments, a detailed comparison of different RL training algorithms on CIFAR-MTL is conducted. The best performer is the WPL algorithm, outperforming tabular Q-Learning by about 4%. Multiple agents work better than a single agent, and tabular versions make better predictions based on task and depth. The WPL algorithm outperforms tabular Q-Learning by about 4% on CIFAR-MTL. Multiple agents are more effective than a single agent, with tabular versions making better predictions based on task and depth. The next experiment compares WPL against other routing approaches for SimpleConvNet's fully connected layers. The WPL algorithm outperforms tabular Q-Learning by about 4% on CIFAR-MTL. Multiple agents are more effective than a single agent, with tabular versions making better predictions based on task and depth. The next experiment compares WPL against other routing approaches for SimpleConvNet's fully connected layers. The router can pick only from layer 0 function blocks at depth 0 and choose any function block from two of the layers. The soft-mixture-fc approach uses a trained softmax attention instead of hard selection. The best performer is routing-all-fc, the fully routed WPL algorithm, which is compared against cross-stitch networks. The curr_chunk discusses comparing routing-all-fc on different domains against cross-stitch networks BID23 and two baselines: task specific-1-fc and task specific-all-fc. Cross-stitch networks BID23 use linear combination models for multi-task learning, with shared input layers and \"cross stitch\" connection layers. The experiments add a cross-stitch layer to each routed layer of SimpleConvNet and compare to a \"soft routing\" version soft-mixture-fc. The comparison between routing-all-fc, cross-stitch networks BID23, task specific-1-fc, and task specific-all-fc is discussed. Routing-all-fc consistently outperforms the other networks in the experiments. The routing net routing-all-fc outperforms cross-stitch networks and baselines in various experiments on different datasets. It beats cross-stitch networks by 7% on CIFAR-MTL and 2% on MIN-MTL. The results are better on CIFAR-MTL due to common task instances, while MIN-MTL tasks are randomly constructed. On MNIST-MTL, routing nets outperform cross-stitch networks by 9%. Overall, routing makes a significant difference over both cross-stitch networks and baselines. The dynamic policy of routing networks outperforms cross-stitch networks and baselines, leading to better accuracy and faster training times. Scaling experiments show that routing networks consistently perform better across various datasets, with no apparent increase in computation with added function blocks. The dynamic policy of routing networks outperforms cross-stitch networks and baselines, leading to better accuracy and faster training times. The hard routing policy can scale to many task learning scenarios that require diverse functional primitives. In comparison, the single-agent approach often chooses only 1 or 2 function blocks at each depth, limiting diversity in agent choices. The routing network outperforms static models by learning diverse policies for each task, leading to better decision-making and independence among agents. This diversity prevents biases and allows each agent to learn what works best for its assigned task. The policy evolution for the first decision is shown in Figure 9, with each task agent eventually converging to a pure strategy for routing. The probability of choosing a specific function block over time is also analyzed. The probability of choosing block 7 over time is analyzed for each task agent, showing oscillation until two agents, pink and green, emerge as the top choices with close to 100% probability. This pattern of early emergence and strong interest in specific blocks is observed in the analysis of other blocks as well. In Figure 9, the policy evolution for the first decision is depicted, with each task agent eventually converging to a pure strategy for routing. The network learns a strategy using 7 function blocks, compresses to 4, then expands to 5, showing improvement over static baselines. A routing and multi-agent router training algorithm outperforms other approaches, allowing networks to dynamically adjust representations. The text discusses the challenges of scaling routing networks to deeper networks and suggests using hierarchical RL techniques. Successful experiments have used a multi-agent architecture with one agent per task trained with the Weighted Policy Learner algorithm. The approach is currently tabular but efforts are being made to adapt it to use neural net approximators. Routing networks have also been tried in an online setting for few shot learning, with mixed results. The text discusses the challenges of scaling routing networks to deeper networks using hierarchical RL techniques. Experiments have been conducted with a multi-agent architecture using the Weighted Policy Learner algorithm. The routing network can become fully recurrent, allowing for the network to shorten the recursion depth. Nine different implementation variants of the routing architectures have been tested. The routing architectures are summarized in TAB4, with different numbers of agents used. Policy representation variations include storing the policy as a table or using MLPs with a hidden layer of dimension 64. The policy input for routing actions involves using a representation vector concatenated with a one-hot task identifier. Tabular policies index agents based on tasks, while approximation-based policies vary for single and multi-agent cases. In the dispatched case, a dispatcher predicts an agent index based on the input vector. The WPL algorithm is a policy gradient algorithm that scales down the learning rate for an agent after a gradient change in its policy. It adjusts the gradient based on the sign, either scaling by 1 - \u03c0(a i ) for positive gradients or by \u03c0(a i ) for negative gradients to slow down learning and encourage convergence. The WPL algorithm adjusts the learning rate for an agent after a gradient change in its policy, slowing down learning to dampen policy oscillation and drive convergence."
}