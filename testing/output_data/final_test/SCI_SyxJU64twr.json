{
    "title": "SyxJU64twr",
    "content": "A new intrinsic reward generation method for sparse-reward reinforcement learning is proposed based on an ensemble of dynamics models. The method uses multiple dynamics models to approximate the transition probability and designs the intrinsic reward as the minimum surprise seen from each model. When combined with the proximal policy optimization (PPO) algorithm, the proposed method outperforms previous single-model based methods for locomotion tasks in reinforcement learning with sparse rewards. Sparse reward reinforcement learning is a research area where the environment only provides a non-zero reward under certain conditions. Exploration is crucial in the early stages, while a balance between exploration and exploitation is needed later on. Various methods like the -greedy strategy and policy gradient control with Gaussian noise have been used in sparse reward RL. Intrinsically motivated RL algorithms have been developed to address the challenge of sparse rewards in reinforcement learning. These algorithms generate intrinsic rewards for exploration, using curiosity and surprise as guiding principles. They have shown success in improving performance compared to previous approaches. In order to enhance sparse reward model-free RL performance, a new method is proposed to generate intrinsic reward based on an ensemble of estimation models for environment dynamics. By utilizing multiple distributions, more flexibility is achieved in modeling unknown dynamics and designing better rewards. Results indicate that this model-ensemble-based method outperforms existing reward generation techniques for continuous control with sparse rewards in a Markov Decision Process setting. The paper aims to optimize the policy \u03c0 in sparse reward RL environments by generating intrinsic rewards to maximize the expected cumulative return. It assumes the true transition model is unknown to the agent and adds intrinsic rewards to the extrinsic rewards for training. In sparse reward RL environments, intrinsic rewards are added to extrinsic rewards to optimize policy \u03c0 by maximizing the expected cumulative return. The design of intrinsic rewards is based on surprise, aiming to improve exploration by measuring the unexpectedness of observing the next state for a given current state and action pair. In sparse reward RL environments, intrinsic rewards are added to extrinsic rewards to optimize policy \u03c0 by maximizing the expected cumulative return. The surprise is quantified as the KLD D KL (P ||P \u03c6 ) at timestep t can be lower-bounded with an arbitrary choice of the parameter \u03c6. The intrinsic reward at timestep t is determined based on a single model P \u03c6 for P, where P \u03c6 for given (s, a) is modeled as a Gaussian distribution. The proposed 1-step intrinsic reward outperforms previously designed intrinsic rewards. In sparse reward RL environments, intrinsic rewards are used to optimize policy \u03c0 by maximizing the expected cumulative return. The surprise is quantified using the KLD D KL (P ||P \u03c6 ) at timestep t, with the intrinsic reward determined based on an ensemble of K dynamics models P \u03c6 1 , \u00b7 \u00b7 \u00b7 , P \u03c6 K for P. This approach increases degrees of freedom for modeling the underlying dynamics and designing a better intrinsic reward. The intrinsic reward at timestep t is determined based on an ensemble of K dynamics models, providing more freedom to design intrinsic rewards. Two objective functions are considered to optimize policy \u03c0 by maximizing the expected cumulative return. The intrinsic reward is determined by an ensemble of dynamics models, allowing for more flexibility in designing rewards. Two objective functions aim to maximize the expected cumulative return for policy optimization. The optimal solutions maximize these functions, with a proposition stating that better estimation of transition probabilities leads to a closer match between actual and expected rewards. Minimizing the estimation model tightens the gap between learned policies. The ensemble of dynamics models determines the intrinsic reward, providing flexibility in reward design. Two objective functions maximize expected cumulative return for policy optimization. Better estimation of transition probabilities leads to a closer match between actual and expected rewards, tightening the gap between learned policies. The minimum among available individual surprises is used for intrinsic reward to reduce estimation model. The ensemble of dynamics models determines intrinsic reward by using the minimum of available individual surprises to reduce the gap between learned policies. The agent selects the index with the minimum intrinsic reward value from K candidates, optimizing policy towards the true optimal policy. In the ensemble of dynamics models, Gaussian distributions are used to update model parameters and mixing coefficients through maximum-likelihood estimation with regularization and KL constraints. Optimization is done with second-order approximation for model parameters and a method proposed by Dempster et al. for mixing coefficients. The ensemble of dynamics models uses Gaussian distributions for updating model parameters and mixing coefficients through maximum-likelihood estimation with regularization and KL constraints. The method proposed by Dempster et al. is applied for setting the mixing coefficient q_i, along with the \"log-sum-exp\" trick for numerical stability. The simultaneous update of all \u03c6_i's and q_i's is found to be more effective than one-by-one alternating updates. The intrinsic reward generation method can be combined with general RL algorithms, but in this case, the PPO algorithm is used, which generates a batch of experiences for training the policy. The policy \u03c0 \u03b8 l is parameterized at batch period l, with the total reward at timestep t given by M AX. Initialize policy \u03c0 \u03b80, transition probability models, and mixing coefficients. Generate trajectories with \u03c0 \u03b80 and update q 1, \u00b7 \u00b7 \u00b7 , q K. Perform gradient updates and update q 1, \u00b7 \u00b7 \u00b7 , q K by drawing batches from the replay buffer. Collect s t from the environment and a t. The policy \u03c0 \u03b8 l is updated using PPO with total rewards for N epochs. Intrinsic rewards are normalized and used with a weighting factor \u03b2 > 0. The policy is updated at every batch using the normalized intrinsic rewards. The policy \u03c0 \u03b8 l can be updated using PPO with total rewards for N epochs, incorporating normalized intrinsic rewards with a weighting factor \u03b2 > 0. The proposed intrinsic reward generation method is applicable to various RL algorithms, evaluated in sparse reward environments using Mujoco and OpenAI Gym tasks. The delay method is employed to accumulate extrinsic rewards over \u2206 timesteps or until the episode ends, providing the sum to the agent at the end of each interval. The proposed approach involves validating intrinsic rewards based on minimum surprise over an ensemble. Various methods are explored to obtain a single intrinsic reward value from multiple preliminary reward values. The experiments were conducted with simulations using fixed random seeds and a specific number of timesteps. The proposed approach involves validating intrinsic rewards based on minimum surprise over an ensemble of models. Different methods for extracting a single intrinsic reward value from multiple models were compared, with the minimum selection method showing the best performance across all environments. The proposed approach validates intrinsic rewards based on minimum surprise over an ensemble of models. The minimum selection method outperforms simple mixture replacement. The impact of model order K is investigated, with performance improving as K increases in sparse reward environments. The performance improves as model order K increases in sparse reward environments. K = 2 is chosen for the model order, yielding similar performance to K = 3. The proposed method is compared with existing intrinsic reward generation methods using PPO as the background algorithm. The study compared the proposed intrinsic reward generation method with existing methods using PPO as the background algorithm. Different weighting factors were optimized for each method to balance extrinsic and intrinsic rewards. The performance improved with higher model order K in sparse reward environments. The proposed model-ensemble-based intrinsic reward generation method outperformed existing methods in sparse reward environments. The hyperparameters were tuned for best performance, and the results showed significant gains, especially in sparse Hopper and sparse Walker2d scenarios. Various types of intrinsic motivation have been studied in cognitive science, inspiring intrinsically-motivated RL approaches. Intrinsically motivated reinforcement learning (RL) draws inspiration from cognitive science studies. Researchers have explored various methods such as using information gain, homeostasis, surprise, and model-based approaches for learning. These approaches involve utilizing dynamics models, intrinsic rewards, and policy optimization techniques to enhance RL performance. In this paper, a new intrinsic reward generation method based on an ensemble of dynamics models for sparse-reward reinforcement learning is proposed. The method uses a mixture of multiple dynamics models to approximate the transition probability and designs the intrinsic reward as the minimum surprise from each model to capture relevant information. The proposed intrinsic reward generation method combines a dynamics model ensemble with PPO for reinforcement learning. Ablation study shows its superiority in sparse environments. Theoretical propositions support the effectiveness of the method. The minimum average KLD between true transition probability distribution P and mixture model P is upper bounded by the minimum average KLD between P and each individual model distribution in the ensemble. The proof involves the convexity of KL divergence and the linearity of expectation. The convexity of the KL divergence in terms of the second argument is utilized in the implementation of policy and dynamics models using fully-connected neural networks with two hidden layers of size (64, 64) and tanh activation function. The means of the Gaussian dynamics models are outputs of the networks, with trainable variances initialized to 1. Initialization is randomized for each dynamics model. The policy model implementation used PPO as the baseline algorithm with specific hyperparameters such as \u03bb = 0.95 for GAE, \u03b3 = 0.99 for discounting, batch size L = 2048, minibatch size L mini = 64, epoch number N = 10, clipping constant = 0.2, and entropy coefficient = 0.0. The maximum number of timesteps was set to 1M for all environments. The experiments involved setting a maximum of 1M timesteps for all environments. Different methods required a replay buffer of size 1.1M, with 2048 \u00d7 B samples added before iterations. For methods not needing a replay buffer, 81920 timesteps were run before measuring performance. Batch size L was set to 2048, L mini to 64, and N to 4 for dynamics model learning. The optimization was based on a second-order approximation. The optimization problem in the model learning process was based on a second-order approximation. The constraint constant \u03ba was set to 0.001, and the value of \u03b1 was tuned to 0.01 for each environment. Another hyperparameter, h, was used to balance intrinsic reward and homeostatic regulation. The weighting factor \u03b2 was optimized for each algorithm and environment. The major hyperparameters for the model-ensemble-based method are the same as those for the single-model surprise method. The intrinsic reward module method involves training with intrinsic reward only or a combination of intrinsic and extrinsic rewards. Reproducibility was confirmed by running timesteps before measuring performance. Recent exploration methods focus on generating intrinsic rewards explicitly and training the agent with a combination of intrinsic and extrinsic rewards. Our work involves combining extrinsic and intrinsic rewards to generate exploration techniques. Unlike previous works, we do not require the concept of a goal. Another recent approach suggests training a module in a delayed reward environment to generate intrinsic rewards separately from the usual policy training. In a delayed reward environment for sparse rewards, agents receive non-zero rewards when achieving a certain physical quantity above a threshold. Recent works have proposed generating intrinsic rewards using different methods such as Wasserstein-1 distance and Jensen-Shannon divergence. Our approach involves using a model ensemble to generate intrinsic rewards, distinct from previous methods that revise the objective function or perturb the environment for exploration. To stimulate exploration, novel state-action pairs are used from the past and sparse reward environments delay extrinsic rewards. Hong et al. (2018) revised the objective function by maximizing the divergence between current and recent policies. Dropout was applied to the PPO algorithm to encourage stochastic behavior. A new exploratory policy is created by combining target and given policies. Colas et al. (2018) proposed goal-based exploration in the parameter space of policy. In a recent study, Shyam et al. (2018) introduced a pure exploration MDP approach using utility based on Jensen-R\u00e9nyi divergence. They considered various transition models to compute utility based on average entropy. Their work focuses on minimizing surprise from multiple dynamics models with explicit rewards."
}