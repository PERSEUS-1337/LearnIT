{
    "title": "Bys_NzbC-",
    "content": "L1 and L2 regularizers are important in machine learning for simplifying solutions. Strong regularization with gradient descent can lead to training failure, limiting neural network generalization. Research investigates why this occurs and proposes Delayed Strong Regularization to address it. Experiment results show improved accuracy and sparsity on public datasets. Regularization techniques like L1 and L2 norms are crucial in machine learning to prevent over-fitting and achieve sparse solutions. While deep neural networks have been successful in various tasks, they often have many parameters that can lead to over-fitting. New techniques like dropout and pruning have been introduced, but traditional methods like L1 and L2 regularization still play a significant role in improving performance. L1 regularization, also known as Lasso, helps in obtaining sparse solutions, reducing memory and power consumption. Applying strong L2 regularization to deep neural networks with dropout layers can significantly reduce error rates by up to 24% on public datasets. Strong regularization is crucial for models with many parameters and limited training data, as it helps prevent over-fitting. However, imposing strong L1 or L2 regularization on DNNs can be challenging due to the vanishing gradient problem. In this paper, the authors address the issue of strong regularization failing in deep neural networks due to the vanishing gradient problem. They propose Delayed Strong Regularization as a solution, which involves a time-dependent schedule of regularization strength. By waiting for the model to reach an \"active learning\" phase before enforcing strong regularization, they were able to achieve superior performance without additional computation. The proposed approach achieves strong regularization with higher accuracy and compression rates in deep neural networks. The network is trained by minimizing the cost function with a regularization term added for simplification. The regularization strength is controlled by the parameter \u03bb, with a higher value indicating stronger regularization. The most commonly used regularization function is squared L2. The regularization function in deep learning includes L2 norm for weight decay and L1 norm for inducing sparsity in the model. The L2 regularizer reduces parameter magnitude to prevent overfitting, while the L1 regularizer makes a portion of parameters zero for computational efficiency. The model parameters are updated using gradient descent with a learning rate \u03b1. The L1 norm is not differentiable at 0, so a subgradient of 0 is often used in practice. The L2 regularizer reduces parameter magnitude proportionally, while L1 regularizer reduces it by a constant. Strong regularization is crucial for deep learning due to the large number of parameters. However, imposing strong regularization can lead to sudden learning failure in gradient descent, as shown in Figure 1 with VGG-16 and AlexNet architectures on CIFAR-100 dataset. The model's accuracy increases with more regularization, but suddenly drops to 1.0% with slightly stronger regularization, causing learning failure. Training loss improves with stronger regularization (\u03bb = 1 \u00d7 10 \u22123), but does not with even stronger regularization (\u03bb = 2 \u00d7 10 \u22123). Gradients show that a model with moderate L1 and L2 regularization learns better than one with stronger L2 regularization. The models with moderate L1 and L2 regularization follow a good path initially, but stronger regularization leads to failure in learning as the gradients from L decrease exponentially. This observation highlights the importance of finding the right level of regularization strength for successful learning in deep neural networks. In deep neural networks, strong regularization causes rapid decrease in gradients \u2202L \u2202wi. Gradients at the l th layer are determined by back-propagation, with a recursive relation for layer residuals \u03b4 (l). Excessive regularization suppresses weights significantly, leading to severe gradient suppression. The factor a (l\u22121) further contributes to this suppression. Normalization techniques such as batch normalization can help mitigate the effects of strong regularization on gradient suppression in deep neural networks. The factor a (l\u22121) in the recursive relation for layer residuals can lead to further suppression of gradients when weights are very small. This is especially true when using ReLU activation function, as it is proportional to the product of weights at previous layers. In the presence of strong regularization, gradients are more suppressed than weights, particularly in fully-connected layers. Similar conclusions can be drawn for convolutional layers. Normalization techniques like batch normalization and weight normalization can help prevent the quick diminishing of gradients. However, combining L2 regularization with normalization does not simplify solutions but only affects the learning rate. To address this, a time-dependent regularization strength, \u03bb t, is proposed to accommodate strong regularization and simplify solutions to reduce over-fitting. A time-dependent regularization strength, \u03bb t, is defined as DISPLAYFORM0 where epoch(t) is the epoch number of time step t, and \u03b3 is a hyper-parameter set through cross-validation. The hypothesis is that strong regularization is imposed after the \u03b3 th epoch to prevent learning failure once a good learning path is established. The hyper-parameter \u03b3 is recommended to be between 2 and 20. This approach differs from imposing weaker regularization throughout training. The proposed method introduces a time-dependent regularization strength, \u03bb t, after the \u03b3 th epoch to prevent learning failure. It achieves higher sparsity than the baseline and shows the best accuracy in preliminary experiments. The method is easy to implement and closely resembles traditional regularization methods. The algorithm for L1 regularizer employs the proximal gradient algorithm to ensure proper sparsity. It uses soft-thresholding to update parameters and has been evaluated on popular architectures like AlexNet and VGG-16. The method does not employ architectures with normalization techniques like batch normalization. The data set statistics for VGG-11 and VGG-19 for SVHN contain 9.8 and 20.6 million parameters. Regularization is applied to all network parameters except bias terms. Modifications were made to the networks to accommodate the data sets, such as changing kernel sizes and neuron counts in fully connected layers. The networks are learned using stochastic gradient descent with momentum of 0.9. The parameters for training VGG-11, VGG-16, and VGG-19 are initialized with a momentum of 0.9 and a batch size of 128. The initial learning rate is set to 0.05 and decays by a factor of 2 every 30 epochs during the 300-epoch training. Different regularization methods and datasets are experimented with, resulting in multiple sets of experiments. Time-dependent regularization strength is introduced, with more than 10 values of \u03bb tested for each regularization method. The experiment results for VGG-16 include testing more than 10 values of \u03bb for each regularization method, reporting average accuracy and 95% confidence interval. Statistical significance tests are conducted for improvements over the baseline method, with p-values reported. The sparsity of each trained model is also noted. The proposed method shows higher accuracy and sparsity compared to the baseline, especially for higher values of \u03bb. L2 regularization is more commonly used than L1 regularization in practice. Using L2 regularization in our VGG-16 experiments resulted in a 14.4% improvement in accuracy compared to models without regularization. Our proposed method helps in tuning the regularization parameter by preventing sharp drops. L1 regularization achieved better sparsity for similar accuracy levels, indicating the importance of strong regularization in compressing neural networks. The improvement was more significant on CIFAR-100 than CIFAR-10, possibly due to overfitting. AlexNet experiment results also showed higher accuracy with our proposed method. The experiment results by AlexNet show higher accuracy and sparsity compared to VGG-16. L1 regularization outperforms L2 regularization in improving accuracy. The proposed method achieves sharper peaks in curves, especially with the sparsity regularizer. Results indicate that avoiding strong regularization in the early stages of training can lead to better exploration and superior local optima. Performance improvement with L1/L2 regularization is more significant on VGG-16 than on AlexNet due to VGG-16 having more parameters. Our proposed model consistently outperforms baselines by up to 3.89%, with statistically significant improvements in 6 out of 7 cases. L1 regularization leads to higher sparsity and compression rates up to 4.2\u00d7 compared to baselines, showing promise for compressing neural networks. The model reaches an \"active learning\" phase with elevated gradients, avoiding vanishing gradients even with strong regularization enforced. The model maintains equilibrium between gradients from L and regularization, coping well with strong regularization. The number of hidden layers affects tolerance to strong regularization, with more layers leading to easier learning failure. Empirical testing on VGG architectures with varying hidden layers supports this hypothesis. The experiment results on the SVHN dataset show that L2 regularization peaks at \u03bb = 1 \u00d7 10 \u22123, with more hidden layers shifting the performance drop tolerance to the left. VGG-19 struggles with sparser parameters under L1 regularization. Despite the method outperforming the baseline in all experiments, the improvement is less significant compared to other datasets like CIFAR-10 and CIFAR-100. The compression rate is good for VGG-19 due to its low tolerance level, making it achieve low sparsity. L2 regularization is important for training DNNs, and combining it with dropout can effectively reduce test error. L1 regularization and group sparsity regularization are promising for reducing computation and power consumption in deep neural networks. However, the phenomenon of learning failure with strong regularization has not been emphasized previously. In this work, the focus is on achieving strong regularization for deep neural networks. Previous studies have shown that strong L2 regularization can lead to learning failure, but the reasons behind this phenomenon have not been fully explored. The goal is to understand why learning fails with strong regularization and propose a method to avoid this failure. Our investigation into the phenomenon of learning failure with strong regularization in deep neural networks led to the proposal of Delayed Strong Regularization as a solution. Experiment results confirmed the effectiveness of our method in achieving higher accuracy and sparsity, especially in networks with multiple hidden layers. L1/L2 regularization, while challenging to tune, can significantly improve performance when optimized. Our approach is particularly beneficial for cases requiring strong regularization. Our proposed method is beneficial for deep learning projects with strong regularization needs, especially when a large labeled dataset is not available. However, it may not be necessary for cases with ample labeled data or networks with few parameters. Experiments were conducted with 750 training sessions, but applying the method to architectures with normalization techniques is not feasible. Our proposed method provides strong regularization benefits for deep learning projects lacking a large labeled dataset. However, it may not be needed for networks with ample labeled data or few parameters. Experiments showed challenges when applying the method to architectures with normalization techniques due to exploding gradients. Future work includes exploring the combination of our method with pruning-related methods and applying it to different regularizers like group sparsity regularizers. The simplicity of our Delayed Strong Regularization allows for easy extension to more complex methods. The proposed method offers strong regularization benefits for deep learning projects with limited labeled data. Experiments on VGG variations show that adding more hidden layers shifts the tolerance level where the baseline method fails. VGG-19 fails to learn after losing 27% of its parameters, while VGG-11 can still learn after losing 84%. The method is simple to implement with only one additional hyper-parameter. The proposed method, similar to traditional regularization, performs well for non-strong regularization and achieves strong regularization. It outperforms other approaches tested, including a warm-start strategy and Ivanov regularization method. The regularization term is applied only when the L1 norm of the weights is above a certain threshold. Setting \u03bb just above the tolerance level found by the baseline method did not lead to any learning. The model needs to go through strong regularization for the first few epochs to reach a low enough L1 norm, causing neurons to lose learning ability. Applying strong regularization first and then reducing the strength is used in sparse learning for convex optimization, contrasting with the proposed method that avoids strong regularization initially. The approach involves avoiding strong regularization initially and then applying it gradually. An experiment with VGG-16 on CIFAR-100 showed that models did not improve beyond random guessing when regularization was not enforced in the beginning. Strong regularization at the start leads to a quick decrease in weight magnitudes, causing the model to lose its learning ability."
}