{
    "title": "BJx1SsAcYQ",
    "content": "Low-precision networks show great promise for energy and area efficiency in embedded deep network inference. ResNet-18, ResNet-34, ResNet-50, ResNet-152, Inception-v3, densenet-161, and VGG-16bn networks at 8-bit precision surpass the accuracy of full-precision networks after finetuning. Additionally, 4-bit ResNet-18, ResNet-34, and ResNet-50 models achieve the highest accuracy to date without the need for training from scratch. Gradient noise from quantization during training increases with reduced precision. To overcome this noise, techniques include starting with pretrained fp32 networks and fine-tuning, using larger batches with matched learning rate annealing, and proper activation function range calibration. These methods aim to discover low-precision networks close to fp32 precision baseline networks. The implementation of low-precision networks for embedded and large-scale applications has gained interest due to the need for energy-efficient deployment. Various methods have been developed to quantize weights and activations of these networks, with 8-bit networks showing decreased computational complexity but lower accuracies compared to full-precision networks. Training 8-bit networks from scratch also fails to bridge this accuracy gap, especially for 4-bit precision. Only one method has matched the accuracy of full-precision networks for ImageNet classification benchmark when quantizing both weights and activations. Fine-tuning After Quantization (FAQ) has matched or exceeded the accuracy of full-precision baseline networks on the Imagenet benchmark for both 8 and 4 bits. It outperforms other quantization methods on state-of-the-art network architectures, showing promising results in terms of accuracy and complexity. The proposed solution, FAQ, has achieved high accuracy on ImageNet benchmark for both 8-bit and 4-bit quantization levels, surpassing full-precision scores after just one epoch. The method involves fine-tuning pretrained high-precision networks for low-precision inference, combating noise during training to discover integer networks. Contributions include demonstrating superior 8-bit scores on various network architectures. The study presents evidence of 4-bit integer networks matching full-precision accuracy on ImageNet, showcasing the state-of-the-art. Gradient noise from weight quantization affects fine-tuning low-precision networks. Larger batches during fine-tuning improve accuracy. Near optimal 4-bit quantized solutions exist close to high-precision solutions, eliminating the need for training from scratch. The goal is to quantize existing networks to 8 and 4 bits without increasing computational complexity. The study aims to train low-precision networks with 8 and 4 bits for weights and activations, achieving high accuracies without increasing computational complexity. The key challenge lies in optimizing quantized networks by addressing the noise introduced during training due to quantization. The study focuses on training low-precision networks with 8 and 4 bits for weights and activations to achieve high accuracies without increasing computational complexity. The challenge is optimizing quantized networks by addressing noise introduced during training, similar to gradient noise in stochastic gradient descent. Strategies to minimize error include starting closer to the solution with pretrained models and minimizing gradient noise level. To combat noise in training low-precision networks, techniques such as larger batches and learning rate annealing are used. Fine-tuning after quantization (FAQ) involves adjusting epochs and utilizing pretrained networks for better accuracy. This approach directly optimizes the final score, rather than using proxies. In the quest for training state-of-the-art low-precision networks, there are various approaches to imposing precision constraints, including non-uniform quantization of weights and activations, stochastic quantization, distillation, layerwise quantization and retraining, introducing noise during training, learning quantization-specific parameters using backpropagation, fine-tuning, using Stochastic Variance-Reduced Gradient, and relaxation methods. Recent attempts at training low-precision networks with integer constraints have shown potential for porting such networks to commercially available hardware for inference. The focus is on training networks with weights and activations constrained to be either 4-bit or 8-bit fixed-point integers, while other scalar values are not constrained. The study focuses on training low-precision networks with integer constraints, specifically 4-bit or 8-bit fixed-point integers for weights and activations. Other scalar values are also constrained, with batch normalization using 8-bit integers and bias values using 32-bit integers. Pretrained high-precision networks are quantized and fine-tuned, with noise being a limiting factor in finding low-precision solutions. Various methods are used to overcome noise during training, including techniques from BID5 and BID6. The quantizer used is parametrized by precision (number of bits) and the location of the least significant bit relative to the radix. A calibration phase is used during initialization to determine a unique least significant bit for each layer of activations. During initialization, a unique least significant bit (l) is determined for each layer of activations and parameter tensors in low-precision networks. All weights and activations are quantized to either 8 or 4 bits, with exceptions for first and last layer weights. The last fully-connected layer allows full-precision linear activations and input is also quantized to 8 bits. In low-precision networks, weights and activations are quantized to 8 or 4 bits, with exceptions for first and last layer weights. The transition from 4-bit to 8-bit is facilitated by the last ReLU activation layer in the network. SGD is used for weight updates, while a fixed-point version is used for inference and gradient calculation. The quantization parameter is updated during every iteration based on a desired quantization step-size. The quantization parameter l for each layer of activation is calibrated using a technique of running training data batches through the unquantized network to determine the maximum range for uniform quantization. The maximum range is determined by finding the 99.99th percentile of the batch of activation tensor for each layer, rounded up to the next even power of two. This process helps in setting the quantization parameter l for each tensor. The quantization parameter for each layer is determined by finding the 99.99th percentile of the activation tensor batch, rounded up to the next even power of two. This parameter is used to quantize the tensor using Q b,l. Activation function parameters are fixed for each tensor and kept during fine-tuning. In experiments starting from random initialization, the maximum range for ReLU activation functions is set to y max = 2 p/2 \u2212 1. Training a quantized network involves updating floating point weights with gradients and quantizing weights and activations in the forward pass. Values exceeding the maximum range are clipped. After quantization, fine-tuning pretrained 8-bit networks requires only one additional epoch with a learning rate of 10^-4. However, for 4-bit networks, matching the performance of the full-precision net requires 110 additional epochs with exponential decay of the learning rate. After quantization, fine-tuning pretrained 8-bit networks requires only one additional epoch with a learning rate of 10^-4. For 4-bit networks like ResNet-18, weight decay is reduced to 0.5 \u00d7 10^-4. The batch size used was 256 split over 2 GPUs with SGD optimization. PyTorch was used for implementation. Trained 8-bit networks outperform other quantization methods and exceed pretrained fp32 network accuracy with minimal training. Sensitivity experiments show that longer training duration, initialization from a pretrained model, larger batch size, lower weight decay, and initial activation affect network accuracy. The study found that larger batch size, lower weight decay, and initial activation calibration improved accuracy when training the 4-bit ResNet-18 network. The learning rate decay schedule had the least impact on accuracy. The experiments showed that the trained 4-bit network accuracy exceeded other quantization methods, with ResNet-18 BID12 surpassing the next closest approach by nearly 0.5%. The networks did not require extensive fine-tuning to surpass pretrained networks. The study found that larger batch size, lower weight decay, and initial activation calibration improved accuracy for the 4-bit ResNet-18 network. The experiments showed that the trained 4-bit network accuracy exceeded other quantization methods, with ResNet-18 BID12 surpassing the next closest approach by nearly 0.5%. Longer fine-tuning -110 epochs- was required for ResNet-18, ResNet-34, and ResNet-50, with accuracy dropping immediately after quantization and requiring significant fine-tuning to match pretrained networks. The study found that larger batch size, lower weight decay, and initial activation calibration improved accuracy for the 4-bit ResNet-18 network. Training longer was necessary, and initializing networks with a discretized pretrained network followed by fine-tuning improved accuracy. Top-1 accuracies of 67.14% and 69.24% were achieved with different learning rate decay schedules. The study explored the impact of various factors on the accuracy of a 4-bit ResNet-18 network. Fine-tuning experiments showed accuracies of 67.14% and 69.24%, slightly below FAQ's accuracy. Neglecting to calibrate activation ranges for each layer led to a 0.63% drop in accuracy. Increasing batch size improved accuracy, with a maximum batch size of 2048 resulting in a top-1 validation accuracy of 69.96%. The study focused on reducing gradient noise to improve final accuracy by using 165 epochs. Results showed that gradient noise limits low-precision training. Replacing exponential learning rate decay with step decay slightly improved results. Increasing weight decay for the 4-bit ResNet-18 network reduced validation accuracy by 0.23%. The smaller ResNet-18 with 4-bit precision had reduced validation accuracy by 0.23% due to insufficient capacity to handle low-precision weights. In contrast, the 4-bit ResNet-34 and 50 networks achieved best results with weight decay of 10^-4. Quantizing weights introduced additional noise in the learning process, affecting the cosine similarity of weight changes in the network. The first conv layer is layer 1, while the fully connected layer is layer 18. Weight discretization increases gradient noise for 8-, 4-, and 2-bit networks. The increase in gradient noise due to weight discretization is measured using cosine similarity. As bit precisions decrease, similarity decreases. The study shows that as bit precisions decrease in quantized networks, there is an increase in gradient noise, affecting fine-tuning and training trajectories. Even with 8-bit precision, noise levels are still significant, impacting performance. Training from scratch may not be necessary, as the 4-bit solution after fine-tuning closely resembles the initial high-precision solution. Cosine similarity between weights at the start and after 110 epochs illustrates this trend. The cosine similarity between the initial weights and the final solution after training from scratch is near 0, indicating significant movement from the initial condition. However, the cosine similarity between the initial weights from the model zoo and the 4-bit solution after FAQ training is close to 1, showing that the 4-bit solution closely resembles the initial high-precision solution used for initialization. The weights of the FAQ trained 4-bit network are similar to those in the full-precision pretrained network. The cosine similarity between the weights of corresponding neurons in the two networks is close to 1 (0.994) after fine-tuning for 110 epochs, indicating that the 4-bit network is similar to its high-precision counterpart due to pretrained initialization. Training from scratch shows a low similarity between initial and final weights (0.023), suggesting it is unnecessary. FAQ trained models are as accurate as full-precision models for ResNet-18 on the CI-FAR10 dataset. The baseline test accuracy was 94.65%, with FAQ 8-bit and 4-bit scores at 94.65% and 94.63% respectively, showing generalization to other datasets. Low-precision quantization followed by fine-tuning achieves state-of-the-art performance for networks using 4-bit and 8-bit weights and activations. Our approach in the 8-bit space requires only one epoch of post quantization training to surpass high-precision network scores consistently. In the 4-bit space, we match high-precision baseline scores with a simpler approach, outperforming published results on ResNet-18, 34, and 50. Noise is identified as the main challenge in successful fine-tuning, with techniques like longer training times, exponential learning rate decay, very low final learning rate, and larger batch sizes contributing to improved results. SGD faces noise from stochastic sampling and quantization, with our techniques potentially reducing one or both sources. Further experiments are needed to explore this further. The success of fine-tuning and the availability of pretrained models signal a shift in training low-precision networks. Experiments suggest that within regions of local minimums for high-precision networks, solutions for lower precision 4-bit nets exist. Fine-tuning for quantization has been studied, with promising results in replacing neurons with low-precision ones. Further experiments are needed to validate these findings. Our results show that low-precision quantization followed by fine-tuning can achieve greater accuracy when quantizing both weights and activations at 4 bits. BID28 demonstrated that a 4-bit ResNet network can match the top-1 accuracy of a full-precision network. Future research includes combining different approaches and developing new training algorithms to address noise introduced by weight quantization. To combat noise from weight quantization, BID0 introduced quantization for 2-bit networks, which poses challenges due to additional noise and potential capacity limits. FAQ offers a principled approach to quantization, aiming to match or surpass validation scores of full-precision networks. This study shows that straightforward methods can achieve performance parity between 8-bit/4-bit quantized networks and their high-precision counterparts, a crucial step towards leveraging energy-efficient low-precision hardware."
}