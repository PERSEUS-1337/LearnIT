{
    "title": "HklXn1BKDH",
    "content": "This work introduces a modular and hierarchical approach for learning policies in 3D environments, combining analytical path planners with learned mappers and global/local policies. Learning provides flexibility in input modalities, leverages world regularities, and enhances robustness to state estimation errors. The hierarchical decomposition and modular training reduce sample complexities. Experiments in simulated 3D environments show the effectiveness of this approach over previous methods in navigation tasks for intelligent agents. Navigation tasks for intelligent agents involve reaching specific coordinates or scenes in known or unknown environments. Exploration is a key challenge, addressed through end-to-end learning for flexibility in input modalities and improved robustness. This approach combines analytical path planners with learned mappers and global/local policies, reducing sample complexities and outperforming previous methods in simulated 3D environments. In this paper, the authors discuss the benefits of using learning for exploration in navigation tasks for intelligent agents. They highlight the advantages of learning in improving robustness and efficiency in previously unseen environments. The authors propose an end-to-end trained neural network based policy that processes raw sensory observations to output actions directly. However, they acknowledge the drawbacks of end-to-end learning for exploration, such as the high cost of learning mapping, state-estimation, and path-planning purely from data. This motivates their work to address these challenges. The authors investigate alternate formulations of using learning for exploration in navigation tasks, focusing on leveraging structural regularities of indoor environments, robustness to state-estimation errors, and flexibility with input modalities. They propose a modular and hierarchical approach within a 'classical navigation pipeline', resulting in navigation policies that work with raw sensory inputs, are robust to errors, and leverage real-world layout regularities for competitive performance. The proposed exploration architecture includes a learned mapper, global policy, and local policy that work together to generate long-term and short-term goals for navigation. The mapper produces free space maps from RGB images, the global policy exploits layout regularities for long-term goals, and the local policy maps images to actions directly. This approach outperforms traditional methods with fewer samples. The proposed exploration architecture includes a learned mapper, global policy, and local policy that generate long-term and short-term goals for navigation. The hierarchical and modular design, along with analytical planning, reduces the search space during training for better performance and sample efficiency. The approach is demonstrated in visually and physically realistic simulators for geometric exploration tasks. The physical realism is improved by using actuation and odometry sensor noise models collected from real mobile robot experiments. Our proposed approach for exploration on a real mobile robot is effective, as shown in realistic simulations. It also performs well in point-goal navigation tasks, winning the AI Habitat challenge at CVPR2019. Classical navigation breaks down into mapping and path planning, with our work combining concepts from classical robotics and learning for navigation. Researchers have developed neural network policies that reason via spatial and topological representations, as well as differentiable and trainable planners. This work focuses on a hierarchical and modular decomposition of the problem, employing learning inside these components instead of end-to-end learning. Research in SLAM focuses on incorporating semantics and active map-building. Works frame the problem as a POMDP and seek to minimize uncertainty in maps through actions like picking vantage points. Recent works have focused on exploration in the context of reinforcement learning, with a particular emphasis on hierarchical and modular policies. These approaches aim to improve learning efficiency by automatically discovering hierarchies. However, challenges remain in this area, leading to the use of hand-defined hierarchies in most cases. In the context of reinforcement learning, recent works have focused on exploration using hierarchical and modular policies. The objective is to train a policy to maximize coverage in a fixed time budget by taking in observations and outputting navigational actions. The experimental setup aims to be realistic for transferring trained policies to the real world using the Habitat simulator. The curr_chunk discusses the use of realistic datasets like Gibson and Matterport for experiments in learning-based navigation. Previous works have made unrealistic assumptions about agent motion and pose estimation, leading to limitations in exploration research. The curr_chunk discusses the implementation of more realistic agent motion and sensor noise models in a simulator to relax assumptions. The agent pose is represented by (x, y, o) with x and y as coordinates in meters and o as orientation in radians. The agent starts at (0, 0, 0) and takes actions implemented as control commands on a robot. The actuation noise is the difference between the actual agent pose after the action. The curr_chunk discusses implementing actuation and sensor noise models in a simulator for navigational actions in the Habitat simulator. Three default actions are used: Forward, Turn Right, and Turn Left, with corresponding control commands. The goal is to collect data for realistic agent motion and sensor noise models. The curr_chunk discusses using a neural network based Mapper to predict a map and agent pose estimate for navigation. A Global policy outputs a long-term goal converted to a short-term goal for a Local Policy to navigate to. Data is collected using a LoCoBot 2 to build actuation and sensor noise models. The pyrobot API and ROS are used for control commands and sensor readings. The curr_chunk introduces the 'Active Neural Mapping' navigation model, consisting of a Mapper, Global policy, and Local policy. It includes Gaussian Mixture Models for actuation and sensor noise, with 6 models in total. The models are implemented in the Habitat simulator for experiments, and the collected data and noise models will be open-sourced. The policy uses a predicted map and agent pose to set a long-term goal, which is then converted into a short-term goal through path planning. The Local policy guides navigational actions based on current observations to reach the short-term goal. The Active Neural Mapping model maintains a spatial map and agent pose, with each element in the map representing obstacle probability and exploration status. The Mapper initializes with zeros at the start of an episode, updating the map and agent pose based on current observations and sensor readings. It consists of a Projection Unit and a Pose Estimator to predict obstacles and agent position. The Mapper uses a Pose Estimator to predict agent pose based on past estimates and egocentric map predictions. The egocentric map is transformed to a geocentric map and aggregated with the previous spatial map to get the current map. The Global Policy takes input from the Mapper to make decisions. The Global Policy model uses transformations on the input data before predicting a long-term goal. It utilizes a 5-layer convolutional neural network for this task. The experiments are conducted using the Habitat simulator with the Gibson and Matterport datasets, which are 3D reconstructions of real-world environments. Gibson mainly consists of office spaces, while Matterport consists of homes with a larger average scene area. The observation and action spaces in the training domain Gibson are defined with RGB images and odometry sensor readings. The objective is to maximize coverage in a fixed time budget by exploring traversable points within 3m. Evaluation metrics are used to measure performance. During training in the Gibson domain, the agent aims to maximize coverage within a 3m radius using RGB images and odometry sensor data. Evaluation metrics include absolute coverage area (Cov) and percentage of area explored (% Cov). The training episodes last for 1000 steps, with train/val/test splits provided by Savva et al. 2019. The Gibson test set is held out on an online evaluation server for the Pointgoal task, with validation used for comparison and analysis. The validation set is not used for hyper-parameter tuning. The Gibson validation set is split into two parts for scene size analysis. The model is trained for the Exploration task in the Gibson domain and then transferred to the Matterport domain. The Projection Unit predicts egocentric projections, while the Pose Estimator predicts agent pose. Global and Local policies are trained using Reinforcement Learning, with rewards based on coverage increase and distance reduction to the goal. All modules are trained simultaneously. The model is trained for the Exploration task in the Gibson domain and then transferred to the Matterport domain. Global and Local policies are trained using Reinforcement Learning, with rewards based on coverage increase and distance reduction to the goal. Modules are trained simultaneously, with parameters independent but data distribution inter-dependent. Actions by the Local policy affect future input to Mapper, changing the map input to the Global policy and impacting the short-term goal. Baselines include RL + 3LConv, RL + Res18, and RL + Res18 + AuxDepth. Code will be open-sourced. The baselines for the model include Res18 + AuxDepth and RL + Res18 + ProjDepth, utilizing depth prediction and sensor pose readings for training with PPO. The proposed ANM model is trained for Exploration task in the Gibson domain and transferred to the Matterport domain, with rewards based on coverage increase and distance reduction to the goal. The proposed ANM model outperforms baselines in the Exploration task on the Gibson training set, achieving higher coverage efficiency. The hierarchical policy architecture reduces the long-term exploration problem by taking fewer long-term goal actions. Domain generalization performance is also reported, showing the model's effectiveness across different domains. The ANM model shows higher domain generalization performance compared to baselines in the Exploration task. It efficiently explores both small and large scenes, outperforming baselines in coverage efficiency. Visualizations of policy executions are provided in the Appendix. The ANM model outperforms baselines in exploring both small and large scenes by using a Global policy on the map for long-term planning. The Local Policy helps adapt to errors in Mapping, improving performance significantly. The Local policy in the ANM model adapts to errors in mapping by overcoming false positives and false negatives. It can understand when obstacles predicted by the Mapper are not actually obstacles based on RGB observations, allowing it to navigate around them. Additionally, the Local policy learns to navigate around obstacles that are not in the field-of-view of the camera, which the deterministic policy would get stuck on. An alternative to learning a Global Policy for sampling long-term goals is to use Frontier-based exploration, which samples points on the boundary between explored and unexplored space. Different sampling strategies exist, with the closest point to the agent on the frontier yielding the best results empirically. However, implementing this variant showed that Frontier-based exploration performed worse than the Global Policy, spending excessive time exploring corners or small areas behind furniture. The trained Global policy in contrast to Frontier-based exploration focuses on distant long-term goals, leading to more efficient area exploration. ANM outperforms baselines even without the Pose Estimator, showing the effectiveness of the additional supervision. The performance of baselines did not improve when using ground truth pose instead of sensor readings. Trained ANM policy is deployed on a Locobot in the real-world for close matching of simulator and real-world observations. In order to closely match real-world observations to simulator observations, the simulator input configuration is adjusted to match the camera intrinsics on the Locobot. The Global policy samples long-term goals towards open spaces in the explored map, indicating learning to exploit the map structure. Success rate and Success weighted by Path Length are used as evaluation metrics. Baseline models for Exploration task need to be retrained or fine-tuned for transfer. The ANM policy, originally trained for Exploration, can be transferred to the Pointgoal task without additional training by fixing the Global policy to output PointGoal coordinates. The transferred ANM model outperformed RL and Imitation Learning baselines on the Pointgoal task, achieving a success rate/SPL of 0.950/0.846 compared to 0.827/0.730 for the best baseline model. It also generalized better to harder goals and the Matterport domain, while being 10 to 75 times more sample efficient than the baselines. The ANM policy, winner of the CVPR 2019 Habitat Pointgoal Navigation Challenge, outperformed baselines by 10 to 75 times in sample efficiency. The model allows for knowledge transfer in obstacle avoidance and control, showing strong generalization across tasks, domains, and goals. The proposed model can be extended to complex semantic tasks such as Semantic Goal Navigation and Embodied Question Answering using a semantic Mapper. It can also be combined with prior work on Localization for efficient navigation in subsequent episodes. The PointGoal task has been extensively studied in recent literature on navigation, with the model following the setup from Savva et al. 2019. The agent is tested on new scenes never seen during training, with the Gibson test set held out. The model's performance is evaluated on the Gibson test set, which is not public but held out on an online evaluation server. The validation set is also used for comparison and analysis, but not for hyper-parameter tuning. The difficulty of the PointGoal dataset is quantified using average geodesic distance and GED ratio. The train/val/test splits in the Gibson dataset have similar average geodesic distance and GED ratio distributions. The proposed model's performance is evaluated on harder sets, Hard-Dist and Hard-GEDR, with specific geodesic distance and GED ratio criteria. Episodes end when the agent stops or reaches 500 timesteps, with success defined as being within 0.2m of the goal. Success rate and Success weighted by Path Length (SPL) are used as evaluation metrics for the PointGoal task. The proposed model achieves a success rate/SPL of 0.950/0.846 on the PointGoal task, outperforming the best baseline model. The model allows for knowledge transfer of obstacle avoidance and control in navigation tasks, with successful trajectories at long distances. The proposed model shows successful trajectories for long distance goals with high GED ratio. It outperforms RL models in sample efficiency, achieving a speedup of >75\u00d7. ANM surpasses all baselines in generalization sets, including harder goal sets in Gibson domain. The results demonstrate that the proposed ANM model outperforms all baselines in generalization sets, especially on harder goal sets. It shows strong performance in long-term planning for distant goals, exhibiting successful trajectories with backtracking behavior. The model's effectiveness is highlighted in comparison to RL methods, which struggle on the Hard-Dist set. Additionally, the performance of ANM compared to baselines CMP and IL + Res18 + GRU is analyzed based on geodesic distance to goal, showing ANM's superior performance, particularly with increasing goal distance. In the Habitat simulator, data is collected for navigational actions using default actions like Forward, Turn Right, and Turn Left. Actuation and sensor noise models are built using a Locobot 7 and control commands implemented with the pyrobot API and ROS. The robot can experience rotational actuation noise in forward action and translation actuation noise in on-the-spot rotation actions. To accurately determine agent pose, an expensive Hokuyo UST-10LX LiDAR sensor is used on the Locobot, costing $1600. While costly sensors can enhance model performance, scalability requires compatibility with cheaper sensors. The LiDAR is utilized solely for data collection to build noise models, not for training or deploying navigation policies. The Kobuki base odometry is used for sensor estimation, approximating the LiDAR pose as the true agent pose. The LiDAR pose estimate is used as the true agent pose on Locobot due to its high accuracy. 600 datapoints are collected for each action from both the base sensor and LiDAR, totaling 3600 datapoints. Gaussian Mixture Models are fitted for actuation and sensor noise using these datapoints, with the model selection based on validation set likelihood. The Gaussian mixture models for actuation and sensor noise are implemented in the Habitat simulator. The Mapper in the simulator takes RGB observations, sensor readings, and maps to update the map. It consists of a Projection Unit and a Pose Estimator Unit, with the Projection Unit predicting obstacles and explored areas in the current observation. The Projection Unit in the Habitat simulator uses Resnet18 convolutional layers to create an embedding of the observation, followed by fully-connected and deconvolutional layers to predict a 2D spatial map. The egocentric map prediction is transformed to the geocentric frame using the Pose Estimator to correct sensor readings and estimate the agent's geocentric pose. The relative pose change is calculated from sensor readings at the current and last time step, and a Spatial Transformation is applied to the egocentric map. The Pose Estimator Unit utilizes a Spatial Transformation to predict the relative pose change based on egocentric map predictions from the last two frames. This predicted change is then added to the last pose estimate to obtain the final pose estimation. The egocentric spatial map prediction is transformed to the geocentric frame using the current pose prediction of the agent and aggregated with the previous spatial map using Channel-wise Pooling operation. PyTorch is used for implementing and training the model, with specific architectures for the Projection Unit, Pose Estimator, Global Policy, and Local Policy. The Global and Local policies in the model are trained using Reinforcement Learning, with rewards based on coverage increase and distance reduction to the goal. The implementation is based on PPO, and input to the Local policy includes relative distance, angle, timestep, and last action. Discretization of input values improves sample efficiency compared to using continuous values directly. The model trains Global and Local policies using Reinforcement Learning with rewards based on coverage increase and distance reduction to the goal. Discretization of input values improves sample efficiency. Training components with 72 parallel threads and using Proximal Policy Optimization for training. Adam optimizer with specific learning rates is used for training units in the Mapper and policies. The model trains Global and Local policies using Reinforcement Learning with specific parameters. Input frame size is 128 \u00d7 128, with a vision range for mapper of V = 64. Map size is adaptive, with training done on M = 960. For Pointgoal evaluation, the goal is within the central 50% of the map. Exploration task uses a constant map size of M = 960, with Global Policy input size of G = 240."
}