{
    "title": "BywyFQlAW",
    "content": "Minimax curriculum learning (MCL) is a method for adaptively selecting training subsets in machine learning. The subsets start small and diverse, then become larger, harder, and more homogeneous. Model weights and training sets are chosen through a minimax optimization process. MCL achieves better performance and requires fewer labeled samples for both shallow and deep models. The method involves solving constrained submodular maximization on a ground set, utilizing a heuristic method for efficient computation. Inspired by human interaction in teaching, curriculum learning and self-paced learning improve learning algorithms by designing training sequences. The curriculum, self-paced learning (SPL) BID31 BID58 BID57 BID59 allows the algorithm to choose samples based on their hardness during training. SPL jointly learns model weights w and sample weights \u03bd via alternating minimization, selecting samples with loss < \u03bb. Self-paced curriculum learning BID27 focuses on designing training sequences to improve learning algorithms. Self-paced curriculum learning introduces a blending of \"teacher mode\" and \"student mode\", where a linear constraint is attached to define a region of \u03bd. SPL with diversity adds a negative group sparse regularization term, preferring samples from different groups. CL, SPL, and SPLD can be seen as a continuation scheme that handles tasks moving from easy to hard. Continuation schemes like SPL and SPLD increase parameters between training rounds to avoid local minima and improve generalization error. Selection of training samples is also important in active learning and experimental design. Active learning aims to reduce annotation costs by selecting informative samples from an unlabeled pool during training. Diversity modeling in active learning uses submodular maximization to choose diverse training batches from uncertain samples. Boosting involves learning an ensemble of weak classifiers sequentially by assigning weights to samples based on loss. Both active learning and boosting prioritize difficult-to-predict samples for learning. The SPL approach in active learning selects samples starting with easy ones and gradually increasing difficulty. SPLD addresses the issue of slow learning by increasing diversity in sample selection over rounds. In SPLD, early stages train on easier samples and later stages train on harder, more diverse samples. Challenges include the possibility of repeatedly selecting similar training sets in early stages and the question of whether a large diversity selection weight in late stages is desirable. It may be best to primarily select the hardest samples near difficult decision boundaries even with a well-trained model. In SPLD, early stages train on easier samples and later stages train on harder, more diverse samples. Challenges include the possibility of repeatedly selecting similar training sets in early stages and the question of whether a large diversity selection weight in late stages is desirable. It may be best to primarily select the hardest samples near difficult decision boundaries even with a well-trained model. Furthermore, a new form of CL is introduced that chooses the hardest diverse samples in early rounds of training and then decreases diversity as training rounds proceed. During early training stages, diversity is crucial as fewer samples are selected. Later rounds naturally offer more diversity due to larger sample sizes. To prevent selecting similar sets, the approach chooses the hardest samples each round based on loss function. Diversity is controlled by a parameter \u03bb and measured by a non-monotone submodular function. The group sparse norm is also submodular for binary variables. Our approach allows the use of submodular functions to measure diversity in the selection phases. This concept is supported by the natural progression of education, where early stages cover a broad range of topics for exposure to diverse knowledge, while later stages focus on deeper knowledge within specific areas. Studies on bilingualism also show the benefits of learning multiple languages in childhood for brain development. Minimax curriculum learning (MCL) is a new approach that increases desired hardness and reduces diversity encouragement in multi-lingual learning. It involves solving a sequence of minimax optimizations to balance the trade-off between hardness and diversity in training. The objective includes evaluating the hardness of samples and measuring their diversity using submodular functions. This approach mimics the natural progression of education, where early stages focus on diverse knowledge and later stages on deeper knowledge within specific areas. The Minimax curriculum learning (MCL) approach balances hardness and diversity in training by scheduling learning rounds with increasing k and decreasing \u03bb. Unlike SPL/SPLD, MCL explicitly schedules the number of selected samples via k, eliminating the need for an explicit hardness parameter. The MCL optimization minimizes the loss on any size k subset of training data. The MCL optimization minimizes an upper bound of the loss on any size k subset of training samples using submodular functions for diversity measurement. Submodular functions have been widely used for diversity models. The optimization involves both continuous variables w and discrete variables A, which can be reduced to the minimization of a piecewise function. Each piece is convex when the loss is convex. The optimization involves minimizing a piecewise function defined by an approximate A computed via submodular maximization. The minimax problem can be seen as a two-person zero-sum game between a teacher and a student, where the teacher chooses training set A based on feedback and the student updates w to reduce loss. Fast approximate algorithms exist to find an approximately optimal A due to submodularity. The teacher-student interaction involves the teacher introducing concepts and asking easy questions, then training the student on difficult topics. MCL's minimax formulation differs from min-min formulation in SPL/SPLD. Minimizing worst case loss is a common strategy in machine learning for better generalization performance. MCL in machine learning aims to achieve better generalization performance and model robustness by addressing the minimax problem through convex optimization. The goal is to minimize the objective function g(w) by solving a submodular maximization problem. This approach is different from SPL/SPLD's min-min formulation and involves discrete optimization over a weighted sum. The fast greedy procedure can be used to optimize G w (A) for any weight w, which is a monotone non-decreasing submodular function. The approximate objective \u011d(w) satisfies \u03b1g(w) \u2264 \u011d(w) \u2264 g(w), where \u03b1 is the approximation factor of submodular optimization. \u011d(w) is piecewise convex if the loss function is convex with respect to w. Minimizing \u011d(w) provides an approximate solution to the minimax problem in machine learning. The algorithm offers an approximate solution to Eq. (2) using gradient descent for minimizing \u011d(w). The key is obtaining \u011d(w) through submodular maximization on the optimization path. The Minimax Curriculum Learning (MCL) algorithm details the optimization process with \u03bb and k. The Minimax Curriculum Learning (MCL) algorithm optimizes Eq. (2) using submodular maximization and gradient descent on \u011d(w) with parameters \u03bb and k. The inner optimization updates w with gradient descent, while the outer optimization adjusts \u03bb and k until convergence is reached after T steps. Different gradient descent methods can be applied based on the size of k, with more complex rules considering historical gradients. The algorithm approximately solves a sequence of Eq. (2)s with decreasing \u03bb and increasing k, using submodular maximization and gradient descent. It starts with A \u2190 \u2205 and selects elements with the largest marginal gain until |A| = k. The greedy algorithm is simple, fast, and outperforms other methods, requiring O(nk) function evaluations for ground set size |V | = n. The greedy algorithm, which starts with an empty set A and selects elements with the largest marginal gain until |A| = k, requires O(nk) function evaluations for ground set size |V| = n. The accelerated lazy greedy algorithm BID40 reduces the number of evaluations per step by updating a priority queue of marginal gains, offering significant speedups while maintaining the same output and guarantee as the original. Additionally, a faster variant BID41 is available. The greedy procedure provides better approximation factors than 1 \u2212 e^\u22121 when the objective G(A) is close to modular, with an approximation factor of \u03b1 = (1 \u2212. The objective G(A) approaches modularity with a curvature \u03ba G \u2208 [0, 1]. As \u03bb decreases, G(A) becomes more modular, leading to improved approximation with the number of outer loops. In MCL, the submodular approximation enhances as \u03bb increases, approaching the true convex objective g(w). The solution\u0175 from applying gradient descent to \u011d(w) converges when p is sufficiently large. In Theorem 1, the upper bound on \u0175 \u2212 w * 2 2 is analyzed under the assumption of \u03b2-strongly convex loss L. Gradient descent in Algorithm 1 converges when the gradient reaches zero. Adding regularization can modify a convex loss to be \u03b2-strongly convex. Noise perturbed SGD can find an -optimal local solution of g(w) in polynomial time. MCL shows good performance on non-convex deep neural networks. Theorem 1 states that for the minimax problem with a \u03b2-strongly convex loss function, running Algorithm 1 until convergence yields a solution \u0175. The bound depends on \u03b2 and the submodular maximization approximation \u03b1. As \u03bb decreases, \u03b1 approaches 1, indicating a tighter bound. The approximation factor \u03b1 approaches 1 as \u03bb decreases, improving the bound in Equation FORMULA13. The algorithm will eventually converge to the minimum of a convex piece as it selects one convex piece associated with the region around w t \u03c4. The bound can be simplified in terms of parameters and \u03bb, k values used at a particular iteration \u03c4. The solution achieved by Algorithm 1 at iteration \u03c4 of the outer loop fulfills certain conditions if the loss function is \u03b2-strongly convex and the submodular function has curvature \u03ba F. The optimal solution with \u03bb, k set as in iteration \u03c4 is denoted w * T. The values of \u03bb and k can either linearly increase or increase exponentially during the iterations. The proof for Algorithm 1 can be found in Appendix 4.5. The upper bound is in terms of the ratio \u03bb/k, which improves with larger subset sizes. Algorithm 1 chooses a schedule to decrease \u03bb exponentially and increase k only linearly. The bound depends on submodular curvature \u03ba F, strongly-convex constant \u03b2, and c 1. Heuristic improvements are employed, such as stopping gradient descent after p steps for warm-start. No benefit is observed for larger p, but eventual convergence is observed. The proof for Algorithm 1 can be found in Appendix 4.5. The upper bound is in terms of the ratio \u03bb/k, which improves with larger subset sizes. Algorithm 1 chooses a schedule to decrease \u03bb exponentially and increase k only linearly. The bound depends on submodular curvature \u03ba F, strongly-convex constant \u03b2, and c 1. Heuristic improvements are employed, such as stopping gradient descent after p steps for warm-start. No benefit is observed for larger p, but eventual convergence is observed. We also have not observed any benefit for larger p, although we do eventually observe convergence empirically when the average loss no longer change appreciably between stages. Also, lines 6-7 of Algorithm 1 require computing the loss on all the samples, and each step of the greedy algorithm needs to, in the worst case, evaluate the marginal gains of all of the unselected samples. Moreover, this is done repeatedly in the inner-most block of two nested loops. Therefore, we use two heuristic tricks to improve efficiency. Fist, rather than selecting individual samples, we first cluster the data and then select clusters, thereby reducing the ground set size from the number of samples to the number of clusters. We replace the per-sample loss L (y i , f (x i , w)) with a per-cluster loss L Y (i) , f (X (i) , w) that we approximate by the loss of the sample closest to the centroid within each cluster. The loss on samples in selected clusters indicates cluster hardness, reducing ground set size and speeding up the algorithm. Gradient computed on all samples in selected clusters, improving efficiency. Labels of centroid samples sufficient for cluster selection. Annotate and compute loss for samples in selected clusters and centroid samples of other clusters. In this section, different curriculum learning methods are applied to train logistic regression models on 20newsgroups, LeNet5 models on MNIST, convolutional neural nets (CNNs) on CIFAR10 and Fashion-MNIST datasets. The ground set can be reduced to save computation during submodular maximization, and pruning methods become more effective as the MCL objective becomes more modular. More details are provided in Section 4.6. In this section, various curriculum learning methods are compared on different datasets using logistic regression models, LeNet5 models, and CNNs. The methods include MCL and its variants, as well as SPL and SGD with a random curriculum. The training sets vary in size and increase gradually during training. The study compares curriculum learning methods using logistic regression models, LeNet5 models, and CNNs on different datasets. SPL and SPLD have varying parameters like \u03c1, \u00b5, and \u03be to minimize loss w.r.t. the model. They do not use the clustering trick applied to MCL and compute the exact loss on each sample in each iteration for accurate estimation of sample hardness. The study compares curriculum learning methods using logistic regression models, LeNet5 models, and CNNs on different datasets. SPL and SPLD have varying parameters like \u03c1, \u00b5, and \u03be to minimize loss w.r.t. the model. They do not use the clustering trick applied to MCL and compute the exact loss on each sample in each iteration for accurate estimation of sample hardness. SPL/SPLD still requires clustering and group sparsity, but does not select samples like MCL. Mini-batch k-means algorithm is applied to features for clustering in MCL and SPLD. In MCL experiments, a simple \"feature based\" submodular function BID61 is used with TF-IDF features for 20newsgroup. Neural networks are trained on small random subsets of training data for one epoch to generate features. ReLU activations ensure nonnegative features for mini-batch k-means clustering in MCL. In MCL experiments, different variants are considered with varying parameters such as inner loop iterations and regularization weights. The number of inner loop iterations is set to p \u2264 50, with specific values chosen based on training loss reduction. Five variants of MCL are explored, each with different settings for submodular regularization and scheduling of k. In MCL experiments, different variants are considered with varying parameters such as inner loop iterations and regularization weights. The number of inner loop iterations is set to p \u2264 50, with specific values chosen based on training loss reduction. Five variants of MCL are explored, each with different settings for submodular regularization and scheduling of k. The algorithm uses different combinations of {q, r} for MCL-RAND(r,q) and different \u2206 values for MCL(\u2206 > 0, \u03bb > 0, \u03b3 > 0), reporting the one with the smallest test error. In MCL experiments, various variants with different parameters are considered. The time cost for MCL and its variants is reported in TAB2, with additional details in FIG1-8. The legend in the figures explains the parameters used for different methods. MCL and its variants use a clustering trick, while SPL/SPLD require knowledge of all sample labels. The left plot shows the number of loss gradient calculations needed for MCL and SPL/SPLD. MCL and its variants can train without needing all labels, reducing annotation costs. They outperform SPL and SPLD in final test accuracy with comparable efficiency. MCL is slightly slower to converge but achieves better results. MCL is slower to converge but achieves smaller error with the same number of labeled samples. MCL variants show better performance by decreasing diversity as training progresses. A large diversity encouragement is not necessary or beneficial. The combination of MCL and random curriculum speeds up convergence and improves final test accuracy on datasets like MNIST, SVHN, and Fashion-MNIST. However, it requires more labeled samples for gradient computation and cannot outperform MCL with certain parameters. Submodular regularization introduces diversity that enhances performance, and adjusting both hardness and diversity leads to better results. The proof shows that the function g(x) is \u03b2-strongly convex by considering the minimax problem and the properties of the loss function. The convex function g(w) achieves a minimum at w*, allowing for further calculations to be made. The proof demonstrates the strong convexity of function g(w) at the minimum w*. The function \u011d(w) is non-convex due to its piecewise nature, leading to the possibility of \u0175 being a global or local minimum. Saddle points are absent in \u011d(w) as each piece is convex. The inequalities g(\u00b7) \u2265 \u011d(\u00b7) and global minima are discussed. The inequalities and optimality of the function g(w) are discussed, with a focus on the convexity and global minima of the function. The proof highlights the strong convexity of g(w) at the minimum w*, contrasting it with the non-convex nature of \u011d(w) due to its piecewise structure. The absence of saddle points in \u011d(w) is also noted. The proof discusses inequalities and optimality of the function g(w), highlighting convexity and global minima. It shows that g(w) is globally optimal, while \u011d(w) is non-convex due to its piecewise nature. The absence of saddle points in \u011d(w) is also noted. The proof discusses inequalities and optimality of the function g(w), highlighting convexity and global minima. It shows that g(w) is globally optimal, while \u011d(w) is non-convex due to its piecewise nature. The absence of saddle points in \u011d(w) is also noted. The algorithm repeatedly runs a greedy procedure to solve submodular maximization, with nested loops. Speeding up this process is described by decreasing \u03bb exponentially and increasing k linearly, and tolerating more computational cost for larger budgets. Exponential increase in k is also considered for more expensive computational costs. The algorithm speeds up the submodular maximization process by reducing the ground set size before a costly procedure. Elements are sorted and removed based on a specific criterion, improving efficiency without compromising approximation quality. Other methods can also be used for slight reductions in quality. The key contribution of this section is a method exploiting a warm start set to achieve a sufficient approximation quality in submodular maximization. The algorithm starts with a previous iteration's solution for a constrained submodular maximization problem, which can nearly satisfy the desired approximation bound for the current function. The succession of submodular functions may not change quickly due to weight updates and parameter changes, leading to efficient optimization. Our method utilizes a warm start set to efficiently optimize submodular maximization. It starts with a previously computed set and offers a trade-off between speed and approximation quality. The approach tests if the warm start set already achieves a sufficient approximation quality and improves it further if needed. It uses a simple modular function upper bound to compute an upper bound on the global maximum value and a submodular semigradient approach for improvement. The algorithm utilizes a warm start set for submodular maximization, with a trade-off between speed and approximation quality. It employs a submodular semigradient approach and a simple modular function upper bound for improvement. The algorithm uses a warm start set for submodular maximization, employing a subgradient optimization procedure to potentially improve the upper bound. It selects a subgradient defined by a permutation of elements and defines a modular function that is tight at the warm-start set. The algorithm utilizes a warm start set for submodular maximization, employing a subgradient optimization procedure to potentially enhance the upper bound. It selects a subgradient defined by a permutation of elements and defines a modular function that is tight at the warm-start set. The specific permutation used is described, leading to a modular lower bound for fast maximization. The algorithm offers a heuristic in lines 6-9 that can improve the objective, with an approximation factor \u03b1 distinct from the submodular maximization factor achieved by the greedy algorithm. In practice, a more lenient bound (often \u03b1 = 1/2) is used as a tradeoff between accuracy and speed. The time cost for WS-SUBMODULARMAX increases if \u03b1 = 1 by a factor ranging from about 3 to 5. The final bound is based on the smaller of \u03b1 and \u03b1."
}