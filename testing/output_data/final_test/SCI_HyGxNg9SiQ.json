{
    "title": "HyGxNg9SiQ",
    "content": "To enable deep neural networks on resource-constrained devices, quantizing models with low-precision weights is beneficial. The straight-through gradient method is a common technique for quantizing neural networks, allowing back-propagation through the quantization mapping. Despite its success, little is known about why this method works. Building upon the straight-through gradient method, ProxQuant proposes a more principled approach for quantized network training by formulating it as a regularized learning problem and optimizing it with the prox-gradient method. ProxQuant outperforms state-of-the-art results on binary quantization for ResNets and LSTMs, challenging the indispensability of the straight-through gradient method. The PROXQUANT algorithm introduces a quantization-inducing regularizer onto the loss function and optimizes using the prox-gradient method with a finite \u03bb. The algorithm is described in Algorithm 1, utilizing a regularizer R, initialization \u03b8 0, learning rates {\u03b7 t} t\u22650, and regularization strengths {\u03bb t} t\u22650. PROXQUANT algorithm introduces a quantization-inducing regularizer onto the loss function and optimizes using the prox-gradient method with a finite \u03bb. The inner SGD step can be replaced by Momentum SGD or Adam. BinaryConnect has a stringent convergence condition for finding a parameter \u03b8 with low loss using stochastic gradients at {\u00b11} d. The PROXQUANT algorithm introduces a quantization-inducing regularizer onto the loss function and optimizes using the prox-gradient method with a finite \u03bb. It performs a gradient update at the current real vector followed by a prox step which encourages quantizedness. Additionally, it compares the straight-through gradient method with the PROXQUANT method. The PROXQUANT algorithm introduces a quantization-inducing regularizer onto the loss function and optimizes using the prox-gradient method with a finite \u03bb. It performs a gradient update at the current real vector followed by a prox step which encourages quantizedness. The derivatives of f1 and f\u22121 coincide at {\u22121, 1}, so any algorithm that only uses this information will have identical behaviors on these two functions. However, the minimizers in {\u00b11} are x 1 = \u22121 and x \u22121 = 1, so the algorithm must fail on one of them. This architecture is then made into a specially designed hardware for efficient inference. The straight-through gradient method is commonly used in training quantized nets, but its effectiveness is not well understood. It fails on simple convex Lipschitz functions and is equivalent to Nesterov's dual-averaging method. In the binary case, it minimizes L(\u03b8) over Q = {\u00b11} d using a lazy projected SGD approach. The straight-through gradient method is used in training quantized nets, but its effectiveness is not well understood. It fails on simple convex Lipschitz functions and is equivalent to Nesterov's dual-averaging method. In the binary case, it minimizes L(\u03b8) over Q = {\u00b11} d using a lazy projected SGD approach. The projection is also a limiting proximal operator with a suitable regularizer, allowing for more generality in the proposed algorithm. The PROXQUANT algorithm introduces a regularization approach for model quantization, utilizing a flexible class of quantization-inducing regularizers. This method involves a proximal gradient descent with \u03bb = \u221e for \"hard\" projection onto a discrete set Q, transitioning to a \"soft\" projection as \u03bb < \u221e. The use of a finite \u03bb is less aggressive, potentially preventing overshoot during training and providing access to more information than the straight-through gradient method. The proposed method introduces a regularization approach for model quantization, utilizing L1 and L2 regularizers to achieve ideal quantization. This framework allows for flexibility in designing regularizers by specifying a set Q and choosing between L1 and L2. By appropriately selecting Q, one can determine which part of the parameter vector to quantize and the number of bits to quantize to. The regularization approach for model quantization involves choosing between L1 and L2 regularizers to achieve ideal quantization. Different distance metrics lead to distinct properties in the solutions, such as inducing exact quantizedness with L1 norm regularization. Examples of regularizers under this framework include binary weights, ternary weights, and multi-bit quantization, with efficient algorithms for solving the prox operators. In binary neural nets, the entries of \u03b8 are in {\u00b11}. In binary neural nets, the entries of \u03b8 are in {\u00b11}. The prox operator with respect to R bin admits a simple analytical solution despite being a non-convex optimization problem. It is advantageous to keep biases and BatchNorm layers at full-precision. The prox operator generalizes the alternating minimization procedure in previous work by governing a trade-off between quantization and closeness to \u03b8. Ternary quantization is a variant of 2-bit quantization where weights are constrained to {-\u03b1, 0, \u03b2}. The prox operator moves \u03b8 towards the quantized set as \u03bb increases. The goal is to converge to exact quantization without being too aggressive. The homotopy method aims to force the net to be quantized upon convergence without being overly aggressive. A linearly increasing sequence \u03bb t is used as a regularization rate, gradually moving towards exact quantizedness during training. The parameter \u03bb can be tuned to control the aggressiveness of falling onto the quantization constraint. This method is applied to language modeling with LSTMs on the Penn Treebank dataset. The study by Marcus et al. (1993) involves training a standard LSTM model with 325 embedding dimension and 300 hidden dimension. They experiment with quantized LSTMs using different bit levels and compare the results with binary LSTMs. The performance of their PROXQUANT method is comparable to the Straight-through gradient method, and it outperforms BinaryConnect on binary LSTMs. This demonstrates the effectiveness of PROXQUANT for training recurrent networks. In the study, PROXQUANT is compared to BinaryConnect for training recurrent networks. The sign change metric is used to measure the distance between full-precision and binary parameters during training. Despite PROXQUANT converging to higher-performance solutions, the sign change of BinaryConnect is unexpectedly higher when starting from the same warm start. PROXQUANT produces binary nets with lower sign change compared to BinaryConnect. A clever trick involves scaling the alternating quantization before taking the gradient, resulting in improved performance. PROXQUANT outperforms the unscaled ALT straight-through method in terms of performance per watt (PPW). PROXQUANT method outperforms BinaryConnect in terms of performance per watt (PPW) by scaling alternating quantization before taking the gradient."
}