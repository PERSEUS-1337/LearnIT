{
    "title": "H1e_cC4twS",
    "content": "Recent efforts in Dialogue State Tracking (DST) for task-oriented dialogues have advanced towards open-vocabulary or generation-based approaches, showing good performance in dynamic dialogue domains. However, these approaches lack the ability to learn signals across domains and slots and incur high time costs with auto-regressive models. A novel framework called Non-Autoregressive Dialog State Tracking (NADST) is proposed in this paper to optimize models for better prediction of dialogue states by considering dependencies among domains and slots. The Non-Autoregressive Dialog State Tracking (NADST) method improves prediction of dialogue states by detecting dependencies among slots at token level, achieving state-of-the-art accuracy on MultiWOZ 2.1 corpus. This method reduces latency for real-time dialogue response generation and outperforms previous models in task-oriented dialogues. Dialogue State Tracking (DST) is crucial for task-oriented dialogue systems to identify user goals expressed as (slot, value) pairs. Existing DST models are categorized into fixed and open-vocabulary types, with recent approaches focusing on open-vocabulary models that generate candidates from dialogue history. These models rely on autoregressive encoders and decoders to sequentially encode dialogue history and generate slot values. Auto-regressive approaches in Dialogue State Tracking (DST) incur high time costs due to the length of dialogue history and slot values, especially in complex dialogues. This is similar to challenges faced in Neural Machine Translation (NMT) research. Recent work aims to improve efficiency in generating real-time dialogue responses. In a task-oriented dialogue, the system recommends the Cambridge Corn Exchange for a theatre, provides its phone number, and then asks about train details for a Tuesday trip from Cambridge to London Liverpool Street. The text discusses a non-autoregressive approach to minimize latency in neural machine translation models, inspired by previous research on generating tokens independently. The concept of fertility, which determines how many times each input token is copied to form a sequence for the decoder, is adopted to improve efficiency without compromising model performance. The text introduces a two-stage decoding process for non-autoregressive decoding in neural machine translation models. It involves reconstructing dialogue state as a structured sequence using the fertility concept, where each input token is copied to form a sequence for the decoder. The structured sequence consists of sub-sequences represented as (slot token\u00d7slot fertility), which is used as input to generate all tokens of the target dialogue state at once. Our approach for dialogue state tracking (DST) models considers dependencies at both slot and token levels, unlike existing models that assume independence among slots. By detecting relations between different slot values, such as train departure and destination, our model optimizes towards Joint Accuracy, a DST evaluation metric. Our contributions in this work include proposing a novel framework of Non-Autoregressive Dialog State Tracking (NADST) that learns inter-dependencies across slots, achieving state-of-the-art performance on the MultiWOZ 2.1 dataset, and reducing inference latency significantly. Our work introduces a novel framework for Non-Autoregressive Dialog State Tracking (NADST) that can detect signals across dialogue domains to generate accurate slot sets for DST. This approach combines NLU and DST to address the credit assignment problem and eliminate the need for NLU in dialogue state tracking. Differentiating between fixed-vocabulary and open-vocabulary DST approaches, our framework achieves state-of-the-art performance on the MultiWOZ 2.1 dataset. Recent work has shifted towards open-vocabulary approaches for dialogue state tracking, generating candidates based on input text. Unlike previous methods, our approach explicitly considers dependencies among slots and domains to decode dialogue state comprehensively. Prior work on non-autoregressive decoding methods in Neural Machine Translation (NMT) has focused on fast translation needs. Schwenk (2012) proposed estimating translation model probabilities in a phase-based NMT system, while Libovick\u1ef3 & Helcl (2018) formulated decoding as a sequence labeling task using CTC loss. In non-autoregressive NMT, various methods have been proposed to improve translation accuracy, such as regularization terms to reduce errors, non-autoregressive decoders with masked attention, and handling sequential latent variables. Dialogue state is reformulated as a structured sequence with sub-sequences defined by slot values for easier inference. Our approach applies a non-autoregressive framework for generation-based DST, allowing joint state tracking across slots for better performance and lower latency during inference. The NADST model consists of encoders, fertility decoder, and state decoder, with input including dialogue history and applicable (domain, slot) pairs. The output is the corresponding dialogue states up to the current history. The dialogue state output is reformulated as a concatenation of slot values, using token-level embedding and positional encoding. Encoders process input dialogue history and (domain, slot) pairs to generate fertility signals for each pair. A slot gating mechanism is added for auxiliary prediction. The slot gating mechanism with 3 possible values is used to support fertility decoding in the dialogue state output. The state decoder incorporates information from previous dialogue turns to predict the current turn state. The dialogue history is delexicalized to preserve key details for the model input, utilizing predicted state from DST model without relying on an NLU module. The system action in the previous turn is also used for delexicalization. The NADST model consists of encoders, fertility decoder, and state decoder to encode dialogue history and generate dialogue state sequences. It utilizes attention mechanisms to learn dependencies across domain and slot pairs. Feedforward, residual connection, and layer-normalization layers are not shown in the figure for simplicity. The NADST model encodes dialogue history into continuous representations for dialogue state generation, addressing the OOV challenge. Each (domain, slot) pair is encoded for contextual signals during decoding. The context encoder includes trainable embedding and positional encoding layers for combining token-level vectors. The embedding weights are shared to encode domain and slot tokens in both fertility decoder and state decoder, with sequential information injected for position-wise decoding. Domain and slot pairs are encoded into continuous representations for dialogue state generation. In summary, X ds and X ds\u00d7fert are encoded using concatenation operation. Output tokens are generated based on all positions of the sequence in fertility decoder and state decoder. Contextual signals are learned and passed into each z ds vector through a sequence of attention layers using multi-head attention mechanism. The mechanism involves scaled dot-product attention between query, key, and value. Each multi-head attention is followed by a position-wise feed-forward network with two linear layers and a ReLU activation. The fertility decoder consists of 3 attention layers that learn contextual signals and incorporate them into input vectors for the next layer. The multi-head structure has shown good performance in NLP tasks like NMT and QA. This attention mechanism allows models to obtain signals of potential dependencies across domain and slot pairs. The attention mechanism involves capturing dependencies across domain and slot pairs. Adding delexicalized dialogue history as input provides contextual signals for the model to learn mappings. Repeating the attention sequence multiple times enhances the model's capability to capture these dependencies. The output from the last attention layer is used to predict fertilities and gates through linear transformations. Training is done using standard cross-entropy loss. The input sequence X ds\u00d7fert is filtered for \"none\" or \"dontcare\" gates. Contextual signals are incorporated using attention sequences in the decoding stage. The final output Z Tstate ds\u00d7fert predicts the state using W state \u2208 R d\u00d7 V. Pointer networks are used to address OOV problems during inference. The final probability of predicted state is calculated using dot-product attention between the decoder output and the encoded dialogue history. The state generation is trained using cross-entropy loss function on the MultiWOZ dataset, which includes multiple dialogue domains. The dialogues are pre-processed by tokenizing and lower-casing before training. The dialogues are pre-processed by tokenizing, lower-casing, and delexicalizing system responses. Label smoothing is used for training dialogue state prediction. Teacher-forcing learning strategy is adopted during training. Inference follows a similar strategy as Lei et al. for generating dialogue state. During inference, dialogue state is generated turn-by-turn using predicted belief state from the previous turn. X ds\u00d7fert is constructed using prediction\u0176 gate and\u0176 fert. The Adam optimizer is used with a learning rate strategy similar to Vaswani et al. Best models are selected based on joint accuracy in the validation set. Parameters are randomly initialized with a uniform distribution. No pretrained embedding weights are used. Hyper-parameters are tuned using grid-search over the validation set. Models are implemented in PyTorch and code is available on GitHub. DST baselines are categorized into open-vocabulary and fixed-vocabulary approaches. The DST baselines are divided into open-vocabulary and fixed-vocabulary approaches. Fixed-vocabulary has the advantage of known candidate sets for each slot but struggles with unseen slot values during inference. GLAD and GCE are models that use self-attentive RNNs to track system actions and compute semantic similarity with ontology terms. MDBT includes separate encoding modules for system utterances and user inputs. The MDBT model, similar to GLAD, includes separate encoding modules for system and user utterances, and (slot, value) pairs. FJST and HJST are models with dialog history encoders, while SUMBT is a Slot-independent Belief Tracker with a multi-head attention layer. Our approach differs from SUMBT by incorporating attention among (domain, slot) pairs to learn dependencies explicitly. We use TSCP as a baseline, adapting it for multi-domain dialogues and focusing on the DST component's performance. Unlike TSCP, our models dynamically learn the length of each state sequence. The models discussed include DST Reader, HyST, and TRADE, with each offering unique approaches to dialogue state tracking. DST Reader reformulates the task as a reading comprehension problem, HyST combines fixed-vocabulary and open-vocabulary methods, and TRADE is the current state-of-the-art model on MultiWOZ2.0 and 2.1 datasets. The TRADE model, a state-of-the-art model on MultiWOZ2.0 and 2.1 datasets, utilizes a pointer network for state generation. Evaluation is based on joint goal accuracy in DST, comparing predicted dialogue states to ground truth. Results are reported for both datasets, showing superior performance compared to other models. Our non-autoregressive decoding models outperform state-of-the-art DST approaches by learning cross-domain and cross-slot signals, optimizing for joint goal accuracy. In the restaurant domain of MultiWOZ 2.0, our model excels in Joint Accuracy and Slot Accuracy. Latency analysis shows our models' efficiency compared to baselines TRADE and TSCP. Refer to Appendix A.3 for performance in other domains. In Table 4, experiments were conducted with TSCP for different values of T = T fert = T state \u2208 {1, 2, 3}. NADST performed the best when T = 3, outperforming baselines with less inference time. The approach is similar to TSCP, decoding complete dialogue state sequences to consider slot value dependencies. Our approach outperforms existing models in dialogue state processing, achieving 49.04% to 50.52% accuracy. Unlike other models that have higher latency due to sequential encoding and decoding processes, our model reduces latency by decoding individual slots in a parallel manner. The latency of our NADST model is only influenced by the number of attention layers in the decoder, making it more efficient compared to other approaches. The study includes an ablation analysis of different model variants, focusing on DST metrics such as Joint Slot Accuracy and Slot Accuracy. It also evaluates the performance of the fertility decoder in terms of Joint Gate Accuracy and Joint Fertility Accuracy. The importance of positional encoding (PE) in the model's success is highlighted, as removing PE before passing it to the state decoder results in a significant performance drop. The ablation analysis on MultiWOZ 2.1 focuses on the impact of removing components like slot gating, positional encoding, and pointer network on model performance. Removing these elements results in performance drops, especially in Joint Fertility Accuracy. Additionally, the study highlights the challenges faced by models in inferring unseen slot values during training. Further ablation experiments and results are detailed in Appendix A.3. The study focuses on the impact of removing components like slot gating, positional encoding, and pointer network on model performance. Experiments with an auto-regressive state decoder show that the proposed NADST models can predict fertilities well. Auto-regressive models are less sensitive to system action delexicalization, as predicting slot gates is easier than predicting fertilities. The study shows that auto-regressive models outperform existing approaches due to learned dependencies among domain-slot pairs. Fertility prediction is removed as it is redundant in auto-regressive models. Visualization includes examples of dialogue state prediction and self-attention scores. The study demonstrates that auto-regressive models excel in learning dependencies among domain-slot pairs. Fertility prediction is eliminated as it is unnecessary in these models. The model accurately predicts train details and attraction types based on token-level and slot-level dependencies. NADST, a novel Non-Autoregressive neural architecture for DST, is proposed to enhance model learning. Our NADST model introduces a Non-Autoregressive neural architecture for DST, improving joint accuracy by learning dependencies at both slot-level and token-level. It enables fast decoding of dialogue states with low inference latency, achieving state-of-the-art accuracy on the MultiWOZ corpus for DST tasks. The MultiWOZ corpus contains 1,000 multi-turn dialogues with an average of 14.7 turns per dialogue and 35 (domain, slot) pairs across 7 domains. Only 5 domains are included in the test data. Dropout of 0.2 was used in network layers, with a batch size of 32 and embedding dimension of 256. The number of attention heads was fixed at 16 in all attention layers. Embedding weights were shared for domain and slot tokens as input to decoder components. In experiments with different values of T, models were fine-tuned for 13K to 20K training steps. Model performance was evaluated in 5 test domains in MultiWOZ2.0 and 2.1, showing better results in restaurant and attraction domains. Taxi and hotel domains had lower performance due to complex slot ontology and dialogue mixtures. The model latency was visualized against the length of dialogue history in Figures 2 and 3. A new version of TRADE was compared to NADST, showing an increase in latency as dialogue extends over time. TRADE * follows an autoregressive decoding framework at the token level. The model latency increases as dialogue extends over time due to overhead processing. TRADE * shows a clearer increasing trend of latency compared to NADST and TSCP. Our models are compared with TSCP and TRADE in terms of scalability and performance. Ablation experiments show that model performance improves as the proportion of prediction input decreases. TRADE is compared in two cases: original TRADE and TRADE * which decodes dialogue state in parallel. Latency increases as dialogue history extends over time. The model performance improves with a decrease in prediction input proportion. Latency remains stable as dialogue history extends over time. Improving model performance practically is achievable through delexicalizing dialog history and using a pretrained NLU model. With access to ground-truth labels, the model can achieve a joint accuracy of 73%. The model can achieve a joint accuracy of 73% with ground-truth labels for X del and X ds\u00d7f ert in MultiWOZ2.1. Additional results are shown in Table 9 for the model's performance when using different percentages of model predictions for dialogue ID PMUL3759."
}