{
    "title": "B1tExikAW",
    "content": "The robustness and security of machine learning systems, particularly deep variational autoencoders, are crucial to prevent attacks. It has been shown that the latent space of a dVAE can be perturbed to flip class predictions without detection. This ability to encode data reliably is essential for tasks like image compression and communication. Error detection and correction is crucial in communication channels to identify erroneous bits, which can be caused by imperfections in the transmitter, channel, or receiver. Deliberate errors, such as those from man-in-the-middle attacks, can appear untampered to the receiver. In deep learning, encoding processes like autoencoders are learned through unsupervised learning, but methods for detecting tampering with encodings are lacking. This problem has two facets: tampering with models and detecting adversarial breaches. This method focuses on tampering with autoencoders, which have two components. Autoencoders are used for data compression and have various forms like denoising AE, compressive AE, and variational AE. They are widely used in data analytics, computer vision, and natural language processing. A proposed attack targets the latent encodings of autoencoders to change the semantic meaning of the output. The autoencoder is designed to reconstruct input data while maintaining label information. An attack transformation is focused on flipping the label of the original input by changing its content through altering the latent encoding. The autoencoder is designed to reconstruct input data while maintaining label information. An attack transformation aims to flip the label of the original input by altering the latent encoding. The success of the attack is measured by the minimal changes in the encoding, indistinguishable decoded samples, and opposite labels to the original data. The study focuses on transforms with these properties, where optimizing for one requirement may implicitly encourage the other. Our approach for attacking deep learning algorithms does not require knowledge of model parameters, only access to encodings and classifier output. The success of the attack is attributed to the linear structure of the VAE latent space. Previous work has focused on creating adversarial examples to cause algorithm failures. The perturbations aim to be undetectable by humans. Our approach for attacking deep learning algorithms focuses on launching attacks on intermediate encodings rather than data samples. Unlike previous methods, our approach does not require knowledge of model parameters, only access to encodings and classifier output. This approach is practical and tangential to previous work on adversarial images for classification. Our approach involves attacking intermediate encodings instead of data samples. Unlike previous methods, we aim to alter the message received by the receiver without detection. This contrasts with previous work where the goal is to change the message, such as images, in a detectable manner while remaining consistent with non-tampered messages. Our approach is similar to attacking variational autoencoders proposed by BID8, focusing on perturbing latent representations. Our approach focuses on perturbing latent encodings, contrasting with previous methods that perturb images. We learn a single adversarial perturbation that can be applied to any encoding for successful attacks, making it practical for larger scale attacks. Comparison to previous work shows that while they attack input images, we design attacks on latent encodings. We describe training a VAE and learning the adversarial transform applied to the latent encoding using a dataset of labeled binary examples. Our approach focuses on perturbing latent encodings using an adversarial transform, T, applied to the latent representation, T z. The objective is to ensure class(D (T z)) 6 = class(D (z)), where E, D, and C are the encoder, decoder, and classifier networks. This method contrasts with previous approaches that perturb images directly. The encoder, decoder, and classifier networks (E, D, C) are involved in three attack methods on a pre-trained variational autoencoder (VAE). The methods include Independent attack, Poisoning, and Poisoning+Class attack during VAE training. The objective is to perturb latent encodings using an adversarial transform, T, to ensure different classes between original and transformed latent representations. In the context of attacking a pre-trained variational autoencoder, the use of a constant z for image transformation is explored. By learning a single transform, it becomes easier to attack multiple times as the perturbation is pre-computed. This approach simplifies the attacker's task by tampering with most encodings using only one vector. The option of using z as a function of variables x, y, z may require online learning for attacks. The focus is on learning a single, constant z, with a mention of a multiplicative perturbation reserved for further explanation. The cost functions used to train a VAE involve minimizing a Kullback-Leibler divergence and binary cross-entropy. A classifier can be trained by minimizing a different cost function. Additionally, a classification loss on reconstructed data samples can be used as an additional cost function. To learn the attack transform, a transform on a latent encoding is minimized to achieve a label flip in the decoded image. Minimizing the L p -norm for p = {1, 2} on a latent encoding encourages sparse perturbations to minimize the number of changed elements. The attacker's goal is to flip the label of the decoded sample, aiming for successful attacks to change the label from 1 to 0. Classifier outputs values between [0, 1] where 0 or 1 indicates high certainty for class assignment, while 0.5 represents uncertainty. An attack's success is determined by a classifier's ability to predict the class of a reconstructed image with high certainty. To be undetectable, the classifier should predict labels of tampered and untampered data samples almost equally. Evaluation of attack quality is based on measuring |\u270f|, with a small |\u270f| indicating an undetectable attack. Epsilon, related to classification standard deviation, is calculated based on classifier outputs between [0, 1] representing confidence in classification. The confidence of a single classification is computed using the output of the classifier, with low confidence corresponding to 0 and high confidence to 1. Comparing class predictions for different classes is not meaningful, instead, comparisons are made based on data samples and their encodings. Performance of attacks is evaluated using the same classifier to facilitate comparison between attack types. In Section 6.4 of the Appendix, we introduce a probabilistic evaluation method for comparing attack types. Three attack methods with two types of regularization (L1-norm and L2-norm) are compared on the CelebA dataset. The experiments involve 6 different scenarios, with qualitative results shown for two examples in the main text and the rest in the appendix. Quantitative analysis is provided through confidence scores for all 6 attack types. In Section 6.4, a probabilistic evaluation method is introduced for comparing attack types on the CelebA dataset. Adversaries trained with L2 regularization target a pre-trained VAE without label information, making it challenging to learn features for \"smile\" and \"not smile\". Visual examples of reconstructed images of smiling and non-smiling faces are provided in FIG2. The study evaluates adversarial attacks on facial images, showing successful tampering with high confidence. Attackers optimize classification scores, making tampered samples more detectable. The success of attacks is evident in decoded tampered encodings, outperforming original and reconstructed images. The study evaluates successful adversarial attacks on facial images, optimizing classification scores to make tampered samples more detectable. The most stealthy attack involves switching from \"no smile\" to \"smile\" on a VAE trained with label information. The attacker finds it easier to move in the direction from \"no smile\" to \"smile\" due to a slight bias in classification results. Both qualitative and quantitative results indicate successful attack strategies. Successful adversarial attacks on facial images are evaluated, optimizing classification scores to make tampered samples more detectable. The study shows that the most stealthy attack involves switching from \"no smile\" to \"smile\" on a VAE trained with label information. Results in TAB0 indicate successful attack strategies, with visual results in the Appendix highlighting the effects of L1 regularization on encoding sparsity. The attack appears successful in all cases, as shown in visual results of reconstructions for tampered encodings. The study evaluates successful adversarial attacks on facial images, optimizing classification scores to make tampered samples more detectable. Results show that confidence values for attacked samples are higher than reconstructed and original data samples, indicating a direct optimization of classification score by the adversary. The classifier used for evaluating attacks is the same for all attacks, not the one used for training the adversary. The difference in confidence scores between reconstructed data samples and decoded tampered encodings can reveal if an attack has occurred. Stealthy attacks involve learning the encoding simultaneously. In this paper, the concept of latent poisoning is introduced as an efficient method for adversarial attacks on variational autoencoders. Additive and multiplicative perturbations in the latent space can flip the predictive class with minimal changes. Additive perturbations are easier to implement due to the linear structure of the latent space. Latent poisoning can lead to smooth transitions between images in the latent space, allowing movement between different classes. Countermeasures include monitoring predictive probability and uncertainty in autoencoder outputs to detect attacks. The importance of robustness in machine learning algorithms is increasing, with alternative methods becoming more challenging when attackers alter the latent distribution minimally. Using L1 regularization results in sparser perturbations compared to L2 regularization, as shown in Figure 10. Understanding the magnitude of adversarial perturbations can help detect attacks in the latent space. Understanding the magnitude of perturbations in the latent space can aid in detecting attacks. By individually testing each element of a latent encoding, we can determine if an attack has occurred. Assuming a well-trained variational autoencoder with a Gaussian distribution, we can detect perturbations to individual elements independently. The formula for calculating the probability of a single element in a tampered encoding lying outside a specific range is given. This information can be used to evaluate attack processes and regularize models to avoid detection. Attacks using L2 regularization are more likely to be undetectable if the encoded data samples follow a Gaussian distribution. In comparing the effect of different regularizations on attacks, L1 regularization may influence epsilon value but does not guarantee sparsity in latent space. Sparse encodings can facilitate attack speed but may not impact attack quality significantly. Images in FIG0 illustrate the impact of adversarial perturbations on data samples. The effect of adversarial perturbations on the latent space in the data space is studied. A multiplicative perturbation is formulated to encode smile or no smile classes with different signs. The code for training and evaluation will be available on Github after the review process. The encoding in the formulation cannot guarantee the desired form due to restrictions on label swaps. Preliminary experiments show that faces classified as \"smiling\" appear more intense after transformation, likely because the autoencoder initially perceived them as not smiling. A single z with Lp regularization is used for the transform, z(1 + z). Multiplicative transforms do not perform as well as additive ones, possibly due to the linear structure of the autoencoder's latent space. The near-linear structure of the latent space learned by the autoencoder can be targeted by an adversary using additive perturbations, while multiplicative perturbations make stronger assumptions that may not hold for all variational autoencoders."
}