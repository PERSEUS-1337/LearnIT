{
    "title": "HkAClQgA-",
    "content": "Neural network model with intra-attention and a training method combining supervised word prediction and reinforcement learning improves abstractive summarization by reducing repetitive and incoherent phrases. This approach addresses \"exposure bias\" in models trained only with supervised learning, resulting in more readable summaries. Our model achieves a 41.16 ROUGE-1 score on the CNN/Daily Mail dataset, outperforming previous models. It also produces higher quality summaries according to human evaluation. Text summarization condenses information into short, informative summaries, aiding in news digests, search, and report generation. There are two types of summarization algorithms: extractive systems copy parts of the input, while abstractive systems generate new phrases. Neural network models based on the attentional encoder-decoder model for machine translation have been successful in generating abstractive summaries with high ROUGE scores. However, these systems are typically used for summarizing short input sequences and often produce unnatural summaries with repeated phrases. A new abstractive summarization model has been introduced, achieving state-of-the-art results on the CNN/Daily Mail dataset and showing promising results on the New York Times dataset. The first end-to-end model for abstractive summarization on the New York Times dataset introduces a key attention mechanism and a new learning objective to address the repeated phrase problem. It uses intra-temporal attention in the encoder and a sequential intra-attention model in the decoder to improve the generation of summaries. Our model combines maximum-likelihood cross-entropy loss with rewards from policy gradient reinforcement learning to reduce exposure bias and achieves 41.16 ROUGE-1 on the CNN/Daily Mail dataset. Human evaluation shows that our model generates more readable summaries compared to other approaches. The intra-attention model is based on the encoder-decoder network BID33, using an intra-temporal attention function at each decoding step to attend over specific parts of the input sequence. The model uses intra-temporal attention to prevent repetitions and improve summarization quality. It defines attention scores at each decoding step and normalizes them using a bilinear function. The attention weights are further normalized with a temporal attention function to penalize high scores from past steps, resulting in improved input context vectors. The model introduces an intradecoder attention mechanism to improve summarization quality by incorporating information from previously decoded sequences. This mechanism is not present in existing encoder-decoder models and helps the model make more structured predictions and avoid repeating information. The model introduces an intradecoder attention mechanism to improve summarization quality by incorporating information from previously decoded sequences. For t > 1, the decoder uses equations to generate tokens using a softmax layer or a pointer mechanism to copy rare or unseen tokens from the input sequence. This method is more simple and widely applicable to other types of recurrent networks compared to previous methods. The model introduces an intradecoder attention mechanism to improve summarization quality by incorporating information from previously decoded sequences. It generates a probability distribution for token generation and uses a pointer mechanism to copy input tokens. The final probability distribution for the output token is computed using specific equations. The ground-truth value for the copy mechanism is provided during training, setting it to 1 for out-of-vocabulary tokens or pre-defined named entities. The model introduces weight-sharing between embedding matrices for encoder and decoder sequences to utilize syntactic and semantic information. To avoid repetitions during testing, the decoder is forced to not output the same trigram more than once. In this section, different ways of training the encoder-decoder model are explored, including reinforcement learning-based algorithms for summarization tasks. The teacher forcing algorithm minimizes a maximum-likelihood loss during decoding. However, minimizing this loss may not always yield the best results on evaluation metrics like ROUGE. This discrepancy is attributed to exposure bias and has been observed in tasks such as image captioning and machine translation. The discrepancy in performance between training and testing with BLEU is due to exposure bias and the variety of valid summaries. To address this, a policy maximizing a specific metric is learned using reinforcement learning, as seen in the self-critical policy gradient training algorithm. In reinforcement learning algorithm, two output sequences are generated at each training iteration: y s, sampled from a probability distribution, and \u0177, obtained by maximizing the output probability distribution. The reward function r(y) compares the output sequence y with the ground truth y* using an evaluation metric. Minimizing L rl is equivalent to maximizing the likelihood of y s if it receives a higher reward than \u0177, improving the model's reward expectation. However, optimizing for specific discrete metrics like ROUGE may not guarantee improved quality and readability of the output. The text discusses the limitations of using discrete metrics like ROUGE to measure the quality and readability of generated summaries. It suggests that a language model's perplexity may better capture human-readability. The proposed mixed learning objective function combines maximum-likelihood training and language model objectives to generate more natural summaries. Neural encoder-decoder models, utilizing RNNs like LSTM, are commonly used in NLP tasks such as machine translation and summarization. Attention mechanisms enhance model performance by allowing them to reference parts of the input sequence. This mixed learning objective function combines maximum-likelihood training with language model objectives to improve the readability of generated summaries. The use of reinforcement learning (RL) in neural encoder-decoder models allows the model to interact with the environment to maximize a reward, especially in cases where traditional supervised learning methods are not applicable. This approach is particularly useful for sequence generation tasks. Traditional supervised learning methods are not suitable for sequence generation tasks due to non-differentiable evaluation metrics like BLEU, ROUGE, and METEOR. BID27 applied the REINFORCE algorithm to train RNN-based models, leading to significant improvements. BID1 used a different reinforcement learning algorithm to optimize BLEU scores in machine translation tasks. Rennie et al. introduced a self-critical sequence training method that does not require a critic model, resulting in further improvements in image captioning tasks. Most summarization models in the past have been extractive in nature. Abstractive summarization models, such as Rush et al. and BID19, are based on neural encoder-decoder architecture and have more freedom to generate novel sequences. These models have been evaluated on the Document Understanding Conference (DUC) tasks, outperforming extractive models on the DUC-2004 dataset. Models trained on the DUC-2004 dataset can generate short summaries up to 75 characters. Different attention mechanisms were applied for summarization on the CNN dataset. Nallapati et al. used various attention and pointer functions on the CNN and Daily Mail datasets. Another abstractive summarization model was developed on the CNN/Daily Mail dataset with an extra loss term to increase temporal coverage of the encoder attention function. The final dataset used for evaluation contains 287,113 training examples, 13,368 validation examples, and 11,490 testing examples. The New York Times dataset (NYT) BID30 is a large collection of articles published between 1996 and 2007. It has been used for training extractive summarization systems, but we are the first to run an end-to-end abstractive summarization model on this dataset. NYT abstracts are more varied and shorter compared to CNN/Daily Mail summaries, making them a good complement for abstractive summarization models. The dataset preprocessing and experiments for abstractive summarization models are described in the Appendix. The study evaluates intra-decoder attention and mixed-objective learning through various experiments on datasets. Different training methods are compared, including maximum-likelihood (ML) training with and without intra-decoder attention, reinforcement learning (RL), and mixed-objective learning (ML+RL). Implementation details and hyperparameters are provided in the Appendix, with results reported using ROUGE metrics. RL and ML+RL training use the ROUGE-L score as a reinforcement reward. The study evaluates the impact of intra-decoder attention on abstractive summarization models using ROUGE metrics. Results show that intra-attention improves ROUGE scores on the CNN/Daily Mail dataset but not on the NYT dataset. Intra-attention enhances performance on longer output sequences, while not benefiting shorter summaries. Both RL and ML+RL models outperform ML training on all datasets. The RL and ML+RL models outperform the ML model on all datasets, including the CNN/Daily Mail dataset. Comparisons with other models show that the mixed-objective model achieves higher ROUGE scores. The extractive summarization model built by BID6, trained on a smaller version of the NYT dataset, shows higher ROUGE scores compared to other models. The ML+RL model outperforms the extractive model and baselines in terms of ROUGE scores and human readability. The ML+RL training objective improves ROUGE scores and human readability. A human evaluation with 100 test examples from the CNN/Daily Mail dataset shows the impact on relevance and readability. Each summary is rated by 5 human evaluators on Amazon Mechanical Turk. Results from human evaluation on the impact of different training methods on summary readability and relevance are presented. RL has high ROUGE scores but low readability due to short sentences. RL+ML summaries have the highest readability and relevance scores, showing the value of the combined training method. Perplexity measurements do not replace human judgment for readability assessment. The new model and training procedure improve text summarization results for CNN/Daily Mail and enhance readability of generated summaries. Despite common use, ROUGE scores have limitations for evaluation and should not be the sole metric for optimizing summarization models. The intra-attention decoder and combined training objective can be applied to other tasks with long sequences, opening avenues for further research. NYT dataset is utilized, removing documents without full article text, abstract, or headline, and concatenating them for input. The text discusses the preprocessing steps for creating input sequences for text summarization using the NYT dataset. Tokens are converted to lowercase, numbers are replaced, and certain words are removed. Semicolons are used to split sentences in the absence of periods. The average input and output token lengths are 549 and 40, respectively. Training, validation, and testing splits are created for the dataset. The dataset for text summarization was split into training, validation, and testing sets based on publication dates. Named entity recognition was used to supervise training but is not needed during testing. During ML training for text summarization, a teacher forcing algorithm is used with a 25% probability of choosing the previously generated token as input. The model utilizes two 200-dimensional LSTMs for the encoder and one 400-dimensional LSTM for the decoder. Input and output vocabularies are limited to 150,000 and 50,000 tokens respectively. The model has 16.9M trainable parameters and uses GloVe for word embeddings. Our final model for text summarization has 16.9M trainable parameters, with 15M dedicated to word embeddings. Training is done with Adam optimizer, a batch size of 50, and a learning rate of 0.001 for ML training and 0.0001 for RL and ML+RL training. Beam search of width 5 is used at test time for generating predictions."
}