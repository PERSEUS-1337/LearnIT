{
    "title": "r1eP5khVKB",
    "content": "The proposed framework utilizes variational autoencoders and normalizing flows for data imputation, allowing for stochastic imputation from a multi-modal latent distribution. Initial results on the MNIST dataset show promising outcomes. Neural network algorithms have proven effective for tasks like classification, retrieval, and prediction, but they typically require access to fully-observed data, which can be challenging to obtain in real-life scenarios. This framework aims to address this limitation by enabling inference on partial data. The framework utilizes variational autoencoders for data imputation, allowing for stochastic imputation from a multi-modal latent distribution. Variational autoencoders have been applied to the data imputation task recently due to the probabilistic nature of the task. Variational autoencoders (VAEs) model the distribution of observed and latent variables with parameters \u03b8 and include an inference network with parameters \u03c6 for approximate posterior distribution. Recent research focuses on VAE-based models' effectiveness in combining different observed parts. This study proposes enriching VAEs' latent space for multi-modal posterior inference and probabilistic imputation through a two-stage model. The first stage learns a representation space from fully-observed data, while the second stage aligns the representation space. The proposed model enriches VAEs' latent space for multi-modal posterior inference and probabilistic imputation through a two-stage approach. The first stage focuses on learning a representation space from fully-observed data, while the second stage aligns the representation space embedded from partially-observed data. The model uses flow-based transformations to construct a rich latent distribution, enabling inference of multi-modal variational latent distributions. Training such a model poses challenges in ensuring correct distribution capture by the proposed posterior. The proposed two-stage schema enhances VAEs' latent space for multi-modal posterior inference and probabilistic imputation. The first stage involves training an encoder to reconstruct full data, while the second stage trains a model on partial data using a different encoder. The latent space of the first model is rich enough to represent the full data's distribution, allowing for the adoption of a divergence loss in training. To improve distribution alignment between two models, a divergence loss is proposed to distill rich information from the first model's latent representation into the second model. This method ensures weak alignment and provides feedback to the encoder. Normalizing Flow model is used to handle multi-modal distributions in the latent space, forcing divergence between normal distribution and the complex distribution created by the flow model. The divergence between a simple and complex distribution is addressed by modeling it using a Monte-Carlo approach for the KLDivergence. The computation of q(x i) from the flow model is a correction term applied to the simple distribution before the flow, utilizing Normalizing Flows properties. The model includes transformations from a simple distribution to a complex one through flow transformations. An extra divergence loss is added to control the support of the distribution. The second stage model is trained with reconstruction and divergence losses, while the first stage model provides supervision for the latent space structure. Stochastic optimization in neural networks makes a previous problem irrelevant. The model utilizes flow transformations to create a multi-modal distribution in neural networks. Results show the benefit of incorporating the NF module in the architecture for latent structure organization. The NF module in the architecture allows for a more flexible latent space, capturing multi-modality in reconstructions. The model aims to recognize digits from partially observed data, with results showing improved reconstructions with the flow module. The NF module in the architecture enables a flexible latent space for capturing multi-modality in reconstructions, improving reconstructions of partially observed digits. The model is data-agnostic and can be applied to various data modalities for tasks like data imputation and generation."
}