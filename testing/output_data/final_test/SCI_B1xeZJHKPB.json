{
    "title": "B1xeZJHKPB",
    "content": "In this paper, the authors explore various methods to combine explanations for neural network decisions and reduce model uncertainty to produce a single aggregated explanation that is more robust and aligns better with the neural network than individual explanation methods. The authors propose a new approach to evaluating explanation methods for neural networks, addressing the challenge of explaining decisions in visual recognition. Various explanation strategies have been proposed, but there is no consensus on what constitutes a sufficient explanation or how to evaluate these methods. This paper explores using ensemble models to reduce epistemic uncertainty in explaining visual decisions made by neural networks. The authors test the hypothesis that aggregating explanations from multiple methods is more robust than using a single method, providing theoretical analysis and empirical evaluation. They aim to combine features from different explanation approaches to enhance the overall explanation quality. The paper proposes two ways to aggregate explanation methods, AGG-Mean and AGG-Var, which combine features to provide more complete and less biased explanations. Experimental results show that the aggregates outperform individual methods in identifying relevant parts of images. Additionally, a new approach called IROF (Iterative Removal Of Features) is introduced to quantitatively evaluate explanation methods without human bias. The ongoing challenge of explainability in neural networks is highlighted, with references to recent related work. The paper discusses various visual explanation methods for neural networks, including Saliency Maps, Guided Backpropagation, and Grad-CAM. These methods aim to provide insights into neural network decisions by highlighting the relevance of input dimensions. Explanation methods for convolutional neural networks include Selvaraju et al.'s method, which uses backpropagation to highlight relevant parts of input images. Integrated Gradients sums gradients from interpolated pictures, SmoothGrad filters out noise, and LIME approximates the neural network with a linear model locally around the input. Explanation methods for convolutional neural networks involve various approaches such as backpropagation to highlight relevant parts of input images, Integrated Gradients for summing gradients, SmoothGrad for noise filtering, and LIME for approximating the neural network with a linear model around the input. Evaluating these methods is a recent topic with few systematic approaches, including quantitative methods proposed by Bach et al. (2015) and further developed by Samek et al. (2016) to make the evaluation feasible for high-dimensional inputs. Explanation methods for convolutional neural networks involve various approaches such as backpropagation, Integrated Gradients, SmoothGrad, and LIME. Evaluating these methods is a recent topic with few systematic approaches. Squares with high relevance are replaced with noise to evaluate the explanation method. ROAR is proposed as a quantitative approach to evaluate explanation methods. Ancona et al. (2018) proposed a different approach involving retraining the network multiple times at a high computational cost. Ancona et al. (2018) introduced Sensitivityn as an alternative method to evaluate explanation methods for neural networks. It involves sampling subsets of inputs and calculating the Pearson Correlation Coefficient (PCC) between the decrease in output and the sum of relevances. Current explanation methods have inherent weaknesses and uncertainties, leading to noisy results in heatmaps. To address this, a solution is needed to reduce noise and improve the reliability of explanation methods. Ensemble methods can reduce noise in explanation methods by combining multiple methods. A neural network F: X \u2192 y is assumed with a set of explanation methods {e j} J j=1. The mean aggregate explanation \u0112 is obtained by combining explanations E j,n. This method is generalizable to inputs of various dimensionalities. The benefit of aggregation is theoretically understood by hypothesizing a 'true' explanation \u00ca. The error of an explanation method is quantified as the mean squared difference between the 'true' explanation and an explanation. The error of an explanation method is quantified as the mean squared difference between the 'true' explanation and an explanation obtained by the method, denoted as MSE. The error of the aggregate explanation is shown to be less than the typical error of individual methods, with the difference representing epistemic uncertainty. The AGG-Mean approach averages explanation methods to reduce variance, while AGG-Var assigns less relevance to segments with high disagreement between methods by dividing the mean aggregate locally with its standard deviation. This helps stabilize the results by adding a constant to the estimate of the local variance. In section 4, AGG-Mean and AGG-Var will be evaluated and compared against basic explanation methods. Quantitative evaluation with IROF involves identifying relevant segments and replacing them with the mean color over the dataset. This is crucial for high-dimensional inputs like images, where locally correlated pixels make up important features. Relevance values of single pixels do not indicate the importance of the feature as a whole, which is addressed by utilizing conventional image processing techniques. Utilizing conventional image segmentation, the methodology involves dividing the image into coherent segments to avoid interdependency between inputs. Each image is partitioned into segments, and the mean importance of each segment is computed using explanation methods. Segments are ranked in order of relevance, allowing for comparison and identification of relevant image segments. The iterative removal of features (IROF) evaluates explanation methods by replacing segments with highest mean relevance with the mean value. This process results in a curve of class scores, allowing for comparison between input samples and networks. The area over the curve (AOC) is computed to identify methods that reliably identify relevant image areas, with a higher AOC indicating a good explanation method. The iterative removal of features (IROF) is a quantitative comparison of explainability methods that does not rely on human evaluation. It produces a single value for each method, allowing for convenient comparison. IROF is dependent on meaningful segments in the input, such as natural images. Dividing non-natural images or text into segments is a challenge for future research. Empirical validation of IROF is presented, followed by evaluation of explanation techniques against vanilla techniques using IROF, Sensitivity-n, and qualitative measures. In appendix A.6.1, comparison of aggregation methods on human-annotated heatmaps was conducted using five neural network architectures pre-trained on ImageNet: VGG19, Xception, Inception, ResNet50, and ResNet101. Experiments were also performed on CNN trained on MNIST and FashionMNIST datasets. Various attribution-based methods were compared, including Saliency, Guided Backpropagation, SmoothGrad, Grad-CAM, and Integrated Gradients, as well as LIME for local approximation. Positive and negative evidence was observed with some methods. In evaluating attribution-based methods for ImageNet tasks, only positive evidence was considered. Comparison of methods with and without negative results showed negligible differences in metrics. An additional parameter was introduced for Agg-Mean, set to be ten times the mean \u03c3 over the dataset. Evaluation involved rejecting the null hypothesis with high confidence, using paired t-tests to compare IROF against random guessing. Multiple explanation methods and networks were used to reduce method impact. IROF and pixel removal were compared. In comparing IROF and pixel removal methods for ImageNet tasks, different replacement values were used. The evaluation focused on the impact of explanation methods on neural networks, with a comparison against Samek et al. (2016). The percentage of segments or pixels removed was chosen ad hoc, with a focus on meaningful information. P-values were used to assess the significance of the evaluation methods. Results were presented in figure 2, showing changes in p-values based on the number of samples. Results from the evaluation of different explanation methods on ImageNet tasks showed that IROF is more sensitive and can distinguish between random guessing and an explanation method with higher confidence. This makes IROF a better choice for quantitative evaluation compared to pixel removal or other methods. The study included two non-informative baselines, Random and Sobel, which do not contain information about the neural network. The evaluation of explanation methods on ImageNet tasks revealed that all methods contain information about image classification, with some surpassing the baseline. The accuracy of explanation methods varies based on neural network complexity. Aggregated methods generally have lower IROF scores than non-aggregated methods, especially for complex networks like ResNet101. Aggregating explanation methods benefits networks with large epistemic uncertainty. Aggregating methods improve over unaggregated methods in identifying relevant parts of images for classification. Heatmaps are shown for individual examination, and qualitative comparisons are made. Visual evaluation of explanations for neural networks can be misleading, but it is the best available method for checking agreement with human understanding. AGG-Var combines features of aggregated methods by attributing relevance to the classified object as a whole and considering smaller details. Combining explainability methods provides a visual improvement over single methods. Sensitivity-n is used to compare explanation methods on low-dimensional inputs like MNIST and FashionMNIST datasets with a basic CNN architecture. The procedure involves testing on randomly sampled subsets for 1000 test images. The study involved testing explanation methods on subsets of input features for CNNs trained on FashionMNIST and MNIST datasets. AGG-Mean and AGG-Var performed well across different scenarios, outperforming other methods. SmoothGrad and GradCAM showed poorer performance compared to aggregation methods. Aggregation methods like AGG-Mean and AGG-Var outperformed individual methods like SmoothGrad and GradCAM on low-dimensional tasks like MNIST. The study found that aggregating explanation methods consistently performed better across different network architectures, providing evidence of their superiority in explanation quality. Ensemble methods can reduce noise in explanation methods by combining multiple approaches. By aggregating explanations from different methods, a more reliable and less biased result can be obtained. This approach has shown superiority in explanation quality across various network architectures. The error of an explanation method is defined as the mean squared difference between the obtained explanation and a hypothetical 'true' explanation. The aggregate error MSE(\u0112) is lower than the typical error of individual methods, indicating reduced uncertainty. Theoretical and empirical analysis confirms that the average MSE of multiple explanation methods is always higher than the error of the aggregated methods. Backpropagation-based methods are preferred for aggregation due to their computational efficiency, making them more practical for both human and machine processing compared to locality-based methods like LIME. In contrast to backpropagation-based methods, locality-based methods like LIME require multiple forward passes to determine relevant input parts. SLIC was chosen for image segmentation over Quickshift due to quicker runtime, with 300 segments used ad hoc. AGG-Var includes a constant in the denominator, set to 10 times the mean std for evaluations with a set random seed for reproducibility. The study used a set random seed for reproducibility and reported stddev for each individual result. P-values were reported for evaluating with 50 images on ResNet101. Human evaluation was based on a benchmark introduced in Mohseni & Ragan (2018) with ninety images from the ImageNet Challenge. The study evaluated neural network explanations in the ImageNet Challenge, comparing human and network judgments using cosine similarity. AGG-Mean and AGG-Var performed similarly to top methods like SmoothGrad and GradCAM. Combining SmoothGrad and GradCAM yielded better results, suggesting their complementary strengths. Combining SmoothGrad and GradCAM improves results by reducing epistemic uncertainty through aggregation."
}