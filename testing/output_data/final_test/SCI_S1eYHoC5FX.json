{
    "title": "S1eYHoC5FX",
    "content": "This paper introduces a differentiable approach to architecture search, allowing for efficient exploration of architecture using gradient descent. Extensive experiments demonstrate the algorithm's effectiveness in discovering high-performance convolutional architectures for image classification and recurrent architectures for language modeling, outperforming non-differentiable techniques. Algorithmic solutions automate architecture design, achieving competitive performance in tasks like image classification and object detection. Existing search algorithms are computationally demanding, requiring thousands of GPU days. Various approaches have been proposed to speed up the process, but scalability remains a challenge. In this work, a method for efficient architecture search called DARTS (Differentiable ARchiTecture Search) is proposed. Unlike traditional approaches, DARTS relaxes the search space to be continuous, allowing for optimization through gradient descent. This method achieves competitive performance with significantly fewer computation resources compared to existing black-box optimization methods. DARTS outperforms other efficient architecture search methods, is simpler, and can handle both convolutional and recurrent architectures. It learns high-performance building blocks within a rich search space and is not restricted to any specific architecture family. DARTS is a versatile algorithm applicable to various network architectures, achieving competitive results with minimal parameters. It outperforms other methods in image classification and language modeling tasks, showcasing its efficiency in discovering high-performance cells. The novel algorithm for differentiable network architecture search based on bilevel optimization achieves competitive results on CIFAR-10 and outperforms the state of the art on PTB. It shows remarkable efficiency improvement and transferability to ImageNet and WikiText-2. The DARTS algorithm, available at https://github.com/quark0/darts, demonstrates transferability from CIFAR-10 to ImageNet and PTB to WikiText-2. The search space is described in general form, with a continuous relaxation scheme for joint optimization of architecture and weights. A computation cell is searched for as the building block, which can be stacked for a convolutional network or recursively connected for a recurrent network. A cell in a recurrent network is a directed acyclic graph with N nodes representing latent features. Each edge is associated with an operation that transforms the nodes. The cell has two input nodes and one output node, with the output obtained by applying a reduction operation to all intermediate nodes. Learning the cell involves learning the operations on its edges. The task of learning a cell in a recurrent network involves understanding the operations on its edges. Candidate operations are represented as functions to be applied to the nodes. The search space is made continuous by using a softmax over all possible operations, parameterized by a vector \u03b1. Architecture search focuses on learning continuous variables \u03b1, which can be used to obtain a discrete architecture by selecting the most likely operation for each pair of nodes. The goal is to jointly learn the architecture \u03b1 and the weights w within the network. After relaxation, the goal is to jointly learn the architecture \u03b1 and weights w in mixed operations. DARTS optimizes validation loss using gradient descent, aiming to minimize Lval(w*, \u03b1*). This involves a bilevel optimization problem with \u03b1 as the upper-level variable and w as the lower-level variable. The nested formulation in gradient-based hyperparameter optimization involves optimizing the architecture \u03b1 as a special type of hyperparameter. A mixed operation is created to update weights w based on the learned \u03b1, using a simple approximation scheme to adapt w. The architecture gradient evaluation is approximated due to expensive inner optimization. The nested formulation in gradient-based hyperparameter optimization involves adapting weights w using a single training step without solving the inner optimization completely. The iterative procedure outlined in Alg. 1 aims to reach a fixed point with a suitable choice of parameters. Momentum for weight optimization modifies the one-step unrolled learning objective, but the analysis still applies. The approximate architecture gradient is obtained by applying the chain rule. The approximate architecture gradient can be simplified using the finite difference approximation, reducing complexity from O(|\u03b1||w|) to O(|\u03b1| + |w|). When \u03be = 0, the second-order derivative disappears, leading to the architecture gradient \u2207 \u03b1 L val (w, \u03b1). This heuristic optimizes validation loss assuming current w is the same as w * (\u03b1), resulting in some speed-up but potentially worse performance. The discrete architecture is formed by retaining the top-k strongest operations from previous nodes. The strength of an operation is defined as DISPLAYFORM0. Setting \u03be equal to the learning rate for w's optimizer is a simple working strategy. The analytical solution for the bilevel optimization problem is (\u03b1*, w*) = (1, 1), highlighted in the red circle. The feasible set where constraint equation 4 is satisfied is indicated by a dashed red line. The example demonstrates how a suitable choice of \u03be helps converge to a better local optimum in the architecture search process. Different values of k are used for convolutional and recurrent cells to ensure fair comparison with existing models. Experiments on CIFAR-10 and PTB involve two stages: architecture search and evaluation. The study involves searching for cell architectures using DARTS and selecting the best cells based on validation performance. These cells are then used to build larger architectures, trained from scratch, and tested. Transferability of the best cells learned on CIFAR-10 and PTB is evaluated on ImageNet and WikiText-2. Various operations are included in the architectures, and the convolutional cell consists of 7 nodes. The convolutional cell consists of 7 nodes, with the output node being a concatenation of all intermediate nodes. The network is formed by stacking multiple cells together, with specific nodes and convolutions inserted as necessary. Reduction cells are located at 1/3 and 2/3 of the total depth, with specific operations and architecture encoding. The convolutional cell consists of 7 nodes, with various operations like tanh, relu, sigmoid, identity mapping, and zero operation. Each architecture snapshot is re-trained from scratch and evaluated on the validation set. Results are compared with existing cells discovered using RL or evolution. The ENAS cell BID24 is used in the recurrent network, with settings similar to ENAS and batch normalization in each node. The architecture is determined by running DARTS four times and selecting the best cell based on validation performance. Initialization sensitivity is crucial for recurrent cells. The selected architecture is evaluated by randomly initializing weights and training from scratch, with performance reported on the test set. Transferability of the best convolutional and recurrent cells is tested on ImageNet and WikiText-2. More details on transfer learning experiments can be found in specific sections. Comparison of different evolution strategies for neural architecture search, including AmoebaNet-A, AmoebaNet-B, Hierarchical evolution, PNAS, SMBO ENAS, RL ENAS, and gradient-based DARTS. The best architecture is chosen based on validation error after 100 training epochs. The search cost for DARTS does not include selection or final evaluation costs. The CIFAR-10 results for convolutional architectures are presented in TAB0. DARTS achieved comparable results with state-of-the-art BID36 and BID26 while using significantly fewer computation resources. DARTS outperformed ENAS BID24 by discovering cells with comparable error rates but fewer parameters, despite a longer search time. The necessity of bilevel optimization was explored through alternative strategies, resulting in a convolutional cell with 4.16% test error using 3.1M parameters, which was worse than random search. In the second experiment, \u03b1 was optimized simultaneously with w using SGD, resulting in a cell with 3.56% test error. The heuristics used in the optimization process may cause \u03b1 to overfit the training data, leading to poor generalization. A cell discovered by DARTS achieved a test perplexity of 55.7 on PTB, outperforming manually or automatically discovered architectures. This highlights the importance of architecture search in addition to hyperparameter search. The efficiency of architecture search is highlighted, with DARTS showing significant improvements over random search in both convolutional and recurrent models. DARTS achieves competitive performance with less computation resources compared to state-of-the-art RL methods. The transferability of cells learned by DARTS is demonstrated on different datasets, showing promising results. DARTS is a efficient architecture search algorithm for convolutional and recurrent networks, outperforming non-differentiable methods in image classification and language modeling tasks. Transferability between datasets is a challenge, but DARTS shows promising results with less computation resources. Improvement of DARTS algorithm involves addressing discrepancies between continuous and discrete architecture encoding, potentially using softmax temperature annealing. Performance-aware architecture derivation schemes based on the one-shot model learned during search process are also of interest. Architecture search for CIFAR-10 involves using batch-specific statistics for normalization and disabling learnable affine parameters during the search process. To conduct architecture search for CIFAR-10, half of the training data is held out as a validation set. A small network with 8 cells is trained using DARTS for 50 epochs, with specific parameters chosen to fit into a single GPU. Momentum SGD is used to optimize weights, while Adam is used for optimizing architecture variables. The optimizer for architecture search uses specific parameters like learning rate, momentum, and weight decay. The network is trained for 50 epochs with SGD without momentum, batch size 256, and variational dropout applied to word embeddings and cell input. Variational dropout is applied to word embeddings, cell input, and hidden nodes in the network architecture. Training settings are similar to previous models, using Adam optimizer with specific parameters. The network consists of 20 cells trained for 600 epochs with batch size 96. Additional enhancements include cutout, path dropout, and auxiliary towers. Training takes 1.5 days on a single GPU. The training of the model includes dropout with probability 0.2 and auxiliary towers with weight 0.4. It takes 1.5 days on a single GPU using PyTorch BID21 implementation. Results are reported as mean and standard deviation of 10 independent runs to address variance. NASNet-A cell and AmoebaNet-A cell are incorporated into the training framework to ensure consistency in results."
}