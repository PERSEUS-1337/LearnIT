{
    "title": "rylJkpEtwS",
    "content": "We explore learning an arrow of time in a Markov Process to capture salient information about the environment, measure reachability, detect side-effects, and obtain intrinsic rewards. A simple algorithm with a deep neural network is proposed to parameterize the problem and learn the arrow of time. Empirical results show agreement with a well-known notion of an arrow of time in stochastic processes. In their work, Kinderlehrer and Otto (1998) discuss the profound effect of the asymmetric progression of time on our perception and actions in the environment. They explore the concept of the Arrow of Time, coined by Eddington (1929), which reflects the inherent asymmetry in the world due to the non-decreasing thermodynamic entropy. The authors investigate how this asymmetry can be leveraged to learn a representation that mimics our understanding of time's nature. The arrow of time concept, rooted in the non-decreasing thermodynamic entropy, has been formalized and explored in various fields like physics, algorithmic information theory, causal inference, and time-series analysis. It represents a monotonous increase as a system evolves, with irreversibility playing a key role. In statistical physics, entropy production drives the arrow of time through irreversible processes. This notion is relevant in understanding how an arrow of time can be beneficial in the context of reinforcement learning, illustrated by the example of a cleaning robot moving a box across a room. The optimal way for a cleaning robot to complete a task may involve disruptive actions like knocking over a vase, leading to irreversible side-effects. These side-effects can be quantified by an arrow of time, similar to entropy in thermodynamic systems. Detecting and preempting such side-effects could be a potential application, such as penalizing policies that increase disorder in the environment. The arrow of time can help a safe agent decide between paths by measuring disorder and reachability. It serves as a directed measure of disorder, making it easier to obtain order from disorder. The arrow of time can be used as a measure of reachability and disorder to help agents learn complex skills without external rewards. By learning to reverse the arrow of time and create order from disorder, agents can develop intricate planning and motor skills. This approach provides intrinsic rewards for achieving goals in a Markov (Decision) Process. The proposed method aims to learn an arrow of time for a Markov Process without external rewards, called the h-potential. It compares this to the free-energy functional of stochastic processes and discusses the challenges of practical utility in reinforcement learning. The h-potential is introduced as a function to quantify disorder in environment states, with irreversible transitions increasing disorder. It aims to be constant along fully reversible transitions but increase along less reversible ones in a Markov Decision Process. The function must overcome conceptual roadblocks to be useful in reinforcement learning. In a Markov Decision Process, a policy \u03c0 maps states to action distributions. By sampling states and actions, a trajectory is obtained with transition probabilities. The function h \u03c0 quantifies disorder in states with a regularizing term to prevent excessive disorder. The regularizing term T [\u0125] in a Markov Decision Process prevents excessive disorder in states by balancing the increase in expectation along sampled trajectories with a scalar coefficient \u03bb. If state transitions are fully reversible, the pressure on h \u03c0 to increase along forward transitions is compensated by counter-pressure to increase along reverse transitions, resulting in h \u03c0 remaining constant in expectation. The regularizer T must be chosen to suit the problem at hand, with different choices resulting in solutions with varying characteristics. Choices for T include the negative of L2 norm and/or a trajectory regularizer, which penalizes changes in the solution h \u03c0. Analytical treatment is feasible for simple Markov chains but becomes impractical for complex environments with unknown transition probabilities. To tackle complex environments with unknown transition probabilities, the functional optimization problem is cast to an optimization problem over the parameters of a deep neural network. Two subtleties are discussed regarding the conditions under which the learned arrow of time (h-potential) can be useful in practice. The first subtlety involves the role of a policy in collecting trajectories, where certain policies may result in unnatural or misleading arrows of time. The h-potential on trajectories may assign smaller values to states with more broken vases. The function h* characterizes the arrow of time with respect to all policies, but determining it is not feasible for most applications. Random actions are used to gather trajectories due to the complexity of determining h*. The simplicity of using random actions to gather trajectories ensures that the learned arrow of time reflects the environment's dynamics, not the agent's behavior. However, this approach lacks exploration in complex environments, a common issue in model-based reinforcement learning. The requirement for informative arrows of time in dissipative environments is also discussed. The state space for a billiard ball on a frictionless table is defined by its velocity and position. Time-reversal symmetry shows equal likelihood of forward and reverse transitions. Closed trajectory systems like a pendulum have a constant arrow of time. Dissipative systems exhibit a pronounced arrow of time. In Markov Decision Processes (MDPs), the arrow of time is studied with a focus on dissipative behavior, which can be irreversible due to design or partial observability. The practical utility lies in assuming the MDP is dissipative, allowing for applications like reachability, safety, and curiosity analysis using the h-potential framework. The reachability of states in MDPs can be measured by the difficulty for an agent to move between them. Previous approaches involve learning a logistic regressor network to predict the probability of states being reachable. Our proposal introduces a directed measure of reachability using h-potential, which considers the difference in potential between states to determine reachability. The reachability measure in MDPs is determined by the difference in potential between states, denoted by h-potential. If a state is more likely to follow another state than vice versa, it is considered more reachable. This leads to a positive value for \u03b7(s \u2192 s) and a negative value for \u03b7(s \u2192 s). The reachability measure is additive, where \u03b7(s0 \u2192 s2) = \u03b7(s0 \u2192 s1) + \u03b7(s1 \u2192 s2). The current text discusses the application of inverse reinforcement learning to maximize a reward function using a random policy. It mentions the existence of powerful off-policy learning methods but states that a full analysis is beyond the scope of the current work. Additionally, it touches on Hamiltonian systems and the concept of reachability in dissipative systems. The text discusses the quantification of reachability between states in the context of AI-Safety. It highlights the importance of interpreting the h-potential and the conditions for guaranteeing reversibility in Markov processes. The problem of detecting and avoiding side-effects in AI safety is crucial for safe exploration. It involves preventing state transitions that permanently damage the agent or environment. This is related to reachability, where the agent must not take actions that reduce the reachability between states. Researchers have developed reset policies to address this issue. The text discusses the use of reset policies to prevent irreversible state transitions in AI safety. It contrasts the approach of measuring reachability relative to a safe baseline policy with the proposal of using a reachability measure directly to derive a reward term. The latter method avoids the need for a causal model of the environment to determine the counterfactual baseline state. The text introduces the concept of using a reachability measure directly to derive a reward term for safe exploration in reinforcement learning. By incorporating a scaling coefficient and a transfer function, the augmented reward penalizes less reversible state transitions while rewarding reversible ones. This approach aims to address the challenge of shaping a good reward function in reinforcement learning applications. The notion of curiosity plays a crucial role in encouraging novel behavior in reinforcement learning. However, the traditional approach of using prediction errors as curiosity rewards can lead to the noisy-TV problem. To address this, Savinov et al. (2018) suggest defining novelty based on reachability, where easily reachable states are considered less novel. This helps circumvent the noisy-TV problem and promotes more meaningful exploration in reinforcement learning. The h-potential and reachability measure \u03b7 offer a way to define curiosity rewards by encouraging agents to seek less reachable states, potentially learning useful skills in the process. This reward is independent of the external reward function defined by the environment but often aligns with it. The h-potential and reachability measure \u03b7 define a curiosity reward that encourages agents to reach less reachable states, potentially learning useful skills. This reward may not incentivize seeking diverse states and could lead agents to ignore interesting but less difficult-to-reach states. The h-potential unifies reachability, safety, and curiosity by discouraging difficult-to-reverse transitions and providing intrinsic reward for decreasing h-potential. The framework defines a general functional objective and converts it to a different approach for complex environments. The functional optimization problem is converted to a deep neural network parameter optimization. The training algorithm involves using a reference policy to sample trajectories and evaluating the objective with sample estimates. Regularizers prevent divergence, and the training process includes sampling state transitions and evaluating the objective function. In this section, the training procedure involves optimizing parameters to maximize the objective by adding a trajectory regularizer or using early stopping. The h-potential is empirically investigated in a 2D-world environment, showing an increase when a vase is broken irreversibly. The spikes in the h-potential indicate the count of destroyed vases. The h-potential is used to measure irreversibility by counting destroyed vases in a 2D-world environment. It can detect side effects in games like Sokoban and capture important features in environments like Mountain Car with Friction. The h-potential fails in non-dissipative environments without friction. In a 7x7 2D world environment, the h-potential is used to measure irreversibility by tracking the breaking of vases. The breaking of a vase corresponds to an increase in the h-potential in steps of constant size, indicating irreversibility. The h-potential measures irreversibility in a 7x7 2D world by tracking vase breakages in steps of constant size. Adding noise affects the h-potential, and a safety penalty reduces vase breakages in Sokoban puzzle game. The h-potential measures irreversibility in a 7x7 2D world by tracking vase breakages in steps of constant size. Pushing a box against a wall results in an irreversible side-effect, while moving the agent around is reversible. The h-potential increases when a box is pushed against a wall, but remains constant when the agent moves about. With friction, the state with the largest h is when the car is stationary at the bottom of the valley. Without friction, the car oscillates up and down the valley, resulting in a constant h-potential. This demonstrates that the h-potential has learned to detect side-effects. The environment is similar to the Mountain-Car environment but with friction to induce an arrow of time. The system is initialized in a random state to avoid exploration issues. The learned h-potential recovers terrain from random trajectories, allowing for intrinsic reward signal generation. The setting involves a particle undergoing Brownian motion influenced by a potential field. The dynamics of a stochastic process X(t) are governed by a stochastic differential equation with a temperature parameter. The Free-Energy functional F is defined as an energy functional minus entropy. It is a Lyapunov functional that defines an arrow of time. Training the learned arrow of time with realizations of X(t) in two dimensions shows how well it agrees with the Free-Energy functional. The H-functional, a Lyapunov functional of the dynamics, agrees well with the arrow of time defined by the Free-Energy functional F. It functions as an arrow of time in expectation over states. The arrow of time was approached in a Markov (Decision) Processes context, defined as the h-potential through an optimization problem. Roadblocks were identified and once cleared, reachability, safety, and curiosity can be unified under a common framework. Theoretical analysis of optimization problem in Eqn 1 for discrete Markov processes with enumerable states. Analytically evaluating solutions to validate consistency with intuition. Future work may explore connections to algorithmic independence of cause and applications in causal inference. The optimization problem in Eqn 1 for discrete Markov processes with enumerable states can be simplified to a scalar product p \u00b7 h in matrix notation. Analytical solutions for certain T can be derived, considering the norm of h or the norm of change in h i along trajectories. If T (h) = \u2212(2N ) \u22121 h 2, a solution is obtained. Proposition 1 states that for T(h) = \u2212(2N ) \u22121 h 2, the optimization problem in Eqn 10 is solved by setting the derivative of the objective to zero. This results in h = 0 if the Markov chain is initialized at equilibrium. An example is provided for a Markov chain with reversible transitions and specific parameters. The optimization problem in Eqn 10 is solved by setting the derivative of the objective to zero, resulting in h = 0 if the Markov chain is initialized at equilibrium. For \u03b1 = 1/2, a Markov chain with perfect reversibility is obtained, while for \u03b1 = 1, the transition from s2 \u2192 s1 is never sampled, leading to irreversible transitions. The weakness of the L2-norm-penalty used in Proposition 1 is exposed in Example 2. The dynamics of two Markov chains are described with transitions between states. The solution for h increases monotonously with timestep but remains 0 for certain states in both chains. The regularizer penalizes the change in h along trajectories, leading to a specific result. The regularizer penalizes changes in h along trajectories in the optimization problem, leading to a specific matrix-equation solution. This solution does not yield an explicit expression for h but is sufficient for analyzing individual cases in Examples 1 and 2. The regularization scheme in Eqn 16 yields specific solutions for Markov chains in Examples 1 and 2. In Example 1, equilibrium is reached when \u03b1 = 1/2, and h(s1) = h(s2) = 0. For a two-state Markov chain, the regularization leads to intuitive behavior for the solution h. In Example 4, a four-state Markov chain is considered with specific transition probabilities. The text discusses analytical solutions for Markov chains with specific transition probabilities, showing that the arrow of time increases along irreversible transitions. However, in real-world scenarios, finding analytic solutions may be infeasible due to unknown transition models or a large number of states. In real-world scenarios, finding analytic solutions for Markov chains may be infeasible due to the large number of states. To address this, parameterizing h as a neural network and training with stochastic gradient descent can optimize the functional objective. The experiments were conducted on a workstation with specific hardware specifications and the environment state comprises three binary images. The h-potential is parameterized using a two-layer deep ReLU network and trained on 4096 trajectories for 10000 iterations. Validation trajectories are used to generate plots and histograms show h values increase with time. Experiments test robustness by augmenting the environment state with images. In an experiment, a safe exploration penalty is derived using a learned arrow of time to guide the agent towards the goal state. Actions are rewarded based on the change in Manhattan norm of the agent's position to the goal, with penalties for steps exceeding the time limit. The safe agent's reward function is augmented with reachability, using a 3-layer deep 256-unit wide ReLU network trained via Duelling Double Deep Q-Learning. The discount factor is 0.99, with a linear decay exploration policy, and a replay buffer storing 10000 experiences. Fig 12a shows the probability of reaching the goal over iterations, while Fig 12b shows the expected outcome. The environment consists of a 7x7 2D world with watered tomato plants in each cell. The agent waters the plant it occupies, but if it doesn't, the plant loses moisture and can die. Using a safety penalty reduces the number of broken vases but makes reaching the goal harder. The state-space in the environment comprises two 7x7 images: one indicating the agent's position and the other quantifying the moisture held by the plant. An intrinsic reward is plotted against the amount of moisture gained by the watered tomato plant, showing the potential to define intrinsic rewards based on environmental information. The h-Potential is parameterized as a two-layer deep ReLU network and trained on trajectories to generate intrinsic rewards. The trajectories generated by a random policy show that the h-potential increases as plants lose moisture. When the agent waters a plant, the h-potential decreases correlating with the moisture gained. This relationship can define a dense reward signal for the agent. The reward function drops significantly when all plants have died, indicating the usefulness of h-potential for defining intrinsic rewards. The h-potential can define intrinsic rewards in a 5x5 2D world with different agent behaviors. Good agents remove a vase from a conveyor belt to prevent it from breaking, while malicious agents remove and replace it for rewards. Inept agents push the vase to a corner. The h-potential awards safety-rewards to these policies. The h-potential assigns safety rewards to different agent policies, including good, passive, malicious, and inept behaviors. These policies are tested using random trajectories, and the h-potential is trained with a 2-layer MLP model. The state space consists of 6 binary images of size 7 \u00d7 7 representing different objects. The h-potential rewards the good policy but penalizes passive, malicious, and inept policies. The safety performance measure assigns scores to different agent behaviors, with our method learning the consequences of actions like pushing or breaking a vase. The h-potential penalizes irreversible behavior, such as pushing a vase into a corner, in addition to falling off the belt. Safety reward from h-potential helps agents avoid irreversible actions. An example is given in the Sushi environment. The under-damped Pendulum environment simulates a pendulum with angle \u03b8 and angular velocity \u03b8. Dynamics are governed by a torque equation with constants m, l, g. Adapted from OpenAI Gym. In the Pendulum environment, dynamics are governed by a torque equation with constants m, l, g. The implementation includes an extra term \u03b1\u03b8 to simulate friction. Parameters are set as g = 10, m = l = 1, \u03b1 = 0.1, and torque \u03c4 is uniformly sampled from [-2, 2]. The h-Potential is parameterized by a two-layer 256-unit wide ReLU network, trained on 4096 trajectories of length 256 for 20000 steps with Adam. The batch-size is 1024, and a trajectory regularizer with \u03bb = 1 is used. The learned h-potential is plotted as a function of the state (\u03b8,\u03b8) and shows larger potential near \u03b8 = 0 due to dissipative action. The modified environment in Mountain Car has dynamics governed by an equation of motion with constants \u03b6 and \u03b1. Friction is simulated by adding an \u03b1\u1e8b term, and trajectories converge to (\u03b8,\u03b8) = 0 due to friction. The initial state is sampled uniformly from the state space, and a two-layer 256-unit wide ReLU network parameterizes the h-potential. The h-potential is trained using a two-layer 256-unit wide ReLU network on 4096 trajectories of length 256 for 20000 steps with Adam optimizer. The batch-size is 1024 and a trajectory regularizer with \u03bb = 1 is used. Random policies are commonly used for gathering trajectories but may lack exploration in complex environments. Strategies to address this issue in learning the h-potential are explored in this section. The h-potential is trained on random trajectories but fails to fully characterize the dynamics of the environment due to a lack of exploration. The Mountain Car with Damping task is chosen for its difficulty in exploration despite being small and easy to visualize. More work is needed to address this issue comprehensively. The approach described involves bootstrapping the h-potential using an exploratory policy alongside a random policy and trajectory buffer. Trajectories from the buffer are used to train the h-potential, showing a bias towards over-specialization in one section of the state-space. Contrasting results are seen when using exploratory policies to maximize random reward functions. The approach involves training an exploratory policy to minimize the h-potential, transitioning to difficult-to-reach states, and gathering trajectories to improve training stability. The h-potential and exploratory policies are reinitialized and trained from scratch at each iteration to counteract exploration bias. The exploratory policy is parameterized by a NoisyNet to aid exploration. To address the exploration bias, trajectories are pre-populated in the buffer using a random policy to counteract the circular adaptation between the exploratory policy and h-potential. This helps in avoiding focusing on one mountain over the other due to initial bias. The trajectory buffer is populated with diverse trajectories by using a random policy to gather trajectories from states reached by exploratory policies trained on random reward potentials. This strategy aims to counteract exploration bias and promote a more varied exploration of the state space. In conclusion, a preliminary investigation of strategies to address exploration issues in training the h-potential was presented. The strategy of bootstrapping was formulated but found to be prone to exploration bias. To overcome this bias, a population of exploratory policies trained on random reward functions was used to generate diverse trajectories for training the h-potential. The dynamics of the environment are defined by a stochastic equation. Further research is needed to explore the potential of combining bootstrapping with random rewards in complex environments. The dynamics of the environment are defined by a stochastic equation, with X(t) representing the particle's position. The potential is two-dimensional Ornstein-Uhlenbeck with a temperature parameter set to 0.3. Monte-Carlo sampling is used to estimate E x\u223c\u03c1(\u00b7,t) [\u03a8], while a non-parametric estimator is used for the differential entropy. A deep neural network is trained on trajectories using stochastic gradient descent with Adam. The network is regularized and linear adjustments are made for scaling. The arbitrary scaling of H and F is accounted for, with H being a Lyapunov functional that decreases monotonously with time. The arbitrariness arises from the dynamics being invariant to a constant shift in potential \u03a8, while the Free Energy functional F is not. The regularizing coefficient \u03bb controls the scale of the h-potential, which is arbitrary with respect to the scale of the Free Energy functional F."
}