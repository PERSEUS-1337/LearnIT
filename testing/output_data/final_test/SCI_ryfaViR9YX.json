{
    "title": "ryfaViR9YX",
    "content": "The Variation Network (VarNet) is a generative model that can manipulate high-level attributes of input data. It can learn dataset attributes on its own and has a probabilistic interpretation for navigating latent spaces and controlling attribute learning. Experimental results show VarNet can manipulate input data effectively and learn relevant attributes. It focuses on generating variations of input elements in a desired manner. The Variation Network (VarNet) is a generative model that can manipulate high-level attributes of input data. It aims to generate transformed versions of input elements with different attributes, allowing for abstract image editing and creative content generation. The model learns relevant attribute spaces and functions from labeled data, addressing the limitations of existing methods that assume given attributes. The Variation Network (VarNet) is a probabilistic neural network that can manipulate input data attributes to generate meaningful variations. The model can learn both fixed and free attributes, allowing for abstract image editing and creative content generation. The approach addresses the challenge of determining relevant attributes and provides empirical evidence of its effectiveness. The Variation Network (VarNet) is a probabilistic neural network that can manipulate input data attributes to generate meaningful variations. It provides a mechanism to control and shape learned attributes for interpretable controls over variations. The architecture allows for a wide range of choices in designing attribute functions, combining free and fixed attributes that can be continuous or discrete. The contributions include a widely applicable encoder-decoder architecture that generalizes existing approaches. The VarNet architecture allows for controlled input manipulation capabilities, a novel approach to navigate in the latent space, and ways to control learned attributes. The Neural Autoregressive Flows (NAF) BID10 involve feeding meta-parameters into a second neural network to obtain actual parameters. The discriminator D acts on Z * \u00d7 \u03a8. The paper presents the VarNet architecture and its training algorithm, introducing all components separately before discussing their interplay and motivation. The VarNet architecture, named Variation Network, combines principles from VAE and WAE architectures, utilizing adversarially learned regularization and a separate latent space for templates. It introduces a novel encoder-decoder architecture with the ability to generate variations in an intended way. The VarNet architecture combines VAE and WAE principles, utilizing adversarially learned regularization and a separate latent space for templates. The architecture includes a specific decoder network and encoder network. The training algorithm and model components are detailed in the following sections. The VarNet architecture combines VAE and WAE principles, utilizing adversarially learned regularization and a separate latent space for templates. Sample random features from the feature space using a specified method. The data x depends on a latent variable z through a decoder parametrized by a neural network. An approximate posterior distribution q(z|x) is used due to the intractability of the posterior distribution p(z|x). The encoder network in our approach conditions on an attribute space \u03a8, allowing for the reconstruction of x with specific features \u03c8. An attribute function \u03c6 is introduced to compute attributes of x based on its metadata, aiding in the correct reconstruction of x. The neural network aims to compute attributes of input data by decoupling a template from its attributes using a latent space Z*. This is achieved through a transformation parametrized by the feature space \u03a8, implemented with a Neural Autoregressive Flow (NAF) model. The mean reconstruction loss is calculated for loss c on X, with regularization on latent spaces Z* and Z. The encoder q(z|x, \u03c8) depends only on x and feature space \u03a8, aiming to reconstruct x with information potentially already contained in z. The encoder-decoder architecture is trained to fool a discriminator by producing a template code z* that discards information about the attributes of x. The discriminator evaluates the probability that the attributes and template code originate from the same (x, m) or are randomly generated. The encoder-decoder architecture aims to generate a template code z* that does not reveal information about the features of x. The discriminator is trained to maximize a certain value, while the architecture is trained to minimize another value. The text discusses the parametrization of attribute functions for sampling fake attributes without relying on existing data pairs. It distinguishes between continuous free attributes and fixed continuous/discrete attributes, introducing attribute vectors and an attention module. This approach is similar to the style tokens approach. The text discusses parametrizing attribute functions for sampling fake attributes independently of existing data pairs. It introduces attribute vectors and an attention module, similar to the style tokens approach. The \u03b1 i values vary to span a hypercube in the attribute space \u03a8, with learned v i values forming an adaptive basis. A probability distribution \u03bd is defined over \u03a8, allowing sampling of random attributes. The objective is detailed, emphasizing the learning of v i values and embeddings e m during training. In Sect. 3.4, the text discusses the choice of regularizations on the latent spaces Z * and Z, specifically using a KL regularization on Z * and an MMD loss on Z. The MMD-based regularization on Z * resulted in approximated posterior distributions with very small variances. The MMD regularization enforces the aggregated posterior to match the prior, affecting the variance of conditional probability distributions. Using KL regularization on Z * eliminates this effect, making it more suitable for VarNet. Introducing a scalar parameter \u03b2 allows control over the variance of q * (\u00b7|x). In this section, the regularization over Z is discussed. While it may be unnecessary, adding MMD regularization can improve reconstruction losses. The parametrization of the attribute function \u03c6 : X \u00d7 Z \u2192 R d is focused on, with the possibility of mixing different attribute functions and attention modules \u03b1. This allows for combining free and fixed attributes naturally and using neural networks with different properties. In this section, the regularization over Z is discussed, focusing on the parametrization of the attribute function \u03c6. Different distributions over attention vectors \u03b1 i are considered, along with the use of neural networks with various properties. The possibility of label-dependent free attributes is explored, allowing for diverse interpretations in applications. The choice of using a discriminator over \u03a8 is highlighted for encompassing discrete values within the same framework. VarNet offers a wide range of sampling schemes for generating random samples with specified attributes, making it a versatile model for various applications. Sampling involves decoding attributes and prior samples to generate variations of input data. VarNet offers diverse sampling schemes for generating random samples with specified attributes, allowing for versatile applications. The MMD regularization in the decoder helps fix the scale of the latent space Z, enabling continuous navigation within the space by adjusting \u03b1 values. This variation space has a probabilistic interpretation due to the feature space measure \u03bd. The architecture allows for diverse sampling schemes and a probabilistic interpretation of the displacement in the latent space. Different variations on templates can lead to significant displacements. The encoder network chosen is NAF 1 for a more expressive posterior distribution. Gaussian distributions are used as priors, and MMD regularization parameters are set as \u03bb = 10 and k(x, y) = C/(C+ x\u2212y 2 2 ). The Variation Network provides a unified probabilistic framework for controlled input manipulation, with parameters such as \u03bb = 10 and \u03b3 = 10. Sampling of \u03b1 values is done using a uniform distribution. The related literature includes the Fader networks paper, which also focuses on modifying attributes of input images in a continuous manner. The Variation Network uses an encoder-decoder architecture with an adversarial loss to modify input image attributes. It has a deterministic encoder and can only handle fixed attributes, unlike VarNet which can learn free attributes. Similar approaches include VAEs, WAEs, Fader networks, and Style Tokens paper. VarNet borrows the idea of conditioning an encoder model on style tokens but in a probabilistic framework. Our approach introduces a probabilistic framework for encoding attributes in the latent space of VAEs. Unlike traditional methods that rely on linear interpolations between two points, our approach considers fixed attributes and allows for more flexible exploration of the latent space. This method addresses the limitations of needing two points for exploration and arbitrary interpolation schemes. Additionally, it incorporates directions in the latent space that account for specific attribute changes, either found a posteriori or given a priori. The BID12 model proposes interpolation paths minimizing energy functional for curves instead of straight lines, but it is computationally demanding. Other trends involve a posteriori analysis on trained generative models for attribute manipulation, but they are costly and not suitable for real-time applications. The BID12 model suggests interpolation paths minimizing energy functional for curves, which is computationally demanding. In contrast, other approaches involve training a Generative Adversarial Network to enforce constraints on decoded output. These methods differ from our work as they operate in a single latent space, while we consider separate latent spaces. Additionally, BID0 introduces an interpretable lens latent space similar to our Z* space, with a joint optimization proposed for their model. The Variation Network is a generative model that can vary attributes of an input with fixed or learned attributes and a probabilistic interpretation. It offers flexibility in attribute function design and simplicity in transforming existing encoder-decoder architectures. Future work includes exploring more applications for this model. The Variation Network is a generative model that allows for varying attributes of an input with fixed or learned attributes. Future work includes extending the approach to handle partially-given fixed attributes and discrete free attributes, as well as investigating the use of stochastic attribute functions. Experiments on MNIST show results for different attribute functions and sampling schemes using a simple MLP with one hidden layer. The text discusses the use of free attribute space in a generative model, showing samples obtained with sampling procedures. It highlights how free attributes can capture high-level features like rotation, which are difficult to describe in an absolute way. The visualization of variations generated by varying the free attribute applies to all digit classes, irrespective of their label. The text discusses the use of free attributes in a generative model to create variations in writing styles for digits. By considering label-dependent attributes, different \"writing conventions\" can be smoothly transitioned between. The sampling scheme exploits encoder stochasticity to generate variations with fixed attributes. Stochasticity is deemed essential as there should be multiple ways to create variations, not just one. The parametrization of attribute functions plays a crucial role in capturing high-level features. Without label information, variations can differ significantly. Different variation spaces are shown in FIG5, related to thinness/roundness. Free attributes capture general dataset attributes, but some variation spaces may move too far from the original input. Adjusting the \u03b2 parameter can control the spread of variation spaces. VarNet proposes a novel solution to explore latent spaces by modifying the \u03b2 parameter in the objective equation. The architecture can decouple templates from learned attributes and control free attributes through the KL term or attribute function. Different high-level features can be captured depending on the fixed attributes. The space of variations using 1Free and 2Free attribute functions is displayed in FIG5 and 6c. Models trained with low KL penalty show different variations in the latent space. The VarNet architecture explores latent spaces by adjusting the \u03b2 parameter. It decouples templates from attributes and controls free attributes through the KL term or attribute function. The approach provides a probabilistic interpretation of attributes and variations, unlike techniques based on euclidean geometry. The attributes are distributed according to \u03bd, with variations seen as a subspace of smaller dimension in the latent space. Figure 7 illustrates this concept. The VarNet architecture explores latent spaces by adjusting the \u03b2 parameter, decoupling templates from attributes. It provides a probabilistic interpretation of attributes and variations, with variations seen as a subspace of smaller dimension in the latent space. Constant steps in the attribute space do not induce constant steps in the Z space, advocating for parametrizing attribute-related displacements in a latent space using flows conditioned on a simpler space. The VarNet architecture decouples templates from attributes by adjusting the \u03b2 parameter in the latent space, providing a probabilistic interpretation of variations. Constant steps in the attribute space do not directly translate to constant steps in the latent space, advocating for parametrizing attribute-related displacements using flows conditioned on a simpler space."
}