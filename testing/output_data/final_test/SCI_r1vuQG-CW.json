{
    "title": "r1vuQG-CW",
    "content": "Convolutional Neural Networks are effective due to their ability to exploit translation invariance. Group convolutions can also leverage rotation invariance, but are limited to certain transformations like integer translations and rotations by multiples of 90 degrees for performance reasons. In this paper, the authors demonstrate the implementation of planar convolution and group convolution over hexagonal lattices, which offer 6-fold rotational symmetry. HexaConv shows improved accuracy compared to square filters due to reduced anisotropy. The increased symmetry of hexagonal grids enhances parameter sharing in group convolutions, outperforming conventional CNNs on the AID aerial scene classification dataset. The implementation of planar convolution and group convolution over hexagonal lattices, known as HexaConv, has shown improved accuracy compared to square filters due to reduced anisotropy. The increased symmetry of hexagonal grids enhances parameter sharing in group convolutions, outperforming conventional CNNs on the AID aerial scene classification dataset. Learning features in neural networks has shifted from manual definition to automated learning, resulting in improved accuracy and reduced workload. The success of generalization still relies on the inductive bias encoded in the network architecture, with Convolutional Neural Networks (CNNs) being a prime example of successful architecture due to their exploitation of convolutional weight sharing and translation symmetry. The true label function is invariant to transformations beyond translations, such as rotations. Group convolutions can exploit this symmetry by defining convolutions for various transformation groups, not just translations. Hexagonal G-CNN utilizes group convolutions for improved feature learning. The filter transform for a filter on the group p6 involves a rotation and orientation channel cycling. Convolutional networks can be made equivariant to and exploit richer groups of symmetries, which was shown to be more effective than data augmentation. Group equivariant convolutional networks (G-CNNs) can be applied to any well-behaved group of symmetries. Group equivariant convolutional networks (G-CNNs) were initially implemented for discrete groups p4 and p4m, which include integer translations, rotations by multiples of 90 degrees, and mirror reflections. Implementing group convolution for a continuous group like the roto-translation group SE(2) requires interpolation to rotate filters, which can be computationally expensive and lead to numerical errors. To address this, the hexagonal grid allows for filter rotation by any multiple of 60 degrees without interpolation, enabling the definition of group convolutions for the groups p6 and p6m. Hexagonal pixelation offers advantages over square pixelation for group convolutions, with improved isotropic properties and efficiency for band-limited signals. Small hexagonal filters with 7 parameters outperform square 3x3 filters. Group convolution is more effective on a hexagonal lattice due to increased weight sharing. The use of hexagonal pixelation in convolutional networks offers improved accuracy and efficiency due to increased weight sharing and symmetry. Challenges include resampling images from a square to a hexagonal lattice and storing hexagonal images efficiently for fast convolution implementation. Various indexing schemes for hexagonal lattices are reviewed, and optimized square convolution routines can be leveraged for hexagonal convolution. The method of hexagonal convolution is evaluated on CIFAR-10 and Aerial Image Dataset (AID) BID21, showing improved performance over baseline models. Group convolutions are shown to enhance data efficient learning and outperform comparable architectures. G-HexaConvs source code is available on Github. The paper also discusses the theory of group equivariant networks in Section 2. In Section 2, the theory of group equivariant networks is summarized. Section 3 discusses different coordinate systems on the hexagonal grid, while Section 4 covers the implementation details of hexagonal G-convolutions. Section 5 introduces experiments and results, followed by an overview of related work in Section 6. The theory of G-CNNs is reviewed, emphasizing translation and rotation equivariance in convolutions. The convolution of a rotated image by a filter is not rotation equivariant. G-convolution is obtained by changing the translation to a transformation from a larger group G. It is defined for a group G and input space X, with signals defined on X and a transformation in G. The standard convolution operation is a special case of G-convolution for X = G = Z2. In a G-CNN, the input is an image with X = Z. The coordinate system uses three integer indexes and both axial and cube coordinate systems may have negative indices with a top left origin. Group convolution is defined mathematically by Equation FORMULA2, but not algorithmically. G-Conv is implemented as a two-step computation: filter transformation (H) and planar convolution (Z2). G-CNNs use two types of group convolutions for planar images and feature maps on group G. The filter transformation step is explained by introducing H in and H out. The group convolution process involves two steps: filter transformation and planar convolution. The filter transformation step introduces H in and H out, where H in is a trivial group and H out is a group of discrete rotations. The input for the filter transformation is a learnable filterbank \u03a8, and the output is obtained by applying each transformation in H out to each filter. The planar convolution is then performed on the input with the transformed filterbank. The hexagonal grid can be indexed using different coordinate systems, each with varying characteristics like memory efficiency and ease of applying rotations. Some systems may require extra memory for padding when storing a rectangular image, and standard planar convolution routines may not work for hexagonal convolutions in certain coordinate systems. The coordinate system for a hexagonal lattice can affect memory efficiency and rotation application. Different systems have varying characteristics, with some requiring extra memory for padding when storing rectangular images. Standard planar convolution routines may not work for hexagonal convolutions in certain coordinate systems. The hexagonal lattice structure uses an axial coordinate system for pixel representation, which requires additional padding for image storage. This system allows for applying rectangular convolution kernels to hexagonal images stored in a rectangular buffer. The cube coordinate system is used for applying transformations to coordinates, while other systems are used for storing images. Rotations and reflections can be easily expressed in the cube system. A counter-clockwise rotation by 60 degrees can be performed with a specific formula, and mirroring over the vertical axis through the origin is computed accordingly. The double width system is based on two orthogonal axes, with the u-coordinate incremented by 2 when stepping to the right by 1 unit in the hexagonal lattice. In the hexagonal lattice, stepping to the right by 1 unit increments the u-coordinate by 2, creating a checkerboard pattern that doubles image and filter size. HexaConv implements hexagonal convolution as rectangular convolution on checkerboard arrays, maintaining sparsity pattern. It is easy to implement but inefficient, recommended for preliminary experiments. The offset coordinate system uses two orthogonal axes for hexagonal lattice navigation. In the offset system, a one-unit horizontal step in the hexagonal lattice corresponds to a one-unit increment in the u-coordinate. Rectangular images can be stored efficiently without padding. Hexagonal convolutions cannot be expressed as a single 2D convolution due to different filter shapes for even and odd rows. Implementing hexagonal convolution in the offset system requires two convolution calls, one for even and one for odd rows, with the results copied to a single buffer. In the offset system, hexagonal convolutions require two convolution calls for even and odd rows, with results copied to a single buffer. A custom HexaConv kernel could simplify this process and improve memory efficiency. The group convolution involves filter transformation and hexagonal convolution steps, using the Axial coordinate system for implementation. The filter transformation procedure also applies to hexagonal group convolution. In practice, convolution with filters and feature maps on the hexagonal lattice involves padding to accommodate planar convolution routines. To address non-zero output in padding areas, a masking operation is performed after each convolution operation. Experiments on CIFAR-10 and AID datasets compare the accuracy of G-HexaConvs to existing G-networks. The accuracy of G-HexaConvs is compared to existing G-networks on CIFAR-10 dataset. The use of hexagonal lattice improves performance, especially with p6-convolutions. G-ResNets consist of 3 stages with 4 blocks per stage, utilizing spatial pooling to maintain orientation equivariance. The HexaConv network maintains orientation equivariance by resampling input images to a hexagonal lattice. Results show HexaConv CNN outperforms standard CNN on CIFAR-10. Using groups with more symmetries consistently improves performance. AID dataset consists of 10000 satellite images. The AID dataset contains 10000 satellite images with 30 target classes. Experiments involve an 80% train/20% test split, different from the 50% split in BID21. Models are evaluated on randomly selected splits for fair comparison. A baseline model from BID21 using VGG16 and SVM is re-evaluated on the 80%/20% split. G-networks with ResNet architectures are also tested. The G-networks with ResNet architectures for the AID dataset have widened networks with two blocks per stage and pooling applied to spatial dimensions and orientation channels. Results show a decrease in error from 19.3% to 8.6% on a p6-ResNet. Mirror symmetry did not significantly impact performance, suggesting it is not an effective inductive bias for AID. The baseline model was pretrained on ImageNet, while our models were trained from random initialization. Group convolutions can significantly improve performance, especially when dataset symmetries match the selected group. The shift from square to hexagonal sampling lattice for image processing has been studied extensively. Hexagonal lattices, with isoperimetry and uniform connectivity, are considered a more natural way to tile the plane. In certain applications, hexagonal lattices have shown superiority over square lattices. Transformational equivariant representations have garnered research interest, with methods like pose normalization and projections to the sphere for invariant representations. Approximate transformational invariance can be achieved through data augmentation. Various approaches have been introduced to obtain equivariant or invariant CNNs with respect to specific data transformations. These methods include rotating filters or feature maps, applying channel permutations, and utilizing steerable filters for group convolutions. Harmonic Networks and Deep Symmetry Networks are examples of CNNs that aim to achieve approximate equivariance with respect to continuous rotations. Deep Symmetry Networks BID6 leverage sparse high dimensional feature maps for handling high dimensional symmetry groups, achieving rotational equivariance through filter rotations and pooling. BID18 propose parameter-sharing schemes for equivariance in neural networks, while Spatial Transformer Networks BID10 and Polar Transformer Networks BID5 use spatial transformations to create invariant models. Henriques & Vedaldi (2016) achieve invariant CNNs by warping input and filters with predefined warps, limited to global symmetries. Filter transformation computes Lr\u03c8 for each rotation r \u2208 H and filter \u03c8, increasing output channels by |H|. Rotation operator Lr is defined as [Lr\u03c8](h) = \u03c8(r\u22121h). The rotation operator Lr is defined as [Lr\u03c8](h) = \u03c8(r\u22121h). To implement this as a single indexing operation \u03a8[I], an array \u03a8 stores the filter and I is a precomputed array of indices related to the elements of the group. The array \u03a8 represents the function \u03c8 : X \u2192 R when combined with an invertible indexing function \u03b9 : X \u2192 I. The transformation operator Lr is implemented by converting indices to group elements, composing with r^-1, and then converting back to indices to index \u03a8. This is done in one step using a precomputed indexing array I. Group elements are represented as rotation-translation pairs, allowing for easy composition, and an indexing map \u03b9 is chosen. The transformation operator Lr is implemented by converting indices to group elements, composing with r^-1, and then converting back to indices to index \u03a8. This is done in one step using a precomputed indexing array I. Group elements are represented as rotation-translation pairs, allowing for easy composition. The translation can be encoded in the Axial coordinate frame as a pair of integers u, v. The rotational composition reduces to addition modulo 6, while r^-1 t can be computed by converting to the Cube coordinate system and using Equation 5."
}