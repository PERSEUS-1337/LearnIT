{
    "title": "Skgge3R9FQ",
    "content": "Convolutional Neural Networks (CNNs) are powerful in computer vision but struggle with classifying unknown samples and adversarial examples due to over-generalization. To address this, a CNN with an extra output class can control over-generalization effectively. By using a representative natural out-distribution set and interpolated in-distribution samples for training, this approach shows promising results. In this work, a simple measurement is proposed to assess the fitness of an out-distribution set. Training an augmented CNN with representative out-distribution datasets and interpolated samples improves its ability to handle unseen out-distribution samples and black-box adversarial examples. Additionally, the generation of white-box adversarial attacks becomes more challenging with the augmented CNN, as attack algorithms need to navigate rejection regions. Despite the success of CNNs in computer vision, vulnerabilities to different types of attacks persist. CNNs are vulnerable to adversarial examples and out-distribution samples, which can mislead them into wrong classifications with high confidence. This susceptibility raises concerns for deploying CNNs in real-world applications, especially for security-sensitive ones. The vulnerability of CNNs to adversarial examples and out-distribution samples is a serious concern for security-sensitive applications. Researchers have proposed different approaches to address these issues, including threshold-based post-processing for out-distribution samples and adversarial training for adversarial examples. The vulnerability of CNNs to adversarial examples and out-distribution samples is a serious concern for security-sensitive applications. Researchers have proposed different approaches to address these issues, including threshold-based post-processing for out-distribution samples and adversarial training for adversarial examples. The performance of these approaches depends on having access to a diverse set of training adversaries, which is computationally expensive. Deep neural networks are prone to over-generalization in the input space, regardless of the fact that in-distribution samples may only be relevant to a small portion of the input space. This paper proposes that an augmented CNN can alleviate these issues through control of over-generalization. An augmented CNN with an extra class called dustbin is proposed to address over-generalization issues. The goal is to learn a more expressive feature space by including a distinct sub-manifold for the dustbin class, reducing over-generalized regions induced by a naive CNN. This approach differs from threshold-based post-processing methods and aims to improve the network's ability to distinguish between different classes. The proposed approach introduces an extra class called dustbin to mitigate over-generalization in CNNs. Instead of using computationally expensive synthetic samples, natural out-distribution datasets can be utilized for training the dustbin class, providing a cost-effective solution. The paper proposes a method to assess the fitness of a natural out-distribution set for a given in-distribution task. It also suggests generating artificial out-distribution samples by interpolating in-distribution samples. The use of an augmented CNN can help identify unseen out-distribution samples and adversarial attacks. The approach aims to reduce the risk of misclassification by limiting over-generalization regions in CNNs. The paper introduces a method to select a representative natural out-distribution set for training augmented CNNs, reducing misclassification rates for unseen out-distribution sets and black-box adversarial examples. The augmented CNN can effectively identify unseen out-distribution samples and adversarial attacks, minimizing over-generalization regions in CNNs. The paper proposes using a CNN augmented with a dustbin class to limit over-generalization in adversarial attack algorithms. Training the CNN on a representative set of out-distribution samples helps in achieving an effective augmented model. The choice of training samples for the dustbin class is crucial for the model's performance. The augmented MLP, when trained on dustbin samples near the decision boundary, only slightly reduces over-generalized regions. However, classifying some adversaries as dustbin can make generating new adversaries harder. Using out-distribution dustbin samples for training also cannot adequately cover over-generalized regions. Training an augmented MLP on out-distribution samples near decision boundaries does not effectively reduce over-generalization. Out-distribution training samples uniformly distributed with in-distribution classes can serve as a representative set for an extra dustbin class. An augmented MLP trained on this representative set can classify a wide range of unseen out-distribution sets and adversaries, improving control over over-generalization. Acquiring training samples for the extra class can range from artificially generated to natural out-distribution sets. Coupling a representative out-distribution set with samples near decision boundaries can further strengthen the augmented neural network against adversarial examples. Instead of using a generator, two cost-effective resources are proposed for acquiring dustbin training samples to train augmented CNNs: a selected natural out-distribution set and interpolated samples. Natural out-distribution datasets contain samples that are statistically and semantically different from the task samples, such as NotMNIST and Omniglot datasets. The challenge lies in selecting a representative set from the available datasets to properly train augmented CNNs. To train augmented CNNs effectively, a representative natural out-distribution set is needed. A visualization metric is introduced to assess the suitability of out-distribution sets for a given task. The goal is to have out-distribution samples misclassified uniformly across in-distribution classes. This is determined by visualizing the misclassification distribution using a histogram. A more uniform distribution indicates a better fit for training an augmented CNN. In Fig. 2, SVHN vs CIFAR-100 and LSUN vs DS-ImageNet are compared as out-distribution sets for CIFAR-10 and CIFAR-100 respectively. SVHN samples show limited misclassification into CIFAR-10 classes, while CIFAR-100 exhibits a more uniform misclassification pattern. CIFAR-100 is considered a more representative out-distribution set for CIFAR-10. LSUN vs DS-ImageNet also show similar behavior, with DS-ImageNet having a more uniform distribution. In this case, DS-ImageNet has a more uniform distribution compared to LSUN. Adversarial examples near decision boundaries can improve adversary identification by augmented CNN, but generating diverse adversarial examples for large-scale datasets is computationally expensive. Using adversaries as training samples without including natural data may not be sufficient. Instead of using adversaries as training samples, a new method proposes acquiring samples around decision boundaries by interpolating pairs of in-distribution samples from different classes. This approach aims to cover regions between classes to improve classification accuracy. The feature space of a CNN is utilized to generate interpolated samples by combining pairs of input samples. Nearest neighbors are found in the feature space for computational efficiency and accuracy. Training an augmented CNN allows for learning an extra sub-manifold corresponding to an additional class, such as a dustbin. This enables the CNN to map a variety of out-distribution sets onto its extra sub-manifold, aiding in distinguishing between in-distribution and out-distribution samples. Training an augmented CNN creates a feature space that separates in-distribution from out-distribution samples, unlike a naive CNN where they are mixed. The augmented CNN can map black-box adversaries to an extra manifold and classify some adversaries correctly while others are rejected. This leads to a more effective classifier in real-world applications. The feature spaces achieved from a naive CNN and its augmented counterpart for CIFAR-10 as an in-distribution task are compared. CIFAR-100 is used as the training set for the extra class of the augmented CNN. The two out-distribution sets, CIFAR-100 and Fast Gradient Sign adversaries, are separated in the feature space of the augmented CNN but mixed in the feature space of the naive CNN. The data distribution in the last convolution layer of the augmented CNN is visualized in 3D using PCA. Several experiments are conducted on MNIST, CIFAR-10, and CIFAR-100 datasets using different neural network architectures. The study evaluates the robustness of augmented CNNs using MNIST, CIFAR-10, and CIFAR-100 datasets with different neural network architectures. Five attack algorithms are considered, and performance is measured using accuracy, rejection rate, and error rate metrics. The study evaluates the robustness of augmented CNNs against transferable black-box attacks by generating adversarial samples using a naive CNN with different initial weights. Different sources for acquiring dustbin training samples are employed to identify adversaries. The study evaluates the generalization performance of augmented CNNs on in-distribution tasks, showing a slight drop in test accuracy rates but reduced error rates. The augmented CNNs also exhibit a rejection rate in addition to accuracy and error rates, which can be beneficial for security-sensitive applications. Augmented CNNs trained on I-FGS adversaries can reject FGS variants but fail against non-FGS adversaries and natural out-distribution sets. Using I-FGS adversaries alone for training does not effectively control over-generalization. Contrary to I-FGS augmented CNNs, models trained on a representative natural out-distribution set show improved performance. The study shows that augmented CNNs trained on a representative out-distribution set, along with some interpolated samples, outperform their naive counterparts and other augmented CNNs in reducing error rates on various adversaries. Training on non-representative natural out-distribution sets leads to over-generalization. Visual comparisons of classification regions between augmented CNNs and naive counterparts are also conducted. The augmented CNN outperforms the naive counterpart in reducing error rates on various adversaries. Visual comparisons show fooling classification regions of naive CNNs are occupied by dustbin regions in augmented counterparts. White-box adversarial examples are generated directly from the model. Robustness of augmented CNNs is evaluated on different white-box attacks. The experiments involve computing the percentage of visiting fooling classes and the dustbin class when moving in the direction of an attack method for clean samples. The attack algorithm must avoid dustbin regions to prevent generating useless adversaries. Results for legitimate directions are reported with varying values. White-box adversaries are generated using both a naive CNN and its augmented counterpart on MNIST and CIFAR-10 test sets. The experiments involve generating white-box adversaries using a naive CNN and its augmented counterpart on MNIST and CIFAR-10 test sets. Adversaries for augmented CNNs encounter the dustbin class more often than a fooling class, making it harder to generate adversaries. This results in the need to skip over dustbin regions, potentially increasing the number of steps or distortions required. The augmented CNNs tend to remain in the current true classes by moving in legitimate directions. Their behavior is evaluated on various out-distribution sets across different in-distribution tasks. The comparison of augmented CNNs and threshold-based approaches on various out-distribution sets shows rejection rates at specific True Positive Rates (TPR). The approaches aim to identify and reject out-distribution samples based on calibrated predictive confidence scores. In experiments with ODIN, hyperparameters are tuned to achieve the highest TNR at specified TPR. The calibrated CNN approach uses a beta parameter to control the effect of calibrated predictions on out-distribution samples. Larger beta values improve calibration for MNIST and CIFAR-10 but hinder convergence for CIFAR-100. Beta values of 1 and 0.01 are used for CIFAR-10/MNIST and CIFAR-100, respectively. The rejection rates of the augmented CNNs are compared with ODIN and \"calibrated CNN\" for CIFAR-10/MNIST and CIFAR100. The augmented CNNs, trained on a representative natural out-distribution set, almost outperform \"calibrated CNN\" and ODIN. The paper proposes augmented CNNs as a solution for controlling over-generalization in the presence of novel unseen out-distribution sets. By training on a specific set of dustbin samples, the augmented CNNs demonstrate effectiveness in addressing issues related to adversarial examples and high confidence prediction for out-distribution samples. The selection of an appropriate natural out-distribution set for training plays a crucial role in the effectiveness of augmented CNNs. Reducing over-generalization through augmented CNNs improves misclassification error rates on adversaries and out-distribution samples while maintaining accuracy on in-distribution samples. MNIST dataset with NotMNIST is used for training and testing, with NotMNIST providing out-distribution samples. The augmented CNN model used in the study includes LeNet with three convolution layers and one Fully Connected layer. It is trained on 50k MNIST samples, 10k NotMNIST samples, and 15k interpolated MNIST samples. CIFAR-10 and CIFAR-100 datasets consist of low-resolution RGB images of objects with training and testing instances over multiple classes. CIFAR-10 has 50k training and 10k testing instances across 10 classes, while CIFAR-100 has 100 classes. Out-distribution samples for CIFAR-10 experiments are taken from CIFAR-100, with certain classes excluded to avoid semantic overlaps. Pixels are scaled and normalized for CIFAR-10. VGG-16 BID24 is used as the CNN architecture for CIFAR-10, with 13 convolution layers and three FC layers. The augmented VGG-16 is trained with 15k samples from CIFAR-100 and 15k interpolated samples from CIFAR-10 training set. The CIFAR-100 dataset consists of 50K training and 10K testing RGB images, similar to CIFAR-10. Out-distribution samples are taken from a down-scaled version of ImageNet dataset called DS-ImageNet. A training set for the out-distribution dustbin class is created by selecting samples from 62 classes with less overlap with CIFAR-100 labels. The training set has 79,856 samples, with 15K randomly selected for training augmented CNNs. ResNet-164 BID9 is used to train the augmented CNN on CIFAR-100. We use ResNet-164 BID9 to train an augmented CNN on CIFAR-100 as the in-distribution task. A non-representative natural out-distribution set may not effectively handle over-generalization. SVHN samples are mostly misclassified into a limited number of CIFAR-10 classes, while CIFAR-100 exhibits a more uniform misclassification. Therefore, CIFAR-100 is considered a more representative natural out-distribution set for CIFAR-10. The DS-ImageNet dataset is considered a more appropriate out-distribution dataset for CIFAR-100 compared to LSUN. Choosing a representative out-distribution set is crucial for reducing over-generalization and confidently classifying unseen samples as dustbin. The augmented VGG-16 trained on CIFAR-100 performs better at rejecting adversaries and unseen out-distribution samples compared to the augmented VGG used for SVHN. Similarly, the augmented ResNet trained with LSUN as the source of out-distribution samples shows improved performance for CIFAR-100 as the in-distribution task. The Resnet trained with LSUN as the source of out-distribution samples is less effective in reducing over-generalization compared to the Resnet trained with DS-ImageNet. The error rates of the augmented CNN on various types of black-box adversaries are lower than ODIN due to higher adversaries rejection rates. Our method reduces over-generalization, leading to a more expressive feature space where out-distribution samples and black-box adversarial examples are classified as belonging to the dustbin class. Adversarial instances can be placed close to their true class, allowing augmented CNNs to classify them correctly. Adversarial generation methods can be targeted or untargeted, with targeted attacks aiming to misclassify to a specific class. Targeted and untargeted attack algorithms aim to manipulate CNNs to misclassify images. Targeted Fast Gradient Sign (T-FGS) minimizes the loss function for a different target class, while the untargeted variant (FGS) aims to misclassify to any class other than the true label. The transferability of T-FGS samples increases with larger hyperparameters, but adds more distortion to the image. The iterative methods for creating adversarial attacks include Iterative Fast Gradient Sign (I-FGS), DeepFool, and Carlini Attack (C&W). I-FGS adds small perturbations iteratively to generate optimal distortions, DeepFool creates sub-optimal perturbations to transfer samples across decision boundaries, and Carlini Attack focuses on finding adversarial examples over loss. The Carlini Attack (C&W) BID3 method defines a new objective function to optimize misclassification by maximizing the difference between output scores. Hyperparameters are crucial in attack algorithms, with details provided in experimental evaluation. The targeted Carlini attack was generated using the authors' github code, considering 100 randomly selected images due to the high time complexity of the method. For the Carlini Attack (C&W) method, 200 adversarial examples were generated by selecting 100 images for each dataset. The target classes were the least likely and second most likely according to CNN predictions. Transferability was increased by using \u03ba = 20 for MNIST and \u03ba = 10 for CIFAR-10, with a higher value of \u03ba (= 20) for CIFAR-100."
}