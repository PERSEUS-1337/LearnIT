{
    "title": "HJlfuTEtvB",
    "content": "Program verification is crucial for ensuring program correctness and eliminating bugs. Continuous Logic Network (CLN) is a new neural architecture that learns loop invariants from program execution traces using Satisfiability Modulo Theories (SMT) formulas. CLN2INV, an inference system based on CLNs, outperforms existing approaches on the Code2Inv dataset. CLN2INV is a tool that solves all 124 problems in the Code2Inv dataset, taking only 1.1 seconds on average per problem, which is 40 times faster than existing approaches. It can even learn 12 more complex loop invariants than required. Program verification is essential for eliminating bugs, but manual effort can be costly. Automated verification of programs with unbounded loops remains a challenge. Automated verification of programs with loops remains a challenge, especially in inferring correct loop invariants. Existing approaches struggle to learn complex, real-world loop invariants. A new approach is introduced in this paper, using a neural architecture to model loop behavior from program execution traces. The paper introduces a novel neural architecture, Continuous Logic Network (CLN), to efficiently infer loop invariants from program execution traces. Existing methods lack precision in extracting logical rules from neural architectures, while inductive logic learning is not expressive enough for verification purposes. The CLN aims to address these limitations by representing loop invariants as explicit SMT formulas for program verification. The paper introduces a novel neural architecture, Continuous Logic Network (CLN), to efficiently learn explicit and precise representations of SMT formulas using continuous truth values. The CLN can represent learned SMT formulas explicitly in its structure, allowing for precise extraction of the exact formula from a trained model. A new semantic mapping for SMT formulas to continuous truth values is introduced, building on basic fuzzy logic to support general SMT formulas in a continuous logic setting. The semantic model is proven to be sound and complete with regard to the discrete SMT formula space, enabling CLNs to represent any quantifier-free SMT formula operating on mixed integer-real arithmetic as an end-to-end differentiable model. The paper introduces CLN2INV, a new inference system for loop invariants that outperforms state-of-the-art tools on the Code2Inv dataset by solving all 124 theoretically solvable problems. CLN2INV finds invariants for each program in 1.1 second on average, 40 times faster than LoopInvGen. It can learn complex loop invariants with combinations of multivariable constraints through backpropagation. The main contributions include a new semantic mapping for assigning continuous truth values to SMT formulas and the ability to learn formulas through backpropagation. The paper introduces CLN2INV, a new inference system for loop invariants that efficiently learns precise SMT formulas through backpropagation. It outperforms existing methods by solving all 124 theoretically solvable problems in the Code2Inv dataset, finding invariants 40 times faster on average. Additionally, CLN2INV can learn complex loop invariants with multivariable constraints. CLN2INV efficiently learns complex loop invariants directly from execution traces, outperforming existing methods by solving all theoretically solvable problems in the Code2Inv dataset. It can find invariants 40 times faster on average and handle multivariable constraints. Our work focuses on learning precise SMT formulas directly, allowing for richer predicates with compact representations in a noiseless setting. Various numerical relaxations have been applied to SAT and SMT solving, with recent work utilizing recurrent and graph neural networks for Circuit SAT problems. FastSMT uses natural language processing embeddings for formula representation, while our approach relaxes the SMT constraints differently. In this section, the problem of inferring loop invariants using Satisfiability Modulo Theories (SMT) is introduced. Loop invariants capture loop behavior regardless of the number of iterations and are essential for verifying programs with loops. The goal is to find a loop invariant that can be derived from a precondition and implies a postcondition. Loop invariants are crucial for verifying programs with loops, capturing loop behavior regardless of the number of iterations. Satisfiability Modulo Theories (SMT) allows reasoning about complex problems efficiently. Loop invariants are usually encoded with quantifier-free SMT, with a desired invariant being a boolean function over program variables. The correct invariant for a program is crucial for ensuring the program's correctness. SMT is defined as a way to reason about complex problems efficiently. It includes loop invariants encoded with quantifier-free SMT formulas. Nonlinear arithmetic theories allow for higher-order terms to express more complex constraints. Basic fuzzy logic uses continuous truth values and t-norms to ensure consistent logic behavior. In fuzzy logic, t-norms satisfy conditions like commutativity, associativity, and monotonicity. They must also be continuous. A common t-norm is the product t-norm x \u2297 y = x \u00b7 y with its associated t-conorm x \u2295 y = x + y \u2212 x \u00b7 y. A continuous semantic mapping, S, for SMT on BL is introduced, preserving formula semantics with continuous truth values. This mapping is analogous to constructing t-norms for BL, operating on continuous logical inputs. Three properties are defined for S to preserve formula semantics. Continuous semantic mapping S is introduced for SMT on BL, preserving formula semantics with continuous truth values. S should be consistent with BL, differentiable almost everywhere, and increasing as terms approach constraint satisfaction and decreasing as terms approach constraint violation. The mapping is defined for certain logical operators, with others derivable from these definitions. The mapping S for boolean operators \"\u2227\", \"\u2228\", and \"\u00ac\" is defined using BL. Specific t-norm \u2297 and t-conorm \u2295 can be used to define mappings for these operators. The mapping S for boolean operators \"\u2227\", \"\u2228\", and \"\u00ac\" is defined using a specific t-norm \u2297 and t-conorm \u2295. This mapping allows for standard arithmetic operations to be used normally and mapped to continuous truth values while maintaining differentiability. Additionally, expressions in SMT with integer or real-valued results can be mapped to continuous logical values through these formulas. Continuous Logic Networks (CLNs) are constructed based on a continuous semantic mapping for SMT on BL. CLNs provide a neural architecture for learning SMT formulas, with learnable coefficients and smoothing parameters acting as activation functions. CLNs are built from an SMT Formula Template, where values are marked as input terms, constants, or learnable parameters, and dynamically constructed as a computational graph. Continuous Logic Networks (CLNs) are constructed as computational graphs based on a continuous semantic mapping for SMT on BL. CLNs are trained with optimization to maximize the expected value by minimizing a loss function that penalizes outputs less than one. The optimization includes selecting a minimum scaling factor, applying hinge loss to scaling factors, and regularizing the offset. Hyperparameters \u03bb and \u03b3 govern the weight assigned to the scaling factor. Continuous Logic Networks (CLNs) are trained with hyperparameters \u03bb and \u03b3 to optimize a loss function for SMT on BL. The resulting continuous SMT formula learned by the CLN is consistent with an equivalent discrete SMT formula, proving soundness, completeness, and convergence to a globally optimal solution for formulas expressed as linear equalities. The CLNs are used to implement a new inference system, CLN2INV, for learning loop invariants directly from execution traces. It differs from other systems by learning a loop invariant formula from trace data. The architecture involves preprocessing the program by performing static analysis and instrumenting it to collect necessary data for training. The CLNs are used to implement a new inference system, CLN2INV, for learning loop invariants directly from execution traces. It differs from other systems by learning a loop invariant formula from trace data. The architecture involves preprocessing the program by performing static analysis and instrumenting it to collect necessary data for training. To record all program variables before each loop execution and after the loop termination, the loop is restricted to terminate after a set number of iterations. Training data is generated by running the program repeatedly on randomly initialized inputs that satisfy preconditions. Unconstrained variables are initialized from a uniform distribution centered on 0 with width r, where r is a hyper-parameter of the sampling process. For variables with precondition constraints, they are initialized from a uniform distribution adjacent to their constraints with width r. The hyper-parameter r is set to 10 for all experiments in the paper. Template generation is done in three stages, starting from direct pre-condition and post-condition templates, then extracting clauses for conjunctions and disjunctions, and finally creating more complex templates with equality and inequality constraints. Higher order terms detection is also discussed. To detect higher order terms in the invariant, log-log linear regression is performed on loop variables. If superlinear growth is detected, higher order polynomial terms are added to the template. CLN construction involves representing equality constraints as Gaussian-like functions for optimization. The model is trained using execution traces, and invariant checking is done using SMT solvers like Z3. After training the CLN for a formula template, SMT solvers like Z3 are used to recover the loop invariant. If the correct invariant is not found, a more expressive template is used for further search. CLN2INV is compared to Code2Inv and LoopInvGen methods, showing efficacy on challenging problems. Ablation studies are conducted to justify design choices. CLN2INV outperforms LoopInvGen and Code2inv in solving benchmark problems, with an average solving time of 1.1 seconds. It requires fewer Z3 calls and shows efficient scaling to complex problems. CLN2INV outperforms LoopInvGen and Code2inv in solving benchmark problems by requiring fewer Z3 calls and showing efficient scaling to complex problems. It strikes a balance between generating candidate invariants quickly like Code2Inv and producing more accurate invariants like LoopInvGen, resulting in lower overall runtime. It excels in solving difficult loop invariant inference problems involving multivariable constraints and polynomials with higher order terms. CLN2INV excels in solving difficult loop invariant inference problems involving multivariable constraints and higher order polynomial invariants. It outperforms LoopInvGen and Code2inv by finding correct invariants for complex problems quickly and efficiently. CLN2INV can correctly learn the invariant for 1st and 2nd order power summations, but struggles with 3rd, 4th, or 5th order summations due to the higher number of terms. Disabling model training limits CLN2INV to static models, solving 91 problems in the dataset. The CLN2INV system can solve simple problems using basic heuristics without training, but for more complex problems, CLN learning is essential. A new neural architecture is introduced to learn SMT formulas through backpropagation, achieving sound and complete results. The CLN2INV system is able to solve all theoretically solvable problems in the Code2Inv benchmark in just 1.1 seconds on average. The CLN architecture is expected to be beneficial for learning SMT formulas in other domains as well. Theorem 1 states that for any quantifier-free linear SMT formula F, there exists a CLN model M as long as the t-norm used satisfies a specific property. The proof involves combining results for different t-norms and showing that the CLN model constructed from S(F) preserves the truth value of F. This demonstrates the soundness and completeness of the CLN model in representing SMT formulas. The proof of Theorem 1 involves constructing a model M for formula F, showing it satisfies specific equations, and demonstrating the soundness and completeness of the CLN model in representing SMT formulas. The proof of Theorem 1 involves constructing a model M for formula F, showing it satisfies specific equations, and demonstrating the soundness and completeness of the CLN model in representing SMT formulas. We know 0 \u2264 M(x; B, ) \u2264 1. In the Negation Case, if F = \u00acF, F can be represented by models M satisfying Eq. (1)(2). In the Conjunction Case, if F = F1 \u2227 F2, F1 and F2 can be represented by models M1 and M2, and their composition M(x; B, ) is continuous. The proof involves constructing a model M for formula F, showing it satisfies specific equations, and demonstrating the soundness and completeness of the CLN model in representing SMT formulas. For any x, if F(x) = True, then F1(x) = True and F2(x) = True. The model M(x; B, ) is continuous for conjunction and disjunction cases. The model M(x; B, ) is continuous for conjunction and disjunction cases, with the proof showing the soundness and completeness of the CLN model in representing SMT formulas. The proof of the CLN model's soundness and completeness involves induction and the use of a continuous mapping function. This leads to the construction of a CLN model and the convergence of SMT formulas at the global minimum. Theorem 2 guarantees convergence of CLNs at the global minimum when constructed from formulas with linear equalities, as long as the t-norm satisfies Property 2. The proof involves defining properties and constructing the CLN model following specific procedures. When constructing a CLN model following specific procedures, the output will be a global minimum if the t-norm satisfies Property 2. The proof involves defining properties and showing that the loss function reaches its global minimum. The proof involves showing that the loss function reaches its global minimum for all x \u2208 R, using Property 2 of the t-norm. This is essential when constructing a CLN model following specific procedures. Theorem 3 states the conditions for a correct loop invariant in a program. The proof involves showing that the loss function reaches its global minimum for all x \u2208 R, using Property 2 of the t-norm. Theorem 3 states the conditions for a correct loop invariant in a program. In the example of training data generation, the variable k is sampled according to a precondition and the loop is executed repeatedly. The training data is generated by enumerating k values and executing a loop repeatedly to create a small set of samples. Templates are generated from pre-and post-conditions, as well as clauses from loop-conditions, to construct generic templates with equality and inequality constraints. The template generation process is summarized in Algorithm 1. The algorithm generates templates from pre-and post-conditions and loop conditions, extracting clauses and estimating degrees using log-log linear regression. It also checks for single inequality constraints and converts loop conditions into learnable SMT templates. Templates are used sequentially to infer possible invariants. The algorithm generates templates from pre-and post-conditions and loop conditions, extracting clauses and estimating degrees using log-log linear regression. It also checks for single inequality constraints and converts loop conditions into learnable SMT templates. Templates are used sequentially to infer possible invariants. When viewing S(t = u) as a function over t \u2212 u, it reaches its only local maximum at t \u2212 u = 0, satisfying the equality. An example is provided with a counterexample invalidating certain problems in the dataset. Measurements show that the system spends most time on solver calls and CLN training in the invariant inference pipeline in CLN2INV. CLN2INV completes CLN training quickly and spends most time on solver checks. It can learn constraints from execution traces in under 20 seconds for complex problems. CLN2INV can quickly learn constraints from execution traces in under 20 seconds for complex problems, such as higher order polynomial invariants. It struggles with problems containing 20 or more terms of 3rd degree or higher. The polynomial invariant problem involves extracting the invariant from a polynomial kernel. The model must learn to ignore most terms except for a few key ones. Learning polynomial invariants using CLN is challenging and an area for future study. The pseudocode provided shows the problem of finding the correct coefficient for a specific term in the polynomial."
}