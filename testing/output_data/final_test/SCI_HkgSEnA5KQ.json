{
    "title": "HkgSEnA5KQ",
    "content": "Behavioral skills for autonomous agents are typically learned through reward functions or demonstrations. However, these methods have limitations. Natural language instructions offer a promising alternative, allowing tasks to be specified for machines. Yet, a single instruction may not fully convey intent or be enough for an agent to understand how to perform the task. This work proposes an interactive approach to task specification, where iterative language corrections are provided. Our proposed language-guided policy learning algorithm integrates instructions and corrections to quickly acquire new skills for autonomous agents. This method enables a policy to follow instructions and corrections for navigation and manipulation tasks, outperforming non-interactive instruction following. Reward functions and demonstrations are traditional methods for specifying behavioral skills, but they have limitations in communicating goals. In real-world environments, defining the goal of a task for end-to-end deep learning can be challenging, especially when operating directly on raw sensory perception. Demonstrations can help bypass this challenge but require a human demonstrator. Language-guided autonomous agents have been researched to follow instructions, but a single instruction may not fully communicate the desired behavior. For example, guiding a robot by telling it which way to move can be more effective than verbally defining a coordinate in space. In this paper, the goal is to enable an autonomous agent to accept instructions and iteratively adjust its policy by incorporating interactive corrections. This in-the-loop supervision can guide the learner out of local optima and provide fine-grained task definition. Iterative language corrections are more informative than simpler forms of supervision and easier to provide than reward functions or demonstrations. Our method focuses on meta-learning policies that utilize iterative language corrections to improve agent performance. We propose an end-to-end algorithm that grounds these corrections in the environment, allowing the agent to adjust its behavior based on feedback. The model is trained on a variety of tasks, using its own past behavior and corrections to refine its actions. The model learns to correct mistakes by interpreting language input and can generalize to new tasks quickly through iterative language corrections. The main contributions include guided policies with language (GPL) via meta-learning and a practical GPL meta-learning algorithm. The approach is evaluated on two simulated tasks - multi-room object manipulation and robotic object relocation. The second domain involves controlling a robotic gripper to move objects to precise locations. Tasks for autonomous agents are specified through reward functions or demonstrations. Previous work has explored imitation learning and reward specification techniques, including combining the two to extract reward functions from user examples and demonstrations. Natural language is a promising modality for task specification as it allows humans to communicate task goals easily and quickly. Natural language commands are efficient for task specification, requiring no task performance ability or manual engineering. They convey more task information compared to other supervision modalities. Research focuses on grounding language commands in behaviors using supervised corpora or known reward functions. Most studies address instruction following tasks, aiming to specify complex tasks effectively. Incorporating language corrections in reinforcement learning tasks, focusing on how to associate language with behavior changes without intermediate supervision on object identities or word definitions. Our approach to learning from language corrections is based on meta-reinforcement learning, where a meta-training procedure is used to adapt to new tasks at meta-test time. Prior work has focused on model-free RL, model-based RL, supervised tasks, and goal specification, but no prior work has proposed meta-training of policies that can acquire new tasks from iterative language corrections. The sequential decision-making framework involves an agent observing states, choosing actions, and transitioning to new states via transition dynamics. The agent's goal is to learn a policy that achieves a desired goal specified by a language instruction. After attempting the task multiple times, the agent receives language corrections to improve its trajectory towards the goal. The corrections guide the agent on how to bring its trajectory closer to accomplishing the task. The agent receives language corrections to improve its trajectory towards the goal by incorporating iterative corrections after each attempt at the task. This process is illustrated in Figure 1 and requires the model to ground the contents of the correction in the environment and interpret it in the context of its own previous trajectory to decide on the next actions. The proposed deep neural network model, illustrated in FIG0, includes three modules: an instruction following module, a correction module, and a policy module. The instruction module interprets language instructions, while the correction module processes previous language corrections in the context of previous trajectories. The correction module converts language corrections into word embeddings and processes them through a 1D CNN. The correction and trajectory embeddings are concatenated and transformed by an MLP to form a tensor. The policy module integrates high-level descriptions, actionable changes, and environment state to generate actions. This model implements a learning algorithm by iteratively incorporating language corrections. Our model implements an interactive, user-guided reinforcement learning algorithm by incorporating language corrections to improve performance quickly. A meta-learning algorithm is described to train the model to adapt to iterative corrections effectively. The training procedure involves collecting data for each task using DAgger, training a GPL policy with supervised learning, and repeating the process until convergence. The GPL policy is executed to obtain a trajectory, corrected by an expert to generate data for the buffer. Corrections are used to generate new trajectories until a maximum number is reached, adding data at each step. The model must be meta-trained to understand instructions and corrections, associating them with previous trajectories, objects, and events. Meta-training is the process of training the model, while meta-testing is using it for new tasks with language corrections. During meta-training, the model learns tasks with distinct goals described by language instructions. Each task has a ground truth objective provided by a reward function, allowing for training a near-optimal policy. The tasks share the same state and action space, but have different language instructions. During meta-training, the model learns tasks with distinct goals described by language instructions and obtains near-optimal policies via reinforcement learning. Corrections for trajectories can be generated using a correction function, allowing for offline learning of task-specific policies transformed into a fast online learner during test time. The DAgger algorithm is used to train the model for each task by mimicking near-optimal experts. During meta-training, the GPL model is meta-trained using a policy that mimics a near-optimal expert. The model generates new trajectories for tasks, labels states with near-optimal actions, and samples corrections to improve performance. This iterative process populates the training set with labeled states, corrections, and prior trajectories observed by the model. During meta-training, the GPL model is trained with labeled states, corrections, and prior trajectories to mimic a near-optimal expert. The model is then trained to maximize the likelihood of the samples in the dataset D. Following the DAgger algorithm, the updated policy is used to collect data for tasks, which is appended to the dataset and used to train the policy iteratively until convergence. This process is illustrated in Algorithm 1 and FIG1 6 LEARNING NEW TASKS WITH THE GPL MODEL. The procedure involves adapting a learned policy with corrections provided by the user. The policy is rolled out in the environment, generating trajectories and corrections iteratively until convergence, similar to meta-reinforcement learning but using grounded natural language corrections. The procedure involves adapting a learned policy with corrections provided by the user in a loop, allowing for quick refinement of behaviors for tasks that are hard to describe. Using natural language feedback in the loop can reduce the supervision needed to learn new tasks, changing behaviors more quickly than scalar reward functions. The experiments analyze the benefits of iterative corrections in object manipulation and block pushing environments with the GPL model. The evaluation aims to assess the benefits of iterative corrections in improving the policy's task success. Comparative analysis is conducted to compare iterative corrections with standard instruction-following methods and an oracle model. Various experiments, including ablations and comparisons with other methods, are performed to evaluate the effectiveness of the approach. The code and supplementary material can be found at the provided link. The first experimental domain involves a discrete environment with a floor-plan of a building with six rooms. The task is to pick up an object from one room and bring it to a goal location in another room, with unique colored doors for each room. The environment is partially observed, allowing for natural language instructions. The environment in the experimental domain involves a building with six rooms and unique colored doors. The task is to move a specified object to a goal location using natural language instructions. The agent must explore or rely on external corrections to find the goal object and square, which are placed randomly in different rooms. Generalization is required as tasks involve different objects and locations. The agent in the robotic object relocation environment must complete a list of subgoals to move objects to specific locations. The task involves remembering and incorporating corrections while solving multiple subgoals. Training and testing tasks are generated to ensure generalization, with 3240 possible lists of subgoals. The robotic object relocation task involves pushing blocks to target locations in an environment with obstacles. Instructions are ambiguous, requiring corrections to guide the agent accurately. Training tasks include multiple subgoals and corrections for generalization. The environment for training involves generating 1000 environments with three types of corrections provided stochastically. The corrections include directional, relational, and indicating the correct block to push. This environment was chosen for its continuous state space and spatial relations between objects, allowing for natural language goal specification. The policy must learn to contextualize corrections in relation to other objects and previous behavior. The correction is contextualized in relation to other objects and previous behavior. The method is compared to alternative instruction following methods and methods using rewards for finetuning on new tasks. Different architectures are tested, including MIVOA, with instruction and full information. Methods are trained with DAgger, with instruction methods receiving ambiguous instructions and full information methods receiving exact information needed by the agent. The full information method for robotic object relocation task receives subgoals without interactive input. Performance is measured by completion rate, calculated as 1 - final block dist initial block dist. Results show model outperforms instruction following baseline and approaches full information method. Comparison of sample complexity with GPL against other baselines is also conducted. In robotic object relocation tasks, a comparison is made between different baselines including a reinforcement learning baseline, pretraining with DAgger, and a method called GPR. The performance of these methods is evaluated using a reward function and language corrections, with results shown in FIG5. In the object relocation domain, test tasks involve new object configurations and goals. The completion rate of the method increases with more corrections, outperforming the baseline with 4 or more corrections in the multi-room domain. Details on meta-training complexity are in appendix A.3. Our method outperforms the baseline with fewer corrections, showing better performance with less information. The interactive nature of our model allows it to receive only necessary information to solve the task efficiently. In the robotic domain, our model surpasses the instruction baseline with just 1 correction and approaches the full information method with more corrections. The comparison against baselines using task reward shows our method achieves high completion rates with minimal test trajectories. Our method achieves high completion rates with minimal test trajectories compared to baselines. The RL baseline requires over 1000 trajectories for similar performance, while the pretraining baseline also needs more than 1000 test trajectories. The reward guided version of our method, GPR, performs poorly in the multi-room domain but shows reasonable performance in the robotic domain. Language corrections in the multi-room domain may provide more information than scalar rewards. Our method achieves high completion rates with minimal test trajectories compared to baselines. In the multi-room domain, GPL quickly incorporates corrections to improve agent behavior with fewer corrections than full information. Ablation experiments show the importance of previous corrections in the model, while removing instructions has less impact. Various analyses are performed to understand GPL better, including model ablations and generalization to out-of-distribution tasks. In ablation experiments, removing previous corrections has the most significant negative impact on performance, while removing instructions has the least impact. Providing more corrections at meta-testing time than during training shows an increase in completion rate. In experiments, providing more varied corrections at meta-testing time leads to small gains in completion rate. Adding directional and binary correction types in the multi-room environment shows that binary corrections have little impact on completion rate, while directional corrections slightly increase completion rate. In experiments, varied corrections at meta-testing time lead to small gains in completion rate. The method can generalize to unseen objects in the multi-room domain, achieving a high completion rate and outperforming baselines. The GPL framework involves interactive learning with language corrections during successive trials in the environment. The model is trained using meta-learning on a dataset of tasks to ground language corrections in behaviors and objects. Future work may incorporate real language at training time and address new concepts not seen during training. Approaches to handling new concepts could involve innovations at the model or interface level. Visualizing failure cases can provide insights for improvement. In the interactive learning framework, failure cases are visualized to understand algorithm behavior. One failure case shows the agent forgetting to pick up the green ball despite receiving corrections. Varying corrections during training could improve performance. Success cases demonstrate iterative learning progress. Failure cases in robotic object relocation show near success in solving tasks. The agent nearly solves the task but fails due to the trajectory being blocked by an obstacle. Success is shown in another case where iterative corrections lead to progress in solving the task. The training details include using Adam optimization with a learning rate of 0.001 and a data buffer size of 1e6. The agent is trained on the whole data buffer for five epochs before adding more data. ReLU activations are used unless specified otherwise. Expert policies are trained using Proximal Policy Optimization with a dense reward function. In the multi-room domain, the agent tracks the next subgoal and receives rewards based on the Euclidean distance to the subgoal. In the robotic object manipulation domain, rewards are based on distances. Meta-training is done on 1700 environments, with convergence in 6 DAgger steps. For the robotic object relocation domain, training is on 750 environments, with convergence in 9 DAgger steps. Observations are a 7x7x4 grid encoding object types and colors. The observation in the robotic object manipulation domain includes object types and colors for objects the agent is holding. The action space consists of 6 moves. Trajectories are processed by CNN and MLP models. Language corrections are converted into word embeddings and processed by CNN and MLP models. The trajectory and correction embeddings are concatenated and fed through a MLP, then averaged across correction iterations. The instruction and observation data are processed by CNN and MLP models to output action distributions in a robotic object manipulation domain. The observation includes gripper and block coordinates, while the action space consists of applying force in cardinal directions. Trajectories are subsampled and processed by CNN and MLP models for language corrections. The language correction and instruction data are processed using CNN and MLP models to output action distributions in a robotic object manipulation domain. Trajectory embeddings, correction embeddings, and instruction embeddings are concatenated and fed through MLP models to generate an action distribution."
}