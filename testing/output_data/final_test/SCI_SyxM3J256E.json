{
    "title": "SyxM3J256E",
    "content": "Adversarial training can improve robust accuracy but may harm standard accuracy. Previous research has explored the tradeoff between standard and robust accuracy, typically in scenarios where no predictor excels in both areas with infinite data. This study demonstrates that even with an optimal predictor in infinite data, a tradeoff can still exist with finite data. The use of robust self-training can help mitigate this tradeoff by utilizing unlabeled data. Adversarial training can improve robust accuracy but may harm standard accuracy. Research shows a tradeoff between the two, with no predictor excelling in both areas even with infinite data. Perturbations can affect the output of the Bayes estimator, leading to a discrepancy in accuracy levels. Adversarial training can improve robust accuracy but may harm standard accuracy. Research shows a tradeoff between the two, with no predictor excelling in both areas even with infinite data. Perturbations can affect the output of the Bayes estimator, leading to a discrepancy in accuracy levels. The hypothesis class is expressive enough in practice, suggesting the tradeoff stems from the worse generalization of adversarial training due to statistical properties or optimization dynamics on neural networks. The text discusses the tradeoff between standard and robust accuracy in adversarial training. It presents a learning problem with a convex loss where adversarial training hinders generalization, even with optimal predictor accuracy. The study reveals a statistical explanation for why adversarial training requires more samples for high standard accuracy. Additionally, it introduces robust self-training as a method to eliminate this tradeoff. In an analysis of standard vs. adversarial training models on CIFAR-10 data, it was found that the accuracy gap decreases with larger sample sizes. Adversarial training may require more samples for optimal performance on the standard learning objective. Adversarial training may require more samples to achieve high accuracy compared to standard training. The learning problem involves fitting the majority of the distribution easily with simple predictors, while perturbations of these points require complex predictors. This causes standard estimators to outperform adversarially trained robust counterparts with few samples. Adversarial training focuses on fitting perturbations of training points, leading to complex estimators that generalize poorly. The text discusses the generation of data with marginal distributions, measuring predictor robustness, and the goal of learning a predictor that is robust to perturbations. It compares standard training to robust training, which seeks to enforce invariance to perturbations by penalizing the worst-case loss over the invariance set. The text discusses the construction of standard and robust estimators for sample size n, comparing their convergence and error rates. It considers linear predictors as \"simple\" and staircase predictors as \"complex\" in fitting the input distribution. The goal is to create a predictor that is robust to perturbations by penalizing the worst-case loss over the invariance set. The text discusses constructing standard and robust estimators for sample size n, comparing convergence and error rates. It considers linear predictors as \"simple\" and staircase predictors as \"complex\" in fitting the input distribution. The goal is to create a predictor robust to perturbations by penalizing the worst-case loss over the invariance set.\u03b4 is small, perturbations are defined for fitting majority of the distribution with complex predictors. Anchor points in X line and local perturbations in X c line are treated, resembling \u221e perturbations in computer vision. Invariance sets B(x) are defined for non-anchor points, ensuring f resembles a staircase with slope m = 1. This ensures the optimal predictor for standard error is robust. Our construction allows for a non-trivial robust and accurate estimator by adding Gaussian noise to the optimal predictor. The illustration shows a convex problem with a simple linear predictor with low test error but not robust to perturbations, and a complex optimal predictor that is both robust and accurate. With a small sample size, any robust predictor fitting the sets B(x) becomes a staircase that generalizes poorly. With a large sample size, the training set contains all points from X line. With a large sample size (n = 25000), the training set contains all points from X line. The robust predictor is close to f by enforcing the right invariances, while the standard predictor has low error but higher than the robust predictor. An illustration of our convex problem with m = 0 shows the optimal predictor f as a simple linear function. Empirical validation shows the staircase problem is sensitive to robust training, with test errors compared for standard and robust estimators. Trends in generalization gap are nearly identical. See Appendix D for more details. FIG3 displays the difference in test errors of the two estimators. With varying sample sizes, a comparison is made between standard and robust estimators through a grid search over regularization parameters. Robust estimators handle low probability perturbations better than standard ones. Data augmentation is a less demanding approach compared to adversarial training. Even with data augmentation, there is a tradeoff with small training sets due to increased complexity. The gap between standard and robust estimator errors decreases as training progresses. The gap between standard and robust estimator errors decreases as training sample size increases. Sampling more training points can help eliminate the tradeoff. Robust self-training (RST) leverages unlabeled data for robustness by using a standard estimator to generate pseudo-labels. Robust self-training (RST) leverages unlabeled data for robustness by using a standard estimator to generate pseudo-labels. For the staircase problem with slope m = 1, RST mostly eliminates the tradeoff and achieves similar test error to standard training. The difference between test errors (robust - standard) decreases as the number of training samples n increases. Positive numbers indicate that the robust estimator has higher Mean Squared Error (MSE) than the standard estimator for small n. When subsampling CIFAR-10, the gap between test errors of standard and robust models decreases as sample size increases. Robust self-training with 1000 additional unlabeled points achieves comparable test MSE to standard training, eliminating the tradeoff seen in robust training. The shaded regions represent 1 STD. The gap in errors between standard and adversarially trained models decreases as sample size increases in CIFAR-10. More training data should eliminate the tradeoff. Robust self-training with additional unlabeled data improves robust and standard accuracy. Adversarial training offers a regularization benefit by enforcing invariance on predictors. Invariance on predictors improves robustness, but may degrade performance by requiring complex predictors and more samples to generalize well. The tradeoff between robustness and accuracy can be mitigated with additional unlabeled data. Invariance condition (restated as FORMULA7) is sufficient for minimizers of standard and robust objectives under P to be the same in the infinite data limit. If f is in hypothesis class F, it minimizes the standard objective for square loss. Consistency of estimators f std n and f rob n is shown when both converge to the same Bayes optimal f as n \u2192 \u221e. Invariance condition implies consistency of f rob n and f std n, where the robust loss upper bounds the standard loss for any other predictor. The robust loss upper bounds the standard loss for any predictor, including the Bayes optimal f. This implies that f achieves optimal robust loss, and f std n and f rob n converge to f with infinite data. The minimizer of the standard population squared loss is optimal for the robust population squared loss as well. The Bayes estimator f* is used in the classification case to ensure consistency, where label invariance is required. The optimal standard classifier is the Bayes optimal classifier f_c. In the staircase problem, the target distribution invariance is maintained within an invariance set B(x). The target distribution invariance is maintained within an invariance set B(x) for all x \u2208 X. In a 1-dimensional regression case, the data distribution is defined with a generative process involving perturbation points and noise in the targets. The distribution over X line is controlled by a parameter m, and the size of perturbations is bounded by 1/2. The data distribution is defined by sampling a point from X line based on categorical distribution w, then perturbing it with probability \u03b4 to obtain sample x. The majority of probability mass is concentrated on the first s 0 stairs of w, with s 0 fixed at 5 to highlight differences between robust and standard estimators for small sample sizes. The distribution is defined by sampling a point from X line based on categorical distribution w, then perturbing it with probability \u03b4 to obtain sample x. The majority of probability mass is concentrated on the first s 0 stairs of w, with s 0 fixed at 5 to highlight differences between robust and standard estimators for small sample sizes. Increase s to create versions with more stairs, where x rounds to the nearest integer. Invariance sets are defined as B(x) = { x \u2212 , x , x + }, with all points in B(x) having the same mean target value. The hypothesis class is the family of cubic B-splines, ensuring expressiveness to include any function in F. The family of cubic B-splines is expressive enough to include any function in F. Cubic B-splines can be seen as a kernel method with a kernel feature map \u03a6 : X \u2192 R 3s+2. Optimization is done with a penalized smoothing spline loss function over parameters \u03b8, where the regularization penalty matrix \u2126 computes second-order finite differences of the parameters \u03b8. The standard spline objective solves a linear system for n samples of training inputs X and targets y drawn from P. The standard spline objective solves a linear system for training inputs and targets drawn from P. The robust objective solves a pointwise maximum of squared losses over invariance sets using CVXPY. Robustness hurts generalization when the slope is large and the probability of drawing samples from perturbation points is small. This example is insensitive to label noise, showing that robustness hurts generalization. If the slope is close to zero, robustness helps generalization. The complexity of the true function is low, and robustness helps generalization by cancelling out label noise. Robust training improves generalization in cases with noise, while in noise-free scenarios, robustness does not impact generalization. Test MSE and generalization gap are similar for robust and standard training, with both converging to the true function with one sample. In the m = 1 case, robust training leads to higher norm estimators for all sample sizes compared to standard training. With enough samples, standard training increases the norm of its solution as it converges to the true function, while robust training's mean squared error (MSE) decreases. In the m = 0 case, robustness helps with consistently low norm estimators. Standard training with small sample sizes may have low norm but high test MSE due to incorrect slope. Both standard and robust estimators converge to the optimal solution in the infinite data limit. The robust self-training procedure involves robust training on a dataset augmented with unlabeled data. Robust self-training involves training on a dataset augmented with unlabeled data using a standard estimator. The targets for the unlabeled data have low error due to the good generalization of the standard estimator. This method aims to improve both standard and robust test error by mimicking the standard estimator on more data distribution while optimizing the robust objective. Robust self-training involves using a standard estimator to compute pseudo-targets for unlabeled data, creating an augmented dataset for training a robust estimator. This method aims to improve both standard and robust test error by leveraging additional unlabeled samples from the data distribution. Robust self-training aims to improve standard and robust test error by using a standard estimator to generate pseudo-targets for unlabeled data, enhancing the training of a robust estimator. In experiments, robust training outperforms standard training in terms of standard test error, with robust estimators showing slightly better accuracy for very small sample sizes. The MNIST dataset consists of 60000 labeled examples of digits. Sub-sampling the dataset by various factors, results are reported for a small 3-layer CNN trained for 200 epochs with 100% standard training accuracy. Adversarial models achieve > 99% accuracy under the \u221e attack model with PGD adversarial training. The final robust test accuracy when training with the full set was 91%. The trade-off for adversarial training in MNIST is due to the model not converging. Xavier initialization leads to faster convergence with no drop in clean accuracy. Adversarial training is significantly affected by initialization. The robust estimator has lower test MSE and consistent norm with more samples. The robust estimator maintains consistent norm and lower test MSE compared to the standard estimator, which has low norm for small samples but high test MSE due to finding a solution with the wrong slope."
}