{
    "title": "HJgKYlSKvr",
    "content": "The paper introduces a novel method for learning a generative model of 3D shapes from natural images in an unsupervised manner. It utilizes a two-stage approach where a generator network outputs 3D objects and a differentiable renderer produces images of these objects from random viewpoints. This training strategy forces the generator network to learn realistic 3D objects that yield realistic renderings from various viewpoints. The proposed training forces the generator network to learn an interpretable 3D representation disentangled from the viewpoint, consisting of a triangle mesh and texture map. The method can learn realistic 3D shapes of faces using only natural images. Generative Adversarial Nets (GAN) have become the gold standard generative model for creating sample images indistinguishable from real ones. Our work combines the advantages of GANs for 3D shape learning, such as unsupervised training and theoretical guarantees, with the interpretability of classical generative models like Gaussian Mixture Models and Naive Bayes classifiers. Our work combines the advantages of GANs for 3D shape learning, such as unsupervised training and theoretical guarantees, with interpretable representation through a generator network that outputs a 3D mesh. The learning process assumes natural images are created by a differentiable renderer, which generates fake images using the 3D mesh, texture, background image, and viewpoint. The discriminator network is trained adversarially to distinguish between fake and real images, enforcing image realism from multiple viewpoints. This approach builds upon previous work by Gadelha et al. (2017) but overcomes limitations by being applicable to real datasets without the need for feature engineering. Our method for 3D shape learning works on natural images without the need for silhouettes as supervision signal. We address ambiguities in training by using suitable priors, building upon the concept of image realism to achieve data realism. The paper introduces a procedure to build a generative model for learning explicit 3D representations from natural images in an unsupervised manner. They utilize a generator network and a differentiable renderer trained in a GAN setting. The novel renderer is differentiable with respect to 3D vertex coordinates, allowing for high-quality results even at object boundaries and self-occlusions. The authors also address training ambiguities and emphasize the importance of using labels or prior knowledge to overcome these challenges. In this paper, the authors discuss the challenges in solving problems without using labels or prior knowledge on data distribution. They provide practical solutions to overcome ambiguities and clarify the definitions of supervised, unsupervised, and weakly supervised learning. The focus is on supervision based on the objective function and optimization, rather than the origin of annotation. The target objective measures the performance of the trained model, defined by the loss function and data. Training objectives vary for supervised, weakly supervised, and unsupervised cases. In most supervised tasks, the target is the same as the training objective. Monocular depth estimation is an example of weakly supervised learning. GAN training is unsupervised with the same target and training objectives. The Basel face model by Paysan et al. (2009) is a successful 3D generative model that represents faces as a linear combination of base shapes. It is used in various methods for learning 3D representations with supervision signals like 3D data. Methods using differentiable rendering and randomly sampled viewpoints for image realism include GAN and Variational autoencoder based approaches. These methods are limited to synthetic data or weak supervision, while our method aims to disentangle 3D and viewpoint factors in images unsupervised. Reed et al. (2015) used image triplets for a similar task. Reed et al. (2015) and Mathieu et al. (2016) utilized unsupervised methods with image triplets and pairs to disentangle factors in GAN training. StyleGAN (Karras et al., 2019) and HoloGAN (Nguyen-Phuoc et al., 2019) also focused on unsupervised disentangling using latent variables. In contrast, our approach learns an explicit 3D mesh representation for disentangling 3D shape from viewpoint without labels, ensuring interpretability and consistency. The renderer is a key component in our model for achieving this disentanglement. The model introduces a novel renderer for shape learning tasks, ensuring exact gradient computation. It aims to map a random vector to a 3D object and background image, forming the scene representation. The combination of scene and camera representations is used by a differential renderer to generate an image. The generator is trained adversarially against a discriminator. The generator G is trained adversarially against discriminator D using zero-mean Gaussian samples as input. The objective is to map Gaussian samples to scene representations for realistic renderings. The discriminator distinguishes between fake and real images, solving an optimization problem with fixed renderer R and randomly sampled viewpoints. G and D are neural networks optimized using stochastic gradient descent. Theoretical analysis of the method and assumptions for successful generator training are discussed. The images in the dataset are formed by a rendering function given 3D representation and viewpoint. Assumptions about probability density functions of scenes and viewpoints are crucial for generator optimization. The ground truth viewpoint and 3D scenes are assumed to be independent random variables. The distribution of 3D representations is unknown and will be learned, while the viewpoint distribution is known. The viewpoint for specific data samples is unknown unless there is a capture bias. This assumption allows for random sampling of viewpoints independently from generated models. The 3D shape learning task is ambiguous due to different representations inducing the same data x \u223c p x. Ambiguities depend on the parametrization of the 3D representation, with acceptable ambiguities arising from mesh representations not causing ambiguity in depth reconstruction. Modeling the whole 3D scene is necessary for natural images to avoid trivial ambiguities. When using a static background, the generator can move the object out of view and generate images on the background during GAN training. Modeling the entire scene with a triangle mesh is impractical due to the large size of meshes. A compromise is proposed where only the object is modeled with a mesh, and a large background is generated and randomly cropped during training. This approach ensures that the generator cannot match real data statistics unless it produces a realistic 3D object. The generator and discriminator have sufficient capacity, reaching the global optimum of GAN training. The generator can learn the 3D geometry of the scene faithfully under certain conditions. The generated scene representation distribution is identical to the real one when the assumptions are satisfied. Traditional polygon renderers are not differentiable with respect to 3D vertex coordinates, causing issues during training. The proposed novel renderer is differentiable with respect to 3D vertex coordinates by extending triangles at their boundaries in pixel space and blending them against the background or occluded triangles. Rendering is done in two stages: first with a traditional renderer to create a crisp image, then with triangle extensions and an alpha map for a soft image, which are blended together. The renderer uses barycentric coordinates to define the distance of a 2D pixel coordinate to a triangle and its closest point within the triangle. The renderer computes depth, attribute, and alpha maps for each pixel and triangle using Euclidean distance. It determines the closest triangle to render based on depth values and barycentric coordinates. UV mapping is supported, with colors sampled from a texture map and blended for the final image. The 3D representation consists of shape, texture, and background color values. The shape image corresponds to vertices in a 3D mesh, while the texture image uses UV mapping for higher resolution.Triangles are defined on a grid, with the texture matching the image resolution. The renderer uses a perspective camera model with a background color image. A random N \u00d7 N section is cropped and placed behind the object. The 3D representation is designed for convolutional neural network generators like StyleGAN. The StyleGAN generator is used with almost vanilla settings to generate shape, texture, and background images. StyleGAN consists of two networks: a mapping network that produces a style vector from latent inputs, and a synthesis network that generates the output image by perturbing layer activations with the style vector. Style mixing allows for combining styles from different latent vectors. Training is done progressively, starting at N = 16 resolution and ending at N = 128. Modifications were made to output channels, resolution, and learning rates. The final resolution for StyleGAN was set to N = 128, with one instance generating shape and texture (G o ) and another generating the background (G b ). Inputs to both generators are 512 dimensional latent vectors z o and z b , sampled independently. The object generator output is sliced into shape and texture images, with the shape image downsampled by a factor of 2. Shape image is multiplied by 0.002 and added s 0 as initial shape. For faces, s 0 is a sphere with radius r = 0.5, for buses and cars, s 0 is a flat sheet. Training showed difficulty in recovering from bad local minima, resulting in artifacts and hollow-mask. To address artifacts and hollow-mask ambiguity, a shape image pyramid is used in the generator to produce blurred shape images. The 3D models tended to grow large, so a background model was added with a size constraint on the object. The output coordinates are computed with a maximum size constraint and pixel-wise interpretation on the shape image. The effects of the shape image pyramid and size constraint can be observed in Figure 4. The discriminator architecture was taken from StyleGAN and trained with default settings. Samples from the methods are shown in Figure 4, with CRISP using the crisp renderer and SOFT using the proposed renderer. Different models like SIZE, FULL, CAR, and BUS were trained on FFHQ faces and LSUN cars and buses datasets. FFHQ contains high-resolution color images resized to 128 \u00d7 128 for training. Random rotations were applied for viewpoint variation during training. For the full model, pyramid shapes of 4 levels with a size constraint of 1.3 were used, trained for 5M iterations. Results are shown in Figure 1, with more samples in the Appendix. The model was also trained on 100k images from LSUN categories, showing ablations of renderer and network architecture settings in Figure 4. The soft renderer had a significant impact on training, while the size constraint prevented background modeling and the shape pyramid reduced artifacts on the face. The shape pyramid reduces artifacts on the face and the viewpoint distribution is crucial. The method struggles with a large viewpoint range, producing self-intersecting meshes. Results on different categories like cars and buses are shown in Table 2 and Figure 5, demonstrating disentangled identity and smooth transitions. Limitations include using a fixed topology triangle mesh and not modeling the background, restricting the method for complex objects and scenes. The current method is limited by not modeling the object at the center of the image in the triangle mesh. The imaging model is currently Lambertian, but can be extended to include specularity and directional light. The method does not use explicit supervision signals other than the images themselves. The current method focuses on unsupervised 3D shape learning using the FFHQ dataset, demonstrating the feasibility of this approach even when annotation is available. The goal is to address the challenge of building a generative model of 3D shapes in an unsupervised manner, providing solutions for ambiguities in the task. Our analysis explores ambiguities in unsupervised 3D shape learning using the FFHQ dataset. Samples from our generator trained on the dataset show anatomically correct shapes with some exaggerated features. However, there are failure cases where the shapes do not resemble faces from certain viewpoints."
}