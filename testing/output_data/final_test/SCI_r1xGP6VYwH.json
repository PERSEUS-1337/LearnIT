{
    "title": "r1xGP6VYwH",
    "content": "Optimistic initialisation is an effective strategy for efficient exploration in reinforcement learning. Model-free deep RL algorithms do not use optimistic initialisation, leading to pessimistic initialisation of Q-values. A proposed count-based augmentation separates the source of optimism from the neural network, proving efficiency in both tabular and deep RL settings. Our algorithm, Optimistic Pessimistically Initialised Q-Learning (OPIQ), enhances DQN-based agent Q-value estimates with count-derived bonuses for optimism during action selection and bootstrapping. OPIQ surpasses non-optimistic DQN variants using pseudocount-based intrinsic motivation in challenging exploration tasks, predicting optimistic estimates for new state-action pairs in reinforcement learning. In deep RL, efficient exploration is vital for gathering data to infer a good control policy, especially as environment complexity increases. In deep RL, model-free approaches have shown superior performance on complex tasks requiring hard exploration. This paper focuses on developing model-free RL algorithms for efficient exploration in large state spaces with strong theoretical foundations. Current deep RL exploration methods lack optimistic initialization, crucial for efficient exploration in tabular algorithms. In deep RL, model-free approaches have shown superior performance on complex tasks requiring hard exploration. This paper focuses on developing model-free RL algorithms for efficient exploration in large state spaces with strong theoretical foundations. Current deep RL exploration methods lack optimistic initialization, crucial for efficient exploration in tabular algorithms. Instead of using neural networks with optimistic initialization, common schemes yield initial Q-values around zero, providing a pessimistic start. To promote exploration, Q-values for novel state-action pairs must remain high until they are explored. Intrinsic motivation, particularly through pseudocounts, has been successful in deep RL exploration, especially in scenarios with sparse rewards. The text discusses the challenge of efficient exploration in deep reinforcement learning algorithms. It highlights the importance of optimistic initialization for Q-values in unvisited state-action pairs to promote exploration. The use of intrinsic motivation, such as pseudocounts, has been successful in addressing this issue in scenarios with sparse rewards. OPIQ introduces Optimistic Pessimistically Initialised Q-Learning to address the issue of inefficient exploration in deep reinforcement learning. By augmenting Q-value estimates with count-based bonuses based on the number of times a state-action pair has been visited, OPIQ maintains optimism in action selection and bootstrapping. This approach ensures efficient exploration without relying on optimistic initialization. OPIQ introduces Optimistic Pessimistically Initialised Q-Learning to address inefficient exploration in deep reinforcement learning. It uses count-based bonuses for state-action pairs to maintain optimism in action selection and bootstrapping, without relying on optimistic initialization. The algorithm retains theoretical guarantees of UCB-H and can be extended to Deep RL settings. The OPIQ algorithm introduces Optimistic Pessimistically Initialised Q-Learning to improve exploration in deep reinforcement learning. It demonstrates performance enhancements in sparse reward tasks compared to baseline DQN and other exploration schemes. Optimism in action selection is crucial for efficient exploration, as visualized by optimistic Q-values for new state-action pairs in a Markov Decision Process. The agent aims to maximize the expected discounted sum of rewards in the discounted episodic setting using a policy \u03c0(\u00b7|s) and Deep Q-Network (DQN) with a nonlinear function approximator. DQN parameters are trained by gradient descent on mean squared regression loss with bootstrapped 'target' y t. Exploration based on intrinsic rewards augments observed rewards with a bonus based on pseudo-visitation-counts. The effect of optimistic initialization in neural networks is demonstrated through the use of different networks with biased initializations and training on data points. A method called Optimistic Pessimistically Initialised Q-Learning (OPIQ) ensures optimism in Q-value estimates for unvisited state-action pairs. This method uses a target network and a replay buffer to improve stability. OPIQ drives exploration by ensuring optimism in Q-value estimates for unvisited state-action pairs. Optimistic initialisation may not work with neural networks due to values for novel pairs not remaining high after training. Due to the generalization of neural networks, it is difficult to use optimistic initialization to drive exploration. Instead, Q-value estimates are augmented with an optimistic bonus to ensure optimism in unvisited state-action pairs. Our approach approximates the discrete indicator function for visitation count, with a smaller hyperparameter M governing optimism decay. Actions are chosen based on Q-value estimates, incrementing visitation counts, and assuming pessimistic Q-value estimates for worst-case analysis. Zero-initializing Q-value estimates leads to a pessimistic initialization, especially in neural networks with unpredictable generalization. In the context of approximating visitation count with a hyperparameter M for optimism decay, a scaling parameter C can be chosen to ensure overestimation of unseen Q-values. This parameter can vary based on the environment for faster convergence, impacting action selection and bootstrapping. In the finite horizon setting, optimistic Q-values are split into pessimistic and optimistic components based on state-action pair counts. This separation incentivizes exploration and encourages the agent to try novel actions during action selection and bootstrapping. Optimistic Q-values incentivize exploration by encouraging the agent to try novel actions. OPIQ ensures efficiency in the tabular domain by being optimistic during bootstrapping. Our algorithm for action selection and bootstrapping uses pessimistic initialisation with Q + -values. Theorem 1 guarantees that our estimates are greater than or equal to Q * with controlled overestimation. As M approaches infinity, our method matches the asymptotic performance of UCB-H, promoting more exploration with smaller values of M. OPIQ uses pessimistic initialization with Q+ values for action selection and bootstrapping. The algorithm ensures controlled overestimation and promotes exploration. It is based on DQN for deep RL, using a deep neural network with parameters \u03b8 as a function approximator Q\u03b8. The greedy action is determined using Q+ values, with a hyperparameter C controlling the optimistic bias. An \u03b5-greedy policy is used in practice, with experiences sampled from an experience replay buffer after each timestep. After each timestep, experiences are sampled from an experience replay buffer for n-step Q-learning. The network is trained using gradient descent on a loss equation with a hyperparameter C bootstrap for optimistic bias. Mixed Monte Carlo (MMC) target is used for experiments on Montezuma's Revenge. Pseudocounts are obtained using static hashing on the first 2 of 3 environments tested. In Montezuma's Revenge experiments, counts are done on a downsampled image of the game frame. OPIQ is a deep version of UCB-H designed to ensure optimism in Q-values during action selection and bootstrapping, addressing limitations of neural networks. It augments Q-value estimates with optimistic bonuses to drive exploration effectively. Optimism during action selection and bootstrapping is crucial for efficient exploration in the tabular domain. Various model-based algorithms like R-MAX, MBIE, UCRL2, and UCBVI rely on the principle of optimism in the face of uncertainty. Osband and Van Roy argue that posterior sampling is more practically efficient than optimistic approaches, matching their performance in Bayesian expectation. Agrawal and Jia show that an optimistic variant of posterior sampling is provably efficient under a frequentist regret bound. The curr_chunk discusses the efficiency of different model-free algorithms in deep RL, focusing on optimistic and pessimistic initialization of Q-values. The OPIQ algorithm pessimistically initializes Q-values, while UCB-H and UCB-B optimistically initialize Q-values and utilize intrinsic motivation for exploration. The idea of augmenting pessimistically initialized Q-values can be applied to any model-free algorithm. The curr_chunk discusses the use of pseudocounts in computing intrinsic rewards in reinforcement learning algorithms. Bellemare et al. (2016) propose a method based on the number of times a state has been visited to calculate intrinsic rewards. Ostrovski et al. (2017) show that RLSVI achieves efficient Bayesian expected regret, while OPIQ achieves efficient worst-case regret. Bootstrapped DQN with a prior is a model-free algorithm with theoretical support but performs worse on sparse reward tasks compared to DQN with pseudocounts. Machado et al. (2015) adjust rewards to be optimistic with zero initialization. When applied to neural networks, the use of pseudocount intrinsic motivation performs better on hard exploration tasks. Generalized counts are used to provide bonuses during action selection, and E-values obtained through SARSA with a constant 0 reward are utilized in deep RL settings. OPIQ generates optimism for exploration in sparse reward environments, compared against baselines on three different environments: Randomised Chain, a two-dimensional maze, and Montezuma's Revenge. OPIQ is compared against various DQN-based exploration algorithms, including DORA agent and two strong exploration baselines: \u03b5-greedy DQN and DQN + PC. Different intrinsic reward mechanisms are evaluated, such as adding intrinsic rewards based on pseudocounts or subtracting a constant from environmental rewards during training. The DQN Bias is initialized positively for optimistic initialization. DQN + DORA uses generalised counts as intrinsic rewards. DQN + DORA OA includes an optimistic bonus during action selection. DQN + RND adds the RND bonus as an intrinsic reward. BSP utilizes Bootstrapped DQN with randomised prior functions. Ablations include Optimistic Action Selection (OPIQ w/o OB) and Optimistic Action Selection and Bootstrapping (OPIQ w/o PC). In the context of different methods using intrinsic rewards, OPIQ without Pseudo Counts significantly outperforms baselines in a simple domain. DQN with pseudocount derived rewards struggles to find the goal state, but adjusting the final layer's bias improves performance. Subtracting a constant from rewards leads agents to favor an inferior reward, hindering goal achievement. In a comparison of different methods using intrinsic rewards, OPIQ without Pseudo Counts outperforms baselines in a simple domain. OPIQ is more stable and successful in tasks compared to its ablations. In a more complex Maze task, only OPIQ is able to find the goal in the sparse reward maze, showing the importance of optimism during action selection and bootstrapping in sparse reward tasks. The final layer bias and reward subtraction have little effect on performance. DQN + RND performs poorly due to lack of visual input variation. DQN+DORA and DQN+DORA OA also struggle as intrinsic motivation diminishes quickly. Neural networks lose their initializations post-training, causing DORA's rewards to drop rapidly. BSP is the only exploration baseline that does not suffer from these issues. OPIQ and its ablations successfully find the goal in the maze, exploring faster with optimism during action selection and bootstrapping. The ablation without optimistic bootstrapping shows unstable episodic rewards. OPIQ without pseudocounts performs worse than other ablations, suggesting that adding PC-derived intrinsic bonuses to the reward aids learning. Adding PC-derived intrinsic bonuses to the reward aids learning, but it is not enough for sufficient exploration. Optimism during action selection is crucial for efficient exploration, as shown in the performance gap between DQN + PC and OPIQ w/o OB. This is especially important in sparse reward games like Montezuma's Revenge. OPIQ significantly outperforms baselines in episodic reward and maximum episodic reward achieved during training, demonstrating the importance of optimism in action selection. OPIQ explores 12 rooms during 12.5mil timesteps, showing improved exploration in complex environments. OPIQ is a model-free algorithm that augments Q-values estimates with a count-based optimism bonus, proving to be efficient in high-dimensional environments. It scales well to deep RL and maintains optimism during action selection and bootstrapping for exploration in hard sparse reward environments like Montezuma's Revenge. In future work, OPIQ aims to integrate with more expressive counting schemes for the tabular setting of a finite-horizon Markov Decision Process (MDP). The MDP is defined by a tuple (S, A, {P t }, {R t }, H, \u03c1), where S is the state space, A is the action space, P t (\u00b7|s, a) is the state-transition distribution, R t (\u00b7|s, a) is the reward distribution, H is the horizon, and \u03c1 is the starting state distribution. The goal is to find policies that maximize the expected sum of future rewards. The Q-value of a policy \u03c0 at time t is defined as the expected sum of future rewards. The agent interacts with the environment for K episodes, yielding a total regret. The worst case total regret is bounded with a probability of 1\u2212p by an online Q-learning algorithm. All Q-values for timesteps t \u2264 H are optimistically initialized. The learning rate is defined based on the number of times a state-action pair has been observed. The update rule for transitioning between states in deep RL involves count-based intrinsic motivation terms. Approximate counting schemes are used to handle continuous or high-dimensional state spaces, such as static hashing for chain and maze environments. The update rule for transitioning between states in deep RL involves count-based intrinsic motivation terms. Approximate counting schemes are used to handle continuous or high-dimensional state spaces. The granularity of counting is controlled by a hyperparameter k, with higher values leading to more distinguishable states. A counting bloom filter is used to efficiently update and retrieve counts for state vectors. Separate data structures are maintained for each action to store counts for state-action pairs. Experimentation on Montezuma's Revenge involves downsampling the greyscale state representation and maintaining tabular counts over the new representation. The agent interacts with a 2-dimensional gridworld maze with sparse rewards, moving Up, Down, Left, or Right. The goal provides a +10 reward, and the episode ends. The state representation is a greyscaled image of the grid, divided by 3 to lie in [0, 1], with shape (24, 24, 1). Each action's effect is randomized at the start of training. The maze environment structure is shown in Figure 11. Stick actions are used with a probability of 0.25, frame skip of 4, and no terminal state shown on loss of life. Training parameters include \u03b3 = 0.99, RMSProp with learning rate 0.0005, MLP network with 2 hidden layers of 256 units, and ReLU non-linearities. Training uses 1 step Q-Learning for 100k timesteps, with a batch size of 64 and a replay buffer of size 10k. Target network is updated every 200 timesteps. The maze environment uses stick actions with a probability of 0.25 and training parameters include \u03b3 = 0.99, RMSProp with learning rate 0.0005, and a MLP network with 2 hidden layers of 256 units. Training involves 1 step Q-Learning for 100k timesteps with a batch size of 64 and a replay buffer of size 10k. The target network is updated every 200 timesteps. In the hyperparameter search, the embedding size for counts is 32, \u03b2 = 0.1 for intrinsic motivation scale, and various values are considered for reward subtraction and optimistic initialization bias. Different methods are evaluated with and without count-based intrinsic motivation, and hyperparameters for OPIQ and its ablations are explored. The best hyperparameters found include a decay rate of 100 timesteps for DQN-greedy, an optimistic initialization bias of 1, and pseudocount intrinsic motivation set to True. Initialisation Bias: Bias: 1, Pseudocount intrinsic motivation: True. Reward Subtraction: Constant to subtract: 1, Pseudocount intrinsic motivation: False. OPIQ: M: 0.5, C action : 1, C bootstrap : 1. OPIQ without Optimistic Bootstrapping: M: 2, C action : 10. OPIQ without Pseudocounts: M: 2, C action : 10, C bootstrap : 10. Training lasts for 1mil timesteps with linear decay of \u03b5. Batch size of 64, replay buffer of size 250k, target network updated every 1000 timesteps. Embedding dimension for counts is 128. Different values of \u03b2 and reward subtraction are considered for DQN + PC. For OPIQ and its ablations, the final layer's bias is set to {0.1, 1, 10} and M is set to 2. Different values are considered for C action and C bootstrap. The RND bonus uses the same architecture as DQN with a different output size. For DQN + DORA, the final layer's weights and bias are initialized to 0. Different values are swept across for the intrinsic reward \u03b2 dora. For DQN + DORA OA, \u03b2 dora = and \u03b2 dora_action is swept across. BSP uses a specific architecture with K = 10 different values. The architecture for BSP involves using 10 bootstrapped DQN heads and sweeping over different values of \u03b2 bsp. Training involves 3-step Q-Learning with a decay in \u03b5 from 1 to 0.01 over 1 million timesteps. The best hyperparameters found include M: 2, C action: 100 for OPIQ without Optimistic Bootstrapping, and M: 2, C action: 100, C bootstrap: 0.1 for OPIQ without Pseudocounts. The network used is the standard DQN for Atari, with training lasting for 12.5 million timesteps. Target network is updated every 8000 timesteps. The target network is updated every 8000 timesteps. Different values of \u03b2 are considered for various methods. The RND bonus uses the same architectures as in previous work with a smaller target network. BSP involves 10 bootstrapped DQN heads and varying values of \u03b2 bsp. All methods are tested with 4 independent runs and results are sorted by median maximum episodic reward. The best hyperparameters found include M: 2, C action: 100 for OPIQ without Optimistic Bootstrapping, and M: 2, C action: 100, C bootstrap: 0.1 for OPIQ without Pseudocounts. The best hyperparameters found for training the DQN + DORA agent include OPIQ without Optimistic Bootstrapping: M=2, C action=0.1, \u03b2 mmc=0.005. The intrinsic reward used is the squared error between the predictor network and the target network. DQN + DORA uses n-step SARSA with \u03b3 E=0.99 for training the E-values network. The replay buffer size is batch size * 4 and batch size elements are sampled for training at every timestep. Additionally, the Q-values used for action selection are augmented. For training the DQN + DORA agent, the output of each head is normalized by 1/K, where K is the number of heads. The 3 step Q-Learning target is mixed with the environmental rewards monte carlo return for the episode in experiments on Montezuma's Revenge. The intrinsic rewards are not used as part of the monte carlo return, as they are recomputed whenever used as targets for training. The intrinsic rewards are recomputed for training targets, but doing so for an entire episode is computationally prohibitive. Results show that OPIQ and ablations explore the environment faster than count-based baselines, with optimism during bootstrapping being important. Ablations without optimistic bootstrapping exhibit more variance, and on a simple task, the ablation without count-based intrinsic motivation performs similarly to full OPIQ. OPIQ emphasizes directed exploration with varying values of M. A small M of 0.1 leads to insufficient exploration, while a large M hinders exploration. M = 0.5 performed best on one task, but M = 2 was better on a harder Maze environment. Optimism in Q-values during bootstrapping incentivizes the agent to explore novel state-action pairs. In Montezuma's Revenge, optimistic Q-value estimates are crucial for exploration. Using a simple failure case, it is shown that pessimistically initialized greedy Q-learning may not find the optimal policy. The agent's actions are based on Q-value estimates, with ties broken uniformly. In a single state MDP, the agent must select the right action to receive a reward. However, with pessimistic initialization, the optimal policy may not be discovered. In Montezuma's Revenge, optimistic Q-value estimates are crucial for exploration. A counterexample shows that without intrinsic motivation, OPIQ still requires it to avoid under-exploration in stochastic environments. OPIQ without intrinsic motivation does not satisfy Theorem 1, as there exists a scenario where linear regret occurs with a probability greater than the allowed failure probability. The MDP used has \u03bb > 1 and a \u2208 (0, 1) with p < 1 \u2212 a. The left action has stochastic rewards, returning +1 with probability a and 0 otherwise, while the right action always gives a/\u03bb reward. OPIQ will choose the sub-optimal policy of taking the right action if the left action consistently yields 0 reward, as the Q-value estimate for the right action will eventually surpass that of the left action. The Q-value estimate for the right action is a/\u03bb > 0. The sup-optimal policy incurs a linear regret, not bounded by the theorem. The probability of failure is at least (1 \u2212 a) R, where R is the number of times the left action is selected. This probability decreases as R increases. The Q-values will be updated accordingly. The optimistic bonus for the right action decays to 0, allowing the left action to be taken. The upper bound for R is ( \u03bb a ) 1/M, leading to an underestimation of the probability of failure. If (1 \u2212 a) R < p, a contradiction arises, leading to the choice of \u03bb to ensure M > log(\u03bb/a)/ log(log(p)/ log(1 \u2212 a)). The proof for Theorem 1 involves bounding the regret of the algorithm after K episodes by adjusting the magnitudes of b T N based on K. The proof closely follows UCB-H with minor adjustments, ensuring the total regret of Q + is at most OPIQ with probability at least 1 \u2212 p. Theorem 2 utilizes Azuma's Inequality for a martingale sequence Z 0 , ..., Z n. Theorem 2 utilizes Azuma's Inequality for a martingale sequence Z 0 , ..., Z n, providing properties for \u03b7 i N and a recursive formula for Q + and Q *. The proof involves defining stopping times for episodes where actions were taken, applying Azuma's Inequality, and using a union bound to establish a probability bound. The proof involves defining stopping times for episodes where actions were taken, applying Azuma's Inequality, and using a union bound to establish a probability bound. A union bound over all s \u2208 S, a \u2208 A, t \u2208 [H] gives a probability of 1 \u2212 2\u03b4 for all (s, a, t, k) \u2208 S \u00d7 A \u00d7 [H] \u00d7 [K]. The algorithm is proven to be efficient with sub-linear regret, following a similar proof to Theorem 1 from (Jin et al., 2018). The total regret of Q + is bounded by O( H 4 SAT log(SAT /p)) for M \u2265 1 and O(H 1+M SAT 1\u2212M + H 4 SAT log(SAT /p)) for 0 < M < 1."
}