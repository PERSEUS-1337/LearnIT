{
    "title": "rJg7BA4YDr",
    "content": "Turing complete computation and reasoning are seen as essential for general intelligence. Neural networks struggle to generalize beyond their training data, but a new model called the Neural Execution Engine can compute basic algorithms accurately and generalize well to unseen data. Neural networks are universal function approximators, capable of learning complex functions with enough data and optimization. Recent proposals aim to implement general programs inspired by conventional computer systems, but struggle with strong generalization to new data distributions. The difficulty may stem from a lack of prior structure, suggesting that supervision over intermediate program states could improve network training. Neural networks can be trained to implement algorithms by breaking them down into subroutines. Each subroutine represents a simple function that can be composed with others to express the algorithm. This approach challenges the network to generalize beyond its training data and operate on raw numbers. The goal is to teach neural networks general computation by operating on raw numbers or distributions. Subroutines must be accurate for overall inference accuracy. While training on complex tasks may not lead to strong generalization, minor modifications and more supervision can improve generalization. Using the transformer, subroutines can be learned to work towards end-to-end learning of complex algorithms. Transformers are utilized to learn subroutines for common algorithms from input/output traces, using binary number representations and conditional masking. This enables effective performance on numeric tasks like comparison and addition. The transformer can modulate its own mask for generalization to longer program runs, achieving near-perfect performance on larger tasks. These networks over subroutines are referred to as neural execution engines (NEEs). Transformers are a family of models that use self-attention to process sequences simultaneously. They consist of stacked blocks for encoding input sequences and decoding output sequences. The transformer learns a mapping from input tokens to output tokens by embedding them individually. The transformer architecture utilizes self-attention layers to transform vectors in a sequence by considering their affinity to each other. Positional encodings are not used in this paper, and single-headed attention is employed. Self-attention blocks, consisting of self-attention and feed-forward layers, form the encoder and decoder. The self-attention mask is crucial for preventing certain positions from influencing others, especially during decoding. In the transformer architecture, self-attention is used to transform vectors in a sequence by considering their relationships. Masking is crucial to prevent positions from influencing each other, particularly during decoding. Sorting is explored as a way for neural networks to mimic general execution, focusing on the task of mapping unsorted numbers to sorted ones. The study distinguishes between generalization and strong generalization, with an emphasis on performance on unseen and longer sequences. The variety of sorting algorithms provides a rich domain for investigation. Neural networks use various number representations for sorting algorithms, with binary numbers showing strong generalization capabilities outside of their training domain. Binary numbers are hierarchical, allowing for exponential extension of the training range with each toggled bit. Neural execution engine (NEE) is a transformer that processes binary numbers with an encoding mask to output data values or pointers. It can modify its own graph, similar to graph attention networks (GATs). The NEE mechanism includes bitwise embeddings for input binary vectors. The NEE mechanism uses bitwise embeddings to process binary vectors. It defines learnable vectors for each bit position and sums them elementwise based on the corresponding bit value. Tokens \"start\" and \"end\" are important for sequence processing, with \"s\" used as input to the decoder and \"e\" denoting the end of a sequence. The model learns an embedding vector for \"e\" such that it is greater than any input value. Conditional masking is applied in the encoder to ignore certain inputs during processing. The NEE mechanism uses masks to focus attention on specific inputs during computation. The mask is a binary matrix where 0 indicates consideration and 1 indicates ignoring. By modifying the mask with the decoder's output pointer, the encoder learns where to focus. The mask vector is broadcasted to create the mask, with 0 indicating consideration and 1 indicating ignoring of inputs. The NEE mechanism uses masks to focus attention on specific inputs during computation. The mask vector, referred to as b, is updated using a one-hot encoding o to represent the position of interest to mask out. The updated mask b is computed using a bitwise XOR function. This mechanism allows NEE to mimic the behavior of various subroutines by training it on execution traces using teacher forcing. At inference, the argmax of the pointer output head is chosen. Future work will focus on allowing the network to make more complex updates. The function operates on a sequence of inputs, treating subsets independently. Reshaping turns the sequence into a mini-batch for parallel processing. Dependent inputs are concatenated with an 'e' token. NEE outputs a pointer for mask updates, useful in merge sort. The mechanism of NEE involves using a start token to compute attention blocks over encoder outputs. The vanilla transformer is studied in sorting tasks using seq2seq fashion with input sequences of unsorted binary numbers and outputting correctly sorted sequences. The decoder employs a greedy decoding strategy. The decoder in the sorting task uses a greedy decoding strategy with one-hot 256-dimensional vectors. Randomly generated numbers are easier to sort than numbers with small differences. Performance of the vanilla transformer is evaluated on exact content and positional match. Modifications are made to better tune the encoder for processing binary numbers due to unsatisfactory performance in distinguishing close numbers. The vanilla transformer's performance in distinguishing close numbers is improved by using bitwise embeddings and a symmetric feed-forward self-attention. Shared linear projections of queries, keys, and values are used for handling small differences, and the residual connection strength is scaled up by a factor of 1.5. These modifications are shown in the Appendix. Five models are compared for sorting performance, with different attention mechanisms and modifications. The vanilla transformer model's performance in sorting lists of numbers is compared with various modifications, including dot-product attention and different embeddings. While the models perform well with sequences \u2264 8 elements long, they struggle with longer sequences. The attention matrix of the last layer in the decoder shows that the model's accuracy decreases as sequence length increases beyond 8 numbers. The study explores improving transformer model performance in sorting tasks by breaking the problem into smaller pieces and providing more supervision during training. By training the model to learn simple sorting algorithms as subroutines, it can achieve high attention resolution and accuracy even with longer sequences. The goal is to select the largest source code function to learn, where the output is data-dependent. The transformer model in sorting tasks is improved by breaking the problem into smaller pieces and providing more supervision during training. By training the model to learn simple sorting algorithms as subroutines, it can achieve high attention resolution and accuracy even with longer sequences. The function is data-dependent, and a conditional mask is used to restrict the locations in the unsorted sequence where the transformer can attend, separating control from computation. This allows the transformer to learn output logits of larger magnitude for sharper attention. The experimental results demonstrate strong generalization in sorting sequences up to length 100. The study explores how well the approach generalizes to other sorting algorithms, such as merge sort. The code for one implementation of merge-sort is shown, which is broken up into data decomposition and an action subroutine. Merge sort divides the list in half until there is one element left, which is already sorted. Merge sort unrolls the recursive tree by combining elements in pairs until the list is fully sorted. The reshape() operation divides or combines lists to emulate start and mid pointers. The transformer in Figure 4 implements the computation described in Figure 5, using a mask to control which numbers are seen by the encoder. The model described in Figure 5 uses a mask to control which numbers are seen by the encoder. It outputs the smallest number from unmasked numbers and the position of the selected number, generating the next mask with SHIFT and XOR operations. The model stops when it outputs an end token. The NEE can learn merge sort with strong generalization over long sequences, trained on sequences of length \u2264 8. In this section, we explore the composition of multiple NEEs to execute a complex algorithm, specifically Dijkstra's algorithm for finding shortest paths. The algorithm involves four main steps: Initialization, Computing new paths, Updating path lengths, and Selecting the node with the smallest distance to the source node. These steps are repeated until completion. The algorithm for finding the shortest path involves steps like Initialization, Computing new paths, Updating path lengths, and Selecting the node with the smallest distance to the source node. The addition subroutine in Dijkstra's algorithm iteratively adds elements in the adjacency list of a selected node to the current shortest path. Learning addition is a more challenging task than sorting, requiring understanding of concepts like zero and infinity. The model learns addition with 100% accuracy by using a modified transformer architecture. The learned embeddings are visualized using PCA, showing a structured number system where numbers increase by 1 are placed on a line. The model achieves high accuracy in sorting embeddings of numbers that increase by 1 on a line. Testing on unseen numbers also shows good generalization. The study suggests the model can generalize to larger bit vectors without needing to observe every number in training. Training on graphs with \u2264 10 nodes generalizes perfectly to \u2264 40-node graphs. Training neural network execution engines (NEEs) on graphs with up to 8 nodes allows them to reliably compute Dijkstra's algorithm on graphs with up to 40 nodes, achieving 100% test accuracy. The NEEs perform necessary subroutines and can be composed to generalize to larger graphs, as shown in Figure 15. Neural networks inspired by computer architectures aim to learn complex algorithms from weak supervision. While they can theoretically represent any computable function, they struggle with generalizing to longer sequences. Our focus is on learning simple functions on raw numerical values that generalize to extremely long sequences using transformers with minimal modifications. Neural program synthesis uses neural networks to generate programs for solving tasks with input/output pairs, aiming for generalization beyond training data. Research at the intersection of logic programming and neural networks involves learning rules from positive examples. For example, in learning to sort, systems are given numbers as one-hot encoded vectors. In contrast to previous works that use neural networks to learn number systems for arithmetic, our approach focuses on learning relationships from binary numbers. Previous studies have embedded integers as vectors and trained on triplets to perform modular arithmetic, showing the capability to generalize to new triplets. Other works have modeled objects as matrices to learn higher-order relationships. Trask et al. (2018) explores neural networks' generalization capability on scalar-values inputs outside their training range, developing architectures better suited for scalar arithmetic. Shi et al. (2019) demonstrated graph neural networks learning from 64-bit binary memory states, improving prediction outside training range. Wallace et al. (2019) found language models proficient in numerical reasoning after training on question/answer datasets. Language models show high numeracy, especially with character-level embeddings. Neural execution engines (NEEs) use learned masks and execution traces to mimic subroutines. Transformers can learn simple subroutines with strong generalization, paving the way for end-to-end learning of complex algorithms. Future work includes expanding subroutine complexity, using binary in transformer outputs, teaching masks more pointer primitives, and exploring reinforcement learning for supervision. In this section, the authors discuss the hyperparameters used in their architecture for sorting and addition tasks. They found that bitwise embeddings of dimension 16 were sufficient for sorting, while a dimension of 24 was necessary for addition. They used single-headed attention for all tasks and an exponential moving average of parameters during learning for stability. The remaining transformer hyperparameters were set to their defaults. The authors modified their architecture for sorting tasks, studying two scenarios: seq2seq setup and selection sort algorithm. They analyzed 3 data distributions, including random sequences and mixed settings with varying levels of difficulty. The second scenario had more supervision than the first, requiring the NEE to output the min value and location iteratively until the e token is returned. In a study on sorting tasks, the authors tested various architectural changes in different data distributions. These included random sequences, mixed settings with varying difficulty, and purely difficult settings with close numerical values. The modifications included scaling up residual connections, using different attention modules, and various encoding methods for input values. Visual representations of the original and modified encoders are shown in Figure 10. The study tested architectural changes in sorting tasks with different data distributions, including random sequences and mixed settings. The changes improved performance on sequences of length 8, but performance dropped as sequence length increased. Ablation with selection sort using NEEs showed most architectural choices performed well and generalized to longer sequences. In this case, most architectural choices perform well and generalize to longer sequences. Exceptions include using raw binary inputs, where a bitwise embedding is crucial for encoding number similarity. Despite some errors, the model with one-hot encoded input numbers shows better performance than in seq2seq settings. The model struggles more with hard examples, as shown in detailed visualizations of learned bitwise embeddings. The network learns coherent number system with structured embeddings, varying based on task. Embeds e as infinity. Even when 50% input numbers held out, network places them correctly in embedding structure, crucial for generalization to larger numbers. The network aims to generalize to larger numbers by scaling NEE to operate on 64-bit values. Future work will explore NEE embeddings for multiple or complex tasks, illustrated with an execution trace of Dijkstra's algorithm using NEE components on a toy graph. Dijkstra's algorithm uses a weighted adjacency matrix and maintains shortest path values from each node to the source, with NEE assisting in the computation. The NEE components perform 3 phases on graphs up to 8 nodes, generalizing perfectly to graphs up to 40 nodes. The phases involve referencing the adjacency matrix, updating shortest path values, selecting the current node, and terminating when all nodes are masked out."
}