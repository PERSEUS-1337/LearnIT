{
    "title": "HJlNpoA5YQ",
    "content": "In reinforcement learning, approximating the eigenvectors of the Laplacian graph provides a concise representation of state transitions. Existing methods are computationally expensive and lack justification in general RL settings. A new scalable method is introduced in this paper, showing promising results beyond tabular, finite-state settings. The paper introduces a scalable method for approximating eigenvectors of the Laplacian graph in reinforcement learning. It shows improved performance in tabular, finite-state settings and potential benefits in goal-achieving RL tasks. The choice of data representation in machine learning methods, particularly in RL, can impact generalization, exploration, and learning speed. In reinforcement learning tasks, the agent's goal is to achieve a specific state configuration, such as reaching a red cell in a gridworld environment. The choice of reward function, like negative Euclidean distance from the goal, heavily influences the agent's success. Using different state representations, such as one-hot or (x, y) coordinates, can impact learning speed and the agent's ability to navigate towards the goal efficiently. The ideal reward structure in reinforcement learning should be based on state representations that correspond to the agent's ability to reach different states. One approach is using the graph Laplacian, which provides an embedding of each vertex in R^d. The graph Laplacian provides an embedding of each vertex in R^d, useful for applications like graph visualization and clustering. In reinforcement learning, the Laplacian eigenvectors can accelerate learning and construct options with exploratory behavior. The Laplacian state representations in reinforcement learning accurately reflect the geometry of the problem, providing a strong learning signal and avoiding spurious local optima. Current techniques for approximating or learning these representations are not suitable for model-free RL due to the computational expense of eigendecomposition methods. Our proposed approach aims to efficiently approximate the eigenvectors of the Laplacian in reinforcement learning, addressing limitations of current methods that are computationally expensive and not suitable for stochastic or online settings. Our method utilizes function approximation based on the spectral graph drawing objective, allowing for stochastic optimization over minibatches of experience to provide better Laplacian eigenvector approximations, especially in non-tabular representations. Our approach efficiently approximates the eigenvectors of the Laplacian in reinforcement learning, overcoming computational limitations of current methods. It utilizes function approximation based on the spectral graph drawing objective for better Laplacian eigenvector approximations in non-tabular representations. The text discusses the concept of a probability distribution over a set S in a Hilbert space H, where elements are represented as vectors. It introduces the inner product and linear operators in this context, defining self-adjoint linear operators as symmetric matrices. The text then delves into a more general form of these definitions within a measure space. The inner product in a Hilbert space gives rise to orthogonality and a norm. The graph Laplacian is constructed using Hilbert-Schmidt integral operators with specific properties. The operator is self-adjoint and compact, leading to spectral properties. The Laplacian operator on a Hilbert space is defined as a compact, self-adjoint linear operator. Eigenfunctions of the Laplacian correspond to eigenfunctions of the operator I - D. The goal is to find the first d eigenfunctions associated with the smallest d eigenvalues of the Laplacian. Spectral graph drawing provides an optimization perspective on finding the eigenvectors of the Laplacian for large graphs with weighted edges. The graph drawing objective is to embed vertices in a low-dimensional space based on pairwise affinities, minimizing a set of orthonormal functions. This objective can be expressed in terms of the Laplacian, where the minimum value is achieved by the d smallest eigenvalues of L. Stochastic optimization can be used for a scalable approach to achieve this objective. In the RL setting, the Laplacian is approximated by optimizing the graph drawing objective using stochastic gradient descent on sampled states and state pairs. An agent interacts with the environment in a standard MDP setting, where observations and actions are determined by a policy. In the RL setting, the Laplacian is defined with respect to a fixed behavior policy \u03c0. The transition distributions form a Markov chain with a unique stationary distribution. The Laplacian in the RL setting is defined using a choice of \u03c1 and D, where D represents pairwise affinity between vertices on the graph. The Laplacian in the RL setting is defined with respect to a fixed behavior policy \u03c0, using a choice of \u03c1 and D to represent pairwise affinity between states. Learning the eigen-decomposition embedding \u03c6 poses challenges in model-free RL due to sampling constraints and the need for orthonormality in state spaces. The Laplacian in the RL setting is defined with respect to a fixed behavior policy \u03c0, using a choice of \u03c1 and D to represent pairwise affinity between states. Learning the eigen-decomposition embedding \u03c6 poses challenges in model-free RL due to sampling constraints and the need for orthonormality in state spaces. The exact orthonormality of functions f1 to fd may be intractable in many state spaces. The graph drawing objective can be minimized with stochastic gradient descent by sampling transition pairs from the replay buffer. To ensure orthonormality, the constraint is relaxed to a soft constraint and transformed into a penalty in the penalized graph drawing objective. The penalty weight \u03b2 may be learned using a neural network function. The Laplacian in RL is defined with respect to a behavior policy \u03c0, using \u03c1 and D to represent state affinity. Learning eigen-decomposition embedding \u03c6 faces challenges in model-free RL due to sampling constraints. The graph drawing objective is minimized with stochastic gradient descent by sampling transition pairs. The penalty weight \u03b2 can be learned using a neural network function approximator. The attractive term minimizes squared distance of embeddings of transitions, while the repulsive term repels embeddings of states. The repulsive term orthogonalizes embeddings of randomly sampled states. Our framework for Laplacian representation learning applies generally to RL, even in settings with innumerable state spaces accessed via sampling. We show that the graph drawing objective can stochastically optimize a representation module approximating Laplacian eigenfunctions, without requiring storage of the entire eigendecomposition. Some approaches use Oja's rule to avoid this limitation, offering a function approximator that yields arbitrary rows of the eigendecomposition. In RL, various approaches like Oja's rule and function approximators are used to approximate Laplacian eigenvectors without storing the entire eigendecomposition. Other methods, such as scaling to large datasets by subsampling subgraphs and extending spectral clustering to out-of-sample points, may also be beneficial in the RL setting. In RL, different methods are used to approximate Laplacian eigenvectors without storing the entire eigendecomposition. Some approaches optimize objectives similar to Eq. 2 but handle the orthonormality constraint differently. Shaham et al. introduce a special-purpose orthonormalizing layer, while Pfau et al. utilize a different approach. However, the orthonormalization process can be numerically unstable and may require large minibatches for stability. Our approach avoids numerical errors from orthonormalization and does not scale quadratically in the number of embedding dimensions. We demonstrate Laplacian representations in RL tasks, specifically reward-shaping in continuous-control environments. Previous works were limited to small discrete state spaces or focused on qualitative assessments of learned options. We evaluate our learned representations by their approximation of the subspace spanned by the smallest eigenfunctions of the Laplacian. In this subsection, the evaluation protocol involves projecting embeddings onto an orthonormal basis to satisfy the graph drawing objective. Using a FourRoom gridworld environment, a dataset of experience is generated by randomly sampling transitions. The embedding learned by the approximate graph drawing objective is compared against methods proposed by Machado et al. (2017a; b), who find the first d eigenvectors of the Laplacian using eigen-decomposition of a matrix formed by stacked transitions. Our method outperforms previous methods in evaluating raw state representations in a gridworld environment. Even with tabular representation, our method performs better, especially with limited training samples. Additionally, our learned representations enhance the performance of reinforcement learning agents in goal-achieving tasks. In goal-achieving tasks, the agent is rewarded for reaching a certain state. The learned representations are well-suited for reward shaping in these settings. A goal-achieving task is defined by an environment with transition dynamics but no reward, along with a goal vector. The learning objective is to train a policy that guides the agent to reach a specific state based on a predefined function mapping states to goal vectors. In goal-achieving tasks, a reward function must be defined for reinforcement learning. Reward shaping with learned representations can speed up learning by defining the reward based on distance in a learned latent space. Different options are proposed for defining the reward function when the goal space is the same as the state space. In goal-achieving tasks, reward shaping with learned representations can speed up learning by defining the reward based on distance in a learned latent space. Two options are proposed for defining the reward function when the goal space is the same as the state space. Experimental evaluation is conducted on gridworld environments with different mazes, showing that the learned rewards reflect the environment dynamics well, especially in TwoRoom and HardMaze scenarios. The shaped reward is defined as a mix of L2 distance in the learned latent space and sparse reward to better guide the agent towards the goal state. This mix helps differentiate between the goal state and states near it, improving learning performance. In goal-achieving tasks, properly shaped reward can accelerate learning of the policy. Laplacian-based shaped reward significantly outperforms other reward settings in environments where the raw feature space cannot reflect an accurate distance. In goal-achieving tasks, Laplacian-based reward significantly outperforms other settings. In continuous control navigation tasks, \"mix\" and \"fullmix\" outperform all other methods, showing the benefits of learned representations in reward shaping. The text discusses the advantages of using learned representations for reward shaping in goal-achieving tasks. Both embedding the goal space and embedding the state space provide significant benefits, despite not being perfect solutions. The challenge lies in aligning the two spaces effectively for better performance in achieving goals. Our approach is general and scalable, applicable to any state space and relying on sampling mini-batches of states. We applied this method to reward shaping in discrete and continuous-control settings. The existence of the smallest eigenvalues of the Laplacian is discussed to ensure well-definedness in Hilbert space. The eigenvalues of a compact self-adjoint linear operator on H have specific properties according to the spectral theorem. These eigenvalues can be a finite set or countably many, all real, and converging to 0 if infinite. The operator norm dictates the range of eigenvalues, with the largest eigenvalue being 1. The largest eigenvalue of a compact self-adjoint linear operator on H converges to 0. If d is smaller than the number of positive eigenvalues of D, the largest d eigenvalues are guaranteed to exist. A more general definition of D is introduced with a discount factor \u03bb \u2208 [0, 1). Sampling from the transition distribution can be done by first sampling \u03c4 \u223c q \u03bb and then rolling out the Markov chain for \u03c4 steps. The Markov chain for \u03c4 steps starting from u is defined in terms of discrete probability distributions. Sampling from P \u03c0 \u03bb (v|u) may require rolling out more than one step from u. Due to finite length trajectories in the replay buffer, exact sampling from the distribution is impossible. The generalized D is defined with discounted transition distributions. The assumption is made that P \u03c0 \u03bb (\u00b7|u) is absolutely continuous to \u03c1 for any u. The assumption is that the behavior policy can explore the whole state space. Proof that D(u, \u00b7) is a density with respect to \u03c1. Discussion on the finite time horizon and the implications for sampling from the replay buffer. When the initial state distribution is concentrated, mixing rate is slow, and time horizon is short, adjusting transition probabilities by adding a small reset probability can approximate termination of trajectories without affecting the Markov property. This allows sampling from the replay buffer to approximate the stationary distribution, aiding in minimizing the graph drawing objective. Adjusting transition probabilities by adding a small reset probability can approximate termination of trajectories without affecting the Markov property. This aids in minimizing the graph drawing objective by sampling state pairs and using discounted multi-step transitions with specific hyperparameters for representation learning and policy training. For policy training, the vanilla DQN is used with online and target networks representing the Q-function. The policy selects actions with the highest Q-value from the online network. The networks are trained to minimize Bellman error by sampling transitions from a replay buffer. The target network is updated every 50 steps with a mixing rate of 0.05. Epsilon greedy with = 0.2 is used for exploration. Reward discount is 0.98. Adam optimizer with learning rate 0.001 is used. Fully connected networks with 3 hidden layers and 256 units each are used for representation mapping and Q-functions. Activation functions are relu. The PointMass agent has a 6-dimensional state space and a 2-dimensional action space. The Ant agent has a 29-dimensional state space and an 8-dimensional action space. The Ant agent has a 29-dimensional state space and an 8-dimensional action space. The success criteria involve reaching an L2 ball around a specific position in the maze. Hyperparameters include using d=20 for representation learning, discounted multi-step transitions with different \u03bb values for PointMass and Ant, and pretraining representations for 50000 steps. Policy training involves using the vanilla DDPG with online and target networks, each containing actor and critic sub-networks. The network consists of actor and critic sub-networks, with the online critic trained to minimize Bellman error and the online actor trained to maximize Q-value. The target network is updated every step with a mixing rate of 0.001. Exploration follows the Ornstein-Uhlenbeck process. The policy is trained with Adam optimizer, batch size 100, actor learning rate 0.0001, and critic learning rate 0.001. Representation mapping uses a fully connected network with 3 hidden layers and 256 units each. Actor and critic networks have 2 hidden layers with units (400, 300) and relu activation functions. The study explores online training of representations in maze navigation tasks, showing equivalent performance to pretraining-and-fix methods. The agent moves faster during policy learning, with annealing of \u03bb in D towards the end of training. Online training offers no benefit due to efficient exploration from randomized starting positions. Future research could investigate the advantages of online training in exploration-hard tasks."
}