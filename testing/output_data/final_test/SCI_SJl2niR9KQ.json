{
    "title": "SJl2niR9KQ",
    "content": "Many machine learning image classifiers are vulnerable to adversarial attacks, where inputs are altered to trigger misclassification. Current methods focus on pixel perturbations within a specified magnitude, lacking practical utility and security motivation. To address this, a novel evaluation measure called parametric norm-balls is proposed, perturbing physical parameters underlying image formation instead of pixel colors. This approach involves a physically-based differentiable renderer to propagate pixel gradients to the parametric space of lighting and geometry. Our approach enables physically-based adversarial attacks by perturbing physical parameters underlying image formation instead of pixel colors. This differentiable renderer leverages models from interactive rendering literature to balance performance and accuracy trade-offs for memory-efficient and scalable adversarial data augmentation. Research in adversarial examples contributes to robust (semi-)supervised learning, data augmentation, and machine learning understanding. Traditional pixel-based adversarial attacks yield unrealistic images under a larger perturbation, while parametric lighting and geometry perturbations produce more realistic images. The reliance on overly simplified attack metrics, such as pixel norm-balls, is common in adversarial attack strategies. The pixel norm-ball game is attractive due to its simplicity and well-posedness, allowing for arbitrary pixel perturbations within a constrained magnitude. The pixel norm-ball is useful for research but limited in real-world security scenarios. Adversarial methods based on pixel norm-balls are divorced from real-world variations, leading to defenders effective only against unrealistic attacks. A new measurement norm rooted in realistic image synthesis processes is advocated for, moving away from overly simplified metrics like pixel norm-balls. Our proposed solution, parametric norm-balls, rely on perturbations of physical parameters of a synthetic image formation model instead of pixel color perturbations. Using a physically-based differentiable renderer, we can perturb the underlying parameters of the image formation process, indirectly controlling pixel colors. Perturbations in this parametric space enable adversarial approaches applicable to real-world applications and allow for more significant perturbations without compromising the realism of the resulting image. This approach is critical for various adversarial tasks. Parametric norm-balls game playing is crucial for adversarial tasks, such as building defenders robust to natural perturbations. A physically-based differentiable renderer allows for perturbations in the image formation parameter space, extending traditional pixel norm-balls to physically valid parametric norm-balls. The renderer achieves state-of-the-art performance in speed and scalability, enabling training with adversarial images for data augmentation. Our analytically-differentiable renderer enables the generation of parametric adversarial examples, different from pixel norm-balls. This approach increases classifier robustness to real-world deformations, demonstrated by training with computer-generated adversarial images and testing on real photographs. Our work evaluates parametric space perturbation against a theory of realistic image generation, different from existing literature that focuses on computer-generated adversarial images. Previous studies have shown that training deep networks with simulated data can outperform networks trained on real photos, indicating the potential of simulated images in computer vision and machine learning for real-world tasks. Rendering synthetic data can provide a vast amount of annotated input data, surpassing existing datasets. Training on synthetic data shows promise in machine learning development, with potential for physically based adversarial training. Szegedy et al. (2014) highlighted the vulnerability of deep neural nets to manipulated images. The advancement of adversarial techniques in deep neural networks has seen significant progress, with various methods proposed to create adversarial examples. These techniques involve manipulating images with imperceptible noise to induce misclassification. Recent studies have explored the transferability of attacks to the physical world by printing and photographing adversarial images. Extensions to non-planar geometry and multiple viewing angles have also been proposed, although they still rely on direct pixel or texture manipulation on physical objects. The current works focus on manipulating pixels or textures on physical objects to create adversarial images, leading to unrealistic results. Zeng et al. (2017) use a data-driven approach to alter physical parameters through a rendering network, but this method requires high-quality training and time to compute adversaries. In contrast, our approach is faster and more efficient. Our approach is based on a differentiable physically-based renderer that models the image formation process, allowing us to alter physical parameters and compute derivatives more rapidly compared to previous non-pixel attacks. Our renderer explicitly models the physics of image formation processes, generating realistic images that can elicit correct classifications from networks trained on real-world data. Adversarial attacks create misleading images that fool neural networks into incorrect classifications. These attacks use cost functions to manipulate images, such as increasing cross-entropy loss for the correct class or decreasing it for the least-likely class. Different combinations of cross-entropies can be used for untargeted and targeted attacks. In adversarial attacks, images are manipulated to deceive neural networks into wrong classifications by adjusting labels and physical parameters. Gradient descent is used to find parametric adversarial examples in the image formation model. Rendering is the process of generating a 2D image from a 3D scene by simulating the physics of light. Light sources emit photons that interact with objects, changing trajectory until reaching a sensor like a camera. A physically based renderer models these interactions mathematically. Differentiable renderer is developed to analyze the physical process. Adversarial lighting can fool classifiers into seeing different objects. The differentiable renderer is developed based on common assumptions in real-time rendering, including diffuse material, local illumination, and distant light sources. These assumptions simplify the rendering process and allow for representation of lighting using spherical harmonics. Our differentiable renderer, based on common assumptions in real-time rendering, uses spherical harmonics to represent lighting. Analytical differentiation of the rendering equation allows for faster computation of derivatives with respect to lighting, geometry, and texture. This approach outperforms previous fully differentiable renderers like OPENDR, handling problems with over 100,000 variables. Adversarial lighting involves changing spherical harmonics lighting coefficients to generate adversarial examples. Our renderer can compute these changes analytically, making it efficient and scalable. Our differentiable renderer uses spherical harmonics for lighting representation, allowing for faster computation of derivatives. Spherical harmonics act as an implicit constraint to prevent unrealistic lighting, filtering out high-frequency signals. Perturbing the parametric space of spherical harmonics lighting results in more realistic images compared to pixel perturbations. The text discusses creating an optical illusion by perturbing the geometry of shapes, specifically a jaguar being classified as a cat and a dog from different views. Adversarial geometry involves changing the position of a shape's surface, encoded as a triangle mesh with vertices and faces. Adversarial shapes are computed using the chain rule, with the Jacobian of a face normal vector with respect to its corner vertices being analytically computable. The text discusses creating adversarial examples by perturbing the geometry of shapes, including lighting and geometry changes. Adversarial examples exist in parametric spaces, with characteristics analyzed using spherical harmonics coefficients for environment lighting. Adversarial lighting attacks can fool classifiers, as shown in Figure 4 with single-view and multi-view examples. Adversarial lighting can optimize cost functions for each view, with gradients computed over all camera views. Adversarial lights can adapt to specific lighting conditions, including skylights. Adversarial geometry exists in both single-view and multi-view cases. In adversarial geometry, upsampling meshes increases degrees of freedom for perturbations. Multiview adversarial geometry creates optical illusions by classifying the same 3D shape differently from different angles. Parametric adversaries are analyzed for generalization to black-box models. In TAB2, 5,000 ResNet parametric adversaries are tested on various unseen networks like AlexNet, DenseNet, SqueezeNet, and VGG, showing shared adversarial examples across models. Parametric adversaries are also evaluated on black-box viewing directions, simulating real-world scenarios for self-driving cars. In TAB3, adversarial lighting and geometry algorithms are applied to a subset of views, with results indicating that adversarial lights are more generalizable than shapes to fool unseen views. The effectiveness of adversarial lighting and geometry perturbations is highlighted in a quantitative comparison using parametric norm-balls. Results show that adversarial lighting/geometry perturbations have a higher success rate in fooling classifiers compared to random perturbations in the parametric spaces. Switching to parametric norm-balls involves changing the norm-constraint from pixel color space to parametric space. The study evaluates robustness by comparing parametric adversarial and random perturbations, showing how many parametric adversaries can fool the classifier out of 10,000 adversarial lights and shapes respectively. The effectiveness of adversarial perturbations is demonstrated through norm-balls, with real-world implications for evaluating robustness. The runtime for computing derivatives is presented, with adversarial examples injected into the training process to increase classifier robustness. This evaluation differs from traditional methods by using differentiable rendering and real-world perturbations. In contrast to traditional methods, the evaluation in this study focuses on real photos rather than computer-generated images to increase classifier robustness. The WideResNet is trained on CIFAR-100 augmented with adversarial lighting examples using a common adversarial training method. In experiments, WideResNets trained on CIFAR-100 with different lighting conditions show comparable performance. Testing involves real photos of oranges captured with controlled lighting and camera settings. The study evaluates the robustness of classifier models trained on different lighting patterns using an LG PH550 projector. Results show improved performance on real photographs when trained on rendered images. Preliminary experiments suggest the potential of using rendered adversarial training to enhance robustness to real-world visual phenomena. Rendering adversarial data augmentation has the potential to enhance robustness in real-world tasks by using a differentiable renderer to manipulate predicted parameters and construct parametric adversarial examples. By incorporating real-time rendering techniques, the quality of rendering can be further improved. Incorporating techniques from the graphics community can enhance rendering quality by removing texture assumptions and extending derivative computations to materials for creating \"adversarial materials\" and poses. These extensions provide tools for modeling real security scenarios and are compared against pixel norm-balls methods. The goal is to show that parametric perturbations are more realistic across different scales. The text discusses the modeling of light flow in computer graphics for rendering photorealistic images efficiently. It mentions approximations made to the Rendering equation for light propagation. The focus is on synthesizing images under various performance requirements. The text discusses approximations made to the Rendering equation for light propagation in computer graphics for rendering photorealistic images efficiently. It mentions that the steady state solution to the rendering equation is intractable for practical applications, leading to the need for simplifications and assumptions to make the problem tractable. Many methods focus on ignoring light interactions that have minimal effects on the final rendered image. Light interactions in computer graphics are simplified to make rendering more efficient. Assumptions include light bouncing off surfaces and limited bounces allowed. A strong simplification is used, with only one iteration assumed for steady state approximation, leading to some artifacts. Light interactions in computer graphics are simplified for efficiency, leading to artifacts like disappearing shadows. Standard rasterization systems like OPENGL compute illumination per pixel based on visible fragments and direct light sources. Each object point has a material model for light transfer. In computer graphics, material parameters determine how light is reflected off an object, forming its texture. There are various material models, including diffuse materials like Lambertian materials, where light is reflected uniformly. The albedo, representing the color of the surface, is a constant function with respect to angle. The integration domain is reduced to the upper hemisphere. In computer graphics, material parameters determine how light is reflected off an object, forming its texture. The integration domain is reduced to the upper hemisphere for modeling light not bouncing through objects. Environment mapping is used to pre-compute or cache contributions to the illumination of a scene, representing the total lighting. In computer graphics, material parameters determine how light is reflected off an object, forming its texture. The integration domain is reduced to the upper hemisphere for modeling light not bouncing through objects. Environment mapping is used to pre-compute or cache contributions to the illumination of a scene, representing the total lighting. The position of the object receiving light from an environment map does not matter, simplifying the process. Spherical harmonics are chosen as a basis for numerically integrating the rendering equation, allowing for pre-computation and spectral accuracy. The rendering of diffuse objects under distant lighting can be approximated by spherical harmonics bases. Spherical harmonics coefficients can efficiently evaluate the rendering equation in computer graphics, considering lighting conditions and material properties on the surface of a 3D shape. Spherical harmonics coefficients efficiently evaluate the rendering equation in computer graphics, considering lighting conditions and material properties on the surface of a 3D shape. The associated Legendre polynomials define the spherical harmonics basis, which are rotationally symmetric along the shape's surface. Spherical harmonics are used to efficiently evaluate the rendering equation in computer graphics by incorporating rotationally symmetric bases. The integral domain is changed to S2 via a max operation, with the integral consisting of a lighting component and a component dependent on the normal. Pre-computing these components using spherical harmonics allows for evaluation via a dot product at runtime. The lighting component can be approximated using spherical harmonics up to a certain band, with coefficients evaluated through the orthogonality of spherical harmonics. The rendering equation is evaluated for each point in a shape V visible through each pixel in the image I, determined by camera parameters \u03b7. The image I is represented as DISPLAYFORM2 with surface normal N(V) and texture \u03c1(V, \u03b7). Derivatives with respect to lighting and material parameters are computed using Jacobian matrices and defined equations. Texture variations are assumed to be piece-wise constant. The texture is defined by piece-wise constant variations with respect to the triangle mesh discretization. Outdoor daylight conditions are modeled using the Preetham skylight model, calibrated by atmospheric data and parameterized by turbidity \u03c4 and two polar angles \u03b8 s, \u03c6 s. The spherical harmonics representation of the Preetham skylight is derived by a non-linear least squares fit. The text discusses computing derivatives of lighting with respect to skylight parameters using spherical harmonics rotation. It also mentions texture variations being piece-wise constant and provides equations for computing derivatives on face i. The relationship between unit normal vector and polar angles is recalled for the derivation process. The text discusses adversarial lighting iterations in training a WideResNet model using real-world lighting data. Lighting gradients are computed with a stepsize of 0.05, and random lighting examples are generated by perturbing coefficients. The model is trained for 150 epochs with specific parameters and optimization techniques. In the late stages of training, the network becomes more robust to different lighting conditions, requiring dramatic changes to fool the model. Rendering quality was evaluated using high-quality textured 3D shapes from online sources, with augmented variations in field of view, backgrounds, and viewing directions. We augmented the shapes by changing the field of view, backgrounds, and viewing directions, then kept the configurations correctly classified by a pre-trained ResNet-101 on ImageNet. The centroid was placed at the origin, shapes were normalized to range -1 to 1, and background images included plain colors and real photos. Viewing directions were chosen to be 60 degrees zenith and uniformly sampled 16 views from 0 to 2\u03c0 azimuthal angle. The histogram of model confidence on correct labels over 10,000 rendered images showed faithful rendering quality recognized by models trained. The rendering quality of our augmented shapes was recognized by models trained on natural images."
}