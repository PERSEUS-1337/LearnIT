{
    "title": "rkgARFTUjB",
    "content": "Neural architecture search (NAS) has advanced in computer vision but not in natural language understanding (NLU). A new search space is defined for NLU tasks, splitting it into encoder and aggregator search spaces. The search algorithm uses DARTS and progressively reduces the search space to save time and resources. The new neural networks generated through the search algorithm can achieve performances comparable to state-of-the-art models without language model pre-training, by reducing the search space every few epochs to save time and resources. Neural architecture search (NAS) has gained attention for methodological innovations like DARTS, SNAS, P-DARTS, and PC-DARTS. NAS has led to better models for vision tasks like image classification, semantic segmentation, object detection, and superresolution. NAS for natural language processing tasks is less explored, with innovations like NASNet, ENAS, and DARTS focusing on new RNN cells. There is limited research on using NAS for NLU tasks, with the evolved transformer showing promising results in machine translation. However, the high computation cost remains a challenge. NAS has not been fully explored for various NLU tasks such as classification, NLI, and NER. One-shot architecture search methods have not been studied for NLU tasks. Existing work on one-shot architecture search methods for NLU tasks aims to reduce search cost by exploring different encoder options like CNN and transformer models. Previous NAS literature focused on discovering new recurrent cells for RNNs, but other encoder options are available. Recent works have proposed sparse versions like star-transformer. In this work, the authors propose a search space for encoders that includes various options such as RNNs, LSTM, GRU, highway networks, convolutional networks, multi-head attention, and star-transformer. The goal is to automate the selection process of an aggregator, which is currently done manually in a trial-and-error manner. The authors propose a search space for encoders including various options like RNNs, LSTM, GRU, and star-transformer. They suggest automating the selection process of an aggregator through an aggregator search cell connected to encoder operations. The search strategy is based on DARTS, with a focus on reducing computation cost. Experiments are conducted on text classification, NLI, and NER tasks. The authors redefine the search space for neural architecture search in NLU tasks by extending the encoder search space and defining the aggregator search space. They conduct NAS experiments on text classification, NLI, and NER tasks with one-shot NAS, achieving results comparable to the state-of-the-art models. Our approach achieves comparable results to state-of-the-art models in NLU tasks like classification, NLI, and NER through neural architecture search. We propose a modularized version of star-transformer and its variant, including a sparse transformer in the search space to reduce search cost and improve network generalization capability. Neural architecture search (NAS) is gaining attention for automatically generating new neural architectures, particularly in computer vision tasks. Neural architecture search (NAS) is widely used in computer vision tasks but less explored in natural language understanding (NLU). Recent works have focused on discovering new recurrent cells for language modeling tasks, achieving competitive results. NAS has also been applied to improve transformer architectures through evolution-based search algorithms. Our work contributes to improving transformer architectures by re-defining the search space and utilizing gradient-based NAS methods. Implemented on DARTS and P-DARTS, our approach consistently outperforms the vanilla transformer on machine translation tasks. The study explores the effectiveness of one-shot NAS in discovering new NN architectures for NLU tasks, focusing on advancements in sentence encoding like the transformer. Various self-attention mechanisms, including dynamic self-attention and DiSAN, have been proposed to address training and generalization challenges of the transformer. The study builds upon recent advancements in self-attention mechanisms like the star-transformer and capsule networks for text classification. It aims to explore the effectiveness of NAS in generating competitive architectures by including prominent attention-based encoders and aggregators in the search space. The study explores the effectiveness of NAS in generating competitive architectures by including attention-based encoders and aggregators in the search space. It analyzes meta architectures for NLU tasks and focuses on the encoding and aggregation layers of NLP models. The study investigates neural architecture search (NAS) for NLU tasks, focusing on encoding and aggregation layers in NLP models. It explores the use of attention-based encoders and aggregators in the search space for tasks like classification, natural language inference, and named entity recognition. The study focuses on neural architecture search (NAS) for NLU tasks, specifically exploring encoding and aggregation layers in NLP models for tasks like classification, natural language inference, and named entity recognition. The proposed approach divides the search space into encoder and aggregator subspaces, aiming to discover and evaluate new model architectures. Cross attentions between inputs are not considered, and the encoder and aggregator are shared by both inputs for the NLI task. Additionally, the use of a CRF layer after the encoder for NER tasks is not considered in this work. The search space is divided into encoder and aggregator subspaces, with the encoder space including zero map, identity map, conv1d, LSTM, GRU, and highway network. These models are commonly used in NLP tasks like classification, question-answering, and relation extraction. The search space includes encoder and aggregator subspaces with models commonly used in NLP tasks like classification, question-answering, and relation extraction. The architecture of Transformers has become popular in recent years due to the multi-head self-attention mechanism's ability to model long-distance contexts. The multi-head attention layer, without the usual residual connection, is included in the search space. Transformers have great expressiveness capability but can be difficult to train and prone to overfitting on small or medium datasets. The star-transformer module proposed by Guo et al. (2019) utilizes multi-head attention for each token in a sentence, leading to over-parametrization. Sparse variants like sparse transformers have been suggested to address this issue. A modified version of the star-transformer, with an enriched semantic representation of the relay node through multi-head attention, is proposed for better modularization. The reversed star-transformer algorithm is defined in Algorithm 3, with attention head number limited to 2. The encoder search space includes operations like null, identity, highway, sep conv1d, lstm, gru, and attention-based operations with 2 attention heads. Aggregation operations like max pooling and average are common. The curr_chunk discusses different aggregation operations used in the encoder search space, including max pooling, average pooling, self-attention pooling, and dynamic routing. The text also mentions the use of Differentiable Architecture Search (DARTS) for architecture search. The goal is to search for an encoder cell and an aggregator cell in a directed acyclic graph (DAG) of N nodes. The search space \u03a6 i,j connects nodes i to j with operations weighted by architecture parameters \u03b1 (i,j). The output of the cell is determined by a multiplier m. The NLU tasks are small or medium-sized, so stacking multiple encoder cells is not considered. A progressive search space is employed due to the large search space. The search for encoder and aggregator cells in a DAG with a large search space is conducted using a progressive reduction strategy. The process involves dropping operations with the lowest scores at intervals until the search space is reduced to specific orders. The final network architecture is derived by selecting the most likely aggregation operation. Two approaches are considered for deriving the discrete network architecture. The architecture involves retaining up to k strongest predecessors for each intermediate node, replacing mixed operations with the most likely operation, and dropping connections with null operations. Experiments are conducted on various tasks with benchmark datasets including text classification, NLI, and NER datasets. The curr_chunk discusses benchmark NER datasets such as SST-1, SST-2, SciTail, MedNLI, and CoNLL2003. Experimental protocols involve architecture search and evaluation stages to determine the best encoder and aggregator cells. In the architecture search stage, a pair of encoder and aggregator cells are derived from the performance on the validation set. The search space is gradually reduced by half every 3 epochs. Layer normalization is enabled in each node during architecture search to prevent gradient explosion. The number of nodes for the encoder cell ranges from 1 to 3, with 9G GPU memory required for N=3. Word embedding is initialized from pretrained Glove for both embedding and hidden size set at 300. The word embedding is initialized from pretrained Glove. The batch size is 32 for classification tasks and 16 for others. The learning rate is 1e-4 with weight decay set to 1e-5 and dropout rate at 0.5. Dropout is applied at different stages. The max number of epochs is 60. Adam is used for optimization. The search takes around 1.5 GPU day for SST-1 and 0.3 for SciTail tasks. Each search configuration is run twice with different random seeds. Results on SST-1 dataset show that DARTS generates a network architecture (DARTS-SST-1-V0) outperforming traditional NN models. The encoder cell of DARTS-SST-1-V0 consists of RNN and CNN operations, with a combination of different features that are difficult to manually design. The DARTS-SST-2-V0 architecture, incorporating a star-transformer operation and an identity map, achieved competitive results on the SST-2 dataset. Results were obtained using code from fastNLP and averaged over 10 runs. Transferability experiments showed that DARTS-SST-2-V0 performed worse on SST-1 compared to DARTS-SST-1-V0, which also showed competitive performance. Results on NLI tasks show that DARTS-SciTail-V0 performs best among architecture candidates derived from the search on SciTail, outperforming baseline models like ESIM and decomposable attention by a large margin. DARTS-SciTail-V0 achieves competitive performance on the test set, with a test accuracy of 79%, surpassing other models in the table such as HCRN, DeIsTe, and CAFE. Our model, DARTS-MedNLI-V0, outperforms star-transformer and transformer even after extensive parameter tuning. It does not use inter-sentence attentions before the prediction layer, outside resources, manually designed features, or extra training mechanisms like adversarial training. On the MedNLI dataset, DARTS-MedNLI-V0 resembles the original multi-head attention implementation in the transformer block but with a sep conv replacing the residual connection. It performs better than the original transformer, ESIM, and InferSent, but worse than the star-transformer. Transferability between tasks is explored, showing differences despite datasets being from different domains. The architecture searched for NER task CoNLL2003 does not include the CRF layer and does not use outside resources or manually designed features. The DARTS-CoNLL2003-V0 architecture performs slightly better than LSTM, with the star-transformer and GRU performing worse. This paper addresses NAS for NLU tasks, focusing on the encoder-aggregator architecture. Our study focuses on Neural Architecture Search (NAS) for Natural Language Understanding (NLU) tasks. We redefine the search space into encoder and aggregator spaces, based on the encoder-aggregator architecture of typical NN models for NLU. Using DARTS and P-DARTS, our NAS approach discovers architectures comparable to state-of-the-art models. Future work includes exploring one-shot architecture search for larger NLU tasks."
}