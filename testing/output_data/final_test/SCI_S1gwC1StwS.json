{
    "title": "S1gwC1StwS",
    "content": "We use gradient complexes to analyze neural networks loss surfaces. An algorithm calculates the barcodes of minima, showing they are in a small range of values and decrease with network depth. This affects learning and generalization in neural networks. The loss function graph is complex and non-convex. The structure of neural networks loss surfaces is complex, with obstacles like non-convexity, local minima, flat regions, and steep slopes. The optimization is based on gradient descent, and the Morse complex captures global topological characteristics of the gradient vector field trajectories. The \"canonical forms\" or barcodes summarize the topology of the gradient vector field flow by decomposing the change of topology into simple \"birth-death\" phenomena of different dimensions. The calculation of barcodes for different functions is essential in topological data analysis. Software packages like GUDHI, Dionysus, PHAT, and TDA are available for this purpose. These packages can handle calculations for functions defined on a grid of up to 10^6 points in dimensions two and three, but scalability remains an issue. A new algorithm is described for computing barcodes of functions in the lowest degree, working with functions defined on randomly sampled or specifically chosen point clouds. The algorithm described works with functions on point clouds, known to be better than grid-based methods in optimization. It reformulates the barcode definition in geometrical terms, with complexity O(n log(n)). The methodology developed describes the loss surface of neural networks using topological features of local minima, emphasizing the topological characteristics of the objective function. The paper discusses the application of a one-to-one correspondence between local minima and 1-saddles in exploring loss surfaces of neural networks. It defines the 1-saddle associated with each local minimum as the point where two connected components of the sublevel set merge, providing insights into optimization algorithms. The correspondence between local minima and 1-saddles in exploring loss surfaces of neural networks is one-to-one. The \"canonical form\" invariant attached to the minimum p is the segment [f (p), f (q)]. The barcode of minima is a robust topological invariant of the objective function, giving a concise summary of its topology and gradient flow structure. An algorithm for calculating these barcodes works with function values on randomly sampled or specifically chosen points, where local minima give birth to clusters of points. The algorithm analyzes neighbors of points to determine cluster membership, birth of new clusters, or merging of existing clusters. It has a complexity of O(n log(n)), where n is the number of points. Calculations on neural networks loss functions confirm observations on behavior. The algorithm calculates barcodes of minima for small neural networks, showing that as network depth increases, minima descend lower in the loss function's range. The approach and algorithms are not limited to optimization. Our algorithm allows for fast computation of persistence barcodes for various functions, with applications in cognitive science, cosmology, chemistry, and material science. The article outlines definitions of barcodes of minima and describes the algorithm for their calculation. The algorithm allows for fast computation of persistence barcodes for various functions, focusing on the \"birth\"-\"death\" phenomena of connected components of sublevel sets. This approach can be applied to explore the topology of loss surfaces and critical points in optimization algorithms. The \"canonical form\" invariants of minima are defined in terms of small indexes, which mark the terminal points of optimization algorithms in high dimensions. As the parameter c increases from f(p)- to f(p)+, new connected components of the sublevel set are born at minima. If a minimum is not global, its connected component merges with a lower minimum's component at a merging point q, creating an index-one saddle. The merging of connected components of sublevel sets at saddles is illustrated in Figure 1. Different colored saddles are associated with corresponding colored minima. The 1-saddles of this type are referred to as \"+\" or \"death\" type, representing the merging of connected components. The correspondence between local minima and 1-saddles is one-to-one. A 1-saddle q paired with a local minimum p can be described by considering paths starting from p and going to a lower minimum. The 1-saddle q is the minimum over all such paths of the maxima. The merging of connected components at 1-saddles is illustrated, where a new connected component is formed as c decreases. Given a 1-saddle q, the minimum p paired with q becomes the new minimum of f. The chain complex represents geometric objects as simple pieces through vector spaces and linear maps. It converts this decomposition into a collection of vector spaces and linear operators. The j-th homology of the chain complex is the quotient of the chain complex. A chain complex is R-filtered if it has an increasing sequence of subspaces defined by real numbers. Theorem 2.3 by Barannikov (1994) states that any R-filtered chain complex can be transformed into a canonical form, which is a direct sum of one-dimensional complexes with trivial differentials and two-dimensional complexes with trivial homology. The full barcode visualizes the decomposition of R-filtered complexes, where each 2-dimensional complex represents a topological feature \"born\" at one real number and \"dies\" at another. Each 1-dimensional complex with trivial differential describes a feature at a specific real number. The proof of Theorem 2.3 by Barannikov (1994) shows that an R-filtered complex can be transformed into a canonical form, consisting of 1-dimensional complexes with trivial differentials and 2-dimensional complexes with trivial homology. The gradient (Morse) complex associated with a smooth function f computes the homology of sublevel sets \u0398 f \u2264s. The gradient (Morse) complex, described by Barannikov (1994) and Le Peutrec et al. (2013), corresponds to critical points of a smooth function f. By perturbing f with a regularization term, non-degenerate critical points can be assumed. The complex's generators represent critical points, with the differential defined by gradient trajectories between them. The canonical form decomposes the gradient flow into simple pieces, where a minimum p, not a global minimum, represents a trivial homology class. The canonical form of the gradient complex represents critical points of a smooth function f. A minimum p is paired with an index-one saddle q, forming the canonical invariant corresponding to p. The barcode of the complex reveals the topological features of the objective function's sublevel sets. The description of the barcode of minima on a manifold with nonempty boundary is modified to account for connected components born at local minima on the boundary. The algorithm for calculating canonical form invariants of local minima involves building an approximation of the function's surface using a graph-based construction. This includes analyzing the local topology near points by comparing function values at neighboring points. In building an approximation of a function's surface using a graph-based construction, the algorithm focuses on understanding the local topology near points by comparing function values at neighboring points. Sampling points uniformly from a rectangular box, edges are added by computing the oriented k-Nearest Neighbor Graph and dropping edge orientation. The algorithm then computes barcodes of a function from this graph-based approximation, monitoring the evolution of connected components of sublevel sets. The algorithm focuses on monitoring the evolution of connected components of sublevel sets by sequentially adding vertices to the graph. Three possibilities for connected components to evolve include a new local minimum forming, neighbors belonging to one connected component, or neighbors belonging to multiple connected components. The algorithm focuses on computing barcodes of minima for a function on a graph. It identifies connected components and tracks their evolution as new local minima form or when neighbors belong to multiple components. The algorithm assumes a connected input graph and precomputes function values at all vertices for efficient processing. The algorithm computes barcodes of minima for a function on a graph by identifying connected components and tracking their evolution. It precomputes function values at all vertices and uses a disjoint set data structure to merge components. The complexity is O(N log N) and efficient HNSW Algorithm is used for nearest neighbor search. The algorithm is applied to describing surfaces of functions and analyzing loss surfaces of neural networks. In this subsection, the algorithm is applied to simple toy functions with different numbers of local minima. Function plots and corresponding barcodes of minima are analyzed. The section also explores barcodes of small fully connected neural networks with up to three hidden layers, referencing previous studies on loss surfaces and local minima. The best loss surfaces of neural networks should have stable slopes to prevent instabilities during training. The obtained minimum should be close to the global minimum to achieve the smallest training error. Additionally, the minima should provide small loss on the testing set for better generalization. Recent developments suggest that good local optima are flat, contrary to previous assumptions. Recent developments challenge the idea that good local optima in neural networks are flat, showing examples of sharp minima that generalize well. The choice of network architecture and loss function shapes the loss surface, impacting weight optimization. Analyzing small neural networks allows for a thorough exploration of the loss surface, emphasizing the importance of understanding the behavior of barcodes in simple examples. The study focuses on analyzing the behavior of barcodes in simple neural network examples with various architectures and loss functions. The objective function is the mean squared error with l2-regularization, and the error is computed for predictions on uniformly distributed inputs. The results show the barcodes of the loss functions on hyper-cubical sets, highlighting the methodology for analyzing loss surfaces of neural networks. The methodology involves computing barcodes to analyze the plots of functions, specifically loss surfaces of neural networks. By using a graph-based construction to approximate the function plot, the algorithm computes the barcodes of minima on the graph. Experimental results show that all barcodes are in a small lower part of the function's range, and as the depth of the neural network increases, the barcodes descend lower. This implies that gradient descent optimization is unlikely to get stuck in high local minima, and transitioning between local minima during learning is not difficult. The method has potential for further research directions beyond testing on small neural networks. The method involves computing barcodes to analyze loss surfaces of neural networks. It can be applied to large-scale modern neural networks for image processing tasks. Wise choice of representative graph vertices is crucial in high-dimensional spaces. Studying connections between barcodes of local minima and generalization properties of neural networks is another research direction. Investigating the relationship between barcodes of minima and the rate of convergence during learning is also suggested."
}