{
    "title": "r1fE3sAcYQ",
    "content": "We introduce *multi-model forgetting* in deep networks when training multiple models sequentially with shared parameters. To address this, we propose a weight plasticity loss that preserves important shared parameters from previous models. This approach improves performance in neural architecture search, natural language processing, and computer vision tasks. Recent work has focused on training models that can generalize across multiple tasks, with a major challenge being catastrophic forgetting. Several articles have addressed this issue by approximating the posterior probability with different datasets representing tasks. When dealing with many large models, a common strategy is to share a subset of weights across models and train them sequentially. The strategy of training multiple models sequentially with shared weights can lead to multi-model forgetting, where the accuracy of one model drops as another model overwrites its weights. This issue has not been extensively explored despite the benefits of weight-sharing in tasks like neural architecture search. To address this problem, an approach is introduced to overcome multi-model forgetting by formulating learning as the maximization of the posterior probability of shared and private weights. The Weight Plasticity Loss (WPL) minimizes multi-model forgetting by evaluating weight importance and updating shared weights inversely proportional to their importance. This approach extends to more than two models and is effective in reducing forgetting by 99% in training scenarios. Weight Plasticity Loss (WPL) reduces forgetting effect by 99% in fully converged model A and by 52% in loose convergence. Implementing WPL in neural architecture search with ENAS method shows a 51% reduction in forgetting on the most affected model and 95% on average. Results confirm better convergence in language modeling and image classification, with perplexity decreasing from 65.01 to 61.9 and top-1 error dropping from 4.87% to 3.81% with WPL. The method of Weight Plasticity Loss (WPL) reduces forgetting in neural architecture search. It also shows a significant reduction in multi-model forgetting. Various techniques are used to slow down learning of weights important for the initial task in sequential learning. In neural architecture search, methods like Weight Plasticity Loss reduce forgetting and address multi-model forgetting. Techniques are used to slow down learning of weights important for the initial task in sequential learning. In neural architecture search, various strategies like reinforcement learning and evolutionary techniques are used to optimize a population of models sharing internal structure. These methods aim to generate custom neural architectures by sharing weights between candidate solutions. Neuro-evolution strategies, such as BID17, use evolutionary algorithms to search for neural network architectures. Weight sharing between candidates has reduced computational costs. For example, BID26 defines weight inheritance for children to inherit parents' weights. Explicit weight sharing in RL-based techniques, like ENAS, has shown significant improvements. Neuro-evolution strategies like ENAS BID25 and DARTS BID18 use reinforcement learning and soft assignment to optimize network architectures. NAO BID19 replaces RL with a gradient-based auto-encoder. BID3 addresses training instability by randomly dropping network paths, highlighting a multi-model forgetting problem. Weight sharing has reduced computational costs but its downsides are not well-studied. In contrast to previous methods like BID3, which addressed training instability by randomly dropping network paths, we introduce a statistically-justified solution to the multi-model forgetting problem in training multiple models with shared parameters. Our framework prevents forgetting when training two architectures sequentially, extending to multiple models in scenarios like neural architecture search within ENAS BID25. Our statistically-motivated framework prevents multi-model forgetting during training by maximizing the posterior probability p(\u03b8 | D) to derive a new loss function. The framework ensures that the training of the second model does not degrade the performance of the first model, addressing the multi-model forgetting problem in scenarios like neural architecture search within ENAS. The integral in the denominator of the loss function is studied to update parameters of the second model without forgetting previous training. Lemma 2 provides an expression for the denominator based on maximum likelihood estimates and the negative Hessian of the log posterior probability distribution. This process can be challenging with deep networks. In practice, deep networks can be trained to convergence to obtain maximum likelihood estimates. Networks relying on positively homogeneous functions have critical points that are either global minimizers or saddle points. Training to convergence yields near-optimal solutions. To prevent multi-model forgetting, a loss function can be derived using cross-entropy and a Gaussian prior on the parameters. The text discusses the use of a loss function to prevent multi-model forgetting in deep networks, incorporating cross-entropy and a Gaussian prior on parameters. It mentions the intractability of the posterior probability and the Laplace approximation used to approximate it. The interaction between models is represented by a term not present in a standard single-model forgetting scenario. The text discusses approximating the posterior with a Gaussian distribution using the negative Hessian of the log posterior. The Fisher information matrix is used as an approximation due to the large parameter space. This allows for the computation of the log posterior without the need for second derivatives. The text discusses the Weight Plasticity Loss, derived from the Fisher information matrix in the diagonal approximation. The last term in the equation simplifies to H ss, leading to the final loss function for optimizing parameters \u03b8s. The Weight Plasticity Loss (WPL) in equation (8) encourages preserving important shared parameters from the first model while allowing others to change for improved accuracy in the second model. This differs from Elastic Weight Consolidation (EWC) by BID13, which addresses catastrophic forgetting in sequential learning on different tasks. EWC relies on Laplace approximations and the connection between the Fisher information matrix and second-order derivatives. The Weight Plasticity Loss (WPL) in equation (8) aims to maximize the posterior p(\u03b81, \u03b82, \u03b8s | D) for two models with shared parameters. It combines the original loss of the second model with a Fisher-weighted MSE term and an L2 regularizer. The interaction term v\u2126v in the WPL loss is specific to the multi-model case and is not present in Elastic Weight Consolidation (EWC) by BID13. The Weight Plasticity Loss (WPL) in equation (8) maximizes the posterior for two models with shared parameters. It includes a Fisher-weighted MSE term and an L2 regularizer. The interaction term v\u2126v in the WPL loss is specific to multi-model cases and differs from Elastic Weight Consolidation (EWC) by BID13. This statistically-motivated loss function can reduce multi-model forgetting and is applicable when training three or more models sequentially. Our approach, the Weight Plasticity Loss (WPL), maximizes the posterior for models with shared parameters. It can be used in neural architecture search within the ENAS strategy. Incorporating WPL in ENAS affects the training of sampled models with shared parameters. Despite not having access to the maximum likelihood estimate of previously-trained models, WPL remains effective in practice. Our Weight Plasticity Loss (WPL) maximizes the posterior for models with shared parameters in neural architecture search within the ENAS strategy. Parameters of new models are randomly initialized at the start of the search to avoid encouraging random parameters. Following the original ENAS training strategy for a set number of epochs, we then incorporate WPL and store optimal parameters after each architecture is trained. Fisher information is updated with virtually no computational overhead to ensure these updates use the contributions of previously-sampled architectures. Our Weight Plasticity Loss (WPL) maximizes the posterior for models with shared parameters in neural architecture search within the ENAS strategy. To ensure updates use contributions from all sampled architectures, a momentum-based update is used with a scheduled decay for \u03b1. The Fisher information is flushed to zero every three epochs for an accurate estimate. WPL is evaluated in training two models sequentially, both in strict convergence and suboptimal weight scenarios, within the ENAS framework using the MNIST dataset. In the strict convergence case, feed-forward networks with 4 (Model A) and 6 (Model B) layers share all layers of A. Model A is trained until convergence, obtaining a solution close to MLE \u03b8 A = (\u03b8 1 , \u03b8 s ). The Fisher information is computed using backward gradients of \u03b8 s on 200 images in the validation set. Model B is then initialized with \u03b8 s of Model A and trained with standard SGD. Training Model B with Weight Plasticity Loss (WPL) maintains the initial performance of A, with no loss of performance for B. In a more realistic loose convergence scenario, the influence of sub-optimal weights for Model A was evaluated by training it to different top 1 accuracies. The Weight Plasticity Loss (WPL) approach significantly reduces multi-model forgetting, with up to 99% reduction for a more converged model and 52% reduction even for the loose case. This suggests that WPL can effectively reduce forgetting in models A and B with shared parameters. The Weight Plasticity Loss (WPL) approach is effective in reducing multi-model forgetting, even when Model A is trained to sub-optimal accuracies. WPL can reduce forgetting by up to 99.99% for more converged models and up to 2% for less converged models. This method remains effective in real-world applications like neural architecture search. Neural architecture search using Weight Plasticity Loss (WPL) in the ENAS framework helps speed up the search process but may suffer from multimodel forgetting. Evaluating prediction errors of trained architectures shows that some architectures are forced to forget by others. Two experiments were conducted: RNN cell search on the PTB dataset and CNN micro-cell search on the CIFAR10 dataset. Mean error differences were calculated for sampled architectures, with a focus on the 5 architectures with the lowest initial error. The study conducted experiments on RNN and CNN architectures, showing that without Weight Plasticity Loss (WPL), there is a significant multi-model forgetting effect during training. WPL reduces this effect, leading to more reliable weight importance information as training progresses. The study demonstrated that using Weight Plasticity Loss (WPL) during training helps prevent the forgetting of good models, leading to higher rewards in the second half of training. This approach allows for better maintenance of models until the end of training. The study showed that Weight Plasticity Loss (WPL) reduces multi-model forgetting during neural architecture search. WPL can decrease forgetting by up to 95% for all models, 59% for the 5 models with lowest initial error, and 51% for the maximum difference. Additionally, WPL initially leads to lower rewards but allows for sampling better architectures later in training, resulting in higher rewards. The study found that Weight Plasticity Loss (WPL) reduces multi-model forgetting during neural architecture search. WPL improves final model accuracy, outperforming the original ENAS paper without extensive hyperparameter tuning. The results demonstrate the benefits of reducing multi-model forgetting in training multiple models sequentially. The study introduces Weight Plasticity Loss (WPL) to address performance degradation in multi-model training due to shared weights being overwritten. WPL effectively reduces multi-model forgetting during neural architecture search, leading to improved results in natural language processing and computer vision tasks. Future work includes integrating WPL in other neural architecture search strategies and exploring its use in ensemble learning contexts. Weight Plasticity Loss (WPL) is introduced to combat performance degradation in multi-model training by reducing forgetting during neural architecture search. WPL shows lower error differences and speeds up training, especially in the context of neural architecture optimization (NAO) method. The approach is general and not limited to ENAS, demonstrating effectiveness in preventing multi-model forgetting. In the context of neural architecture optimization (NAO), Weight Plasticity Loss (WPL) is used to reduce forgetting during multi-model training. WPL shows lower error differences and accelerates training, especially when combined with output dropout. The dropout rate affects the level of forgetting, but WPL consistently reduces it, even with aggressive dropping policies. Our Weight Plasticity Loss (WPL) method reduces multi-model forgetting in neural architecture optimization. Compared to Nao + path dropping, WPL allows models to learn while minimizing forgetting. The approach generalizes beyond ENAS for neural architecture search, as shown in error reductions of up to 99% for CNN architectures."
}