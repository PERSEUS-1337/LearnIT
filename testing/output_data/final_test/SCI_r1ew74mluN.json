{
    "title": "r1ew74mluN",
    "content": "An adversarial feature learning (AFL) framework aims to learn representations invariant to a nuisance attribute through an adversarial game. However, AFL often leads to unstable behavior and slow convergence. An alternative approach called \"attribute perception matching\" is proposed, based on pair-wise distribution matching. This method requires the same number of parameters as AFL but shows better convergence properties. Experiments demonstrate that the proposed method converges to better invariant representations faster. The proposed method aims to learn invariant representations faster than AFL by reducing information about a while maintaining information about y. Invariant representation learning involves an adversarial game between a feature extractor and an attribute classifier. The key of AFL is to measure invariance by leveraging the discriminative power of neural networks. The AFL method leverages neural networks to go beyond predefined metrics like l2 distance or maximum mean discrepancy. It aims to maximize conditional entropy for superior performance in fair classification, privacy protection, and domain generalization tasks. The AFL method aims to maximize conditional entropy for superior performance in fair classification, privacy protection, and domain generalization tasks. However, the min-max formulation used in the approach raises practical issues such as vanishing gradients and instability, as shown in FIG1. The Generative Adversarial Networks community avoids similar problems by incorporating alternative objectives like the Wasserstein distance, but applying it to multiple distributions is not straightforward. The paper discusses practical issues with the AFL method in invariant feature learning, proposing a reformulation using pair-wise distribution matching to address convergence slowdown. However, computational feasibility remains a challenge due to the high computational requirements of calculating Wasserstein distance between distributions. The paper addresses convergence issues of AFL optimization in invariant feature learning by proposing a reformulation using pair-wise distribution matching. The method's superior performance is empirically validated on artificial and real-world datasets. Visualizations show the behavior of AFL optimization on synthesized data with samples from three Gaussian distributions. The paper proposes a reformulation of AFL optimization using pair-wise distribution matching to address convergence issues. Empirical validation on artificial and real-world datasets demonstrates superior performance. Visualizations illustrate AFL optimization behavior on synthesized data from three Gaussian distributions, revealing slow distribution alignment and unstable behavior. The discriminator struggles to distinguish behaviors in AFL optimization, leading to unstable performance. Despite some alignment in distributions initially, catastrophic failure occurs due to the discriminator's inability to capture true conditional entropy. AFL's loss function aims to pull distributions away from undesired points but fails to ensure they approach desired invariant points, resulting in instability. The gap between NLL of D and D eval highlights this issue. The maximum conditional entropy H(A|Z) is \u2212 log 1 K, and is maximized if p(z|a=i) = p(z|a=j) for all i = j \u2208 1, \u00b7 \u00b7 \u00b7 , K and z \u2208 Z. This implies that empirical measurements of conditional entropy should be bounded, posing a problematic point for AFL. The theorem allows us to view conditional entropy maximization as aligning pairs of distributions. A computationally efficient method for pairwise distribution matching, called APM, is proposed. APM involves alternating training of attribute classifier D and feature extractor E. The APM matching objective is defined using a distance function over hidden representations. The proposed method, APM, maximizes conditional entropy by aligning pairs of distributions using a distance function over hidden representations. It ensures attribute representations approach each other and leverages discriminative power for invariance measurement. The method outperforms AFL in maximizing conditional entropy significantly faster. The proposed method, APM, maximizes conditional entropy significantly faster than AFL. It enforces semantic alignment with a simple modification to prevent performance degradation in predicting y. This modification does not help maximize conditional entropy but is crucial for applications requiring information about y. The method is used in three domain generalization tasks and two user-anonymization tasks. The study involves generalization tasks and user-anonymization tasks using various datasets. Different methods like AFL, FORMULA4 RevGrad, CrossGrad, and Activation Matching are used as baselines for training neural networks to prevent access to user information. The study involves generalization tasks and user-anonymization tasks using various datasets. Different methods like AFL, FORMULA4 RevGrad, CrossGrad, and Activation Matching are used as baselines for training neural networks to prevent access to user information. For all datasets and methods, RMSprop was used for optimization, with specific learning rates and batch sizes set accordingly. Hyperparameters were tuned on a validation set for each baseline, and parameters like weighting parameter \u03bb and decay rate \u03b3 were optimized for different experiments. Test sets were selected from one or several domains, with training/validation data from a disjoint domain. The study used a disjoint domain for training/validation data, splitting it into 80% and 20%. Invariance was measured using a post-hoc classifier, showing that the proposed method achieved better invariant representations compared to other methods, except for the speech dataset. The results indicate stable performance with nearly perfect invariance in some cases. The study compared the performance of different methods on various datasets, showing that APM outperformed other methods including CrossGrad. The Wilcoxon rank sum test confirmed APM's statistical superiority over CNN, RevGrad, and AFL with p < 0.01. The study demonstrates that APM outperforms CNN, RevGrad, and AFL with statistical significance (p < 0.01), as well as CrossGrad (p < 0.05). The proposed method addresses issues with current state-of-the-art AFL, showing stable performance in learning invariant features and superior domain generalization capabilities. The proof of Theorem 1 utilizes the Lagrange multiplier method to maximize conditional entropy, resulting in equal probabilities for all classes given a certain input."
}