{
    "title": "rkfbLilAb",
    "content": "We develop a reinforcement learning based search assistant for subjective search tasks, using a stochastic virtual user to efficiently train the agent. Our A3C algorithm based architecture enables contextual assistance to users, outperforming Q-learning in average rewards and state values evaluation with the virtual user. The experiments show that the agent improves rewards and state values with the virtual user in validation episodes. Recent advances in search focus on personalizing results, but the search interface lacks innovation in incorporating contextual cues. Conversational interactions can enhance the search process by collecting more contextual cues from the user. The search engine can interact with users to gather contextual cues, enhancing the search experience. A Reinforcement Learning based conversational search agent assists users in finding relevant results by providing contextual assistance. Unlike other dialogue agents, this agent allows for subjective discussions and helps users narrow down their search effectively. The search process can be modeled as a slot filling exercise or a sequence of interactions between the user and the RL agent. User preferences are dynamic, leading to a large search space. The RL agent receives intermediate rewards based on user behavior to improve search results. In the RL framework, intermediate rewards are given to the agent based on its actions and state in conversational search. Query and session log data are used to create a virtual user environment for training the agent. The agent interacts with the user to understand intent and treats the search engine as a black box service. Qualitative experiments are conducted to evaluate the trained agent's performance using different reinforcement learning algorithms. The study focuses on developing a stochastic user model for efficiently sampling user actions in a conversational search environment. A3C algorithm is used to predict policy and state value functions of the RL agent, compared with other algorithms for performance evaluation. Various attempts have been made in modeling conversational agents, including goal-driven RL agents for indoor navigation and visual input agents for object search in a 3-D environment. Domain-independent RL-based dialogue systems have also been explored in the past. In our formulation, we provide rewards for task success, extrinsic, and auxiliary rewards at intermediate steps. Other RL-based information seeking agents extract information by asking questions sequentially, but these are not designed for search tasks involving human interaction. The Neural Conversation Model uses an end-to-end encoder-decoder architecture to generate grammatically correct sentences, although they may lack engagement and consistency. The Neural Conversation Model combined with deep RL aims to improve sustained conversations by using rewards to enhance coherence and engagement. This model, initialized with MLE parameters, is further tuned using policy gradient to capture ease of answering and semantic coherence. Additionally, batch policy gradient method is used to improve models with labeled data. However, labeled conversational data is not available for subjective search tasks, so our model assists users through a set of assist actions instead. Our agent engages the user directly in the search process, involving a sequence of alternate turns between user and agent with more freedom in actions. Simulated speech outputs are used to train agents in spoken dialogue systems, reducing the need for human intervention. This approach relies on word-based features from raw text dialogues to represent the state of the Reinforcement Learning Agent, minimizing dependence on hand-engineered features. User models for simulating user responses have been obtained using LSTM to reduce dependence on hand-engineered features. Memory Networks store information internally for later use in tasks like question answering. RL is used in visual tasks where the agent is trained to answer questions about images. Visual Dialog tasks are different from machine comprehension based question answering tasks. Visual Dialog tasks require the model to maintain context in conversations for formulating replies. Responses are not grounded in the given image and need knowledge beyond the immediate context. Evaluation is complex for subjective search systems due to the absence of labels. The system is evaluated through rewards obtained while interacting with the user model and real-world metrics through human evaluation. Two different RL algorithms are experimented with in this paper. Reinforcement Learning (RL) involves training an agent to operate in an environment by interacting in episodes with turns where the agent observes the state and performs actions to receive rewards. Two RL algorithms, A3C and Q-learning, are experimented with in this paper. The agent's optimal policy aims to maximize cumulative rewards by taking actions in the action space A to assist the user in searching for assets. The agent uses probe intent actions to query the user for more context and may prompt the user to refine queries or choose from clustered search results. The agent clusters search results, prompts user to choose categories, displays results, suggests adding to cart or bookmarking, asks for feedback, provides help, and greets/goodbyes. The set G consists of generic actions like displaying assets, providing help, and prompting user actions for business use cases. The agent learns to perform different actions at appropriate time steps in search episodes, modeling the state representation to encapsulate facets of search and conversation. The variables history user and history agent capture the context of the conversation by representing user and agent actions in the last k turns. Each user and agent action is encoded as a one-hot vector. Zero padding is used when the history is less than 10 turns. Score results quantify similarity between the most recent query and the top 10 results. Reinforcement Learning is used to train an agent to maximize cumulative reward by balancing long term and short term rewards. This is crucial in conversational search where the exploration-exploitation problem is more pronounced. For example, when a user searches for \"nature\", the optimal action for the agent needs to be determined. In conversational search, determining the most optimal action for the agent is crucial for maximizing rewards. Reward modeling is essential for optimizing dialogue strategy based on user feedback. PARADISE framework metrics may not be applicable for evaluating the system's performance in this context. Reward modeling is crucial for optimizing dialogue strategy in conversational search. The PARADISE framework metrics are not suitable for evaluating the system's performance. The total reward in a dialogue consists of task completion, instantaneous extrinsic rewards based on user responses, and interaction rewards. In conversational search, rewards are provided based on user actions categorized as good, average, or bad feedback. Extrinsic rewards are given at each time step, along with auxiliary tasks for additional reward signals. The PARADISE framework metrics are not suitable for evaluating system performance in optimizing dialogue strategy. Our methodology involves training an RL agent to improve performance on various auxiliary tasks in conversational search. To address the lack of actual conversational search data, we propose using a user model to simulate user behavior during training and validation. This model can be applied to any query and log sessions data to create a virtual user that interacts with the agent. The virtual human user is modeled using query sessions data from a stock photography and digital asset marketplace. This data is used to generate a user that simulates human behavior during search episodes. The virtual user is modeled as a finite state machine by extracting conditional information from query and session log data. The virtual user is modeled as a finite state machine using conditional probabilities extracted from query and session log data. This data is from an asset search platform where marketers can define offers based on user actions, such as prompting users to add images to their cart. The virtual user's behavior is modeled using conditional probabilities from query and session logs on an asset search platform. To train the agent, Q-learning is used with a Q-function mapping state-action pairs to expected rewards. The agent's actions are adjusted realistically to cover the full probability space of user behavior. The Q-values for state-action pairs are updated using a learning rate and immediate rewards. An exploration policy is employed to prevent always choosing the best action. Q-learning uses a table storage method to update values at each step in a training episode. The policy defines a probability distribution over actions the agent may take in a given state. The proposed neural architecture utilizes a LSTM BID9 to process state information and predict action probabilities and state values. The architecture aims to approximate the policy and value functions for conversational search. The LSTM model uses parameters \u03b8 p and \u03b8 v for policy and value outputs respectively. The LSTM state is reset at the start of each search episode. It remembers previous states to predict actions based on observed transitions and user behavior, mimicking a real agent's strategy. Parameters are optimized by minimizing the total loss function. The A3C architecture for predicting policy and value in reinforcement learning involves optimizing the loss function by updating network parameters after every n-step roll-out episode. The LSTM model retains cell and hidden states to process search states and make predictions at different time steps within an episode. The A3C architecture optimizes the loss function by updating network parameters after every n-step roll-out episode. The n-step roll-out allows estimation of target values using actual rewards and the value of the last observed state. The network is trained on the value loss function and policy loss function to shift the policy in favor of actions that provide better advantage given the state. The A3C architecture optimizes the loss function by updating network parameters after every n-step roll-out episode. To prevent bias towards certain actions, entropy loss is added to the total loss function. The total loss function balances exploitation and exploration through policy and entropy loss functions optimization. The value function V \u03c0 (s) is used for determining the value of a state, and Adam optimizer is used for optimizing the loss function on model parameters \u03b8. A3C includes a global model to improve exploration capacity of the final agent trained. The A3C architecture includes a global model with multiple asynchronous agents for faster convergence. The agents update the global model parameters using local gradients of the loss function. LSTM unit aggregates local context into a global context to capture user behavior influenced by past queries. The trained agent is evaluated with the virtual user model using A3C and Q-learning reinforcement learning techniques. Validation episodes are simulated after each training episode to plot average rewards and mean state values. A chat-search interface allows real users to interact with the agent, with conversations provided in the appendix. The global model is created from 10 local agents trained in parallel threads. The global model is compared for different state representations and hyper-parameter settings such as discount factor and LSTM size. Three values of discount factor are experimented with, while the LSTM size is fixed at 250. A larger discount factor results in lower weights for future rewards, leading to the agent maximizing immediate rewards by taking greedy actions. Variance in results is computed to validate this, with values before 100 episodes not considered due to under-fitting. The variance values for different discount factors (\u03b3 = 0.90, 0.70, 0.60) are 1.5267, 1.627, and 1.725 respectively. Higher discount factors lead to more greedy actions and increased variance in rewards. A lower \u03b3 value results in higher reward variance, indicating a greedy policy. The discount factor of 0.90 achieves a better exploration-exploitation balance. LSTM sizes of 100, 150, and 250 are tested to determine the impact on context preservation. In the experiment, different LSTM sizes (100, 150, 250) were tested to see the impact on context preservation. Larger LSTM sizes result in better states observed by the agent, indicating improved performance in actions. Adding history of actions to the state vector is not necessary for a large LSTM size (250) as it can preserve context effectively. The LSTM with different sizes (100, 150, 250) was tested to assess context preservation. Including local context in state representation proved useful for aggregating into global context. Optimal hyper-parameter values for Q-learning were determined as \u03b3=0.70 and exploration control parameter=0.90. A comparison between A3C agent (LSTM size 250, \u03b3=0.90) and Q-learning agent showed A3C agent achieving better results. The A3C agent outperforms the Q-agent in obtaining higher averaged awards in validation episodes. The system trained using A3C algorithm was evaluated by professional designers who rated the conversational search system on various metrics. The A3C agent outperforms the Q-agent in obtaining higher averaged awards in validation episodes. The system trained using A3C algorithm was evaluated by professional designers who rated the conversational search system based on metrics like information flow, appropriateness of actions, repetitiveness, and engagement compared to conventional search interfaces. The designers rated the conversational search system on engagement, time required for search, and ease of use compared to conventional search. The system achieved an average rating of 2.67 for engagement. About 33.3% of designers said it required more time, 16.7% said it was faster, and 50% said it took about the same time. The designers evaluated the conversational search system on engagement, time required for search, and ease of use compared to conventional search. Results showed that 33.3% found conversational search easier, 41.7% said it was the same, and 25% found it difficult. The system performed well with actual users, driving their search forward without being repetitive. Conversational search was found to be more engaging and resulted in varied search times for designers. In this paper, a Reinforcement Learning based search assistant was developed to help designers search for digital assets. The system was found to be easier to use by 33.3% of designers, with the majority not experiencing cognitive load. The trained agent achieved higher rewards in validation episodes, indicating a better search experience. Additionally, a virtual stochastic user model was proposed for interaction. The system proposed a virtual stochastic user model to train the RL agent without labeled conversational data, accelerating the process of obtaining a bootstrapped agent. Future work includes deploying the system to collect conversational data for model fine-tuning and designing a system to automate state space characterization. The chat interface features a two-pane window for text dialogues and search results. The chat interface allows users to input queries in dialogue form, parsed by NLU to determine user actions and redirect to search engine. The RL agent then generates a response based on learned policies. Users can also perform actions like liking search results. NLU uses rule-based methods to understand queries and user actions. The chat interface allows users to input queries in dialogue form, parsed by NLU to determine user actions and redirect to search engine. A distinction is made between user actions new query and refine query based on the presence of the current primary keyword in previous queries. A database stores user queries to provide context for formulating search queries. Rules and stop words are used to determine different user actions, such as providing feedback signals, requesting more results, clicking on categorical options, or searching similar results. The search engine retrieves assets from an asset database based on the formulated search query. The search engine retrieves assets from a database of 5 million assets using tags and metadata for search queries. It clusters similar assets and calculates relevance scores based on metadata matching. The chat interface allows users to input queries and browse through categorized options for images. The chat interface allows users to input queries and browse through categorized options for images. The user requests urban city cars, adds assets to cart, refines query for racing images, and organizes a racing competition. The agent provides matching images, user adds more to cart, and ends the conversation. The user interacts with the agent to search for images, adds assets to cart, and receives assistance in finding specific categories like \"Internet golden CPU\" and \"Windows.\" The agent offers to help further, but the user declines, thanking for the assistance. The user interacts with the agent to search for images, adds assets to cart, and receives assistance in finding specific categories like \"Internet golden CPU\" and \"Windows.\" The agent offers to help further, but the user declines, thanking for the assistance. The user then refines their search query for images of mountains for organizing adventure sports. The agent offers to show images in that category and asks where the user will use the images. The user interacts with the agent to search for images of mountains for organizing adventure sports. The agent offers to show images in the hiking sports mountains category and asks for the user's email to sign up."
}