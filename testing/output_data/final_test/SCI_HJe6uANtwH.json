{
    "title": "HJe6uANtwH",
    "content": "A new routing algorithm for capsule networks is introduced, where a child capsule is routed to a parent based on agreement between their states. This approach simplifies routing and improves performance on CIFAR-10 and CIFAR-100 datasets. The mechanism includes inverted dot-product attention, Layer Normalization, and concurrent iterative routing. The model outperforms existing capsule networks and matches a ResNet-18 CNN with fewer parameters. In a digit recognition task, the capsule model performs well against CNNs with the same architecture. Capsule Networks (CapsNets) represent visual features using groups of neurons called \"capsules.\" Each capsule encodes a feature and represents one visual entity, allowing for interpretable hierarchical parsing of a visual scene. Capsule routing ensures that one part cannot belong to multiple parents, making it easier to incorporate priors. This hierarchical relationship modeling has sparked interest in designing capsules and their routing, showing potential for complex real-world tasks. Relationship modeling in Capsule Networks involves designing capsules and their routing algorithms. Lower-level capsules vote for the state of higher-level capsules, which aggregate the votes, update their state, and explain the lower-level capsules. This iterative process determines routing probabilities, similar to the Expectation-Maximization algorithm. Dynamic Routing and EM-routing are variants of this scheme. In Capsule Networks, different routing algorithms like Dynamic Routing and EM-routing determine how lower-level capsules vote for higher-level capsules. A novel routing algorithm, Inverted Dot-Product Attention Routing, uses dot products to measure agreement between parent and child units. This approach differs from traditional attention models by having higher-level units compete for the attention of lower-level units. The proposed modifications in the routing procedure for Capsule Networks include using Layer Normalization and joint inference of latent capsule states across multiple layers. These changes improve model scalability on challenging datasets like CIFAR-10 and CIFAR-100. Additionally, a new DiverseMultiMNIST dataset is introduced for recognizing single and overlapping objects. The CapsNet model shows better convergence compared to a baseline CNN, showcasing the effectiveness of the routing mechanism. The proposed Capsule Network architecture includes a routing mechanism that makes it practical for real-world tasks. The network consists of feedforward convolutional layers, primary capsules, convolutional capsule layers, fully-connected capsule layers, and class-specific capsules for classification. Source code for reproducing experiments will be released. The Capsule Network architecture includes a routing mechanism for inference, with capsules in different layers represented by matrices. Activation probability is not explicitly represented, unlike in prior works. The Capsule Network architecture includes a routing mechanism for inference, with capsules in different layers represented by matrices. Activation probability is not explicitly represented. The proposed routing process involves computing agreement between lower-level and higher-level capsules, followed by updating the pose of higher-level capsules using the Inverted Dot-product Attention Routing algorithm. The Capsule Network architecture includes a routing mechanism for inference, with capsules in different layers represented by matrices. Activation probability is not explicitly represented. The proposed routing process involves computing agreement between lower-level and higher-level capsules, followed by updating the pose of higher-level capsules using the Inverted Dot-product Attention Routing algorithm. L ij is an inverted attention score representing how higher-level capsules compete for attention of lower-level capsules. Using the routing probabilities, we update the pose p L+1 j for capsule j in layer L + 1 from all capsules in layer L. We adopt Layer Normalization as the normalization, which we find to improve convergence for routing. The routing algorithm is summarized in Procedure 1 and Figure 2. The goal is to obtain a backbone feature F from the input image I. The backbone model can vary for different experiments. The Capsule Network architecture includes a routing mechanism for inference, with capsules in different layers represented by matrices. The goal is to obtain a backbone feature F from the input image I. The backbone model can be a single convolutional layer or ResNet computational blocks. The primary capsules P1 are computed by applying a convolution layer and Layer Normalization to the backbone feature F. Non-primary capsules layers P2:N are initialized to zeros. Routing involves sequential and concurrent iterations to update capsule layers. The Capsule Network architecture includes a routing mechanism for inference, with capsules in different layers represented by matrices. The routing iterations are performed concurrently, with capsules looking at their preceding lower-level capsule layer simultaneously. This weight-tied concurrent routing architecture with Layer Normalization aims to converge to fixed points, unlike previous CapsNets that used sequential layer-wise iterative routing. The Capsule Network architecture includes a concurrent routing mechanism for inference, aiming to prevent vanishing gradients and improve stability during training. The goal is to obtain predicted class logits from the last capsule layer using a shared linear classifier. Parameters are updated by stochastic gradient descent, and multiclass cross-entropy loss is used for classification. The Capsule Network uses multiclass cross-entropy loss for classification. Margin loss and Spread loss were also tested but did not perform as well. The concurrent routing is a parallel-in-time routing procedure for all capsule layers. CIFAR-10 and CIFAR-100 datasets consist of small real-world color images with 10 and 100 classes respectively. Comparisons with other CapsNets and CNNs show the test accuracy of the model. In comparing CapsNets and CNNs, different routing approaches were evaluated. CapsNets outperformed Dynamic Routing CapsNets, which in turn outperformed EM Routing CapsNets. Performance varied more on CIFAR-100 than CIFAR-10. CapsNets with ResNet backbone showed improved performance compared to those with a simple convolutional backbone. The ResNet backbone outperforms a single convolutional layer in CapsNets. CapsNets struggle to match CNN performance with a simple backbone but achieve competitive results with a ResNet backbone. Convergence analysis shows optimization for CapsNets with different routing mechanisms. The performance of different routing mechanisms in CapsNets is analyzed. Dynamic Routing CapsNets show a mild performance drop with increased iterations, while EM Routing CapsNets perform best with 2 iterations. The proposed routing mechanism shows a positive correlation between performance and iterations, with the smallest variance. However, selecting a larger iteration number may increase memory usage and inference time. Performance jitters are observed during training when the model has not converged, especially with a high number of iterations. The ablation study examines the impact of different routing approaches on performance in CapsNets. Removing Layer Normalization leads to a significant drop in performance, highlighting its importance for stability. Replacing concurrent with sequential iterative routing disrupts the positive correlation between performance and iteration number. This finding is consistent with Dynamic Routing CapsNets, which also use sequential iterative routing. In the comparison between CapsNets and CNNs with the same number of layers and neurons, the focus is on the representation power difference between routing in CapsNets and pooling in CNNs. The challenge lies in handling overlapping objects and varying object numbers in images, leading to the creation of the DiverseMultiMNIST dataset. The study also explores the impact of adding activations to capsule design, noting a performance deterioration possibly due to difficulties in gradient flow. Future work aims to find the best strategy for incorporating activations in capsule networks. The DiverseMultiMNIST dataset, an extension of MNIST, includes single-digit and overlapping digit images for multilabel classification. Results show improved performance of CapsNet over CNN, with CapsNet achieving better test accuracy. In comparison to CNN, CapsNet demonstrates superior test accuracy, with CapsNet achieving 85.74% accuracy and CNN achieving 79.81%. CapsNet's routing mechanism outperforms CNN's pooling operation, although the former requires additional parameters. To address this, the baseline CNN replaces pooling with a fully-connected operation, increasing the number of parameters to 42.49M. The CapsNet outperforms CNN in test accuracy, with CapsNet achieving 85.74% compared to CNN's 79.81%. The routing mechanism in CapsNet shows better representation power than pooling when recognizing diverse objects. CapsNet with vector-structured poses performs better than matrix-structured poses, but requires more parameters, memory usage, and inference time. Increasing parameters in matrix-pose CapsNet to 42M boosts test accuracy to 91.17%, but at the expense of higher memory usage and inference time. The idea of grouping neurons into capsules was first proposed in Transforming AutoEncoders (Hinton et al., 2011) to represent multi-scale recognized fragments of input images. Sabour et al. (2017) extended this concept to learn part-whole relationships in images systematically. Hinton et al. (2018) introduced a routing mechanism fitting a mixture of Gaussians for object recognition from novel viewpoints. Stacked Capsule AutoEncoders achieved state-of-the-art results on unsupervised classification by segmenting and composing image fragments without supervision. Our proposed new routing mechanism for capsule networks aims to handle more complex data by allowing lower-level units to \"attend\" to parents, enforcing exclusivity among routing to parents, and not imposing limits on the number of lower-level units that can be routed to any parent. This model combines the ease and parallelism of dot-product routing from Transformers with the interpretability of building a hierarchical parsing of a scene. The proposed new routing mechanism for capsule networks combines the ease and parallelism of dot-product routing from Transformers with the interpretability of building a hierarchical parsing of a scene. Various works have presented different routing mechanisms for capsules, including Dynamic routing formulated as an optimization problem, generalized routing methods, approximation of routing processes, and the integration of capsules layers with SOTA CNN backbones. DeepCaps, proposed by Rajasegaran et al. (2019), achieved 92.74% test accuracy on CIFAR-10 by stacking 10+ capsules layers. In contrast to prior work on capsule networks, a novel Inverted Dot-Product Attention Routing algorithm is proposed in this study. This method determines routing probability based on agreements between parent and child capsules, achieving 95.14% test accuracy for CIFAR-10 and 78.02% for CIFAR-100. This approach demonstrates comparable performance against state-of-the-art CNNs. The study introduces a new Inverted Dot-Product Attention Routing algorithm for capsule networks, achieving competitive performance on CIFAR-10 and CIFAR-100. The approach removes the constraint of explaining child capsules by parent capsules, allowing for efficient use of parameters. Future directions include combining capsule layers with state-of-the-art CNN backbones for scaling up to larger datasets like ImageNet, and exploring the potential of infinite-iteration routing through concurrent routing processes. The configuration choices for Dynamic Routing CapsNets and EM Routing CapsNets are based on prior work. Model specifications are listed for CapsNets with a simple convolutional backbone, with modifications for a ResNet backbone. The ResNet backbone is used with an input dimension of 128, consisting of convolutional and residual building blocks. Different optimizers and learning rates are used for various methods. Learning rate is decreased at specific epochs during training. Data augmentation is consistent across all networks. During training, images are padded and cropped to 32x32 size, with horizontal flipping. Evaluation does not include data augmentation. Models are trained on an 8-GPU machine with batch size 128. The architecture includes layers with specific neuron counts leading to 10 class logits. Optimizers are fixed with stochastic gradient descent and learning rate decay. Diverse MultiMNIST dataset includes single-digit and overlapping-digit images generated on the fly. The DiverseMultiMNIST dataset contains single-digit and overlapping-digit images generated on the fly. Single-digit images are created by shifting digits in the MNIST dataset, while overlapping-digit images are generated by overlaying two digits from the same MNIST dataset. The probabilities of generating single-digit and overlapping-digit images are 1/6 and 5/6 respectively. Training models are evaluated on 21M generated images, with 10,000 test images for each evaluation step. Training and test images are ensured to be from disjoint sets. The DiverseMultiMNIST dataset includes single-digit and overlapping-digit images generated by shifting or overlaying two digits from the MNIST dataset. The generated images are 36x36 in size, with no data augmentation used for training or evaluation. Models are trained on an 8-GPU machine with a batch size of 128, using specific convolutional and capsule network configurations."
}