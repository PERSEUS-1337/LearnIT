{
    "title": "B1l8BtlCb",
    "content": "Existing approaches to neural machine translation condition each output word on previously generated outputs. A new model is introduced that avoids this autoregressive property, producing outputs in parallel for lower latency during inference. Through knowledge distillation, input token fertilities as a latent variable, and policy gradient fine-tuning, the model achieves near-state-of-the-art performance on various language pairs. The text discusses how network-based models outperform traditional statistical models for machine translation. State-of-the-art neural models are slower at inference time due to autoregressive decoders. A non-autoregressive translation model based on the Transformer network is introduced to address this issue. The text introduces a non-autoregressive translation model based on the Transformer network BID14, which modifies the encoder to predict fertilities. These fertilities provide a globally consistent plan for the decoder during inference, improving the distribution over possible output sentences. The text discusses autoregressive neural machine translation models using an encoder-decoder architecture with a unidirectional RNN-based decoder for maximum likelihood training. It also explores non-autoregressive NMT without RNNs, where later conditional probabilities are independent of earlier decoding steps during training. The autoregressive factorization in NMT models is being replaced with masked convolution layers or self-attention computations in the decoder to allow information flow across long distances in a constant number of operations. Autoregressive decoding captures the word-by-word nature of human language production and achieves state-of-the-art performance. Non-autoregressive decoding aims to overcome limitations of autoregressive models and beam search in translation tasks. Removing the autoregressive connection from an encoder-decoder model allows for parallel processing and improved efficiency during inference. This approach involves modeling the target sequence length with a separate conditional distribution. Non-autoregressive decoding involves modeling the target sequence length with a separate conditional distribution, allowing for parallel processing during inference. However, this approach may result in poor results due to complete conditional independence, where each token's distribution depends only on the source sentence. The \"multimodality problem\" in neural machine translation (NMT) arises from the inability of models to capture the diverse target translation possibilities. This issue is addressed by introducing a modified model and new training techniques to handle the highly multimodal distribution of target translations. The Non-Autoregressive Transformer (NAT) is a novel NMT model that can produce an entire output translation in parallel. It consists of an encoder stack, a decoder stack, a fertility predictor, and a translation predictor for token decoding. The model allows for non-autoregressive decoding by using feed-forward networks and multi-head attention modules in both the encoder and decoder stacks. The encoder remains unchanged from the original Transformer network, while the decoder stack is modified to enable non-autoregressive and parallelized decoding. The Non-Autoregressive Transformer (NAT) model consists of an encoder stack, a decoder stack, a fertility predictor, and a translation predictor for token decoding. To generate all words in parallel, the NAT initializes the decoding process using copied source inputs from the encoder side. Two methods are proposed to handle different lengths of source and target sentences: copying source inputs uniformly or using fertilities. The Non-Autoregressive Transformer (NAT) model uses fertilities to copy encoder inputs as decoder inputs, allowing for parallel word generation. It eliminates the need for a causal mask in self-attention, improving decoding efficiency. The Non-Autoregressive Transformer (NAT) model uses fertilities to copy encoder inputs as decoder inputs for parallel word generation, eliminating the need for a causal mask in self-attention. In the decoder, positional attention modules are included to incorporate positional information directly into the attention process, improving the decoder's ability for local reordering. Additionally, a latent variable z is introduced to model the nondeterminism in the translation process. The Non-Autoregressive Transformer (NAT) model uses fertilities to copy encoder inputs as decoder inputs for parallel word generation, eliminating the need for a causal mask in self-attention. In the decoder, positional attention modules are included to incorporate positional information directly into the attention process, improving the decoder's ability for local reordering. Additionally, a latent variable z is introduced to model the nondeterminism in the translation process by sampling from a prior distribution and generating a translation non-autoregressively. This latent variable acts as a sentence-level \"plan\" and should be simple to infer, account for correlations across time, and not make the output translations too predictable for the decoder neural network to learn effectively. The proposed Non-Autoregressive Transformer (NAT) model uses fertilities to copy encoder inputs for parallel word generation, introducing an informative latent variable. Fertilities are integers for each word in the source sentence, indicating the number of words in the target sentence aligned to that source word. This approach eliminates the need for a causal mask in self-attention and improves the decoder's ability for local reordering. The fertility prediction model uses a neural network to predict fertility values for each input word independently. Fertilities serve as a latent variable in non-autoregressive machine translation, aiding in alignment and output space factorization. An external aligner simplifies inference, addressing the multimodality problem. The fertility prediction model uses fertilities as a latent variable in non-autoregressive machine translation, aiding in alignment and output space factorization. Including fertilities and reordering in the latent variable would provide complete alignment statistics, simplifying the decoding function and reducing modeling complexity in the encoder. Using fertilities alone allows the decoder to share the burden with the encoder, eliminating the need for explicit modeling of translation length. The model uses fertilities to condition the decoding process for generating diverse translations. At inference time, the model identifies the translation with the highest conditional probability by maximizing local probabilities for each output position. Three heuristic decoding algorithms are proposed to reduce the search space of the NAT model. The model uses fertilities to condition the decoding process for generating diverse translations. At inference time, the model identifies the translation with the highest conditional probability by maximizing local probabilities for each output position. Three heuristic decoding algorithms are proposed to reduce the search space of the NAT model. Since the fertility sequence is also modeled with a conditionally independent factorization, the best translation can be estimated by choosing the highest-probability fertility for each input word. Different decoding methods like Noisy Parallel Decoding (NPD) and autoregressive teacher are used to approximate the true optimum of the target distribution. The proposed NAT model uses a discrete sequential latent variable f 1:T to approximate the conditional posterior distribution. A proposal distribution q is defined by a fixed fertility model, simplifying the overall maximum likelihood loss. This approach allows for diverse translations to be generated efficiently. The proposed NAT model uses a fixed fertility model to simplify the inference process. The loss function allows for supervised training of the translation model and fertility neural network. The latent fertility model improves the output distribution but does not completely solve the issue of multiple correct translations for a single sequence of fertilities. The proposed NAT model utilizes a fixed fertility model for simplified inference. It allows for supervised training of the translation model and fertility neural network. The latent fertility model enhances the output distribution but does not fully address the challenge of multiple correct translations for a single sequence of fertilities. Additionally, sequence-level knowledge distillation is applied to create a new corpus for training an autoregressive machine translation model. The proposed NAT model utilizes a fixed fertility model for simplified inference and supervised training. A fine-tuning step is introduced after training the NAT to convergence, incorporating word-level knowledge distillation. The model is trained jointly with a weighted sum of distillation loss terms, including an expectation over predicted fertility distribution and an external fertility inference model. The proposed NAT model uses a fixed fertility model for training and inference. It is evaluated on three machine translation datasets and utilizes byte-pair encoding for tokenization. Teacher sequence-level knowledge distillation is applied to improve training dataset multimodality. Knowledge distillation is used to address multimodality in the training dataset, employing autoregressive models as teachers. The same teacher model is also used for scoring in fine-tuning and noisy parallel decoding. Autoregressive teachers are implemented using the Transformer architecture for high translation quality. Students and teachers have the same sizes and hyperparameters, except for positional self-attention and fertility prediction modules. BLEU scores on official test sets show the effectiveness of the approach. Knowledge distillation is utilized to address multimodality in the training dataset, using autoregressive models as teachers. The NAT student's encoder is initialized with the teacher's encoder weights to improve performance. Fertility predictions are supervised during training. The NAT student's encoder is initialized with the teacher's encoder weights to improve performance. Fertility predictions are supervised during training using a fixed aligner as a fertility inference function. Hyperparameters are set for experiments on different datasets, with smaller settings for IWSLT. The PyTorch implementation of the NAT has been open-sourced, showing a slight decrease in BLEU scores compared to its autoregressive teacher. In the case of WMT16 English-Romanian, noisy parallel decoding improves the performance of the non-autoregressive model significantly. Latencies for decoding with NPD show a speedup of more than a factor of 10 over greedy autoregressive decoding. Ablation study on the IWSLT dataset reveals that training on the distillation corpus rather than the ground truth results in a consistent improvement of around 5 BLEU points. Training on a distillation corpus rather than the ground truth consistently improves performance by around 5 BLEU points. Switching to fertility-based copying boosts performance by four BLEU points with ground-truth training or two with distillation. Fine-tuning with all three terms together leads to a 1.5 BLEU point improvement. Politicians use words to shape reality, but reality changes words more than words can change reality. Examples of translations from the IWSLT development set show instances of repeated words in non-autoregressive output. Noisy parallel decoding can filter out mistakes in translations produced by the NAT with NPD. The NAT with NPD produces translations that are more literal compared to the autoregressive model. Noisy parallel decoding in the process demonstrates translation diversity. The translation latency of the NAT is nearly constant for typical lengths, even with NPD sample sizes of 10 or 100, leading to high parallelism. The NPD with a sample size of 100 saturates the GPU with high parallelism, resulting in linear latencies."
}