{
    "title": "SylUzpNFDS",
    "content": "This work introduces a novel loss function for robust event localization in the presence of temporally misaligned labels. The approach allows for training models with noisy labels, demonstrating strong performance in challenging experiments without sacrificing raw performance. Temporal localization under label misalignment is a challenge in training models with noisy labels for precise event localization. Deep neural networks are susceptible to memorizing label noise, leading to training robustness issues and poor generalization performance. The need for large datasets and efficient annotation processes remains a bottleneck in the field. This work addresses the challenge of precise temporal localization of events in sequential data despite poorly aligned annotations for training. Effective models must infer event locations more accurately than the noisy training labels they rely on. The challenge of precise temporal localization in sequential data is difficult for classical approaches that rely on inaccurate annotations. There is a need for more flexible learning methods that are less reliant on exact label locations due to temporal noise in tasks. Temporal labeling involves a trade-off between annotation precision and time investment. This work introduces a novel model-agnostic loss function for robust temporal localization under label misalignment, reducing the reliance on exact temporal annotations. It aims to make deep learning models more efficient in temporal localization applications by increasing robustness to temporally misaligned labels. The proposed approach aims to improve temporal localization by reducing reliance on exact annotations and modeling label misalignment. It focuses on the instantaneous nature of events and aims to estimate true event occurrence times using noisy data for training. In a discrete setting, predictors in training data are observable temporal sequences. The label sequence is the discrete equivalent of the temporal sequence. The literature on temporal noise robustness is limited, with proposed solutions requiring clean annotations. In this paper, a novel and flexible loss function is proposed for robust training of temporal localization networks in the presence of label misalignment. This approach is different from traditional methods that rely on clean annotations and introduces a new way to handle temporal noise robustness. Our approach involves label smoothing to increase robustness to temporal misalignment of annotations. This converts the point prediction problem into a distribution prediction problem, with the algorithm trained to predict these distributions. The distributions are then transformed back to point predictions using peak picking heuristics. This methodology is common in applications dealing with spatial uncertainty, such as human pose estimation and facial landmark detection. The traditional solution for event detection presents drawbacks such as suboptimal model predictions, spread out predictions over time-steps, the need for additional heuristics for precise localization, and struggles with disentangling close events. The presence of strong label misalignment worsens issues with event prediction models, causing problems for causal models, one-sided recurrent networks, and convolutional architectures. This leads to poor generalization performance and limits the range of possible architectures. Increased noise exacerbates the problem by dispersing the label mass even further. This work introduces a novel paradigm for addressing temporal label uncertainty by inferring point predictions directly, integrating noise modeling into the loss function. The proposed systematic loss function resolves issues with noise robustness and label misalignment, scaling well to extensive misalignments. Additionally, weakly-supervised learning models utilize weaker annotations to infer more detailed concepts. Some weakly-supervised models leverage weaker annotations to infer fine-grained concepts, bypassing noisy labels with higher-level labels. Smoothing labels transforms point prediction into distribution prediction, causing issues. Filtering both labels and predictions with a softness parameter can alleviate drawbacks. The comparison of two smoothed processes with a unique softness parameter yields a relaxed loss function for soft learning of location, dealing with temporal uncertainty of labels. The model infers point predictions from input sequences of point-like events without resorting to distributions or heatmaps. The loss function views point labels and predictions as smoothed processes, allowing for flexibility in event mass concentration and temporal shifts without abrupt loss increases. The model's reliance on exact label locations is relaxed by using a bounded average local mean-squared error as the loss function. Smoothing both labels and predictions solves issues related to temporal misalignment, ensuring optimal predictions and adding detection mass before and after the prediction time. The model's reliance on exact label locations is relaxed by using a bounded average local mean-squared error as the loss function, solving issues related to temporal misalignment and adding detection mass before and after the prediction time. The prediction mass for each event is not necessarily dispersed over time anymore, and the potential dispersion of the prediction mass and its consequences on localization performance need to be addressed by leveraging the properties of a weakly supervised model for precise temporal localization. The SoftLoc model incorporates a mass convergence loss to achieve precise impulse-like localization, eliminating prediction ambiguity by concentrating scattered mass towards single points in time. This allows the model to mimic localization annotations while converging scatter mass towards impulse-like predictions. The regularization parameter \u03b1 \u03c4 balances mass convergence and soft learning during training iterations. The SoftLoc model uses mass convergence loss for precise impulse-like localization, balancing soft learning with mass convergence during training iterations. The proposed end-to-end solution eliminates the need for hand-crafted components and is designed to better serve the task at hand. The SoftLoc model is a generalization of past works and can be adapted for continuous-time frameworks. By adjusting the softness parameter, the model can cover a range of training regimes from fully-supervised to weakly-supervised. Tending the softness parameter towards zero makes the model similar to a count-aware localization RNN with soft learning loss. This loss function can be identical to those used in various temporal detection works. The SoftLoc model can be adapted for various training regimes, from fully-supervised to weakly-supervised, by adjusting the softness parameter. This parameter allows the model to handle uncertainties and noisy annotations, improving performance across different sequences. Experiments show robust performance even with variations in softness levels. The SoftLoc model's performance is robust to variations in the softness parameter, acting as a coarse indicator of temporal uncertainty. The approach is effective in challenging experiments like music event detection and video action segmentation. Piano transcription, specifically onset detection, is a complex task requiring precise detection from multiple channels. The experiment is based on the MAPS database, following a strict dataset creation protocol. Training labels are evaluated for robustness. The study evaluates the robustness of training labels by artificially perturbing them while keeping test labels intact. Three benchmarks are considered, including a state-of-the-art model, a smoothed version with extended onset length, and the use of soft bootstrapping loss for increased robustness. The network architecture consists of six convolutional layers for representation. The network architecture includes six convolutional layers, a 128-unit LSTM, and two fully-connected layers. Training involves using mel-spectrograms and their derivatives as input, with data augmentation for robustness. The Adam algorithm is used for optimization, and evaluation is done with F1-scores. The proposed approach shows piano onset detection performance compared to benchmark models. Our proposed SoftLoc approach demonstrates strong robustness against label misalignment, with performance almost invariant to noise levels. Label uncertainty is taken into account, leading to unattainable scores for classical approaches that do not consider this factor. Standard tricks like label smoothing have limited effectiveness compared to our approach. The SoftLoc approach shows robustness against label misalignment and noise levels, with L SLL driving performance in noise-free settings and L MC ensuring stability. The combined L SoftLoc produces competitive scores and strong robustness. The softness S M is a crucial model hyperparameter, with 210 independent runs conducted for drum detection experiments. In 210 independent runs for drum detection experiments, the correlation between label noise, softness levels, and localization performance is highlighted. A Gaussian Nadaraya-Watson kernel regression is used to analyze the F1-score response to varying label noise levels, showing the model's robustness to misalignments. The results are displayed in Figure 6, indicating optimal performance across a wide range of softness levels as long as SM \u2265 \u03c3. The SoftLoc model demonstrates outstanding performance with F1-scores over 95% across all noise levels, even outperforming benchmarks trained on noise-free labels. Despite noise-free advantages, SoftLoc achieves state-of-the-art performance on various metrics, showing robustness without sacrificing localization performance. The task in this section involves precise temporal detection of smoking episodes using wearable sensors features. The model focuses on robustness rather than raw performance, with detections needing to align perfectly with ground-truth to be considered correct. Noise distributions are applied in a discrete fashion for robustness analysis. The model architecture for noise-free drum detection is kept simple with a fully connected layer and LSTM unit. Results from cross-validation show the effectiveness of the proposed L SoftLoc loss function. Training with the proposed L SoftLoc loss function improves robustness compared to standard cross-entropy. The recurrent model outperforms the LR-M model on all metrics with low standard deviations, showing consistency. Normal smoothing filters are effective regardless of noise distribution. Testing with skew normal noise confirms results even in non-symmetric settings. Video action segmentation differs from other classification problems. The SoftLoc loss function is utilized in video segmentation experiments to measure performance gains compared to standard cross-entropy. The ED-TCN model shows robustness against label misalignment, with additional gains observed when using the SoftLoc loss. Experiments involve delaying or advancing label sequences to assess robustness. Replacing standard crossentropy loss with L SoftLoc significantly increases the robustness of the ED-TCN model and achieves competitive performance in noise-free settings. Increasing model softness as noise levels increase leads to optimal performance. Increasing model softness can lead to optimal performance, with the SoftLoc loss function showing strong results in temporal segmentation tasks. By relaxing annotation requirements and reducing reliance on exact event locations, the model becomes robust to temporal noise without compromising performance on clean data. This contrasts with traditional approaches that struggle with noisy labels, as demonstrated in various challenging tasks. The SoftLoc loss function demonstrates state-of-the-art performance on challenging tasks and is versatile for use in various recurrent architectures. It shows strong invariance to label misalignment even in extreme noise settings, as confirmed by experiments with noise levels up to \u03c3 = 1000ms. The SoftLoc model shows robustness to label misalignment, with detection capability outperforming classical approaches in noisy settings. Increasing model softness could further improve results. Predictions are consistently precise regardless of noise levels. Training labels are compared to clean ground-truth to illustrate the complexity of the localization task. The SoftLoc model demonstrates robustness to label misalignment and outperforms classical approaches in noisy settings. Increasing model softness can enhance performance. Training labels are compared to clean ground-truth to showcase the complexity of the localization task, with performance improving when using the SoftLoc loss function. The model achieves optimal performance when softness level SM is slightly larger than noise level \u03b4. Performance can still be close to optimal with a wide range of softness values. The ED-TCN model's relative performance is shown in Figure 10 for different softness levels and noise levels."
}