{
    "title": "BklSwn4tDH",
    "content": "Noisy labels in training data can lead to poor generalization on test data due to overfitting. A new method called Prestopping involves early stopping training a deep neural network before noisy labels are memorized, then resuming training using a \"maximal safe set\" of true-labeled samples. This approach outperforms state-of-the-art methods in reducing test error under real-world label noise. Deep neural networks (DNNs) have shown remarkable success in various machine learning tasks, but their generalization performance drastically decreases with noisy labels in the training data. A common approach to address this issue is \"sample selection\" to extract true-labeled samples from noisy data. The noisy training data is separated into small-loss samples treated as true-labeled ones to update DNN robustly. This loss-based separation is justified by the memorization effect, where DNNs learn easy patterns first. Training on small-loss samples improves generalization performance under artificial noise scenarios, but its effectiveness varies with the type of label noise. DenseNet (L=40, k=12) is tested on CIFAR-100 and FOOD-101N datasets with different types of synthetic and real-world noises. The study compares the performance of standard training methods with a proposed \"Prestopping\" method, showing how the loss-based approach separates true-labeled samples from false-labeled ones effectively in symmetric noise but struggles with pair and real-world noises. The study compares the performance of standard training methods with a proposed \"Prestopping\" method, showing how the loss-based approach separates true-labeled samples from false-labeled ones effectively in symmetric noise but struggles with pair and real-world noises. In real-world noise or pair noise, many false-labeled samples are misclassified as true-labeled ones due to overlapping distributions, highlighting the need for a new approach to handle various types of label noise. The loss-based separation method performs well in symmetric noise but struggles with pair and real-world noises. There is a period where the network accumulates label noise severely, leading to a rapid increase in false-labeled samples at a late stage of training. This error-prone period negatively impacts generalization performance. Eliminating this period could have a profound impact on robustness. The paper proposes Prestopping, a novel approach that eliminates the error-prone period in training neural networks. By early stopping before this period begins, Prestopping prevents overfitting to false-labeled samples. It then resumes training using a maximal safe set of true-labeled samples, improving generalization performance significantly. Prestopping significantly improves the generalization performance of deep neural networks in noisy data sets. Compared to state-of-the-art methods, Prestopping reduces test error by up to 18.1pp 2 across various noise rates. Other methods for learning from noisy labels include loss correction, Bootstrap, and F-correction. F-correction (Patrini et al., 2017) reweights the forward or backward loss of training samples based on the label transition matrix estimated by a pre-trained normal network. R3: D2L (Ma et al., 2018) uses local intrinsic dimensionality to modify the forward loss and reduce the impact of false-labeled samples. Active Bias (Chang et al., 2017) evaluates uncertain samples with high prediction variances and assigns higher weights to their backward losses. Ren et al. (2018) incorporate small clean validation data into training and re-weight the backward loss of mini-batch samples to minimize the loss of validation data. However, these methods can accumulate noise from false correction, especially with a large number of classes or false-labeled samples (Han et al., 2018; Yu et al., 2019). Many recent researches have adopted \"sample selection\" to train networks on selected true-labeled samples, aiming to avoid false correction. Decouple maintains two networks simultaneously and updates models using samples with different label predictions. MentorNet introduces a collaborative learning paradigm where a mentor network guides the training of a student network based on small-loss criteria. Wang et al. proposed an iterative learning framework that learns deep discriminative features from well-classified noisy samples. The mentor network provides correct label samples to the student network. Co-teaching and Co-teaching+ use two networks to select small-loss samples for training. ITLM minimizes trimmed loss iteratively by selecting small-loss samples. INCV divides noisy data and classifies true-labeled samples using cross-validation. Unlike previous methods, we focus on the maximal safe set initially. The algorithm SELFIE combines loss correction and sample selection to train the network on false-labeled and small-loss samples. It minimizes falsely corrected samples and explores the entire training data. Early stopping helps prevent the network from memorizing all noisy samples if trained for too long. The algorithm SELFIE combines loss correction and sample selection to train the network on false-labeled and small-loss samples. Early stopping prevents the network from memorizing all noisy samples if trained for too long. Our novelty lies in merging early stopping with learning from the maximal safe set to achieve noise-free training. When training with mini-batches from noisy data, the network parameter is updated based on expected loss. Network memorization occurs when a sample's predictions mostly match the label. The probability of a label for a sample is estimated based on its history of predicted labels. The key idea of Prestopping is learning from a maximal safe set with an early stopped network. It raises questions about when to early stop training and what the maximal safe set is for noise-free training. The best stop point is when label precision and recall cross, to minimize noise accumulation and maximize information acquisition. The best stop point in training is when label precision and recall cross, marking the error-prone period. Practical heuristics can approximate this point with minimal supervision using clean validation sets or noise rates. The text discusses practical heuristics for determining the best stop point in training neural networks, including using clean validation sets or noise rates to approximate the point where label precision and recall cross. The text discusses the heuristic of determining the best stop point in training neural networks based on the cross point of label precision and recall values. It suggests using a maximal safe set derived from memorized samples at the early stop point to resume training without accumulating label noise. The text discusses the concept of a maximal safe set in training neural networks, which is defined as the set of memorized samples at a specific time point. This set is used to resume training without accumulating label noise, even in high noise rate scenarios. The label recall of the maximal safe set remains high, demonstrating its effectiveness in learning from true-labeled samples. The text discusses the effectiveness of the maximal safe set in training neural networks, showing high label precision even in high noise rate scenarios. The set excludes unsafe samples to prevent label noise accumulation, outperforming the small-loss trick method. The text describes the Prestopping algorithm, which involves training a neural network on noisy data and using a validation heuristic to determine the best point for early stopping. The algorithm then continues training on a maximal safe set of samples to update the network parameters. Prestopping algorithm improves robust learning by selectively refurbishing false-labeled samples in SELFIE. The final safe set from Prestopping is used as true-labeled set for SELFIE, updating network parameters with modified gradient rule. Samples in mini-batch are selectively refurbished if label can be corrected with high precision, omitting samples already credible. Refurbished samples are used for training the network. The Prestopping algorithm improves robust learning by selectively refurbishing false-labeled samples in SELFIE. The mini-batch samples in the refurbished set are used to update the network. Data sets used for verification include CIFAR-10, CIFAR-100, ANIMAL-10N, and FOOD-101N. Noise injection was applied to artificially corrupt labels for evaluation. We corrupted labels in data sets using label transition matrix for evaluation. Noise rates from 0.0 to 0.4 were tested. ANIMAL-10N and FOOD-101N had real label noise. Clean validation set is necessary for validation heuristic. For the validation heuristic, a clean validation set was created by randomly selecting 1,000 images from the original training data of 50,000 images for CIFAR data sets. DenseNet (L=40, k=12) and VGG-19 were trained for the classification task with specific hyperparameters. Prestopping utilized a unique hyperparameter, history length q, set to 10 after grid search. Comparing algorithms used optimized hyperparameters. The compared algorithms used optimized hyperparameters from the original papers. The network was trained for 120 epochs with an initial learning rate of 0.1, decreased at 50% and 75% of epochs. Prestopping and Prestopping+ were compared with baseline and four robust training algorithms: Default, Active Bias, Co-teaching, and Co-teaching+. All algorithms were implemented for testing. The algorithms were implemented using TensorFlow 1.8.0 on 16 NVIDIA Titan Volta GPUs. The best test errors of two CNNs on two data sets with varying noise rates were evaluated. Results were repeated thrice for reliable evaluation, and the average test error was reported. The clean validation set was used to select the best model during training. Figures 5 and 6 show the test error of seven training methods. Prestopping and Prestopping+ results were obtained using validation heuristic. Noise-rate heuristic results can be found in Appendix A. The results of the noise-rate heuristic are highlighted in Appendix A, showing the test error on real-world noisy data sets. Prestopping and Prestopping+ outperformed other methods, achieving the lowest test error in a wide range of noise rates on CIFAR data sets. Experimental results on Tiny-ImageNet and Clothings data sets are discussed in Appendices B.2 and B.3, showing similar performance trends in network architectures. Prestopping+ slightly outperformed Prestopping in CIFAR-10 with refurbished samples, but the opposite trend was observed in CIFAR-100 due to falsely corrected labels under pair noise. The test error of SELFIE was worse than Prestopping, Co-teaching+ performed worse than Co-teaching, and Active Bias did not compare to Prestopping. Prestopping and Prestopping+ significantly reduced test error compared to other methods, especially at heavy noise rates. Prestopping and Prestopping+ outperformed other methods, with Prestopping+ showing the best performance at any noise rate on all data sets. The methods significantly reduced test error by 0.3pp-17.5pp compared to other robust methods at a heavy noise rate of 40%. Co-teaching and SELFIE achieved low test error comparable to Prestopping due to their small-loss criteria. Prestopping and Prestopping+ maintained dominance over other methods under real-world label noise, with Prestopping+ achieving the lowest test error for small numbers of classes. Prestopping and Prestopping+ were found to be the best methods for different numbers of classes, with Prestopping+ showing superior performance at high noise rates. These methods improved test error by 0.4pp-8.2pp in various scenarios. The error-prone period was eliminated by some methods, while others continued to reduce validation error even during that period. The remaining methods continued to reduce validation error even during the error-prone period. Prestopping+ achieved the lowest validation error by combining advanced sample selection and sample refurbishment. The performance of Prestopping was comparable to Prestopping+ in pair and symmetric noises. SELFIE also showed faster convergence but was still inferior to Prestopping+. These results confirm the effectiveness of the two methods in overcoming errors. Average accuracy of sample selection using DenseNet was shown in Figure 9, with Prestopping and Prestopping+ being the best methods for different numbers of classes. During training, the small-loss trick had lower accuracy with pair noise compared to symmetric noise, dropping to 75.0% with increased noise rate in CIFAR-100. However, the maximal safe set consistently outperformed the small-loss trick in all datasets regardless of noise type, with an average accuracy of 93.2%. Figure 9 shows the average accuracy of selecting true-labeled samples using Co-teaching and the maximal safe set in Prestopping with synthetic noises. Figure 10 illustrates the disagreement ratio for true-labeled samples when using Co-teaching+ on CIFAR-100 with 40% symmetric noise. The performance of Co-teaching+ was worse than expected, attributed to the fast consensus of label predictions for true-labeled samples, especially when training a complex network. This led to early exclusion of samples, impacting accuracy on CIFAR-100 with 40% pair noise. The study found that Co-teaching+ performed poorly on CIFAR-100 with 40% pair noise due to early exclusion of samples when training a complex network. A novel two-phase training strategy called Prestopping was proposed to address this issue, with the first phase focusing on retrieving true-labeled samples and the second phase completing the training process using only high-quality samples. The study introduced a two-phase training strategy called Prestopping to improve robustness to label noise. By focusing on retrieving true-labeled samples in the first phase and completing training with high-quality samples in the second phase, Prestopping achieved the lowest test error among compared methods. This approach divides the training process into two phases, showing promise for future research in robust training methods. Algorithm 2, Prestopping with Noise-Rate Heuristic, introduces a new way of determining the best stop point in the training process. It involves two phases: learning from a noisy training dataset and learning from a maximal safe set. The algorithm aims to improve robustness to label noise and has shown promising results in reducing test error compared to other methods. Algorithm 2, Prestopping with Noise-Rate Heuristic, aims to improve robustness to label noise by training a VGG-19 network on simulated noisy data sets. The test error of Prestopping and Prestopping+ was the lowest at most error rates with any noise type, significantly improving test error compared to other robust methods. The performance of the noise-rate heuristic was worse than the validation heuristic, even though it outperformed other training methods. The noise-rate heuristic's lower performance is attributed to its imperfect assumption, but it can potentially be improved by stopping slightly earlier than the estimated point. Hyperparameter tuning for Prestopping involved choosing the history length q, and testing on CIFAR-10 and CIFAR-100 datasets with a noise rate of 40%. Test error results of Prestopping obtained through grid search on the noisy CIFAR datasets are shown in Figure 13. In a larger-scale experiment, the image classification task was repeated on Tiny-ImageNet with 100,000 training and 10,000 validation images. The test error of seven training methods using VGG-19 on Tiny-ImageNet with varying noise rates was analyzed. In a larger-scale experiment, the image classification task was repeated on Tiny-ImageNet with 100,000 training and 10,000 validation images. The test error of seven training methods using VGG-19 on Tiny-ImageNet with varying noise rates was analyzed. Similar to CIFAR data sets, there was no synergistic effect from collaboration with sample refurbishment due to the larger number of classes. Prestopping showed a significant reduction in absolute test error at pair noise and symmetric noise levels. Additionally, Prestopping was evaluated on the Clothing70k data set, showing superiority with a noise rate of 38.5%. Figure 15 displays the test error of seven training methods on Clothing70k with 38.5% real-world noise. Prestopping and Prestopping+ outperformed other methods, with Prestopping+ achieving the lowest test error due to the smaller number of classes. It improved test error by 1.1pp-3.3pp using DenseNet and 0.7pp-5.8pp using VGG-19 compared to state-of-the-art methods. Investigating the impact of classes on Prestopping+ under 40% pair noise, Figure 16 shows the ratio of refurbished samples used for training and their accuracy on data sets with 10 and 100 classes. Prestopping+ accurately refurbished false-labeled samples, with accuracy consistently over 91.6% when the number of classes was small. The refurbishing accuracy was consistently over 91.6% during training, but dropped to 76.7% with a large number of classes. Collaboration with sample refurbishment may not always be beneficial, as it could lead to falsely corrected samples. Tables 1 and 2 show the test error of seven training methods using CNNs on noisy data sets."
}