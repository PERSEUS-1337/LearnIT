{
    "title": "HklvmlrKPB",
    "content": "We propose an approach for sequence modeling using autoregressive normalizing flows, which act as reference frames for modeling dynamics. This technique improves sequence modeling and is demonstrated on video datasets, showing enhanced performance. Sequential structure in data is valuable for learning self-supervised representations and planning actions. Improved computational techniques, like deep networks, have enabled learning sequential models from high-dimensional data such as video and audio. These models combine stochastic and deterministic variables using simple distributions to model data observations. However, capturing all sequential dependencies with unstructured dynamics may hinder learning. A method for altering dynamics to track input changes is sought instead of expanding computational capacity. To incorporate more structured dynamics, an approach for sequence modeling based on autoregressive normalizing flows is proposed. This method involves using one or more autoregressive transforms in time, which can be stacked to create more expressive models. Each transform acts as a moving reference frame to model higher-level structure, allowing for the separation of different forms of dynamics. This approach generalizes the technique of modeling temporal derivatives. Incorporating autoregressive normalizing flows for sequence modeling simplifies dynamics estimation. This method demonstrates improved performance in sequential latent variable models, emphasizing their use in conjunction with normalizing flows. Experimental results on video datasets show enhanced log-likelihood. This technique contextualizes previous work and utilizes affine autoregressive transforms for modeling dynamics. Affine autoregressive transforms are used to model discrete sequences of observations, converting correlated inputs into less correlated variables. Autoregressive models express joint distributions over time steps as products of conditional distributions, predicting future time steps. These models are often formulated in forward temporal order to handle variable-length sequences. Autoregressive models predict future time steps by assuming simple distribution forms like diagonal Gaussian density. They can incorporate latent variables to capture higher-level structure, improving model performance. The joint distribution in models like Gaussian state space models and hidden Markov models is flexible but requires integrating over latent variables, making exact inference intractable. Deep neural networks have been used to parameterize these models, with variational inference techniques like amortized variational inference for inference and learning. Temporal conditioning is handled through deterministic recurrent networks and stochastic latent variables. Our approach is based on affine autoregressive normalizing flows for temporal sequences, which have shown success in audio and video modeling. Design choices for these models are actively researched, with new combinations of deterministic and stochastic dynamics proposed. Flow-based models, initially developed in static settings, involve sampling from an autoregressive Gaussian model as an invertible transform resulting in a normalizing flow. Flow-based models transform between simple and complex probability distributions while maintaining exact likelihood evaluation. Sampling a Gaussian random variable can be expressed using the reparameterization trick, where x t is an invertible transform of y t. The inverse transform acts to normalize and decorrelate x 1:T, converting probabilities between spaces using the change of variables formula. Autoregressive flows involve chaining multiple transforms to create a hierarchical model. These flows were initially used in variational inference and generative modeling contexts. The change of variables formula enables efficient evaluation and sampling of the model. Autoregressive flows, a hierarchical model using multiple transforms, were initially applied in static settings like images. Recent works have extended flow-based models to sequential data, demonstrating their usefulness in improving sequence modeling. This work focuses on using autoregressive flows as a general-purpose technique for sequential latent variable models. Our work is the first to use flows to pre-process sequential data for improved downstream dynamics modeling. We focus on affine flows, such as NICE, RealNVP, IAF, MAF, and GLOW, but non-affine flows could offer more flexibility. Our approach for sequence modeling with autoregressive flows is a simple extension that can enhance autoregressive latent variable models. Incorporating affine autoregressive transforms in autoregressive latent variable models improves dynamics modeling. A simple example illustrates a discrete dynamical system with position, velocity, and stochastic forces. The dynamics are second-order Markov in the position space and first-order Markov in the velocity space. Incorporating affine autoregressive transforms simplifies dynamics modeling by converting a second-order Markov system into a first-order Markov system. This process leads to temporal decorrelation, eliminating dynamics and resulting in a representation with no dynamics. This technique is commonly used for modeling temporal changes. Applying affine autoregressive flows simplifies dynamics modeling by transforming a second-order Markov system into a first-order Markov system. This technique allows for non-linear transform parameters and multiple transforms to decorrelate input sequences in time, focusing model capacity on less correlated fluctuations. Autoregressive flows are applied across time steps within a sequence, redefining the state-space to be first-order Markov. Within a sequence, observations at each time step are modeled as an autoregressive function of past observations and a random variable. Flows are constructed using parameterized neural networks, with shift and scale functions at each transform. The final transform results in a base distribution that can range from simple to more complex distributions. Flows of greater depth can improve model capacity but have limiting drawbacks, such as maintaining the same dimensionality as inputs. Autoregressive flows in neural networks require outputs to maintain input dimensionality, use affine transforms, and operate element-wise within a time step. These flows can be combined with non-invertible latent variable models to parameterize dynamics. Multiple transforms can be applied within each flow, with a focus on parameterizing conditional likelihood within a latent variable model using autoregressive flows. This involves transforming a base conditional distribution into the desired output via an affine transform. The latent variable model's log-joint distribution is expressed using the change of variables formula. The latent prior can be conditioned on x or y, with potential extensions like autoregressive flows or a hierarchy of latent variables left for future work. Training the model is done via maximum likelihood. Training a latent variable model via maximum likelihood involves marginalizing over latent variables to evaluate the marginal log-likelihood of observations. Variational inference introduces an approximate posterior distribution, q(z 1:T |x 1:T ), which provides a lower bound on the marginal log-likelihood known as the evidence lower bound (ELBO). The structured distribution q(z 1:T |x 1:T ) captures temporal dependencies in the model. Filtering inference is focused on, with conditional dependencies in q modeled through a direct, amortized function or optimization. The text discusses training a latent variable model using variational inference to evaluate the evidence lower bound (ELBO). It mentions conditioning on variables x \u2264t or y \u2264t and demonstrates the framework on video datasets like Moving MNIST. The model involves learning a latent variable model on top of the intermediate space provided by y, with a penalty on the scaling between x and y. The proposed framework implements three classes of models on video datasets: Moving MNIST, KTH Actions, and BAIR Robot Pushing. Experimental setups, qualitative experiments, and quantitative comparisons are provided in different sections. Flows are used with convolutional networks to output shift and scale parameters. Sequential latent variable models include convolutional and recurrent networks for encoder and decoder networks. Further implementation details and code are available in the appendix. The models in the study consist of convolutional and recurrent networks for both the encoder and decoder networks. Flow-based conditional likelihoods are used to model the noise variable with fewer parameters compared to sequential latent variable models. Parameter comparisons and architecture details can be found in Appendix B. The study utilizes flow-based conditional likelihoods to model noise variables with fewer parameters compared to sequential latent variable models. Visualizations of components like data, shift, scale, and noise variable are shown for standalone flow-based models and flow-based conditional likelihoods on random sequences from different datasets. Shift parameters capture static backgrounds, while scale parameters show blurring around regions of uncertainty. The study compares standalone flow-based models with flow-based conditional likelihoods in sequential latent variable models. Flow-based conditional likelihoods show more structure in the noise variables, indicating a more expressive noise distribution. While a single flow can decorrelate simple datasets like Moving MNIST, natural image datasets like KTH Actions and BAIR Robot Pushing require additional model capacity to capture the underlying structure. In Appendix C.1, the study quantifies temporal decorrelation in flow-based models by evaluating correlation between frames. Standalone flow-based models perform well, even outperforming sequential latent variable models in some cases. Increasing flow depth generally improves performance. Sequential latent variable models with flow-based conditional likelihoods outperform baseline models. Sequential latent variable models with flow-based conditional likelihoods outperform their baseline counterparts due to reduced overfitting, especially evident on datasets with high separation between training and test sets. Removing static components like backgrounds improves generalization. Previous works often do not properly evaluate lower bounds on log-likelihood. Sequential latent variable models with flow-based conditional likelihoods have shown improved performance compared to baseline models, particularly in reducing overfitting. This is especially noticeable on datasets with significant differences between training and test sets. By removing static elements such as backgrounds, generalization is enhanced. Previous studies have not adequately assessed lower bounds on log-likelihood. The presented technique utilizes affine transforms to decorrelate sequential data, simplifying dynamics estimation and enhancing sequential latent variable models. Sequential latent variable models with flow-based conditional likelihoods have shown improved performance by utilizing affine transforms to decorrelate sequential data. This technique simplifies dynamics estimation and enhances the models by reducing overfitting and enhancing generalization on datasets with significant differences between training and test sets. The joint distribution over all time steps is parameterized with autoregressive flows, and variational inference is performed using a filtering approximate posterior. The model includes a fixed number of past frames in the buffer of each transform to generate the shift and scale for the transform, with convolutional layers applied to each data observation in the buffer to preserve data shape. The data observations in the buffer undergo convolutional layers to preserve shape. Outputs are concatenated and pass through additional convolutional layers. A DC-GAN structure is used for latent variable models, followed by an LSTM and fully connected layers to estimate the approximate posterior distribution of the latent variable. Another LSTM models the conditional prior distribution. The SLVM and combined model use LSTM layers for conditional prior and posterior distributions. Adam optimizer with a learning rate of 1 \u00d7 10 \u22124 is used for training. Different batch sizes and iteration numbers are used for Moving MNIST, BAIR Robot Pushing, and KTH dataset. Batch norm is applied to convolutional layers. The model architecture diagrams show layers like convolutional, LSTM, and fully connected for the sequential latent variable model. Anonymized code is available for evaluation. Additional experimental results demonstrate the effectiveness of flows. Experimental results show that flows can remove structure from observations, resulting in whitened noise images. Temporal decorrelation is confirmed by evaluating the correlation between successive frames. The correlation is calculated as the average normalized auto-covariance with a time delay of 1 step. This was done for data observations and noise variables in SLVM w/ 1-AF. The results for training sequences in SLVM w/ 1-AF show a decrease in temporal correlation. Figure 7 plots the quantity during training for KTH Actions. Figure 10 displays generated Moving MNIST samples, while Figure 11 shows generated BAIR Robot Pushing samples."
}