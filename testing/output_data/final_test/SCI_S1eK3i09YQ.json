{
    "title": "S1eK3i09YQ",
    "content": "This paper explains how randomly initialized gradient descent can achieve zero training loss in non-convex and non-smooth neural networks. It focuses on two-layer fully connected ReLU activated networks, showing that with a sufficiently large number of hidden nodes and non-parallel inputs, gradient descent converges to a globally optimal solution at a linear rate for the quadratic loss function. Our analysis shows that over-parameterization and random initialization constrain weight vectors to stay close to their initial values, enabling gradient descent to converge linearly to the global optimum. This insight is valuable for understanding deep models and first-order methods. Despite non-convexity and non-smoothness, stochastic gradient descent can still find global minima, even when labels are randomly generated. In this paper, the authors investigate the phenomenon of neural networks fitting all training labels despite non-convexity and non-smoothness. They focus on two-layer neural networks with ReLU activation, aiming to demystify how randomly initialized first-order methods can achieve this. The over-parameterization and random initialization help constrain weight vectors, allowing gradient descent to converge linearly to the global optimum. The paper analyzes two-layer neural networks with ReLU activation and focuses on optimizing the first layer using gradient descent. Despite being a shallow network, the objective function remains non-convex due to ReLU. The paper rigorously proves that gradient descent can achieve zero training loss at a linear convergence rate for two-layer neural networks with ReLU activation, as long as inputs are not parallel and m is large enough. This result contrasts with previous works that did not explain why randomly initialized first-order methods can achieve zero training error. Theoretical proof shows gradient descent achieves linear convergence rate for two-layer neural networks with ReLU activation. Analysis focuses on dynamics of individual predictions rather than parameter space. Gradient descent's linear rate is governed by spectral property of Gram matrix. The proof demonstrates that gradient descent achieves linear convergence for ReLU activated neural networks by showing the Gram matrix's spectral properties and the relationship with activation patterns. This result is obtained by leveraging insights on over-parameterization, random initialization, and weight vector constraints. Our proof shows that ReLU activated neural networks can achieve linear convergence with gradient descent using only linear algebra and standard probability bounds. This analysis can be easily extended to deep neural networks. In analyzing non-convex optimization problems, researchers found that smooth objective functions with certain properties can be optimized efficiently using stochastic gradient descent. Recent studies have shown that deep neural networks may also exhibit these properties. BID28 showed that for a specific objective function, training error is zero at differentiable local minima if md \u2265 n. However, convergence to a differentiable local minimum is challenging due to the non-smooth nature of the objective function. Our analysis focuses on the convergence rate of the gradient norm in deep neural networks, using the Gram matrix DD. Previous works have studied landscape properties of ReLU activated neural networks, but none have proven convergence to a global minimizer. Recent studies have also identified negative results and proposed new procedures to escape strict saddle points, but these procedures do not guarantee finding global minima. The landscape of ReLU activated neural networks has been studied, but proving convergence to global minima remains a challenge. Previous works have shown geometric properties for certain activation functions, but extending these analyses is unclear. Analyzing the dynamics of first-order methods directly can provide convergence results. Many studies assume Gaussian input distribution and label generation from a planted neural network, under which conditions stochastic gradient descent can learn various network architectures. Our paper focuses on providing theoretical justification for randomly initialized gradient descent achieving zero training loss in practice. Unlike previous works, we do not aim to recover the underlying true neural network. We focus on the convergence of gradient descent in a two-layer neural network with ReLU activation function, proving the stability of the Gram matrix. Our paper proves the stability of the Gram matrix for infinite training time in a two-layer neural network with ReLU activation function. Building on previous work by BID19, we show that gradient descent can achieve zero training loss without the need for excessive over-parameterization. Our simpler and more transparent proof can be easily applied to analyze other neural network architectures. The BID3 analysis used optimal transport theory to study gradient descent on over-parameterized models, requiring the second layer to be infinitely wide. Mei et al. analyzed SGD for optimizing population loss, showing dynamics can be captured by a partial differential equation. BID4 connected neural networks with kernel methods, demonstrating that SGD can learn competitive functions in the conjugate kernel space. However, it remains unclear why first-order methods can minimize empirical risk. The analysis focuses on gradient flow, specifically gradient descent with infinitesimal step size, as a foundation for understanding discrete algorithms. The main assumption involves the Gram matrix induced by the ReLU activation function and random initialization. The study aims to provide a quantitative bound for gradient descent with a positive step size. During training, the Gram matrix remains close to H \u221e, which determines the convergence rate. The least eigenvalue is strictly positive if no two inputs are parallel. The main theorem states the convergence rate of gradient flow under certain conditions. The theorem establishes that with a large enough number of hidden nodes, the training error converges to 0 at a linear rate. Over-parameterization is crucial for gradient descent to find the global minimum. The convergence rate depends on \u03bb0 but is independent of the number of hidden nodes. The convergence rate depends on \u03bb0 but is independent of the number of hidden nodes. The dynamics of predictions are characterized by H\u221e as m \u2192 \u221e. H(t) is a time-dependent symmetric matrix with properties analyzed at t = 0. If m is large, H(0) has a lower bounded least eigenvalue with high probability. The convergence rate depends on \u03bb0 but is independent of the number of hidden nodes. The dynamics of predictions are characterized by H\u221e as m \u2192 \u221e. H(t) is a time-dependent symmetric matrix with properties analyzed at t = 0. If m is large, H(0) has a lower bounded least eigenvalue with high probability. In the appendix, the concentration bound is deferred. Lemma 3.2 shows that for any W close to W(0), the induced Gram matrix H is close to H(0) and has a lower bounded least eigenvalue. Lemma 3.2 plays a crucial role in the analysis, proving that the event occurs if and only if a certain condition is met. By utilizing Gaussian anticoncentration inequality, the deviation on the induced matrix H can be bounded for a set of weight vectors. Matrix perturbation theory is then used to further bound the deviation from the initialization. Additionally, the smallest eigenvalue is lower bounded, leading to the convergence of loss at a linear rate and closeness of weight vectors to the initialization. The lemma demonstrates the power of over-parameterization by showing a linear convergence rate. The dynamics of predictions and loss function dynamics are analyzed, leading to an exponential convergence of u(t) to y. The gradient norm and distance from initialization are bounded, with conditions holding for all t \u2265 0 if R < R. The proof technique showcased in this subsection demonstrates linear convergence of gradient flow for jointly training both layers. The theorem states that using gradient flow, we can achieve a linear convergence rate towards zero loss with probability at least 1 \u2212 \u03b4 over the initialization. The convergence rate is shown to be the same as that of only training the first layer, relying on similar arguments as in the proof of Theorem 3.2. The dynamics of predictions are characterized by a Gram matrix, showing convergence for all t > 0. The dynamics of predictions are characterized by a Gram matrix, showing convergence for all t > 0. Gradient descent with a constant step size converges to the global minimum at a linear rate, even for non-smooth and non-convex objective functions. The proof is based on induction and the convergence rate of the empirical loss. Condition 4.1 ensures deviation from initialization is bounded. The proof is deferred to the appendix. The condition holds for every iteration k. With probability at least 1 \u2212 \u03b4, predictions differ by a constant C. The difference between predictions at consecutive iterations is analyzed. The text discusses the analysis of terms that may change in a pattern, viewing them as perturbations and bounding their magnitude. It also mentions the analysis of terms that do not change in the pattern, providing expressions for analysis. The text further discusses obtaining upper bounds and estimates to prove the induction hypothesis. In this section, synthetic data is used to validate theoretical findings. The experiments involve running 100 epochs of gradient descent with fixed step size on 1000 data points from a 1000-dimensional unit sphere. The analysis includes testing the impact of overparameterization on convergence rates and the relationship between the amount of data points and convergence metrics. The study examines the impact of over-parameterization on convergence rates and pattern changes in gradient descent. Results show that as the number of parameters increases, convergence rates improve, and pattern changes decrease. The findings align with theoretical predictions. In this paper, it is shown that with over-parameterization, gradient descent converges to the global minimum of the empirical loss at a linear rate. The key idea is that over-parameterization ensures the Gram matrix remains positive definite, leading to linear convergence. Future directions include generalizing the approach to deep neural networks. The proof of Theorem 3.1 relies on standard real and functional analysis. In a Hilbert space H of integrable d-dimensional vector fields, the inner product is defined as f, g H = E w\u223cN (0,I) f(w) g(w). The ReLU activation induces an infinite-dimensional feature map \u03c6(x)(w) = xIw x \u2265 0. To prove H \u221e is strictly positive definite, we need to show that \u03c6(x1), ..., \u03c6(xn) \u2208 H are linearly independent. This involves proving \u03b1i = 0 for all i, where \u03b11\u03c6(x1)(w) + ... + \u03b1n\u03c6(xn)(w) = 0 a.e. The discontinuity sets D_i = {w \u2208 Rd : wxi = 0} characterize the basic property of these discontinuities. The proof of Lemma A.1 shows that \u03b1i = 0 for all i, completing the proof. Lemma 3.1 states that H ij (0) is an average of independent random variables, with a probability of 1 - \u03b4. The proof involves applying Hoeffding inequality to independent random variables, ensuring a desired result with high probability. Lemma 3.4's proof shows contradictions if certain conditions are not met at time t. The rest of the proof follows a similar pattern. Theorem 3.3 is proven using gradient flow to jointly train the first layer. The proof of Theorem 3.3 demonstrates that by using gradient flow to train both the first layer and the output layer jointly, a training loss of 0 can still be achieved. The dynamics of individual predictions are computed, and concentration arguments are used to show certain properties with high probability. The proof of Lemma A.6 shows that if certain conditions are met, then specific inequalities hold for all t \u2265 0. By utilizing Lemma A.2 and ensuring certain constraints are satisfied, the proof is completed. Additionally, Corollary 4.1 is proven by bounding the distance using the norm of the gradient."
}