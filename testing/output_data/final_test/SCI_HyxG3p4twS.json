{
    "title": "HyxG3p4twS",
    "content": "Detection of photo manipulation relies on statistical traces, often removed by online compression. End-to-end modeling of dissemination channels allows for codec optimization with provenance objectives. A lightweight trainable image codec was designed, showing improved manipulation detection accuracy at low bit-rates. The codec outperformed standard JPEG compression, offering competitive rate-distortion performance with lower computational requirements. In this study, a deep compression network (DCN) was optimized alongside a facial alignment network (FAN) with a fixed camera model. The DCN model follows an auto-encoder architecture with unique quantization and coding schemes. It consists of convolutional layers, residual blocks, and implements distribution shaping through entropy regularization. The decoder mirrors the encoder and uses sub-pixel convolutions for up-sampling. Various latent representation variants were experimented with. The study optimized a deep compression network (DCN) and a facial alignment network (FAN) with a fixed camera model. The DCN model utilized unique quantization and coding schemes, including a 5-bpf uniform codebook. The model aimed to minimize distortion between input and reconstructed images, using a simple L2 loss in the RGB domain and SSIM as the validation metric. New quantization and entropy estimation mechanisms were developed due to existing approaches being overly complex or inaccurate. The recent solutions for quantization and entropy estimation in image compression include adding random noise, using differentiable entropy upper bounds, regularization techniques, and PixelCNN. The approach in this study builds upon soft quantization, addressing stability issues and improving entropy estimation accuracy. In image compression, quantization can be done using hard or soft methods. Hard quantization replaces input values with the closest codeword, while soft quantization uses a linear combination of all codewords. The comparison of both methods and potential numerical issues are illustrated in Fig. A.1. Soft quantization is used in the backward pass, implemented in Tensorflow as z = tf.stop_gradient(\u1e91 - z) + z. The weights for code-words are computed using a kernel \u03ba on distances, often using a Gaussian kernel which may have numerical problems for edge cases. To address numerical problems with edge cases in image compression, a t-Student kernel is adopted instead of a Gaussian kernel. The weight matrix is normalized numerically to ensure correct proportions. Entropy of quantized values is estimated by summing the weight matrix along the sample dimension. The quality of the estimate is assessed using synthetic random numbers and actual RGB image patches. The study compares the performance of t-Student and Gaussian kernels in estimating entropy of image patches. The t-Student kernel consistently outperforms the Gaussian kernel. The best kernel parameters (v = 50, \u03b3 = 25) are highlighted and used in subsequent experiments. The study at ICLR 2020 compared entropy coding methods for image compression. They used an entropy coder based on asymmetric numeral systems and observed bitrate savings of around 12% for 512x512 px images. Different compression modes can be switched with a flag to address overhead for small patches. The format container design is left for future work. The study at ICLR 2020 compared entropy coding methods for image compression using variable-length, per-channel entropy coding with random channel access. The DCN model was pre-trained on mixed natural images from various sources, totaling 32,000 square crops ranging from 512 \u00d7 512. The dataset consisted of 32,000 square crops ranging from 512 \u00d7 512 to 1024 \u00d7 1024 px, down-sampled to 256 \u00d7 256 px. Three augmentation strategies were used, including training on 128 \u00d7 128 px patches, flipping patches, and applying random gamma correction. The training set size was reduced to \u224810k images. Training was done using batches of 50 images, with Adam optimization algorithm and minimizing MSE until convergence of SSIM on a validation set with 1,000 images. Image quality was controlled by changing the number of feature channels. The study evaluated three configurations for image quality with 16, 32, and 64 channels. Three standard codecs were considered: JPEG, JPEG2000, and BPG. The effective payload of the codecs was measured to ensure a fair comparison. Rate-distortion trade-off was assessed using 3 datasets for final evaluation. The final evaluation included 3 datasets: raw images from 4 different cameras, images from the CLIC professional validation subset, and images from the standard Kodak dataset. Image quality was measured using PSNR, SSIM, and MS-SSIM. Rate-distortion curves were shown for the clic and raw datasets. The DCN model outperforms JPEG and JPEG2000, approaching BPG in image quality. Processing times for DCN on various platforms are reported, showing negligible inference times on GPU-enabled platforms. The inference time is negligible for DCN on GPU-enabled platforms, with over 100 fps for 512 \u00d7 512 px images and over 20 fps for 1920 \u00d7 1080 px images. The FSE codec outperforms arithmetic coding, and channel EC can be parallelized. Processing times for standard codecs on a CPU are also provided. The BPG codec with 4 parallel threads has varying processing times, ranging from 2.4 s to 0.72 s. In comparison, deep learned codecs can take minutes to process small images. The fastest learned codec runs at approximately 100 fps on a GPU-enabled desktop computer. The study focuses on photo manipulation detection in real-world conditions where images are analyzed after transmission through a lossy dissemination channel. Forensic analysis may fail after transmission through lossy dissemination channels. Various versions of channels are considered, including standard JPEG compression and different codecs optimized with the FAN. The study analyzes 128 \u00d7 128 px patches without down-sampling to isolate codec impact. Six benign post-processing operations are examined, such as sharpening, resampling, Gaussian filtering, and JPEG compression. These operations preserve image content but alter low-level traces that can reveal forgery. The study examines various post-processing operations on image patches, including Gaussian filtering, JPEG compression, AWGN noise, and median filtering. The operations are difficult to distinguish visually from native camera output. The FAN model is used for image forensics, with 1.3 million parameters, to classify RGB image patches. The entire workflow is jointly trained and optimized for classification of manipulation classes. The study examines post-processing operations on image patches, including Gaussian filtering, JPEG compression, AWGN noise, and median filtering. The FAN model is used for image forensics with 1.3 million parameters to classify RGB image patches. The FAN model is trained to minimize a cross-entropy loss, while the DCN model minimizes a combination of fidelity/entropy loss. The models are trained on native camera output using differentiable dJPEG model for JPEG compression. The study utilized the DNet pipeline for Nikon D90, training on native camera output. RGB patches were randomly sampled from full-resolution images, with validation on 30 images. Training involved batches of 20 images for 2,500 epochs, with learning rate decay. Results show a trade-off between bpp, SSIM, and manipulation detection accuracy, comparing JPEG compression, pre-trained DCN models, and fine-tuned DCN models with different regularization strengths. Fine-tuned models are labeled with a delta in the auxiliary metric, and the text is colored to indicate deterioration or improvement. JPEG delivers the worst trade-off due to better image fidelity of the DCN codec and presence of JPEG compression. Fine-tuning the DCN model leads to an increase in payload requirements, minor quality improvement, and gradual manipulation detection accuracy increase. Fine-tuning the DCN model results in increased payload requirements, minor quality improvement, and gradual manipulation detection accuracy increase, especially at low JPEG quality levels. The rate-distortion trade-off remains stable with DCN fine-tuning, except for the smallest regularization parameter showing better accuracy but worse fidelity. The behavior of the models is examined by analyzing frequency attenuation patterns through FFT spectra of compressed images compared to uncompressed images. Results show gradual attenuation of high frequencies in pre-trained models, which start to be retained in fine-tuned models with decreasing regularization parameter \u03bb c. Increasing emphasis on accuracy through cross-entropy loss gradually changes the attenuation patterns. The cross-entropy loss increases importance and changes attenuation patterns in compressed images. Frequencies are irregularly selected, with some bands emphasized by the codec. Compressed images show no obvious artifacts, with the main change being an increase in entropy. The proposed approach can aid in pre-screening online photographs. Further research is required for validation. Further research is needed to improve model generalization in pre-screening online photographs. Fine-tuning bias towards the secondary image dataset was observed, leading to occasional artifacts and deterioration of the rate-distortion trade-off. Additional experiments skipping photo acquisition and fine-tuning directly on the original training set were conducted. The optimized codec showed artifact-free performance on all test sets, but with reduced performance due to a smaller training set. Existing forensics models are sensitive to data distribution, requiring further work on establishing universal training protocols. Future work includes exploring short fine-tuning and new transfer learning protocols for improved generalization in forensic tasks. Lossy image codecs can be optimized to detect photo manipulation by retaining subtle low-level traces, even at very low bit-rates. The inclusion of high frequencies alone is not enough, as the models learn complex frequency patterns for authentication. Additional pre-screening may be necessary, such as analyzing sensor fingerprints or identifying synthetic content. The proposed approach improves manipulation detection accuracy at low bit-rates. It is valuable for online media platforms needing to optimize bandwidth. Issues with standard soft quantization can be addressed by increasing numerical precision or adding conditional statements in the code. In graph-based machine learning frameworks like Tensorflow, a t-Student kernel was used to increase computation precision to 64-bits, eliminating issues in experiments and improving entropy estimation accuracy. The best results were observed with a t-Student kernel with 50 degrees of freedom and bandwidth \u03b3 = 25. Different codebooks and entropy regularization strengths were experimented with, showing changes in quantized latent representation with codebook size. The binary codebook was found to be sub-optimal. The binary codebook was found to be sub-optimal, limiting image quality as feature channels increase. Using entropy-based regularization effectively shaped the quantized latent representation, eliminating the need for other normalization techniques. A single scalar multiplication factor was used for scaling the distribution, with \u03bb H = 250 for all models. Visual comparisons showed the impact of weak and strong regularization on the quality of the baseline low-quality codec. Feature channels were independently entropy-coded in a rudimentary bit-stream structure for successful decoding. Feature channels are entropy-coded independently and can be accessed randomly after decoding. Different fine-tuned models with varying emphasis on manipulation detection show significant benefits. The trade-offs in image compression and forensic analysis performance are visualized, highlighting the impact of compression and fine-tuning settings on manipulation detection accuracy. The achieved manipulation detection accuracy varies for different quality levels in JPEG codec and DCN models. Fine-tuning shows improvement compared to baseline models, with some fluctuations in accuracy. Frequency attenuation/amplification patterns in FFT domain reveal complex behavior beyond high-frequency content inclusion. The study shows stable trajectory patterns in high-frequency content despite different regularization strengths. Fine-tuning DCN models with weak cross-entropy objectives did not improve manipulation detection accuracy. Imaging artifacts from digital camera acquisition lead to biases in the codec. The study shows stable trajectory patterns in high-frequency content despite different regularization strengths. Fine-tuning DCN models with weak cross-entropy objectives did not improve manipulation detection accuracy. For NCO (raw test set), there is an improvement in image quality and bitrate. The kodak set shows mostly unaffected quality with an increased bitrate. The clic set experiences minor quality loss and occasional artifacts. Example images from all test sets are compressed with baseline and fine-tuned models, ordered by SSIM deterioration due to weak fine-tuning. Color artifacts and distortions are observed in the images. In the kodak set, the worst image quality was observed for kodim05, but no artifacts were seen. Additional experiments were conducted by skipping photo acquisition and finetuning directly on mixed natural images (MNI). Images in this dataset tend to have more details and depict objects at a coarser scale. Adjustments in manipulation strength were made to maintain visual similarity between photo variations. In experiments with image manipulation, various techniques were applied such as weaker sharpening, Gaussian filtering, down&up-sampling, Gaussian noise, and JPEG quality adjustments. Results showed improved manipulation detection accuracy, especially for fine-tuned DCN models. Confusion mainly occurred between native, sharpen, and awgn classes due to subtle differences. Fine-tuned models reached around 86% accuracy, with negligible improvement for high-quality models. The fine-tuned DCN models show subtle changes in rate-distortion behavior, with most models being equivalent except for the weakest regularization. No obvious artifacts were observed, except for one outlier image. Frequency attenuation patterns show similar behavior, with more subtle changes on MNI. Additional difference plots highlight visible changes compared to baseline and weakly fine-tuned models. In this study, two classes of images were considered: native camera output (NCO) and mixed natural images (MNI) with different pixel distributions. DCN models were pre-trained on a large MNI dataset and fine-tuned on NCO or a subset of MNI. Testing was done on various sets of images, revealing limited generalization of FAN models to different pixel distributions. Additional experiments were conducted to quantify this phenomenon. The study ran additional experiments to quantify the limited generalization of FAN models to different pixel distributions. Results showed that models trained on NCO or MNI datasets had reasonable generalization within their respective datasets but performed poorly when the data distribution changed. There was also a bias towards images down-sampled to the same resolution as the training data, with a consistent difference of 5.2-6% based on linear fit analysis."
}