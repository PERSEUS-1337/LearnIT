{
    "title": "Bke4KsA5FX",
    "content": "Generative models for source code use a graph to represent the output, combining grammar-driven expansion with neural message passing. The model can generate semantically meaningful expressions, outperforming strong baselines. Understanding and generating programs is crucial for procedural artificial intelligence and software engineering tools. Source code combines formal semantics with aspects of natural languages for communication. Generative models of source code focus on combining formal semantics and natural language aspects for communication among developers. While early works in the field have shown success in applying natural language processing to source code, recent methods have demonstrated achievements in handling both modalities. However, current generative models often only focus on one modality at a time, leading to limitations in distinguishing between likely and unlikely code outputs. In this work, the focus is on improving generative code models by incorporating structured information available at generation time. The approach involves lifting grammar-based tree decoder models into the graph setting to model diverse relationships between elements of the generated code. This method aims to address challenges such as generating semantically relevant programs with consistent variable usage. The generative code models incorporate structured information to model relationships between elements of the generated code. The syntax tree is augmented with additional edges to denote known relationships, and neural message passing is used to compute precise representations of intermediate states. This approach differs from sequential generative models and focuses on a graph-based generative procedure for highly structured objects. ExprGen is introduced as a new code generation task within this framework. The generative code models incorporate structured information to model relationships between elements of the generated code. The most general form of the code generation task is to produce a (partial) program in a programming language given some context information. Context information can be natural language, input-output examples, or partial program sketches. Early methods generate source code as a sequence of tokens and sometimes fail to produce syntactically correct code. More recent models focus on generating small, semantically complex expressions conditioned on source code context. In recent models, the issue of producing syntactically correct code is addressed by using the target language's grammar to generate abstract syntax trees (ASTs). The AST generation approach involves constructing the AST sequentially by expanding one node at a time using production rules from the programming language grammar. This simplifies the code generation task to a sequence of classification problems, where the appropriate production rule is chosen based on context information and the partial AST generated so far. The order of sequence expansion is fixed to always expand the left-most, bottom-most nonterminal node. The ExprGen task involves filling in code within a hole of an existing program, similar to auto-completion in a code editor but generating whole expressions. It focuses on Boolean, arithmetic, string types, or arrays, excluding other types or project-specific APIs. The task involves generating code expressions within a hole in an existing program, focusing on Boolean, arithmetic, string types, or arrays. The context includes pre-existing code and variables in scope to guide the decoding process. The method is not limited to code generation and can be extended to other tasks and domains. Two design choices need to be made to tackle the code generation task. The task involves generating code expressions within a hole in an existing program, focusing on Boolean, arithmetic, string types, or arrays. Two design choices need to be made: encoding the code context and constructing a model to learn well. The decoder model follows a grammar-driven AST generation strategy, with a core difference in computing the representation of the node to expand. The representation of a node to expand is constructed using a log-bilinear model. Different approaches consider relationships to parent nodes or the last expansion step. Gated recurrent units are used to encode these relationships. A graph is proposed to structure information flow, utilizing attribute grammars from compiler theory. Each node in the AST is associated with inherited and synthesized information nodes. In classical compiler theory, inherited attributes contain information like declared variables and types, while synthesized attributes provide a summary of a subtree. To apply this concept to deep learning, attributes are represented by distributed vectors and neural networks are trained to compute them. The method for getRepresentation from Alg. 1 involves a deterministic procedure that transforms a partial AST into a distributed vector representation. The method in Alg. 1 involves a deterministic procedure that transforms a partial AST into a graph by adding edges encoding attribute relationships. Programs are represented as graphs with nodes representing AST nodes or attribute nodes, connected by edges that show information flow. Edge types represent different syntactic or semantic relations. Functions like parent(a, v) and lastSibling(a, v) are used to navigate the AST. The AST derivation process involves expanding nodes using production rules, with attribute relationships shown in the constructed graph. Attribute nodes overlay corresponding AST nodes, indicating inherited and synthesized attributes. Edges in neural attribute grammars are discussed for information flow. Neural attribute grammars (NAG) use different edge types to connect attribute nodes in the AST derivation process. Child (red) edges connect inherited attributes to children, Parent (green) edges connect synthesized attributes to AST parents, and NextSib (black) edges connect attributes to next siblings. These connections are crucial for information flow in syntax-driven decoders. The synthesized attribute node is connected to the inherited attribute node of its next sibling. NextUse edges connect variable attribute nodes to their next use. NextToken edges connect terminal nodes to the next token in the program text. InhToSyn edges connect inherited attributes to synthesized attributes. In FIG0, attribute nodes are connected to synthesized attribute nodes, aiding in training. The panels show when attribute node representations are computed and added to the graph. For instance, in the second step, attributes for the terminal token i are computed from its AST parent Expr and last variable use. The neural attribute representation h v of an attribute node v is computed using incoming edges and a state update function from GGNN. The state update function in Gated Graph Neural Networks (GGNN) transforms attribute representations at edge sources, aggregates them, and combines them with node labels using a learned function. Node representations are computed in a specific order to ensure all necessary attributes are already calculated. The root inherited attribute is initialized with the encoder's context information representation. Production rules can be selected easily in this process. In selecting production rules, variables, and literals, a classification approach is used to mask out invalid choices. Nonterminal nodes are processed with inherited attributes, and variables are chosen from a set using representations. The model always selects variables from the in-scope set. The generation model ensures it only predicts variables from the in-scope set, combining a vocabulary of common literals with UNK tokens for unknown literals. A pointer network copies tokens from the context to pick a literal at a node, potentially producing an UNK literal. This is achieved by learning functions to score tokens from the vocabulary and copying tokens from the context, allowing the model to approximate the desired probability of a literal given the node. The generation model predicts variables from a set of common literals using a pointer network to copy tokens from the context. The training procedure involves combining an encoder to initialize representations in a syntax graph, allowing efficient training of a graph neural network. The code for this is available on GitHub. The system is trained end-to-end without pre-trained components, with improvements including an attention mechanism and additional information for Child edges by changing the setup to require additional labels for some edge types. In extending the graph model, additional labels for edge types are introduced, including labeled edges for Child with tuples indicating production and child index. The pickProduction function now considers available variables, and the representation of all variables in scope is taken into account in e(hv) by implementing a max pooling operation. The text discusses source code generation using different methods, such as generating code as sequences of tokens or trees. Modern models can generate programs that look natural but may not always follow semantic rules. Tree-based generative models vary in how they decide which expansion rule to use next, with some considering the parent node's representation and others suggesting more information like nearby tokens. The text discusses different methods for source code generation, including tree-based models that vary in how they decide on expansion rules. Some models consider the parent node's representation, while others suggest considering nearby tokens for more information. The text discusses a method for source code generation that updates a single state through the generation procedure, providing access to the full generation history and parent node representation. Previous work has not considered a task where a generative model fills a hole in a program with an expression, unlike the task of BID21 which focuses on filling holes in sequences of API calls. ExprGen requires handling arbitrary code in the context. ExprGen requires handling arbitrary code in the context to build complex expressions from a small set of operators. A dataset was collected from 593 highly-starred open-source C# projects on GitHub, removing duplicates. All C# files were parsed to identify expressions of numeric, Boolean, and string types. Samples were extracted by removing expressions and creating abstract syntax trees, resulting in 343,974 samples overall. The dataset for ExprGen was collected from 593 highly-starred open-source C# projects on GitHub, resulting in 343,974 samples. The data was split into four sets, including a \"test-only\" dataset and training-validation-test sets. The decoder uses a grammar with 222 production rules observed in the ASTs of the training set. Two models are considered for encoding context information: a bi-directional recurrent neural network and a GRU. The text discusses the use of GRU and bi-directional two-layer GRU to encode tokens before and after a \"hole\" in generating an expression. It also explains how a representation for each variable in scope is computed using a graph neural network approach introduced by BID1. The text introduces a graph neural network approach to capture context information for variables in scope. Comparisons with baseline decoders show that the model outperforms simpler sequence decoders. Additionally, a model called Tree, which uses only Child edges without labels, is discussed as an evolution of previous models. The curr_chunk discusses the use of GRU in ASN, an extension of the Tree model with edge labels for encoding productions. It also mentions the use of a new NextExp edge in Syn for action flow. The re-implementations improve variable selection mechanisms for generated programs. This contrasts with prior work that uses a copying mechanism from the context. The authors discuss the limitations of the PHOG BID5 language model in generating valid expressions based on code context. They suggest that extending the model to consider more context and analyze variable choices would improve results.Metrics for evaluating the ability of a model to generate valid expressions are considered for the ExprGen task, which requires a conditional language model of code. The evaluation metrics for the ExprGen task include per-token perplexity, well-typed generated expressions, and ground truth expression generation frequency. The results show that the graph encoder architecture is best-suited for the task, with all models able to generate syntactically valid code. The different encoder models perform differently on semantic measures such as well-typedness and retrieval of ground truth expressions. Most type errors are due to usage of an \"UNK\" literal. Results show better semantic results with more information about partially generated programs. Transferring a trained model to unseen projects worsens results. The NAG model seems to perform best and is least impacted by transfer. The ExprGen task is challenging even for the strongest models, achieving no more than 50% accuracy. The task is challenging even for the strongest models, with no more than 50% accuracy. Classical logico-deductive program synthesis systems cannot solve it due to imprecise code context. Machine learning systems show promise in solving the task, as demonstrated by the predictions of the evaluated models. The G \u2192 ASN model struggles to recognize when variables have already been used, leading to incomplete suggestions. Another example shows models learning patterns but failing to produce correct outputs. A generative code model uses known semantics to guide program generation by creating a graph representation and using graph neural networks for precise guidance. The presented model utilizes known semantics to guide program generation, particularly in scenarios like program repair and code review. It can generate small, semantically interesting expressions from imprecise context information and has potential applications in semantic parsing, neural program synthesis, and text generation. Sample snippets from the training and test sets for the ExprGen task are provided, with highlighted expressions to be generated."
}