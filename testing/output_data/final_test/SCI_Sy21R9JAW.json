{
    "title": "Sy21R9JAW",
    "content": "In this work, the flow of information in Deep Neural Networks (DNNs) is analyzed through four gradient-based attribution methods. Conditions of equivalence and approximation between these methods are formally proven, leading to a unified framework for comparison and implementation. A novel evaluation metric called Sensitivity-n is proposed and tested on various datasets for image and text classification using different network architectures. In this study, the focus is on analyzing the flow of information in Deep Neural Networks (DNNs) using various attribution methods to assign relevance to input features. The goal is to enhance interpretability and reliability of DNNs, especially in critical domains like autonomous driving, medical applications, and finance. The attribution method aims to determine the contribution of each input feature to the output of the network for a given target neuron in a classification task. The attribution method aims to assign relevance to input features in Deep Neural Networks (DNNs) for a classification task. Attribution maps display feature contributions as heatmaps, with red indicating positive contributions and blue indicating suppressive effects. Previous works have addressed the problem of finding attributions for deep networks, but a comprehensive comparison is lacking due to different formulations and lack of compatibility with various DNN architectures. New attribution methods have been developed in recent years, but there is a need for a better evaluation framework. The contribution of this work is twofold: 1. It proves that LRP BID2 and DeepLIFT (Rescale) BID20 can be reformulated as computing backpropagation for a modified gradient function, allowing for a unified framework of gradient-based attribution methods. 2. It introduces the definition of Sensitivity-n, comparing various methods on datasets and architectures. The empirical results support theoretical findings and suggest directions for using the analyzed attribution methods. Perturbation-based methods calculate feature attribution by altering input features and measuring the output difference. These methods have been applied to CNNs for image classification, but can be slow due to the nonlinear nature of DNNs. In the context of analyzing attribution methods, perturbation-based methods alter input features to measure output differences. The occluding method by BID30 is used as a benchmark, replacing one feature at a time with a zero baseline to observe the effect on the target output. Different sizes of occlusions influence the results, with larger patches focusing on the main subject in the image. Integrated Gradients and Gradient * Input are methods for computing attributions in neural networks. Integrated Gradients calculates the average gradient along a linear path, while Gradient * Input computes a single derivative at a given input. These methods aim to improve the sharpness of attribution maps by computing partial derivatives of the output with respect to input features. Integrated Gradients computes average gradient along a linear path from a user-defined baseline to the input. The attributions sum up to the target output minus the output at the baseline. Layer-wise Relevance Propagation assigns relevance starting from the output layer. Layer-wise Relevance Propagation redistributes prediction scores layer by layer until reaching the input layer. The final attributions are defined as LRP, with a propagation rule called -LRP. DeepLIFT BID20 also proceeds in a backward fashion, assigning attributions to each unit. DeepLIFT assigns attributions to each unit in a backward fashion, similar to LRP. The relevance propagation is determined by reference values for hidden units, and the attributions at the input layer are calculated using a rescale rule. The method was designed to satisfy Completeness and uses a specific rule for propagation. The \"Reveal-Cancel\" rule BID20 is not considered here. Attribution methods applied to an Inception V3 network for image classification show gradient-based methods have higher local variance compared to perturbation-based methods. Saliency maps BID22 use absolute value of partial derivatives for attributions, but this method is not used for comparison in the paper. In Section 4, it is shown that Deep Taylor Decomposition and other methods designed for specific architectures may not hold assumptions for certain tasks. Methods like Gradient * Input and Integrated Gradients are computed based on partial derivatives of the target output with respect to each input feature. LRP and DeepLIFT can also be computed using the chain rule for gradients in a DNN with linear transformations and nonlinear mappings. In a neural network, the chain rule along a path connecting units involves the product of partial derivatives of linear and nonlinear transformations. The derivative of the output of one unit with respect to another is computed as the sum of contributions over all connecting paths, with the option to use zero weight for non-existing paths. The text discusses the equivalence of LRP and DeepLIFT (Rescale) methods in neural networks, providing propositions and proofs in Appendices A.1 and A.2. The methods involve feature-wise products of inputs and modified partial derivatives, with examples of attribution maps shown in TAB0. The text discusses the equivalence of LRP and DeepLIFT methods in neural networks, with examples of attribution maps shown in TAB0. The formulation allows for immediate applicability to existing models in TensorFlow without the need for custom layers or operations. Listing 1 provides an example of how to implement these methods using gradient override in TensorFlow. The text discusses the equivalence of LRP and DeepLIFT methods in neural networks. Proposition 3 states that LRP is equivalent to Gradient * Input for ReLUs, and to DeepLIFT with a zero baseline for networks without additive biases and specific nonlinearities. This relationship is derived from previous propositions and observations. The propagation of the baseline in neural networks produces a zero reference value for all hidden units, proving the second part of the proposition. For ReLU and Tanh, the average gradient of the nonlinearity in [0, z] is crucial for meaningful attributions with LRP. However, with Sigmoid or Softplus, LRP fails to produce meaningful attributions due to extremely large values of g LRP (z) for small z values. This causes attribution values to concentrate on a few features. DeepLIFT and Integrated Gradients are related methods for computing feature attributions in neural networks. DeepLIFT approximates the average partial derivative of each feature in a single step, while Integrated Gradients computes it as the input varies from a baseline to its final value. Empirically, DeepLIFT is a good approximation of Integrated Gradients, especially with simple models. However, DeepLIFT diverges and fails with Recurrent Neural Networks with multiplicative interactions. When applied to RNNs with multiplicative interactions, DeepLIFT fails to satisfy Completeness, unlike Integrated Gradients. The methods are computed from a quantity dependent on model weights and architecture, multiplied by the input itself. Occlusion-1 can also be seen as the input multiplied by the average value of partial derivatives. The multiplication with the input in attribution methods distinguishes between global and local attribution methods. In a linear model example, the input is multiplied by coefficients to predict the total capital. In a linear model example, the input is multiplied by coefficients to predict the total capital. C = 1.05 \u00b7 x 1 + 10 \u00b7 x 2. Local attributions show investing in x 2 yields ten times more return than x 1. Global attribution for x 1 is larger than x 2 when x 1 = $100,000 and x 2 = $1,000. The global attribution for x 1 is larger than x 2 in this case, opposite to the local model results. Gradient * Input is used as a global attribution method, equivalent to other methods. Global attribution methods are listed in TAB0. Local attribution methods, like Saliency maps, aim to explain how input changes affect output. Global and local attributions converge only in linear models. One practical application of attributions methods is generating adversarial perturbations to disrupt output. Evaluating these methods is challenging due to difficulty in distinguishing model errors from attribution method errors. Qualitative evaluation based on inspection of attribution maps introduces bias towards explanations aligning with human expectations. This bias penalizes methods that may better reflect network behavior. In order to develop better quantitative tools for evaluating attribution methods, defining the goal of an ideal method is crucial. A comparison of attribution maps on MNIST shows differences between Integrated Gradients and Occlusion-1. To test the hypothesis, a variation of the region perturbation method is applied by removing pixels based on the attribution maps' ranking. The attribution maps ranking pixels based on impact on target output. Occlusion-1 highlights individual features, while Integrated Gradients captures effects of multiple features together. Integrated Gradients shows stronger target output variation after removing 20+ pixels. Sensitivity-n is a property that tests the impact of removing subsets of features on the output score. It is a generalization of Completeness or Summation to Delta, with Occlusion-1 satisfying Sensitivity-1 and Integrated Gradients satisfying Sensitivity-N. LRP satisfies Sensitivity-N if the conditions of Proposition 3-(ii) are met. However, no methods in TAB0 can satisfy Sensitivity-n for all n. All attribution methods in TAB0 satisfy Sensitivity-n for all values of n when applied to a linear model or a model that behaves linearly for a selected task. This implies that there are not enough degrees of freedom to capture nonlinear interactions when only assigning a scalar attribution to each feature. The text discusses how different attribution methods perform in satisfying Sensitivity-n for all values of n. It mentions using Pearson correlation coefficient to measure the correlation between the sum of attributions and the variation in the target output. Sampling is done to evaluate the methods on various tasks, not just limited to images. The text discusses testing attribution methods on various tasks, including image classification on MNIST and CIFAR10, sentiment analysis on IMDB dataset, and Inception V3 architecture on ImageNet samples. The goal was not to achieve state-of-the-art performance but to show applicability across different models. Details of the architectures can be found in Appendix C. The text discusses testing attribution methods on various tasks, including image classification on MNIST and CIFAR10, sentiment analysis on IMDB dataset, and Inception V3 architecture on ImageNet samples. The goal was not to achieve state-of-the-art performance but to show applicability across different models. Details of the architectures can be found in Appendix C. In a case study, a DNN degenerates into nearly-linear behavior, demonstrating the effects of Proposition 4. Input samples may contain negative evidence, supported by results showing an increase in target output when negative evidence is occluded. Gradient-based methods on complex models like Inception V3 show low accuracy in predicting attribution sign, leading to noisy heatmaps. Occlusion-1 better identifies important features, satisfying Sensitivity-1. The correlation decreases as n increases in all experiments. Integrated Gradients, DeepLIFT, and LRP capture global nonlinear effects and cross-interactions between features. Occlusion-1 is slower than gradient-based methods. Gradient * Input approximates Occlusion-1 behavior in some cases. Integrated Gradients and DeepLIFT have high correlation, with DeepLIFT being a faster approximation. In presence of multiplicative interactions between features, DeepLIFT formulation should be avoided. LRP is equivalent to Gradient * Input with ReLUs but fails with Sigmoid or Softplus. Adjusting the propagation rule for multiplicative interactions allows LRP to be applied to LSTM networks. All methods are equivalent when the model behaves linearly. In this work, Gradient * Input, -LRP, Integrated Gradients, and DeepLIFT (Rescale) were analyzed from theoretical and practical perspectives. Despite different formulations, these methods are strongly related, with conditions of equivalence or approximation between them. By reformulating -LRP and DeepLIFT (Rescale), implementation becomes as easy as other gradient-based methods. A metric called Sensitivity-n was proposed to uncover properties of existing attribution methods and suggest research directions for more general ones. The DeepLIFT multiplier for nonlinear operations with single inputs is discussed, while operations with multiple inputs are not addressed. Linear models and their equivalence to various methods are also explored in the context of different attribution methods. In the linear case, Gradient * Input, -LRP, DeepLIFT with zero baseline, and Integrated Gradients are equivalent. Occlusion-1 and other methods satisfy Sensitivity-n for all n in linear models. In non-linear models, if two features x i and x j exist such that Sensitivity-1 or Sensitivity-2 is violated, it implies that the feature plays a role in the model's output. Current neural network architectures make it difficult to test a process with and without a specific feature. To simulate the absence of a feature, a baseline like a black image or zero input is used to represent the absence of information. In non-linear models, a baseline like a black image or zero input is used to represent the absence of information. Choosing a baseline in the input space creates ambiguity between a valid input and a missing feature placeholder. Marginalizing over features can simulate their absence, but it is slow and only marginally improves over a predefined baseline. The method by BID30 is preferred over BID31 due to its applicability to images with strong correlation between contiguous features. In non-linear models, a baseline like a black image or zero input is used to represent the absence of information. The choice of zero for the baseline in neural networks is justified by the observation that the output is often near zero even with different biases. This makes the choice of zero as a baseline reasonable, although arbitrary. The MNIST dataset was pre-processed to normalize input images between -1 and 1 for training both a DNN and a CNN with different activation functions. The study tested attribution methods with four activation functions on different architectures using Adadelta for training. The networks achieved final test accuracies, with CIFAR-10 dataset pre-processed to normalize input images and MNIST dataset trained with ReLU achieving 80.5% accuracy. Gradient-based methods computed pixel attributions by summing color channels, while Occlusion-1 involved setting all color channels to zero simultaneously. The study utilized a pre-trained Inception V3 network on a test dataset of 1000 ImageNet-compatible images, achieving 95.9% accuracy. Shallow MLP and LSTM networks were trained on the IMDB dataset for sentiment analysis, with a final test accuracy of 87.3%. ReLU nonlinearities were used for hidden layers, and Adam optimization with early stopping was employed. The study involved using gradient-based methods to compute word attributions in architectures. Occlusion-1 was performed by setting embedding vector components to zero for each word tested."
}