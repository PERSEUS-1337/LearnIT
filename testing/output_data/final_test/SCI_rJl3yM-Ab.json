{
    "title": "rJl3yM-Ab",
    "content": "In this paper, the authors propose two re-ranking methods, strength-based and coverage-based, to aggregate evidence from multiple passages for answering open-domain questions. Their model outperformed existing QA models on three public datasets. The model achieved state-of-the-art results on three public open-domain QA datasets by effectively using evidence from large knowledge sources like Wikipedia and structured knowledge bases. Recent work in open-domain QA focuses on using unstructured text from the web to build machine comprehension models, combining information retrieval and reading comprehension models to infer answers. In this paper, a method is proposed to improve open-domain QA by aggregating evidence from multiple passages. The correct answer is often suggested by more passages repeatedly, enhancing evidence for the correct answer. Additionally, questions covering multiple answer aspects may require evidence from multiple passages. The method proposed in this paper aims to enhance open-domain QA by aggregating evidence from multiple passages. Multiple passages can provide complementary evidence to infer the correct answer to a question, as seen in the example of \"Galileo Galilei\" versus \"Isaac Newton\". This approach can lead to more accurate answers in open-domain QA. The method proposed aims to improve open-domain QA by aggregating evidence from multiple passages. This evidence aggregation is formulated as an answer re-ranking problem, where global information from multiple pieces of textual evidence is efficiently incorporated for each answer candidate without significantly increasing complexity. The method proposed aims to improve open-domain QA by aggregating evidence from multiple passages through re-ranking answer candidates based on strength and coverage of evidence. The proposed method aims to improve open-domain QA by aggregating evidence from multiple passages through re-ranking answer candidates based on strength and coverage of evidence. The approach involves concatenating passages containing the answer to create a new context for better question entailment. Two re-rankers are used for evidence aggregation, with the second re-ranker outperforming the first on two out of three public datasets. The method leads to state-of-the-art results on three different datasets. The proposed method improves open-domain QA by aggregating evidence from multiple passages through re-ranking answer candidates. It achieves state-of-the-art results on three datasets and outperforms previous methods by a large margin, with up to 8% improvement on F1 scores. The approach involves using an IR model to find relevant web passages and a reading comprehension model to extract answers. This differs from standard reading comprehension tasks where a single fixed passage is provided. The proposed method improves open-domain QA by aggregating evidence from multiple passages through re-ranking answer candidates using an existing RC model called R 3. The goal is to rank the top-K answer candidates generated by a baseline QA system so that the top-ranked candidates are more likely to be the correct answer. The method enhances open-domain QA by re-ranking answer candidates based on evidence strength and coverage. This approach aims to prioritize answers with strong evidence from multiple passages, improving the accuracy of the top-ranked candidates. To improve open-domain QA, answer candidates are re-ranked based on evidence strength and coverage. The method involves counting occurrences of each answer in top-K answer spans and selecting the answer with the highest count. Another approach involves summing up probabilities of answer spans referring to the same answer, with the answer having the highest probability being the final prediction. These methods do not require training and have negligible time complexity at test time. At test time, strength-based re-ranking has negligible time complexity. A coverage-based re-ranker is proposed to rank answer candidates based on how well the union of their evidence covers the question. This involves concatenating passages containing the answer into a \"pseudo passage\" to measure how well it entails the answer for the question. The coverage-based re-ranker ranks answer candidates by concatenating passages containing the answer into a \"pseudo passage\" to measure how well it entails the answer for the question. The coverage-based re-ranker measures how well a union of passages matches each aspect of a question using word-by-word attention and comparison modules. Matrices are created for answer candidates, questions, and passages, which are then fed into a bidirectional LSTM. The question representation is enhanced by concatenating the answer candidate. The text discusses how attention vectors are computed to match aspects of a question with a passage. It involves creating matrices for answer candidates, questions, and passages, enhancing the question representation, and computing attention weights for each hidden state. This process aims to find answer-related information from the passage. The text discusses computing attention vectors to match aspects of a question with a passage. It involves creating matrices for answer candidates, questions, and passages, enhancing question representation, and computing attention weights for each hidden state. This process aims to find answer-related information from the passage. The matching function DISPLAYFORM3 is used to check if aspects in the question can be matched by the union passage. Matching representations are integrated into aspect-level matching representations 3 M \u2208 R 2l\u00d7(A+Q) through non-linear transformation. Next, the entire question matching with the union passage is measured by considering the matching result at each aspect. The text discusses using bi-directional LSTM to aggregate aspect-level matching information by considering matching results at each aspect. Different methods, such as replacing answer spans with special tokens, were explored but concatenation was found to be more effective. LSTM is used to capture conjunction information among aspects. The text explores using low-rank tensor inspired neural architectures for matching representation and re-ranking based on the entire matching representation. Matching representations are transformed into scalar values for ranking using parameters W r and b r, followed by a non-linear transformation to get hidden representation R. Finally, the transformed matching representations are mapped into scalar values through parameters w o. The text discusses mapping transformed matching representations into scalar values using parameters w o and w o, with o representing the normalized probability for candidate answers. The objective function involves using KL distance to determine ground-truth answers. By combining coverage-based and strength-based re-rankers, the model aims to address both difficult and common cases in open-domain QA datasets without further training. The study combines coverage-based and strength-based re-rankers to improve open-domain QA performance on datasets like Quasar-T, SearchQA, and TriviaQA without additional training. The datasets consist of passages retrieved from search engines, with human performance evaluated in an open-book setting. The human subjects in open-book setting had access to passages retrieved by the IR model to find answers. Various baseline models were evaluated for reading comprehension, including GA, BiDAF, AQA, and R3. TriviaQA does not provide a leaderboard under the open-domain setting. The TriviaQA open-domain setting lacks public baselines, so comparisons are made with the R3 baseline. A pre-trained R3 model is used to generate top 50 candidate spans for training, development, and test datasets. Groundtruth answers not in candidates are manually added. Adam BID19 optimizes the coverage-based re-ranker model. Word embeddings are initialized with GloVe BID23 and remain static during training. Parameters like batch size, learning rate, and dropout probability are set, with candidate answer re-ranking options explored. In this section, results and analysis of different re-ranking methods on public datasets are presented. The evaluation focuses on RC models over filtered passages with guaranteed correct answers. Model R3 achieves competitive results on TriviaQA data, with F1 56.0, EM 50.9 on Wiki domain and F1 68.5, EM 63.0 on Web domain. The performance of the models, including the full re-ranker and combination of re-rankers, outperforms previous best results by a large margin, especially on Quasar-T and SearchQA datasets. The model also surpasses human performance on SearchQA. The coverage-based re-ranker consistently performs well on all datasets, slightly lower than the strength-based re-ranker on SearchQA. The BM25-based re-ranker improves F1 scores compared to the R3 model but is still lower than the coverage-based re-ranker with neural network models. However, it sometimes gives lower EM scores due to its reliance on a bag-of-words representation, which limits its ability to consider context and model phrase similarities. Shorter answers are also preferred, impacting its performance. The coverage-based re-ranker outperforms the baseline in different lengths of answers and question types, showing improved F1 scores but sometimes lower EM scores due to the model's inability to consider phrase similarities. The re-ranking performance is decomposed based on answer lengths and question types, with results indicating better performance across various scenarios. The strength-based re-ranker (counting) and (probability) show improvement but less stability across datasets. The coverage-based re-ranker and strength-based re-ranker have similar trends, except for \"why\" questions where the strength-based re-ranker performs significantly worse. This is possibly due to non-factoid answers being less likely to have the same text spans predicted on different passages. The baseline R3 method shows potential improvement in re-ranking answers, with a high probability of correct answers being included in the top-K predictions. Comparing the upper bound performance of top-5 predictions and the re-ranking approach, there is a clear gap of about 10% on both datasets for both F1 and EM scores, indicating room for improvement. The selection of K for coverage-based re-ranker significantly impacts the recall of top-K predictions. Using a larger K increases the likelihood of good answers but also makes re-ranking harder. There is a trade-off between list coverage and re-ranking difficulty, highlighting the importance of selecting an appropriate K. The coverage-based re-ranker's performance improves with higher K values, but there is no significant advantage in using K=10 over K=5 due to increased computation cost. The strength-based re-ranker's performance is also evaluated with K values of 10, 50, 100, and 200. The strength-based re-ranker's performance is evaluated on the top-K predictions from the baseline, with K values of 10, 50, 100, and 200. Results show that the best performance is achieved when K=50, with a significant drop in performance when K increases to 200. An example from Quasar-T demonstrates how the re-ranker corrected a wrong answer predicted by the baseline, showcasing the effectiveness of the coverage-based re-ranker. The strength-based re-ranker improves performance by correcting wrong answers predicted by the baseline, as shown in an example from the Quasar-T dataset. The task of open domain question answering involves producing answers by utilizing resources such as documents, webpages, or structured knowledge bases. Recent efforts benefit from advances in machine reading comprehension. Recent advances in machine reading comprehension focus on search-and-read QA direction, using deep learning methods with a document retrieval module. Various approaches aim to improve passage retrieval and reduce noise in the ranking step. This work introduces the use of multiple passages for evidence aggregation in neural open-domain QA systems, specifically addressing the problem of text evidence aggregation. The problem addressed is modeling the relationship between questions and multiple passages in neural open-domain QA systems. Previous research did not utilize the information of union/co-occurrence of multiple passages in answer re-ranking. This work introduces re-ranking methods to improve passage retrieval and reduce noise in the ranking step. The approach discussed in the curr_chunk is a two-step extractor-reasoner model for open-domain QA, which extracts answer candidates and constructs hypotheses. It differs from previous approaches by not matching hypotheses to all sentences in the passage and using sentence embedding vectors for matching. The curr_chunk discusses different methods for answer extraction and aggregation in open-domain QA, including using sentence embedding vectors for matching, passing probabilities to the reasoner, and a two-step approach for generative QA. These methods aim to handle phrases as answers and multiple evidence aggregation. The curr_chunk discusses advancements in open-domain QA by combining evidence from multiple passages using two types of re-rankers. The results significantly improve performance on three QA datasets, but challenges remain in reasoning and commonsense inference. Future work will focus on addressing these harder problems. The curr_chunk discusses future directions for open-domain QA, aiming to generalize the proposed approach to more challenging multipassage reasoning scenarios. The work was supported by a DSO grant, and thanks were given to Mandar Joshi for testing the model on the TriviaQA dataset."
}