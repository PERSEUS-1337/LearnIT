{
    "title": "BJgedkStDS",
    "content": "Deep neural networks are powerful predictive models but lack the ability to detect when their predictions are incorrect. A new post-hoc framework called Relevant features based Auxiliary Cells (RACs) is proposed to efficiently detect natural errors by using consensus from RACs appended at hidden layers. This technique helps distinguish correctly classified inputs from misclassified ones, allowing for early termination of classification. Our technique, Relevant features based Auxiliary Cells (RACs), efficiently detects errors in image classification datasets like CIFAR10, CIFAR100, and Tiny-ImageNet. Results on CIFAR100 dataset with VGG16 network show 46% misclassified examples detected with 12% less energy usage compared to the baseline, while 69% are correctly classified. Machine learning classifiers excel in various tasks like object detection and speech recognition. Detecting incorrect predictions and having well-calibrated uncertainty is crucial for AI safety. Abnormal inputs include natural errors, adversarial inputs, and out-of-distribution examples. Different techniques have been proposed to distinguish abnormal samples, such as a threshold-based method suggested by Hendrycks & Gimpel (2017). Different techniques have been proposed to detect abnormal samples in machine learning classifiers. Lee et al. (2018) introduced a unified framework using hidden layer activations and a generative classifier to compute Mahalanobis distance for confidence scoring. Bahat et al. (2019) and Mandelbaum & Weinshall (2017) focus on detecting natural errors, with the latter using a distance-based method in the embedding space. Bahat et al. (2019) also utilize KL-divergence to distinguish correctly classified examples from adversarial and natural errors. To improve natural error detection, a Multi Layer Perceptron (MLP) is incorporated at the final layer to detect misclassifications. Prior works on error detection often overlook the energy and latency overheads introduced by detectors. Deeper networks consume more energy and latency during inference, making them less feasible for edge devices. Efforts towards energy-efficient deep neural networks have explored early exit techniques to bypass computations of latter layers for high confidence predictions. In efforts to improve natural error detection, various techniques have been explored, including adding binary linear classifiers at intermediate layers of deep neural networks. These techniques aim to enable energy-efficient inference while enhancing robustness in detecting abnormal samples, such as OOD examples and adversarial data. Trained DNNs use consensus between classifiers for early classification and error detection. Early exit prevents overfitting and saves energy. Additional linear classifiers identify inconsistent behavior for error detection. Training these classifiers is crucial for accurate and efficient error detection. Using class-specific binary classifiers trained on relevant feature maps at a hidden layer improves error detection capability compared to using a fully connected classifier. Training these binary classifiers encodes prior knowledge on the learned hidden feature maps, resulting in better detection capability with less parameter overhead. This approach also helps in early classification and energy saving in DNNs. The proposed framework utilizes class-specific binary classifiers at selected hidden layers to improve error detection capability in DNNs. Training these classifiers on relevant features encodes prior knowledge on hidden feature maps, leading to better detection with fewer parameters compared to using a fully connected classifier. These hidden layers, referred to as validation layers, contain maximal information for improved classification. The proposed framework utilizes class-specific binary classifiers at selected hidden layers to improve error detection capability in DNNs. Validation layers, known as Relevant feature based Auxiliary Cells (RACs), are used to detect natural errors and enhance network robustness. The consensus of RACs is used for early classification, improving energy efficiency. DNNs compute features at each convolutional layer, with some feature maps being highly relevant to specific classes. Linear classifiers are added to trained DNNs to enhance classification accuracy. The approach involves adding linear classifiers to trained DNNs by selecting relevant convolutional layers based on False Negative Rate (FNR) and True Negative Rate (TNR). Class-wise features are then calculated at these layers to train binary linear classifiers. The approach involves adding linear classifiers to trained DNNs by selecting relevant convolutional layers based on False Negative Rate (FNR) and True Negative Rate (TNR). Class-wise features are then calculated at these layers to train binary linear classifiers. The relevance score vector at layer l is computed by updating the feature-relevance matrix, which assigns class-wise relevance scores to every feature map. The feature-relevance matrix indicates the contribution of each feature map in activating the output node for a specific class. The relevance scores of feature maps are computed using Layer-wise Relevance Propagation (LRP), which determines the contribution of nodes in the network to the prediction for an input image. Relevance scores are back propagated based on the \u03b1\u03b2-decomposition rule with \u03b1 = 2 and \u03b2 = 1. The relevance score of each feature map at layer l is calculated by averaging the scores of nodes corresponding to that feature map. The relevance vector of a feature map is obtained by averaging the scores of nodes corresponding to that feature map at layer l. This feature-relevance matrix is used to determine relevant features for each class at validation layers in DNNs with RACs. Fig. 1 illustrates the conceptual view of DNNs with RACs, where output relevant features from two hidden layers are fed to RACs consisting of Binary Linear Classifiers. Binary Linear Classifiers (BLCs) in a Relevant Activation Classifier (RAC) correspond to unique classes in the dataset and are trained on relevant features. The output of a BLC indicates the probability of an instance belonging to a specific class. The confidence score of the RAC is determined by the maximum probability generated. An activation module uses the consensus of RACs and their confidence scores to decide on instance classification termination at the current layer. The training process for Relevant Activation Classifiers (RACs) involves determining relevant features for each class at validation layers using feature-relevance matrices. The relevant features are then used to train Binary Linear Classifiers (BLCs) for each class. The BLCs in an RAC can be trained in parallel as they are independent of each other. The training process for Relevant Activation Classifiers (RACs) involves determining relevant features for each class at validation layers using feature-relevance matrices. RACs can be trained in parallel as they are independent of each other. Algorithm 2 outlines the methodology to train an RAC at layer l. The overall testing methodology for DNNs with RACs is shown in Algorithm 3, which includes an early exit strategy for efficient classification and abnormal input detection. The methodology involves Relevant Activation Classifiers (RACs) producing a class label C test or No Decision (ND). RACs are monitored to determine early classification. If RACs across validation layers disagree, ND is outputted. If RACs predict the same class label, a confidence threshold is used for early classification. If confidence score is above threshold, output the class label and stop inference. If below threshold, input is passed to later layers for prediction. The DNN utilizes Relevant Activation Classifiers (RACs) for early classification by activating specific layers. Early classification is determined based on the output of the final layer and can help detect errors. Adjusting the user-defined threshold can optimize efficiency and error detection accuracy. This methodology can be applied to various image recognition applications, leading to energy-efficiency improvements. The proposed methodology involves using Deep Neural Networks (DNNs) with Relevant Activation Classifiers (RACs) for image recognition tasks. The framework is tested on VGGNet and ResNet for image classification on CIFAR and Tiny-ImageNet datasets. LSUN, SVHN, and TinyImageNet datasets are used for out-of-distribution sample detection, and Carlini and Wagner attack is used for generating adversarial examples. Performance is measured using metrics like percentage of good decisions and bad decisions. The performance metrics include the percentage of good decisions, bad decisions, and early decisions. The goal is to increase the true negative rate and improve energy efficiency while keeping the false negative rate low. Hyper-parameters related to Relevant Activation Classifiers (RACs) affect these metrics, such as validation layers and the number of relevant features used. Experiments were conducted to determine their impact, and heuristic methods were used to tune these hyper-parameters. Validation layers are chosen based on their effect on detection capability, with hidden layers closer to the final classifier being more suitable. As validation layers move deeper into the network, the true negative rate and false negative rate tend to decrease. A pair of hidden layers is heuristically selected to minimize the false negative rate in the range of 5%-10%. The smallest false negative rate of 5%-10% is achieved by choosing validation layers at layer 7 and layer 8 for the VGG16 network trained on CIFAR10 dataset. The number of relevant features 'k' is a hyper-parameter that impacts false negative and true negative rates, with both rates decreasing as 'k' increases. The optimal value of 'k' varies depending on the dataset and network used, and is selected by incrementing 'k' by powers of 2 and observing the corresponding FNR and TNR. Note that the number of operations increases as 'k' increases. The FNR and TNR of VGG16 on CIFAR10 are influenced by the number of relevant features 'k' at validation layers 7 and 8. Increasing 'k' from 64 to 128 decreases FNR by 1.6% and TNR by 5%. Choosing 'k' as 64 is optimal. The confidence threshold \u03b4 th affects energy efficiency and detection capability, regulating the number of inputs passed to later layers. Changes in \u03b4 th have a negligible impact on FNR, around 0.1% for a 0.1 change. The variation in normalized OPS with different \u03b4 th for VGG16 network trained on CIFAR10 dataset shows that TNR increases as \u03b4 th increases, but beyond a certain point, natural errors may occur due to confusion. The maximum TNR is achieved at \u03b4 th = 0.9. The number of OPS increases significantly beyond this point. The proposed technique involves adding RACs to DNNs to improve error detection rates. Results show that DNNs with RACs can detect 43-49% of natural errors while maintaining accuracy at 67-89% for different datasets. However, using deeper networks like DenseNet may further improve accuracy. The decrease in accuracy is due to false detections, not misclassifications. The proposed technique involves adding RACs to DNNs to improve error detection rates. Results show that DNNs with RACs can detect around 50% of natural errors while maintaining accuracy for different datasets. The framework's robustness against adversarial and out-of-distribution inputs is also evaluated, showing success rates in fooling the final classifier of the DNN. The proposed framework involves adding Relevant features based Auxiliary Cells (RACs) to DNNs for energy-efficient detection of natural errors and out-of-distribution examples. This technique improves error detection rates and maintains accuracy for different datasets. RACs help in detecting natural errors and out-of-distribution examples while being more energy efficient than baseline networks. The framework involves using Relevant features based Auxiliary Cells (RACs) in DNNs for energy-efficient error detection. Layerwise Relevance Propagation (LRP) identifies relevant hidden features for classification, with RACs detecting natural errors and making early classifications based on confidence. The framework also enhances robustness against adversarial inputs and out-of-distribution samples. This approach not only increases error detection efficiency but also suggests further exploration of energy-efficient error detection mechanisms."
}