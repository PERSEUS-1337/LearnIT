{
    "title": "SJD8YjCpW",
    "content": "Weight-sharing is crucial for deep neural network success, increasing memory efficiency and incorporating inductive priors. HashedNets and ArbNets are methods for neural network compression and efficient weight-sharing. Common neural networks can be expressed as ArbNets with different hash functions like Dirichlet and Neighborhood hash. Experimentally, balanced and deterministic weight-sharing improves neural network performance. The weight-sharing structure is crucial for the success of deep neural networks, with architectures like MLP, CNN, and RNN utilizing different weight-sharing mechanisms. For example, convolutional layers use weight-sharing to learn translation-invariant representations. Applying pre-processing with random convolutional filters significantly boosted accuracy in image classification tasks. The importance of weight-sharing in deep neural networks is highlighted, with CNNs being crucial for image classification tasks. Balanced and deterministic weight-sharing improves network performance, and tying weights in encoder-decoder networks can be beneficial. Tying weights in language modeling tasks also enhances performance and reduces the number of parameters used. The paper introduces ArbNet, a weight-sharing framework for neural networks that allows for efficient arbitrary weight-sharing between parameters. It shows that deep networks can be formulated as ArbNets, reducing the study of weight-sharing to properties of associated hash functions. ArbNet is a weight-sharing framework for neural networks that utilizes hash functions to enable arbitrary weight-sharing. This approach enhances network performance by enforcing hard weight-sharing through a hash table mechanism. The 'hashing trick' in machine learning literature involves using hash functions like the modulus hash to force weight-sharing between weights. The balance of the hash table affects performance by indicating the evenness of weight sharing. Load factor, while controlling network capacity, is not the most crucial factor in network performance. The balance of the hash table indicates weight sharing evenness. Noise in the hash function affects performance by introducing unpredictability. The hash function's parameter can be adjusted to control the amount of noise. In experiments on image classification tasks, balance helps while noise hurts neural network performance. MNIST is simpler than CIFAR10, showing differences in model capacity. MLP ArbNet's hash function specifies weight-sharing patterns, unlike an ordinary MLP. An MLP in the ArbNet framework is augmented with identity as the hash function. A CNN involves repeated convolutional layers, producing feature maps by applying weight matrices and bias matrices. The weight matrix W i has a sparse Toeplitz form. The ArbNet framework shows that MLPs, CNNs, and RNNs can be expressed as MLP ArbNets with different hash functions, demonstrating the generality of the framework for deep networks. The ArbNet framework demonstrates the generality of weight-sharing in neural networks by showing how MLPs, CNNs, and RNNs can be expressed as MLP ArbNets with different hash functions. This allows for the exploration of various weight-sharing strategies beyond the standard patterns. Additionally, converting a DenseNet BID4 with 769,000 weights into an ArbNet with a uniform hash function showcases the power of ArbNets in studying weight-sharing mechanisms. ArbNets demonstrate weight-sharing in neural networks by converting a DenseNet BID4 with 769,000 weights into an ArbNet with a uniform hash function. The ArbNet version achieved 85% accuracy with a hash table of size 10,000 and 92% accuracy with a hash table of size 100,000. Multiple hash tables and functions can also be used for layer-wise hashing in feed-forward networks. Our work generalizes the technique of weight-sharing in neural networks, exploring both hard weight-sharing and soft weight-sharing strategies. Previous studies have used layer-wise hashing and Gaussian mixture models for weight regularization. In this paper, the focus is on studying properties of MLP ArbNets for image classification tasks, specifically on MNIST and CIFAR10 datasets. The aim is not to beat existing benchmarks but to analyze the impact of changing hash function properties on test accuracy. The balance of the hash table is measured using Shannon entropy, controlled by a Dirichlet hash involving sampling from a symmetric Dirichlet distribution. The symmetric Dirichlet distribution is used to sample parameters for a multinomial hash function. The hash function's balance is controlled by the Dirichlet hash with varying \u03b1 values. A high \u03b1 leads to a balanced distribution, while a low \u03b1 leads to an unbalanced distribution. The expected load of entries in a hash table is the same for modulus and uniform hashes, resulting in the same expected Shannon entropy. The study investigates the effects of noise on neural network performance using a Neighborhood hash, composed of a modulus hash and a uniform distribution within a specified radius. The hash function varies based on the radius, controlling the level of 'noise'. On MNIST, an ArbNet three-layer MLP was trained using SGD. The study trained two networks, BID1 and BID6, using SGD with specific parameters and a learning rate scheduler. They used layer-wise hash functions for both datasets, with multiple hash tables. Experimenting with network-wise hash functions showed similar trends. Bias weights in MLPs were also tested without hashing. Increasing \u03b1 has a positive effect on test accuracy on MNIST dataset, especially with sparsity. On CIFAR10, higher \u03b1 has a positive effect on sparse weights, indicating balance improves network performance. Re-plotting the MNIST graph with Shannon Entropy instead of \u03b1 shows trends of diminishing returns. Higher sparsity levels result in smaller accuracy drops, with noise potentially causing distant weights to share values. An outlier is observed in CIFAR10 at sparsity 0.1, radius 0, suggesting a similar effect but less pronounced. The study explores the impact of noise on weight-sharing in neural networks, highlighting the importance of local weight-sharing in image classification tasks. It is found that reducing noise in the ArbNet hash function improves network performance, especially in dense networks. Weight-sharing is crucial for the success of convolutional neural networks, as it ensures balanced and deterministic weight allocation. Weight-sharing is crucial for the success of deep neural networks. ArbNets are proposed as a framework to study weight-sharing, with a focus on balance and noise in MLP ArbNet and image classification datasets. Further research is needed to explore the role of weight-sharing in deep networks."
}