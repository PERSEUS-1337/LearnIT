{
    "title": "SJe7mC4twH",
    "content": "Recent research has highlighted the vulnerability of deep neural networks, particularly convolutional neural networks (CNNs), to adversarial samples in image recognition tasks. A new quantization-based method is proposed in this work to effectively filter out these adversarial perturbations in CNNs. Unlike previous approaches, this method applies quantization in the intermediate layers of the CNN, aligning with the clustering of semantic information. Additionally, a multi-head quantization technique is introduced to address the information loss caused by quantization. The Q-Layer, a quantization layer, is added to a CNN to improve its robustness against adversarial attacks on image recognition tasks. Previous research has shown the vulnerability of deep neural networks to adversarial examples, which are imperceptible perturbations added to legitimate images. The Q-Layer approach demonstrates significant improvements in robustness on MNIST and Fashion-MNIST datasets. The vulnerability of deep neural networks to adversarial attacks has led to a continuous arms race between attacking and defending these models. Prior studies have focused on defending against adversarial attacks through input quantization, filtering out small-scale perturbations to improve robustness. The vulnerability of deep neural networks to adversarial attacks has led to a continuous arms race between attacking and defending these models. Prior studies have focused on defending against adversarial attacks through input quantization, filtering out small-scale perturbations to improve robustness. In prior work, it has been shown that shallow layers of a CNN capture fine-grained features, while deeper layers learn coarse-grained, semantically critical features. To better filter out adversarial perturbations, an alternative approach is proposed that quantizes data representations in the feature space produced by the CNN's intermediate layers. In response to the vulnerability of deep neural networks to adversarial attacks, a new approach is proposed to defend against such attacks by quantizing data representations in the feature space of CNN classifiers. This method involves the use of an extra intermediate layer called the quantization layer (Q-Layer) to improve robustness. Previous studies have explored similar approaches for different purposes, such as discovering explainable visual concepts using k-means clustering on feature maps. However, the application of intermediate layer quantization for defending against adversarial examples is a novel concept that has not been extensively explored. The Q-Layer in CNN models splits information flow into quantized and non-quantized paths to defend against gradient-based attacks. Gradient-based attacks can still be conducted through the non-quantized path. Our proposed multi-head quantization method aims to compensate for the loss of information caused by quantization in defending against adversarial examples. By projecting data points to different sub-spaces and performing quantization within each sub-space, we extract features from different perspectives to retain the effectiveness of our method. This approach can be combined with other defenses, such as adversarial training, to further improve protection against attacks. The contribution includes proposing a quantization-based defense method for adversarial examples by integrating a Q-Layer into CNN models, using multi-head quantization to enhance robustness, and evaluating the method against various attacks on MNIST and Fashion-MNIST datasets. Results show significant improvement in robustness against black-box and white-box attacks, with further enhancement possible by combining with other defenses. The adversarial attack aims to create perturbations to generate adversarial examples that mislead the neural network classifier. These attacks are categorized as black-box or white-box attacks, with the latter assuming the attacker has full knowledge of the target model. Combining the proposed defense method with adversarial training can further improve robustness against such attacks. In white-box attacks, attackers have full knowledge of the target model, enabling the generation of impactful adversarial examples. Various methods like FGSM, BIM, CW attack, and DeepFool have been proposed. On the other hand, black-box attacks restrict access to detailed information about the target model, leading to the development of transfer black-box attacks that exploit the transferability of adversarial examples. Transfer black-box attacks involve training a substitute model to conduct white-box attacks on it, generating adversarial samples with attack power on the target model. Representative white-box attacks include FGSM, BIM, and CW attacks, commonly used as benchmark methods. FGSM is a one-step approximation method proposed by Goodfellow et al. in 2015. In the context of white-box attacks like FGSM, BIM, and CW, various defense methods have been developed to combat adversarial attacks. The BIM attack iteratively performs FGSM to create impactful adversarial examples, while the CW attack aims to find the smallest perturbation to deceive the target model. Defense methods have been introduced by different researchers to counter a wide range of attacking techniques. Three representative adversarial training methods are discussed, including the use of FGSM for generating target-specific adversarial examples, iterative attacks like BIM for augmenting the training set, and input quantization techniques such as feature squeezing to transform input images. These defense methods are effective against various adversarial attacks. Input purification methods aim to enhance the robustness of CNNs against adversarial attacks by modifying the pixel distribution of input images. PixelDefend utilizes PixelCNN to differentiate between adversarial and legitimate samples, purifying input pixels to achieve a natural distribution before processing them through the target model. The input goes through the quantization path (Q-path) before the fully connected block or between two convolutional blocks. This path includes projection, quantization, and concatenation steps to produce quantized output for the subsequent network. The quantization step replaces pre-quantization data representation with post-quantization representation. Due to the non-differentiability introduced by quantization, backpropagation cannot be directly applied. To address this, multihead projection results are concatenated and passed to a separate subsequent network along the non-quantization path (E-path) to ensure the model remains trainable. In experiments, connecting paths to networks with different weights showed better performance than connecting to identical networks. Figure 1 shows four backward paths updating the model, with paths 1, 3, and 4 for quantization and path 2 for non-quantization. A neural network N can be split into N F and N B, an image encoder and classifier network. This work splits a CNN into these sub-networks for processing input x. The neural network consists of convolutional blocks and a fully-connected classifier network. The output y is calculated based on the input x. The intermediate output z e is composed of panels, each represented by a vector of length d. The Q-Layer quantizes the panels using a concept matrix Q. The quantization operation involves multiplying Q by an identification vector. The outputs y q and y e are produced by different subsequent networks N Q and N E. The final output y is determined by the Q-path and E-path outputs y q and y e. Multi-head quantization involves sub-space clustering to address challenges in high dimensional space clustering and prevent information loss. Input representation is projected to K sub-spaces, followed by quantization. In multi-head quantization, input representation is projected to K sub-spaces, followed by single-head quantization within each sub-space. The final output is obtained by concatenating the quantization results from all sub-spaces. The projection step involves applying a re-weight mechanism to d channels of the input representation, with each sub-space having its own projection parameters. The outputs from all sub-spaces are concatenated to obtain the final output. The multi-head quantization process involves projecting input representation into K sub-spaces, followed by single-head quantization within each sub-space. The final output is obtained by concatenating the quantization results from all sub-spaces. The quantization loss is decomposed into separate terms, including standard cross-entropy loss for optimizing weight parameters and two additional loss terms for optimizing quantization performance. The quantization process involves projecting input representation into sub-spaces and optimizing quantization performance with loss terms. The third loss measures distance between concept vectors, while the fourth loss shapes learned projections to be similar to clustering centroids. In experiments, tuning hyper-parameters \u03b1 and \u03b2 had little impact on final performance. If concept matrix is not properly initialized or optimized, some concepts may remain constant, labeled as \"inactive\", while others can be updated, labeled as \"active\". In experiments, tuning hyper-parameters \u03b1 and \u03b2 had little impact on final performance. Some concepts may remain constant, labeled as \"inactive\", while others can be updated as \"active\". To address this issue, two updating strategies were designed: forcing an inactive concept to move to its closest panel directly or re-initializing it as its closest panel. The first strategy, implemented by adding a special loss term to calculate the distance between each inactive concept and its closest panel, was found to be more effective. The proposed Q-Layer was evaluated under black-box and white-box attacks to compare the accuracy and robustness of CNNs built with and without it. The study compares the accuracy and robustness of CNNs with and without the Q-Layer on clean and adversarial testing sets. White-box attacks target the E-path or Q-path, with the latter involving ignoring quantization to build a substitute attack path. The CNNs' performance was evaluated under both types of attacks. In the study, CNNs with and without the Q-Layer were evaluated for accuracy and robustness under different attacks. The models were trained with and without adversarial training, referred to as adversarial-trained and raw-trained models. The focus was on the classification accuracy of the Q-path in CNNs with the Q-Layer. The experiments used MNIST and Fashion-MNIST datasets with subsets for training, adversarial training, validation, clean testing, and adversarial testing. The study evaluated CNNs with and without the Q-Layer for accuracy and robustness under different attacks. Models were trained with and without adversarial training, focusing on the classification accuracy of the Q-path in CNNs with the Q-Layer. A clean validation set was constructed by randomly selecting 10,000 samples from the original training set, and adversarial examples were created using a pre-trained CNN and the FGSM method. The final validation set was a mix of clean and adversarial samples to select models with the best performance against adversarial examples. During testing, different adversarial testing sets were constructed for black-box and white-box attacks. Two source models were independently trained for generating adversarial samples, one with the same structure as CNN A and the other with a different structure denoted as CNN B. White-box attacks involved directly attacking the target model to generate the adversarial testing set. The method could be applied in conjunction with adversarial training. In experiments, adversarial examples were created with different perturbation scales on MNIST and Fashion-MNIST datasets to test the robustness of three target models: CNN A, Q-base, and Q-large. The setup aimed to simulate critical scenarios where FGSM-based adversarial training is less effective against larger perturbations. The study evaluated the robustness of target models CNN A, Q-base, and Q-large against adversarial attacks on MNIST and Fashion-MNIST datasets. Results showed that attacks using CNN A were more impactful than those using CNN B. Additional details on model architectures and hyperparameters are provided in the appendix. The study compared the impact of attacks using CNN A and CNN B on model robustness. Results for attacks performed by CNN B are provided in Appendix E. Three representative attacks (FGSM, BIM, CW) were used, with results shown in Table 1a and 1b for FGSM and BIM attacks. Inserting the Q-Layer into CNN improved robustness significantly, with notable accuracy improvements under FGSM and BIM attacks. The study compared the impact of attacks using CNN A and CNN B on model robustness. Results for attacks performed by CNN B are provided in Appendix E. The accuracy under BIM attack of = 0.3 rises significantly with adversarial training, surpassing the accuracy obtained by the adversarial-trained CNN. The robustness of all models increases with adversarial training, especially under large-scale perturbations. Improved robustness was observed for raw-trained Q-base and Q-large models compared to CNN, with Q-base and Q-large outperforming CNN under large perturbations. The study explored the influence of sub-spaces and concepts on black-box robustness through ablation studies. Using PixelCNN, the distribution of pixels and concepts in clean and adversarial samples was compared. Results showed that quantization effectively blocked adversarial perturbations, leading to improved robustness in the MNIST dataset under the FGSM attack. The study compared classification accuracy of different models on MNIST under white-box attack. Results showed that Q-large and Q-base outperformed CNN, with Q-large and Q-base showing superior robustness as perturbation scale increased. Similar results were observed for Fashion-MNIST. In experiments, the Q-Layer inserted between convolution blocks enhances robustness against white-box attacks. The Q-inner model, with parameters K=4, nc=64, outperforms Q-large on MNIST under white-box attacks. The Q-Layer enhances CNN classifier robustness against adversarial attacks, with Q-inner outperforming Q-large. Applying quantization early filters out perturbations effectively, improving adversarial robustness under both white-box and black-box attacks. Combining the Q-layer with adversarial training further boosts CNN performance against attacks with larger perturbations. In this work, the impact of random initialization of concept matrix is addressed by reactivating inactivate concepts. Future work will explore different approaches for constructing the concept matrix and using the E-path as a learned index for information retrieval. Model A and B are neural network architectures used for MNIST and Fashion-MNIST datasets. Model A is used for both target CNN and substitute model, while model B is only used for the substitute model. RAdamOptimizer is used with a learning rate of 0.001 when training CNN with a Q-Layer. Losses are set with c1 = 1. In experiments, RAdamOptimizer is used with a learning rate of 0.001 for optimizing different parts of the network. Hyperparameters like \u03b1 and \u03b2 are tuned based on the train and validation sets. Inactive concepts updating strategy involves optimizing the distance between inactive concepts and their closest data point using AdamOptimizer with a learning rate of 0.001. The models are trained on train and validation sets, with the highest validation accuracy model selected for testing. Adversarial samples are generated using Adversarial Robustness 360 Toolbox. Two CNN models on MNIST are compared, with the model selected on the mixed validation set showing more robustness. Q-path and Epath classification accuracy are compared under black-box conditions. The Q-path classification accuracy is compared to the E-path accuracy under black-box and white-box attacks. Results show that the Q-path accuracy is higher, especially for large models, indicating the efficiency of quantization. However, after adversarial training, the Q-path accuracy may decrease slightly under small perturbations compared to the E-path accuracy. The Q-path accuracy is higher than the E-path accuracy under black-box attacks due to quantization causing information loss. Results comparing different target models on MNIST and Fashion-MNIST show improved robustness with a Q-Layer. Further analysis on the effect of sub-spaces and concepts on robustness is conducted on MNIST under FGSM black-box attack. After adversarial training, models with larger K show higher accuracy when n c = 16, but the relationship between K and accuracy is unclear for n c = 64/128. Adding more sub-spaces improves accuracy when n c = 16 due to reduced information loss from quantizing features. The importance of K decreases when n c is large enough to represent different concepts. Comparing robustness of models with shared and separated subsequent networks, separated networks enhance robustness. Black-box and white-box attack results on MNIST demonstrate the effectiveness of separated subsequent networks in improving robustness. The results show that separated subsequent networks improve robustness under black-box and white-box attacks. It is recommended to use separated subsequent networks for shallow models and shared subsequent networks for deeper models. White-box attacks on the Q-path are discussed, showing that attack through a shortcut path is usually weak. The results indicate that attacking Q-path reduces accuracy less than attacking E-path in white-box attacks. The comparison of classification accuracy on MNIST models under different attacks is shown in Table 11."
}