{
    "title": "S1eUd64tDr",
    "content": "Due to resource constraints, network compression is crucial in deep neural networks research. A new compression method, Inter-Layer Weight Prediction (ILWP), and quantization method are proposed in this paper. The weights in adjacent convolution layers exhibit a Smoothly Varying Weight Hypothesis (SVWH), leading to a second ILWP and quantization method. Predicted weight residuals follow Laplace distributions with low variance, enabling more effective weight quantization and producing more zero weights. The proposed method in this paper introduces weight quantization and an inter-layer loss to enhance weight compression in deep neural networks. It achieves a higher compression rate with the same accuracy level compared to previous methods. Several studies have focused on creating resource-efficient neural networks while maintaining accuracy by using methods like network pruning and weight quantization. In Fiesler et al. (1990), Gong et al. (2014), and Han et al. (2015a), weights were quantized and stored in memory to reduce memory usage in deep neural networks. Some studies decomposed convolution operations into sub-operations, like depth-wise separable convolution, to lower computation costs while maintaining accuracy levels. This paper introduces the Smoothly Varying Weight Hypothesis (SVWH), showing that weights between adjacent convolution layers share high similarity. A new weight compression scheme based on inter-layer weight prediction is proposed, inspired by techniques used in video compression. The paper introduces the Smoothly Varying Weight Hypothesis (SVWH) and proposes an Inter-Layer Weight Prediction (ILWP) technique for more compressed neural networks. A new regularization function, inter-layer loss, minimizes differences between adjacent layer weight values, resulting in significant bit savings. The proposed scheme achieves a 53% compression ratio on average in 8-bit quantization compared to traditional methods in MobileNetV1 and MobileNetV2. Network pruning reduces redundancy in weight parameters through removing unimportant connections without affecting accuracy. Structured pruning methods have also been developed. Neural network models have been improved through structured pruning methods, reducing weight parameters and computational costs. AutoML for channel pruning has shown higher accuracy compared to filter pruning. Weight compression is achieved by assigning more zero weights. Quantization reduces the representation bits of original weights, with quantization and pruning working together for weight compression without accuracy loss. Deep Compression, a framework proposed by Lin et al. (2016), combines quantization and pruning to compress weight values without accuracy loss. Various studies have explored methods such as fixed-point quantization with adaptive bit-widths, clipping weights before linear quantization, and Binary Neural Networks for 1-bit quantization. These approaches have shown improvements in accuracy compared to traditional linear quantization methods. Hubara et al. (2017) proposed a method of quantization that focuses on the residuals of weights in neural networks, rather than the weights themselves. This approach significantly reduces memory consumption by taking advantage of the narrow Laplace distributions of residuals. In video compression, prediction techniques play a crucial role in minimizing signal magnitudes by subtracting input signals from encoded signals. In video compression, prediction techniques are used to minimize signal magnitudes by subtracting input signals from encoded signals. Two prediction techniques, inter-and intra-predictions, search for the best prediction signals from neighboring frames and generate prediction signals from input signals, respectively. Intra frames using only intra-prediction are used as reference frames for subsequent predictions. Transform techniques of video and/or image have also been explored in some studies. Our work focuses on weight compression in neural networks without using transform techniques like DCT. Instead, we propose using inter-prediction techniques for weight compression under the SVWH condition. This approach is found to be effective in reducing model sizes without introducing excessive computation during inference. The proposed inter-layer loss reinforces SVWH for reducing non-texture bits in training. The inter-prediction and quantization framework for weight compression shows impressive performance enhancement compared to baseline models. The ILWP method with full search strategy (FSS) searches for similar weights in previous layers and quantizes the residuals for storage. The ILWP method with full search strategy (FSS) aims to compress weights by storing the best predictions in memory space. However, it tends to produce more bits than standard weight values, making it ineffective. To address this issue, SVWH is proposed to reduce non-texture bits by leveraging the similarity of weight values in adjacent layers. The proposed SVWH aims to reduce non-texture bits in weight compression by utilizing similarity between weight values in adjacent layers. An enhanced version, ILWP-LSS, incorporates a local search strategy to improve prediction accuracy from the previous layer only. This strategy effectively reduces non-texture bits and memory buffer requirements, enhancing compression performance in deep neural networks. The ILWP regularization loss is proposed to increase the effectiveness of SVWH in deep neural networks. It focuses on making collocated filter weights between adjacent layers have similar values, particularly in depth-wise convolution layers. This approach is more efficient compared to traditional 3D convolutions due to the hindrance caused by high dimensionality of weights in the latter. The proposed ILWP regularization loss aims to enhance SVWH in deep neural networks by promoting similarity in collocated filter weights between adjacent layers, especially in depth-wise convolution layers. Depth-wise convolutions with canonical 3x3 kernel shapes and point-wise convolutions contribute to the efficiency of weight compression in neural networks. The popularity of depth-wise separable convolutions in recent architectures further supports the usability of ILWP. The proposed method applies inter-layer loss to enhance similarity in collocated filter weights between adjacent layers in deep neural networks. This loss function helps eliminate non-texture bits and regularizes weights, improving weight compression efficiency. The total loss function includes conventional classification loss using cross-entropy loss. The proposed method applies inter-layer loss to enhance similarity in collocated filter weights between adjacent layers in deep neural networks. This loss function helps eliminate non-texture bits and regularizes weights, improving weight compression efficiency. The control parameter \u03bb is set to 1 in all experiments to balance neural network performance and parameter size. The inter-layer loss explicitly controls SVWH, as shown in Heatmaps for different neural networks. Reconstruction of weight kernel involves quantized residuals and filter positions. The method involves quantizing residuals of weights in deep neural networks and using Huffman coding for efficient model capacity. The approach retains high-valued non-zero weights, contributing to prediction performance and weight compression. Weight kernels in the first layer are not quantized due to their importance, similar to Intra-frames in video coding schemes. The proposed method involves quantizing weight kernels in deep neural networks to improve accuracy. Experiments were conducted on CIFAR-10 and CIFAR-100 datasets using MobileNet, MobileNetV2, ShuffleNet, and ShuffleNetV2 models. The training process utilized a learning rate of 0.1, SGD optimizer with Nesterov momentum, and Nvidia 2080-Ti GPU with Intel i9-7900X CPU. The experiments involved training neural networks on CIFAR-10 and CIFAR-100 datasets using MobileNet, MobileNetV2, ShuffleNet, and ShuffleNetV2 models. The optimal \u03bb in Eq. (3) was determined to balance accuracy performance and parameter size. Parameter sizes and test accuracy in MobileNet were compared for different \u03bb values, showing that \u03bb = 1 had the smallest parameter size with slightly lower accuracy on CIFAR-10. The experiments involved training MobileNet, MobileNetV2, ShuffleNet, and ShuffleNetV2 on CIFAR-10 and CIFAR-100 datasets. Setting \u03bb to 1 in Eq. (3) yielded higher compression ratios in weight parameters. ILWP-ILL outperformed ILWP-FSS and ILWP-LSS in compression, attributed to non-texture bits. Table 1 compares parameter sizes and test accuracy for depthwise convolution kernels. The proposed methods show reduced texture bits compared to the baseline, with ILWP-ILL demonstrating the most significant reduction in total bits while maintaining good accuracy. ILWP-ILL outperforms other methods in the trade-off between compressed bits and accuracy on CIFAR-10 and CIFAR-100 datasets. ILWP-ILL significantly reduces weight bits by eliminating non-texture bits and improving quantization effectiveness. Compared to ILWP-FSS, ILWP-LSS shows slight improvement in parameter size vs. accuracy trade-off. Both ILWP-FSS and ILWP-LSS have worse compression efficiency than baseline models due to introducing non-texture bits. The proposed inter-layer loss in SVWH allows the network to utilize advantages in inter-layer prediction. ILWP-FSS produces a single modal Laplace distribution for weight kernels, contributing to high compression capacity. ILWP-ILL reduces weight bits by eliminating non-texture bits and improving quantization effectiveness. ILWP-ILL achieves remarkable weight compression performance by generating a large amount of zero coefficients after quantization, with residuals following a sharp Laplace distribution. The Shannon entropy of the Laplace distribution is proportional to the scale parameter, controlling the width of the distribution. ILWP-ILL exhibits a much narrower Laplace distribution compared to other methods. The ILWP-ILL method achieves efficient weight compression by utilizing a narrow Laplace distribution for residuals, resulting in lower information entropy compared to ILWP-FSS. This leads to more compressed Huffman coding, making ILWP-ILL more efficient after quantization and entropy coding. The proposed inter-layer weight prediction framework combines prediction, inter-layer loss, quantization, and Huffman coding to reduce weight entropy significantly. The proposed prediction scheme reduces weight entropy by using narrower Laplace distributions, leading to a high compression ratio in neural networks. Additionally, the inter-layer loss eliminates non-texture bits for improved predictions. This work is the first to observe weight similarities between neighboring layers and develop a prediction-based weight compression scheme in deep neural networks."
}