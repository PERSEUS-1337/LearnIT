{
    "title": "BJQPG5lR-",
    "content": "The degradation problem is a common issue in deep learning. In addressing the degradation problem in deep learning, novel architectures like ResNets and Highway networks have introduced skip-connections or gating mechanisms. However, plain feed-forward networks still face this issue. A proposed method treats learning weights in deep networks as a constrained optimization problem, penalizing skip-connections with Lagrange multipliers. This allows for the introduction and phased-out removal of skip-connections during training. Experiments on various datasets show that this method significantly reduces degradation effects and competes with ResNets in performance. The representation view of deep learning suggests that neural networks learn abstract representations hierarchically, which can be utilized for tasks like image classification and speech recognition. While deeper networks are expected to learn more detailed representations, performance degradation has been observed in feed-forward networks beyond a certain depth. Residual Networks and Highway Networks have addressed this issue by introducing skip-connections or gating mechanisms, allowing for the training of increasingly deep networks. The introduction of skip-connections in deep networks, such as ResNets, helps improve optimization conditioning and gradient properties. Skip-connections diagonalize the Fisher information matrix, retain gradient correlation, and reduce vanishing gradient effects by introducing a linear term. This contrasts with deep feed-forward networks where gradients resemble white noise. The work addresses the degradation issue in plain feed-forward networks by leveraging optimization properties of ResNets. Skip-connections are introduced and later removed in a principled manner, leading to improved generalization error compared to architectures without skip-connections. The approach is competitive with ResNets in some cases. The proposed method introduces skip-connections with Lagrange multipliers to reduce performance degradation in deep feed-forward networks. It demonstrates improved generalization error compared to plain networks and competes with ResNets on benchmark datasets. The hierarchical structure of feed-forward networks is inspired by the visual cortex. Neurons in early layers capture simple features like edges, which are aggregated in deeper layers. The depth of a network should be maximized to learn more abstract representations. However, deeper networks are harder to train due to optimization challenges like vanishing gradients. Solutions include pre-training, parameter initialization, batch normalization, and skip-connections like ResNets. ResNets introduce residual blocks with skip-connections for training deeper networks. They are related to Highway Networks, where each layer's output is determined by a gating function inspired by LSTMs. Various variations like DenseNet have been proposed based on the success of these methods. Recently, novel approaches to deep network training have emerged, such as DenseNet with skip-connections, shortening networks by dropping layers, and re-parameterization of weights in feedforward networks. Additionally, there are proposals to initialize weights in a CReLU activation function to maintain linearity during training. The goal of this work is to train deep feed-forward networks without suffering from the degradation problem described in previous sections. Parameters are learned by minimizing an objective function using first-order methods, as directly minimizing the objective is not practical in the context of deep networks. The goal is to train deep feed-forward networks without degradation issues. Inspired by ResNets, the proposed method modifies equations to include skip-connections with variable weights. Networks using these residual blocks are called Variable Activation Networks (VAN). The objective is to train the network with \u03b1 l = 1 for all layers, removing skip-connections at the end of training. The proposed method involves modifying equations to include skip-connections with variable weights in Variable Activation Networks (VAN). By setting \u03b1 l = 1 for all layers during training, the optimization problem is relaxed, allowing information flow through skip-connections to alleviate gradient issues. The activation function introduces no additional parameters, and weights are trained through a constrained optimization problem with Lagrange multipliers. Iterative updates of \u03b1 l are done via stochastic gradient descent steps. The experiments demonstrate that the proposed method effectively addresses the degradation problem in deep networks by adjusting Lagrange multipliers and enforcing constraints on \u03b1 l through stochastic gradient descent steps. The proposed method effectively alleviates degradation in deep networks by adjusting Lagrange multipliers and enforcing constraints on \u03b1 l through stochastic gradient descent. It is demonstrated using simple architectures on MNIST and Fashion-MNIST datasets, with comparisons on CIFAR datasets. Networks with varying depths and architectures are evaluated against popular models like ResNets and Highway Networks. The study evaluated VAN networks with varying depths and architectures, comparing them to popular models like ResNets and Highway Networks. The networks were trained using SGD with momentum, fixed learning rate, and batch size. VAN networks had \u03b1 l values initialized to 0 for all layers during training. During training, \u03b1 l values were set to 0 for all layers in VAN networks. The step-size parameter for Lagrange multipliers, \u03b7, was half of the SGD step-size. Results in FIG0 show test accuracy versus network depth for MNIST and Fashion-MNIST datasets. Plain networks degrade after around 10 layers, while ResNets, Highway Networks, and DiracNets are designed to avoid this. VAN networks show less degradation with increasing depth, indicating the importance of skip-connections. The introduction of variable skip-connections in VAN networks leads to improved generalization performance compared to plain networks across all depths. VAN networks outperform plain networks, suggesting convergence at local optima with better generalization performance. Results show stable performance of architectures as the number of layers increases, with VAN (\u03bb = 0) networks obtaining competitive results. CIFAR-10 and CIFAR-100 datasets are considered as more challenging benchmarks. The CIFAR-10 and CIFAR-100 datasets contain 60000 32\u00d732 pixel color images with 10 and 100 classes respectively, divided into training and test sets. Deep convolutional networks with residual layers were trained using SGD with momentum over 165 epochs. Data augmentation included random cropping and horizontal flips. Data augmentation involved random cropping and horizontal flips. Weights were initialized following BID8. Experiments were conducted on CIFAR-10 and CIFAR-100 datasets with varying depths of networks. Results show that the performance of VAN networks deteriorates less as depth increases compared to plain networks. Training and test error curves are shown for networks with 26 layers. Results from experiments on CIFAR-10 show that while plain networks perform well for networks with fewer than 30 layers, their performance declines rapidly after that. VAN networks exhibit a similar trend but with less dramatic effects, performing similarly to ResNets for networks up to 40 layers. However, beyond this depth, ResNets outperform VAN networks by a growing margin. This is consistent with the observation that scalar modulated skip-connections in VAN networks may vanish or explode in very deep networks when the scalar is not the identity. In deep networks, skip-connections in VAN networks may vanish or explode if the scalar is not the identity. The training and test accuracy of VAN networks surpass plain networks, with skip-connections fully removed around the 120th iteration. VAN networks do not outperform ResNets but outperform other alternatives. In comparison to ResNets, VAN networks outperform Highway networks and FitNets BID20 with similar depths. Both Highway networks and FitNets did not use batch-normalization. The best performance for VAN networks is with 26 layers, while ResNets improve with depth. State-of-the-art performance is achieved by Wide ResNets BID24 and DenseNet Huang et al. (2016a) on the CIFAR-100 dataset, which is more challenging than CIFAR-10. VAN and plain networks show decreased performance beyond approximately 20 layers. The performance of VAN networks with 18 and 26 layers is competitive with alternatives in the literature, even though both VAN and plain networks show decreased performance beyond 20 layers. VAN networks with \u03bb = 0 are competitive with ResNets on the CIFAR-100 dataset. Training curves for VAN and plain networks with 18 layers show improvements in generalization error with skip-connections. The manuscript proposes a new training regime for deep feed-forward networks that reduces the degradation problem by gradually phasing out skip-connections using Lagrange multipliers. This approach maintains the optimization benefits of ResNets while stabilizing the performance of VAN networks as depth increases. The original formulation for the VAN residual block may degrade performance in very deep networks due to the scaling constant being less than one, causing lower layer contributions to vanish exponentially. Comparing two different residual block formulations for VAN networks, it was found that as network depth increases, there is a more pronounced drop in performance. In experiments comparing different residual block formulations for VAN networks, it was observed that as network depth increases, there is a significant drop in performance. The use of residual blocks with non-identity scaling coefficients leads to a larger performance drop as the network depth increases, attributed to vanishing contributions from lower blocks. Additional figures show the evolution of Lagrange multipliers throughout training. The updates to Lagrange multipliers are modulated by the current value of \u03b1 l. Mean residuals of \u03b1 l parameters measure skip-connections in the network. Once skip-connections are removed, residuals become zero and Lagrange multipliers stabilize. This is observed in FIG3, where Lagrange multipliers remain constant after skip-connections are eliminated, occurring sooner for shallow networks and later for deeper networks. The mean residual of \u03b1 l parameters directly modulates changes in Lagrange multipliers, stabilizing them after skip-connections are removed, occurring sooner for shallow networks and later for deeper networks."
}