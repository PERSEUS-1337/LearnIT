{
    "title": "rkmu5b0a-",
    "content": "In this paper, a new approach to training Generative Adversarial Nets (GANs) with multiple generators is proposed to overcome mode collapsing. The use of multiple generators has proven to be highly effective in covering diverse data modes and delivering state-of-the-art results. A minimax formulation involving a classifier, discriminator, and a set of generators is employed, where the generators create samples from the training data distribution, the discriminator distinguishes between real and generated samples, and the classifier identifies the generator of a sample. The final output is randomly selected from samples generated by multiple generators. The proposed method, Mixture Generative Adversarial Nets (MGAN), utilizes multiple generators to overcome mode collapsing in GANs. The Jensen-Shannon divergence (JSD) between the mixture of generators' distributions and the data distribution is minimized at equilibrium, while the JSD among generators' distributions is maximized. By sharing parameters, MGAN adds minimal computational cost and achieves superior performance in generating diverse and appealing objects on various datasets. Generative Adversarial Nets (GANs) are deep generative models that specialize in generating diverse and recognizable objects. They consist of a discriminator and a generator playing a two-player minimax game. Training GANs can be challenging due to the mode collapsing problem where the generator focuses on producing samples from only a few modes. Many GAN variants have been proposed to address the mode collapsing problem, focusing on training either a single generator or many generators. These methods aim to help generators recover the data distribution but struggle to achieve convergence in practice. Experiments are often conducted on toy or narrow-domain datasets, with only a few studies performing quantitative evaluations on larger datasets. Recent research by Bengio (2016) and Nguyen et al. (2017) evaluates models trained on diverse datasets like STL-10 and ImageNet. Some recent approaches use a multi-generator strategy due to limitations in single-generator GAN training. Tolstikhin et al. (2017) employ boosting techniques to train a mixture of generators, but this method is computationally expensive. The assumption that a single-generator GAN can cover the entire data space is not practical, as current models struggle to generate high-quality images across all modes. Recent research has shown that single-generator GANs trained on diverse datasets like ImageNet can generate unrecognizable objects. BID2 and BID8 propose different approaches to training multiple generators, but they have limitations. In this paper, a novel approach is introduced to simultaneously train a set of generators with a new objective function. Our proposed model, Mixture Generative Adversarial Nets (MGAN), trains a set of generators to approximate the data distribution and specialize in different data modes. It involves a minimax game among a classifier, discriminator, and generators to create samples resembling the training data. The model aims to minimize the Jensen-Shannon Divergence (JSD) between the mixture of generator distributions and the data distribution while maximizing the JSD among generators. Our proposed model, Mixture Generative Adversarial Nets (MGAN), efficiently trains a mixture of generators by utilizing parameter sharing and enforcing Jensen-Shannon Divergence (JSD) among them. Trained on CIFAR-10, STL-10, and ImageNet datasets, the generators specialize in different classes and achieve state-of-the-art Inception scores. The model outperforms GANs trained in a semi-supervised fashion. The proposed MGAN model efficiently trains a mixture of generators by enforcing Jensen-Shannon Divergence (JSD) among them. The objective function minimizes JSD between the generators' distributions and real data while maximizing JSD among generators. Training GAN involves optimizing discriminator D and generator G using stochastic gradient-based learning. The proposed approach in this study aims to address mode collapse in GAN by using a mixture of distributions to approximate the data distribution and enlarging the divergence among these distributions to cover different data modes. This method effectively tackles the issue of generating less diverse samples in GAN by incentivizing the generator to map each input to a single output that is most likely to be classified as true data. The proposed approach addresses mode collapse in GAN by using a mixture of distributions to cover different data modes. A game involving generators, a discriminator, and a classifier is formulated, where each generator maps input to an output distribution. The Mixture Generative Adversarial Nets (MGAN) model is introduced to handle this process. The proposed MGAN architecture utilizes parameter sharing among components to effectively train models and minimize complexity. It involves a multi-player minimax optimization game with generators, a discriminator, and a diversity hyper-parameter. The proposed MGAN architecture involves a multi-player minimax optimization game with generators, a discriminator, and a diversity hyper-parameter. The interaction between generators and the classifier encourages each generator to produce data separable from others. The network can be trained by updating D, C, and G 1:K alternately. At the equilibrium point, the JSD between the mixture induced by G 1:K and the data distribution is minimal, and the JSD among K generators is maximal. The mathematical statement and proofs for fixed generators and their mixture weights are presented. The optimal solution is derived, showing the closest generator to the true data distribution while avoiding mode collapse. The equilibrium point of the minimax problem is discussed, emphasizing the importance of maintaining diversity among generators. The equilibrium point of the minimax problem is discussed, emphasizing the importance of minimizing JSD (P data P model ) while maximizing JSD \u03c0 (P G1 , P G2 , ..., P GK ). Theorem 3 clarifies the equilibrium point for the specific case where the data distribution has well-separated mixture components. The minimax problem in Eq. (2) or the optimization problem in Eq. (3) has a solution when the mixture components are well-separated. The optimal solution is explicitly offered in Thm. 3 for this specific case, showing that by maximizing the divergence between generated components, the original mixture components can be exactly recovered. This supports the development of MGAN. In developing MGAN, generators C, D, and G 1:K are optimized in the parameter space using neural networks. All generators share the same objective function, allowing for efficient weight updates. Techniques like minibatch discrimination, Unrolled GAN, and Denoising Feature Matching have been used to address mode collapse in GANs. Unrolled GAN improves learning by including additional optimization steps for the discriminator, reducing mode collapsing. Denoising Feature Matching augments the generator's objective function with a Denoising AutoEncoder to guide sample generation. DFM is effective at avoiding mode collapse, but adds computational cost. An alternative approach is D2GAN, which uses two discriminators to minimize divergences. Another method uses multiple discriminators to boost learning, while another direction is training multiple generators like MIX+GAN. Training a mixture of multiple generators and discriminators with different parameters in a min-max game can lead to computational expense due to lack of parameter sharing. Previous attempts to train a mixture of GANs include an additive procedure to train new GANs on poorly modeled data and using boosting algorithms, but these methods have not effectively addressed the mode collapsing problem. AdaGAN was proposed as a solution to the mode collapsing problem in GANs by introducing a robust reweighing scheme for training data. It is based on the idea that a single-generator GAN may excel at generating images of certain modes, like dogs or cats, but struggle with others, like giraffes. However, this assumption is not always true in practice, as single-generator GANs trained on diverse datasets tend to generate unrecognizable objects. MAD-GAN BID8 is a related approach that trains multiple generators and uses a multi-class classifier as the discriminator to address mode collapse. Our approach to addressing mode collapse involves augmenting the generator's objective function with a similarity-based function and using an additional classifier to discriminate samples from different generators. This method maximizes the JSD among generators, promoting diverse sample generation and avoiding mode collapse. Experiments were conducted on synthetic and real-world datasets to evaluate the proposed MGAN. Our proposed MGAN aims to address mode collapse by visualizing, examining, and evaluating learning behaviors using real-world datasets. We implement our model using TensorFlow and share parameters among generators and discriminators. Experiments show the efficacy and scalability of our approach compared to state-of-the-art GAN-based models. In the first experiment, the MGAN explores multiple data modes by sampling from a 2D mixture of 8 isotropic Gaussian distributions with small variance to create low density regions. 8 generators with a simple architecture are employed for this purpose. Refer to Appendix C for detailed model architectures and additional experimental results. In the experiment, 8 generators with a simple architecture generate data from a 2D mixture of 8 Gaussian distributions. The models, including MGAN, distribute data across all components, avoiding mode collapse seen in regular GANs. Our proposed MGAN model distributes data across all 8 mixture components, learning multimodal data successfully and converging faster than other models. It captures data modes more precisely than UnrolledGAN and D2GAN, generating samples that spread out the entire mode without exceeding boundaries. Quantitatively comparing the quality of generated data using Wasserstein distance and symmetric Kullback-Leibler measures against the true distribution P data. Our approach, using Wasserstein distance and symmetric Kullback-Leibler divergence, outperforms GAN, UnrolledGAN, and D2GAN in terms of distance metrics. The stability of our MGAN and D2GAN models during training is evident, with our symmetric KL metric significantly better than D2GAN's. Experimenting with different numbers of generators, our MGAN successfully explores 8 modes with 2, 3, 4, and 10 generators. Models with varying numbers of generators successfully explore 8 modes, but more generators result in fewer scattered points between modes. The diversity coefficient \u03b2 affects the clustering behavior of the generated samples. Training with different values of \u03b2 shows that a balance is needed to cover all 8 modes effectively. The proposed method is then tested on real-world datasets for performance and scalability. Datasets CIFAR-10, STL-10, and ImageNet are used for evaluation. CIFAR-10 has 50,000 32\u00d732 images of 10 classes, STL-10 has about 100,000 96\u00d796 images, and ImageNet has over 1.2 million images from 1,000 classes. The images are resized for fair comparison. Inception score is used for quantitative evaluation. The conditional label distribution for image x is estimated by the reference Inception model BID15. Inception scores are computed for randomly generated samples to demonstrate image quality. Generator and discriminator architectures follow DCGAN's design with batch normalization applied to all layers except the output layer. MGAN achieves the best performance in terms of fast convergence rate and high inception score. During training, MGAN achieves fast convergence and high inception score by sharing parameters between the classifier and discriminator, except for the output layer. This parameter sharing allows for leveraging common features and representations, improving training speed. An issue observed was a decline in active neurons, potentially due to batch normalization offset shifting negatively. An ad-hoc solution was implemented by fixing the offset at zero for all generator network layers. For each feature map, ReLU gates open for 50% highest inputs in a minibatch across all locations and generators. Experimented with Leaky ReLU and MaxOut units, achieving good Inception scores but generating unrecognizable samples. Tried SeLU but failed to train the model. Three key hyperparameters are number of generators, coefficient \u03b2 for diversity, and minibatch size. Models with 10 generators perform better than 4 generators. Additional setting with 32 generators for ImageNet. For ImageNet, experiments with 32 generators and a minibatch size of 4 showed no significant improvement compared to models with 10 generators. Varying the diversity coefficient \u03b2 did not affect Inception scores significantly, but generated images were less realistic with low or high \u03b2 values. A reasonable range for \u03b2 was found to be (0.01, 1.0), with values set to 0.01 for CIFAR-10, 0.1 for ImageNet, and 1.0 for STL-10. Our MGAN outperforms baselines by large margins and achieves state-of-the-art performance on all datasets. It achieves a score of 8.33 on CIFAR-10, better than models trained with labels. Additionally, training on the original 96\u00d796 resolution of STL-10 results in a score of 9.79\u00b10.08, showing success with higher resolution images. MGAN outperforms baselines by large margins and achieves state-of-the-art performance on all datasets, with a score of 8.33 on CIFAR-10 and 9.79\u00b10.08 on STL-10 at the original 96\u00d796 resolution. Lower FID is better, and MGAN is roughly 28% better than DCGAN and DCGAN + TTUR, 9% better than WGAN-GP, and 8% weaker than WGAN-GP + TTUR in terms of FID. This result further proves that MGAN helps address the mode collapsing problem. MGAN outperforms baselines on all datasets, achieving state-of-the-art performance. Generated images include a wide range of objects and themes, with some recognizable objects. While visually appealing, some images are incomplete and unrealistic, leaving room for improvement. In our MGAN trained on CIFAR-10, generators specialize in different objects early on, becoming more consistent over training epochs. Generator 7 collapses in epoch 250, possibly due to different object classes having distinct themes. The study by Wang et al. identified a cause for non-convergence in GANs - the generators and discriminators constantly vary, leading to significantly different images being generated at consecutive epochs. A novel adversarial model was proposed to address mode collapse in GANs by approximating data distribution using a mixture of multiple distributions. This approach involves a minimax game with one discriminator, one classifier, and multiple generators to minimize the Jensen-Shannon Divergence (JSD) between the data distribution and the model distribution. The proposed model, Mixture Generative Adversarial Network (MGAN), utilizes a mixture of generator distributions to maximize Jensen-Shannon Divergence (JSD) and prevent mode collapse. It can efficiently generate diverse images and is scalable for real-world large-scale datasets. Experimental results show that MGAN achieves state-of-the-art Inception scores, generates appealing objects at different resolutions, and specializes in capturing various object types. The generators G1 to GK are deep convolutional neural networks with shared parameters, except for the input layers. The input layer maps noise to hidden layer activation, and the shared layers generate data. Classifiers C and D are also deep neural networks with shared parameters, except for the last layer. The algorithm describes sampling from the mixture of generators and learning parameters using stochastic gradient descent. The algorithm involves training a mixture of generators using stochastic gradient descent. Generators G1 to GK and mixture weights \u03c01 to \u03c0K are used to generate data. Classifiers C and D are updated based on their gradients, and the mixture of generators G is updated as well. The optimal classifier C* and discriminator D* for the mixture of generators G1 to GK are derived by optimizing in the functional space and setting functional derivatives to zero. The objective function for the generator is reformulated with the optimal C* and D*, showing a relationship to Jensen-Shannon divergence. The JSD for the K distributions in Eq. FORMULA29 is related to the optimization problem for finding the optimal G* in Eq. (3). The solution is given by Eq. FORMULA30, with the equality condition specified. In the optimization problem for finding the optimal G* in Eq. (3), the inequality and equality conditions are specified. The true data is sampled from a 2D mixture of 8 Gaussian distributions with specific parameters. The network architecture used for experiments is detailed in Tab. 3. The network architecture used for experiments does not use batch normalization. The models with 2, 3, and 4 generators successfully cover 8 modes, while the model with 10 generators also covers 8 modes. Fig. 6 shows samples produced by MGANs with different numbers of generators trained on synthetic data for 25,000 epochs. The MGAN models with 2, 3, and 4 generators cover 8 modes, while the model with 10 generators also covers 8 modes. The effect of diversity coefficient \u03b2 on generated samples is shown in Figure 7, where different values of \u03b2 result in samples clustering around different modes. During training of MGAN models, the percentage of active neurons declined, affecting the quality of generated images. The batch normalization center gradually shifted to the negative range, leading to a decline in image quality. An ad-hoc solution was implemented by fixing the offset at zero for all layers in the generator. The ad-hoc solution for the declining quality of generated images involved fixing the offset at zero for all layers in the generator networks. This measure aimed to maintain ReLU units active by introducing competition through batch normalization. However, late in training, the right-skewed input to generators' ReLU units caused them to open less frequently. Experimenting without parameter sharing among generators on CIFAR-10 revealed that even with 4 generators and 128 feature maps, learning failed. The model achieved an Inception Score of 7.42 when the number of feature maps in the penultimate layer of each generator was set to 32. The parameter sharing scheme balances the capacity of generators and the discriminator/classifier. Experiment settings closely followed the network architecture and training procedure of DCGAN on three large-scale natural scene datasets (CIFAR-10, STL-10, ImageNet). Specifications of models trained on different datasets are described in Tabs. (4, 5, 6, 7). The last layer of the model consists of separate fully connected layers for the classifier and discriminator. Figures (9, 10, 11, 12, 13) are enlarged versions of figures (3a, 3b, 3c, 4a, 4b) in the main manuscript. The optimizer used is Adam with specific parameters for weight and bias initialization."
}