{
    "title": "HkenPn4KPH",
    "content": "The technique presented aims to enhance deep representations learned on small labeled datasets by incorporating self-supervised tasks as auxiliary loss functions. Self-supervised learning (SSL) has shown to reduce the error rate of few-shot meta-learners by 4%-27% on small datasets, especially in challenging tasks with smaller training sets. However, SSL may have a negative impact on performance in cases of domain shift between meta-learning and SSL image distributions. A technique is proposed to automatically select SSL images from a large pool of unlabeled images for a given dataset using a domain classifier. Recent work explores the effectiveness of SSL for few-shot learning by training a feature representation on a training dataset of \"base\" classes to generalize to novel classes with limited labeled data. Humans can quickly learn new concepts from limited training data by relying on past visual experience. Training for base class classification can lead to the network discarding semantic information critical for novel classes. To recover this information, representation learning techniques like unsupervised or self-supervised learning can be leveraged to learn statistical regularities. These techniques have only been applied to a few domains so far. This paper explores the use of self-supervised training to improve few-shot learning techniques across various domains without the need for additional training data. The study shows that adding a self-supervised task as an auxiliary task enhances the performance of existing few-shot techniques on different benchmarks, especially in more challenging tasks. The benefits of self-supervision increase with task difficulty, such as training from a smaller dataset or using degraded inputs like low resolution images. Additional unlabeled images can improve performance, but they should be from the same domain as the base classes. Using a domain classifier to select similar-domain unlabeled images for self-supervision can enhance few-shot learning techniques. Few-shot learning aims to generalize representations to novel classes with few images. Meta-learning approaches evaluate representations by sampling few-shot tasks within a base dataset domain. Self-supervision benefits from similar-domain unlabeled images, improving few-shot learning performance. This method enhances model training beyond domain-specific self-supervised learning, leading to better performance on standard classification tasks. Recent advancements in few-shot learning include methods such as gradient unrolling, closed form solvers, convex learners, matching networks, prototypical networks, and graph networks. Some approaches model the mapping between training data and classifier weights using feed-forward networks. A study by Chen et al. (2019) found that deeper network architectures diminish differences between meta-learners. They developed a strong baseline for few-shot learning based on ResNet18 architecture and prototypical networks. Auxiliary self-supervised tasks have shown to provide additional benefits in few-shot learning. Self-supervised learning aims to extract structural information from unlabeled data, reducing the need for expensive human labels. Various methods involve predicting missing parts of visual data or patch positions in images. These approaches have shown benefits in few-shot learning, especially when combined with state-of-the-art deep networks like prototypical networks. The advantages of self-supervised learning extend to different meta-learners such as MAML. Various self-supervised tasks in computer vision involve predicting rotation, noise, clusters, number of objects, missing patches, motion segmentation labels, artifacts, next patch feature, and maximizing mutual information. Researchers have also explored training adversarial generative models and jointly training multiple self-supervised tasks for improved results. In a study comparing self-supervised learning tasks, jigsaw puzzles and image rotation prediction were found to be effective. Prior works focused on replacing fully supervised learning with unsupervised learning on unlabeled datasets. However, our work combines supervised and self-supervised losses to learn representations on small datasets, showing a different approach. Our work explores the use of self-supervised tasks like jigsaw puzzles and rotation prediction as data-dependent regularizers for shared feature backbone. It also investigates the impact of domain shifts in SSL and multi-task learning, which aims to improve task objectives by training on multiple tasks together. Previous studies have shown mixed results on combining tasks, with some indicating a decrease in performance on individual tasks. The study explores the benefits of combining self-supervised tasks and few-shot learning, showing their mutual advantage without the need for annotations. Other related works also support the effectiveness of self-supervised pretraining and domain generalization. In the study, the authors demonstrate that self-supervision enhances few-shot learning, particularly on smaller datasets with challenging recognition tasks. They also introduce a new method to automatically select similar-domain unlabeled data for self-supervision, aiming to improve generalization on novel classes. The framework combines meta-learning for few-shot learning with self-supervised learning on unlabeled images. It involves training a convolutional network to map input images to an embedding space and then to the label space using a classifier. The goal is to minimize a loss function on predicted labels and apply suitable regularization. The framework combines meta-learning for few-shot learning with self-supervised learning on unlabeled images. It involves training a convolutional network to map input images to an embedding space and then to the label space using a classifier. A commonly used loss is the cross-entropy loss and a regularizer is the 2 norm of the parameters of the functions. Self-supervised losses act as a data-dependent regularizer for representation learning, derived from inputs x alone. The domain of images for supervised and self-supervised losses (D s and D ss) can be different. Various losses are considered for image classification tasks, with results presented using a meta-learner based on prototypical networks. The meta-training involves sampling N classes from the base set D b, creating support and query sets for N-way K-shot classification tasks. The objective is to minimize prediction loss on the query set. The prototypical networks aim to minimize prediction loss on the query set by recomputing class prototypes for classification and classifying query examples based on distances to prototypes. The approach is related to distance-based learners like matching networks and metric-learning based on label similarity. Additionally, results using a gradient-based meta-learner called MAML and standard classification results are presented. Two losses are considered, including the Jigsaw puzzle task loss. The Jigsaw puzzle task loss involves tiling input images into 3x3 regions and permuting them randomly to create a target label. The Rotation task loss rotates input images by angles of 0\u00b0, 90\u00b0, 180\u00b0, or 270\u00b0 to obtain target labels. Cross-entropy loss is used for both tasks, with different batches of images used for SSL and meta-learning experiments. The experiments involve datasets from various domains such as Caltech-UCSD birds, Stanford cars, FGVC aircrafts, Stanford dogs, and Oxford flowers. Mini-ImageNet and tiered-ImageNet benchmarks are used for few-shot learning, with classes split into base, val, and novel sets for training and validation. The model is trained on base categories, validated on val set, and tested on novel categories with few examples per class. Different datasets are used with original splits for some and a \"degraded\" base set for others to study SSL effectiveness on smaller datasets. Meta-learners and feature backbone follow best practices for few-shot learning. The study by Chen et al. (2019) utilizes a prototypical network with a ResNet18 backbone for few-shot learning experiments. They compare their method with other meta-learners like MAML and softmax classifiers using 5-way, 5-shot learning scenarios. Training is done with ADAM optimizer and a learning rate of 0.001 for 60,000 episodes, reporting mean accuracy over 600 test experiments. Test episodes involve selecting N classes from a novel set with support and query images. Data augmentation significantly impacts few-shot learning performance. Chen et al. (2019) outlined the procedure for strong baseline performance in meta-learning. Images are resized and cropped for label, rotation, and jigsaw puzzle predictions. Experiments include significant data augmentation such as cropping, flipping, and color jittering. The benefits of self-supervised learning for few-shot tasks are highlighted, showing improved accuracy over baseline models on harder datasets. Self-supervised models avoid accidental correlations between background features and class labels, demonstrating effectiveness even on smaller datasets. The study demonstrates that self-supervised learning enhances few-shot learning accuracy on various datasets. Tasks like jigsaw puzzles and predicting rotations improve the baseline ProtoNet model, with combined tasks showing potential benefits. The results align with previous research on the effectiveness of self-supervised learning for improving model performance on challenging datasets. The study shows that tasks like rotation can improve performance on challenging datasets. Low-resolution images are harder to classify for man-made categories, while color information is crucial for natural categories. Self-supervision leads to higher improvements on certain datasets compared to color images. In experiments with low-resolution images, SSL shows larger benefits when only 20% of images are used for training. Random sampling of unlabeled data can hurt performance, while domain-weighted sampling improves it. Selected images from related domains can enhance performance. In experiments with low-resolution images, SSL shows larger benefits when only 20% of images are used for training. Selected images from related domains can enhance performance, improving generalization to other meta-learners. Combining SSL with MAML and a standard feature extractor trained with cross-entropy loss boosts accuracies across fine-grained datasets. Self-supervision alone lags behind supervised learning in experiments. Initialization with SSL followed by meta-learning did not improve accuracy compared to meta-learning from random initialization. SSL acts as a feature regularizer, and scaling it to massive unlabeled datasets is a promising research direction. The effectiveness of SSL on tasks with more unlabeled data remains unclear, as prior works focus on curated datasets like ImageNet. Experiments are conducted to analyze the impact of image size and distribution on SSL. Experiments were conducted to characterize the effect of image size and distribution on semi-supervised learning (SSL) for few-shot learning on a specific domain. Varying the number of unlabeled images used for SSL showed improved accuracy, with diminishing returns as the size of the unlabeled set increased. Additionally, replacing a fraction of unlabeled images with images from other datasets was also explored. The effectiveness of SSL decreases as the fraction of out-of-domain images increases. Training with SSL on the available 20% within domain images is often better than increasing the set of images by five times to include out of domain images. The magnitude of the domain shift affects performance, with all images in base classes used and SSL images sampled from different datasets. The study analyzed the effectiveness of SSL on different domains using image datasets from iNaturalist and miniImageNet. The distance between domains was computed using image embeddings, showing that SSL's effectiveness decreases with greater domain distance. The study proposes a method to select images for SSL from a large pool based on domain similarity. The study proposes a method to select images for semi-supervised learning (SSL) from a large pool based on domain similarity. They use a \"domain weighted\" model to select top images based on a domain classifier, evaluating the approach using images from different datasets. The study introduces a method for selecting images for semi-supervised learning (SSL) based on domain similarity. Results show that selecting images based on importance weights improves performance on certain datasets compared to random selection. This approach enhances transferability of representations in SSL. Self-supervision improves transferability of representations on few-shot learning tasks across different domains, especially for more challenging problems with a small number of images. Additional unlabeled images only enhance performance if they are from the same or similar domains. A novel approach is presented to automatically identify similar-domain images from a larger pool. Experiments involve splitting classes into base, val, and novel sets for evaluation. Image representations learned on the base set are tested on the novel set, with self-supervision improving performance on various datasets. However, supervised labels still outperform self-supervision. Jigsaw puzzle loss is also tested on harder benchmarks for few-shot learning tasks. ProtoNet with jigsaw puzzle loss shows improved performance on harder benchmarks, especially when using only 20% of images in base categories. Results on few-shot transfer tasks using different meta-learners also demonstrate the effectiveness of jigsaw puzzle loss, with ProtoNet outperforming others on all five datasets. SSL is explored to enhance training of deep networks like ResNet18 from scratch, showing potential for improvement in image classification tasks. Training with self-supervision improves performance on various datasets, with accuracy improvements seen in predicting rotation and jigsaw puzzle loss. Self-supervision enhances the accuracy of a ResNet18 network trained from scratch over supervised training with cross-entropy loss on birds, cars, dogs, aircrafts, and flowers datasets. Visualizing pixel contributions to correct classification helps understand the generalization of representations. Training with self-supervision improves performance on various datasets by focusing more on foreground regions, reducing reliance on background features. Saliency maps show the importance of pixels for correct classification, enhancing fine-grained recognition and localization for few-shot learning. During training for few-shot learning, it is beneficial to estimate batch normalization statistics independently for each batch instead of tracking running mean and variance. The accuracy increases with batch size but saturates at 64. A trade-off term \u03bb between supervised and self-supervised losses can be used, with \u03bb = 0.5 generally working best. However, for training on mini-and tiered-ImageNet with jigsaw loss, setting \u03bb = 0.3 is more effective due to higher variation in image size and categories. When training meta-learners, 16 query images per class are used, reduced to 5 when only 20% of labeled data is available. MAML utilizes 10 query images and an approximation method for backpropagation to save GPU memory. Self-supervised loss is incorporated in the outer loop during training. PyTorch is used for experiments, and a domain classifier is trained using features from a ResNet101 model. Positive class images come from the labeled dataset, while negative class images come from the pool. The positive class images come from the labeled dataset, while negative class images come from the pool. A loss for the positive class is scaled by the inverse of its frequency. Optimization details for standard classification involve training a ResNet18 network from scratch with an ADAM optimizer. For self-supervised tasks like the jigsaw puzzle task, a ResNet18 architecture is used with a fully-connected layer added on top. The ResNet18 network is used with a fully-connected layer for self-supervised tasks like the jigsaw puzzle task. Nine 512-dimensional feature vectors are concatenated and passed through fc layers for the jigsaw and rotation tasks. The dataset composition for SSL experiments is described in Table 6. The \"domain weighted\" dataset selects more images from related domains based on domain distance, showing larger improvements for birds, dogs, and flowers."
}