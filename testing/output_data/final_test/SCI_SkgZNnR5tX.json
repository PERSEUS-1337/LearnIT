{
    "title": "SkgZNnR5tX",
    "content": "Reinforcement learning agents are evaluated based on their performance across various environment settings. This study focuses on worst-case analysis to identify potential failures in agent generalization. In a 3D first-person navigation task with procedurally generated mazes, agents with high average performance can still experience catastrophic failures on simple mazes. These failures can transfer between different agents. Reinforcement learning agents can experience failures in generalization that transfer between different agents and architectures. Worst-case analysis is crucial in identifying these failures to develop more robust agents. Initial results show enriching training with settings causing failure can help address these issues. To uncover biases in training distribution and agent strategies, evaluating worst-case performance over environment settings is proposed. This analysis can aid in understanding robustness and generalization in reinforcement learning agents, revealing biases and failures to generalize. The curr_chunk discusses how catastrophic failures can help reveal biases in training and the importance of robustness in critical systems. It also highlights the need to limit exploitability by ensuring agents can generalize to different environment settings. In this work, worst-case analysis is used to study a state-of-the-art agent's performance in a first-person 3D navigation task. The study reveals mazes where agents repeatedly fail to find the goal, termed Catastrophic Failures. Key contributions include introducing an approach to identify settings leading to failure, showcasing surprising failure cases in navigation tasks, demonstrating failure transfer across agents with different parameters, and initiating an investigation into training effects. The study focuses on adapting the training distribution for agents in first-person 3D navigation tasks. The tasks involve navigating a procedurally generated maze to reach a fixed goal location. The agent spawns at different locations each time they reach the goal during training. Agents receive RGB observations of size 96 \u00d7 72 pixels during navigation tasks. Episodes last for 120 seconds with a framerate of 15 frames per second. The agent gets a reward of 10 for reaching the goal and 0 otherwise. The study analyzes A2CV agents trained for human-level performance in navigation tasks. The training procedure details are provided in the appendix. If interested in worst-case performance, how can we determine environment settings? To find environment settings leading to worst-case agent performance, a local search procedure is proposed. This involves generating initial candidate mazes, modifying them based on agent scores, and visualizing the process. The goal is to identify settings causing catastrophic failures using black-box methods. The method involves using the lowest agent score to modify candidate mazes by moving walls, ensuring solvability. This iterative process results in a maze where the agent score is 0.09, demonstrating the ability to find catastrophic failure cases. The study evaluates agent performance in worst-case scenarios by testing on random mazes with altered wall structures. Agents demonstrate the ability to generalize to these random mazes, maintaining average scores. The study evaluates agent performance in worst-case scenarios by testing on random mazes with altered wall structures. Agents demonstrate the ability to generalize to random mazes, with an average of 45 goal reaches per two-minute episode. The fixed agent spawn location contributes to increased performance. The investigation focuses on whether agents have learned a general navigation strategy for all solvable mazes or if catastrophic failures are possible. The study evaluates agent performance in worst-case scenarios by testing on random mazes with altered wall structures. The search algorithm quickly identifies mazes where agents fail to find the goal, with some mazes leading to failure across multiple agents tested. The average number of goals reached per episode is 45, but some mazes only result in the goal being found 10-20% of the time. The study evaluates agent performance in worst-case scenarios by testing on random mazes with altered wall structures. The search algorithm quickly identifies mazes where agents fail to find the goal, with some mazes leading to failure across multiple agents tested. The objective used for the optimizer is the average number of goal reaches during an episode, with a low probability of at least one goal retrieval. Despite defining catastrophic failure as the failure to find the goal, the optimization process focuses on finding mazes with a lower average number of captures rather than those where the agent rarely finds the goal. The study evaluates agent performance in worst-case scenarios by testing on random mazes with altered wall structures. Despite finding the goal on average 45 times per episode on randomly perturbed mazes, on mazes optimized to reduce score, agents find the goal on average only 0.33 times per episode, showing a significant decrease in performance. The failure trajectories demonstrate a lack of efficient memory usage in exploring the maze, with agents repeatedly visiting the same locations. The complexity of mazes during failures suggests that more complex mazes should be included in training for agents to truly master navigation. To improve agent navigation mastery, more complex mazes should be included in training. The study questions if simple mazes also lead to catastrophic failure due to increased complexity, such as more dead ends and lower visibility. Understanding failure causes in mazes with numerous wall structures is challenging. The exploration focuses on identifying simple mazes that result in catastrophic failures. The study explores maze complexity by evaluating the total number of walls and human performance on mazes. Simple mazes leading to catastrophic failures are identified using a search algorithm that removes walls iteratively. Results show that even with a majority of walls removed, catastrophic failure can still occur. The study investigates maze complexity by analyzing the impact of maze structure on agent performance. Simple mazes with the goal placed in a small room lead to catastrophic failure for the agent. Human experiments show that humans can consistently find the goal in simplified mazes, indicating that these mazes are easily solvable with a general navigation strategy. The study compares agent and human performance in navigating simple mazes, finding that specific maze structures lead to agent failure regardless of spawn locations. This suggests the presence of local maze structures causing agents to fail, highlighting the importance of understanding training strategies for agents. In this section, the study investigates whether maze structures that cause one trained agent to fail also lead to failure in other agents. Two types of transfer are considered: between different hyperparameters of the same model architecture and between different model architectures. Sets of A2CV agents with varying entropy costs and learning rates, as well as MERLIN-based agents BID23 with sophisticated memory structures, are trained to test transfer between agents of the same and different architectures. The study compares transfer between different hyperparameters of A2CV agents and MERLIN-based agents with sophisticated memory structures. Both achieve human-level scores on a navigation task, with MERLIN scoring higher on average. Transfer of failure cases is observed across all agents, indicating some level of transfer exists between agents. The study compares transfer between different hyperparameters of A2CV agents and MERLIN-based agents with sophisticated memory structures. Transfer of failure cases is observed across all agents, suggesting a common cause of failure related to training distribution and methods used. Transfer within agent type is stronger than between agent type, indicating the need for enriching the training distribution to encourage more general solutions. The study suggests that there are common biases in agents based on their architecture type. A2CV agents show less susceptibility to transfer compared to MERLIN agents. Analyzing structural differences in mazes could provide insights into behavioral differences among agents. Transfer of failure cases is observed across agents of the same architecture with different hyperparameters, as well as across agents of different architectures. The study explores transfer across agents with different architectures, noting weaker transfer compared to agents with the same architecture. It investigates improving agent performance by adapting the training distribution with adversarial mazes. A comparison between adversarial mazes and original mazes reveals differences in catastrophic failure probabilities. The study compares adversarial mazes to non-adversarial mazes, noting differences in goal placement and path length. Adversarial mazes are found to be out-of-distribution and far from the training distribution due to the Modify function used in the search procedure. Incorporating adversarial mazes into training can improve agent's performance. Two approaches are considered: adding adversarial mazes to training distribution and modifying original mazes. 6000 unique adversarial mazes are created for training. Incorporating adversarial mazes into training can enhance agent performance by creating a dataset of 6000 unique adversarial mazes. A new set of A2CV agents is trained using both adversarial mazes and standard distribution, with 50% of training episodes on adversarial mazes. The maze generator is altered to ensure diverse adversarial mazes are produced. The study focuses on the robustness of agents trained using these approaches. The study focuses on the robustness of agents trained using approaches that incorporate adversarial mazes into training. Despite agents performing well on richer distributions of mazes, they still experience catastrophic failures. Agents trained on enriched distributions with 6000 adversarial mazes were able to find the goal most of the time on the trained mazes, but extreme failures still occurred. This suggests that the set of adversarial mazes used for training may not have provided sufficient coverage of the maze space. Enlarging the set of adversarial mazes used for training could lead to more general and robust agents. Results show that agents trained on randomly perturbed mazes required more iterations to reach the same failure level as those trained on standard distributions. However, challenges need to be addressed before testing this approach. Using a larger set of environment settings leading to failure may be necessary for training more robust agents, similar to adversarial training in supervised learning. Challenges in using more adversarial examples for learning in reinforcement learning include the high cost of generating adversarial settings and the sparse training signal due to low rewards. Faster methods for generating adversarial settings and designing a curriculum may help address these challenges. A possible solution to the challenges of using adversarial examples in reinforcement learning is to design a curriculum that includes easier variants of adversarial settings in the training distribution. This approach aims to establish the utility of adversarial retraining in RL, similar to its success in supervised learning tasks. However, overcoming these challenges will require significant effort and work before a conclusive answer is reached. The RL community has recently focused on agent navigation in simulated 3D environments, with a community-wide challenge for agents in such settings. Recent advances in the RL community have focused on agent navigation in 3D environments, such as the VizDoom BID14 challenge. These tasks require agents to effectively perceive and interpret the 3D world to make strategic decisions. This work can be compared to adversarial attacks in supervised learning, where systems are tested on inputs outside the original distribution. In the context of agent navigation in 3D environments, recent work focuses on adversarial attacks outside the original distribution. The new approach involves changing latent semantic features of the environment, unlike previous methods that altered individual pixels in input images. The failure occurs over multiple interactions between the agent and environment, rather than a single neural net pass. This differs from constrained adversarial attacks in supervised learning for image classification. In the context of interpretable adversarial examples in image classification, approaches similar to the simplification approach have been explored. Findings on transfer in adversarial examples for computer vision networks show that perturbations often transfer across different networks. Previous works have extended adversarial attacks to RL settings by manipulating inputs directly, changing the environment renderer. In the context of testing generalization in RL systems, recent work has shown that simple agents trained on restricted datasets struggle to learn general navigation strategies. Our method automatically identifies significant failures in exploration during navigation, contrasting with previous studies focusing on exploiting knowledge from previous goal retrievals. This research aims to characterize how agents generalize beyond statistical averages. In this work, despite strong average-case performance of RL agents, worst-case analysis reveals failures in generalization. Simple catastrophic failures exist that agents should have learned from, and failures transfer between agents and architectures. Understanding worst-case performance and generalization modes in complex tasks like AirSim BID18 and CARLA BID5 is crucial. Investigating and addressing these behaviors in real-world applications like self-driving cars and robotics is essential for robust and generalizable agents. Training robust and generalizable agents is crucial for addressing failures in generalization. Enriching the training distribution with settings leading to failure may require large-scale efforts. Despite higher performance, modifications were made to the training procedure for the A2CV agents in this work. The A2CV agents were trained for 10 billion steps using a simplified action set and rewards were clipped to [-1, 1]. The increase in training steps by approximately 30x led to higher performance, with agents achieving average rewards between 310 and 320. The agent model was a simplified variant of a model presented in BID23, with the stochastic latent variable model removed to reduce training time in multi-task scenarios. The state representation in the agent model was a deterministic transformation based on the recurrent controller state and memory reads. The policy network used a feedforward multi-layer perceptron to compute the action distribution. After training, the agents achieved average rewards between 340 and 360. The search algorithm in the study used 10 candidate mazes per iteration, each evaluated 30 times, across 20 iterations, totaling 6000 episodes. The search procedure took around 30 minutes to complete, with a 3x reduction in resources by reducing evaluations per maze. Training agents took 4 days with 150 parallel workers, while the search procedure used 200 parallel workers and took 30 minutes. In 44/50 optimization runs, the search algorithm found adversarial mazes where the agent's success rate was <50%. Humans successfully completed all mazes, ruling out visual acuity as a factor. In most episodes, humans found the goal in less than a third of the episode, while the best-performing agent found the goal less than 50% of the time. Transfer experiments between different agents were detailed, including A2CV and MERLIN agents trained with varying entropy costs and learning rates."
}