{
    "title": "B16yEqkCZ",
    "content": "In this paper, intrinsic fear is introduced as a reward shaping technique to accelerate deep reinforcement learning and prevent catastrophic states. A second model trained via supervised learning predicts imminent catastrophes, penalizing the Q-learning objective. The approach shows improvement in solving toy environments and Atari games like Seaquest, Asteroids, and Freeway. Researchers have explored practical applications of deep reinforcement learning (DRL) in robotics, dialogue systems, energy management, and self-driving cars. The concern arises about trusting DRL agents in real-world environments due to the potential for catastrophic outcomes. Extensive prior knowledge of the environment is necessary to prevent agents from making catastrophic mistakes. In this paper, a specific notion of avoidable catastrophes is introduced, defined as states that an optimal policy should never visit based on prior knowledge. Proximity to these catastrophes is defined in trajectory space, and states near them are labeled as danger states. It is assumed that a catastrophe detector exists to help agents avoid dangerous states after encountering a catastrophic event. The paper addresses the challenges of whether DRL agents can learn to avoid dangerous states. The paper introduces the concept of avoidable catastrophes and the challenges of DRL agents learning to avoid dangerous states. Experiments show that DQNs struggle to learn from catastrophic failures and are prone to repeating mistakes, posing a significant obstacle to using them in real-world applications. This raises concerns about handing over control to DRL agents, such as self-driving cars, if they continue to make grave errors while learning. The tabular approach in RL guarantees eventual convergence to a globally optimal policy, but becomes infeasible in high-dimensional, continuous state spaces. DQNs face challenges due to function approximation when training a neural network based on experiences, leading to states that a learned policy never encounters forming a small region of the training distribution. The DQN faces catastrophic interference issues when training under distribution D, leading to policy drift and avoidable mistakes. A simple problem called Adventure Seeker highlights the brittleness of modern DRL algorithms, showcasing the failure of DQN in certain dynamics. In response to the DQN's failure in dynamic environments like Cart-Pole, a new approach called intrinsic fear is proposed. This method involves training a fear model to predict catastrophic states and penalize Q-learning targets accordingly. Inspired by intrinsic motivation, the approach aims to discourage revisiting dangerous states. Empirical and theoretical validation is conducted on various environments including Adventure Seeker, Cartpole, Seaquest, Asteroids, and Freeway. The intrinsic fear agent successfully learns to avoid catastrophic states in these environments. The intrinsic fear agent successfully avoids catastrophic states in dynamic environments like Seaquest and Asteroids, showing significant improvement. The method is proven to be robust to noise in the danger model and demonstrates that the policy learned on the altered value function has a return similar to the optimal policy on the original value function. Temporal differences methods like Q-learning model the Q-function to maximize cumulative discounted return. Q-function is approximated by neural networks for large state spaces. In Q-learning with function approximation, the agent collects experiences and updates parameters to minimize the squared Bellman error. In Q-learning with function approximation, the agent minimizes the squared Bellman error by updating the Q-function using experience replay. The safety problem is addressed by identifying catastrophic states that optimal policies rarely encounter. In Q-learning with function approximation, the agent minimizes the squared Bellman error by updating the Q-function using experience replay. The safety problem is addressed by identifying catastrophic states that optimal policies rarely encounter. Moreover, a novel algorithm called Intrinsic Fear (IF) is proposed for avoiding catastrophes when learning by defining danger states and catastrophe detectors. Intrinsic Fear (IF) is a novel algorithm for avoiding catastrophes in online learning with function approximation. It involves maintaining a DQN and a supervised fear model F, which penalizes the Q-learner for entering dangerous states. The fear model helps shape rewards away from suboptimal states, accelerating learning and protecting DQNs from catastrophic forgetting. Once trained, the fear model allows the Q-learner to update and avoid catastrophes without experiencing them again. Our instantiation of intrinsic fear involves maintaining a fear model alongside a DQN to predict catastrophic outcomes in states. The fear model helps the agent avoid dangerous situations and prevents catastrophic forgetting during training. Our agent adds experiences to its replay buffer during training and identifies danger states leading up to catastrophes. Updates to the fear model are made by sampling from danger and safe states, perturbing the TD target to introduce intrinsic fear. The model selects actions based on probabilities and executes them in the environment, observing rewards and successor states. A fear model is updated by sampling from danger and safe states, perturbing the TD target to introduce intrinsic fear. The optimal policy \u03c0* of an MDP maximizes the average reward return, where the fear model can boost learning the optimal policy by detecting danger zones. The RL agent uses an imperfect detector and fear model empirically learned from collected data. The RL agent, with access to an imperfect detector and fear model, trains with intrinsic fear to learn a different value function. Discount factors \u03b3 < 1 are used to reduce planning horizon and computation cost. Smaller discount factors are recommended when there is estimation error in the MDP model. Using a discount factor \u03b3 \u2264 \u03b3 eval can reduce the average deviation in value functions under modeling errors. The average deviation in value functions can be reduced by using a discount factor \u03b3 \u2264 \u03b3 eval under modeling errors. Theorem 2 states that as the number of samples N increases, the average deviation on value functions vanishes with probability at least 1 \u2212 \u03b4. This result holds for both tabular and continuous state-action MDPs. Additionally, a deeper theoretical analysis on deterministic and stochastic fear is provided. In our experiments, we found that intrinsic fear models are more effective with a larger fear radius, allowing for correction of policies without experiencing many catastrophes. Wider fear factors train more stably when phased in gradually over many episodes. We gradually phase in the fear factor \u03bb from 0 to full strength at a predetermined time step k \u03bb. Our algorithms were tested on three environments, including Adventure Seeker. Our algorithms were tested on three environments: Adventure Seeker, Cartpole, and three Atari games. Adventure Seeker involves a player on a hill, moving left or right with added noise. Falling off the hill results in termination of the episode. The successor state is calculated with noise added, and rewards are proportional to the player's position on the hill. The player in Adventure Seeker moves on a hill with noise, earning rewards based on height. If the player goes above 1.0 or below 0.0, the game ends. Despite an obvious solution, a DQN model struggles to consistently choose the correct direction, oscillating between optimal and suboptimal policies. This inconsistency persists even with continued training, raising doubts about the system's reliability. In the Cart-Pole RL environment, an agent balances a pole on a cart by choosing actions to keep it upright. The game has four catastrophe modes where the pole can fall to the right or left, or the cart can run off the screen. The agent receives a reward for each time step the pole remains upright, but the episode ends if the pole falls. The system's reliability is questioned due to its struggles even with continued training. In experiments, the implementation CartPole-v0 in the openAI gym BID4 is used. The problem admits an analytic solution where a perfect policy should never drop the pole. However, a DQN converges to a constant rate of catastrophes per turn. Additionally, Atari games like Freeway, Asteroids, and Seaquest are addressed. In Freeway, the agent controls a chicken crossing the road while dodging traffic. Asteroids involve piloting a ship and shooting asteroids while avoiding collisions. Seaquest requires the player to swim underwater, periodically rising to the surface for oxygen. In the Adventure Seeker problem, the player swims underwater, shoots fishes for points, and must rise to the surface for oxygen. The agent has 3 lives, with death resulting from colliding with a fish or running out of oxygen. The intrinsic fear model is evaluated using a standard DQN and one enhanced by fear. Fear factor is phased in quickly, reaching full strength in 1000 moves, with a fear radius set to 5. For Cart-Pole, a DQN converges to a constant rate of catastrophes per turn. The fear factor \u03bb is set to 40 for the Adventure Seeker problem. The fear radius for Cart-Pole is set to kr = 20. Models with shorter fear radius had more catastrophes. Unsuccessful models output danger probability > .5 5 moves before catastrophe. Successful models output predictions in the range .1 \u2212 .5, providing a richer reward signal to the DQN. The DQNs augmented by intrinsic fear outperform their counterparts on Adventure Seeker and Cart-Pole environments. The fear models do not terminate episodes after several deaths, unlike DQN and related approaches. Traditional methods for mitigating catastrophic forgetting did not improve over DQN. Fear radius and factor settings were used for Seaquest, Asteroids, and Freeway, resulting in IF models outperforming DQN on all Atari games. The IF models outperform DQN counterparts on all Atari games, showing higher rewards. Seaquest models with Intrinsic Fear have fewer catastrophes early on but eventually exchange more catastrophes for higher rewards. Asteroids and Freeway see dramatic improvements with intrinsic fear reward shaping, addressing safety in RL and stability of Q-learning with function approximation. Several papers address safety in RL, with BID7 providing a review on methods that perturb the objective function or use external knowledge to improve exploration safety. BID10 defines fatality as returns below a threshold and proposes a safety function to identify unsafe states. BID11 suggests a Q-learning objective focused on minimum return instead of expected return. Safety in reinforcement learning (RL) is a key objective, with various approaches proposed to improve exploration safety. Some papers suggest modifying the objective to penalize high-variance returns, while others focus on maximizing expected returns while minimizing variance. Definitions of safety based on ergodicity and penalties to discourage accidents have also been explored. Incorporating external knowledge into the exploration process is another avenue, although this often requires access to an oracle or extensive prior knowledge of the environment. The challenge of distributional shift leading to perpetual revisiting of catastrophic failure modes remains unaddressed in existing works. In RL, safety is a key concern with approaches to improve exploration safety. Some papers suggest confining policy search to known safe policies. The issue of covariate shift has been extensively studied. Addressing catastrophic forgetting in RL with function approximation, a memory-based solution is proposed. Intrinsic rewards are used for exploration and acquiring skills. Some papers refer to intrinsic rewards for discovery as curiosity. This paper introduces a new approach to safe reinforcement learning by avoiding catastrophic transitions and catastrophic forgetting. It proposes algorithms to prevent catastrophic states and provides theoretical analysis to support its methods. The paper introduces a new class of algorithms to avoid catastrophic states in reinforcement learning. Experiments show that DQNs can make periodic mistakes, raising concerns about their real-world applicability. Examples like a domestic robot acting as a barber highlight the risks of rewards leading to unintended consequences. The intrinsic fear model suggests that recognizing catastrophic states after the fact can help prevent them. This work aims to address safety issues in reinforcement learning by preventing catastrophic forgetting. It focuses on optimizing policies to maximize expected average rewards while considering the stationary distribution induced by the policy. The joint distribution of states and actions is defined to reframe the optimization problem. The optimization problem is reformulated in terms of the joint probability distribution \u00b5 \u03c0, turning it into a linear function. The feasible values for \u00b5 \u03c0 form a polytope on the simplex in R S\u00d7A, leading to a constrained linear program. The introduction of intrinsic fear is analyzed by minimizing the optimal policy's return in dangerous situations. The introduction of intrinsic fear in the environment affects the policy's return. The return on the environment with intrinsic fear is lower bounded by a certain value. Even with an imperfect classifier, the overall performance in practical RL problems is not significantly impacted. In practical RL problems, using discount factors \u03b3 eval < 1 helps reduce planning horizon and computation costs. It is recommended to use \u03b3 \u2264 \u03b3 eval when there is an estimation of the MDP model. Choosing the discount factor \u03b3 \u2264 \u03b3 eval ensures that the learned Value function remains close to the Value function under a perfect classifier. The text discusses the importance of choosing a discount factor \u03b3 \u2264 \u03b3 eval in reinforcement learning to minimize deviation in value functions caused by imperfect classifiers. The goal is to find an optimal \u03b3 * that minimizes this loss with high probability. The upper bound on the deviation can be decomposed into two parts: one related to applying the same policy with different discount factors, and the other related to different policies and discount factors. The text discusses the importance of choosing a discount factor \u03b3 \u2264 \u03b3 eval in reinforcement learning to minimize deviation in value functions caused by imperfect classifiers. The upper bound on the deviation can be decomposed into two parts: one related to applying the same policy with different discount factors, and the other related to different policies and discount factors. The quantity V \u03c0 F ,\u03b3 is the difference between the performance of the same policy on two different environments. The trajectory produced by the algorithm does not produce i.i.d. samples of state, so Hoeffding's inequality is used for analysis. The text discusses using Hoeffding's inequality and union bounds to analyze the upper bound on deviation in reinforcement learning. It involves considering multiple states and optimal policies under a noisy classifier. The analysis involves a set of binary classifiers and i.i.d. samples from a stationary distribution. The MDP transition process is reduced to a Markov chain, and PAC analysis of binary classification is applied for further analysis. Using PAC analysis, the upper bound on deviation in reinforcement learning is analyzed for binary classification in BID9. The analysis extends to continuous state and action spaces with slight modifications. The maximum eigenvalue is bounded above due to \u03b3 < 1."
}