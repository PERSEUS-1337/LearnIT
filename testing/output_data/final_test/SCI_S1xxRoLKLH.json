{
    "title": "S1xxRoLKLH",
    "content": "Neural population responses to sensory stimuli can exhibit nonlinear stimulus-dependence and structured shared variability. Adversarial training is used to optimize neural encoding models to capture deterministic and stochastic components of neural data, with the REBAR method accounting for the discrete nature of neural spike trains. This approach is illustrated using population recordings from the primary visual cortex, showing that adding latent noise-sources to a convolutional neural network captures stimulus-dependence and noise correlations in population activity. Generative adversarial networks (GANs) offer an alternative approach to fitting encoding models for neural population spike trains, providing a flexible and powerful method that can capture the statistics of empirical data. Unlike likelihood-based approaches, GANs are not limited to simple models of variability and can handle latent variables more effectively. Generative adversarial networks (GANs) use a pair of neural networks - a generator and discriminator - to match the statistics of empirical data. This approach has been used for image and text generation, as well as training models of spike trains and neural populations. Conditional GANs are proposed for training neural encoding models, as an alternative to likelihood-based approaches. The challenge lies in the discrete nature of neural spike trains, requiring the use of unbiased gradient estimators like REINFORCE and REBAR for better fitting performance. This approach is demonstrated by fitting a convolutional neural network model to multi-electrode recordings from V1. The study aims to train a GAN to generate binary spike counts matching empirical data statistics from V1 recordings. A CNN is used to model spike trains conditioned on stimulus and latent variables, with a discriminator receiving both stimulus and spike trains for probability output. The objective is to find the Nash equilibrium of a minmax game between the generator G and the discriminator D in GANs. Training GANs is challenging due to sensitivity to discriminator gradients. Cross-entropy objective is used with spectral normalisation for discriminator gradients and gradient norm clipping for generator gradients. Dealing with discrete data like spikes requires backpropagation through both networks. Previous solutions include concrete relaxation, REINFORCE, and REBAR. Attempts to address the sensitivity of discriminator gradients in training GANs include using concrete relaxation, REINFORCE, and REBAR to estimate gradients. Concrete relaxation treats binary variables as continuous values, while REINFORCE provides unbiased but high-variance gradients. REBAR combines concrete relaxation and REINFORCE, with both performing better than concrete relaxation in certain applications. The GANs were fitted to a dataset of cells recorded from macaque V1 while watching a repeated movie. The study involved recording cells from macaque V1 while the animals watched a repeated 30s movie consisting of 750 frames downscaled to 27 x 27 pixels. Spike data was binned at a 40ms resolution and binarized. A 3-layer CNN architecture was used for the generator, with convolutional layers and LeakyRELUs. Gaussian white noise was added to capture stimulus-independent variability shared between neurons. The study involved training two networks in parallel for 15k epochs using ADAM optimization. A 3-layer CNN generative model was fitted to neural population data recorded in the V1 area of macaque visual cortex, with adversarial training. The study trained a 3-layer CNN generative model in the V1 area of macaque visual cortex using adversarial training. A CNN without shared noise layers and a Dichotomised Gaussian model were also fitted to the dataset. All approaches captured firing rates accurately but the supervised model did not reproduce total pairwise correlations between neurons due to constrained noise-correlations. The GAN generator, with a few shared noise parameters added to the supervised model, accurately captured total correlations, population spike histogram, and pairwise noise-correlation matrix. However, both models did not generalize well to the held-out test-dataset due to short training-set and high constraints. The CNN and DG model performed similarly well in reproducing correlations and spike count histograms on the V1 dataset. Adversarial training can capture heteroscedastic noise and was limited only by the network's flexibility. The CNN trained with the GAN framework accurately captured covariances from 'low signal' and 'high signal' timebins, unlike the DG model. Adversarial training of conditional generative models can generate spike train data matching in-vivo recordings, using unbiased gradient estimators and spectral normalization for stability. Training of discrete GANs remains sensitive to discriminator architecture and hyper-parameter settings. Adversarial training can capture higher-order structure in neural data and temporal features in neural population data. It can be used for transfer learning across multiple datasets and to generate realistic spike trains for different stimuli or noise perturbations."
}