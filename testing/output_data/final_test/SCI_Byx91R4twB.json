{
    "title": "Byx91R4twB",
    "content": "Generative models of natural images have advanced in producing high-quality samples by leveraging scale. The Dual Video Discriminator GAN (DVD-GAN) model, trained on the Kinetics-600 dataset, can generate video samples with higher complexity and fidelity. It outperforms previous work in video synthesis and prediction tasks, achieving state-of-the-art results in Fr\u00e9chet Inception Distance and Inception Score on different datasets. Modern deep generative models trained on high-resolution and diverse datasets can produce realistic natural images. Video generation poses a challenge due to increased data complexity and computational requirements. Prior work focused on simple datasets or tasks with strong temporal conditioning. This study aims to extend generative image model results to video synthesis and prediction tasks using the BigGAN architecture. The study introduces a new model, DVD-GAN, based on the BigGAN architecture, for generating high-quality, coherent videos at resolutions up to 256x256 and lengths up to 48 frames. It outperforms previous models on video synthesis and prediction tasks. The study introduces DVD-GAN, a new model based on BigGAN, for high-quality video generation up to 256x256 resolution and 48 frames. It achieves state-of-the-art results on UCF-101 and Kinetics-600, establishing class-conditional video synthesis as a new benchmark. Conditioning signals vary from unconditional synthesis to strongly-conditioned models using different inputs like segmentation masks or pose information. In this work, the focus is on class-conditional video synthesis and future video prediction using Generative Adversarial Networks (GANs). The objective is to generate realistic temporal dynamics for videos of specific categories.GANs are defined by a minimax game between a Discriminator D and a Generator G, with improvements proposed for training stability. Generative Adversarial Networks (GANs) are known for their stability issues and limitations like mode collapse. Despite these drawbacks, GANs have produced high-quality samples in various visual domains. The Kinetics dataset, specifically Kinetics-600, is a large collection of high-resolution YouTube clips used for human action recognition tasks. The Kinetics dataset (Carreira et al., 2018) consists of 600 classes with around 500,000 diverse and unconstrained videos. It allows training large models without overfitting concerns. Compared to UCF-101, Kinetics-600 has almost 50 times more videos and increased diversity. Despite artifacts like cuts and visual effects from YouTube, state-of-the-art results were achieved on UCF-101. In this work, the authors focus on generative modeling of the Kinetics video dataset, specifically the full Kinetics-600 dataset. They mention prior studies that used a subset of Kinetics but highlight the differences in preprocessing and dataset size. The authors also discuss the challenges of designing metrics to evaluate the quality of generative models, particularly GANs. In this work, the authors introduce the Dual Video Discriminator GAN (DVD-GAN), a generative video model based on the BigGAN architecture. They utilize Inception Score (IS) and Fr\u00e9chet Inception Distance (FID) metrics, adapting them for video models using the Inflated 3D Convnet (I3D) network trained on Kinetics-600. Their Fr\u00e9chet Inception Distance is similar to Fr\u00e9chet Video Distance (FVD) but with a different implementation aligned with the original FID metric. The DVD-GAN architecture, based on BigGAN, introduces scalable video-specific generator and discriminator architectures. Unlike prior work, it relies on a neural network to learn foreground, background, and motion without explicit priors. The model is not autoregressive in time or space, making it computationally challenging to generate long, high-resolution videos. The DVD-GAN architecture introduces scalable video-specific generator and discriminator architectures. It addresses the challenge of generating high-resolution videos by using two discriminators: a Spatial Discriminator (DS) and a Temporal Discriminator (DT). DS critiques single frame content and structure by sampling k full-resolution frames, with k set to 8. This approach helps in determining if a video is generated by comparing spatial locations across frames. The DVD-GAN architecture introduces scalable video-specific generator and discriminator architectures. The Temporal Discriminator (DT) provides the learning signal for movement generation, while a spatial downsampling function is applied to reduce the number of pixels processed per video. Despite this reduction, the discriminator objective can still penalize inconsistencies effectively. The DVD-GAN discriminator judges temporal discrepancies and high-resolution details in videos. It is similar to MoCoGAN's per-frame discriminator but focuses on high-resolution details. D S is crucial for learning signals in DVD-GAN, unlike in MoCoGAN. Generative video modeling includes work on VAEs and recurrent models. Various models have been used in generative video modeling, including auto-regressive models, normalizing flows, and GANs. Prior work often focuses on decomposing objects into texture, spatial consistency, and temporal dynamics. Different approaches include splitting foreground and background models, incorporating optical flow or motion, and treating pose, content, and motion separately in the generator. Various models in generative video modeling, such as MoCoGAN and TGANv2, discriminate individual frames or groups of frames to reduce the number of pixels to discriminate. TGANv2 introduces \"adaptive batch reduction\" for efficient training by sampling subsets of videos within a batch and temporal subwindows within each video. Heads in TGANv2's G project intermediate feature maps directly to pixel space before applying batch reduction, with corresponding discriminators evaluating these lower resolution outputs. The TGANv2 model uses adaptive batch reduction to efficiently train by sampling subsets of videos and temporal subwindows. DVD-GAN's resolution reduction for full videos leads to performance loss, and further reduction may degrade quality. The training setup includes using TPU pods with 32 to 512 replicas and an Adam optimizer. Video synthesis models are trained for around 300,000 learning steps, while video prediction models are trained for up to 1,000,000 steps. The primary results focus on Video Synthesis, with benchmarks for Class-Conditional Video Synthesis on the Kinetics-600 dataset. Results establish a strong baseline for future work, showing Inception Score and Fr\u00e9chet Inception Distance for different resolutions and video lengths. Truncation curve analysis is also conducted by varying the standard deviation of latent vectors. The results show a high level of fidelity in generating videos at different resolutions and lengths, with Inception Score and Fr\u00e9chet Inception Distance used as metrics. Comparing videos of different lengths is not meaningful, and longer videos pose a more challenging modeling problem. DVD-GAN can generate plausible videos at resolutions of 64x64, 128x128, and 256x256, with lengths up to 4 seconds. The study demonstrates the ability of DVD-GAN to generate high-quality videos at various resolutions and lengths, with a focus on 48-frame models. Despite the challenges posed by higher resolutions, the generated scenes maintain coherence and detailed textures. The model's performance is further validated on the UCF-101 dataset, showcasing its effectiveness in video synthesis and prediction. Our model, DVD-GAN, outperforms state-of-the-art models in video synthesis with an IS of 27.38. Video prediction involves generating frames following initial conditioning frames, requiring G to analyze and evolve elements in the scene over time. The Fr\u00e9chet Video Distance is used for evaluation. Our model, DVD-GAN-FP, outperforms prior work on frame-conditional prediction for Kinetics-600 by generating 11 frames at 64x64 resolution conditioned on 5 frames without frame skipping. The results are shown in Table 4, with the final row corresponding to 16-frame class-conditional Video Synthesis samples. The video synthesis model, generated without frame conditioning and skipping, shows notably better FVD performance. The advantage of being able to select videos to generate outweighs having a ground truth distribution of starting frames, indicating a surprising result. The experiment favors the synthesis model for FVD, analyzing choices for k and \u03c6 to reduce discriminator input size while maintaining high generator quality.\u03c6 options include 2 \u00d7 2 and 4 \u00d7 4 average pooling, identity, and random half-sized crop of input. The study focused on optimizing the discriminator input size for a GAN model by exploring different options for reducing input size, such as using 2 \u00d7 2 and 4 \u00d7 4 average pooling, identity, and random half-sized crop of input. Results showed that a k value of 8 was settled on, with diminishing effects as k increased. Additionally, the study highlighted the reduced performance of 4 \u00d7 4 downsampling compared to 2 \u00d7 2, and the negative impact of using half-sized crops on model performance. The study emphasized the benefits of training generative models on large video datasets like Kinetics-600, showcasing state-of-the-art results with DVD-GAN. Experiments were conducted on various datasets with different resolutions, showing promising progress towards generating realistic videos. The study focused on training generative models on large video datasets like Kinetics-600, achieving state-of-the-art results with DVD-GAN. The datasets were processed at different resolutions, with a random sequence of frames selected for videos with more frames. Data augmentation techniques were applied to UCF-101 by randomly flipping videos. The model architecture was inspired by Brock et al. (2019), with network width determined by a channel multiplier and layer-specific constants. The DVD-GAN model uses different channel multipliers for videos of different resolutions. The input to the generator includes Gaussian noise and a class embedding. The generator starts by transforming the input into a tensor shape and uses it for class-conditional Batch Normalization layers. The input to the Convolutional Gated Recurrent Unit (CGRU) is the gray line in Figure 7. The CGRU processes the input using elementwise sigmoid and ReLU functions, followed by convolution and elementwise multiplication. The output is then passed through two residual blocks, with the time dimension combined with the batch dimension. Each frame proceeds independently through the blocks, resulting in doubled width and height dimensions after each iteration. The Convolutional Gated Recurrent Unit (CGRU) processes input using sigmoid and ReLU functions, followed by convolution and elementwise multiplication. The output is passed through residual blocks, with dimensions doubling after each iteration. The spatial discriminator functions similarly to BigGAN's discriminator, while the temporal discriminator preprocesses videos with downsampling. The temporal discriminator's first two residual blocks are 3-D. The DVD-GAN architecture utilizes 3-D convolutions with a kernel size of 3 \u00d7 3 \u00d7 3, following the structure of BigGAN. Sampling from DVD-GAN is efficient, with a feed-forward convolutional network allowing for quick video generation. The dual discriminator D is updated twice for every generator update, using Spectral Normalization for weight layers and orthogonal weight initialization. Sampling is done using an exponential moving average of G's weights, optimized with Adam and batch size 512. The FID metric used for Synthesis on Kinetics-600 is calculated similarly to Fr\u00e9chet Video Distance, with a different feature network (I3D trained on Kinetics-600) and features from the final hidden layer. This metric can be implemented by changing the TF-Hub module name and loading the necessary components. The TF-Hub module 'https://tfhub.dev/deepmind/i3d-kinetics-600/1' is loaded to extract the tensor 'RGB/inception_i3d/Logits/AvgPool3D'. A modification to DVD-GAN is made for future video prediction tasks, involving passing conditioning frames through a deep residual network. The extended model diagram is shown in Figure 8, with each row representing a video and the leftmost column as the conditioning frame. The design of G and D S's residual blocks allows for intermediate tensors to be generated for each conditioning frame, which are then stacked and processed through a convolution and ReLU activation. These features are used as the initial state for the model. The conditioning frames are passed through a deep residual network in a modification to DVD-GAN for video prediction tasks. The smallest features from the conditioning frames are input earliest in G, while larger features are input towards the end. D T operates on the concatenation of conditioning frames and the output of G. D S does not sample the first C frames for real or generated data to reduce wasted computation. The video prediction variant does not condition on any class information. Our video prediction variant does not condition on any class information, achieving an Inception Score of 27.38, outperforming the state of the art. The DVD-GAN architecture on UCF-101 is trained on 16-frame 128 \u00d7 128 clips, with an improved model reaching an Inception Score of 32.97 when trained with class labels. The improved Inception Score of 15.83 in 2017 is close to the ground truth data score of 34.49. Recent video generation papers often do not test in this regime. The improved score is partially due to memorization of training data. Interpolation samples from the best UCF-101 model show abrupt jumps in latent space between distinct samples, with some generated samples correlating with training set samples. The Inception Score metric fails to correlate with samples from the training set, indicating that UCF-101 dataset lacks complexity and diversity for video generation. Larger and more challenging datasets like Kinetics-600 are needed. Various normalization techniques were experimented with, with Group Normalization performing the best. Group Normalization performed almost as well as Batch Normalization in a batch of data, while Layer Normalization, Instance Normalization, and no normalization significantly underperformed Batch Normalization. Removing Batch Normalization in G after ResNet caused learning failure, but removing it within G's residual blocks still led to good generative models with higher IS scores. However, these models had worse FID scores and produced lower-quality video samples. Early variants of DVD-GAN included different normalization techniques. Early variants of DVD-GAN contained Batch Normalization which normalized over all frames of all batch elements, giving G an extra channel to convey information across time. However, normalizing over timesteps independently worked just as well without the dependence on statistics. Models based on BigGAN's residual blocks outperformed those based on BigGAN-deep's residual blocks in terms of accuracy. It is challenging to accurately convey generated video through still frames, so viewers are recommended to watch the videos themselves for a better understanding. The Truncation Trick (Brock et al., 2019) is used in DVD-GAN to produce higher quality samples from latents near the mean of the distribution. Interpolations in the latent space and class embedding show that G has learned a smooth mapping, capable of interpolating between distinct classes. DVD-GAN can interpolate between distinct classes smoothly in both intra-class and class interpolation scenarios. The model shows a smooth transition between videos generated under different classes and latent vectors, even without being trained on interpolated class data. DVD-GAN can smoothly interpolate between distinct classes in both intra-class and class interpolation scenarios, producing reasonable samples even without training on interpolated class data."
}