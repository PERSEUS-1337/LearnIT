{
    "title": "BkUHlMZ0b",
    "content": "The paper introduces a novel robustness metric called CLEVER, which stands for Cross Lipschitz Extreme Value for nEtwork Robustness. It proposes converting robustness analysis into a local Lipschitz constant estimation problem using Extreme Value Theory. The CLEVER score is attack-agnostic and computationally feasible for large neural networks. Experimental results on networks like ResNet, Inception-v3, and MobileNet show that CLEVER aligns with robustness indications from powerful attacks. The CLEVER metric evaluates the robustness of neural network classifiers independently of specific attacks. It shows that defending networks with defensive distillation or bounded ReLU improves CLEVER scores, indicating better robustness. Recent studies have highlighted the lack of robustness in state-of-the-art neural network models, as visually imperceptible adversarial images can easily mislead well-trained networks. These adversarial examples are not only valid in the digital space but also plausible in the physical world, raising concerns for safety-critical applications like autonomous driving systems and malware detection protocols. Research on adversarial examples serves two main purposes: devising attack algorithms and evaluating model robustness to perturbations. Existing approaches use attack results as a measure of neural network robustness, leading to biased analysis and limited evaluation. The dependency between robustness evaluation and attack approaches can skew results. The robustness of neural networks can be improved by generating adversarial examples and retraining with corrected labels. However, adversarially trained networks can still be vulnerable to unseen attacks. A new comprehensive and attack-agnostic robustness metric for neural networks is proposed, based on a universal lower bound on the minimal distortion required to craft an adversarial example. This lower bound applies to any attack algorithm and p norm for p \u2265 1. The lower bound on minimal distortion for crafting adversarial examples is associated with the maximum norm of local gradients, making robustness evaluation a local Lipschitz constant estimation problem. To estimate this constant efficiently, extreme value theory BID6 is proposed. This theory helps infer the local Lipschitz constant using independently sampled local gradients, leading to the creation of the CLEVER metric for network robustness evaluation. Unlike other metrics, CLEVER is attack-independent and applicable to any neural network classifier. The paper introduces a novel robustness metric called CLEVER for neural network classifiers with one hidden layer. CLEVER is attack-independent, scalable to large networks like ImageNet, and supported by theoretical analysis. Experimental results on models like ResNet, Inception-v3, and MobileNet validate the effectiveness of CLEVER. The paper introduces CLEVER, a robustness metric for neural network classifiers. Experimental results on models like ResNet, Inception-v3, and MobileNet validate the effectiveness of CLEVER in defending against adversarial examples. The norm constraint on \u03b4 can be implemented in a clipping manner or as a penalty function. Different p norms of \u03b4 are used for crafting adversarial examples, such as \u221e, 2, and 1 norms. State-of-the-art attack methods include I-FGSM, CW attack, and EAD, falling into the category of white-box attacks. Adversarial examples can also be crafted from a black-box network model using various approaches. Various defense methods have been proposed to improve the robustness of neural networks against adversarial examples. Defensive distillation uses distillation and a modified softmax function to retrain the network parameters. Changing the ReLU function to a bounded ReLU function can also increase resilience. Adversarial training is another popular defense approach. Various defense methods have been proposed to improve the robustness of neural networks against adversarial examples. Adversarial training generates and augments adversarial examples with the original training data. Model ensemble and detection methods like feature squeezing and example reforming can also increase resilience. However, the CW attack can bypass multiple detection methods. This paper focuses on evaluating the intrinsic robustness of neural network models to adversarial examples. Global Lipschitz constants are computed for each layer to explain the robustness issue in neural networks. The global Lipschitz constant is used to explain the robustness issue in neural networks. BID11 provided a robustness lower bound for a multi-layer perceptron with a single hidden layer. BID31 used topology terminologies to study robustness but did not provide bounds for neural networks. BID7, BID15, and BID14 focus on formally verifying properties in neural networks using SMT and LP based approaches with high computational complexity. The LP-based approaches for finding adversarial examples have high computational complexity and are only suitable for small networks. These methods provide an upper bound on distortion, dependent on specific attack algorithms. In contrast, the CLEVER robustness measure estimates a lower bound on distortion independently of attack algorithms, making it computationally feasible for large networks. The CLEVER method is computationally feasible for large networks like Inception-v3, providing formal robustness guarantees for classifiers. It defines minimum distortion and upper/lower bounds, with a focus on adversarial examples and perturbed examples. The CLEVER method provides formal robustness guarantees for classifiers by defining minimum adversarial distortion and upper/lower bounds for perturbed examples. The minimum adversarial distortion is the smallest distortion over all adversarial examples, with instance-specific lower and upper bounds. The lower and upper bounds for perturbed examples in the CLEVER method are instance-specific, with \u03b2 U easily obtained but \u03b2 L harder to find. \u03b2 L ensures classifier robustness to perturbations, with Lipschitz continuity used to derive a formal robustness guarantee. Theorem 3.2 provides a formal guarantee on the lower bound \u03b2 L for untargeted attacks, ensuring classifier robustness to perturbations using Lipschitz continuity. The function value g(x) near point x 0 is bounded by g(x 0 ), \u03b4, and the Lipschitz constant L q. The Lipschitz constant Lq bounds the change in function value g(x) near x0, with \u03b4, ensuring robustness to perturbations. The analysis distinguishes from previous work by showing equivalence and providing tighter bounds using global Lipschitz constant. Theorem 3.2 and Corollary 3.2.2 provide formal guarantees on \u03b2 L for targeted attacks, extending to non-differentiable functions like ReLU neural networks. Lemma 3.3 offers a formal guarantee on \u03b2 L for ReLU networks, with the analysis still holding for this case. An algorithm to compute is provided in this section. In this section, an algorithm is presented to compute the robustness metric CLEVER using extreme value theory. CLEVER serves as an efficient estimator of the lower bound \u03b2 L and is the first attack-agnostic score applicable to any neural network classifiers. The lower bound of network robustness is linked to g(x 0 ) and its cross Lipschitz constant L j q,x0, where g(x 0 ) is the output of a classifier and L j q,x0 is defined as max x\u2208Bp(x0,R) \u2207g(x) q. Calculating L j q,x0 is more complex as it involves finding the maximum value of \u2207g(x) q in a ball, which is challenging for image classifiers with large feature dimensions. The paper discusses using Extreme Value Theory to estimate the Lipschitz constant for image classifiers with large feature dimensions. This approach involves sampling points in a ball around x0 to estimate the maximum value of \u2207g(x) q. Extreme Value Theory helps in estimating this value with a tractable number of samples, differentiating it from previous work like BID32. The paper compares its sampling methodology with BID32, showing that BID32 performs poorly for high-dimensional classifiers like deep neural networks. It also discusses estimating \u2207g(x) q using Extreme Value Theory for neural networks. The Fisher-Tippett-Gnedenko theorem states that the limit distribution of a sequence of independent and identically distributed random variables can only belong to the Gumbel, Fr\u00e9chet, or Reverse Weibull class. These classes are determined by specific conditions on the cumulative distribution functions. The maximum values of samples follow one of three families of distributions: Gumbel, Fr\u00e9chet, or Reverse Weibull. The Reverse Weibull class is of interest due to its finite right end-point, denoted as a W, which reveals the extreme value. This extreme value represents the unknown local cross Lipschitz constant L j q,x0 to be estimated. Sampling x (i) over a fixed ball B p (x 0 , R) and computing \u2207g(x (i) ) q helps estimate L j q,x0 by storing maximum values in set S. The Reverse Weibull distribution parameters are estimated using maximum likelihood estimation with samples in set S, which stores maximum values of each batch. The CLEVER score for targeted and untargeted attacks is computed using the estimated local cross Lipschitz constant L j q,x0. Algorithm 1 computes CLEVER-t for targeted attacks, while Algorithm 2 computes CLEVER-u for untargeted attacks. The experiments were conducted on CIFAR-10, MNIST, and ImageNet datasets using various network architectures. For smaller datasets like CIFAR and MNIST, CLEVER scores were evaluated on networks with different structures and defense techniques. For ImageNet, deep network architectures like ResNet-50, Inception-v3, and MobileNet were used. For the experiments, different network architectures were used on CIFAR-10, MNIST, and ImageNet datasets. MobileNet, ResNet, and Inception net were chosen for their diverse architectures and high performance. MobileNet achieved 70.6% accuracy on ImageNet with a width multiplier of 1.0. Pretrained weights were used for all ImageNet models. Sampling parameters were set to N b = 500, N s = 1024, and R = 5 for all experiments. Targeted attacks were evaluated on 500 test-set images for CIFAR and MNIST, and 100 test-set images for ImageNet, with CLEVER scores calculated for random and least likely target classes. The experiment code is publicly available for targeted and untargeted attacks on MNIST and CIFAR datasets. The reverse Weibull distribution fits the data well, validating its use for calculating the CLEVER score. The fitted location parameter can be used as an estimation of the local cross Lipschitz constant. The CLEVER score is calculated using the cross Lipschitz constant. White-box attack methods are applied to find adversarial examples for various networks trained on CIFAR, MNIST, and ImageNet datasets. Iterations and attack learning rates are individually tuned for each model. The study evaluates the attack-specific robustness of networks using CLEVER scores, which are compared with attack-induced distortion metrics. The CLEVER score is effective in determining the upper bound of attacks and is the first attack-independent robustness score for large networks. The CLEVER score is compared with attack-induced distortion metrics to evaluate network robustness. CLEVER serves as an estimated lower bound, indicating the distortion of the best possible attack. It is independent of attack algorithms and can indicate the strength of specific attacks like CW. Defense mechanisms impact CLEVER scores consistently. When using Defensive Distillation or Bounded ReLU, CLEVER scores increase, except for CIFAR-BReLU, suggesting increased resilience to adversarial perturbations. However, for CIFAR-BReLU, both CLEVER scores and p norm of adversarial examples decrease, indicating ineffectiveness as a defense. CLEVER scores act as a security checkpoint for unseen attacks, highlighting potential vulnerabilities. The percentage of inaccurate estimations where CLEVER score exceeds the distortion of adversarial examples is shown in TAB4. The CLEVER score provides accurate estimations for most examples in ImageNet networks, serving as a strict lower bound for MobileNet and Resnet-50 in over 96% of cases. Comparison tables show the effectiveness of CLEVER scores against CW and I-FGSM attacks, as well as the impact of Defensive Distillation and Bounded ReLU defending methods. SLOPE is not included in ImageNet networks due to its ineffectiveness, as shown by the Defensive Distillation and Bounded ReLU defending methods applied to the CNN network. SLOPE significantly overestimates distortions compared to attacks, making it an inappropriate estimation for lower bound \u03b2 L. In Figure 6, distortion and CLEVER scores are plotted for images in TAB4. A positive gap indicates near-optimality of CW attack in terms of distortion. Figure 7 shows CLEVER scores for Inception-v3, ResNet-50, and MobileNet with varying sample sizes. 50 or 100 samples are usually sufficient for accurate robustness estimation. On a single GTX 1080 Ti GPU, the cost of 1 sample is measured as 2.9 s. The CLEVER score is proposed as a metric to evaluate neural network classifier robustness to adversarial examples. It is attack-agnostic, applicable to any classifier, has strong theoretical guarantees, and is computationally feasible for large networks. Experimental results show its effectiveness across various networks. The CLEVER score is a metric to evaluate neural network classifier robustness to adversarial examples. The proof of Theorem 3.2 shows that no adversarial examples can be found under certain conditions. The proof of Corollary 3.2.1 provides a bound based on Lemma 3.1. The ReLU network has a finite number of points where the function does not exist, due to discontinuities caused by ReLU activations. The output of a one-hidden-layer neural network can be computed using weight matrices and the ReLU activation function. The ReLU network divides the d-dimensional space into different regions using hyperplanes. Each region satisfies a different set of inequality constraints. The gradient norm is the same for all points in the same region, allowing for at most M different values for \u2207g(x) q. The ReLU network divides the d-dimensional space into regions with hyperplanes, allowing for at most M different values for \u2207g(x) q. Uniform sampling in a ball centered at x 0 with radius R results in a discrete probability distribution for \u2207g(x) q. The CDF of Y is piece-wise constant with at most M pieces. Results show that maximum gradient norm samples fit reverse Weibull distribution well. CLEVER scores with different sample sizes for MNIST and CIFAR models are depicted in Figure 3. For most CIFAR models, changing the number of samples has minimal impact on CLEVER scores. However, for MNIST-BReLU, increasing the number of samples improves the estimated lower bound. Starting with a small number of samples and gradually increasing can help determine if CLEVER scores significantly change. If scores remain consistent, using the initial number of samples is sufficient."
}