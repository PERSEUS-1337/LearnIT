{
    "title": "BJxvEh0cFQ",
    "content": "The novel method enables parameter-efficient transfer and multi-task learning with deep neural networks by learning specialized model patches instead of fine-tuning the entire network. It shows that re-learning low-parameter layers and adjusting scales and biases can significantly improve transfer-learning accuracy while reusing most parameters. This approach allows for both simultaneous and sequential transfer learning with fewer parameters. In multi-task learning problems, using fewer parameters than traditional fine-tuning can match single-task performance. Deep neural networks have revolutionized machine intelligence, shifting computation to consumer devices for faster response and better security. Model sizes have decreased while accuracy has improved, but maintaining and updating models on embedded devices remains costly. In this paper, the focus is on building models with minimal parameters for re-purposing to different tasks, aiming to reduce bandwidth, energy, and storage costs. The novel learning paradigm involves each task having its own set of parameters along with a shared set, allowing for efficient transfer learning and minimizing the number of model parameters when solving multiple tasks together. In transfer learning and multi-task learning scenarios, the model patch is fine-tuned for new tasks, with significant accuracy improvements shown by training less than 10% of the model's parameters. This method is effective for transfer learning between different problems. Fine-tuning on transfer learning between different problems involves training multiple models simultaneously with shared parameters. Results show that training two MobilenetV2 models on ImageNet and Places-365 reaches high accuracies. The multi-task learning paradigm is also applied to domain adaptation, demonstrating the ability to train models at different resolution scales. The study explores MobilenetV2 models at 5 resolution scales, sharing parameters for power efficiency. A cascade algorithm reduces running time by 15% without accuracy loss. The method introduces model patches for improved performance. The study introduces model patches, specifically scale-and-bias patches, as a method to improve performance in deep networks. These patches are small sets of per-channel transformations that can be folded with other layers in the network, minimizing the need for explicit addition of layers. The choice of scale-and-bias patches is crucial for network optimization. These patches apply per-channel scale and bias adjustments to each layer, which can often be integrated into normalization layers like Batch Normalization. The batch-normalized version of an activation tensor X is computed using mean and standard deviation per minibatch, with learned parameters \u03b3 and \u03b2. During inference, global averages are used. Scale-and-bias patches encompass all \u03b3, \u03b2, \u00b5, and \u03c3 in the network, with Batch Normalization satisfying the criterion of small patch size. For example, in MobilenetV2 and InceptionV3 networks, Batch Normalization parameters amount to less than 1% and 0.2% of total parameters, respectively. Depthwise separable convolutions were introduced as a way to reduce parameters in deep neural networks. This technique decomposes a standard convolution into two layers: a depthwise convolution layer and a pointwise layer. The depthwise layer applies one filter per input channel, while the pointwise layer combines the output linearly across channels. This approach has been further developed to include linear bottlenecks and expansions, resulting in efficient network optimization. The depthwise convolutional layers can be repurposed as lightweight model patches, accounting for less than 3% of MobilenetV2's parameters. These model patches can be used in transfer and multi-task learning, where they are applied to adapt pretrained models to new tasks and train multiple neural networks simultaneously. In multi-task learning, multiple neural networks share most weights and learn all parameters for adaptability to tasks. Distributed TensorFlow is used for training with a central parameter server and workers updating weights. Different workers can train different tasks with task-specific input pipelines and loss functions. One common approach for domain adaptation and transfer learning is fine-tuning the last layer of a neural network for a new task. This involves training a linear classifier on existing features. Another approach is full fine-tuning, where a pretrained model is used as a starting point for training, leading to improved accuracy but potential overfitting. In this work, the focus is on producing highly accurate models by reusing a large fraction of the weights of the original model to address overfitting. The method involves learning small model patches and fine-tuning them along with the last layer for improved performance. Contrary to previous studies, it is found that the linear classifier does matter when training full networks. Additionally, re-computing batch normalization statistics for different domains and learning batch normalization layers in a randomly initialized network are suggested methods to improve accuracy. In this work, batch normalization layers are used in a randomly initialized network to build non-trivial models and prevent model activation space drift during model quantization. The approach is scaled up by performing transfer and multi-task learning across different tasks, providing a powerful tool for practical applications. The work also has connections to meta-learning, where small model patches are utilized to increase expressivity at low cost. Model-patch based fine-tuning, especially with the scale-and-bias patch, is shown to be comparable or better than last-layer-based fine-tuning in experiments. In this section, the study compares the effectiveness of last-layer-based fine-tuning with a significantly smaller set of parameters. The observation is that individual channels in hidden layers form an embedding space, leading to significant changes in network classification with simple transformations. The deep neural network for classification consists of a network base mapping input space to an embedding space, and a linear transformation mapping embeddings to logits for each class. The study compares fine-tuning model patches with fine-tuning only the final layer. Fine-tuning only the last layer has limitations due to linear transformations preserving convexity, restricting model tuning in new input space manifolds. The study compares fine-tuning model patches with fine-tuning only the final layer, showing limitations in disentangling folded regions in the input space through linear transformations. The analysis demonstrates how adjusting biases and scales can change the folded regions and classification function of a neural network. The study explores how a single bias in a neural network can alter the topology of the mapping. It discusses the variation in the number of connected components in the preimage of a one-dimensional segment based on the bias variable. The network with two hidden units and a single output value can fold the input space twice, and by composing multiple functions, the network can fold the input domain multiple times. The number of \"folds\" can vary from 2k to 0. The study evaluates the performance of transfer and multi-task learning using image recognition networks like MobilenetV2 and InceptionV3 on various datasets. It also explores transfer learning across different tasks using MobilenetV2 and Single-Shot Multibox Detector networks, experimenting with scale-and-bias and depthwise-convolution patches. The effect of fine-tuning the patches along with the last layer of the network is also examined. In experiments with MobilenetV2 and InceptionV3, the study fine-tunes the entire network using TensorFlow BID0 and NVIDIA GPUs. Different image sizes are used for each network. An experiment on MobilenetV2 shows adjusting biases and scales of random embeddings can improve feature accuracy even with a linear classifier. The study fine-tunes MobileNetV2 and InceptionV3 models pretrained on ImageNet, achieving comparable accuracies with fewer parameters by only fine-tuning the scale-and-bias patch. Transfer learning between different tasks also shows the effectiveness of training the model patch along with the last layer. In transfer learning, fine-tuning small model patches like the last layer can lead to significant accuracy improvements. The choice of learning rate is crucial, with higher rates beneficial for fine-tuning small patches. Fine-tuning a patch including the last layer is more stable compared to full fine-tuning. Results on MobilenetV2 with different learning rates and model patches are illustrated in FIG4. In transfer learning, fine-tuning small model patches like the last layer can significantly improve accuracy. Results on MobilenetV2 with different learning rates and model patches show the effectiveness of small patches. Combining model patches and fine-tuning leads to a synergistic effect. Additionally, experiments comparing custom bias/scale learning with updating batch-norm statistics are shown in Appendix B. Using model-specific patches during multi-task training can achieve performance comparable to independently trained models. In this experiment, a combination of scale-and-bias patch and the last layer is chosen as the private model patch. The rest of the weights are shared among tasks, with comparable learning rates ensured by setting hyperparameters. Results show that multi-task validation accuracy using separate S/B patches for each model is comparable to single-task accuracy, outperforming setups using separate logit-layers for each task. In this experiment, scale-and-bias patch is used as the private model patch, with shared weights among tasks. Bilinear interpolation is employed to scale images. Results in TAB4 show the effectiveness of S/B patch compared to baseline setups. The scale-and-bias patch introduced in the experiment closes the accuracy gap between setups, leading to increased accuracy for some models with a slight increase in parameters. This new transfer and multi-task learning method patches only a small fraction of model parameters, resulting in high accuracy across different tasks. Using biases and scales alone allows pretrained neural networks to solve diverse problems, with a significant accuracy boost when the last layer is also trained. From an analytical perspective, biases alone maintain high expressiveness in neural networks, but predicting important parameters remains a future research topic. Cross-domain multi-task learning shows promise, and there is potential for extending federated learning approaches. This involves user devices maintaining personalized models while sending common updates to a central server. The approach includes a simple network that \"folds\" input space during training. The approach involves a deep neural network that \"folds\" the input space during training, leading to identical embeddings for different input points. Fine-tuning the final linear layer is not sufficient for transfer learning to a new dataset. An alternative embedding can be learned to avoid input space folding and enable transfer learning. The model fine-tuning process is depicted in various figures, showing that fine-tuning all variables can lead to a perfect fit for new data. However, restricting the set of trainable parameters can make fine-tuning less efficient. Poor performance in logit fine-tuning extends to higher embedding dimensions. Results for different embedding dimensions are shown in different figures, with the final layer fine-tuning eventually achieving acceptable results as the embedding dimension increases. The explanation for poor logit fine-tuning results can be seen in the embedding space plots. The network's embedding space is visualized in FIG9, showing two circular regions with the same embedding. Training on different classes breaks the symmetry, allowing for a new learned embedding that improves fine-tuning efficiency. Adjusting Batch Normalization statistics aids domain adaption but hinders transfer learning unless biases and scales are learned. Fine-tuning the last layer with adjusted batch-norm statistics helps in keeping activation. Fine-tuning the network's last layer with batch-norm statistics readjusted to maintain activation space at mean 0/variance 1 leads to under-performance compared to fine-tuning with frozen statistics. However, incorporating learned bias/scales improves performance. Experiments are summarized in TAB5. Domain adaptation using model patches results in cost-efficient model cascades, reducing the average cost of MobilenetV2 inference by 15.2% with minimal increase in model parameters. Training speed remains consistent. Fine-tuning approaches for models required 50-200K steps, with variations depending on learning rate and method. Convergence steps differ among approaches but are comparable to changes in other hyperparameters like learning rate."
}