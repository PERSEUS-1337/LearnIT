{
    "title": "rkPLzgZAZ",
    "content": "A modular continual reinforcement learning paradigm is described, inspired by human intelligence's ability to learn new tasks quickly and switch between them flexibly. The paradigm includes a visual interaction environment for unifying various tasks, a reward map prediction scheme for robust task learning, and a module architecture that outperforms standard neural network motifs. Additionally, a meta-controller architecture for task switching based on a dynamic neural voting scheme is presented. The text discusses a neural module approach for task switching, where decision modules are dynamically allocated on top of a fixed sensory system to improve learning efficiency. This approach aims to mimic how humans adapt to shifting tasks in real-world environments. The text discusses the use of a general-purpose visual representation for decision modules in visually-driven tasks. The TouchStream environment allows for continual learning agents to pose visual reasoning tasks in a unified action space. In TouchStream environment, tasks are posed in a unified action space with the agent aiming to maximize reward by choosing optimal actions. The agent comprises neural networks including a visual backbone, learned modules, and a meta-controller for task solving using the ReMaP algorithm. In a unified environment, the agent learns to estimate rewards based on actions and recent history to maximize aggregate rewards. Unlike standard supervised learning, where output spaces are predefined, this approach allows for the learning and deployment of decision structures in diverse tasks with various output types. In a unified environment, an agent interacts with a TouchStream GUI to encode various tasks using a single output space. Reward Map Prediction (ReMaP) networks enable continual reinforcement learning to discover task-specific interfaces in large action spaces like TouchStream. The ReMaP algorithm addresses challenges in learning task interfaces in large action spaces like TouchStream. It focuses on efficient module architectural motifs and system architectures for task switching. The algorithm outperforms standard neural network motifs by incorporating design principles like visual bottlenecks and symmetry-inducing concatenations. Additionally, a meta-controller architecture based on a dynamic neural voting scheme improves learning efficiency by leveraging information from previously-seen tasks. In this work, the TouchStream environment is formalized, the ReMaP algorithm is introduced, and the performance of different ReMaP module architectures on various tasks is evaluated. The Dynamic Neural Voting meta-controller is described in detail, showcasing its ability to transfer knowledge efficiently between ReMaP modules during task switches. The use of deep convolutional neural networks has made a significant impact on computer vision and artificial intelligence, as well as in computational neuroscience. This work applies the concept of modules to address visual learning challenges in a continual learning context, moving away from pre-specified module primitives used in existing works. In this paper, a single generic module architecture is shown to automatically learn to solve various tasks in a unified action/state space. A simple controller scheme can switch between modules, addressing the issue of lifelong learning and knowledge transfer effectively. The use of modules eliminates catastrophic forgetting but raises the question of how newly-allocated modules can be learned efficiently. The curr_chunk discusses how transfer learning can lead to higher performance but does not focus on improving learning speed. It highlights the need to identify architectures that achieve high performance quickly on individual tasks before exploring how to efficiently reuse components of these architectures. Most works in continual learning do not address which specific architecture types learn tasks efficiently. In contrast to most works in continual learning, the curr_chunk focuses on meta-reinforcement learning approaches and schema learning for continual learning. It emphasizes the burden of environment learning on individual modules and the light-weight nature of the meta-controller in this context. Unlike previous work limited to small state or action spaces, the curr_chunk addresses multitask transfer learning in large action spaces. The curr_chunk discusses transfer learning in large action spaces, focusing on agents learning on-the-fly in a real-world environment. The TouchStream environment is introduced as a simplified two-dimensional domain where an agent interacts with an environment by receiving RGB images and rewards to choose actions from a two-dimensional pixel grid. The agent in the TouchStream environment interacts with a two-dimensional pixel grid, using a neural network to learn a policy that maximizes rewards over time in a continuous learning context. Unlike episodic reinforcement learning, the agent is not signaled to reset to an initial state during learning. The TouchStream environment presents a unique challenge for the agent as it experiences a continual stream of tasks with different reward schedules. The agent's action space is all possible pixel locations, and the state space is any arbitrary image, requiring the agent to quickly recognize task interfaces and transfer knowledge efficiently across tasks. The large size of the agent's state and action space poses efficiency challenges in learning and adaptation. The agent faces challenges due to the large size of its state and action spaces in the TouchStream environment. It works with modern computer vision datasets and is inspired by visual psychology and neuroscience for reinforcement learning paradigms. Task paradigms like Stimulus-Response, Match-To-Sample, and Localization tasks are utilized to formulate discrete and continuous estimation tasks. In our experiments, we use images from the ImageNet dataset for Stimulus-Response (SR) tasks and Match-to-Sample (MTS) tasks. MTS tasks require working memory and spatial control, with more complex tasks involving sophisticated relationships between sample and match images. In more complex MTS tasks, the sample screen displays a template image from the MS-COCO dataset, while the match screen shows a scene with the same class. The agent is rewarded for correctly identifying the class and location of the object. This task combines categorical and continuous elements, requiring both class identity and object location for successful completion. The task involves predicting the bounding box of an object by touching opposite corners on two successive timesteps, with rewards based on the Intersection over Union (IoU) value. The choice made at one timestep constrains the agent's optimal choice on a future timestep. The tasks presented require fixed-length memory and future prediction, with a perfect solution always existing within a certain number of timesteps. The required values of past and future timesteps vary across tasks. The agent learns to ignore irrelevant information and predict efficiently in various tasks. It starts with a single semantic task and progresses to handling unpredictable task transitions with a meta-controller. The TouchStream environment involves large action and state spaces. In the TouchStream environment with large action and state spaces, a neural network is used to approximate the mapping between action space and reward space in the ReMaP algorithm for efficient multitask reinforcement learning. The agent aims to learn an optimal policy based on the estimated reward, making the interaction manageable for simple choice policies. The ReMaP algorithm uses a neural network to approximate the expected reward map across its action space for future timesteps. It considers the agent's own actions and state space history to predict reward maps for a subsample of actions. The ReMaP algorithm uses a neural network to predict reward maps for future timesteps by normalizing predictions into probability distributions and sampling an action with maximum variance. The ReMaP algorithm utilizes a distribution-based sampler to explore a large action space efficiently by exploiting spatial and temporal structure. It focuses on timesteps with the highest reward map variance to reduce uncertainty and uses the VarArgmax function to prioritize timeframes with the most relevance for reward outcome. This approach is found to be effective in exploration strategies. The ReMaP algorithm efficiently explores a large action space by learning parameters through gradient descent on reward prediction error. It uses a distribution-based sampler and focuses on timesteps with high reward map variance to reduce uncertainty. The ReMaP algorithm utilizes a fixed backbone state space encoder, VGG-16 convnet, pretrained on ImageNet, with an input resolution of 224x224 pixels. The action space is defined as A = {0, . . . , 223} \u00d7 {0, . . . , 223}, and the action selection scheme involves using a low-temperature Boltzmann distribution. Reward prediction errors are calculated using cross-entropy loss. The ReMaP algorithm uses a VGG-16 convnet pretrained on ImageNet with an input resolution of 224x224 pixels. The action space is defined as A = {0, . . . , 223} \u00d7 {0, . . . , 223}, and reward prediction errors are calculated using cross-entropy loss. The main question addressed is the neural network structure for ReMaP modules, which should be easy to learn and reuse components efficiently. An example is given for a binary StimulusResponse task. The ReMaP algorithm utilizes a VGG-16 convnet pretrained on ImageNet with an input resolution of 224x224 pixels. The action space A = {0, . . . , 223} \u00d7 {0, . . . , 223}, and reward prediction errors are calculated using cross-entropy loss. Equation FORMULA6 explains the relationship between the x and y components of the action a \u2208 A relative to the screen center, and the matrix W expressing the class boundary. The formula predicts reward based on the position of the touch on the screen. The FC6 layer shows a multiplicative interaction between action vector and visual features, with symmetry in the formula. A parameterized family of networks can emerge naturally for any task. ReMaP module encodes design principles and uses few learnable parameters. DisplayFORM0 and DisplayFORM1 represent nonlinearity functions. The CReS nonlinearity introduces multiplicative interactions and symmetry via CReLU. The (n 0 , n 1 , . . . , n k )-Early Bottleneck-Multiplicative-Symmetric (EMS) module allows the agent to discover implicit interfaces for each task, capturing natural physical constructs before task-specific decision rules. Examples include onscreen \"buttons\" appearing before their specific semantic meaning is learned. The EMS structure incorporates three principles: early bottlenecking of visual inputs, multiplicative symmetric interactions between visual features and actions, and a two-layer module for binary SR tasks. Visual features can come from any encoder, such as fully connected or convolutional features from the VGG-16 backbone. The EMS module is compared to various control motifs in experiments. The EMS module is tested with 24 different architectures using various activation functions. Action vectors are concatenated to the output of the visual encoder in late bottleneck architectures. The architectures are compared across different visual tasks with fixed visual encoding features from layer FC6 of VGG-16. The experiments involved testing 24 different architectures with various activation functions on challenging visual tasks, including localization tasks with complex backgrounds and scenes from MS-COCO. Module weights were initialized with a normal distribution and optimized using the ADAM algorithm. Learning rates were optimized per-task and per-architecture, with optimizations run from five different initialization seeds to account for variability. The experiments tested 24 architectures on visual tasks with complex backgrounds. Module sizes varied, with ReMaP modules able to discover output domain spaces for different tasks. Decision structures are interpretable and reflect environmental interfaces. Learning patterns are consistent across tasks and seedings, potentially modeling human interface use and learning. Modules uncover physical structures before specific decision rules for task-solving. The experiments tested 24 architectures on visual tasks with complex backgrounds, where modules uncover physical structures before learning specific decision rules needed to solve the task efficiently. The EMS module outperforms other architectures across all tasks, achieving the highest final reward level. Ablations of the EMS structure result in decreased performance, with removing low-order polynomial interaction having the largest negative effect. Fully-ablated models perform worse than the EMS module but better than models without symmetry or multiplicative interactions. The special-case architecture in eq. (5) efficiently solves the binary SR task by combining helpful conceptual features that can be parameterized and learned for various visual tasks. Using visual features with explicit spatial information significantly enhances task performance and learning efficiency. Explicit spatial information significantly improves task performance and learning efficiency. Results on MS-COCO demonstrate the use of reinforcement learning for object segmentations. Models with EMS-like variants achieve higher IoU values. Agents in real environments often switch between tasks and need to repurpose knowledge from previous tasks. Agents in real environments need to repurpose knowledge from previous tasks by dynamically allocating a mixture of pathways through computation graphs of old and new modules, determined by a meta-controller neural network. The meta-controller, a neural network, learns a distribution over modules to build a composite execution graph. It assigns weights to each layer in modules, probabilistically selecting layers for the graph. The composite graph combines parts of existing modules and new components to solve tasks. The Dynamic Neural Voting Controller generates composite execution graphs by computing weighted activations of components at each layer. The controller assigns probabilistic weights to scale activations within modules, allowing for the integration of existing and new components to solve tasks efficiently. The dynamic neural voting process involves a controller network learning a Boltzmann distribution over module activations to maximize reward prediction accuracy. This process is performed at each module layer to determine the probabilistic weights for scaling activations within modules. The meta-controller in the dynamic neural voting process operates online, continuously learning its meta-policy while the agent takes actions. It uses a fully-differentiable neural network learned by gradient descent. The meta-controller assigns probabilistic weights to neurons in each module, creating composite neurons with activations for improved accuracy. The dynamic neural voting mechanism optimizes a neural network online via gradient descent while modules solve tasks, eliminating the need for fully-connected adaptation layers and reducing parameter requirements. Specialized transformations and initialization schemes of controller parameters are crucial for efficiency. Refer to the Supplement for more details. Dynamic Neural Voting quickly corrects for \"no-switch\" switches by reusing old module components when the new task is identical to the original task. The voting mechanism responds well to \"no-switch\" switches, where a new module is allocated but the task remains the same. The study found that when no new module is needed, the system quickly returns to pre-switch performance levels with minimal penalty. The controller assigns low weightings to the new module, focusing on the existing one instead. This suggests the possibility of implementing an autonomous monitoring policy for task switches. Future work will explore this further. The dynamic neural voting controller was evaluated on 15 switching experiments involving various types of environment policy changes. This included adding new classes, replacing class sets, introducing visual variability, and transforming interface elements. The study suggests the potential for implementing an autonomous monitoring policy for task switches. The study evaluated the dynamic neural voting controller on 15 switching experiments involving different environment policy changes, such as adding new classes and transforming interface elements. Controller hyperparameters were optimized in a cross-validated fashion, and post-switch learning curves for the EMS module were shown for different methods. Relative gain in AUC was used to quantify cumulative reward gains. The study compared the Layer Voting and Single-Unit Voting methods on the EMS module for a 4-way Quadrant SR task after learning a 2-way SR task and a 4-way MTS task. Results showed that the dynamic voting controller facilitated positive transfer across all task switches, with Single-Unit voting being a more effective transfer mechanism than Layer Voting. The large fully-ablated module also benefited from dynamic neural voting. The EMS module showed higher efficiency in single-task performance compared to the large fully-ablated module. Transfer Gain metric quantifies the speed of reward improvement, with EMS scoring significantly higher, indicating it is more \"switchable\" between tasks. This is attributed to EMS achieving high task performance with fewer resources. In this work, the TouchStream environment is introduced as a continual reinforcement learning framework for spatial decision-making tasks. The ReMaP algorithm is described for learning light-weight neural modules that can adapt to various tasks efficiently. The EMS module is highlighted for its compactness and high task performance, making it ideal for flexible task learning and switching. Additionally, a dynamic task-switching architecture is discussed, showing the ability to transfer knowledge when new tasks are learned. Future work will focus on expanding insights into a more complete continual-learning agent, scaling to handle multiple task switches, determining when to build new modules, consolidating modules when necessary, and extending the approach to visual tasks with longer horizons. The goal is to develop agents that can autonomously discover and operate interfaces in real-world applications. The research focuses on developing agents that can autonomously operate interfaces in real-world applications, utilizing spatially-informed techniques for two-dimensional problem domains. The EMS module was evaluated on various stimulus-response tasks and multitask learning scenarios. The research focuses on developing agents for real-world applications using spatially-informed techniques for two-dimensional domains. Various stimulus-response tasks and multitask learning scenarios were evaluated using the EMS module, including different variants of MTS tasks with specific class assignments and template placements. The study evaluated different variants of Match-To-Sample tasks, including stationary, permuted, localization, and MS-COCO tasks. The match screens used images from the Image-Net class set, with fixed class templates at 100x100 pixels and a buffer around the images. The Localization task involves synthetic images with a single salient object on a complex background, offering variance in scale, position, and rotation. It includes 59 unique classes and requires non-trivial policies for action selection. The task involves using the MS-COCO detection challenge dataset BID18 with samples from 80 classes. The agent is rewarded for selecting the correct class on a sample screen. The modules use low-temperature Boltzmann policy for more precise reward map prediction. The number of units per layer for EMS and ablated modules is aggregated in TAB2 for single-task and task-switching experiments. Only fully-connected modules' layer sizes are shown. Refer to C.2 for details on the convolutional bottleneck EMS module. The convolutional bottleneck EMS module is an extension of the EMS module with skip connections linking conv5 and FC6 representations. The scene-level representation in the FC6 ReMaP memory buffer is spatially tiled to match convolution dimensions and concatenated onto its channel dimension. A series of 1x1 convolutions act as a shallow visual bottleneck before being concatenated with input A for CReS layers. The bottleneck consists of a single tanh and two CReS convolutions with 128 units each, followed by downstream layers with 128 units each. This approach aims to leverage lower-level features for tasks like Localization and Object Detection. The EMS module features are useful for tasks like Localization and Object Detection, allowing for precise policy through multiplicative template-matching. Various ablations were tested on the EMS module, including symmetry and multiplicative ablations using different nonlinearities. The weight initialization scheme optimized for the dynamic voting controller involves initializing weight matrices and biases for the layer-voting mechanism. This technique was also adapted for single-unit voting. The weight initialization scheme optimized for the dynamic voting controller involves initializing weight matrices and biases for the layer-voting mechanism. This technique was also adapted for single-unit voting, with additional switching mechanisms added to the controller for task remapping. Efficient modules produce minimal representations of the interaction between action space and observation. Remapping is achieved through a fully-connected transformation using a small number of learnable parameters. The weight initialization scheme optimized for the dynamic voting controller involves initializing parameters to embed a \u03c4 into a new action space A with minimal learnable parameters. The transformation is pseudo identity-preserving to maintain the original module's representation. Each map reflects the agent's uncertainty in the environment's reward policy, and a mechanism allows for targeted transformation of the policy if the environment transitions to a new reward schedule. In this work, a shallow \"adapter\" neural network is investigated to map R \u2192 R, with layers defined by a transformation on A and learnable matrices W1 and W2. The reward-map transformation is also modified to be pseudo identity-preserving, achieved by initializing W1 and W2 accordingly. The intended map-preserving transformation is achieved by initializing W1 and W2. A grid search was conducted to optimize hyperparameters for test task switches. Optimal hyperparameters were fixed for use in the dynamic voting controller. Three tests were conducted using the stimulus-response paradigm: class reversal, horizontal rotation of reward boundaries, and a switch to the original task. In this work, a single non-activated linear transformation is found to be optimal for a new state-space embedding. The learning rate for this transformation was determined to be 0.1. Optimal activations were found to be CReS and ReLU with 4 units in the hidden layer. Weight initialization scheme was optimized at 0.001, and the learning rate for this transformation was found to be 0.01. The study found that the primary contribution of the dynamic neural controller was the voting mechanism. When transitioning tasks, the controller freezes old task-module parameters, deploys a new uninitialized module, and initializes action and reward map transformation networks on top of the old module. Switching performance metrics RGain and T Gain were used to quantify controller performance. The dynamic neural controller's switching performance is quantified using two metrics: \"relative gain in AUC\" and \"transfer gain.\" These metrics measure the overall gain relative to scratch and the speed of transfer, respectively. The curve shown is the EMS module with Single-Unit voting method evaluated on a switch between different tasks."
}