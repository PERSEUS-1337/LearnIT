{
    "title": "r1x-E5Ss34",
    "content": "Catastrophic forgetting is a challenge for continual learning systems, hindering neural networks from retaining old knowledge while learning new tasks sequentially. A Differentiable Hebbian Plasticity (DHP) Softmax layer is proposed to address this issue by adding a fast learning plastic component to the slow weights of the softmax output layer. The DHP Softmax acts as a compressed episodic memory that reactivates existing memory traces and creates new ones. This model, when combined with existing consolidation methods, prevents catastrophic forgetting and outperforms comparable baselines on benchmarks like Permuted MNIST and Split MNIST. Additionally, a new dataset called Imbalanced Permuted MNIST is introduced to test the model's performance in handling class imbalance and concept drift. The model requires no additional hyperparameters and effectively reduces forgetting, showcasing the flexibility and adaptability of human intelligence. Recent advances in machine learning have shown improvements in solving complex tasks through extensive training on large datasets. However, the assumption of a stationary real-world leads to performance degradation when models are fine-tuned with new data, known as catastrophic forgetting. This poses a crucial problem for deep neural networks. Interference in deep neural networks during continual learning poses a crucial problem, as models struggle to adapt to consecutive tasks without forgetting previously learned ones. Real-world applications like autonomous vehicles and fraud detection require this type of learning. DNN architectures typically rely on iid samples from a stationary distribution, but in scenarios with concept drift or class imbalance, this assumption is violated. In continual learning, deep neural networks face the \"stability-plasticity dilemma\" where they must balance integrating new knowledge with preserving existing knowledge. Two theories explain human continual learning: synaptic consolidation in the neocortex and the complementary learning systems theory. Differentiable plasticity is extended to a continual learning setting to develop a model that can adapt quickly to changing environments. Our model combines traditional softmax layer with Differentiable Hebbian Plasticity (DHP) to adapt quickly to new environments and consolidate previous knowledge. It integrates task-specific synaptic consolidation methods like elastic weight consolidation and synaptic intelligence to overcome catastrophic forgetting. The model unifies Hebbian plasticity, synaptic consolidation, and CLS theory for rapid adaptation to new data while consolidating synapses. Recent approaches in meta-learning incorporate fast weights into neural networks, utilizing Hebbian learning for weight plasticity and memory consolidation. This includes augmenting fully-connected layers with fast weights and proposing a softmax layer for improved learning of rare classes. BID25 proposed a softmax layer to improve learning of rare classes by interpolating between Hebbian and SGD updates. BID19 introduced differentiable plasticity using SGD to optimize synaptic connections. This work addresses catastrophic forgetting by implementing task-specific synaptic consolidation and CLS theory for dual memory systems. The neocortex and hippocampus play roles in slow and rapid learning, respectively. Various works focus on synaptic consolidation to address catastrophic forgetting by estimating the importance of each synapse. A regularizer is added to the loss function when learning new tasks to prevent changes to important parameters of previously learned tasks. In Elastic Weight Consolidation (EWC), the diagonal values of an approximated Fisher information matrix are used for regularization. BID28 proposed an online variant of EWC to control computational cost. Zenke et al. introduced Synaptic Intelligence (SI) for parameter importance computation. Memory Aware Synapses (MAS) measure parameter importance based on sensitivity to perturbations. Various approaches involve pseudo-rehearsal based on CLS principles. In our work, we focus on neuroplasticity techniques inspired by CLS theory for representing memories. Previous research has explored methods like fast weights in RNNs, Hebbian Softmax layer, augmenting FC layer with fast weights matrix, differentiable plasticity, and neuromodulated differentiable plasticity for rapid learning on simple tasks or meta-learning. These methods did not address learning a large layer. In our model, each synaptic connection in the softmax layer has two weights: slow weights \u03b8 and a Hebbian plastic component with a plasticity coefficient \u03b1 and Hebbian trace Hebb. The \u03b1 parameter scales the Hebbian trace, which accumulates mean activations of the penultimate layer for each target label in a mini-batch. The unnormalized log probabilities z at the post-synaptic connection can be computed using a specific equation, followed by applying the softmax function to obtain the desired output. The softmax function is applied on z to get the logits \u0177, with the learning rate parameter \u03b7 controlling the acquisition of new experiences. Network parameters are optimized by gradient descent during training on different tasks in continual learning. Hebbian traces are updated based on Algorithm 1 in the forward pass, with the Hebbian update involving changes in weights and activation levels of neurons. In the model, Hebbian weight updates are used to update activation levels of neurons for different inputs. The plastic component quickly stores memory traces for recent experiences without interference. Hidden activations of the same class are accumulated into one vector, improving learning of rare classes and speeding up binding of class labels to data representations. The model uses Hebbian weight updates to update neuron activation levels for different inputs, improving learning of rare classes and speeding up binding of class labels to data representations. The approach incorporates existing consolidation methods to prevent catastrophic forgetting and was tested on various benchmarks. In the benchmarks, neural networks were trained with Online EWC, SI, and MAS methods on sequential tasks. A multi-layered perceptron network was trained on Permuted MNIST with varying pixel permutations for each task, causing concept drift. The DHP Softmax alone showed significant improvement in test accuracy. The DHP Softmax, when combined with synaptic consolidation methods, maintains higher test accuracy after learning T 10 tasks compared to without DHP Softmax. This is shown in Figure 1, where DHP Softmax alleviates catastrophic forgetting across all tasks. Additionally, Figure 2 illustrates the average test accuracy improvement as new tasks are learned. The DHP Softmax with MAS achieves 88.8% test accuracy after learning 10 tasks, outperforming all other methods. The compressed episodic memory mechanism in Hebbian traces allows rare classes to be remembered longer. Split MNIST involves training a multi-headed MLP network on a sequence of 5 tasks with disjoint output spaces. The DHP Softmax improves test performance by 4.7% compared to a finetuned MLP network. It consistently enhances performance across all tasks and alleviates catastrophic forgetting in continual learning environments by adding compressed episodic memory in the softmax layer. The DHP Softmax improves test performance by 4.7% compared to a finetuned MLP network, consistently enhancing performance and alleviating catastrophic forgetting in continual learning environments. Combining DHP Softmax with MAS leads to superior results on benchmarks, indicating that Hebbian plasticity enables neural networks to learn continually and remember distant memories. Regularization hyperparameters for task-specific consolidation methods are specified for Imbalanced Permuted MNIST experiments. The hyperparameters for synaptic consolidation methods (Online EWC, SI, MAS) were optimized through a grid search using a task sequence determined by a single seed. Random probabilities were kept consistent across experiments to artificially remove training samples. The plastic components were not regularized, and DHP Softmax did not affect the hyperparameters."
}