{
    "title": "SJxIkkSKwB",
    "content": "The study focuses on training machine learning models incrementally using active learning with imperfect or noisy oracles. It explores batch active learning, selecting multiple samples to reduce training overhead. The approach combines uniform randomness and score-based importance sampling for selecting new sample batches. Experiments on image classification datasets show improvements over existing strategies, with the addition of a denoising layer to deep networks for robustness to label noises. Supervised learning is widely used in machine learning but requires labeled data, which can be time-consuming to annotate. Active learning selects relevant samples for annotation to train models incrementally, especially for deep networks with limited annotated data. Batch active learning appends multiple samples at each iteration to reduce training overhead, improving model complexity. Batch active learning procedures have been developed to reduce training overhead, but they suffer from the cold-start problem when initializing the model with a small seed dataset. This leads to performance degradation as the model is inaccurate at the beginning of the active learning process. The reliance on the model's output for sample selection can result in uncertain estimation of selection criteria and the selection of wrong samples. In real-world scenarios, the assumption of a perfect oracle for annotating samples is not always valid, especially with the increasing use of crowd sourcing platforms like Amazon Mechanical Turk. In real-world scenarios, crowd sourcing platforms like Amazon Mechanical Turk are used for data labeling, but the oracles are often noisy. Multiple annotations on the same sample do not guarantee noise-free labels due to systematic bias, leading to consistent mistakes. An experiment on the ESC50 dataset showed that even with 5 annotators, 10% of samples were annotated incorrectly. Classical active learning algorithms under-perform in noisy oracle scenarios, prompting the development of a batch active learning strategy to be robust to noisy oracles. The main contributions of this work include proposing a batch sample selection method based on importance sampling and clustering, incorporating model uncertainty into sampling probability, and introducing a denoising layer to deep networks for robust active learning in noisy oracle scenarios. Results show that the method outperforms state-of-the-art methods in noisy scenarios. Active learning is a well-studied problem in deep learning, with two key approaches being discrimination and representation. Representation focuses on selecting samples to represent the training set, while discrimination selects tough examples using methods like entropy. Ensemble methods and recent works like Bayesian Active Learning by Disagreement are also used in this area. Batch Active Learning: Utilizing submodularity for set selection in batch acquisition manner has been explored in various works. Adaptive submodularity and pool-based active learning are concepts related to active learning, with approaches like Bayesian Active Learning by Disagreement using these ideas for optimization. Batch active learning methods have been explored in various works, including pool-based active learning and adversarial learning of variational auto-encoders. Model uncertainty in deep learning models has been addressed using dropout and Batch Normalization. In the context of deep learning models, model uncertainty is emphasized through stochastic layers like Dropout and Batch Normalization. The importance of noisy labels from an oracle is also highlighted in various works, with some focusing on adaptive submodularity for theoretical guarantees. Active learning with noisy oracles has been studied, but not in the deep learning setup. In (Du & Ling, 2010), a task with a noisy oracle is discussed. (Khetan et al., 2018) used an Expectation Maximization algorithm to estimate correct labels and worker quality. Previous works in noisy oracle settings for deep learning models include (Jindal et al., 2019; 2016), who suggest adding a full-connected dense layer to the model. However, their denoising layer lacks a probability simplex constraint and uses a modified loss function for noise accountability. The paper introduces notations and formally defines batch active learning with noisy oracles, including the probability simplex and Shannon entropy. The Shannon entropy is defined for vector p \u2208 \u2206 K\u22121. The KL-divergence is non-negative and 0 only if p = q. Concerned with K class classification problem with sample space X and label space Y = {1, 2, . . . , K}. Classification model M is g \u03b8 : X \u2192 Y with softmax output p = softmax(g \u03b8 (x)). Batch active learning selects important samples to maximize prediction accuracy E p X \u00d7Y [(h \u03b8 (x) = y)]. Batch active learning selects important samples to maximize prediction accuracy by updating the training dataset with labels from an oracle routine. The oracle can be an 'Ideal Oracle' or a 'Noisy Oracle' which flips true labels with a certain probability. The noise channel is modeled as a K-symmetric channel, a generalization of the binary symmetric channel. The K-symmetric channel (K-SC) is defined with \u03b5 as the probability of label flip. It simplifies the oracle noise strength with a single parameter. In noisy active learning, the training dataset is updated after selecting a subset B. The proposed solution for noisy batch active learning involves selecting important and diverse samples for the current model. The active learning process faces challenges when dealing with uncertain models, leading to inaccurate quantification of sample diversity and importance. Incorporating model uncertainty in batch selection is crucial, with various score functions like max-entropy and BALD used to assign importance scores to samples. The BALD score quantifies uncertainty reduction in model samples. Clustering is done using JS divergence for diversity, selecting centroids with importance sampling. The BALD score quantifies uncertainty reduction in model samples through clustering with JS divergence for diversity and selecting centroids with importance sampling. The distance metric d is used for Agglomerative hierarchical clustering to group similar samples together, ensuring both importance and diversity in the selected samples. The boundedness of the distance metric helps in incorporating uncertainty, with cluster centroids selected based on median score samples. Importance sampling of the cluster centroids is performed, with a random centroid selected based on its importance score. The uncertainty of a model is measured using importance score and similarity distance. Methods like dropout and batch normalization are used to approximate uncertainty in deep learning models. Model uncertainty is denoted as \u03c3 \u2208 [0, 1], representing the model's confidence in its output. Confidence can be measured through statistical dispersion of softmax outputs. The uncertainty of a model is denoted by \u03c3 \u2208 [0, 1], with 0 representing complete certainty and 1 for full uncertainty. The uncertainty measure \u03c3 is computed as the variation ratio of the model's output from multiple stochastic forward passes. As the model becomes more accurate (low \u03c3) through active learning, sample selection shifts towards importance sampling and clustering using deterministic annealing with the Boltzmann-Gibbs distribution. The uncertainty of a model is represented by \u03c3 \u2208 [0, 1], where 0 indicates complete certainty and 1 signifies full uncertainty. In active learning, if model uncertainty is high (\u03c3 \u2192 1), all points in the pool should be equally probable for selection, while if uncertainty is low (\u03c3 \u2192 0), the centroid should be chosen. This selection process is analogous to state energy levels influenced by temperature. The text discusses uncertainty estimation in active learning using Gibbs distribution. The model's uncertainty estimate \u03c3 is used to select cluster centroids with low uncertainty. The distance metric d is bounded between 0 and 1, providing interpretation for state energy levels. Gibbs distribution is used to draw samples from the pool for constructing batches. The probability of drawing a sample \u03b6 given the cluster centroid c, distance matrix D, and inverse temperature \u03b2 is determined by the choice of inverse function. Setting \u03b2 to 0 results in a uniform random distribution, while setting it to infinity selects cluster centroids with importance sampling. The approach of uncertainty-based batch active learning is summarized in Algorithm 1, with a soft bridge between selecting cluster centroids with importance sampling and a uniform random distribution. To address noisy oracles, a denoising layer is appended to the model, using softmax outputs as inputs. The approach involves updating the model with a noisy-channel layer, detaching the noisy model, and setting weights for transition probabilities. The active learning algorithm with a noisy oracle is outlined, leading to the evaluation of proposed methods on different datasets for training CNNs. The evaluation of algorithms for training CNNs on datasets like MNIST, CIFAR10, and SVHN is conducted using specific architectures and tools like Adam and PyTorch. The denoising layer is initialized with the identity matrix, and clustering is done using Scikit-learn. The uncertainty measure is computed through stochastic forward passes, and the inverse uncertainty function is chosen accordingly. The inverse uncertainty function \u03b2 = f (\u03c3) in Algorithm 1 is chosen using cross-validation with a scaling constant. Cross-validation is performed for noise-free settings to verify parameter robustness against different noise magnitudes. The approach is compared with Random, BALD, and Coreset methods for active learning. The study compares active learning algorithms including 2 \u2212 OP T, Entropy, and VAAL. The experiments start with a small number of images and retrain the model from scratch after each batch acquisition. A total of 20 random initializations are performed, showing that the proposed algorithm outperforms all existing algorithms. Random selection is noted to work better in the initial stages. The proposed algorithm outperforms all existing active learning approaches by using uncertainty-based randomization to improve model performance across various datasets. Noisy oracle negatively impacts performance, with denoisification through a noisy-channel layer showing improvement. The proposed algorithm improves model performance by using uncertainty-based randomization in active learning. Denoisification through a noisy-channel layer helps combat the negative impact of a noisy oracle. Recent baselines like VAAL and Coreset may not always perform well due to issues with representing training and pool data. The proposed algorithm enhances model performance by incorporating uncertainty-based randomization in active learning. It addresses the impact of noise from a noisy oracle through denoisification. The importance of model uncertainty in assigning confidence to judgments is emphasized, with a focus on strengthening confidence over time. Incorporating denoising implicitly in the model is found to be more effective than robustness against oracle noise. The uncertainty measure \u03c3 plays a crucial role in the algorithm, particularly in scenarios where strong noise influences the model's performance and affects the estimation of uncertainty measures. The proposed algorithm enhances model performance by incorporating uncertainty-based randomization in active learning to combat label noise. The denoising layer aids in improving uncertainty estimates, leading to better judgment in sample selection. The algorithm utilizes mutual information for importance scoring and cluster sampling for noisy oracles. The proposed method incorporates model uncertainty into Gibbs distribution for cluster sampling in active learning. It introduces a denoising layer in deep networks to estimate label noise. Experimental results show improved robustness against noisy labels and outperforming state-of-the-art methods on MNIST, SVHN, and CIFAR10 datasets. The method also shows promise for batch active learning in imperfect data acquisition scenarios. The crowd worker annotates sound tracks in the ESC50 dataset using Amazon Mechanical Turk. They select the class the sound belongs to with confidence, or choose \"Unsure.\" Gold standard tracks are included for quality control, and annotator labels are discarded if they mislabel these tracks. The confusion table shows disagreement between majority-voted crowd labels and ground truth for some classes like frog and helicopter, even with 5 annotators. The experimental results presented in Section 5 show that the majority vote of annotations still does not fully agree with the ground truth class. Additional experimental results for active learning algorithm performance with different levels of oracle noise strength are shown in Figures 6 and 7. The proposed algorithm outperforms existing works for low noise strength (\u03b5 = 0.2), but struggles to match other algorithms for higher noise strength (\u03b5 = 0.4) due to increasing model uncertainty. The proposed uncertainty-based algorithm prioritizes random sampling even with more data. Using a denoising layer improves model uncertainty estimates, leading to better performance compared to other approaches. Results on CIFAR10 and SVHN datasets show the algorithm's superiority, especially with oracle noise strength of \u03b5 = 0.2. The proposed algorithm, along with the denoising layer, outperforms other benchmarks for \u03b5 = 0.2 and shows improvement in uncertainty estimates for SVHN dataset. The algorithm prioritizes random sampling even with more data, leading to better performance compared to other approaches. The proposed uncertainty-based algorithm, with a denoising layer, outperforms other benchmarks for various active learning algorithms on CIFAR100 dataset. Results show similar or better performance compared to baselines, especially in countering noisy oracles. Quantitative results are presented in tables for MNIST, CIFAR10, SVHN, and CIFAR100 datasets."
}