{
    "title": "HkfYOoCcYX",
    "content": "Weight pruning is an efficient model compression technique, but conventional sparse matrix formats limit memory reduction and slow down computations. Viterbi-based pruning attempts to compress index information while keeping decoding parallelizable, but decoding non-zero weights remains sequential. A new sparse matrix format combining pruning and weight quantization is proposed for highly parallel decoding of the entire sparse matrix in this paper. The proposed sparse matrix format combines pruning and weight quantization to compress LSTM parameter storage by 19x compared to baseline models. Compressed weights and indices can be quickly reconstructed using Viterbi encoders, allowing for faster parameter feeding to processing elements in deep neural networks. Pruning removes redundant connections without accuracy loss, and the results are stored in sparse matrix formats like CSR or CSC. The Viterbi-based pruning BID14 reduces memory footprint of sparse matrix formats by compressing indices. Weight compression can be improved by quantizing non-zero values and using Viterbi Decompressor for faster sparse-to-dense matrix conversion. Various quantization techniques can be applied to compress non-zero values. The paper proposes a novel weight-encoding scheme using Viterbi-based pruning to compress non-zero values in neural networks, reducing memory footprint. The weight matrix composition ratio of '0' and '1' is crucial for efficient encoding. The proposed method utilizes Viterbi-based pruning to compress non-zero values in neural networks, demonstrating its applicability to RNNs and CNNs of various sizes and depths. This approach allows for a highly parallel sparse-to-dense reconstruction architecture, improving computational efficiency and reducing redundancy in DNNs. Magnitude-based pruning methods like BID6 and BID14 are popular for their computational efficiency. BID6 showed a 9\u00d7-13\u00d7 pruning rate on AlexNet and VGG-16 networks without accuracy loss on ImageNet dataset. BID14, using a Viterbi-algorithm based pruning method, demonstrated a 38.1% memory reduction compared to BID6 with no accuracy loss. However, the reduction in memory was limited due to uncompressed non-zero values. Several weight quantization methods were suggested to compress neural network parameters. BID25 reduced weights to binary, activations to 2 bits, and gradients to 6 bits with a 9.8% top-1 accuracy loss on AlexNet. BID5 achieved a binary-weight AlexNet with a 2.0% top-1 accuracy loss and \u223c10\u00d7 compression rate. RNNs can also be quantized to reduce memory footprint, with a proposed method reducing memory \u223c10.5\u00d7 with negligible performance degradation. Combining pruning with weight quantization can result in a 35\u00d7 increase in compression rate. Several methods have been proposed to compress neural network parameters, including weight quantization and pruning. While magnitude-based pruning methods showed high compression rates, they did not improve computation time significantly due to decoding sparse matrix formats. BID7 suggested using dedicated hardware and custom sparse matrix formats for acceleration, while BID10 focused on limiting irregularity in weight indices. BID14 also explored pruning neurons or feature maps to reduce computation, but faced limitations in compression rate due to additional pruning conditions. The compression rate was limited due to additional pruning conditions. BID14 used the Viterbi encoder for fast index matrix construction, but pairing non-zero weight values with corresponding indices was still sequential and slow. The proposed compression method involves Viterbi-based pruning followed by quantization using a multi-bit method. The quantized binary code matrices are then encoded using a Viterbi-based approach. The flowchart of the Double Viterbi compression method is illustrated in Figure 1. The proposed weight encoding scheme involves Viterbi-based pruning to compress the indices of non-zero values in a sparse weight matrix. This method minimizes accuracy degradation and reduces memory footprint. Quantization of non-zero values is then used for further memory reduction. The Viterbi-based pruning method is explained in detail in Appendix A.1. After pruning, the sparse matrix undergoes alternating multi-bit quantization (BID22). This quantization method is chosen for its high compression capabilities and compatibility with the Viterbi algorithm. The Viterbi Compression Matrix (VCM) format is used to represent the sparse matrix, resulting in significant memory savings compared to the original dense weight matrix. The process of reconstructing a sparse matrix from the VCM format is challenging due to the sequential counting of ones in indices. To overcome this, binary weight codes are encoded alongside indices using the Viterbi algorithm. This allows for parallel sparse-to-dense matrix conversion, requiring a specific quantization method to minimize accuracy loss. The VD structure acts as a random number generator in this process. The VD structure acts as a random number generator in the process of encoding binary weight codes using the Viterbi algorithm. The composition ratio of '-1' and '+1' in each generated binary weight code is 50% each, due to symmetric initialization of weights in DNNs. The alternating quantization method results in a high probability of finding an output matrix close to the target matrix with the Viterbi algorithm. Accuracy differences before and after Viterbi encoding were measured for various quantization methods. When Viterbi encoding is applied to weight quantized using alternating quantization BID22, validation accuracy only degrades by 2%, compared to 71% degradation with other methods like linear and logarithmic quantization BID16 BID19. The uneven weight distribution, caused by normal distribution of neural network weights, affects accuracy. Considering pruned parameters as \"Don't Care\" terms can increase the probability of finding good Viterbi encoded weights. The Viterbi algorithm is used for weight encoding by selecting the best matching cases among all possible cases generated by the VD. A trellis diagram is constructed with cost functions for transitions, and branch metrics are computed to maximize accumulated values. The branch metric is defined to maintain accuracy in selecting transitions. The branch metric is defined to minimize the number of incorrect bits in the encoded binary code compared to the original binary code. The network is retrained with specific parameters, alternating quantization is applied, and Viterbi encoding is performed repeatedly to reduce the number of incorrect bits. During retraining, the straight-through estimate is used, and after the last Viterbi encoding, location data for incorrect components is stored for correction. After retraining, a compressed parameter in Viterbi Weight Matrix (VWM) format is obtained, including DISPLAYFORM0, compressed index in VCM format, and indices where DISPLAYFORM1. Experiments on Penn Tree Bank (PTB) corpus with 10K vocabulary are conducted, evaluating performance using perplexity per word. Training used training and validation datasets only, with accuracy measurement on the test dataset done after training without tuning hyperparameters on the test dataset. After retraining, the RNN model with 1 layer of LSTM and 600 memory units is pruned with 80% rate using Viterbi-based technique, then quantized and retrained. This process is repeated 5 times with varying quantization bits k. Increasing k improves perplexity per word (PPW) but also increases memory requirement. With further quantization and Viterbi-based compression, parameter size is reduced by 78% to 90% compared to BID14. The Viterbi-based compression reduces parameter size by 78% to 90%. Increasing the number of VD outputs N o degrades PPW and increases memory requirement. The optimal N o is 100/(100-pruning rate (%)). Applying the compression method maintains PPW in pruned networks but degrades it in dense networks. The Viterbi-based compression method reduces parameter size significantly. By combining Viterbi pruning and alternating quantization, the probability of finding close matches between bits is increased, resulting in no degradation in Perplexity (PPW). Testing on the latest RNN model BID23 shows improved performance on PTB and WikiText-2 corpus. Pruning 75% of parameters and quantizing to 3 bits reduces memory requirements by 94.7% without PPW degradation. Our proposed compression method can be applied to various types of networks, including RNN models for machine translation and CNNs like VGG-9 on CIFAR-10 dataset. Validation and test errors are measured during the retraining process, with optimal parameters chosen based on pruning rates. The proposed compression method for DNNs, including VGG-9 on CIFAR-10, results in a 39% smaller memory footprint compared to BID8. The VWM format generated by the scheme utilizes compressed indices and binary weight codes. Combining Viterbi-pruning BID14 with alternating quantization BID22 yields a 10% smaller memory requirement than VWM format. The VWM format allows for parallel sparse-to-dense matrix conversion, increasing the parameter feeding rate by 40.5%. The proposed method for DNN compression results in a 39% smaller memory footprint compared to BID8. It utilizes compressed indices and binary weight codes, combining Viterbi-pruning BID14 with alternating quantization BID22 for a 10% smaller memory requirement. The VWM format enables parallel sparse-to-dense matrix conversion, increasing the parameter feeding rate by 40.5%. The speed of sparse-to-dense matrix conversion is analyzed in detail, with quantization conditions specified for non-zero values, convolution filters, fully-connected layer weights, and sparse matrix indices. A cycle-level simulator demonstrates fast sparse matrix-matrix multiplications with parallel reconstruction of dense matrices. The proposed method for DNN compression achieves a higher feeding rate by reconstructing index masks and binary codes using highly compressed data fed by DRAM. Simulation results show a 20.0-106.4% increase in feeding rate compared to the baseline case. Higher sparsity leads to a higher feeding rate, allowing for faster reconstruction of binary codes. The proposed DNN compression technique achieves a higher feeding rate by reconstructing index masks and binary codes using highly compressed data from DRAM. The reconstruction rate of binary codes increases with reduced non-zero values and bit corrections. The proposed scheme shows a faster parameter feeding rate compared to the baseline structure, allowing for fast dense matrix reconstruction. The proposed DNN compression technique utilizes high compression rate and fast dense matrix reconstruction process through Viterbi-based pruning and multi-bit quantization. This scheme significantly reduces memory requirements for RNN and CNN parameters. The proposed DNN compression technique uses Viterbi-based pruning and multi-bit quantization to achieve high compression rates and fast dense matrix reconstruction. The Viterbi algorithm is employed to select the index matrix that minimizes accuracy loss among binary matrix candidates generated by the VD. The pruning process involves constructing a Trellis diagram, computing the cost function using path and branch metrics, and selecting branches that maximize the path metric. Different branch metrics are used for pruning, enhancing the efficiency of the compression process. The Viterbi-based pruning technique uses a metric to determine which parameters to prune based on their magnitude and penalties. Scaling factors are empirically determined, and the ideal pruning rate is 50%. The algorithm selects the state with the maximum path metric and traces back to the first state by selecting surviving branches. The VD structures act as random number generators with a 50% probability to generate '0' or '1'. Comparators and threshold values are used for various pruning rates. A N-bit comparator receives N VD outputs and determines if the combined value is greater than a threshold. The pruning rates are controlled by comparators and threshold values, affecting the index compression ratio. A low N is preferred for pruning convolutional layers, while a high N can be used for fully-connected layers due to the trade-off between index compression ratio. In our paper, we prune weights of LSTMs and fully-connected layers in VGG-6 on CIFAR-10 using N ind = 50 and N c = 5. For pruning weights of convolutional layers in VGG-6 on CIFAR-10, we use N ind = 10 and N c = 5. The RNN model in BID23 consists of three LSTM layers and utilizes techniques like mixture-of-softmaxes for better perplexity. Parameters in the first layer have high sparsity, so N o = 6 is used, while in the remaining layers with 70% pruning rate, N o = 3 is used. Experiments are extended to RNN models for machine translation, including an encoder, decoder, and attention layer with 4-layer LSTMs of 1024. The model includes an encoder, decoder, and attention layer with 4-layer LSTMs of 1024 units each. LSTM weights are pruned by 75% using Viterbi-based pruning and quantized with k = 4. Optimal N o values are chosen based on sparsity, reducing memory requirements by 93.5%. The proposed technique can be extended to RNNs for complex tasks."
}