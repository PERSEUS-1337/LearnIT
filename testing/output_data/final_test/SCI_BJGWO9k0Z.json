{
    "title": "BJGWO9k0Z",
    "content": "In this paper, the focus is on deep learning topics related to graph structured input data and understanding deep networks. The study specifically looks at the topological classification of reachability in planar graphs (Mazes) using a suitable CNN architecture. The cost function around the solution is derived, showing that it does not depend on the maze size due to rare events in the dataset. Poorly performing local minima are identified as obstacles to learning in this context. In this study, the focus is on deep learning with graph structured input data and understanding deep networks. The research identifies obstacles to learning, such as poorly performing local minima, in networks with up to $128$ layers. Deep convolutional networks have shown success in tasks like image classification, speech recognition, and natural language processing, which typically involve data on regular lattices in Euclidean space. However, attention is now shifting to problems with non-Euclidean input data, such as social networks, knowledge bases, and 3D shapes. The literature explores graph-like input data, deep network functioning, and the minimization of high dimensional non-convex loss functions using stochastic gradient descent techniques. The focus is on understanding deep networks and their performance in tasks involving non-Euclidean input data like social networks, knowledge bases, and 3D shapes. In this work, the focus is on advancing topological classification of graphs, specifically planar graphs on a regular lattice. This subset of graphs retains real-world applications like human body pose and road networks, while also being processable with standard CNN architectures. The class of graphs chosen has unique statistical properties from percolation theory and conformal field theories, enabling analytical computations. The authors introduce Maze-testing, a specialized version of the reachability problem in graphs, where random mazes are classified as solvable or unsolvable based on the existence of a path between starting and ending points. This problem provides analytical tractability and theoretical insights into the learning process. A deep network with O(L^2) layers is proposed to tackle the Maze-testing problem. The authors propose a deep network with O(L^2) layers for the Maze-testing problem, which can express an exact solution called the optimal-BFS minimum. Gradients around the minimum do not scale with L due to rare events in the data. Sub-optimal local minima, called \"neglect minima,\" occur when the network discards important features. These minima dominate the training dynamics. The Maze-testing problem involves rare events in data leading to sub-optimal local minima dominating training dynamics. The framework also relates to neural network architectures with augmented memory, such as Neural Turing Machines and memory networks. Hot-spot images track the state of the graph search algorithm, resembling external memory, and analyzing errors in the cost function is analogous to understanding errors in memory architectures. The paper discusses memory architectures for graph structured data, focusing on natural language reasoning and graph search tasks. It aims to improve understanding of cost functions, weight initialization, and optimization algorithms for these architectures. The paper is organized into sections detailing the Maze-testing problem, suggesting an appropriate architecture, describing optimal weights, training experiments, and analytical insights. The Maze-testing classification problem involves random two-dimensional black and white mazes with a defined starting point and ending point. Samples are labeled Solvable if the ending point is reachable from the starting point by moving along black cells, and Unsolvable otherwise. The proposed architecture processes an initial hot-spot image by generating a series of hot-spot images. These images represent an optimal solution where the red region spreads on the black cluster in the maze. The Solvable/Unsolvable label is determined by the values at the center of the maze. The maze in question is Unsolvable as the On cells do not reach the ending point. Maze-testing samples have non-trivial statistical properties derived analytically from percolation theory and conformal field theory. The maze problem at the percolation-threshold \u03c1 marks a phase transition in connectivity properties. Below \u03c1 c, solvability decays exponentially with distance, above it tends to a constant, and at \u03c1 c it decays as a power law. Maze-testing datasets can be easily generated by random methods. The maze problem at the percolation-threshold \u03c1 marks a phase transition in connectivity properties. Maze-testing datasets can be easily generated by random methods. Complexity increases with L. An image classification architecture is introduced to tackle the Maze-testing problem by framing maze samples as a subclass of planar graphs. Our architecture, a deep feedforward network with skip layers and a logistic regression module, can find good solutions for small Mazes during training. It can handle planar graphs well, unlike graph-oriented architectures that struggle with large sparse graphs. The network consists of alternating convolutional and sigmoid layers, processing two images at each layer to find solutions to the Maze-testing problem. The deep feedforward network with skip layers and a logistic regression module can find solutions for small Mazes during training. It consists of convolutional and sigmoid layers, processing two images at each layer to solve the Maze-testing problem. The logistic regression layer outputs the label Solvable based on the output probabilities. The network architecture can provide an exact solution by forming a cellular automaton executing a breadth-first search. The neural network processes the image to find solutions for small Mazes during training. It uses convolutional and sigmoid layers to spread On regions on black clusters containing the starting point. The final logistic regression layer checks if the center of the maze is On to output the labels accordingly. Activation thresholds v l and v h define On and Off states. The quantity v l is the smallest real solution of an equation, chosen above a critical value \u03bb c. The convolutional-sigmoid layer switches activations between Off and On, resembling a BFS. The layer's action is shown on different cases, bounding the expression for sub-arrays. There are 210 possibilities to be examined. Case A depicts the desired action of the layer on a white cell at a point x. The convolutional-sigmoid layer operates on different cases to determine activations. Case A shows the layer's action on a white cell at point x, while Cases B and C cover scenarios with black cells and nearby activations of the previous layer. All possible cases are exhausted, totaling 210 possibilities. The logistic regression layer in the maze architecture outputs Solvable or Unsolvable based on On/Off activation at the center. The network depth needs to scale with the number of sites in the maze for optimal performance. Probabilistic bounds on minimal depth can be determined using percolation theory. The text discusses the scaling of the shortest path in critical percolation, with probability distribution falling as l^-2. Training experiments were conducted on mazes of different sizes and depths. The experiments were split into two groups based on observed phenomena during training. The training experiments analyzed in detail in the next section found that for L = 16, M = 10000 mazes, and positive random initialization for K hot and K in [0, 6/8], the network achieved \u2248 9% normalized test error performance in 3 out of 7 different initializations. The successful cases showed a checkerboard-BFS minima variant of Optimal-BFS, spreading On activations in a checkerboard pattern. However, test error did not improve below 44% when negative entries in K and K hot were allowed. After 14 attempts and 500 epochs, the network did not improve below 44%. A 6% improvement over the baseline error came from identifying the inverse correlation between white cells near the maze center and solvability. Despite various initialization strategies, the network consistently settled into a partial neglect minimum. Further training with a biased dataset did not push the weights away from this behavior. The study involved randomly selecting maze shapes and starting points to create a biased dataset for training neural networks. Despite efforts, the L = 16 and L = 32 mazes performed poorly with a 50% test error. The networks settled into a \"total neglect minimum\" where weights favored certain values, leading to weak dependence on starting points and maze shapes. Further analysis is needed to understand this training phenomenon. The study analyzed the cost function landscape around the optimal-BFS minimum and the total neglect minimum obtained during training on a biased dataset. The optimal-BFS minimum shares similarities with the checkerboard-BFS minimum. Two separate models were used to capture the short and long scale behavior of the network near the optimal-BFS minimum. The study analyzed the cost function landscape around the optimal-BFS minimum and the total neglect minimum obtained during training on a biased dataset. In the first model, the network is approximated by linearizing its action around weak activations to identify the density of \"bugs\". The second model discretizes activation levels into binary variables to study the behavior of cellular automaton when bugs are introduced. A numerical example in FIG3 shows the dynamics of the bug development in the network. The study analyzed bugs in neural networks, showing how errors can be pinpointed in a sharp fashion by identifying fast and local processes causing unwanted switching between activations. Bugs are considered harmful and have a clear meaning in the algorithmic context. The study focused on bugs in neural networks, analyzing how errors can be identified through fast and local processes causing unwanted activation switching. As network weights are perturbed, the chance of generating bugs scales as DISPLAYFORM0 for (\u03bb \u2212 \u03bb c ) <\u2248 0 and becomes zero for \u03bb \u2212 \u03bb c >= 0 where C \u2248 1.7. This behavior is expected to hold for small perturbations with different C and \u03bb c values. Testing on various perturbations confirmed this claim. At the critical value, the solution corresponding to a certain variable vanishes, leading to a deviation from the expected behavior in neural networks. Black Off cells surrounded by white cells are less likely to switch on spontaneously compared to those in a cluster of black cells. At the critical value, a cluster of black cells causes network malfunction. The probability of bugs is studied by analyzing the output of the final convolutional-sigmoid layer. Linearizing the system around low activation levels is essential for assessing the chance of bugs. The linear Hermitian operator (L) for a given maze (I) has random off-diagonal matrix elements dictated by I, with stability of Off activations ensured if the operator is contracting. The spectrum of L consists of localized eigenfunctions centered around random sites, with exponential decay away from the site. Repeated actions of the convolutional-sigmoid layer will make the function grow in size in a vicinity of the site. The repeated actions of the convolutional-sigmoid layer cause localized eigenvalues to grow, potentially leading to bugs. The task is to find the number of eigenvalues larger than 1 in magnitude in a given linear operator. The maximal eigenvalue occurs when the pattern is uniform on a large region. The chance of this uniform region existing decreases as the ratio of dimensions decreases. The chance of a uniform region existing decreases as the ratio of dimensions decreases. The number of isolated unwanted activations on specific regions is calculated to find the density of bugs near a certain threshold. The last step involves expressing the bug density in terms of a specific parameter. The text discusses the impact of a bug on test error and cost function, with key results showing that despite its appearance, the bug effect is smaller than a certain threshold. The bug quickly spreads in a maze and affects connected clusters. The bug quickly spreads in a maze and affects connected clusters, impacting test error and cost function. The chance of classifying an unsolvable maze as solvable is determined by the number of sites in the central cluster. The probability distribution for percolation is known, and the maximal cluster size is determined by the fractional dimension of clusters. The text discusses the relationship between test error, cost function, and the spread of bugs in a maze. It explains how the mislabeling chance affects the test error and provides key results in equations (3, 4). The text also mentions a potential training obstacle related to bug density and cluster size. The text discusses the impact of bug density on test error and cost function. It highlights how rare events in the dataset can flatten gradients near a critical point, affecting generalization robustness. The rarity of events in a finite dataset may still lead to perfect performance, but encountering a larger dataset with rarer events can pose challenges. The text discusses the impact of rare events in larger datasets on neural network performance. It explores the division of activations in the upper layer into dependent and independent parts, with implications for training errors and generalization. The variance of these parts is normalized, and near total neglect minima, their product approaches e^-10. In biased datasets, the maze is uncorrelated with labels, treating beta as noise. Solving the Maze testing problem requires the dependent part (alpha). The text discusses the impact of rare events in larger datasets on neural network performance, focusing on the division of activations into dependent and independent parts. It argues that increasing the dependent part (\u03b1) leads to a noisy region where \u03b1 < \u03b2 2, with the negative log-likelihood proportional to \u03b2 2 \u2212 \u03b1. This penalizes false predictions and reaches a minimum when \u03b1 = \u03b2 = 0, known as a total neglect minima. The conjecture of \u03b1 < \u03b2 2 is supported by numerical evidence in the Appendix. The deep convolution network with a finite kernel has a notion of distance and locality, with a typical correlation length (\u03c7) that needs to grow to an order of L to solve the problem. As \u03c7 grows, relevant and irrelevant information mix and propagate to the final layer. \u03b1 and \u03b2 scale differently, with \u03b1 depending on information traveling from the first to the last layer, while \u03b2 depends on locally accessible information at each layer. Numerical support for the conjecture of \u03b1 < \u03b2 2 is provided in the Appendix. In this work, an upper bound on the ratio \u03b1/\u03b2 2 is studied on 100 different paths from the total neglect minimum to the checkerboard-BFS minimum. The focus is on how a deep CNN architecture learns to classify topological properties of graph structured data, specifically planar graphs over regular lattices. A toy problem (Maze-testing) is used to show that a simple CNN architecture can provide an exact solution. The main contribution is an asymptotic analysis of the cost function landscape near different types of minima. Near two types of minima, the network settles into BFS type minima and poorly performing minima. Gradients near BFS minima do not scale with maze size L, allowing global optimization approaches to find them efficiently. A singularity in the cost function around the exact solution results in moderate gradients, preventing sharp increases in cost. Neglect minima, where the network ignores important features, present an obstacle to learning that worsens with maze size L. The network settles into poorly performing minima called neglect minima, where important features are ignored, leading to noisy predictions. These minima are stabilized by a \"wall of noise\" hindering the network's performance. The study's findings could potentially be applied to tasks with deep architectures, rare-events, bugs, and could be tested on real-world problems like basic graph algorithms and textual reasoning tasks. The importance of rare-events can be analyzed by studying errors on the dataset as it deviates from a minimum. Bugs can be identified by comparing network activations on the minimum and a perturbation. This analysis can lead to safer designs where the network fails typically and mildly. Neglect minima, where important features are ignored, can be identified. Rare-events can be analyzed by studying errors on the dataset as it deviates from a minimum. Neglect minima, where important features are ignored, can be identified by studying correlations or mutual information between features and activations. Adding extra terms to the cost function can overcome neglect and improve training dynamics. The architecture was implemented using Theano BID38 and tested on mazes of different sizes and depths. The study focused on analyzing rare-events by studying errors on datasets as they deviate from a minimum. The architecture was tested on mazes of various sizes and depths, with a slight change in the hot-spot distance fixed at L/2. The normalized performance was obtained numerically, showing curves collapsing on each other for specific results. The fit to the theory captured the behavior well over three orders of magnitude, with minor discrepancies expected due to asymptotic results. The study analyzed rare-events by studying errors on datasets as they deviate from a minimum. The architecture was tested on mazes of various sizes and depths, with a slight change in the hot-spot distance fixed at L/2. The normalized performance was obtained numerically, showing curves collapsing on each other for specific results. The fit to the theory captured the behavior well over three orders of magnitude, with minor discrepancies expected due to asymptotic results. To prepare the action of sigmoid-convolutional for linearization, variables on locations with black and white cells are introduced. Destabilization occurs at \u03bb = 1/5 and is not blurred by inhomogeneous terms. The eigenvectors of S with large s are associated with large black regions in the maze, with stability analysis carried out on the homogeneous equation \u03c8 = S \u03c8. The study analyzed rare-events by studying errors on datasets as they deviate from a minimum. Stability analysis can be carried out on the homogeneous equation \u03c8 = S \u03c8 where s n < 1 means stability and s n > 1 implies a bug. Abstract classification tasks involve classifying data points into two categories using a deterministic function. Comparing different probability distributions, a penalty to false confidence or noise is observed. The study discusses penalties for false confidence or noise, suggesting taking both \u03b1 and \u03b2 to zero for maximal entropy distribution. Logistic regression is used to estimate probabilities for solvable and unsolvable cases. The study discusses penalties for false confidence or noise, suggesting taking both \u03b1 and \u03b2 to zero for maximal entropy distribution. Logistic regression is used to estimate probabilities for solvable and unsolvable cases. Numerical evidence shows that \u03b1 \u03b2 2 in a large region around the total neglect minima found during training on a biased dataset. Variance of the top layer is studied for a given set of parameters. The study analyzes the variance of top layer activations with fixed parameters in different maze shapes to estimate \u03b1 and \u03b2. Random paths are chosen to explore the total neglect minima and checkerboard-BFS minima. The graph shows the statistics of \u03b1/\u03b2 2 on these paths. The graph displays statistics of \u03b1/\u03b2 2 on 100 different paths, showing max, mean, and median values. No path violated the \u03b1 \u03b2 2 constraint near the total neglect minima. Different hypercube lengths were tested, all leading to the same conclusions."
}