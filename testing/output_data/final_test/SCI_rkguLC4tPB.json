{
    "title": "rkguLC4tPB",
    "content": "An Unknown-aware Deep Neural Network (UDN) is proposed to improve image classification systems by accurately classifying known objects and safely rejecting unknown objects. UDN enhances existing CNNs by modeling the product relationship among features to reduce the probability of assigning an object to a target class with high confidence. UDN uses an ensemble of product operations to balance classifying known objects and rejecting unknowns. An information-theoretic regularization strategy is proposed to improve UDN's performance in detecting unknowns. Experimental results show UDN outperforms other methods by 25 percentage points in rejecting unknowns while maintaining classification accuracy. State-of-the-art classifiers like CNNs assume all testing objects belong to target classes, but in real-world scenarios, unknown objects can be encountered. Blindly assigning unknowns to target classes can reduce prediction accuracy and pose safety risks, especially in applications like autonomous cars or healthcare systems. To address this, an ensemble approach called UDN balances classifying known objects and rejecting unknowns, with a regularization strategy to enhance performance. Experimental results demonstrate UDN's superior ability to reject unknowns while maintaining classification accuracy. In collaboration with a top US hospital, a seizure detector was developed using EEG signals from 4,000 patients. The detector, trained on 6 types of seizures, may misclassify rare or unknown seizure types, posing serious risks. A deep neural network is proposed to accurately classify test objects into known classes and reject unknowns to prevent potential harm. A neural network is designed to accurately classify test objects into known classes and reject unknowns by leveraging confidence levels from the softmax layer output. This approach aims to detect unknown objects based on lower confidence levels compared to known objects. The neural network aims to reject unknown objects with lower confidence levels, but current methods struggle to effectively do so. CNNs' strong generalization ability can lead to misclassification of unknowns as target classes due to the weighted sum operation in the convolutional layers. The Unknown-aware Deep Neural Network (UDN) is proposed to address the challenge of classifying unknown objects with high confidence. By using a product operation to model the relationship among features, UDN reduces the probability of assigning an object to a target class if key features are not matched, leading to lower confidence in classification. This approach aims to overcome the conflict between accurately classifying knowns and rejecting unknowns. The Unknown-aware Deep Neural Network (UDN) uses product operations to detect unknown objects more effectively than classical CNNs. It leverages a set of product relationship subnets and an information-theoretic regularization strategy to balance accurate classification of known objects and rejection of unknowns. This approach improves the confidence gap between known and unknown objects, enhancing the accuracy of UDN in rejecting unknowns. UDN is a deep neural network that effectively detects unknown objects by enlarging the confidence gap between known and unknown objects. The final loss function of UDN is fully differentiable, allowing for learning through back-propagation. UDN outperforms the state-of-the-art by up to 20 points in unknown rejection accuracy while maintaining high accuracy in classifying objects from target classes. The OpenMax layer assigns testing objects to target classes based on distance, with a probability threshold determining assignment to an unknown class. Various methods like Prior Networks and ODIN use different strategies to detect out-of-distribution objects. ODIN employs a two-pass inference approach to enhance softmax probabilities for unknown detection, albeit introducing extra hyper-parameters. Our UDN method outperforms OpenMax and ODIN by using maximal path probability as a confidence measure, which is more effective in rejecting unknowns. MC-Dropout also shows promise in rejecting unknowns but sacrifices accuracy in classifying known classes. Our UDN method focuses on rejecting \"unknown unknowns\" without needing labeled unknowns in training, unlike methods that detect \"known unknowns\". It enhances CNNs to reject unknown objects during inference, unlike outlier detection methods that do not consider classification objectives. UDN enhances CNNs to reject unknown objects during inference by incorporating the objective of unknown rejection into the learning process. It differs from outlier detection methods that do not consider classification objectives and from DNDF, which enhances a random forest classifier with the feature learning ability of a CNN. UDN enhances CNNs by incorporating the objective of unknown rejection into the learning process. It distinguishes unknown from known objects and improves classification accuracy through independence among trees. A regularization strategy is introduced in Sec. 4 to improve UDN accuracy at rejecting unknowns. UDN architecture consists of a convolutional module and M independent product relationship (PR) subnets. Each PR subnet contains a fully connected (FC) component connected to a binary tree structure to model product relationships among features. The UDN architecture includes a convolutional module and multiple product relationship (PR) subnets. Each PR subnet has a fully connected (FC) component linked to a binary tree structure for modeling product relationships among features. The nodes in the binary tree are split nodes denoted as L, with each split node converting output from an FC node into a value in the [0, 1] range using the sigmoid function. Leaf nodes are parameterized with a C-dimensional probability distribution, where each element corresponds to a class. The path probability \u00b5l(x) is used to model the product relationship in the PR subnet. The UDN architecture includes a convolutional module and multiple product relationship (PR) subnets with fully connected (FC) components linked to a binary tree structure. The path probability \u00b5l(x) models the product relationship in the PR subnet, where each split node determines the probability of input x reaching the next node on the path. The prediction at each leaf node is produced by a PR subnet, with \u03c0 ly denoting the probability that the input belongs to class y. UDN architecture includes multiple PR subnets with FC components linked to a binary tree structure. Prediction is made by averaging outputs of each subnet. Ensemble of PR subnets results in good generalization performance. Parameters for each node are learned through backpropagation. Important features for a known class are automatically identified during training. The UDN architecture includes multiple PR subnets with FC components linked to a binary tree structure. Prediction is made by averaging outputs of each subnet, resulting in good generalization performance. Important features for a known class are automatically learned and mapped to split nodes on the same path during training. The UDN architecture consists of multiple PR subnets with FC components connected to a binary tree structure. The max path, with the highest probability among all paths, determines the classification of an object. The max path probability can measure the classifier's confidence in its decision. The confidence is calculated as the max path probability of the subnet for a given object. The final confidence of object x in the UDN architecture is measured by the max path probability of the subnet PR. Using the max path probability to reject unknowns is more effective than the maximal weighted sum in CNN, as it limits the probability of unknowns by not giving large probabilities at every split node. Incorporating the objective of unknown rejection ensures the effectiveness of using the max path probability. Incorporating unknown rejection into the learning process of UDN involves penalizing paths with high entropy in their probability distribution. This ensures that the max path probability of each subnet is significantly higher than other paths, improving unknown rejection effectiveness.softmax transformation is applied for normalization of the probability distribution. The entropy of the path probability distribution in a UDN is calculated using softmax transformation. To penalize subnets with uniform distributions, entropy is added to the log-loss term. The total log-loss for the UDN is defined by the sum of individual subnet losses. Training involves minimizing the total log loss by adjusting parameters using SGD. We employ SGD to minimize loss w.r.t. \u0398 and \u03c0 in deep neural networks. UDN is effective on CIFAR-10, CIFAR-100, and SVHN datasets. Models are trained on different datasets and tested with unknown examples. Evaluation includes CNNs with Weighted-Sum, OpenMax, ODIN, Softmax. The study compares various unknown rejection methods like OpenMax, ODIN, MC-Dropout, and Softmax with the proposed UDN and UDN-Penalty models, showing superior performance. Experiments were conducted on Google cloud using Pytorch, with hyper-parameters set for training and testing on different datasets. The study evaluates the effectiveness of UDN in distinguishing known and unknown images using TNR at 95% TPR and AUROC metrics. TNR measures the probability of correctly recognizing unknown images, while AUROC represents the Area Under the Curve. UDN and UDN-Penalty methods were tested on DenseNet using the same architecture as ODIN. The output is connected to 10 depth-5 trees with 3 FC layers in each set. Training time for UDN was 9.4 hours, slightly slower than DenseNet. ODIN evaluation was also conducted. UDN and UDN-Penalty outperform various methods in rejecting unknowns while maintaining accurate classification of CIFAR-10 images. They achieve up to 10 points higher True Negative Rate (TNR) and significantly higher Area Under the Receiver Operating Characteristic (AUROC) scores, particularly excelling at detecting SVHN as unknowns. These methods effectively separate knowns and unknowns across different parameter settings. Our UDN architecture effectively separates knowns and unknowns by computing confidence through probabilities at split nodes. MC-Dropout performs best at rejecting MNIST as unknowns, while UDN-Penalty outperforms UDN in rejecting unknowns in all cases. In this work, a regularization strategy is proposed for UDN to enhance its performance in rejecting unknown objects. UDN replaces the softmax layer in traditional CNNs with a novel tree ensemble to effectively separate knowns and unknowns. The ensemble of PR subnets helps alleviate the drop in classification accuracy caused by regularization, resulting in a very small decrease in accuracy. A regularization strategy is proposed for UDN to enhance its unknown rejection capacity. The gradient of the loss with respect to the weights of prediction nodes can be decomposed using the chain rule. The output of UDN and UDN-Penalty is connected to 10 trees with a depth of 6. The model architecture used is DenseNet for all methods, including ODIN. UDN and UDN-Penalty are connected to 10 trees with a depth of 6. The evaluation of ODIN uses recommended parameters. Experiments on CIFAR-10 and SVHN show UDN outperforms other methods significantly. The model architecture used is DenseNet for all approaches. UDN and UDN-Penalty are connected to 10 trees with a depth of 4. ODIN uses a temperature parameter of 1000 and perturbation magnitude of 0.0014. MC-Dropout drop rate is set at 0.5 with 100 forward passes. Images from CIFAR-10, CIFAR-100, and randomly selected SVHN classes are used as unknowns. UDN outperforms other methods in TNR and AUROC, with at least a 12-point difference. Key hyper-parameters for UDN are the number of PR subnets and the depth of the binary tree. In experiments, a depth of 6 or more and over 10 PR subnets make the UDN method effective. For datasets with fewer classes like MINIST and CIFAR-10, a depth of 4 works well too. Increasing tree depth or subnets doesn't reduce effectiveness, making these parameters easy to adjust."
}