{
    "title": "rkxDon4Yvr",
    "content": "Current work on neural code synthesis involves training sophisticated architectures on simplified domain-specific languages using genetic programming with an iteratively retrained discriminator. This approach generates a population suitable for labeled training data, improving network performance compared to uniform sampling techniques. Automated code synthesis aims to make coding more accessible to nonexperts. Neural code synthesis simplifies software creation for nonexperts by using programming by example, where input-output examples guide program functionality. Recent research shows promising results with DeepCoder and Zohar & Wolf. The research aims to use a neural-network-based approach for code synthesis in a general-purpose low-level programming language, expanding the search space for possible programs. Existing methods using uniform sampling or enumerative approaches are inadequate for larger search volumes, leading to poor inference results. The research proposes a novel discriminator-based system to address the training corpus generation problem in code synthesis. By iteratively creating new sub-corpora and measuring their properties against user-requested problems, the system selects programs with similar I/O mappings and source code features to solve the problems. This approach outperforms conventionally generated corpora using uniform sampling, especially in a more generalised programming language. Our approach in neural code synthesis outperforms uniform sampling by a factor of 2 in find-rates. Comparing against genetic programming, our method can find a wide range of useful programs based on input/output examples without labeled data. The structure of the training corpus greatly impacts a neural network's performance in code generation tasks. The paper discusses the importance of neural code synthesis and introduces a methodology using synthesis and discriminator networks for corpus generation. It evaluates this approach against traditional methods and highlights the significance of training corpus structure in code generation tasks. Neural code synthesis is a key area of study in programming, with different subfields like logic-based solvers, stochastic-search-based Genetic Programming, and neural network approaches. While solvers are limited in programs with loops, Genetic Programming lacks cross-program inference compared to neural networks. The potential for inter-program inference within neural code synthesis is highlighted, building on the results of DeepCoder. Neural code synthesis focuses on programming by example, with DeepCoder being a core work in the field. Subsequent work improved upon DeepCoder by using a partial program execution model to enhance search speed and increase program length. Both approaches operate in the same Domain Specific Language, focusing on arrays of integers. DeepCoder uses a simple enumerative strategy for corpus generation. Neural code synthesis involves programming by example, with DeepCoder using an enumerative strategy for corpus generation. Other domains in the field include string-manipulation functions and deduction of behavioral algorithms in grid environments. Synthetic training sets are often generated by uniform sampling in these domains. Our work introduces a strategy for generating a targeted training corpus without the need for human-provided labeled data, boosting inference performance. In contrast to other approaches using random uniform sampling or enumerative strategies, we focus on programming generation in grid environments. Shin et al. (2019) explore the effects of inputs on program training success, while we present a new program without specifying inputs. Our approach to code synthesis involves feeding input/output examples to a neural network to select operations for each line of code within a given length limit. The neural network's probabilistic output allows for generating a set of possible programs to search through. This technique is similar to DeepCoder Balog et al. (2017) in attempting to guess programs. Our approach to code synthesis involves using a neural network to select operations for each line of code based on input/output examples. This method is inspired by DeepCoder Balog et al. (2017) and focuses on generating programs from a larger search space without human input. Our approach combines genetic programming with a neural network acting as a discriminator to distinguish between generated algorithms. The genetic programming element generates algorithms based on requested I/O examples for human-useful functions using a simplified C-like language. The language includes variable assignments, conditional operators, and loop operators for complex flow-control operations. Problem complexity is enhanced by allowing operations like instantiating empty arrays of given length and recursive functions. The methodology avoids using human-provided hints about source code features. Our methodology generates algorithms based on I/O examples for human-useful functions using a simplified C-like language. The set of problems includes reversing arrays, appending arrays, and summing values. The corpus consists of 200 I/O examples for input-to-output transformations. The system generates algorithms based on I/O examples for human-useful functions using a simplified C-like language. The first three examples for each problem follow specific conventions, while the remaining seven are randomly generated. The synthesis pipeline uses neural network architectures, including a synthesiser and a discriminator, to generate source code from I/O examples. The synthesiser network receives input and output values for each pair of examples and consists of 8 layers of 256 neurons with selu activation. The synthesiser neural network, connected to previous layers with selu activation, is a simplified version of the net used in Zohar & Wolf (2018). It has one output layer per line of the program to synthesise, with each layer containing neurons for all valid operations. The network uses crossentropy training loss on each line and ranks the activities of output layers' neurons to determine confidence for each option. A beam search technique is used to search over the top 1,000,000 programs returned as 'most confident'. Training requires a corpus of example programs' source code and I/O pairs. The training corpus is generated using genetic programming and discriminator training to create relevant corpora. An initial corpus of 1,000 unique programs is sampled randomly. New corpora are created from accepted ones by finding human-useful IO mappings. Accepted child corpora are retained, others are discarded. The training corpus is created by selecting programs from a parent corpus that are most similar to human-useful programs in their input/output mappings. This is done using a neural network discriminator to classify the generated programs. The discriminator neural network consists of 2 dense layers of 16 nodes and is trained to classify I/O examples as either human-useful or generated. Training continues until a threshold T k is reached. The discriminator neural network is trained to classify programs based on their human-likeness, with a threshold T k determining which programs are retained. The estimate F d ranges from 0 to 1, with programs passing if F d > 0.1. Selected programs form a new child corpus for further expansion. The child corpus is expanded by selecting programs and mutating them using roulette wheel selection. The fitness function F q determines if the mutated program is accepted based on how much it exceeds the discrimination threshold (F d - 0.1). Roulette selection is preferred over tournament selection due to bias towards programs just passing the threshold. New child corpora are created in this manner until reaching a desired total number, with no access to human-useful programs during the process. Multiple rounds of self-training data generation are conducted in this fashion. After multiple rounds of self-training data generation, solutions to I/O problems requested by users are successfully synthesized. The synthesis pipeline produces a collection of solutions and training corpora. Trained neural networks are kept as parents or final children to solve new problems not present in the initial data. Trained neural networks can be re-used for new I/O problems, processing each problem in about 0.25 seconds. A combined synthesiser neural network can be trained using all corpora. Results of both individual and combined experts are compared against competitive baselines. 200 sub-corpora are generated using this approach, repeated 20 times. Evaluation is done against two baselines, with consistent testing and repeatability ensured by using the same HU corpus and I/O examples. In this experiment, the system is evaluated against two baselines: one using randomly-generated corpora and one using genetic programming. The approach is also compared to using uniformly-sampled training data. The find rates of programs are compared against the baselines, with one baseline having the discriminator removed and the other using genetic programming. The effects of decisions made by the discriminator are explored in more detail. The genetic programming technique used a population size of 1,000 and a maximum generation count of 10,000, with tournament selection and a mutation probability of 0.15. The fitness function is based on the Euclidean distance between desired and target outputs. Results in Table 1 show discriminated sub-corpus generation produced the highest resolution rate for 200 human-useful I/O mappings. The genetic programming technique found more unique functionalities compared to the sub-corpus technique. While the sub-corpus had a higher probability of matching user-supplied IO mappings, the GP algorithm was more consistent in finding programs. The discriminator's nature may drive the system towards producing certain types of algorithms predominantly, highlighting a key area for future research. In a set of experiments, the performance of an iteratively-produced training corpus is evaluated against a baseline randomly-generated training corpus. The collated corpus discards duplicated functionalities, resulting in a training corpus of approximately 4,200 examples. Training examples are generated with randomly generated inputs for each function, and a synthesis neural network is trained on these examples. The comparative baseline training data involves sampling 5,000 programs uniformly at random and generating training examples for each program. The performance of a discriminated sub-corpus generation process is compared to a randomly generated training corpus. The discriminated corpus doubles the performance of the randomly generated corpus, producing programs more representative of human-useful behavior. Randomly generated programs lack specific features like loops or writing to all output cells. The discriminator drives towards useful training programs by emphasizing programs with uniform output writing, loops, and sensible value ranges. It generates programs with outputs dependent on inputs and learns to maximize useful features without training labels. The difference in performance between the discriminator and random selection of parents is shown in Figure 1. The discriminator's role in corpus generation is highlighted in Figure 1, showing the increase in solved programs over time. The discriminated sub-corpus consistently outperforms the non-discriminated corpus, demonstrating the value of the discriminator in generating unique functionalities. The find rates over the ancestry of sub-corpora further illustrate the system's response to the discriminator's use. The ancestry of a sub-corpus is determined by the number of parents since the starting corpus, with some experiments accepting a minimum ancestry of 5 and a maximum of 12. Certain programs are easily found without the need for a discriminator, with a find rate close to 1.0 before its use. Program find rates are shown to increase reliably past an ancestry of 1, such as the identity program. The discriminator was found to improve the identification of specific programmatic behaviors, such as loops and sequential array write operations, necessary for producing the identity function. Subsequent uses of the discriminator revealed programs like Identity Parity and 'Iterate from Start', which were rarely found without discrimination and appeared more frequently at later depths. This suggests the discriminator's ability to iterate on previous selections and distinguish between programs generated by different generations of discriminators and the human useful corpus. The discriminator's effectiveness diminishes as programs exhibit more complex behaviors, indicating a loss of guidance towards human-useful functionalities. Reasons include the genetic algorithm's failure to produce required functionalities, limitations in representing behaviors, or the inability to identify functionalities solely from input/output mappings. Further research is needed to explore these issues. The study presented a discriminator-based corpus generation technique for neural program synthesis, which generates training programs without the need for labeled data. The framework selects training examples using a discriminator, resulting in a stronger collated training corpus for training a large neural network. The approach outperforms random sampling techniques, highlighting the importance of how training corpora are generated for neural program synthesis. In future work, the study will explore the ability of discriminator-style networks to identify specific code features for problem-solving. The programs are implemented in a constrained version of C, expanding upon previous work by increasing variable limits and introducing runtime-defined array lengths and recursive function calls. The programs are limited to length 14, with maximum variable counts and array lengths specified. The programs in the study are implemented in a constrained version of C, with variable limits and runtime-defined array lengths. The search depth ranges from one to ten options, with operators described in Table 3. The beam search iteratively increases the search depth to maximize exploration. The exploration value function ranks option confidences from a neural network and normalizes them. The study involves implementing programs in a constrained version of C with variable limits and runtime-defined array lengths. The search depth ranges from one to ten options, with operators described in Table 3. The beam search iteratively increases the search depth to maximize exploration based on neural network confidence rankings. Once a set of >= 1,000,000 options with high confidence is generated, the top 1,000,000 are selected for exhaustive search. Table 4 shows outputs from 15 randomly generated programs from 3 subcorpora, with the first being the starting corpus and the second having a discriminator trained. The study involves implementing programs in a constrained version of C with variable limits and runtime-defined array lengths. The search depth ranges from one to ten options, with operators described in Table 3. The beam search iteratively increases the search depth to maximize exploration based on neural network confidence rankings. Once a set of >= 1,000,000 options with high confidence is generated, the top 1,000,000 are selected for exhaustive search. Table 4 shows outputs from 15 randomly generated programs from 3 subcorpora, with the first being the starting corpus and the second having a discriminator trained. The outputs are responses to the function being run with an input of input array = [0, 1, 2, 3, 4, 5, 6, 7] and input integer = 2. The programs in the starting corpus differ greatly from the style of program we are attempting to train the network to synthesize, with majority of returned values being 0 and little evidence of input array being read in or use of loops. The second generation corpus shows little use of input values but has outputs in more consistent ranges. The third generation corpus, ancestry=2, displays more complex programs with negative values and varied output patterns. Programs receive arrays of length 2 to 16 with values between -8 and 8, along with an integer in the same range. The discriminator behavior is illustrated through the output patterns, suggesting potential for further analysis in future research. The third generation corpus contains complex programs with negative values and varied output patterns. Programs receive arrays of length 2 to 16 with values between -8 and 8, along with an integer in the same range. Some functions do not use the input variable, but this is achieved by ignoring it. Human useful programs are listed in table 5, specified by the user as a set of IO mappings. The programs return arrays with shifted values based on the input array length. The functions in the third generation corpus manipulate input arrays and integers to generate output arrays with specific patterns. These patterns include shifting values, moving zeros to the end, sorting in ascending order, and performing arithmetic operations on the input values. The functions in the third generation corpus manipulate input arrays and integers to generate output arrays with specific patterns, such as shifting values, moving zeros to the end, sorting in ascending order, and performing arithmetic operations on the input values. Returns arrays with modified values based on specific conditions like input comparisons and arithmetic operations. The third generation corpus functions manipulate input arrays and integers to generate output arrays with specific patterns. Output values are determined based on conditions like input comparisons and arithmetic operations."
}