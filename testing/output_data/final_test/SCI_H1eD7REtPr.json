{
    "title": "H1eD7REtPr",
    "content": "AltQ, unlike DQN, does not fully fit a target Q-function at each iteration, making it unstable and inefficient. This paper explores AltQ's performance with Adam and introduces parameter restart to enhance implementation. Tested on Atari 2600 games, the proposed algorithms outperform DQN. The convergence rate is analyzed under linear function approximation, marking the first theoretical study on this topic. This is the first theoretical study on Adam-type algorithms in Q-learning, which is essential in model-free reinforcement learning. Parametric approximation of the Q-function is necessary for large or continuous state-action spaces. The combination of parametric Q-learning with deep learning, known as Deep Q-Network (DQN) learning, has been successful in various applications. The DQN algorithm, a variant of on-policy continuous learning, has achieved success in robotics locomotion. It involves a nested-loop process where the Q-function is updated in an outer loop and fitted with a neural network in an inner loop using supervised learning. The inner loop undergoes multiple iterations with an optimizer like SGD or Adam to fit the neural network to the target Q-function effectively. This contrasts with conventional Q-learning algorithms that only run one SGD step in each inner loop, updating the Q-function and fitting the target Q-function alternately. AltQ is a Q-learning algorithm with alternating updates known for its instability and weak performance. To address this, new exploration strategies and asynchronous sampling schemes have been proposed to improve its performance compared to DQN. Other alternatives, such as natural gradient propagation, have also been suggested to enhance AltQ's performance. However, these schemes require more sophisticated designs or hardware support, potentially making AltQ less advantageous compared to DQN. In this paper, the authors introduce a variant of the AltQ algorithm called AltQ-Adam, which utilizes the Adam optimization technique. They further enhance this by implementing parameter restart, resulting in AltQ-AdamR. This is the first time restart is applied in this context, aiming to achieve better and more stable performance compared to the popular DQN algorithm. The authors introduce AltQ-AdamR, a new algorithm combining AltQ-Adam with parameter restart for improved performance in RL. Experimental results show significant performance gains over DQN in Atari games and faster convergence than model-based VI in LQR problems. Theoretical analysis supports the convergence guarantee of AltQ algorithms. The existing studies focus on AltQ algorithms using simple SGD steps, but not on AltQ-Adam and AltQ-AdamR which implement Adam-type updates. The question addressed is providing convergence guarantees for these algorithms and their modified variants. Adam may not always converge, leading to the acceptance of AMSGrad as an alternative. Theoretical analysis also includes AltQ-AMSGrad and AltQ-AMSGradR variants. The proposed algorithms AltQ-AMSGrad and AltQ-AMSGradR converge to the global optimal solution under standard assumptions for Q-learning. This is the first non-asymptotic convergence guarantee on Q-learning incorporating Adam-type update and momentum restart. Additionally, AMSGrad for strongly convex optimization is studied for the first time, providing a convergence rate. The AltQ algorithms, including AltQ-AMSGrad and AltQ-AMSGradR, are proposed for Q-learning updates. These algorithms are compared to well-accepted methods like DQN and its variants, showing competitive performance. Additionally, Lu et al. (2018) introduced non-delusional Q-learning with pre-conditioned Q-networks to address value overestimation issues. In this paper, two simple and computationally efficient schemes are proposed to improve the performance of AltQ, a Q-learning algorithm. The study focuses on a Markov decision process with a large or continuous state space and action space, aiming to solve a discrete-time sequential decision problem. The optimal Q-function is defined in relation to the optimal policy \u03c0. The paper focuses on the Alternating Q-learning (AltQ) algorithm, which uses a parametric function to approximate the Q-function. AltQ performs updates by alternating between temporal target updates and parameter learning. DQN is also included for performance comparison, with a different parameter update approach. AltQ algorithm utilizes a neural network to fit the target Q-function through optimization. The update rule involves a supervised learning process with a target update. When using momentum-based optimizers like Adam, the optimizer is initialized once at the beginning and accumulates historical gradient terms. This stabilizes DQN training empirically, but lacks theoretical understanding on how the optimizer affects training with moving targets. The AltQ algorithm, when combined with Adam, can provide insights into ambiguity and inspire future work. AltQ and DQN differ in how the Q-function evolves. To improve AltQ's slow convergence and instability, incorporating Adam and restart schemes can yield better performance. A new AltQ algorithm with Adam-type update (AltQAdam) is proposed for improved performance. The AltQ algorithm is enhanced with an Adam-type update, creating AltQAdam. This algorithm updates historical gradients and squared gradients with hyper-parameters \u03b21 and \u03b22. Unlike standard Adam, AltQ does not have a fixed target for supervision, leading to noisy gradient estimations. AltQAdamR introduces a restart technique for momentum, addressing challenges in reinforcement learning. AltQ-AdamR introduces a restart technique for momentum in traditional momentum-based algorithms to mitigate estimation errors caused by incorrect historical information. By periodically resetting the momentum at restart iterations, the algorithm adjusts the trajectory and improves numerical performance, as demonstrated in Section 4 through empirical evaluation. The proposed algorithms in this section include the linear quadratic regulator (LQR) for convergence analysis under linear function approximation. The Atari 2600 game is used as a benchmark for evaluating the effectiveness of the algorithms for complex tasks. A small adjustment is made by re-scaling the loss term to stabilize the learning process. AltQ-AdamR outperforms AltQ-Adam and DQN in terms of convergence speed and variance reduction in empirical experiments. AltQ-Adam and AltQ-AdamR outperform DQN in Atari games by 50% on average, showing improved convergence speed and variance reduction. The algorithms are validated through an LQR problem, using the discrete-time algebraic Riccati equation to derive the optimal policy. Performance is evaluated at each step, with results averaged over 10 trials. All algorithms share the same random seeds and initialization, with consistent hyper-parameters. Further details are provided in Table 1. AltQ-AdamR outperforms DARE in LQR experiments, converging faster and showing better performance than traditional Q-learning with target update. In ideal conditions, DARE would be equivalent to DQN-like update with a linear function, but in practice, AltQ-AdamR performs significantly better and converges faster than DARE. AltQ-AdamR outperforms AltQ-Adam in convergence speed and variance reduction when applied to challenging tasks of deep convolutional neural network playing Atari 2600 games. The algorithms are tested on a batch of 23 games with 10 million steps of iteration. The study tested AltQ-AdamR and AltQ-Adam algorithms on 23 Atari games with 10 million steps. Results were based on OpenAI's infrastructure and hyperparameters listed in Table 2. DQN did not consistently outperform other methods across all games, as shown in Figure 2. The study compared AltQ-Adam and AltQ-AdamR algorithms with DQN on 23 Atari games. AltQ-Adam and AltQ-AdamR showed significant improvement over DQN, with AltQ-AdamR resolving variance issues. Both algorithms performed on par with DQN on most games. The convergence guarantee for AltQ-learning algorithms was characterized, focusing on linear approximation analysis. The text introduces AltQ-AMSGradR, a variant of the AltQ-learning algorithm, which resets certain parameters periodically. The theoretical analysis focuses on AltQ-AMSGrad and AltQ-AMSGradR, with technical assumptions about unbiased and bounded noisy gradients. Assumption 2 states that the equation has a unique solution, implying the existence of a bounded domain for approximation parameters. Assumption 1 is standard in the analysis of Adam-type algorithms. Assumption 2 is a key lemma in the theoretical guarantee for Q-learning with function approximation. Theoretical guarantee for Q-learning with function approximation is studied in Algorithm 3, where AltQ-AMSGrad achieves a convergence rate of O(T). This analysis differs from AMSGrad as AltQ-AMSGrad is an alternating Q-learning algorithm, while AMSGrad is an optimizer for conventional optimization. Theorem 2 provides the convergence result for AltQ-AMSGradR under certain conditions. Two accelerated AltQ algorithms are proposed, showing superior performance in various optimization problems. Adam is not the only optimization scheme, with Heavy ball and Nesterov also being popular momentum-based methods. In AltQ-learning for RL problems, methods optimize over a shorter historical horizon than AltQ-Adam, leading to less stable learning. The restart scheme shows remarkable performance, prompting further investigation into its potential. Developing an adaptive restart mechanism with a changing period could simplify hyper-parameter tuning. The linear quadratic regulator (LQR) problem is of interest in the control community, with PQL applied to both discrete-time and continuous-time problems by Lewis et al. The proposed algorithms apply PQL to discrete-time and continuous-time problems, validated through an infinite-horizon LQR problem. The optimal policy is determined by iterating the discrete-time algebraic Riccati equation. Experiments are conducted on 23 Atari games, evaluating the algorithms' performance. AltQ-Adam and AltQ-AdamR algorithms are evaluated on 23 Atari games, showing significant performance improvement in tasks like Asterix and BeamRider. However, they exhibit instability with complete failure on Amidar and Assault. Other techniques like asynchronous exploration and training with decorrelated loss are compatible with these algorithms. The AltQ-Adam and AltQ-AdamR algorithms show performance improvement in Atari games like Asterix and BeamRider but exhibit instability with complete failure on Amidar and Assault. The use of a small buffer size with experience replay can lead to biased exploration, but AltQ-AdamR with a restart scheme resolves high variance issues and provides consistent performance. Momentum restart corrects errors and stabilizes training, ensuring \u03a0 D,V 1/4 t (\u03b8 ) = \u03b8 due to Assumption 3. The update of \u03b8 t when t \u2265 2 is derived using the Cauchy-Schwarz inequality and the assumption that v t+1,i \u2265 v t,i. By taking the expectation over all samples up to time step t, we maintain the inequality. Key steps involve rearranging terms and bounding certain values to avoid errors in the proof. The proof involves bounding the sum using adjustments and Jensen's inequality. The convergence for AltQ-AMSGradR is proven by dealing with parameter restarts. Moment approximation terms are reset every r steps, and arguments similar to Equation (19) are used to show convergence in a time window without restarts. The proof for AltQ-AMSGradR's convergence involves bounding the sum using adjustments and Jensen's inequality. Convergence is shown by dealing with parameter restarts, where moment approximation terms are reset every r steps. Equation (19) is used to demonstrate convergence in a time window without restarts."
}