{
    "title": "SkeAaJrKDS",
    "content": "SAVE combines model-free Q-learning with model-based Monte-Carlo Tree Search by using a learned prior over state-action values to guide MCTS. This results in improved Q-estimates that are then updated with real experience, creating a cooperative relationship between model-free learning and model-based search. SAVE consistently achieves higher rewards with fewer training steps and performs well with small search budgets. SAVE combines model-free Q-learning with model-based Monte-Carlo Tree Search to improve performance and reduce computational costs. Model-based methods in reinforcement learning offer flexibility and efficient reasoning, with the model capturing world dynamics and the planning algorithm guiding decision-making. Recent work on model-based reinforcement learning focuses on model learning rather than planning, using generic planners like Monte-Carlo rollouts or search. However, even with a perfect model, large amounts of computation are needed in high-dimensional, sparse reward settings. For example, methods like Monte-Carlo Tree Search require hundreds or thousands of model evaluations per action during training and millions of simulations per time step at test time. In this paper, the authors propose a method called \"Search with Amortized Value Estimates\" (SAVE) to improve performance and reduce planning cost by using a neural network to preserve value estimates computed during search. This approach works well even with small search budgets, combining real experience and past search results to guide future search. SAVE uses MCTS during training to estimate Q-values and fit a Q-function, reducing computation during search. The learned Q-function guides MCTS at test time, enabling effective behavior with small search budgets in challenging environments. This approach unifies learning and search, drawing from previous work on planning in-the-loop. SAVE combines planning in-the-loop with Q-learning to address training stability issues. Previous methods have used planning to recommend actions for model-free learning, but problems can arise when learning from planned actions. By simultaneously using MCTS to strengthen an action-value function and Q-learning to strengthen MCTS, SAVE offers a more stable training approach. When learning with actions produced via planning, off-policy algorithms like Q-learning may face challenges. Planning can lead to a biased action distribution favoring good actions, hindering the propagation of information about suboptimal actions back to the Q-function. This results in poorly approximated Q-values. One solution is to use a mix of on-policy and planned actions to address this issue. In SAVE, values estimated during search are used to fit the Q-function, reflecting information about poor actions. Prior knowledge in MCTS has been leveraged in research, with successful methods using a prior policy to guide search and improve the policy based on past behavior. In SAVE, the policy is improved by explicitly encoding knowledge about value, rather than relying on visit counts like in count-based policy learning approaches. This prevents bias towards suboptimal actions in environments with sparse rewards. SAVE relies on a prior that encodes knowledge about value, avoiding bias towards poor actions. This contrasts with count-based approaches and has been tested in Section 4.1. Combining model-based and model-free experience has been a focus in RL, with works like the Dyna algorithm proposing using real experience to train a model-free policy. Recent works have explored incorporating this idea into deep architectures. Recent works in reinforcement learning have focused on combining model-based and model-free approaches. One method, SAVE, integrates planning and learning closely to avoid bias towards poor actions. This contrasts with count-based approaches and has been tested in experiments. The approach involves training the policy or value function using on-policy rollouts from the model without additional planning. Another approach, implicit planning, incorporates planner computation into neural network architecture. While SAVE is not an implicit planning method, it shares similarities with such methods. SAVE integrates planning and learning by using a Q-function as a prior during search and updating it with real experience through Q-learning and an amortization loss. SAVE leverages search by combining the TD-error from Q-learning with an amortization loss, enabling future searches to build on previous ones for stronger performance. MCTS uses a simulator to explore future states and actions, aiming to find a good action from the current state within a budget of K iterations. MCTS involves K iterations and consists of three phases: selection, expansion, and backup. The selection phase expands a search tree based on the current state using a search policy. The expansion phase executes a new action in the simulator, updating the search tree with the new state's value estimated through a state-value function or Monte-Carlo rollout. During the backup phase of MCTS, the value of the current state is used to update its parent states in the tree by estimating backed up returns. These returns are calculated using a discount factor and rewards obtained during traversal. SAVE modifies the standard MCTS by assuming prior visits to all state-action pairs, initializing their values, and using them as initial estimates. It also incorporates a prior estimate of value for each pair and updates the Q-function accordingly. SAVE modifies MCTS by incorporating Q-based prior knowledge, using the same state-action value function for estimation, and performing Bayesian inference over Q-values. This contrasts with UCT and PUCT, with Q MCTS (s, a) being returned after K iterations for action selection using epsilon-greedy. During learning, the results of the search are amortized into an updated prior Q \u03b8. An amortization loss L A encourages the distribution of Q-values output by the neural network to be similar to those estimated by MCTS. The loss is defined as the cross-entropy between the softmax of the Q-values before and after MCTS, achieving better performance than alternatives like L2. During learning, the results of the search are amortized into an updated prior Q \u03b8. An amortization loss L A encourages the distribution of Q-values output by the neural network to be similar to those estimated by MCTS. The amortization loss is combined with a Q-learning loss, where \u03b2 Q and \u03b2 A are coefficients to scale the loss terms. L Q may be any value-based loss function, such as that based on 1-step TD targets, n-step TD targets, or \u03bb-returns. SAVE was evaluated in four distinct settings, demonstrating good performance in challenging environments like the Construction domain. SAVE was tested in various environments, including a new construction task called Marble Run and Atari games. The approach showed significant improvement over Q-learning. Additionally, a hypothesis was tested using a toy environment called Tightrope to compare count-based policy learning with value-based learning. Tightrope is a deterministic MDP with 11 labeled states linked in a chain. There are 100 actions at each state, with some being terminal. Two reward settings were considered: dense rewards give 0.1 for reaching the next state, while sparse rewards give 1 only for reaching the final state. The transition function remains the same across episodes, except for the final state in the sparse reward setting. In a tabular setting, SAVE behavior was compared to UCT, PUCT, and Q-Learning agents after 500 episodes. UCT uses MCTS with a UCT search policy, PUCT is based on AlphaZero with a policy prior and state-value function, and Q-Learning performs tabular Q-learning during training and MCTS at test time. SAVE outperforms UCT, PUCT, and Q-learning in the tabular setting after 500 episodes. While UCT struggles with brute-force search, Q-learning is slow to converge, especially in sparse reward settings. PUCT improves results but struggles with small search budgets and terminal actions. In contrast, SAVE solves the Tightrope environment in most settings and reliably converges with increased search budgets. If a large search budget is available, both PUCT and SAVE may fare equally well. SAVE outperforms UCT, PUCT, and Q-learning in the tabular setting after 500 episodes, especially with small search budgets. In the Tightrope environment, SAVE shows reliable performance with increased search budgets. Additionally, SAVE is evaluated in Construction tasks involving stacking blocks and achieving functional objectives. In Construction tasks, the goal is to cover obstacles without touching them, which is challenging for both model-free and traditional search methods due to the complex dynamics and large branching factors. The simulator is expensive to query, limiting search budgets to 10-20. SAVE, using an agent architecture similar to Bapst et al. (2019), outperforms UCT, PUCT, and Q-learning in tabular settings, especially with small search budgets. It also shows reliable performance in the Tightrope environment and Construction tasks involving stacking blocks. In Construction tasks, the goal is to cover obstacles without touching them, which is challenging due to complex dynamics and large branching factors. SAVE outperforms UCT, PUCT, and Q-learning in tabular settings, especially with small search budgets. The poor performance of UCT highlights the need for prior knowledge to manage the branching factor. Model-free Q-learning improves performance, but simply performing search on learned Q-values only results in small gains. The performance of SAVE without amortization loss highlights the issue discussed in Section 2.1. Without amortization loss, SAVE's Q-learning component only learns about selected actions via search, leading to poorly approximated Q-values. Using search at test time can partially compensate, but large search budgets can worsen performance. Incorporating amortization loss during training results in higher rewards across tasks. Ablation experiments were conducted on the Covering task to compare different versions of SAVE. Results show that the choices made in SAVE lead to the highest performance levels, with the softmax cross entropy loss playing a crucial role. The softmax cross entropy loss is crucial for SAVE's performance due to noisy Q MCTS estimates and the ability to focus on relative action values. Comparisons with a policy-based PUCT agent showed SAVE's superiority on harder tasks like Covering. This highlights the limitations of count-based policy training and small search budgets. SAVE achieves near-ceiling performance on Construction tasks. Marble Run is a new challenging task developed by SAVE, involving stacking blocks to guide a marble to a goal while avoiding obstacles. The agent receives a reward of one if it solves the scene, and zero otherwise. The task is more complex with sparser rewards and a variety of block shapes to choose from. The Marble Run task requires agents to stack blocks to guide a marble to a goal while avoiding obstacles. An adaptive curriculum is used for training, allowing agents to progress to the next difficulty level only after solving at least 50% of the scenes at the current level. Results show that SAVE outperforms Q-learning, reaching higher difficulty levels and achieving higher rewards. Q-learning becomes unstable and collapses around difficulty 4-5, while SAVE remains stable. SAVE outperforms Q-learning in the Marble Run task by building structures that allow the marble to reach raised targets and span multiple obstacles. With a search budget of 10, SAVE sees 10 times more transitions than a model-free agent, resulting in better performance despite the model-free agent seeing more episodes. This positive interaction between planned and model-free experience is also observed in other Construction tasks. SAVE, a method for combining model-free interaction with experience generated from planned actions and values estimated during search, outperforms or equals a controlled version of R2D2 in Atari games. Particularly high performance is seen in Frostbite, Alien, and Zaxxon. SAVE is a method that combines model-free Q-learning with MCTS to infer Q-values during training, using real experience and estimated Q-values to fit a Q-function. This approach improves search performance and achieves high rewards with small search budgets in Tightrope, Construction, Marble Run, and Atari games. Future searches with information from prior searches and real experience offer advantages for model-based RL. Combining Q-values from both sources should consider the quality or confidence of the estimates. Count-based policy methods leverage confidence based on visit counts, but relying solely on visit counts can lead to poor performance with small search budgets. A key future direction is to combine the computation of value and reliability for improved performance. Encoding confidence estimates into Q-values may also benefit applying SAVE to settings with learned models. SAVE can be helpful for settings with learned models by attenuating search-estimated Q-values to the Q-prior in cases of insufficient exploration or high model error. Amortizing Q-estimates during MCTS leads to higher performance levels compared to model-free approaches, with less computation needed than other model-based methods. This approach ensures valuable computation during search is preserved, enhancing the integration of planning and learning for more powerful hybrid approaches. In experiments, a distributed training setup with 1 GPU learner and 64 CPU actors was used. TensorFlow and Sonnet were implemented for the setup, with Adam optimizer for gradient descent. Q-learning with experience replay and target network was employed, controlling the replay ratio. Parameters included batch size of 16, learning rate of 0.0002, and replay size of 4000 transitions. The SAVE agent was implemented with epsilon-greedy exploration and annealed average values over episodes. The algorithm can be used in any Q-learning setup. In our experiments, we use the distributed setup described in Section A.1. When performing epsilon-greedy exploration, we choose actions based on the highest value of Q MCTS. The PUCT search policy is based on that described by Silver et al. (2017a) and we use a UTC exploration constant of c = 2. During training, actions are chosen based on Equation 1 with an exploration constant c, prior policy \u03c0(s, a), and total number of times action a was taken from state s at iteration k. Dirichlet noise is added to the prior policy with \u03b7 \u223c Dir(1/n actions). At test time, the action with the maximum visit count is selected. The PUCT agent is trained using separate policy and value heads with a combined loss (Equation 6) based on the Monte-Carlo return observed from state s. Fixed values of \u03b2 Q = 0.5 and \u03b2 A = 0.5 are used in all experiments with PUCT. During training, the PUCT agent includes episodic Monte-Carlo returns and policies in the replay buffer for learning. It does not use -greedy exploration due to Dirichlet noise enabling sufficient exploration. Various hyperparameter settings and PUCT agent variants were tested, including using a 1-step TD error for learning values. Changes like reducing replay ratio and size did not improve results. Different Dirichlet noise settings were also tried, but lower values led to too little exploration and higher values to too much. The Tightrope environment consists of 11 states connected in a chain, each with 100 actions. M % of actions terminate the episode, while the rest transition to the next state. States are represented by a vector of 50 random values. In sparse reward setting, a final state is randomly selected for the agent to reach, receiving a reward of 1 upon reaching it. In the dense reward setting, the final state is the last state in the chain. The agent receives a reward of 0.1 upon reaching it and the episode terminates. Training involves executing the agent in the environment until the episode ends, followed by a learning step using the experience gained. This process is repeated for a set number of episodes (e.g., 500). After training for 500 episodes, each agent is executed in the environment 100 times to compute the average reward. Tabular Q-learning initializes state-action values to zero and uses epsilon-greedy exploration with a replay buffer. Q-values are updated after each episode using a learning rate of 0.01. At test time, the Q-learning agent uses MCTS similar to SAVE, which also initializes state-action values to zero. During search, values are retrieved from the table. Tabular PUCT uses two tables with state values initialized to zero and action probabilities initialized to a uniform distribution. During search, action probabilities are used in the PUCT term, while state values are used for bootstrapping. Search proceeds as described in Section A.3. During learning, \u03c0 MCTS is copied back into the action probability table, and the value at episode t is calculated accordingly. The value at episode t is determined by the return obtained after visiting state s during episode t \u2212 1. In experiments, \u03b1 = 0.5 was used. Q-learning was tested but showed similar performance to Monte-Carlo returns. The UCT agent estimates V(s) at unexplored nodes using Monte-Carlo rollout. Unvisited actions were initialized to zero. For Tightrope, default Q-values are optimal because all possible rewards are \u2265 0. Once a non-zero reward action is found, it is best to stick with it. The UCT agent selects the final action with the maximum estimated value. Experimenting with selecting uniformly at random from unexplored actions improved performance slightly. In Tightrope, default Q-values are optimal due to all possible rewards being \u2265 0. Effect size is small: in the dense setting with M = 95%, median reward is 0.08 with thresholding action selection policy compared to 0.07 with max of visited actions. Agents used shared MLP torso with two layers of size 64 and ReLU activations. Q-values predicted using MLP head with two layers of size 64 and ReLU activations. Policy in PUCT agent predicted using same network architecture as Q-value head. State values in PUCT agent predicted using separate MLP head with two layers of size 64 and ReLU activations. Weights were initialized using the default scheme in Sonnet. Both SAVE and PUCT agents used loss coefficients of \u03b2 Q = 0.5 and \u03b2 A = 0.5. Results were reported after 1e6 episodes in Tightrope with 95% terminal actions. PUCT performed well for larger budgets but struggled with small budgets, while SAVE performed well in all experiments, even with small budgets. For SAVE, \u03b2 Q was annealed from 1 to 0.1 and \u03b2 PI from 0 to 4.5. The agent details for SAVE involved annealing \u03b2 Q from 1 to 0.1 and \u03b2 PI from 0 to 4.5 over 5e4 episodes. This allowed the agent to rely more on Q-learning early on to build a good Q-value prior, and then more on MCTS later in training. The Q-Learning agent used pure Q-learning during training with no search, but could perform MCTS at test time. The SAVE without an amortization loss is equivalent to the GN-DQN-MCTS agent. The curr_chunk discusses different variations of the SAVE agent, including UCT, SAVE with L2 loss, and SAVE without Q-learning. These variations involve using different amortization losses and annealing beta values over episodes to improve training and performance. The use of L2 loss is highlighted as essential for ensuring real Q-values. The Q-values in the curr_chunk do not have grounding in the actual scale of rewards. Experimentation showed slightly worse performance with cross-entropy loss and no Q-learning compared to using L2 loss and no Q-learning. SAVE with PUCT uses a different search policy involving softmax over actions and Dirchlet noise for exploration. Action selection is based on epsilon-greedy rather than visit counts. The curr_chunk discusses the use of a graph network architecture for processing graphs representing scenes in a task involving object-based actions. Results on Construction tasks are reported, including Connecting, Covering, and Covering Hard tasks. The agents in the study have shown high performance levels. The agents in Bapst et al. (2019) reached ceiling performance on the Silhouette task. They used 10 MCTS simulations during training and were evaluated on 0 to 50 simulations at test time. Results were reported after 1e6 episodes, with detailed learning progress and final performances shown in figures. All agents were evaluated on the hardest level of difficulty for the task they were trained on. Incorporating search during training provides main gains in performance, with L2 loss resulting in higher variance and lower average performance compared to cross-entropy loss. The L2 loss encourages exact matching of Q-values, leading to noisy results with a search budget of 10, while cross-entropy loss allows for exploitation of information acquired during search. The agent with L2 amortization loss performs worse than the agent with no amortization loss when using a search budget of 10. Matching Q-values too closely during search can harm performance. Q-learning focuses on refining estimates for high-valued actions due to search avoiding poor actions. Softmax cross entropy loss ensures lower values for low-valued actions but does not force exact values. By using cross entropy instead of L2 loss, the neural network can prioritize representing high-valued actions over low-valued ones. Removing Q-learning loss improves on-policy learning and prevents staleness in Q-values estimated during search. Without Q-learning, the SAVE agent's performance suffers significantly. Without the Q-learning loss, the learning algorithm becomes more on-policy, leading to issues with performance. Comparing to a variant using prior knowledge, initializing Q-values with small search budgets results in values close to zero, regressing the Q-function towards a uniform distribution. By initializing the Q-values with UCT rather than PUCT, we can effectively reuse knowledge across multiple searches. SAVE uses an epsilon-greedy exploration strategy for selecting actions, which was found to work the best compared to other strategies like UCB or categorical sampling. Using epsilon-greedy for exploration works best compared to UCB and categorical methods. It ensures all actions are well approximated, preventing catastrophic forgetting and unreliable Q-values for unexplored actions. This approach consistently explores all actions, maintaining accuracy in Q-values. Using a separate state-value function for bootstrapping, like AlphaZero, can improve data efficiency. SAVE sees 10 times more transitions than a model-free agent with the same environment interactions. Comparing SAVE to Q-learning on the Covering task shows SAVE converges to higher rewards with the same data. Similar results are found in the Marble Run environment. In the Marble Run environment, the agent interacts with blue blocks, a goal, a marble, and obstacles to avoid. The obstacles are sampled from a tessellation with random sequences of blocks. The Marble Run environment involves interacting with blue blocks, a goal, a marble, and obstacles sampled from a tessellation. The sampling process includes setting the vertical positions of the goal and marble, sampling horizontal distances, and obstacles according to the level. Probabilities are assigned to remaining objects after removing those too close to the goal or obstacles. Probabilities are assigned to remaining objects in the tessellation to prevent unsolvable scenes. Criteria with different weights are used to pick objects, such as selecting objects close to the marble or goal horizontally. To prevent unsolvable scenes, probabilities are assigned to remaining objects in the tessellation. Different criteria with varying weights are used to select objects, like choosing ones close to the marble or goal horizontally. Additionally, a curriculum is used to sample scenes of increasing difficulty during training and testing, ensuring diversity in the episodes generated. To train agents effectively in a task with complex and sparse rewards, an adaptive curriculum is used to gradually increase the difficulty level. This curriculum involves binning past episode results based on various scene properties like target height, distance between marble and goal, number and height of obstacles. The agent must solve 50% of scenes in each bin before progressing to the next level. The task involves block placement and settlement phases with sticky blocks receiving negative rewards. The physics simulation runs until blocks settle, affecting the position of previously placed blocks. The physics simulation involves the marble colliding with objects for a maximum of 80s. If the marble reaches the goal, the task is solved. After the marble dynamics phase, the marble and blocks are moved back to their original positions to prevent manipulation. The task setup is similar to Bapst et al. (2019). The task involves construction with one-hot encoding of object shapes and block positions. The agent does not observe the marble's dynamics directly. Episode termination conditions include block overlap and obstacle contact. Future work could incorporate marble dynamics feedback into the agent's learning. During the block settlement phase, the marble's position overlaps with a block. Obstacle collisions during the marble dynamics phase do not end the episode, allowing the agent to correct by adding blocks to re-route the marble. Checkpoints of weights achieving the highest reward on the highest curriculum level are used for evaluation. Training performance details at each difficulty level are shown in Figure D.2. The Q-learning agent becomes unstable and crashes around difficulty level 4-5, while the SAVE agent stays stable and improves. The Q-learning agent fails to reach difficulty level 5 or 6, but the SAVE agent can handle harder levels with fewer learning steps. Results on Atari show the SAVE agent outperforming the controlled version of R2D2. SAVE was evaluated on 14 Atari games in the Arcade Learning Environment, showing performance within 5% of the best score. It was implemented on top of the R2D2 agent and outperformed the controlled version on harder levels. The R2D2 agent was enhanced with SAVE, optimizing the combined loss function instead of the TD loss. Hyperparameters were set with a search budget of 10, \u03b2 Q = 1, \u03b2 A = 10. Little tuning was done, with \u03b2 A = 10 performing slightly better. Increasing the number of actors from 256 to 1024 was necessary due to the slower rate of transition tuple additions to the replay buffer when running MCTS. In the experiment, the number of actors was increased from 256 to 1024, and the actor parameter update interval was changed from 400 to 40 steps. The learning curves and final performance of the Baseline, Controlled, and SAVE agents were compared, with SAVE outperforming both the Controlled agent and the original R2D2 baseline in most games. The SAVE agent outperformed the Controlled agent and the original R2D2 baseline in most games, showing big improvements in strategic games like Ms. Pacman and also in action games. This suggests that model-based methods like SAVE can be useful even in domains that do not require as much longterm reasoning."
}