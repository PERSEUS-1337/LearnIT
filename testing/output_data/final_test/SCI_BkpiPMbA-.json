{
    "title": "BkpiPMbA-",
    "content": "Deep neural networks are vulnerable to adversarial examples, carefully crafted instances that cause prediction errors. Research has focused on local neighborhoods in the input space, but this paper argues for considering larger neighborhoods to better understand the relationship between adversarial examples and DNN models. The OPTMARGIN attack generates robust adversarial examples that evade defenses focusing on small perturbations. Analyzing a larger neighborhood around input instances reveals properties of surrounding decision boundaries. The study examines decision boundaries around adversarial examples, finding differences compared to benign examples. The OPTMARGIN attack generates robust adversarial examples that do not mimic benign examples convincingly. Previous research has focused on local neighborhoods, but this study suggests considering larger neighborhoods to understand the relationship between adversarial examples and deep neural network models. In this paper, the authors argue that analyzing larger neighborhoods around input instances is crucial for understanding adversarial examples in high-dimensional datasets. They introduce the OPTMARGIN attack method to generate robust adversarial examples that can evade small perturbation defenses. Additionally, they propose a technique to analyze decision boundaries in the model's input space. The authors introduce a technique to analyze decision boundaries around input instances, specifically focusing on robust OPTMARGIN adversarial examples. They find that while OPTMARGIN examples can fool region classification, their decision boundaries differ from benign examples in terms of distances to adjacent classes. Additionally, a classifier is trained to differentiate between OPTMARGIN and benign examples with 90.4% accuracy, surpassing region classification's limitations. In this study, the authors introduce OPTMARGIN, a new attack that evades region classification systems with low-distortion adversarial examples. They analyze decision boundaries around input instances to explain the effectiveness of OPTMARGIN adversarial examples and demonstrate the expressiveness of decision boundary information in classifying different input instances. The code used for the study is available at https://github.com/sunblaze-ucb/decision-boundaries. In this study, the authors use MNIST and CIFAR-10 datasets for experiments on adversarial examples. Adversarial examples are perturbed versions of correctly classified instances that are misclassified. The focus is on untargeted attacks, where examples are misclassified as any class other than the correct one. In this paper, the authors quantify the distortion of adversarial examples using the root-meansquare (RMS) distance metric. They discuss defenses against adversarial examples, including adversarial training with examples generated by projected gradient descent (PGD) and region classification. Adversarial training involves substituting a portion of training examples with adversarial examples generated using PGD. In this paper, experiments are conducted on two models trained from one architecture for MNIST and CIFAR-10 datasets. The models include a defended model trained with PGD adversarial training and an undefended model trained with normal examples. The study focuses on the effect of PGD adversarial training on decision regions of the models. The authors also present a concrete example demonstrating evasion attacks on an adversarial example defense by limiting the analysis of a neighborhood to a small ball. Region classification is proposed as a defense against adversarial examples, where the majority prediction on slightly perturbed versions of an input is taken. Region classification is a defense strategy that considers majority predictions on perturbed input versions to defend against adversarial attacks. Cao & Gong's study shows that this approach can effectively counter low-distortion adversarial examples. They also highlight that attacks robust to region classification, like Carlini & Wagner's high-confidence attack, exhibit higher distortion and can be detected by other methods. The OPTMARGIN attack introduced in the study generates low-distortion adversarial examples resilient to small perturbations, aligning with the principles of region classification. The study introduces a surrogate model of the region classifier, forming an ensemble of models to generate adversarial examples with minimal distortion. The attack utilizes optimization techniques to fool the ensemble while considering class weights and confidence margins in the loss function. The study extends Carlini & Wagner's L2 attack by using an objective function with multiple loss terms. It utilizes an ensemble of 20 classifiers with random orthogonal vectors to generate adversarial examples. The attack is stable due to fixed values of the vectors used in optimization. The technique was previously employed in Carlini & Wagner's attack on Feinman et al.'s defense. The OPTMARGIN attack is compared with other attacks like Carlini & Wagner's L2 attack with low and high confidence levels. Results show higher distortion in OPTMARGIN examples compared to OPTBRITTLE, but lower distortion than OPTSTRONG examples. The attack is effective in generating adversarial examples with visible distortion but still distinctly visible original classes. The OPTMARGIN attack has higher distortion compared to OPTBRITTLE but lower distortion than OPTSTRONG examples. OPTSTRONG attacks are not considered due to low success rate and high distortion. The effectiveness of the OPTMARGIN attack is evaluated by testing generated examples on Cao & Gong's region classification defense. In experiments testing attacks on Cao & Gong's region classification defense, hypercube radii were determined for different models. The accuracy of region classification and point classification on examples from various attacks was compared, with more effective attacks resulting in lower accuracy. The OPTMARGIN attack successfully evades region classification and point classification, achieving high attack success rates with minimal distortion. Using multiple models in an ensemble increases computational cost, with optimization based on Carlini & Wagner's method. In the OPTMARGIN attack, 4 binary search steps are used with up to 1,000 optimization iterations each. The attack takes around 8 minutes per image on a GeForce GTX 1080. By examining decision boundaries in the model's input space, we analyze the neighborhood around input instances to distinguish between benign and adversarial examples. By examining decision boundaries in the model's input space, the distance to the nearest boundary in various directions and adjacent decision regions' classes is considered. Information on the sizes and shapes of a model's decision regions is gathered by estimating the distance to a decision boundary in random directions from a given input point. The model's prediction on perturbed inputs along each direction is used to estimate the distance to a decision boundary. The search for decision boundaries involves examining the distance in various directions and recording predicted classes of adjacent regions. This search is conducted over random orthogonal directions for CIFAR-10 and MNIST datasets. The decision boundary distances for benign and adversarial examples are compared using different attack methods on models trained normally and with adversarial examples. The boundary distance plots show differences between adversarial and benign examples generated by different attack methods. The OPTMARGIN attack creates a margin of robustness in random directions, making classification challenging due to neighboring incorrect decision regions. The decision boundary distances of images are summarized by looking at minimum and median distances across random directions. The plots show why OPTMARGIN and FGSM examples are more robust to perturbations than OPTBRITTLE attacks. However, they are still less robust than benign examples to random noise. On MNIST, simple thresholds do not accurately separate benign examples from OPTMARGIN examples. PGD adversarial training on MNIST architecture results in decision boundaries closer to benign examples, reducing robustness to random perturbations. In CIFAR-10, boundaries in PGD adversarially trained model are farther from benign examples. The effect of PGD adversarial training on robustness to random perturbations varies. Adversarial examples tend to lead to a boundary adjacent to a single class. The purity of top k classes around an input image is computed as the largest cumulative fraction of random directions encountering a boundary adjacent to one of k classes. Purity scores are higher for OPTBRITTLE adversarial examples compared to benign examples, especially in CIFAR-10. Region classification benefits from high purity of the top 1 class, making it harder to distinguish OPTMARGIN and FGSM adversarial examples from benign ones. Cao & Gong's defense considers a fixed hypercube region, limiting its effectiveness. In response to the limitations of a defense system based on a fixed hypercube region, a new approach is proposed to differentiate between adversarial examples and benign inputs by analyzing the distribution of distances to decision boundaries in randomly chosen directions. This method aims to provide more information than previous approaches, such as Cao & Gong's, by considering the neighborhood of an input to determine its adversarial nature. The text discusses the design of a classifier to differentiate between adversarial examples and benign inputs by analyzing boundary information from random directions. A neural network is constructed to process boundary distances and classify them using convolutional and fully connected layers, resulting in binary classification. The classifier uses rectified linear units for activation and dropout during training. Training is done with an Adam optimizer, batch size of 128, and learning rate of 0.001. Results show lower success with FGSM attacks compared to other methods. The boundary classification experiment on FGSM examples in Appendix C shows false positive and false negative rates for the decision boundary classifier. The classifier achieves high accuracy on the attacks studied in the paper, suggesting that the current best attack, OPTMARGIN, does not accurately mimic the distribution of decision boundary distances and adjacent classes. On MNIST, the model with normal training had better accuracy, while the model with PGD adversarial training had better accuracy on CIFAR-10. The experiment focused on decision boundary information around inputs, with models showing better accuracy on CIFAR-10 with PGD adversarial training. The iterative approach was costly, taking around 70 seconds per image for decision boundary computation. The experiment focused on decision boundary information around inputs, with models showing better accuracy on CIFAR-10 with PGD adversarial training. It took around 70 seconds per image to compute decision boundary information for 1,000 directions on a GeForce GTX 1080. The time varies depending on the image, and collecting information for OPTBRITTLE examples was faster. Collecting information in fewer directions can save time as long as the samples adequately capture the distribution of distances and adjacent classes. The training phase took about 1 minute for each model and training set configuration. Running the decision boundary classifier on the information is fast compared to training and boundary collection. Examining large neighborhoods around a given input in input space was considered beneficial. We demonstrated an effective OPTMARGIN attack against a region classification defense, analyzing decision boundaries around examples generated by this attack. Differences between robust adversarial examples and benign examples were revealed, with FGSM creating fewer successful adversarial examples, especially for adversarially trained models. Our experiments showed higher distortion levels compared to OPTMARGIN examples. In experiments on a small subset of ImageNet, a classifier was trained on successful FGSM adversarial examples for normal models. The experiments used a hypercube with radius 0.02 for region classification and \u03b5 = 8/255 for OPTMARGIN. The set was divided into 350 images for training and 100 images for testing. The results of our CIFAR-10 experiments are more preliminary compared to ImageNet. The effectiveness of OPTMARGIN and other attacks on Cao & Gong's region classification defense and decision boundary classification are summarized in TAB8. OPTMARGIN has the highest attack success rate under region classification, while our decision boundary classifier accurately classifies OPTBRITTLE and OPTMARGIN adversarial examples. FGSM examples have higher distortion and lower success rates."
}