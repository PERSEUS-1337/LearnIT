{
    "title": "rJxgknCcK7",
    "content": "A continuous-time invertible generative model is proposed in this paper, utilizing Hutchinson\u2019s trace estimator for unbiased density estimation. This model allows for unrestricted neural network architectures and achieves state-of-the-art results in high-dimensional density estimation, image generation, and variational inference. The approach involves mapping points from a simple distribution to a complex distribution through an invertible neural network, enabling efficient sampling and exact likelihood methods. Reversible generative models like NICE, Real NVP, and Glow use invertible neural networks to transform samples from a base distribution. These models can be trained using the change of variables formula, allowing for complex normalized distributions to be specified implicitly. However, restrictions on architectures are needed to avoid high computation costs. The log density of the transformed variable follows a specific formula involving the Jacobian of the invertible function. The log determinant computation in neural network architectures has a time cost of O(D^3). Different approaches have been developed to make computing the Jacobian's determinant more manageable, including normalizing flows and autoregressive transformations. These models are useful for various tasks such as density estimation and approximate posteriors for variational inference. Reversible generative models use partitioned transformations to make the determinant of the Jacobian cheap to compute and enable efficient density estimation and sampling. This method allows the use of convolutional architectures for image data. Generative adversarial networks (GANs) use large neural networks for sample transformation. Generative adversarial networks (GANs) use large neural networks to transform samples from a fixed base distribution. Autoregressive models directly specify the joint distribution using conditional distributions. Variational autoencoders use an unrestricted architecture to specify conditional likelihood but provide a stochastic lower bound on the marginal likelihood. Chen et al. (2018) define a generative model for data by replacing the warping function with an integral of continuous-time dynamics. Continous Normalizing Flows (CNF) involve sampling from a base distribution and solving an ODE to obtain data samples. The change in log-density follows a differential equation, and we can compute total change by integrating across time. By solving the combined initial value problem, we can compute log p(x) under the model. The existence and uniqueness of the solution require specific conditions on the parametric function f. CNFs are trained to maximize an objective involving the solution to an initial value problem with dynamics parameterized by \u03b8. The derivative of a scalar loss function on the solution takes the form of another initial value problem, known as the adjoint state of the ODE. Black-box ODE solvers are used to compute z(t 1 ) and then to compute another value with the initial value \u2202L /\u2202z(t1). This approach is analogous to backpropagation and can be combined with gradient-based optimization to fit the parameters \u03b8. Switching from discrete-time dynamics to continuous-time dynamics in normalizing flows reduces computational bottlenecks by introducing a numerical ODE solver. This allows for more expressive architectures, such as planar continuous normalizing flows with multiple hidden units. An unbiased estimate of log-density with O(D) cost enables the use of unrestricted neural network architectures. Switching to continuous-time dynamics in normalizing flows reduces computational bottlenecks by using an ODE solver. Hutchinson's trace estimator can provide an unbiased estimate of the trace of a matrix using a noise vector. This allows for more expressive architectures in neural networks. Typical choices of p() are a standard Gaussian or Rademacher distribution. Bottlenecks in the dynamics network can be reduced by using the cyclic property of trace. The bottleneck trick can help reduce the variance of the trace estimator, but it limits model capacity. This trick can be useful when a bottleneck is used, as shown in ablation studies. FFJORD is a scalable and reversible generative model with an unconstrained Jacobian. The cost of evaluating f is O(DH), and the likelihood computation cost is reduced to O((DH + D^2)L) for CNFs. With FFJORD, this cost reduces further. FFJORD is a scalable generative model with an unconstrained Jacobian. Experiments were conducted using GPU-based ODE-solvers and the adjoint method for backpropagation. The RungeKutta 4(5) algorithm was used to solve the ODEs with low tolerance for negligible numerical error. FFJORD is a scalable generative model with an unconstrained Jacobian. Hutchinson's trace estimator was used during training, and the exact trace for test results. In experiments, the dynamics are defined by a neural network f taking input z(t) and time t. Simply concatenating t onto z(t) at each layer input worked well. Training on 2D data visualized the model and learned dynamics. By warping an isotropic Gaussian, FFJORD fits multi-modal distributions. The ODE solver is evaluated roughly 70-100 times on all datasets. Comparisons were made against a Glow model with 100 discrete evaluations. Comparing FFJORD and Glow models, Glow learns to stretch unimodal base distribution into multiple modes but struggles with low probability areas between disconnected regions. In contrast, FFJORD can model disconnected modes and discontinuous density functions effectively. FFJORD can model data on MNIST as effectively as Glow and Real NVP using a single flow defined by a neural network, while Glow and Real NVP require composing many flows to achieve similar results. FFJORD achieves comparable performance to Glow on CIFAR10 and better performance on MNIST using a single flow neural network, while Glow and Real NVP require composing multiple flows. FFJORD uses less than 2% of the parameters of Glow and uses a fixed Gaussian base distribution. The adjoint method allows for larger batch sizes compared to competing methods. More details can be found in TAB4 and Figure 3. FFJORD achieves comparable performance to Glow on CIFAR10 and better performance on MNIST using a single flow neural network. The encoder network outputs a low-rank update to a global weight matrix and an input-dependent bias vector. Neural network layers defining the dynamics inside FFJORD take a specific form. More details on model architectures and experimental setup can be found in Appendix B.2. FFJORD outperforms all other normalizing flows on every dataset tested. A series of ablation experiments were conducted to understand the proposed model, with training losses plotted on MNIST using an encoder-decoder architecture. The computational cost of integrating the change of variables is O(DH L). The change of variables formula in the context of FFJORD is O(DH L), where D is the data dimensionality, H is the hidden state size, and L is the number of function evaluations. The NFE used by the adaptive ODE solver is independent of data dimension, as shown in Figure 5. Training VAEs with FFJORD flows reveals that NFE increases throughout training but converges to the same value regardless of D, suggesting it depends on the distribution complexity rather than data dimensionality. The distribution complexity affects the number of function evaluations required for FFJORD flows. Comparing single-scale and multiscale FFJORD models on the MNIST dataset shows that while the single-scale model uses fewer function evaluations, it does not achieve the same performance as the multiscale model. The number of function evaluations needed for integrating dynamics is not fixed and depends on data, model architecture, and parameters. It tends to increase during training, even with constant memory due to the adjoint method. Regularization methods like weight decay and spectral normalization can reduce this quantity but may slightly impact performance. General-purpose ODE solvers limit us to non-stiff differential equations that can be efficiently solved, as solvers for stiff dynamics require evaluating f many more times for the same error. FFJORD is a reversible generative model using continuous-time dynamics and an unrestricted neural network for parameterization. It computes exact log-likelihoods and can be sampled efficiently. The model stands out for its flexibility compared to other methods, achieving comparable or better performance in density estimation and variational inference. Further exploration and improvement of this method are encouraged. FFJORD is a reversible generative model with continuous-time dynamics and an unrestricted neural network for parameterization. It is slower to evaluate than other reversible models like Real NVP or Glow, so efforts are focused on reducing the number of function evaluations without compromising predictive performance. Advancements in this area are crucial for scaling the method to higher-dimensional datasets. Samples from FFJORD models trained on MNIST and CIFAR10 can be seen in Figure 7, with a grid-search performed on tabular datasets for different network architectures. The best performing models with hidden dimensions of 5, 10, or 20 times the data dimension were tested using tanh and softplus nonlinearities. Two model architectures were experimented on image datasets, a single flow with an encoder-decoder style and a multiscale architecture. The encoder-decoder architecture fit MNIST well but struggled with more complex datasets like CIFAR10. The MNIST architecture had four convolutional layers with 64 \u2192 64 \u2192 128 \u2192 128 filters and transpose-convolutional layers with softplus activation function. The multiscale architectures in the model involve stacking multiple flows with \"squeeze\" operations to down-sample images and increase channels. Each flow consists of 3 convolutional layers with 64 filters and a softplus activation function. The models were trained using the Adam optimizer for 500 epochs with a learning rate decay from .001 to .0001 after 250 epochs. Training was done on six GPUs and completed in approximately five days, following the experimental procedure of Berg et al. (2018). The experimental procedure followed Berg et al. (2018) with a 7-layer encoder and decoder, learning rate of .001, Adam optimizer, batch size of 100, and early stopping after 100 epochs. Grid-search was done for neural network architectures in FFJORD dynamics. Models were trained on a single GPU for four hours to three days, with the best performing models listed in TAB11. In reproducing Glow, comparable results to Real NVP were achieved by removing invertible fully connected layers. ODE solvers have inherent errors in their outputs, with adaptive solvers adjusting step-size to reduce error. It is crucial to consider solver error when using them for density estimation to avoid biased training objectives and divergent dynamics. When training models, it is important to consider solver error to avoid biased objectives and divergent dynamics. By setting tolerance values for numerical integration, we can control the error, with a tolerance of 10^-5 providing reasonable performance. Tabular experiments use atol=10^-8 and rtol=10^-6 for accuracy."
}