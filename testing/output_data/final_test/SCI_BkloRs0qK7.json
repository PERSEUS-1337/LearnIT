{
    "title": "BkloRs0qK7",
    "content": "A large-scale empirical study of catastrophic forgetting in modern Deep Neural Network models for sequential learning is presented. A new experimental protocol is proposed to address typical constraints in application scenarios. The study evaluates CF behavior on a large number of visual classification datasets, constructing Sequential Learning Tasks aligned with previous works on CF. Our study shows that no model can completely avoid catastrophic forgetting in Deep Neural Networks during sequential learning tasks. We discuss potential solutions for this issue, particularly focusing on the EWC and IMM models. Sequential Learning Tasks (SLTs) are common in real-world applications where DNNs need to update their capabilities over time, leading to the problem of catastrophic forgetting. The study highlights the issue of catastrophic forgetting in Deep Neural Networks during sequential learning tasks, even with models like EWC and IMM. SLTs involve updating DNN capabilities over time, leading to significant knowledge loss from previous subtasks after training on current subtasks. Recent comparisons of DNN approaches to mitigate CF have been conducted. Recent comparisons of different DNN approaches to prevent catastrophic forgetting (CF) have been conducted. Principal methods include ensemble techniques, dual-memory systems, and regularization approaches. Some methods suggest using Dropout or adding a term to the energy function to protect important weights from previous sub-tasks. Other approaches focus on enforcing sparsity of neural activities within a layer. Most methods are evaluated on a limited number of datasets. Most methods for preventing catastrophic forgetting in DNNs are evaluated on a few datasets like MNIST and CIFAR10. Model selection is often based on a \"prescient\" evaluation, ignoring causality. Storage of data from previous sub-tasks is a technical challenge. DNNs can be retrained without storing training data from previous sub-tasks, but regularization parameters control retention of previous knowledge. Different studies use parameters like \u03bb and \u03b1 for this purpose. Cross-validation for regularization parameters is done in BID2, but requires storing all previous training data. Progress has been made in preventing catastrophic forgetting in DNNs, but there are shortcomings in applied scenarios that need to be addressed. The evaluation strategy for testing CF in DNNs must consider formal constraints such as class-incremental learning and retention of performance on previously trained classes. Additional conditions may apply for DNNs operating in embedded devices or autonomous robots, including low memory footprint and avoiding the use of data from future sub-tasks for model selection. The original contributions of our work include proposing a training and evaluation paradigm for incremental learning in DNNs that enforces typical application constraints. This paradigm takes into account application constraints that lead to different conclusions about CF compared to other recent studies. In this study, various DNN approaches were examined for their incremental learning capacity using a large number of classification datasets. It was found that all models experienced catastrophic forgetting or violated application constraints. The use of \"permuted\" SLTs for testing CF was cautioned against. The study did not propose a method to avoid CF but instead focused on measuring the effect using a novel approach with visual classification datasets. The experimental protocol enforces application constraints for DNN models tested using TensorFlow. Models include a fully-connected DNN, a CNN optimized for image classification, Elastic Weight Consolidation (EWC), and LWTA model. Source code is available on a public repository. In the experiments, weight-transfer techniques like EWC, LWTA, and IMM are examined using provided implementations. D-FC and D-CONV models combine FC and CONV with Dropout to address the CF problem. Model selection involves hyper-parameter optimization for hidden layer number and size, learning rate, and computational resource constraints. In the experiments, weight-transfer techniques like EWC, LWTA, and IMM are examined using provided implementations. The hyper-parameters for model selection include hidden layer number and size, learning rate, and computational resource constraints. The size S, learning rates 1 and 2, batch size, training epochs E, and hyper-parameters L are specified for each processed dataset. The set of all hyper-parameters for a model is formed as a Cartesian product of allowed values. In the experiments, weight-transfer techniques like EWC, LWTA, and IMM are examined with fixed dropout rates and hyper-parameters for different models. The specific settings for CNNs, EWC, and IMM models are detailed, including optimizer parameters and dataset selection. The datasets used for constructing SLTs uniformly across datasets include MNIST, EMNIST, Fruits 360, Devanagari, and FashionMNIST, each with specific classes of images. FashionMNIST BID27, SVHN BID19, CIFAR10 BID15, NotMNIST, and DISPLAYFORM0 are datasets used for investigations in classification tasks. They consist of images of clothes, house numbers, real-world objects, letters, and handwritten digits, respectively. The datasets vary in the number of classes and types of images they contain. The study presents various types of SLTs for incremental learning, including D5-5 type SLTs and permutations (DP10-10). More D5-5 type SLTs are created to avoid biased results. The classes are assigned to different sub-tasks, with exceptions for DP10-10. This study presents an experiment implementing constraints from Sec. 1.2, applying DNN models to SLTs as defined in Sec. 2.3. The experimental protocol involves determining the best hyper-parameter vector for sub-task D1, re-training the model, and evaluating the quality of re-training based on test accuracy criteria. The experiment involved determining the best hyper-parameter vector for sub-task D1, re-training the model, and evaluating the quality of re-training based on test accuracy criteria. Permutation-based SLTs should be used with caution as DP10-10, the SLT based on permutation, does not show CF for any model and dataset. The experiments with different sub-tasks showed varying performance of models against CF. While FC and CONV models exhibit CF, D-FC and LWTA perform poorly. EWC is effective against CF for simple SLTs but less impressive for D9-1 type SLTs when not using the \"best\" evaluation criterion. This violates application requirements and is illustrated in FIG4. The experiments showed that EWC is ineffective against catastrophic forgetting for more complex problems, as seen in D5-5 type SLTs. EWC cannot prevent CF when the number of samples in sub-tasks is similar, indicating a limitation in protecting all relevant weights for D1. The experiments demonstrated that EWC is ineffective in preventing catastrophic forgetting for complex problems like D5-5 type SLTs, especially when the number of samples in sub-tasks is similar. IMM is effective for all SLTs but impractical. Tab. 4 shows that wtIMM outperforms other models in Tab. 3, particularly for D5-5 type SLTs. The discrepancy in results compared to MNIST is attributed to the model selection process using only D1. Neurons chosen beforehand for MNIST results, implicit use of D2. Determining balancing parameter \u03b1 in IMM is challenging. Optimal value requires cross-validation, not guessed from relative sizes of D1 and D2. Repeated calculation of Fisher matrices time and memory-consuming. Evaluation of IMM limited to few datasets due to complexity. TensorFlow implementation may be a factor. IMM violates application constraints. The present state of IMM violates two application constraints. Training an IMM model on different SLTs shows variability in tuning curves for determining the optimal balancing parameter \u03b1. Best wtIMM experiments for SLT D5-5b were conducted, with results indicating that CF remains a major problem when training DNNs. When training DNNs under application constraints, prior knowledge can simplify model selection and improve results. Keeping a subset of data in memory allows for a \"best\" type evaluation for re-training. Evaluation methodologies may vary, with a focus on application scenarios. The need for consensus on measuring CF in application scenarios is highlighted, with a focus on model selection being crucial for training DNNs on SLTs. It is emphasized that wrong hyper-parameter choices can have disastrous effects on sub-tasks, necessitating DNN variants with less dependency on hyper-parameters. Workarounds for making EWC or IMM practicable in some scenarios are suggested, including keeping a small subset of data in memory for optimal values determination. This study emphasizes the importance of model selection for training DNNs on SLTs and the impact of hyper-parameter choices. Small changes to \u03b1 do not significantly affect final accuracy for IMM, and accuracy loss for EWC is gradual with re-training time. The study suggests that CF is still a problem for DNNs, and more research is needed for solutions. Results on permutation-type SLTs should be approached with caution in future CF studies. The performance of CONV and D-CONV models on the SVHN dataset showed an increase in \u2126 all, but this was not considered significant due to the difficulty of the problem for the simple architectures used. The study concludes that \u2126 all is an important measure when baseline performance is better than random. A combination of measures may be useful to cover different cases. The best results of algorithms on various datasets are presented, showing CF behavior for all algorithms except EWC and IMM in one case. 2D representations of experimental results can provide clearer insights. The study found clear CF behavior for all algorithms except EWC and IMM for dataset D9-1b. There was no discernible dependency between CF occurrence and hyperparameter settings. EWC showed CF for the Devanagari dataset, while IMM did not exhibit CF for D9-1b. The balancing parameter for IMM cannot simply be set to 0.9 or 0.1."
}