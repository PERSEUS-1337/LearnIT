{
    "title": "ByGUFsAqYm",
    "content": "Memorization of data in deep neural networks is a topic of significant research interest. In this paper, the memorization of images in deep convolutional autoencoders is linked to downsampling through strided convolution. Linear convolutional autoencoders store linear combinations of training data as eigenvectors in the linear operator when downsampling is used. Nonlinear networks also exhibit this effect, with downsampling causing the model to memorize individual training images. These findings provide insights into the phenomenon of memorization in over-parameterized deep networks. Recent work has analyzed the memorization properties of deep convolutional neural networks (CNNs) in classification tasks. Popular CNNs can achieve almost zero training error on randomly labeled datasets, indicating their ability to memorize large training datasets. Experiments show that CNNs can learn patterns from realistic data before memorizing training images, and modern architectures are capable of remembering and distinguishing a large number of images from unseen ones. In this study, downsampling is presented as a mechanism for deep CNNs to memorize specific training images. The focus is on the memorization properties of linear and nonlinear fully convolutional autoencoders, using architectures commonly employed in image-to-image tasks. Autoencoders are chosen for their role as building blocks of many CNNs and their potential for layerwise pre-training to improve training. This paper discusses the memorization properties of over-parameterized autoencoders, particularly in the linear case. It explores how autoencoders learn and provides insights into how deep CNNs memorize training data. The study focuses on the phenomenon of memorization in a U-Net architecture when trained on a single image, showing that the output always contains the training image regardless of the input. In Section 3, the text discusses the tight connection between memorization and downsampling in linear autoencoding CNNs. It introduces the concept of storing linear combinations of training images as eigenvectors of a matrix representation. Strong evidence for this conjecture is provided on 2 \u00d7 2 images. Section 5 analyzes eigenvalue decay, showing that linear combinations of training examples are stored as eigenvectors with eigenvalues close to 1 in downsampling linear CNNs. Finally, Section 6 explores memorization in the nonlinear setting, suggesting it is even more pronounced. In Section 6, the text delves into the stronger phenomenon of memorization in nonlinear networks, emphasizing the memorization of actual training images. The mathematical framework is introduced, focusing on two functions learned by autoencoding CNNs: the identity function and the point map. The training set consists of square images denoted by X, with a discussion on convolutional autoencoders, specifically linear CNNs. In this work, an architectural mechanism is identified that can alter the function learned by an autoencoding CNN on a training set. The function is denoted by C X and the training procedure minimizes the mean squared error loss. Linear CNNs are represented by matrix A X, and the flattened version of an image y is denoted by y f. In this work, an architectural mechanism is identified that can alter the function learned by an autoencoding CNN on a training set. The function is denoted by C X and the training procedure minimizes the mean squared error loss. Linear CNNs are represented by matrix A X, and the flattened version of an image y is denoted by y f. The identity function and point map are crucial in the subsequent analysis, showcasing how downsampling serves as a mechanism for learning the point map in deep CNNs. The preference for learning the point map over the identity function is illustrated, even when a downsampling network has the capacity to learn the latter. Linear CNNs defined by Network ND (non-downsampling) and Network D (downsampling) are compared, both employing 128 filters in each convolutional layer except the last layer. In FIG2, Network D and Network ND both use 128 filters in each convolutional layer except the last layer, which has 3 filters. Network D downsamples the image with stride 8 in the first layer and upsamples with scale factor 8, while Network ND does not perform any downsampling. Network D downsamples from 32 \u00d7 32 to 4 \u00d7 4 images, showing a memorization phenomenon similar to other networks. The networks were initialized using default weights drawn from a uniform distribution in the PyTorch deep learning library. Network D and Network ND were trained on a single image from CIFAR10, with Network ND reconstructing the digit 3 with a training loss of 10^-4 and Network D with a loss of 10^-2. Test images were then used, showing that Network D learned the point map while Network ND learned the identity function up to sign due to the absence of biases. This behavior remained consistent throughout the training process, even after only 10 iterations. Network D learns a point map even after 10 iterations, while Network ND learns a visually similar function to the identity map. The memorization phenomenon in Network D is not due to the small number of filters in the downsampling layer. A downsampling network with the capacity to learn the identity function still prefers to learn the point map as a result of optimization. Initializing a network with a manual initialization can result in learning the identity function, while using default PyTorch uniform distribution leads to a point map. Xavier uniform initialization also shows similar effects, but Kaiming initialization prevents this behavior in nonlinear networks with ReLU/PReLU activations. In Appendix E, it is shown that nonlinear networks memorize training images under various initializations. The linear operator describing the function learned by the linear CNN is extracted and analyzed. The matrix A X can be decomposed into matrices for each layer, with a focus on converting convolutional and upsampling layers into matrices. The eigenvectors of A X are linked to the mechanism of memorization. The images after vectorization and zero padding lie in R stride size t, kernel volume f \u00d7 3 \u00d7 3, and 1 layer of zero padding. The matrix corresponding to the operator of this layer is circulant-like, with careful accounting for zero padding. An example of nearest neighbor upsampling operation is provided in Appendix B. Nearest neighbor upsampling with scale factor k involves creating a matrix of size f(k(s-2)+2)^2 x f(s)^2, composed of circulant-like matrices with shifted one-hot vectors. The linear operator A X for a linear CNN is obtained by multiplying matrices of each layer, allowing analysis of memorization mechanisms through eigenvalues and eigenvectors. Each eigenvector represents an image in the space of c x s x s images. The linear operator A X in a downsampling network trained to zero loss has eigenvalues equal to 1 and eigenvectors as linear combinations of input training images. In a non-downsampling network, the linear operator A X also trained to zero loss. Linear downsampling networks learn a low rank solution when the number of linearly independent training examples is not sufficiently large, storing linear combinations of training images as eigenvectors of the linear operator. On the other hand, linear non-downsampling networks learn a much higher rank solution, often visually indistinguishable from the identity function. Linear downsampling CNNs need a minimum number of training examples before learning the identity function. Realistic images suggest a low-dimensional manifold, leading to the network learning a basis for the manifold instead. The network can learn solutions of ranks 1 through 4 for 1x2x2 images based on the training set's span dimension. The linear Network DS can learn a linear operator of rank min(dim(span(X), 4)) with 1 \u2264 dim(span(X) \u2264 4. Empirical evidence shows that downsampling networks learn operators with low effective rank, while non-downsampling networks learn operators with higher rank when trained on CIFAR10 color images. The downsampling networks learn operators with low effective rank, while non-downsampling networks learn operators with higher rank when trained on CIFAR10 color images. The downsampling networks use 3 convolutional layers with stride 2 and upsample with a 1 nearest neighbor upsampling layer with scale factor 8. At least 192 filters are needed for color CIFAR10 images to learn the identity function. The networks are trained using the Adam optimizer with a learning rate of 10^-4 and trained until the loss decreased by less than 10^-4 or for 20,000 epochs. Overfitting is disregarded as an overfitted autoencoder is expected to learn the identity function. The memorization effect is robust throughout the training process. The goal is to train until the training error is around 10^-3 to understand the rank of the operator. Network D 64,192 stores images as eigenvectors with eigenvalues close to 1. The network, when trained on images, stores them as eigenvectors with eigenvalues close to 1, producing a rank k solution. The top eigenvectors represent the training images, with the real component showing inverted colors. Training on multiple images results in higher-rank solutions. When trained on multiple images, network N D 4,4 produces higher-rank solutions with independent rank and spectrum for k > 2 images from different classes of CIFAR10. The phenomenon of memorization for non-linear networks is analyzed using linear CNNs D a,b and N D a,b with LeakyReLU activations. The non-linear downsampling network N LD 128,128 is trained on a set of 10 images from CIFAR10, resulting in interesting outcomes. The non-linear downsampling network N LD 128,128 trained on 10 images from CIFAR10 consistently outputs one of the training images when presented with a new image, Gaussian noise, or white squares. This indicates a strong memorization effect, with individual training examples acting as fixed points. Different initializations such as Xavier or Kaiming uniform/normal still result in memorization during training. This paper shows that downsampling in linear and nonlinear CNNs leads to memorization of training images, even when using different initializations like Xavier or Kaiming. The preference for low-rank solutions in downsampling networks is similar to phenomena observed in matrix completion problems. In the nonlinear case, nearly any input image can be mapped to a visually identifiable output image. The downsampling in linear and nonlinear CNNs leads to memorization of training images, even with different initializations. Non-downsampling auto-encoders learn a \"high rank\" map visually similar to the identity map. This sheds light on the generalization properties of downsampling networks for image tasks. The downsampling in linear and nonlinear CNNs leads to memorization of training images, even with different initializations. Further exploration is needed to understand why downsampling forces the network to learn low rank solutions instead of the identity. This involves developing a better grasp of optimization and initialization, starting with linear autoencoders and proceeding to non-linear settings. Additionally, connections between the conjecture and the manifold hypothesis need to be explored to better understand the space of realistic images. Nearest neighbor upsampling will be applied to outputs after setting filter indices. Algorithms for converting convolutional layers and upsampling layers into matrices are presented. Example of converting the first layer from a network into a matrix for 1 \u00d7 2 \u00d7 2 images is shown. The text explains the process of applying convolutional layers and upsampling in a neural network. It details the creation of matrices for convolutional layers and provides an algorithm for constructing rows of the matrix. Additionally, it mentions the upsampling matrix for an upsampling operation. The text provides the upsampling matrix for a layer with a scale factor of 2 on a vectorized zero-padded 1x1x1 image. It also demonstrates matrix factorization for a network and shows that the rank of AX is k when training on k linearly independent examples. Additionally, it explains that when X's span has dimension k, then AX also has rank k. After training network D 64,128 with 10 examples from CIFAR10, it achieves an MSE of 0.0046. The top ten eigenvalues have magnitudes close to 1, with corresponding eigenvectors presented in FIG21. The eigenvectors are linear combinations of the training input, making it more challenging to analyze. The eigenvectors in network D 64,128 are now linear combinations of the training input, making it difficult to decipher how the training examples are presented in the first 10 eigenvectors. Some training examples are visible in these eigenvectors, such as the bird, boat, frog, and car examples. The rank of the learned operators when training on thousands of examples from a single class of CIFAR10 is considered next. The rank of the learned solution should be less than 3072 even though training on more than 3072 images, as there are unlikely to be 3072 linearly independent images in each set. The eigenvalue and eigenvector plots show the rank of the resulting matrix for D 64,192 on 2000 dogs, 5000 dogs, and 5000 planes to be around 500, 900, 750 respectively. To address this, the SVD of the resulting operator was taken, and the MSE of reconstructions using the resulting operator was compared to the original. The reconstructions were compared using the resulting operator against the original, showing that only the top 500, 900, and 750 components were needed for low training error. The training errors were around 10^-3, justifying these approximations. Different initializations did not affect learning the point map in a nonlinear downsampling network. After comparing reconstructions using different initializations, it was found that Kaiming initialization resulted in a blurred version of the training image."
}