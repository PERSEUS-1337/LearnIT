{
    "title": "La2hggaEcW",
    "content": "Augmented Reality (AR) can assist with physical tasks like object assembly through situated instructions in the form of videos, pictures, text, or guiding animations. The system AuthAR helps in authoring AR tutorials for assembly tasks with minimal effort by automatically generating key components while the author assembles physical pieces. This approach allows for portable tutorials tailored to different end users' preferences. Augmented Reality (AR) can aid in physical tasks like assembly by providing guidance through situated instructions. AR tutorials can reduce the need for context switching between instructions and physical objects, benefiting fields such as Facilities Management, Maintenance, and IoT device management. While previous work has focused on the benefits of following tutorials in AR, there is less emphasis on how to author these tutorials. AuthAR is a system for creating tutorials in AR for assembly tasks, catering to different user preferences with animations, static images, or video tutorials. It generates visual representations as the author completes the task, allowing for in-situ tutorial creation and reducing post-processing needs. AuthAR system generates mixed media assembly tutorials with a focus on non-obtrusive content authoring and tutorial playback. The system creates tutorials by building a laptop stand and automatically generating an XML representation. Initial observations show the tool's value and potential for future refinement. AuthAR builds on prior research in AR tutorials and mixed media tutorial design. The use of augmented reality (AR) for assembly tasks has been shown to support faster assembly compared to traditional methods. Prior research suggests that AR is most effective for complex tasks, with abstract representations being more effective than concrete ones. Information-rich 2D representations have also been found to be effective in some cases. AR is justified when the task is sufficiently difficult, making the time to process information insignificant compared to the time to perform the task. The need for mixed media tutorials for physical tasks is apparent, with little work exploring their authoring. AR content authoring systems allow users to create AR content without programming expertise, making participation easier. AR content creation systems target specific end users for participation in various domains such as museum exhibition curation, tour guidance, and assembly/maintenance. Authoring tools allow for the creation of training experiences, with tools like Zauner et al.'s component-based framework enabling the authoring of assembly task guides in augmented reality. Experts can collaborate remotely and annotate live video feeds for real-time assistance. Our system generates content semi-automatically for tutorial authoring, requiring manual input only for augmentation and refinement. Design guidelines for mixed media tutorial authoring include scannable steps, legible videos, visualized mouse movement, and user control over viewing format. ShowHow system by Carter et al. also emphasizes these components for building tutorials. Our work builds upon the concept of mixed media tutorials, focusing on AR authoring of physical task tutorials. Three popular media used for tutorials are videos, images, and interactive guidance. Prior work includes strategies for video segmentation and multiple perspectives, such as DemoCut for semi-automatic video segmentation and Chronicle for generating video tutorials based on file changes. The system generates video tutorials showing file changes. Multiple video perspectives are used for physical tasks. Image-based tutorials can be automatically generated from demonstrations. AR is used to provide spatially relevant augmentations for physical tasks. Interactive tutorials guide users on actions to take. Interactive tutorials for smartphone tasks and physical assembly utilize visual and depth tracking to automatically generate tutorials based on user actions. Hand tracking and color cues are used to indicate correct and incorrect positioning. DuploTrack enables users to create block models with depth sensing and projective guidance. Our design of the AuthAR system prioritizes hands-free interaction for assembly tasks, allowing users to keep their hands free. The system is grounded in exploration of the assembly task design space and related research, aiming to simplify tutorial generation. Our system allows authors to create tutorials using multiple media types, balancing between manual and automatic content creation modes. Authors can generate tutorials by completing assembly tasks normally, with the ability to refine and augment step representations. Our tool allows authors to create tutorials while performing tasks, enabling in situ editing to add callouts or make changes easily. The design space for assembly task tutorials in augmented reality is vast, with various ways to present instructions to users. The key dimension of variability is how tutorial information is displayed within the AR environment. The AR environment allows for presenting assembly tutorials with static text, images, and videos. It can display information as a heads-up display in the user's headset, leaving hands free for the task. Unique to AR is the ability to spatially associate elements with physical objects and provide live assembly guidance graphics or animations. The system uses traditional presentation methods alongside dynamic instructions for added benefits. The HoloLens records muted videos to listen for the keyword \"Stop Recording\" during video recording. Future versions may include microphone input for dictation. Text supplements the muted video for content authoring. In situ authoring reduces effort and maintains context. Tutorials can be automatically generated as authors move through steps or manually created. The process of tutorial authoring involves automatically generating tutorials while allowing manual additions for expressivity. Editing content concurrently with content collection is emphasized to maintain flow. The focus is on creating a well-designed concurrent editing process for efficient tutorial creation and sharing. AuthAR is a suite of software tools that enable tutorial authors to create AR content using voice and gaze controls. Optitrack motion capture cameras track assembly materials and a screwdriver, automatically updating the tutorial. The HoloLens captures first-person videos and guides authors through the tutorial creation process. The AuthAR system utilizes the Microsoft HoloLens and a Samsung Tab A 10.1 Android tablet for creating AR content. It includes features like location-specific callout points, untrackable screw markers, and image specifications. The system uses Optitrack visual markers for object recognition and point-cloud generation, with the vision of future headsets having onboard object recognition capabilities. The system coordinates object positions between Optitrack's Motive software and the HoloLens to make physical objects interactive. The HoloLens generates virtual replicas of objects for interactivity, using invisible renderings at the object's position. A tracked handle on a screwdriver allows for tracking screw events. Data is sent from the server to the HoloLens via UDP for updating object and tool transforms. The system coordinates object positions between Optitrack's Motive software and the HoloLens to make physical objects interactive. Parts are outfitted with Optitrack visual markers and defined as rigid bodies. Simple representations of these parts are passed to the HoloLens as invisible colliders, allowing physical components to act as interactive objects in AR. This approach can be easily extended to other assembly workflows by defining combinations of visual markers as rigid bodies and building simplified models of individual parts. The initial step of predefining shapes of parts in AuthAR allows for in situ editing with voice and gaze interaction, keeping hands free for building. Users can add augmentations at any point and follow a two-phase process for each step: Step Recording and Step Review. AuthAR enables users to start recording changes to object transforms by saying \"Start Recording\", allowing physical object manipulation to directly affect virtual representations. The system also records interactions like the screwdriver's tip creating screw holes on objects. Tracked attachments provide the tip's position based on the handle's orientation, with real-time position data used to calculate the tip's location. The HoloLens records dictation and prompts the user for step descriptions. After recording a step, the user can enter review mode to add manual augmentations. The system plays a looped video for the user to reference while making additions, with augmentations shifting focus based on user proximity. When adding manual augmentations to recorded steps on the HoloLens, the user can focus on small points (~3cm) by using gaze-based commands. The author can draw attention to specific points by adding virtual spheres or captioned images through voice commands like \"Add Point\" or \"Add Picture\". The system guides the user through the process with countdowns and prompts on the Heads-up Display. The HoloLens captures the current frame after a countdown, allowing the author to add text by speaking the image caption. Callout points can be designated as warnings by saying \"Warning\", turning them red with a red border around the associated image. Points can be moved or deleted as needed. During step recording, AuthAR records the use of a tracked screwdriver to generate screw holes. The author can associate a virtual screw with the hole by speaking commands like \"Add Screw\" and cycling through screw options. Physical screws can be compared to virtual ones for selection. New fasteners can be manually added in untrackable areas by using the \"Add Screw\" command. The author demonstrates how to manually place virtual screws in untrackable areas using a gaze-based cursor. After completing the step building process, the user can return to the idle state by saying \"Finish Review\". Initial observations and feedback on AuthAR are discussed, along with an example application for end users and considerations for future improvements. The initial feedback on AuthAR from two users creating an AR tutorial for assembling an Ikea laptop stand highlighted the system's usefulness in generating tutorials. One user found the ability to take pictures for annotations particularly helpful, indicating that the embodied gaze-based interaction is well-suited for picture and video recording. This feedback provides insights for possible improvements to AuthAR and building similar systems. AuthAR's functionality for making refinements in tutorials is enabled by the user looking near objects. Adding new callout points requires accurate cursor hovering on the object while speaking a command. Users found it awkward to point at certain points with their head. A hands-free tutorial generation system, AuthAR's use of dictation recognition for text entry was challenging for users due to automated prompting for step descriptions and titles. Future iterations of AuthAR will likely incorporate users explicitly starting dictation recognition for a title to be prepared to give one. Users can navigate between steps and review their work, enhancing the in situ content authoring tool. Additionally, users can add virtual screws to the tutorial and compare them to physical screws for accuracy. AuthAR allows for authoring of assembly tutorials and includes a playback mode for validation. The tutorial author can save the tutorial, which is then generated into an XML representation for playback. The application projects guidance lines for each piece's location and provides notifications when objects are correctly placed. Users also receive guidance on adding screws during the tutorial. The AuthAR application allows for authoring assembly tutorials with a playback mode for validation. The tutorial includes guidance lines for piece locations, notifications for correct placements, and instructions for adding screws. The third person video representation requires postprocessing for tutorial completion, as videos need to be manually moved to the headset's file system. The application was demonstrated with an Ikea laptop stand assembly but can be extended to other physical tasks with virtual models of the pieces. The AuthAR application simplifies virtual models for assembly tasks by using scaled cubes and disabled \"Renderer\" components. It allows for extensibility to various assembly tasks and tools. Future iterations could include more advanced logic for tool detection. However, the need for a large truss with Optitrack cameras limits widespread deployment. The AuthAR application simplifies virtual models for assembly tasks using scaled cubes and disabled \"Renderer\" components. Future iterations could include more advanced logic for tool detection. However, the need for a large truss with Optitrack cameras limits widespread deployment. The in situ authoring approach offered by AuthAR allows users to craft tutorials while assembling pieces, but the gaze/voice multimodal interface lacks efficient tools for fine-tuning tutorials. A 2D interface for precise editing of tutorial components is recommended. AuthAR enables semi-automatic generation of mixed media tutorials for assembly tasks, allowing for in situ editing and refinement. The application focuses on coarse-grain tutorial generation and object assembly, with the ability to easily load different virtual models. This approach aims to simplify the authoring process and enable the creation of tutorials that can reach a widespread audience. AuthAR enables the creation of tutorials with mixed media, tailored to individual user preferences, reaching a widespread audience."
}