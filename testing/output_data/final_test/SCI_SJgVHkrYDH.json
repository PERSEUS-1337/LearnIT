{
    "title": "SJgVHkrYDH",
    "content": "This paper introduces a new graph-based recurrent retrieval approach that learns to retrieve reasoning paths over the Wikipedia graph to answer multi-hop open-domain questions. The retriever model trains a recurrent neural network to sequentially retrieve evidence paragraphs in the reasoning path by conditioning on previously retrieved documents. The reader model ranks the reasoning paths and extracts the answer span included in the best reasoning path. Experimental results demonstrate state-of-the-art performance in open-domain QA datasets, with a significant improvement in HotpotQA. Current approaches leverage non-parameterized models for document retrieval and neural reading comprehension for answer extraction. However, these methods struggle with multi-hop questions that require retrieving evidence from multiple paragraphs. Recent open-domain QA methods face challenges for entity-centric questions as they struggle to capture lexical information in entities. Cognitive Graph incorporates entity links between documents for multi-hop QA, extending the list of retrieved documents. However, it still relies on compiling a fixed list of documents independently. In this paper, a new recurrent graph-based retrieval method is introduced for answering complex questions by sequentially retrieving evidence documents to form reasoning paths in a graph of entities. The method leverages a reading comprehension model to rank the retrieved paths, enabling accurate answers to complex questions compared to other methods. Unlike existing multi-step retrieval methods, this approach does not have hard-coded termination conditions and allows for arbitrary steps of reasoning. Our method leverages the Wikipedia graph to retrieve documents for answering complex questions, adapting to any reasoning path lengths. It significantly improves over previous work in HotpotQA and SQuAD Open by using a graph-based recurrent retrieval method and extending a reading comprehension model. The retrieval method narrows down the search space for the reader model in a robust pipeline process. Our QA model utilizes Wikipedia for open-domain QA, with each article divided into paragraphs. The framework aims to derive answers by retrieving and reading reasoning paths represented by sequences of paragraphs. The task is decomposed into the retriever objective and the reader objective, where the method learns to retrieve reasoning paths across a graph structure. Evidence paragraphs for complex questions may not have lexical overlaps with the question, but are likely to be retrieved, leading to entity mentions and further entailment. The QA model utilizes Wikipedia for open-domain QA, with each article divided into paragraphs. A graph of paragraphs is constructed to enable multi-hop reasoning, using hyperlinks to connect articles and allowing paragraphs to hop within the same article. The Wikipedia graph is densely connected, covering a wide range of topics for evidence in answering questions. This graph is constructed offline and reused for training and inference. The QA model utilizes Wikipedia for open-domain QA, with paragraphs connected by hyperlinks for multi-hop reasoning. A Recurrent Neural Network (RNN) selects paragraphs based on hidden states to model reasoning paths for questions. The process ends when the end-of-evidence symbol is selected, allowing for arbitrary length reasoning paths. The QA model uses Wikipedia for open-domain QA, with paragraphs linked for multi-hop reasoning. A Recurrent Neural Network (RNN) selects paragraphs based on hidden states to model reasoning paths for questions, allowing for arbitrary length paths. The next candidate set includes linked paragraphs from the selected paragraph, with the model learning multiple reasoning paths. Beam search is used for candidate paragraphs to avoid computational expense. The QA model uses Wikipedia for open-domain QA, with paragraphs linked for multi-hop reasoning. A Recurrent Neural Network (RNN) selects paragraphs based on hidden states to model reasoning paths for questions. To navigate the retriever effectively in the large-scale graph, candidate paragraphs are initialized with TF-IDF-based retrieval and guided search over the Wikipedia graph using beam search. The reasoning paths are defined by probabilities of selecting paragraphs, and the top paths with the highest scores are passed to the reader model. The QA model uses Wikipedia for open-domain QA, with paragraphs linked for multi-hop reasoning. A Recurrent Neural Network (RNN) selects paragraphs based on hidden states to model reasoning paths for questions. To navigate the retriever effectively in the large-scale graph, candidate paragraphs are initialized with TF-IDF-based retrieval and guided search over the Wikipedia graph using beam search. The computational cost of processing paragraphs is bounded by O(|C 1 | + B t\u22652 |C t |), where B is the beam size and |C t | is the average size of C t over the B hypothesises. Data augmentation involves training the retriever in a supervised fashion using annotated evidence paragraphs for each question. For multi-hop QA, multiple paragraphs are used, while single-hop QA uses a single paragraph. Ground-truth reasoning paths are derived from annotated data, and additional reasoning paths are added to the training data to relax and stabilize the training process. The QA model uses Wikipedia for open-domain QA, with paragraphs linked for multi-hop reasoning. A Recurrent Neural Network (RNN) selects paragraphs based on hidden states to model reasoning paths for questions. To navigate the retriever effectively in the large-scale graph, candidate paragraphs are initialized with TF-IDF-based retrieval and guided search over the Wikipedia graph using beam search. Data augmentation involves training the retriever in a supervised fashion using annotated evidence paragraphs for each question. Ground-truth reasoning paths are derived from annotated data, and additional reasoning paths are added to the training data to relax and stabilize the training process. Negative examples are used to train the graph-based recurrent retriever to discriminate between relevant and irrelevant paragraphs at each step, with two types of negative examples employed. Loss function is used to optimize the training process. The loss function for the sequential prediction task involves maximizing probability values of all possible paths by using binary cross-entropy loss. The model parameters, including those in BERT, are jointly optimized. The reader model verifies reasoning paths and outputs an answer span from the most plausible path. The framework involves a reader model that extracts an answer span from a reasoning path and re-ranks the paths based on the probability of including the answer. BERT is used for reading comprehension and re-ranking tasks, leveraging self-attention mechanism for multi-hop reasoning. The model parameters are jointly optimized for sequential prediction task. The multi-task reader model uses BERT for re-ranking and selects the best evidence to output the answer span. Training examples include ground-truth evidence paragraphs and distantly supervised examples from a TF-IDF retriever to simulate the inference process. To improve the reader model's training, additional negative examples are included to simulate incomplete evidence. For multi-hop QA, a ground-truth paragraph is swapped with a TF-IDF top ranked paragraph. For single-hop QA, the ground-truth paragraph is replaced with TF-IDF-based negative examples. The goal is to minimize P. The objective is to minimize the loss for question and evidence candidate pairs by considering cross entropy losses. The method is evaluated on three Wikipedia-sourced datasets: HotpotQA, SQuAD Open, and Natural Questions Open. Negative examples are masked to prevent unexpected effects on span predictions. HotpotQA is a large-scale multi-hop QA dataset with questions answered from 10 paragraphs in the distractor setting and the entire Wikipedia in the full wiki setting. Two evidence paragraphs are used for training. The dataset includes annotations for supporting fact prediction. SQuAD Open consists of questions from the original SQuAD dataset. The SQuAD Open dataset contains questions from the original SQuAD dataset, while the Natural Questions Open dataset includes questions based on Google Search queries. Metrics such as F1 and EM scores are used to evaluate QA accuracy in HotpotQA, SQuAD Open, and Natural Questions Open. The paragraph discusses evaluating paragraph-level retrieval accuracy for multi-hop reasoning using metrics like Answer Recall (AR), Paragraph Recall (PR), and Paragraph Exact Match (P EM). English Wikipedia is used as the evidence corpus, and Wikipedia dumps are utilized to construct the Wikipedia graph by extracting hyperlinks from raw HTML source files. The constructed graph for HotpotQA consists of 5.2M nodes and 23.4M edges. BERT models are used for retrieval and reading, with hyper-parameters tuned using the HotpotQA development set. Our method outperforms previous results on the HotpotQA development set, achieving significant gains in F1 and EM scores compared to state-of-the-art models. Even with BERT base configuration, our method shows improvement in predicting supporting facts. Our approach outperforms other models on HotpotQA and SQuAD Open, showing significant improvements in QA performance. Despite challenges with lexical overlap on Natural Questions, our method matches the performance of the best end-to-end retriever. Additionally, our retriever can be handled on a single GPU machine, making it more accessible than industry-scale computational resources required for fully end-to-end retrievers. Our retriever outperforms other models on HotpotQA and SQuAD Open, showing significant QA performance improvements. It can be handled on a single GPU machine, making it more accessible than industry-scale computational resources needed for fully end-to-end retrievers. Comparisons with competitive retrieval methods for HotpotQA full wiki are discussed, including TF-IDF scoring, re-ranking with BERT, and 2-hop reasoning extensions. Our recurrent retriever re-ranks linked paragraphs to improve retrieval performance. Comparing with Entity-centric IR and Semantic Retrieval, our method outperforms all QA EM scores. Ablation study shows the effectiveness of our modeling choices on the HotpotQA full wiki development set. The study evaluates different variants of the framework, including ablations on the retriever and reader models. Ablations include removing recurrence, beam search, and link-based negative examples from the retriever, as well as reasoning path re-ranking and negative examples from the reader model. Performance with different link structures is also discussed. The study evaluates different variants of the framework, including ablations on the retriever and reader models. Ablations show that removing components like recurrence and link-based negative examples result in notable performance drops. The most critical component is the recurrent module, with a 17.4 point drop in EM. Training without hyperlink-based negative examples also leads to a significant performance decrease, highlighting the importance of negative sampling for training. The study evaluates different variants of the framework, including ablations on the retriever and reader models. Ablations show that removing components like recurrence and link-based negative examples result in notable performance drops. The most critical component is the recurrent module, with a 17.4 point drop in EM. Training without hyperlink-based negative examples also leads to a significant performance decrease, highlighting the importance of negative sampling for training. Replacing beam search with greedy search results in a 4-point drop in EM, showing the importance of graph structure in finding reasoning paths. Removing reasoning path re-ranking also impacts performance, emphasizing the need to verify reasoning paths in the reader. Using an off-the-shelf entity linking system for constructing the document graph is evaluated on the development set of HotpotQA full wiki. Our approach with entity linking system shows slightly lower scores than hyperlinks but still achieves state of the art. Our method allows for arbitrary reasoning steps, outperforming fixed-step methods. Adaptive retrieval performs best, even though HotpotQA reasoning paths are typically two steps. Some questions can be answered with a single paragraph, as observed in previous studies. Based on the interplay between retriever and reader models, the retriever tends to favor shorter paths while the reader selects longer, more convincing multi-hop reasoning paths. The average length selected by the reader is notably longer than that by the retriever, with the best performance seen when selecting paths with a length of 3. This dynamic is illustrated through qualitative examples from HotpotQA full wiki. In examples from HotpotQA full wiki, our approach successfully retrieves correct reasoning paths and answers, while Re-rank fails. The retriever model selects a wrong paragraph with high lexical overlaps, leading to incorrect answers. Additionally, the reader model tends to favor longer, more convincing multi-hop reasoning paths for better performance. This paper introduces a new graph-based recurrent retrieval approach for answering multi-hop open-domain questions. The retriever model learns to sequentially retrieve evidence paragraphs to form reasoning paths, which are then re-ranked by the reader model to determine the final answer. Experimental results show significant advancements in HotpotQA and state-of-the-art performance on SQuAD Open and Natural Questions Open. The method presented in the paper demonstrates robustness without architectural changes, providing insights into entity relationships and reasoning paths. Future work includes end-to-end training of the graph-based recurrent retriever and reader for improvement. The use of weight matrices, bias vectors, and scalar parameters are detailed, along with the parameterization of vectors for symbols. Layer normalization is applied to align vectors, inspired by previous works. The paragraph discusses the importance of layer normalization in BERT for maintaining consistency in norms. It contrasts the approach of encoding paragraphs and questions separately with the benefits of attention-based interactions for improved retrieval accuracy. This method enhances entity-centric question performance by allowing for explicit interactions between paragraphs and questions. The work aims to balance scalability and accuracy in retrieval by using lexical matching retrieval, graphs, and question-paragraph encodings. A question-independent variant of the retriever model is proposed, with modifications to Equation (2) and conditioning the initial RNN state on question information. The reasoning path retrieval remains the same. Our retriever model learns to predict plausibility of reasoning paths by capturing paragraph interactions through BERT's [CLS] representations. The reader predicts plausibility and answers questions, leveraging self-attention across concatenated paragraphs for multi-hop reasoning. Additional re-ranking improves robustness in the HotpotQA dataset. In the HotpotQA dataset, the model extends binary classification to multi-class classification for yes-no questions. If \"yes\" or \"no\" has the highest probability, the answer is directly outputted without span extraction. Otherwise, the model uses span extraction. The recurrent retriever is adapted for supporting fact prediction by outputting sentences that support the answer in HotpotQA. Our framework includes a recurrent retriever for supporting fact prediction in HotpotQA. It selects supporting sentences from the most plausible reasoning path along with the answer. The model is trained using examples for supporting fact prediction, with separate parameters from the paragraph retriever. During training, the question-answer-sentence encoding is used to maximize the probability of selecting the ground-truth supporting fact sentences. At test time, the best reasoning path and predicted answer string are used from the retriever and reader models. The HotpotQA framework includes a recurrent retriever for supporting fact prediction. It selects supporting sentences from the reasoning path and answer. The model is trained separately for supporting fact prediction. The best reasoning path and predicted answer string are used at test time. The datasets contain a large number of questions. The retriever model is trained with different settings for the distractor and full wiki settings. The reader model remains the same for both settings. The reader model is used for both settings, trained with augmented references and negative examples. For SQuAD Open, the original training and development sets are used. Additional negative examples are added for training the reader model. Pre-trained BERT models are utilized for optimization. The code base pytorch-transformers is used for optimization with Adam optimizer and warm-up strategy. Recurrent retriever is trained with learning rate of 3e-5 and 3 epochs. Reader model is trained with learning rate of 3e-5, 2 epochs, and batch size of 120. Additional negative examples are used for training the reader model. The code base pytorch-transformers is utilized for optimization with Adam optimizer and warm-up strategy. Recurrent retriever is trained with a learning rate of 3e-5 and 3 epochs, while the reader model is trained with a learning rate of 3e-5, 2 epochs, and a batch size of 120. Additional negative examples are incorporated for training the reader model. The mini-batch example includes a question with its evidence paragraphs, and different Wikipedia dumps are used for various open-domain QA datasets to prevent inconsistent evaluations due to temporal changes in Wikipedia articles. The SQuAD development set faces challenges with answer consistency due to article updates. Natural Questions Open dataset struggles with temporal relevance in Google search queries. Wikipedia remains a valuable knowledge source despite these issues. The text discusses the selection process for initial candidates for different question answering datasets using a TF-IDF based retriever. The importance of considering Wikipedia's evolving nature for reproducibility in open-domain QA research is highlighted. Incorporating an entity linking system with a framework, replacing Wikipedia hyperlinks, involves retrieving seed paragraphs using TF-IDF and running an entity linker over them. If entities are detected, corresponding Wikipedia articles are retrieved and edges are added to the graph. The experiments are then rerun with the same components. The retrieval module is bootstrapped with TF-IDF for scalability and efficiency. Candidate paragraphs are expanded using the Wikipedia graph. Incorporating an entity linking system with a framework involves retrieving seed paragraphs using TF-IDF and running an entity linker over them. The paragraph EM upper-bound is estimated by checking if gold paragraphs are included in the top TF-IDF paragraphs. Results show that TF-IDF-based retrieval struggles with multi-hop paragraphs, but improving the number of F increases the upper-bound. In HotpotQA, there are two types of questions. In HotpotQA, there are two types of questions: bridge and comparison. The model achieves a significant EM gain for bridge-type questions and a higher QA EM for comparison-type questions. Some comparison-type questions can be answered with a single paragraph, resulting in lower EM scores. Examples of single-paragraph answers are provided in Section C.5. In Section C.5, the study aims to reduce the search space by increasing the number of initial TF-IDF candidates. Comparing three approaches, including a greedy one, the results show that increasing the candidate paragraphs can improve performance but may introduce noise. The approach is robust and consistently improves with more candidate paragraphs. The study aims to reduce the search space by increasing the number of initial TF-IDF candidates. Comparing three approaches, including a greedy one, increasing candidate paragraphs can improve performance but may introduce noise. The importance of question-paragraph encoding in the retriever model is shown through an experiment on HotpotQA's development set. The importance of question-dependent paragraph encoding is highlighted in the experiment results, showing a significant drop in QA performance without it. The performance drop is more pronounced in the full wiki setting compared to the distractor setting due to the closed nature of the latter. The recurrent retriever model narrows the search space, making retrieval easier. The reader model selects gold paths using re-ranking. Experiment results show the importance of question-dependent encoding for QA performance. The reader model performs similarly with question-dependent/independent approaches. Our model conducts qualitative analysis on reasoning paths predicted, selecting paragraphs for each question. Examples show shorter or longer paths than gold standard. One example is a bridge-type question where our retriever selects a single paragraph. Our model conducts qualitative analysis on reasoning paths predicted, selecting paragraphs for each question. Examples show shorter or longer paths than gold standard. One example is a bridge-type question where our retriever selects a single paragraph. The comparison-type question in Table 12 also shows that even comparison-type questions do not always require two paragraphs to answer. Min et al. (2019b) also observed that some questions do not necessarily require multi-hop reasoning, while HotpotQA is designed to require multi-hop reasoning. Our model automatically detects single-hop questions by selecting reasoning paths longer than the original annotations. It can accommodate arbitrary steps of reasoning, often choosing longer paths. An example is shown in Table 13, where an additional paragraph is selected for a HotpotQA question. The model selects additional paragraphs for HotpotQA questions, including \"Blue Jeans\" by Lana Del Rey. It shows state-of-the-art performance in the distractor setting and conducts qualitative analysis on its behavior. The retriever may not initially recognize relevant evidence in the closed setting. Our framework's multi-step retrieval method effectively selects the correct paragraph, P2, by considering previous retrieval history. This approach improves paragraph selection accuracy compared to simply adopting a Re-rank strategy. The model stops prediction at the third step, outputting [EOS]. In 7.9% of cases, the paragraph selection by our retriever differs from the graph-based recurrent retriever in the distractor development dataset. The graph-based recurrent retriever successfully selects paragraph pairs for multi-hop reasoning, showing competitive performance on open-domain QA datasets. The recurrent retriever model selects paragraph pairs for multi-hop reasoning, with the ability to output longer reasoning paths in the re-ranking process. The impact of selecting more than one paragraph is not significant. The recurrent retriever model shows a preference for longer reasoning paths in multi-hop reasoning tasks, resulting in improved performance on Natural Questions Open dataset compared to SQuAD. An example in Table 16 demonstrates the model's effectiveness in finding multi-hop reasoning paths. The recurrent retriever model excels in multi-hop reasoning tasks by effectively finding reasoning paths, as shown in an example in Table 16. The model's ability to retrieve multihop reasoning paths from Wikipedia without gold annotations is demonstrated through experimental results on open-domain QA datasets. The recurrent retriever model excels in multi-hop reasoning tasks by effectively finding reasoning paths with evidence to answer questions based on each dataset's nature."
}