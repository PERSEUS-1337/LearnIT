{
    "title": "HJGXzmspb",
    "content": "Researches on deep neural networks with discrete parameters and their deployment in embedded systems have been active and promising. A new method called \"WAGE\" is developed to discretize both training and inference processes using low-bitwidth integers. This method improves accuracies on multiple datasets and acts as a form of regularization. The potential to deploy training in hardware systems is also demonstrated. Empirically, training deep neural networks (DNNs) in hardware systems like deep learning accelerators and neuromorphic chips shows potential for higher energy efficiency and comparable accuracy. This is crucial for future AI applications with transfer and continual learning demands. DNNs have powerful feature extraction abilities but require energy-intensive devices like GPUs and CPUs, making their extensive use challenging for portable devices. Reducing the size of networks is of interest to prevent overfitting. In hardware systems, training deep neural networks (DNNs) shows potential for energy efficiency and comparable accuracy. There is interest in reducing network size to prevent overfitting. The focus is on processing training and inference with low-bitwidth integers for implementing DNNs in dedicated hardware. The framework proposed addresses quantization of operands and operations, as well as the number of bits needed for SGD computation and accumulation. The \"WAGE\" framework proposes constraints on weights, activations, gradients, and errors to low-bitwidth integers for SGD computation and accumulation. Techniques like ternary weights, 8-bit integers for activations, and simplification of fine-tuning methods are used. Inference is streamlined into accumulate-compare cycles, while training involves low-bitwidth multiply-accumulate cycles with alignment operations. The bitwidth requirements for error computation and gradient are explored heuristically. Experiments show that relative values, not absolute values, guide previous layers to converge. Small values have minimal impact on orientations and can be discarded in quantization. A higher bitwidth is needed for gradient accumulation despite ternary weight quantization. The framework is tested on various datasets and shows comparable accuracy to models only quantizing weights and activations at inference. WAGE produces bidirectional low-precision integer dataflow for DNNs, suitable for training and inference in dedicated hardware. The code is available on GitHub. The focus is on reducing precision in operands and operations for both training and inference. Techniques like network compression and compact architectures are efficient but not discussed in this paper. Methods for training DNNs with binary weights and activations are proposed, adding noise for regularization. High precision accumulation may be required for SGD optimization. XNOR-Net is also mentioned. High precision accumulation is crucial for SGD optimization. XNOR-Net uses filter-wise scaling factor for weights to enhance performance. Convolutions in XNOR-Net are efficiently implemented with XNOR logical units and bit-count operations. TWN and TTQ introduce symmetric thresholds to constrain weights to ternary values. Gradient computation in DoReFa-Net quantizes gradients to low-bitwidth floating-point numbers. TernGrad quantizes gradient updates to ternary values to reduce synchronization overhead in distributed training. The DoReFa-Net and TernGrad models store and update weights with float32 during training. Batch normalization quantization is ignored, making the training process complex with external quantization. DoReFa-Net training is challenging to apply directly on integer-based hardware but shows potential for exploring discrete spaces. WAGE quantization constrains four operands to low-bitwidth integers for weight, activation, error, and gradient. Error is defined as the gradient of activation for each layer, while gradient refers to the accumulation of weight. The gradient accumulation of weight W in feed-forward networks involves separating the gradient of weight g and the gradient of activation e. Quantization functions are used to map weights and activations to lower precision reflections for memory efficiency in hardware deployment. The text discusses the quantization of weights and activations in neural networks to improve efficiency in hardware deployment. It introduces functions to constrain the bitwidth of errors and gradients during training with low-bitwidth integers. The text discusses quantization in neural networks, using functions to constrain bitwidth of errors and gradients during training with low-bitwidth integers. In WAGE quantization, a linear mapping with k-bit integers is adopted to discretize continuous values. The text discusses quantization in neural networks, using functions to constrain bitwidth of errors and gradients during training with low-bitwidth integers. Clip is the saturation function that clips unbounded values to [\u22121 + \u03c3, 1 \u2212 \u03c3]. Equation 3 is used for simulation in floating-point hardware like GPU, while in fixed-point devices, quantization and saturation are satisfied automatically. A scaling factor is introduced before applying linear mapping to shift values distribution to an appropriate order of magnitude. Stochastic rounding is proposed to substitute small and real-valued updates for gradient accumulation in training. The implementation of operator Q G (\u00b7) in WAGE involves constraining high bitwidth gradients to k G -bit integers stochastically using a 16-bit random number generator. Batch normalization is crucial for Binary Neural Networks (BNN) to converge, as weight values \u00b11 are too large for typical DNNs. Batch normalization helps prevent exploding and vanishing gradients and reduces the need for proper initialization. However, normalizing outputs for each layer and computing their gradients without a floating point unit (FPU) can be complex. The implementation of operator Q G (\u00b7) in WAGE involves constraining high bitwidth gradients to k G -bit integers stochastically using a 16-bit random number generator. BNN simplifies batch normalization to a constant scaling layer, requiring cautious weight initialization. A modified initialization method based on MSRA BID5 is proposed to maintain variance between inputs and outputs of the same layer theoretically. The modified weight initialization in Equation 5 ensures weights can be quantized to non-zero values after random initialization. To alleviate the amplification effect, XNOR-Net proposes a filter-wise scaling factor and a layer-wise shift-based scaling factor \u03b1. The modified weight initialization in Equation 5 ensures quantization of weights to non-zero values. XNOR-Net proposes a shift-based scaling factor \u03b1 to attenuate the amplification effect. This factor approximates floating-point weights to integer representations, maintaining precision of weights presented by k W -bit integers. CNNs are typically followed by pooling, normalization, and activation, with average pooling avoided due to increased precision demand. Batch normalization is simplified to a scaling layer with trainable parameters replaced by \u03b1 from Equation FORMULA13. Activation quantization is also discussed. The quantization of activations in neural networks involves calculating errors layer by layer using the chain rule during training. DoReFa-Net applies an affine transform to map errors into a range of [-1, 1], eliminating the need for an inverse transformation after quantization. Orientation, rather than magnitude, of errors guides previous layers to converge. The quantization process in DoReFa-Net eliminates the need for inverse transformation. Errors are scaled and quantized, discarding values smaller than \u03c3. Gradients are updated based on relative error values, rescaled with a factor and adjusted with a shift-based learning rate. In WAGE, the shifted gradients represent minimum step numbers and directions for weight updates. The learning rate implementation differs from vanilla DNNs, with step sizes being integer multiples of minimum step \u03c3. Gradients may exceed 1 for faster training at the start and decrease during learning rate decay. To handle this, gradients are separated into integer and decimal parts, with high bitwidth gradients constrained to k G -bit integers using a random number generator. The WAGE method uses a random number generator to stochastically constrain high bitwidth gradients to k G -bit integers. This quantization helps avoid local minimums, overfitting, and reduces communication costs for distributed training. The weights are clipped to ensure they stay within a certain range after updating with discrete increments. The quantization methods for weights, activations, gradients, and errors are detailed in Algorithm 1. The WAGE method utilizes random number generation to constrain high bitwidth gradients to k G -bit integers, aiding in avoiding local minimums and reducing communication costs for distributed training. It quantizes weight updates to integer multiples of \u03c3 and scaled by \u03b7, enabling the use of pure mini-batch SGD without momentum or adaptive learning rate. L2 weight decay and dropout are retained as supplementary regularization methods, while the Softmax layer and cross-entropy criterion are commonly employed. In classification tasks, the Softmax layer and cross-entropy criterion are commonly used, but for low-bitwidth linear mapping, the calculation of e x is challenging. To address this, a sum-square-error (SSE) criterion is used instead of mean-square-error for tasks with few categories. The WAGE method sets default W-A-G-E bits to 2-8-8-8 for all layers in a CNN or MLP, with a bitwidth of 2 for ternary weights. XNOR-Net achieves 1-bit activations, emphasizing the importance of matching bitwidth for activations and errors in backpropagation. In our method, we increase the bitwidth to 8 for activations and errors simultaneously to maintain accuracy. We use 8-bit integers for weights during training and ternarize them during inference. The computation graph is built for a vanilla network, with quantization nodes inserted in forward propagation and gradients overridden in backward propagation. Evaluation on various datasets shows promising results, with MNIST achieving an average accuracy of 10 runs on the test set. For CIFAR10 dataset, a VGG-like network BID22 is used with data augmentation during training. The model is trained with a mini-batch size of 128 for 300 epochs. Learning rate \u03b7 is adjusted at epoch 200 and 250. For SVHN dataset, random flip augmentation is omitted, and training epochs are reduced to 40. ImageNet evaluation is done using the WAGE framework on ILSVRC12 dataset with AlexNet BID11 model. The WAGE framework is evaluated on the ILSVRC12 dataset using the AlexNet BID11 model without dropout and local response normalization layers. Images are resized to 256\u00d7256, cropped to 224\u00d7224, and horizontally flipped for training. Softmax is added and quantizations are removed in the last layer due to the difficulty of the ImageNet task. The model is trained with a mini-batch size of 256 for 70 epochs, with a learning rate of 4 that is divided by 8 at epoch 60 and 65. WAGE variations are compared with a vanilla CNN on CIFAR10, which has batch normalization in each layer and Softmax for the last layer. The WAGE framework is evaluated on the ILSVRC12 dataset using the AlexNet BID11 model without dropout and local response normalization layers. Images are resized to 256\u00d7256, cropped to 224\u00d7224, and horizontally flipped for training. Softmax is added and quantizations are removed in the last layer due to the difficulty of the ImageNet task. The model is trained with a mini-batch size of 256 for 70 epochs, with a learning rate of 4 that is divided by 8 at epoch 60 and 65. WAGE variations are compared with a vanilla CNN on CIFAR10, which has batch normalization in each layer and Softmax for the last layer. In the WAGE variations, the 28ff pattern has no quantization nodes in backpropagation, resulting in decreased weight updates by the rescale factor \u03b1, leading to a 3% reduction in error rate. The 2888 pattern shows comparable convergence rate to the vanilla CNN, better accuracy than those only discretizing weights and activations in inference time, albeit slightly more volatile. The WAGE framework is evaluated on the ILSVRC12 dataset using the AlexNet BID11 model without dropout and local response normalization layers. Images are resized to 256\u00d7256, cropped to 224\u00d7224, and horizontally flipped for training. Softmax is added and quantizations are removed in the last layer due to the difficulty of the ImageNet task. The model is trained with a mini-batch size of 256 for 70 epochs, with a learning rate of 4 that is divided by 8 at epoch 60 and 65. WAGE variations are compared with a vanilla CNN on CIFAR10, which has batch normalization in each layer and Softmax for the last layer. In the WAGE variations, the 28ff pattern has no quantization nodes in backpropagation, resulting in decreased weight updates by the rescale factor \u03b1, leading to a 3% reduction in error rate. The 2888 pattern shows comparable convergence rate to the vanilla CNN, better accuracy than those only discretizing weights and activations in inference time, albeit slightly more volatile. The discretization of backpropagation acts as another type of regularization and has a significant error rate drop when decreasing the learning rate \u03b7. Training curves of WAGE variations and a vanilla CNN on CIFAR10 are shown in Figure 3. The bitwidth k E is set to 8 as default in previous experiments. Errors from the vanilla CNN for CIFAR10 after 100 training epochs are exported to explore a proper bitwidth and its truncated boundary. The histogram of errors in the last convolution layer among 128 mini-batch data is shown in FIG2, indicating errors approximately obey a logarithmic normal distribution with values relatively small and a significantly large range. Proper window functions should be chosen to truncate the distribution while retaining the approximate orientations for backpropagation. For more details about the layerwise histograms of all W, A, G, E operands, refer to Figure 5. The upper boundary for errors is set to the maximum absolute value, while the left boundary is based on the bitwidth k E. Experiments show that 4-8 bits of errors are sufficient for CIFAR10 classification, with a default bitwidth of 8 matching image color levels. The histogram of errors in WAGE-2888 layers shows that errors reshape and aggregate into a truncated window, retaining most orientation information. Smaller error values have minimal impact on previous orientations and are partially discarded. The window width is optimized by left-shifting it with factor \u03b3 to explore its horizontal position, with the right boundary formulated as max{|e|}/\u03b3. Shifting errors have a critical role in backpropagation training, with large values being important while small values act as noise. Weight updates are done with ternary values in inference for a 16\u00d7 compression rate, but saved and accumulated in 8 bits for backpropagation training, resulting in an overall compression rate of 4\u00d7. Inconsistent bitwidth between weight updates and their effects in inference provides necessary buffer space. In backpropagation training, weight updates are saved and accumulated in 8 bits for a 4\u00d7 compression rate. The buffer space provided by k W prevents slow and unstable training due to changing ternary values. Experimenting with different bitwidths for gradients (k G) in CIFAR10 and ImageNet shows similar requirements as previous experiments. The study compares different patterns for gradient accumulation and batch normalization in the context of training neural networks. Overfitting is addressed by adding L2 weight decay, and the importance of buffer space for gradient accumulation is highlighted. In ImageNet dataset, gradient accumulation and batch normalization play a crucial role due to the variability of samples. BID3 achieves 1-bit weights representation to reduce memory consumption during training. The study focuses on using a larger minibatch size of 1000 and float32 backpropagation dataflow to improve weight updates in training and inference. By increasing the buffer space in WAGE with external bits of k G, they aim to enhance convergence rate, accuracy, and reduce memory usage. This approach also reduces memory access costs and size requirements during training, benefiting mobile devices with on-site learning capability. The WAGE framework is tested with different bitwidth configurations for MAC operation, with a possible solution being the 2-2-8-8 pattern. Ternary weights can slow down convergence and accuracy, suggesting a non-linear quantization method like logarithmic representation. This method may be more efficient due to the natural logarithmic distributions of weights and activations in a trained network. Normalization layers like Softmax and batch normalization are avoided in WAGE demonstrations, but they are considered essential for multi-channel perception and cross-model features encoding. The use of logarithmic representation for integer dataflow in DNNs is promising, with a focus on quantizing normalization methods for further studies. The new WAGE framework introduces an initialization method and layer-wise scaling factor to replace batch normalization for network quantization. It explores bitwidth requirements for error computation and gradient accumulation, achieving state-of-the-art accuracies with 2-8-8-8 bitwidth configuration. Further works are needed for compression and reducing memory consumption during training. The WAGE framework introduces a method to replace batch normalization for network quantization, achieving high accuracies with a 2-8-8-8 bitwidth configuration. The framework allows for discrete training and inference on integer-based ASIC or FPGA devices with on-site learning capability. Histograms of a trained VGG-like network with 2-8-8-8 bitwidth configuration and learning rate \u03b7 of 8. Y-axis represents probability in W-plots and G-plots, and logarithmic probability in A-plots and E-plots. A-plots show quantized input data."
}