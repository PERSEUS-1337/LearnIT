{
    "title": "rygpbR2Pi7",
    "content": "Pruning neural networks for wiring length efficiency is considered through three proposed techniques: distance-based regularization, nested-rank pruning, and layer-by-layer bipartite matching. Distance-based regularization with weight-based pruning shows the best performance in experiments, suggesting potential for creating neural networks for specialized circuits. Layer-by-layer bipartite matching can further optimize energy of layouts by minimizing wiring length between nodes in different layers. The optimization algorithm for minimizing wiring length between nodes in different layers is equivalent to the weighted bipartite matching problem. Pruning experiments were conducted on fully-connected neural networks for MNIST and a 10-layer convolutional network trained on the street-view house numbers dataset. Results of a hyperparameter grid search for these datasets are shown in TAB1. Nested rank pruning can outperform pure weight-based pruning, but distance-based regularization tends to perform even better. Results show that distance-based regularization with nested-rank pruning performs best in the lower accuracy, low energy regime. The tables display a wide range of values at the highest accuracy, but more consistency at lower accuracies. For MNIST, the best set of hyperparameters yielded promising results. In this paper, the authors introduce weight-distance regularization, nested rank pruning, and layer-by-layer bipartite matching to learn accurate neural networks with low total wiring length for reduced energy consumption. Results show that distance-based regularization outperforms weight-based regularization on average across both datasets, sometimes by close to 70%. The best performing set of hyperparameters for MNIST achieves a compression ratio of 1.64 percent at 98% accuracy, comparable to state-of-the-art results. The study introduces weight-distance regularization, nested rank pruning, and layer-by-layer bipartite matching for efficient neural network compression. Results show that these algorithms are effective, reaching state-of-the-art compression ratios. The techniques may be worth the computational effort for wide deployment, lower energy consumption, or specialized circuits. The algorithm outperforms baseline techniques in terms of energy and remaining edges metrics. The study introduces weight-distance regularization, nested rank pruning, and layer-by-layer bipartite matching for efficient neural network compression. Results show that these algorithms are effective, reaching state-of-the-art compression ratios. Average and standard deviation over 4 trials presented for the best performing distance-based regularization method before and after applying layer-by-layer bipartite matching."
}