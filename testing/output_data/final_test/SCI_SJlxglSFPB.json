{
    "title": "SJlxglSFPB",
    "content": "The paper adapts methods for detecting out of distribution images to detect out of distribution pixels, localizing unusual objects. Evaluation on new datasets shows methods perform worse than image-level counterparts. The LostAndFound dataset (Pinggera et al., 2016) highlights the Max Softmax method for out of distribution (OOD) detection, especially for safety critical applications like autonomous driving. Image-level OOD detection may not be sufficient for localizing unusual objects in images taken from onboard cameras. The framework extends to semantic segmentation networks for pixel-level OOD classification in autonomous driving. Unknown objects are identified and caution is advised or control is handed over to a safety driver. OOD objects in images can be automatically tagged for human labeling. This adaptation improves OOD detection by accurately identifying unusual objects in images. The study adapts image-level OOD detection methods to pixel-level OOD classification, evaluating their performance on a new dataset. New pixel-level OOD detection metrics are proposed, and two datasets are designed for evaluation with different network architectures. Results show that the best performing pixel-level OOD detection methods were derived from image-level OOD detection methods. The paper adapts image-level OOD detection methods to pixel-level OOD detection, evaluating their performance using a new dataset. New pixel-level OOD detection metrics are introduced, and existing image-level OOD detection methods are selected based on performance and computational feasibility for semantic segmentation. The paper adapts image-level OOD detection methods to pixel-level OOD detection for semantic segmentation. Methods like Entropy, Sum of Variances, and Mutual Information are used as uncertainty baselines. Ensemble methods like Vyas et al.'s are not feasible due to GPU memory limitations. GANs and AEs are excluded. Selected image-level OOD detection methods are described in Table 1 with necessary architecture modifications. The paper adapts image-level OOD detection methods to pixel-level OOD detection for semantic segmentation, using methods like Entropy, Sum of Variances, and Mutual Information as uncertainty baselines. The Dim reduction modification involves a 1 \u00d7 1 convolution reducing depth to 32. Threshold-independent metrics are used for evaluation, with no discussion on thresholds. Input perturbation steps are removed as they require access to OOD data before evaluation. Blum et al. (2019) attempted to address the challenge of semantic segmentation OOD detection by inserting animals from the COCO dataset or internet images into the Cityscapes dataset. However, the lack of realism in the inserted images made the created dataset insufficient for OOD detection. The training dataset used for all networks is the unmodified Cityscapes dataset. The Cityscapes dataset is widely used for semantic segmentation in road scene images, with evaluation datasets like the SUN dataset and the India Driving Dataset. The SUN dataset offers a wide variety of scenes and labels, while the IDD dataset includes all Cityscapes labels plus the autorickshaw class. However, the IDD dataset features tend to be out-of-distribution due to differences in architecture and infrastructure. The SUN dataset labels need to be modified for evaluating OOD detection, similar to the open world dataset. Ambiguous classes are sorted into a third set, and a mapping function is defined to assign new labels for each pixel in the SUN dataset. Pixels with the \"ignore\" class are given a weight of 0 during evaluation. The SUN dataset is resized to the same size as Cityscapes for evaluation. A synthetic dataset of random normally distributed images and Perlin noise images are used for OOD detection. All OOD datasets have the entire image labelled as OOD. The OOD datasets used for evaluation have the entire image labeled as OOD. The ratio of ID to OOD pixels for IDD/Cityscapes and SUN/Cityscapes is about 7.5:1 and 3:1 respectively. ODIN requires OOD data for hyperparameter tuning, so Cityscapes training set is mixed with a held-out training set of OOD data. Five metrics are used to evaluate model performance, including metrics commonly used in previous works on OOD detection. The output of semantic segmentation requires approximating metrics using 400 linearly spaced thresholds between 0 and 1. The curr_chunk discusses various metrics used for evaluation, including true positives, true negatives, false positives, false negatives, AUROC, AUPRC, FPRatTPR, and MaxIoU. These metrics are calculated based on ground truth and OOD prediction values across all images. The AUROC measures the area under the ROC curve, while the AUPRC measures the area under the precision-recall curve. FPRatTPR represents the false positive rate at 95% true positive rate, and MaxIoU calculates the maximum intersection over union. The detection error metric is excluded from the analysis. The MaxIoU metric, inspired by semantic segmentation, targets threshold-dependent tasks and punishes false positives more than AUROC. Experimental comparison showed a mean absolute difference of 0.039 in optimal thresholds selected by MaxIoU and Youden index. PSPNet and DeeplabV3+ network architectures are used with Resnet and Xception. The network architectures used in the study are Resnet and Xception, with a focus on achieving top performance on the Cityscapes benchmark. The final operation before the softmax layer is a bi-linear upsample to ensure OOD prediction correlation with pixel prediction. The Xception feature extractor is larger than Resnet, limiting the evaluation of the Mahalanobis method to PSPNet due to hardware constraints. The paper addresses two main research questions with associated experiments. The paper addresses two main research questions with associated experiments focusing on the impact of modifications on semantic segmentation performance and the evaluation of OOD detection methods. Performance comparisons of different methods using the PSPNet architecture are presented, showing trends in performance on different datasets. The study compares various OOD detection methods using the SUN and IDD datasets, showing different performance trends. The Confidence method excels in image-level detection but struggles at the pixel level. VarSum performs better on synthetic datasets than Mutual Information. VarSum shows significant performance improvement on modified datasets, except for the Perlin noise dataset. The Confidence method focuses on highlighting class boundaries to reduce prediction errors, making it less effective on certain datasets. The study compares OOD detection methods using different datasets. Mutual Information performs best on the IDD dataset, followed by ODIN. VarSum has the worst performance overall. Confidence method works better with DeeplabV3+ architecture than PSPNet, especially on random datasets. In comparing OOD detection methods, the Confidence method shows better performance with the DeeplabV3+ architecture compared to PSPNet, especially on random datasets. Results for DeeplabV3+ are less conclusive than PSPNet, with significant differences in performance observed. ODIN performs well across all datasets and architectures, aided by access to OOD data during hyperparameter tuning. Mutual Information method performs poorly on PSPNet but better on DeeplabV3+. The Mutual Information method performs poorly on PSPNet but better on DeeplabV3+. Both architectures label different parts of the auto-rickshaw as different classes. DeeplabV3+ shows an increase in OOD prediction for all cars compared to PSPNet. However, both architectures struggle with pixel-level OOD detection, especially when confident about incorrect predictions. The drop in performance for pixel-level OOD detection is likely due to features causing disruptions at the pixel-level, such as shadows, occlusion, and far away objects. A classical computer vision algorithm was developed to address this issue, resulting in a slight performance increase. There are limited works on pixel-level OOD detection. A new dataset is created by overlaying animals and objects on top of the Cityscapes dataset for testing various OOD detection methods. Bevandic et al. (2018) train a segmentation network with two datasets - one ID and one OOD dataset, comparing it to the Max Softmax baseline. Active Learning for semantic segmentation is also explored to reduce the number of training examples needed. Pham et al. (2018) create an open world dataset by relabelling labels from the NYU dataset as unknown if they don't exist in COCO. They develop a generic object instance level segmentation algorithm that splits the image into visually distinct connected regions but is too slow for real-time applications. Various methods for detecting OOD pixels and pixel uncertainty estimation are adapted from image-level OOD detection. This paper introduces new methods for pixel-level OOD detection and uncertainty estimation, comparing them using established metrics. Two new datasets for pixel-level OOD classification are also presented. The methods struggle to differentiate between class boundary pixels and OOD pixels, with ODIN and Mahalanobis showing the best performance. Mutual Information performs well with DeeplabV3+. The ODIN, Mahalanobis, and Mutual Information methods are considered the baseline for further research in pixel-level OOD detection. Understanding the faults of pixel-level OOD detectors is crucial for progress, including categorizing failure cases and analyzing differences in detection accuracy. The notation used in each subsection is consistent with the original works, and additional details can be found in the appendix. At test time, dropout can estimate model uncertainty by randomly choosing a multiplier of 0 or 1 for each neuron. Kendall et al. use dropout to compute model uncertainty, with the output being a variance per class. This variance sum per pixel provides the estimate of model uncertainty. Hendrycks & Gimpel (2016) demonstrate that the max softmax value can detect OOD examples. Hendrycks & Gimpel (2016) and Liang et al. (2017) propose methods to detect out-of-distribution (OOD) examples in image classification. The max softmax value is used to identify OOD pixels, while ODIN introduces temperature scaling and input preprocessing to improve separation of ID and OOD samples. The best temperature value is chosen from a set of 21 values. The Mahalanobis distance is used for detecting out-of-distribution (OOD) samples by measuring the number of standard deviations a vector is away from the mean in multiple dimensions. Each feature vector in the penultimate layer is assumed to be normally distributed with pixel class mean and global class covariance parameters. Initial tests showed better performance using pixel class means and global class covariance compared to other covariance methods. The class covariance outperforms global or pixel class mean and pixel class covariance. Labels are resized to match the penultimate layer. Quantities \u00b5 ci and \u03a3 c are computed based on class examples and training dataset. Class and minimum distances are computed for each spatial location. A dimensionality reduction layer reduces depth from 512 to 32. Input preprocessing and normalization of minimum distance are performed. The minimum distance is normalized with zero mean and unit variance using the sigmoid function \u03c3. A secondary branch in the image classification network outputs a confidence value learned via regularization loss to penalize low confidence. The new branch is trained to create a new prediction vector for each pixel. The new branch in the image classification network is trained to create a new prediction vector for each pixel using a Bernoulli distribution. A regularization term is added to enforce high confidence, and a total loss function with a hyperparameter \u03bb is used for training. At test time, a preprocessing step is applied to each pixel using the gradients of the loss function. Shannon entropy is used to determine the information content of a source. The entropy equation H : R n \u2192 R is used to determine the information content of a source. Hendrycks et al. (2018) train an image-level classifier with an auxiliary loss minimizing the entropy of predictions for outlier images. Mukhoti & Gal (2018) develop Bayesian uncertainty estimation methods for OOD detection. The entropy equation is used for OOD detection, along with predictive entropy and Aleatoric Entropy. Mutual information MI(y|x) is used to determine if a pixel is OOD or ID."
}