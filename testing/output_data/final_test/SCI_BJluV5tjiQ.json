{
    "title": "BJluV5tjiQ",
    "content": "We propose a novel neural network architecture for topic modelling based on an autoencoder structure with a bottleneck for topics distribution. An auxiliary decoder prevents mode collapsing. Sparse topics and words distributions are achieved through L-2 regularization. Our model shows competitive results on \"New York Times\" and \"20 Newsgroups\" datasets compared to deep models. Topic models, such as Latent Dirichlet Allocation (LDA), aim to represent a large body of text using a few topics in an unsupervised manner. LDA achieves a balance between representing documents with few topics and having a large number of words with high probabilities in those topics. The sparsity of distributions in LDA is controlled by concentration parameters in the Dirichlet distribution. The distribution of words in a document according to LDA is a mixture of multinomials. In our model based on LDA, the distribution of words in a document is a mixture of multinomials. We encode documents to the topic space using a neural network, ensuring a probability space with softmax. The latent representation is left unconstrained for finding topics, not generating new documents. To explain constraints on \u03b2k's, an L-2 norm constraint is imposed on topic and words distributions. Maximizing the L-2 norm concentrates probability mass, with hyperparameters \u03b3 and \u03b7 controlling sparsity trade-off. Training the model may lead to mode collapsing, where only a few topics have meaningful words while the rest contain random words. The addition of an auxiliary decoder to the latent representation helps resolve issues with capturing document variations in the model. By reconstructing input documents, this decoder separates document representations in the latent space, avoiding the need for sampling from a multinomial distribution. The overall objective of the model is to avoid sampling from a multinomial distribution by using an auxiliary decoder in the latent space. The performance of the proposed algorithm is compared with LDA using coherence and perplexity scores. The dataset used for topic modeling has already been preprocessed, and experiments were conducted with 25 and 50 topics. The 20 Newsgroups dataset contains 11,000 training documents. The 20 Newsgroups dataset contains 11,000 training documents. Preprocessing includes tokenization, removing non UTF-8 characters, and English stop word removal using scikit-learn package. The vocabulary size after preprocessing is 2,000 with 50 and 200 topics for model training."
}