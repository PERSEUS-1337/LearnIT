{
    "title": "rJe8pxSFwr",
    "content": "In this paper, the focus is on learning representations of signals, images, and image sequences from irregularly-sampled data with missing data. Two energy-based representations are explored: one from auto-encoders and one from Gibbs energies. The learning process involves a joint interpolation issue to solve. The paper focuses on learning representations from irregularly-sampled data with missing data using energy-based methods. The proposed representations are demonstrated to be relevant for various case studies, such as multivariate time series and images. Irregular sampling can be caused by sensor characteristics, sampling strategies, and environmental conditions. The text discusses interpolation methods for irregularly-sampled signals and images, including various energy forms such as Markovian priors, patch-based priors, gradient norms, Gaussian priors, and dynamical priors. Optimal interpolation and kriging are highlighted as state-of-the-art schemes in geoscience, involving covariance-based priors inferred from irregularly-sampled data. These methods may not always apply Gaussianity and linearity assumptions to real signals and images. Recent works have focused on learning interpolation schemes for irregularly-sampled signals and images, with a focus on deep learning models. The goal is not only to learn an interpolator but also to capture a representation of the data for other applications. Restricted Boltzmann Machines (RBM) models are introduced in this section. In this section, the issue of end-to-end learning of representations and interpolators from irregularly-sampled data is formally introduced within a Bayesian or energy-based framework. The goal is to minimize interpolation issues for signals or images with observations only available on a subdomain. Various energy priors have been proposed in the literature, and the aim is to learn the parameters of the energy from the provided irregularly-sampled observations. In this section, the end-to-end learning issue involves minimizing reconstruction error for observed data using interpolation operator I. A neural-network implementation is proposed for learning the parameters of energy U \u03b8 () and interpolation operator I. The implementation of the end-to-end formulation involves parameterizations for energy U \u03b8 (), NN-based interpolation operators, and overall NN architectures for learning representations and interpolators from irregularly-sampled datasets. Auto-encoders are used for NN-based energy representations, with encoding operators mapping the state into a low-dimensional space. Minimizing energy involves retrieving the hidden state with a low-dimensional representation. The Gibbs models provide an alternative energy-based representation without dimensionality reduction constraints. They decompose the energy function into a sum of potentials related to interacting sites, offering a framework widely explored in computer vision and pattern recognition. The Gibbs energy formulation in statistical physics represents the global energy of a system as the sum of local energies over interacting sites. It involves a parameterization of the potential function with a set of neighbors for each site. The low-energy state is characterized by a potential function that predicts well at each site based on its neighborhood. This Gibbs energy model is implemented using a neural network-based parameterization of the operator \u03c8, involving convolutional operators \u03c81 and \u03c82. The convolutional kernel for operator \u03c81 has coefficients set to zero at the center of the window, ensuring X(s) is not involved in the computation of \u03c8(XNs) at site s. This parameterization involves \u03c81 as a convolutional layer with N F filters of size 3x3x1 and \u03c82 as a convolution layer with one filter of size 1x1xN F. The energy U\u03b8 can be rewritten as X - \u03c8(X)2, with \u03c8(XNs) representing the operator \u03c8 at site s. The NN-based energy formulation involves the interpolation operator I and a fixed-point algorithm to solve for the parameterization chosen for operator \u03c8. The algorithm is parameter-free and can be implemented in a NN architecture with iterative gradient-based descent. The gradient descent method involves using a trainable CNN G() to approximate the true Jacobian J \u03c8 for operator \u03c8. This approach embeds the fixed-point algorithm when G is the identity, leading to NN-based interpolators I F P and I G. These interpolators implement N I iterations of the proposed interpolation updates. The architecture implements a N I-step interpolation algorithm using a CNN with ReLu activations and a linear convolutional layer. The operator \u03c8 is defined through energy representation, and G is the NN-based approximation of the gradient-based update for minimization. The input includes a mask \u2126 for missing data and an initial gap-filling X (0) for the state X. Missing data is initially filled with zeros for centered and normalized states. We design an end-to-end learning approach for energy representation and interpolator using observed samples and a normalization preprocessing step. The architecture is implemented under keras using tensorflow as backend with adam optimizer for training strategy. Increasing the number of blocks helps avoid training divergence. In experiments, the number of gradient steps is adjusted to prevent training divergence. The learning rate is decreased from 1e-3 to 1e-6 across iterations. Various datasets are used to evaluate the proposed scheme, including MNIST, Lorenz-63 dynamics, and ocean remote sensing data with missing patterns. Different frameworks like FP(d)-ConvAE and G(d)-ConvAE are utilized with varying interpolation steps. The proposed framework, FP(d)-GENN and G(d)-GENN, is evaluated on MNIST datasets with simulated missing data patterns. The dataset consists of 60000 28x28 grayscale images, and only the AE-based setting is evaluated. A convolutional AE architecture with a 20-dimensional encoding space is considered. Random missing data patterns are generated, including squares of different sizes. Performance is measured based on the evaluation of the framework. The evaluation of the proposed framework, FP(d)-GENN and G(d)-GENN, on MNIST datasets with simulated missing data patterns includes performance measures such as interpolation score (I-score), global reconstruction score (R-score), auto-encoding (AE-score) score, and classification score (C-score). The performance of the framework is compared with the DINEOF framework using various metrics on the test dataset. The proposed scheme guarantees a good representation in terms of AE score with an additional gain in interpolation performance, typically between \u2248 15% and 30% depending on missing data patterns. The performance of PCA-based and AE schemes is relatively poor when trained from gap-free data, showing that representations trained from gap-free data may not apply well to significant missing data rates. Training an AE using a zero-filling strategy also lowers the auto-encoding power when applied to gap-free data. For Fashion MNIST dataset, various models were evaluated including PCA, ConvAE, DINEOF, Zero-ConvAE, and proposed scheme. Different measures were used to evaluate the models' performance. An application to Lorenz-63 dynamics was also presented. Lorenz-63 dynamics are chaotic dynamics with parameters \u03c3 = 10, \u03c1 = 28, and \u03b2 = 8/3. They serve as a reduced-order model of turbulence dynamics. Time series of 200 steps are simulated using a Runge-Kutta-4 ODE solver with an integration step of 0.01. The series is subsampled and missing data is generated with a 75% mask. Training and test series consist of subsequences of 200 steps. Experiments with the GE-NN setting are reported, excluding the AE-based framework. The GE-NN architecture includes a 1D convolution layer with 120 filters and a kernel width of 3. The curr_chunk discusses the architecture used for missing data interpolation in Lorenz-63 dynamics, including a 1D convolution layer with 6 filters, a residual network with 4 units, and a final convolutional layer with 3 filters. It also shows an example of the interpolation results visually. The interpolation performance of the proposed approach (FP(15)-GE-NN) is compared to an ensemble Kalman smoother (EnKS) and DINEOF scheme in the context of missing data interpolation in Lorenz-63 dynamics. The EnKS with specific parameter settings serves as a benchmark, showing superior performance compared to DINEOF. The proposed approach outperforms DINEOF by about one order of magnitude in experiments with a time-step of dt = 0.02. The proposed approach (FP(15)-GE-NN) outperforms DINEOF by about one order of magnitude for experiments with a time-step of dt = 0.02. The interpolation error for observed states also highlights the improved prior from the Gibbs-like energy setting. Global PCA representation is not well-suited for chaotic dynamics, while local representations from the Gibbs energy setting are more effective. The third case-study involves satellite-derived Sea Surface Temperature (SST) image time series with high missing data rates due to cloud cover sensitivity. Groundtruthed datasets are created from high-resolution numerical simulations for evaluation. The architectures for the AEs and GE-NNs include ConvAE 1 with 400,000 parameters and ConvAE 2 with 900,000 parameters. ConvAE 1 has five blocks with Conv2D layers and 2x2 average pooling, while ConvAE 2 has double the number of filters. The GE-NN model involves \u2248 900,000 parameters and includes two architectures, GE-NN 1 and GE-NN 2. GE-NN 1 has 5x5 kernels, N = 20, and 3 residual units, while GE-NN 2 has 11x11 kernels, N = 100, and 10 residual units. The models differ in convolutional parameters and total parameters, with GE-NN 1 having \u2248 30,000 parameters and GE-NN 2 having \u2248 570,000 parameters. The GE-NN architecture involves different parameterizations with 11x11 kernels and 10 residual units, totaling \u2248 570,000 parameters. The models are applied to downscaled grids by a factor of 4, showing poor performance at the finest resolution. The upscaling process includes various layers such as Conv2DTranspose and Conv2D with different filters. Performance is evaluated using interpolation score, reconstruction score, and auto-encoding score on the training and test datasets. The study compares the performance of four models using fixed-point and gradient-based interpolation. Models trained with a zero-filling strategy show good interpolation and reconstruction performance but poor AE score. GE-NNs perform slightly better than AE settings with lower complexity. Gradient-based interpolation shows slightly better results than fixed-point interpolation. The study evaluates four different auto-encoder models and two GE-NN models for interpolation and reconstruction performance on ocean remote sensing data. Results show significant improvement compared to the current operational tool, OI, especially for finer-scale structures. Performance is measured in terms of interpolation, reconstruction, and auto-encoding scores for both training and test datasets. In this study, PCAs, ConvAE, and GE-NN models with different interpolation strategies are evaluated for learning energy-based representations of signals and images from datasets with missing data. The models show improved performance compared to the current operational tool, OI, especially for finer-scale structures. The study evaluates PCAs, ConvAE, and GE-NN models for learning energy-based representations of signals and images with missing data. Experiments show that gap-free data may lead to poorly adapted representations, and Gibbs priors in neural networks offer a lower complexity solution. Future work may explore multi-scale extensions and hybrid minimization schemes. A supplementary section illustrates masking patterns and reconstruction examples for the proposed framework. The study evaluates GE-NN models for learning energy-based representations of signals and images with missing data. Reconstruction examples for the proposed framework applied to MNIST dataset and SST time series with real missing data masks are provided. The interpolation performance of the GE-NN representation is compared with a PCA-based scheme and an ODE model lowerbound. The proposed approach demonstrates the ability to extract a generic representation from irregularly-sampled data, showing better reconstruction of fine-scale structures compared to traditional methods. Interpolation results for a 75% missing data rate with different sampling time steps are reported, highlighting the superior performance of the learning-based scheme over existing approaches. Interpolation examples for SST data using satellite-derived earth observation data are shown, including results from the FP(15)-GE-NN 2 model and an optimal interpolation scheme with a Gaussian covariance model. The approach demonstrates better reconstruction of fine-scale structures compared to traditional methods, especially for a 75% missing data rate with different sampling time steps. The FP(15)-GE-NN 2 model utilizes an optimal interpolation scheme with a Gaussian covariance model for interpolation."
}