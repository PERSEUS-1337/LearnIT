{
    "title": "ByxmXnA9FQ",
    "content": "With the rapid development in deep learning, deep neural networks are widely used in real-life applications. However, they lack control over uncertainty for test examples, leading to harmful consequences. This paper focuses on designing a higher-order uncertainty metric for deep neural networks and its performance in out-of-distribution detection. The method assumes an underlying higher-order distribution $\\mathcal{P}(z)$, approximates it with a parameterized posterior function $p_{\\theta}(z|x)$ under variational inference, and uses the entropy of the learned posterior distribution as uncertainty. The study focuses on designing a log-smoothing function to address the over-concentration issue in the entropy-based uncertainty measure for out-of-distribution detection in deep neural networks. The proposed variational Dirichlet framework shows significant improvements over baseline systems in various datasets and architectures. Deep neural networks have replaced traditional machine learning algorithms in applications like speech recognition, image classification, machine translation, and reading comprehension. Deep neural networks lack the ability to measure uncertainty in unseen cases, leading to over-confident predictions. This issue is harmful in real-world applications and raises concerns about AI safety. Designing a robust uncertainty metric is crucial for deploying deep neural networks effectively. An out-of-distribution detection task has been proposed as a benchmark to promote uncertainty research in the deep learning community. Several algorithms have been developed to improve out-of-distribution detection in deep neural networks, including methods like temperature scaling, input perturbation, adversarial training, and leveraging semantic dense representation. These techniques aim to increase the model's confidence in distinguishing between in-distribution and out-of-distribution data. The existing methods for out-of-distribution detection in deep neural networks conflate different levels of uncertainty, leading to misclassification of data. They fail to distinguish between lower-order (aleatoric) uncertainty and higher-order (epistemic) uncertainty, resulting in inferior performance in detecting out-domain examples. The existing methods for out-of-distribution detection in deep neural networks struggle to differentiate between lower-order and higher-order uncertainty. To address this, a new higher-order uncertainty measure inspired by Subjective Logic is proposed, using variational inference and deep neural networks to approximate the latent distribution. However, experiments have revealed an issue of over-concentration. The paper proposes a variational Dirichlet algorithm to address the over-concentration issue in deep neural networks for out-of-distribution detection. By smoothing the Dirichlet distribution with a calibration algorithm and input perturbation methods, significant improvements in detection accuracy are achieved on various datasets and architectures. The contributions include defining a higher-order uncertainty measure and proposing a method to alleviate the over-concentration problem in the Dirichlet framework, particularly focusing on image classification tasks. The paper discusses the image classification problem, focusing on the higher-order distribution P(z) over a K-dimensional simplex S k. It analyzes the uncertainty using entropy and mutual information within a Bayesian inference framework with dataset D. The plate notation in FIG1 illustrates the relationship between observed input data x, groundtruth label y, and latent variable z. The paper discusses the image classification problem, focusing on the higher-order distribution P(z) over a K-dimensional simplex S k. It analyzes the uncertainty using entropy and mutual information within a Bayesian inference framework with dataset D. The plate notation in FIG1 illustrates the relationship between observed input data x, groundtruth label y, and latent variable z. Groundtruth label y encapsulates the true posterior distribution, P(z) = p(z|y), which is approximated during testing with input image x. A posterior model p \u03b8 (z|x) is parameterized and optimized to approach the true posterior p(z|y) by minimizing KL-divergence. Leveraging variational inference, the KL-divergence is decomposed into two components for inference of higher-order distribution over z and estimation of higher-order uncertainty. The variational inference framework decomposes the problem into two components: DISPLAYFORM1 with the variational evidence lower bound L(\u03b8) and log p(y) as the marginal likelihood. By using the Dirichlet family for the higher-order distribution p \u03b8 (z|x), we can maximize the evidence lower bound L(\u03b8). The Dirichlet distribution with concentration parameter \u03b1 and normalization factor is utilized for its tractable properties. The empirical lower bound on dataset D is rewritten with a closed-formed solution for the expectation of log probability. The prior distribution in variational inference is assumed to be a Dirichlet distribution with concentration parameters \u03b1. Three intuitive prior functions are discussed, including a uniform prior and *-preserving priors. These priors affect the posterior concentration parameter \u03b1, allowing for a closed-form solution for the evidence lower bound. In variational inference, the concentration parameter \u03b1 is used to obtain a closed-form solution for the evidence lower bound. The derivative of L(\u03b8) with respect to parameters \u03b8 is calculated based on the chain-rule. Dir(z|\u03b1) is parameterized via a neural network with \u03b1 = f \u03b8 (x) and optimized using mini-batch gradient descent. During inference, the maximum \u03b1's index is used as the model prediction class. The concentration parameter \u03b1 is optimized using a neural network and its entropy is used as a confidence score. The Dirichlet distribution is heavily concentrated at a corner of the simplex, making the model sensitive to out-of-distribution examples. To address this issue, the proposal is to decrease the model's confidence by smoothing the concentration parameters \u03b1. To address the model's sensitivity to out-of-distribution examples, smoothing the concentration parameters \u03b1 is proposed. Experimentation with different smoothing functions showed that the log-smoothing function \u03b1 = log(\u03b1 + 1) effectively reduces the model's overconfidence. Adding perturbation to the input data before feeding it into neural networks is also suggested, inspired by the fast gradient sign method. Adding perturbation to the input data before feeding it into neural networks aims to improve entropy scores by adding belief to predictions. The parameter DISPLAYFORM1 represents the perturbation magnitude, with a rule-of-thumb value of 0.01 used throughout experiments. The Dirichlet framework's classification accuracy on various datasets and architectures is shown in Table 1. Detection involves using input perturbation to compute concentration \u03b1, calibrating it with log-scaling to obtain C(\u03b1), and comparing it to a threshold \u03b4 to determine in-distribution data. The variational Dirichlet method is evaluated for out-of-distribution detection by training a neural network on in-distribution datasets like CIFAR10/100 and SVHN. Dirichlet entropy is calculated based on output concentration \u03b1 to predict sample distribution. Different evaluation metrics are used to compare detection methods for separating the two distributions. The study evaluates the variational Dirichlet method for out-of-distribution detection using neural networks trained on CIFAR10/100 and SVHN datasets. Various models are trained and tested on in-distribution datasets with minimum impact on classification accuracy. The evaluation includes different metrics for comparing detection methods. The study evaluates the variational Dirichlet method for out-of-distribution detection using neural networks trained on CIFAR10/100 and SVHN datasets. Models are trained with stochastic gradient descent, Nesterov momentum, and weight decay for 200 epochs. Learning rate is reduced at specific epochs, and gradient norm is capped at 1 to prevent errors. The model is saved after validation accuracy converges for out-of-distribution detection. Performance is measured using established metrics like BID12, BID21, and BID30. Results show improvements compared to other methods in Tables 2 and 3. The proposed variational Dirichlet framework shows remarkable improvements in out-of-distribution detection for CIFAR datasets. However, the FPR score on CIFAR100 remains unsatisfactory. A series of ablation experiments were conducted to study the effectiveness of entropy-based uncertainty measure, concentration smoothing, and input perturbation. Concentration smoothing and input perturbation have similar influences on performance, with the best results achieved when both are utilized. In experiments with different priors, preserving groundtruth information leads to better detection performance. Smoothing functions and their impact on out-of-distribution detection accuracy are also investigated. The combination of input perturbation and smoothing yields the best results for out-of-distribution detection. The impact of compression capability of smoothing functions on detection accuracy is investigated. Smoothing functions with characteristics of unbounded range and compression of large values outperform baseline functions. Previous research focused on low-dimensional tasks, but recent works in deep learning propose adversarial training for anomaly detection to improve robustness. In order to enhance deep model robustness against outliers, approaches like BID2, BID17 have been proposed. Bayesian Networks like BID7, BID5, BID15 provide stochasticity in neural networks. However, Bayesian Neural Networks' uncertainty measures rely on Monte-Carlo estimation, reducing detection speed. A variational Dirichlet framework aims to improve uncertainty expression in deep neural networks, showing promising results but compromised accuracy on challenging setups like CIFAR100. In the future work, the method will be applied to broader applications like natural language processing or speech recognition tasks. LSUN dataset consists of 10,000 images from 10 scene classes, while iSUN dataset contains 8,925 images downscaled to 32 \u00d7 32 pixels. The impact of KL-divergence on classification accuracy and detection errors is investigated by gradually increasing the weight of KL loss. Increasing the balancing factor \u03b7 from 0 to 10 in the KL loss weight affects the model's classification accuracy and detection error. A too strong KL regularization decreases classification accuracy, while values too large or too small compromise performance. A hyper-parameter of \u03b7 = 0.01 balances these factors. Experimental results for ResNet architecture show that adopting a hyper-parameter of \u03b7 = 0.01 can balance stability and detection accuracy in out-of-distribution detection on CIFAR100. The results are listed in TAB5, where Semantic refers to the BID30 algorithm for semantic representation."
}