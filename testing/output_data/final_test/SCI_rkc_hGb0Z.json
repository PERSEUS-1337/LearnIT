{
    "title": "rkc_hGb0Z",
    "content": "The method presented evaluates the sensitivity of deep reinforcement learning policies and formulates a zero-sum dynamic game for designing robust policies. It aims to mitigate policy brittleness when agents transition from simulated to real environments. Through a zero-sum dynamic game against an adversarial agent, the system dynamics are driven to a saddle region to train robust policies efficiently. The approach outperforms the guided policy search algorithm and ensures maximal robustness to adversarial disturbances. Deep reinforcement learning aims to guarantee performance under disturbances by integrating function approximation techniques with learning-based control. However, deep RL policies often struggle to generalize to real-world scenarios due to differences between training and testing environments. Recent efforts focus on end-to-end integration of function approximation and learning-based control to optimize objectives while ensuring generalization to uncertainties in the environment. Examples include trajectory-based optimization for known dynamics and guided policy search algorithms for unknown dynamics. Optimization methods like guided policy search algorithms aim to improve agent performance in real-world tasks, but questions of robustness arise due to external disturbances and modeling errors. Transferring policies from simulated to physical environments can lead to brittleness in learned policies for continuous control tasks, defeating the purpose of having a robust policy. The paper demonstrates the brittleness of a state-of-the-art deep RL policy when faced with adversarial perturbations. It formulates an iterative dynamic zero-sum game to achieve a saddle point equilibrium. Lack of generalization of learned reward functions is likened to external disturbances. The paper formulates a zero-sum two-player Markov game to learn robust control policies in the presence of external disturbances. It uses a dynamic game approach to generate maximally robust policies and provides a formal treatment of the algorithm within the guided policy search framework. The iDG algorithm is discussed within the guided policy search framework, with experimental evaluation on multiple robots and conclusions provided. Robustness studies in classical control have formalized algorithms for stable feedback control and dynamic game tasks. Researchers are now addressing the incorporation of robustness guarantees into deep RL controllers for rich, robust performance. BID8 proposed a learning framework for agents in locomotion tasks involving simple reward functions and exposure to various difficult environments to achieve sophisticated performance. However, this strategy contradicts the goal of RL to make agents discover good policies with limited data. An ideal robust RL controller should be data-efficient and focus on building robust signals into the reward function. Building robust signals into the reward function for agents in locomotion tasks can be achieved through obstacles like hurdles, slopes, and walls. Pinto et al. introduced a method for learning robust RL rewards in a two-player MDP, focusing on adversarial policies to ensure robustness in the learned reward. This approach aims to address the challenge of expensive physical barriers and long training times for emergent locomotion behaviors. The text discusses the need for robustness in learned reward functions for reinforcement learning controllers. It introduces H \u221e control as a measure for classical RL problems but highlights the limitations in adapting it for complex tasks and policy optimizations. The lack of theoretical analyses on saddle-/pareto-point or Nash equilibrium guarantees and the global optimum for maximal robustness is also noted. The work is compared to BID9's neural fictitious self-play for large games with imperfect information, showing how deep reinforcement learning approaches a Nash equilibrium in such environments. The formulation of robustness in RL controllers within an H \u221e framework is emphasized. In the context of deep robot motor tasks, the robustness of RL controllers is formulated within an H \u221e framework. This involves a zero-sum game where both agents maintain a security level that never falls below the other's. The strategy pair of both agents constitutes a saddle-point pure strategy when their security levels coincide. Reinforcement learning in robot control tasks involves selecting control commands from a control policy that acts on a high-dimensional state composed of internal and external components. The performance of a robot in motor tasks is evaluated by an accumulated reward function defined by instantaneous and final rewards. Tasks in robot learning involve optimizing the expectation of accumulated rewards through a locally optimal policy. The distribution over trajectories is determined by the robot's dynamics and environment. Policy search methods are used to manage high-dimensional state and action spaces and address exploration challenges. Guided policy search algorithms guide the search for parameterized policies by minimizing a cost function with trajectory samples. In guided policy search algorithms, trajectory samples are used to minimize cost functions and generate guiding distributions for locally learned time-varying control laws parameterized by neural network policies. To ensure robustness to uncertainties and errors, an iterative dynamic game is proposed where an agent interacts with an adversarial agent in a closed-loop environment over a finite horizon. The stochastic dynamics of a markovian system state are governed by policies \u03c0 \u03b8 (u t |x t ) and \u03c0 \u03b8 (v k |x k ) in control tasks with complex dynamics. The system's controller aims for high rewards while an adversarial agent aims for low rewards, resulting in a zero-sum game equilibrium at the origin. The policies governing agent behavior are defined as \u03c0 \u03b8 (u t |x t ) and \u03c0 \u03b8 (v k |x k ), while the learned controllers are defined as p(u k |x k ) and p(v k |x k ). The motor policies p(u k |x k ) are sensitive to small disturbances, and the goal is to ensure robustness and stability in learning tasks. The paper discusses the non-robustness of guided policy search algorithms to perturbations, specifically additive disturbances. Policy search algorithms in reinforcement learning are popular due to their built-in robustness mechanisms, such as starting policy parameterization from multiple initial states or adding a KL constraint term to the reward function. However, these mechanisms only provide tolerance to slight changes in conditions and do not effectively model parametric uncertainty. Additional noise introduced into the system may not adequately address this issue, necessitating the need for a different approach. A methodical approach to solving the robustness problem in deep RL involves techniques from H \u221e control theory, addressing the sensitivity of a system to reject disturbances from the training environment or modeling errors. The lack of robustness in RL trained policies may stem from discrepancies between plant models and real systems, or learning environments. By measuring system sensitivity, \u03b3, policy robustness can be achieved by ensuring \u03b3 is small enough to handle disturbances effectively. The H \u221e control objective is to minimize the effect of disturbances on the system by finding the \"worst\" possible disturbance. This involves adapting to uncertainties and unmodeled dynamics to maintain robustness. The goal is to find the optimal policy for the plant to satisfy performance requirements and handle disturbances effectively. The H \u221e control objective aims to design a controller that minimizes the system's H \u221e norm, ensuring stability for any stable mapping. In a differential game setting, a minmax solution is sought for the plant's dynamics to address disturbances effectively. The optimal control and disturbance are derived from the Hamilton-Jacobi-Isaacs equation. The section discusses testing the sensitivity of a deep neural network policy for an agent by considering additive disturbance to a deep RL policy. The goal is to study the degradation of performance of a trained neural network policy in the presence of the \"worst\" possible disturbance in the parameter space of the policy. The uncertainty injected by an adversary is denoted by \u03a3 \u2206, and the nominal system is reached when the transfer matrix of \u03a3 \u2206 is zero. The adversary's control, v's effect on the output z, is examined. The adversary's control, v's effect on the output z, is minimized using a suitable cost function as a min-max criteria, quantified as an H \u221e norm on the system. The augmented reward for the closed-loop system is optimized by learning the optimal policy for the controller and adjusting the strength of the adversary with a sensitivity parameter \u03b3. The optimal closed-loop policy is determined by adjusting the sensitivity parameter \u03b3. As \u03b3 decreases, the adversary's actions have a greater impact on the system's performance. To enhance robustness, an alternating optimization algorithm is used to maximize the cost function for the adversarial controller and minimize it for the protagonist's policy. In a two-player, zero-sum Markov game framework, strategies for the protagonist and adversary are learned to reach a saddle-point equilibrium. The proposal involves developing locally robust controllers for trajectory optimization using a neural network function approximator. GPS incorporates off-policy guiding samples to improve learning. GPS algorithms optimize trajectory by adding off-policy guiding samples to guide the policy towards high reward spaces. By minimizing KL divergence from the previous policy, GPS reduces the effect of low entropy regions. The trajectory is optimized using optimal control principles under linear quadratic Gaussian assumptions, minimizing expected cost over state-action pairs distribution. The GPS algorithm optimizes trajectory by generating local control laws and using supervised learning to generalize to high-dimensional policy space settings. Multiple local control laws are generated during the C-step, and the global policy is regressed to all local actions in the S-step. To avoid divergence in dynamics, the KL divergence is constrained between current and previous trajectories. The GPS algorithm optimizes trajectory by generating local control laws and using supervised learning to generalize to high-dimensional policy space settings. To make the computed neural network policy robust to uncertainties, a zero-sum, two-person dynamic game scenario is proposed. The algorithm ensures robust performance during training of policies for a stochastic system by introducing the \"worst\" disturbance in the H \u221e paradigm. The reward function is augmented to withstand disturbances in the search for a guiding distribution problem. A zero-sum game is played where the protagonist aims for high rewards while the adversary pulls in its own direction, resulting in a robust control strategy. The proposed approach involves solving an MPC-based trajectory optimization problem within the framework of DDP, improving upon generic optimal control policies. The iLQG algorithm of BID21 is applied to a two-player, zero-sum dynamic game by iteratively approximating nonlinear dynamics and running passive dynamics with nominal control and adversarial input. Linearizing the system and using LQG approximation, the optimal control problem is solved with an augmented reward function. The dynamic programming problem transforms the min-max over an entire control sequence to a series of optimizations over a single control, proceeding backward in time. The Hamiltonian can be considered as a function of perturbations around the tuple {x k , u k , v k }. To solve the Bellman partial differential equation, a second-order local model of perturbed Q-coefficients is maintained. The LQR problem involves finding the best and worst possible actions by minimizing and maximizing a quadratic Q-function using linear controllers. The actions consist of a state feedback term and an open-loop term. The tuple {g u k , G u k , g v k , G v k } can be efficiently computed. Linear Gaussian controllers can be efficiently computed using the tuple {g u k , G u k , g v k , G v k }. These controllers have mean based on optimal solutions and covariance proportional to Q function curvatures. The controllers optimize an objective function with maximum entropy, producing trajectories that follow wide distributions while minimizing expected cost. Open-loop control strategies depend on the other player's actions. Equations ensure a cooperative game where players alternate between best and worst actions to maintain equilibrium. The protagonist's optimal strategy influences the adversary's choice for a more favorable outcome. The protagonist should choose a control strategy that is an optimal response. The protagonist should choose a control strategy that is an optimal response to the adversary's choice. The game is iterative and dynamic, with strategies depending on previous actions. Q coefficients are updated using a Gauss-Newton approximation. Local controllers produce linear Gaussian controllers with robustness to disturbances and modeling errors. The local controllers generated during trajectory optimization become robust to external perturbations and modeling errors. A prior generated by GMM helps obtain different linear modes at separate time steps. The prior generated by GMM enables the extraction of different linear modes at separate time steps based on observed transitions. Sample efficiency is improved by refitting the GMM at each iteration and constructing a good prior for dynamics. Linear Gaussian dynamics are obtained by fitting Gaussian distributions to samples, with maximum a posteriori estimates for mean and covariance given by specific parameters. The prior parameters \u03a6, \u00b5 0 , m, and n 0 are chosen to fit the prior to available samples. Trajectories are used to generate training data for global policies. Local policies are generated for all possible initial states. KL-divergence constraint is used to ensure the protagonist policy does not diverge too much. The learning problem involves imposing KL constraints on the cost function to align the protagonist controller distribution with the global policy. The algorithm involves alternating optimization between two steps at each iteration. The first step solves for a robust local policy by constraining it against the global policy using a KL divergence constraint. The second step projects the local linear Gaussian controller distribution onto a constraint set. The state is linearly dependent on the mean of the distribution, resulting in a linear Gaussian controller for the guided policy search algorithm. The iterative KL constraint minimizes the KL-divergence between policies for linear Gaussian dynamics and policies. The robust GPS algorithm involves minimizing the KL-divergence between policies by flipping the term in the S-step. Experiments confirm that guided policy search methods fail when exposed to simple perturbation signals. The algorithm follows prior works in computing the KL divergence term. In this paper, the authors present a trajectory optimization scheme and a robust guided policy framework to enhance robustness under unknown dynamics. They use physics engines for policies without visual feedback, such as MuJoCo and pybox2d. Experiments show that guided policy search policies are sensitive to disturbances in the action space. The 7-DoF robot results are briefly mentioned. The 7-DoF robot results were briefly mentioned in the abstract, showcasing robust GPS BID20. Experimental tasks included a 3D peg insertion task and a 2-link arm swing-up experiment, both facing challenges due to discontinuities in dynamics. Linear-Gaussian controllers were initialized using a PD control law for these tasks. For the peg insertion task with a robotic arm, a sensitivity algorithm is implemented using a PD control law. The protagonist's policy is trained using the GPS algorithm and tested against an adversarial disturbance in closed-loop. The closed-loop cost function includes terms for the end effector's position and peg insertion target. Sensitivity analysis is conducted for various disturbance values. The sensitivity of the trained policy to an adversarial disturbance is evaluated for different values of \u03b3. The adversary significantly impacts task performance for \u03b3 < 1.5, destabilizing the GPS-trained policy. However, for \u03b3 \u2265 1.5, the adversary's effect decreases as \u03b3 increases. The arm swing-up task involves balancing a 2D arm vertically around its origin with 7 states consisting of joint angles. The experiment involves optimizing the global policy using the mirror descent GPS algorithm on an agent balancing a 2D arm vertically. Adversarial disturbances are introduced with varying values of \u03b3 to evaluate sensitivity. The cost function evolves as \u03b3 changes, showing that higher disturbance magnitudes lead to larger augmented rewards. The experiment confirms that reinforcement learning algorithms fail in the presence of disturbances, leading to instability. To address this, a robust two-player game framework is implemented to develop more resilient controllers. The goal is to improve the controller's policy robustness in the face of modeling errors and uncertainties by generating noise samples in each iteration. Various values of \u03b3 are employed to enhance robustness. The robust two-player game framework is used to enhance controller policy robustness by generating noise samples in each iteration. Different values of \u03b3 are tested to improve performance compared to the vanilla GPS algorithm on the peg insertion task for a 7-DoF robot. The algorithm shows smoother execution and requires less data during the modeling phase. Our GMM algorithm now requires fewer samples to generalize to the global model, showing sensitivity of deep reinforcement learning algorithms in real-world scenarios. Testing the dynamic trajectory optimization two-player algorithm on a robot motor task, focusing on robustness parameters that cause policy failure. The two-player game framework enhances controller policy robustness by generating noise samples in each iteration. Our two-player game framework improves agent robustness to unmodeled nonlinearities and high frequency modes in dynamical systems. We aim to replace the Gaussian Mixture Model with a more sophisticated nonlinear model in future work."
}