{
    "title": "S1erHoR5t7",
    "content": "In SGAN, the discriminator estimates the probability of real data and the generator aims to increase the probability of fake data being real. It is suggested that the generator should also decrease the probability of real data being real to account for a priori knowledge and minimize divergence, potentially leading to equivalence with IPM GANs. Relativistic GANs (RGANs) and Relativistic average GANs (RaGANs) introduce a relativistic discriminator to estimate the realism of real data compared to fake data. These approaches can be applied to non-standard GAN loss functions, with IPM-based GANs being a subset of RGANs using the identity function. RGANs and RaGANs are more stable and generate higher quality data samples than non-relativistic counterparts. Standard RaGAN with gradient penalty produces better quality data than WGAN-GP, requiring fewer updates. RaGANs can generate high-resolution images from a small sample, surpassing the quality of images generated by other GAN models. Generative adversarial networks (GANs) involve a game between two neural networks, the discriminator D and the generator G, where D distinguishes real from fake data and G generates fake data. The original GAN has two variants for the generator loss functions: saturating and non-saturating, with the latter being more stable. The saturating loss has zero gradient when real and fake data are perfectly classified. The code for this can be found on https://github.com/AlexiaJM/RelativisticGAN. The discriminator in SGAN may struggle to be trained optimally due to vanishing gradients, especially in high-dimensional settings. Various GAN variants have been proposed to address this issue, but a large-scale study suggests that these approaches do not consistently outperform SGAN. Many successful GAN variants, like WGAN-GP, are computationally demanding. Recent GANs based on Integral probability metrics (IPMs) have shown benefits in improving stability. IPM constraints have been beneficial even in non-IPM based GANs. Spectral normalization and gradient penalty techniques improve stability in various GANs. IPM constraints improve GAN stability, but the reason for their effectiveness is not fully understood. IPM-based GANs are more computationally demanding, often requiring multiple discriminator updates per generator update. Non-IPM-based GANs lack a relativistic discriminator, which is crucial for divergence minimization and producing accurate predictions. GANs with a relativistic discriminator are more stable and produce higher quality data. They can be defined in terms of the discriminator and generator functions, with real data denoted as x_r and fake data as x_f. Most GANs have non-saturating or saturating loss functions to be minimized. Saturating GANs alternate between maximizing and minimizing the same loss function, approximating a divergence after training D. Non-saturating GANs optimize by swapping real and fake data, assuming a cross-entropy loss in SGAN. In SGAN, the discriminator output C(x) represents the realism of input data, with negative numbers indicating fake data and positive numbers indicating real data. IPMs are statistical divergences defined by a class of real-valued functions. The discriminator and generator loss functions in IPM-based GANs are unbounded but IPMs assume the discriminator is a function that does not grow too much. IPMs assume the discriminator is a function that does not grow too quickly to prevent loss functions from diverging. Each IPM applies a different constraint to the discriminator. SGAN should have the property where the probability of real data being real decreases as the probability of fake data being real increases. This is not the case for the discriminator in SGAN. IPM-based GANs implicitly account for the fact that some samples must be fake by comparing the realism of real and fake data. In SGAN, the discriminator loss function is equal to the Jensen-Shannon divergence, which is minimized when D(x r ) = D(x f ) = 1/2 and maximized when D(x r ) = 1, D(x f ) = 0. This behavior ensures that the discriminator can distinguish between real and fake data without the need for retraining. In SGAN, the dynamics differ from directly minimizing the Jensen-Shannon divergence. While minimizing the saturating loss, D(x f ) increases without decreasing D(x r ). To improve SGAN, the generator should not only increase D(x f ) but also bring it closer to 1. In SGAN, the dynamics differ from directly minimizing the Jensen-Shannon divergence. While minimizing the saturating loss, D(x f ) increases without decreasing D(x r ). To improve SGAN, the generator should not only increase D(x f ) but also bring it closer to 1. The gradients of standard GAN and IPM-based GANs are compared for further insight, showing that SGAN leads to the same dynamics as IPM-based GANs under certain conditions. SGAN leads to the same dynamics as IPM-based GANs under certain conditions, where the discriminator and generator are trained optimally and can perfectly distinguish real from fake data. If all assumptions are met, SGAN would have the same gradients as IPM-based GANs, potentially improving its stability. SGAN, similar to IPM-based GANs, can enhance stability. In standard GAN, the discriminator is defined as D(x) = sigmoid(C(x)). To make it relativistic, D(x) = sigmoid(C(x_r) - C(x_f)) is used. This modification allows the discriminator to estimate the probability of real data being more realistic than fake data. D_rev(x) = sigmoid(C(x_f) - C(x_r)) estimates the probability of fake data being more realistic than real data. Notably, D_rev does not need to be included in the loss function. The Relativistic Standard GAN (RSGAN) introduces a new class of models called Relativistic GANs (RGANs) by incorporating a relativistic discriminator. This discriminator can be applied to most GANs, leading to more stable models. The discriminator in RGANs estimates the probability of real data being more realistic than fake data, while a separate function estimates the probability of fake data being more realistic than real data. In Relativistic GANs (RGANs), the discriminator estimates the probability of real data being more realistic than fake data. A new alternative to the Relativistic Discriminator is proposed, called the Relativistic average Discriminator (RaD), which maintains a similar interpretation as the discriminator in Standard GANs while still being relativistic. The Relativistic average Discriminator (RaD) compares the critic of input data to the average critic of samples of the opposite type. RaD estimates the probability that real data is more realistic than fake data, similar to the standard discriminator. Experiments were conducted on CIFAR-10 and CAT datasets using Pytorch and Adam optimizer for 100K generator iterations. The study compared various GAN architectures on CIFAR-10 and CAT datasets using standard CNN and DCGAN models. FID was used to measure image quality, with lower values indicating better quality. Source code for replication is available in their repository. The study compared different GAN architectures on CIFAR-10 and CAT datasets using standard CNN and DCGAN models. Results showed that RSGAN and RaSGAN outperformed SGAN, while RaHingeGAN performed better than HingeGAN. RaLSGAN performed similarly to LSGAN. WGAN-GP had mixed results depending on the setup. The study compared various GAN architectures on CIFAR-10 and CAT datasets. RSGAN and RaSGAN performed better than SGAN, while RaHingeGAN outperformed HingeGAN. RaLSGAN had similar performance to LSGAN. WGAN-GP had mixed results. In the CAT dataset, after preprocessing, 9304 images were \u2265 64x64, 6645 images were \u2265 128x128, and 2011 images were \u2265 256x256. The dataset is challenging due to its small size. RSGAN-GP showed promising results with a FID of 25.60, comparable to the best reported FID of 25.5 using spectral normalization. The CAT dataset is challenging with small sample size and high-resolution images, making it ideal for testing GAN loss functions. Different GAN loss functions were trained on 64x64, 128x128, and 256x256 images. RaGANs were compared to SpectralSGAN and WGAN-GP on 256x256 images, showing lower FID values. To address mode collapse, the discriminator took a concatenated pair of images for 256x256 images. Results showed better performance for RGANs and RaGANs compared to non-relativistic counterparts. In 64x64 resolution, SGAN and LSGAN generated unstable images with fluctuating FID values. RaGANs, on the other hand, were more stable with lower FID values. Using gradient-penalty did not improve data quality but increased stability. SGAN failed to converge on 128x128 or larger images, while RaGANs were successful across all resolutions. SpectralSGAN and WGAN-GP could generate 256x256 cat images. In this paper, the relativistic discriminator was proposed as a way to enhance standard GANs, showing significant improvements in data quality and stability. The approach was further generalized to any GAN loss, introducing a more stable variant called RaD. Combining relativism with other techniques like spectral normalization and gradient penalty may lead to state-of-the-art results. Future research is needed to fully understand the mathematical implications of adding relativism to GANs. Further experiments are needed to determine the best relativistic GAN loss function across various datasets and hyper-parameters. Researchers are encouraged to explore this approach further, as shown in Table 3 comparing standard GAN and Relativistic average Discriminator outputs. The relativistic discriminator in GANs measures the probability that input data is more realistic than a randomly sampled data of the opposing type. To make it act more globally, the idea was to average the discriminator over random samples of the opposing type. However, this approach faces challenges in implementation. The Relativistic average Discriminator (RaD) is proposed as a solution to avoid the high complexity of comparing all possible combinations of real and fake data in a mini-batch. By comparing the critic of input data to the average critic of samples of the opposite type, RaD achieves O(m) complexity. Additionally, RGANs can be simplified by following two properties: f2(\u2212y) = f1(y) and non-saturating loss assumptions for the generator. SGAN could be equivalent to IPM-GANs under strict conditions and assumptions, but the unrealistic assumption of the generator being trained to optimality makes this equivalence unlikely. An experiment on the CAT dataset showed that SGAN gets stuck generating noise after around 200 iterations, indicating a difference in the distribution of D(x r). In 100 iterations, the distance between real and fake data is maximal as the generator only produces noise. The goal was to see if the discriminator's output approaches 0 in Relativistic GANs. Results in FIG2 show that with n G = 1, RSGAN and RaSGAN do not reach an average D(x r ) of 0, but the distribution is less concentrated around 1 compared to SGAN. With n G = 2, RSGAN and RaSGAN sometimes reach an average D(x r) of 0 and form an almost uniform distribution around [0, 1]. This suggests that using Relativistic GANs can make SGAN more similar to IPM-based GANs, but never equivalent. Relativistic Standard GANs can be seen as having a dynamic in-between SGAN and IPM-based GANs. Comparisons were made with SGAN, LSGAN, WGAN-GP, RSGAN, RaSGAN, RaLSGAN, and RaHingeGAN using the standard CNN architecture on unstable setups in CIFAR-10. RaLSGAN outperformed LSGAN in all setups, while RaHingeGAN performed slightly worse than HingeGAN in most setups. Overall, the study compared different GAN models on unstable setups in CIFAR-10. RaLSGAN outperformed LSGAN in all setups, while RaHingeGAN performed slightly worse than HingeGAN. RSGAN and RaSGAN performed better than SGAN in two setups. WGAN-GP generally performed poorly, possibly due to its update mechanism. The results support the stability of using the relative discriminator with LSGAN, but not with HingeGAN and SGAN. The study suggests that CIFAR-10 may be too easy to fully observe the effects of using the relative discriminator. The text discusses training algorithms for non-saturating RGANs with symmetric loss functions on the difficult CAT dataset with high-resolution pictures. It includes details on the distribution of x, updating parameters using SGD, and iterating through D iterations. Update \u03b8 using SGD by ascending with DISPLAYFORM3."
}