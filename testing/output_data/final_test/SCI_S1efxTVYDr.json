{
    "title": "S1efxTVYDr",
    "content": "The proposed data-dependent Gaussian prior objective (D2GPo) augments the MLE loss by introducing a Kullback--Leibler divergence term to address the drawback of negative diversity ignorance in sequence prediction problems. This approach considers the detailed token-wise structure of incorrect predictions, unlike MLE which treats all incorrect predictions equally. The proposed data-dependent Gaussian prior objective (D2GPo) enhances the MLE loss by incorporating a Kullback--Leibler divergence term to tackle negative diversity ignorance in sequence prediction. It utilizes a more detailed prior in the data, leading to improved performance in various language generation tasks. Language generation models have advanced with the use of deep neural networks, including sequenceto-sequence models, generative adversarial networks, variational autoencoders, and auto-regressive networks. These models are typically used for sequence prediction tasks with maximum likelihood estimation as the standard training criterion. Sequence prediction using maximum likelihood estimation (MLE) has been successful but faces challenges such as exposure bias, loss mismatch, generation diversity, and negative diversity ignorance. Various approaches have been developed to address these issues in training language generation models. To address negative diversity ignorance in MLE training, an extra Gaussian prior objective is added to compare incorrect predictions more effectively. This helps guide model training by considering the nuances of sequences' detailed structures. The proposed data-dependent Gaussian prior objective (D2GPo) enhances model training by comparing detailed predictions with ground-truth distributions through a KL divergence term. This method improves performance in language generation tasks like machine translation, text summarization, storytelling, and image captioning. NLG techniques are crucial in various tasks like sentence generation and machine translation. Input-conditioned language generation tasks pose challenges due to information imbalances. The proposed method focuses on enhancing model training for language generation tasks. Neural networks, particularly DNNs, have shown promise in NLP tasks. The NN language model (NNLM) by Bengio et al. (2003) extends the n-gram paradigm using NNs. Mikolov et al. (2010) introduced the recurrent NN language model (RNNLM) to capture long-term dependencies effectively. The Transformer model addresses the vanishing gradient problem in RNNs by using a self-attention mechanism for handling long-term dependencies in text. GPT and BERT are Transformer-based language models that incorporate left-to-right and bidirectional architectures, respectively. Current language generation models use RNNLM or Transformer structures, but there are challenges in fitting the distribution. Reinforcement learning, GANs, and end-to-end re-parameterization techniques have been proposed to address the exposure bias problem in language generation models. Using MLE for training can lead to loss mismatch, which has been tackled by incorporating evaluation metrics into training strategies like MIXER. There is a growing interest in integrating domain knowledge into machine learning models. One approach is to use specialized network architectures or features, while another involves using posterior regularization in structured probabilistic models to impose knowledge constraints during model estimation. This method has been expanded to incorporate knowledge constraints as extrinsic rewards in reinforcement learning, allowing for the incorporation of prior knowledge in language generation. Incorporating prior knowledge into language generation learning is essential. Welleck et al. (2019) introduced unlikelihood training to address unlikely generations. Their focus is on low-frequency words, while our model targets negative tokens. The likelihood objective may lead to models assigning excessive probability to repetitive and common words, deviating from human training data. Our goal aligns with theirs in making model predictions consistent with human training distribution. Sequence prediction models are typically trained using Maximum Likelihood Estimation (MLE), which minimizes the negative log-likelihood of the predicted sequence. However, this approach may not lead to good generalization due to noise in the training data. Models trained with MLE evaluate predictions as either correct or incorrect, ignoring similarity. To address the limitations of MLE training in sequence prediction models, an additional objective O is introduced to capture the diversity of negative predictions. This objective aims to more accurately assign scores to diverse model predictions, especially negative ones, by introducing a general evaluation function f(\u1ef9, y) independent of the model prediction. A higher f(\u1ef9, y*) value indicates a better prediction for a target candidate \u1ef9. In the model, learning f(\u1ef9, y) involves maximizing E p\u03b8[f(\u1ef9, y)] for GAN-like models or computing the gradient \u2207\u03b8 E p\u03b8[f(\u1ef9, y)] for non-parameterizable distributions. A prior distribution q(y) is defined based on the model predictions p\u03b8(\u00b7) and ground-truth data. The text discusses deriving a prior probability distribution q(y) from ground-truth data to guide model predictions. Kullback-Leibler divergence is used to match the probability distributions, with a loss function calculated for the objective. The final objective includes a balancing hyperparameter \u03bb, and the prior distribution q(y*) is obtained from the evaluation function f(\u00b7,\u00b7) with a softmax operation. The relationship between q(y*) and the evaluation function f(\u1ef9, y) is determined by a softmax temperature mechanism introduced by Hinton et al. in 2015. The temperature parameter T controls the distribution over classes, with T \u2192 0 leading to a Kronecker distribution and T \u2192 +\u221e resulting in a uniform distribution. The softmax operation transforms any evaluation function f(\u00b7,\u00b7) into a probability distribution, focusing solely on f(\u00b7,\u00b7) to find a good evaluation function. Token-wise diversity is mined for all y* by considering all token types \u1ef9j in a vocabulary, with a prior topological order ORDER(y*) established among known tokens. In language generation tasks, the evaluation function f(\u00b7,\u00b7) for the distribution q is defined using cosine similarity of pre-trained embeddings to sort token order. The Gaussian probability density function is adopted as the evaluation function for q. The text discusses the use of the Gaussian probability density function as an evaluation function in language generation tasks. It introduces the data-dependent Gaussian prior objective (D2GPo) as a departure from the data-independent Gaussian prior, explaining the mathematical differences. The central limit theorem is mentioned to support the use of a normal distribution for modeling random variables. The text introduces the concept of the data-dependent Gaussian prior objective (D2GPo) for language generation tasks, which differs from the data-independent Gaussian prior. It explains how embedding features extracted from data follow a Gaussian distribution, which can be effectively modeled using a Gaussian distribution for embedding-distance-determined order. The Gaussian prior in machine learning optimization theory assumes a zero-mean Gaussian distribution for model parameters, acting as a soft target for language generation using knowledge distillation. D2GPo is evaluated on various language generation tasks like NMT, text summarization, storytelling, and image captioning, with hyperparameters and effect analysis provided in Appendix A.7. The approach requires word embeddings or bytepair-encoding (BPE). The D2GPo approach for experimental tasks utilizes word embeddings or bytepair-encoding (BPE) subword embeddings. Pretrained embeddings were generated using fastText with specific parameters. Evaluation was done on translation tasks like WMT14 EN-DE, EN-FR, and WMT16 EN-RO. Data details are provided in Appendix A.3. The study utilized BPE sub-word encoding for NMT evaluation, with a shared vocabulary of 40,000 sub-word units. The Transformer NMT model was used as the baseline, following hyperparameters from Vaswani et al. (2017). BLEU scores were calculated for evaluation, showing significant improvement over the baseline Transformer models. Performance comparison with existing systems was reported in Table 1. Our model outperformed existing strong baselines for all language pairs, including unsupervised machine translation with EN-DE, EN-FR, and EN-RO. Evaluation on EN-DE used newstest2016 for comparability. The baseline used the masked sequence to sequence pre-training (MASS) model. The study utilized the MASS model as a baseline for pre-training, then fine-tuned the models using back-translation cross-entropy loss and D2GPo loss on various monolingual datasets. D2GPo consistently outperformed MASS in unsupervised translation pairs according to BLEU score comparisons. The study utilized the MASS model as a baseline for pre-training and fine-tuned it using D2GPo loss. Text summarization is a language generation task that creates a concise summary of a long-text document. The proposed method showed efficiency by consistently outperforming the baseline model on all evaluation metrics. The study demonstrated the effectiveness of supervised pre-training in text summarization, showcasing improvements in storytelling tasks. Utilizing the hierarchical story generation model as a baseline, D2GPo loss was added for enhancements. The prompt generation model remained consistent with previous work for evaluation purposes. The addition of D2GPo improved the Conv seq2seq + self-attention model for language generation, reducing perplexity and enhancing the quality of generated stories. This enhancement was particularly beneficial in settings with fewer restrictions on story generation tasks, such as image captioning, which continues to be a focus of research at the intersection of computer vision and natural language processing. In experiments, D2GPo was tested on image captioning to assess its performance on a diverse language generation model. Evaluation was done on the MSCOCO 2014 caption dataset using various metrics. The results were compared with a ResNet Top-down baseline and Self-critical Sequence Training (SCST). Our ResNet baseline outperforms the SCST models on the Karpathy splits test portion. Incorporating the D2GPo loss further improves our model across all metrics. Different evaluation functions were tested on the supervised NMT EN-DE task, showing changes in BLEU scores on the test set. The study compared BLEU score changes on the test set using different evaluation functions for language generation tasks. The Gaussian density function showed the greatest improvement, indicating that distance information from embeddings can guide the generation process effectively. Linear and cosine functions performed similarly to the Gaussian density function, suggesting they are rough approximations. The proposed data-dependent Gaussian prior objective (D2GPo) aims to improve language generation tasks. The proposed data-dependent Gaussian prior objective (D2GPo) enhances language generation tasks by imposing linguistic data as a prior over sequence prediction models. D2GPo outperformed strong baselines in various language generation tasks like neural machine translation and text summarization. The method calculates embedding cosine similarity for target sequences and sorts token types based on distance, achieving topological order. For supervised NMT data, 4.43M bilingual sentence pairs were used for the EN-DE translation task. For the supervised NMT data, bilingual sentence pairs from various datasets were used for different translation tasks like EN-DE, EN-FR, and EN-RO. The datasets included Common Crawl, News Commentary, Europarl v7, and WMT'14. Different test sets were used for validation and testing, following previous configurations. Additionally, synthetic training data was utilized for low-resource machine translation studies. The synthetic training data (STD) of Sennrich et al. (2016a) provided 2.8M sentence pairs for training in EN-RO translation. The Annotated Gigaword corpus (Napoles et al., 2012) was used as a benchmark, derived from news articles with 3.8M training samples. The hierarchical story generation model (Fan et al., 2018) was proposed for generating stories based on prompts. The study by Fan et al. (2018) utilized a self-attention gated convolutional language model for prompt-based story generation. They collected data from Reddit's WRITINGPROMPTS forum and trained a model with a novel fusion technique to enhance prompt relevance. Additionally, they incorporated a gated multi-scale self-attention mechanism for long-range context modeling. The top-down image captioning model employed a ResNet convolutional neural net for image encoding. The final convolutional layer of Resnet-101 was used with bilinear interpolation to resize the output to a fixed-size spatial representation of 10\u00d710. During training with D2GPo, the standard deviation of the KL diversity item \u03bb was set to 0.1, and the softmax temperature was T = 2.0 in all experiments. Experimental results show that the value of \u03bb affects the model training process, with a small value resulting in the model not fully utilizing prior knowledge, and a larger value making the model more uncertain. Additionally, a small value of T can improve the model. Experimental results show that a small value of T can improve the model, while a large value of T decreases performance. Priors are more helpful in low-data regimes. D2GPo outperforms the baseline model in different low-resource scenarios, as demonstrated in Table 7. D2GPo outperforms the baseline model in low-resource scenarios, showing that prior knowledge can significantly enhance model performance with limited training data. The injection of prior knowledge helps train the model parameters and improve translation performance. However, as the number of training data increases, the impact of prior knowledge diminishes compared to traditional MLE training. D2GPo encourages negative diversity and differs from MLE models in handling high-and low-frequency words in the training set. The comparison between D2GPo and MLE models in handling low-frequency words showed that D2GPo generated more low-frequency words with a higher ratio compared to the baseline model. However, the number of low-frequency words generated by D2GPo was still lower than the golden reference. D2GPo increases model output diversity by generating low-frequency words. Comparison with MLE shows D2GPo generates more low-frequency words but still fewer than the golden reference. Examples include a woman holding an umbrella, an airplane on a runway, a woman with a surfboard, and a traffic light. The models trained with SCST provide more accurate and detailed image summaries, while the models trained with D2GPo produce more grammatically complete sentences."
}