{
    "title": "SkgVRiC9Km",
    "content": "Deep networks have shown impressive results but struggle with data that differs from the training distribution, such as adversarial examples. \"Fortified Networks\" improve deep networks by identifying and mapping hidden states back to where the network performs well, enhancing robustness to adversarial attacks. The study demonstrates improvements in adversarial robustness by fortifying hidden layers instead of the input space in deep neural networks. These improvements are shown across various datasets and attack parameters, enhancing security in critical domains like self-driving cars, healthcare, and malware detection. The system's poor performance raises reliability concerns due to differences in input data distributions during training and evaluation. Adversarial examples result from attacks on neural network models by making small changes to inputs to alter predicted classes. Recent studies show that even subtle modifications can change predicted classes, posing challenges for defenses like feature squeezing, adapted input encoding, and distillation approaches. Existing defenses against adversarial attacks are not easy to deploy and may only provide the illusion of defense. Our approach differs by focusing on robustification of hidden representations rather than generative models in the input space. Experimental evidence supports our hypothesis, showing success in detecting adversarial examples. Fortified Networks are introduced as a method to detect off-manifold examples by training denoising autoencoders on top of hidden layers. This approach robustifies networks against adversarial attacks and provides a reliable signal of input data not on the trained manifold. The text discusses fortifying hidden layers in networks to improve robustness against adversarial attacks. It evaluates the approach on various datasets and explains the Empirical Risk Minimization Framework for model parameter optimization. The text discusses adversarial attacks and robustness in neural networks, highlighting the limitations of the empirical risk minimization framework. It introduces the concept of adversarial robustness as a min-max problem and explains denoising autoencoders for feature learning and generative modeling in deep learning. Denoising autoencoders are used for feature learning and generative modeling in deep learning. They are trained to minimize a reconstruction error or negative log-likelihood of generating the clean input. The autoencoder is trained to reconstruct the hidden unit that corresponds to the clean input, using a corruption function with Gaussian noise. Denoising autoencoders are trained to minimize reconstruction error by using noise to generate clean input. The reconstruction vectors form a vector field pointing towards the data manifold, but may not hold for points distant from it. To address this, DAEs can be inserted between layers to denoise transformed data points away from the original manifold. The method aims to regularize hidden representations by applying a Denoising Autoencoder (DAE) on the hidden layers instead of the raw input signal. This helps in keeping activations on the surface of the data manifold, providing stronger protection from adversarial attacks. Experimental support for these claims is provided in Section 4, where the method substitutes a hidden layer with a denoised version to flatten data manifolds in deeper layers of a neural network. The method regularizes hidden representations by using a Denoising Autoencoder (DAE) on the hidden layers. This helps protect against adversarial attacks by flattening data manifolds in deeper layers of a neural network. The process involves substituting a hidden layer with a denoised version, known as a fortified layer, and training the network with reconstruction and adversarial losses. The method involves using Denoising Autoencoders (DAEs) on hidden layers to regularize representations and protect against adversarial attacks. The fortified network includes DAEs as part of the training process, with a dual objective for the DAEs and tuning parameters for strength. This training process produces robust hidden representations against small perturbations and adversarial attacks. The reconstruction losses act as a reliable signal for detecting off-manifold examples, providing more robust classification results and identifying adversarial or significantly different distribution examples. The placement of fortified layers in hidden states instead of input space raises questions about where exactly they should be placed, with considerations for higher-level hidden layers making it easier for the network to identify off-manifold points. The inclusion of multiple fortified layers across the network enhances its ability to identify off-manifold points and defend against adversarial attacks like the Fast Gradient Sign Method. The Fast Gradient Sign Method (FGSM) is a popular one-step attack that can be effective against many networks. Another attack, the projected gradient descent attack, is slower but stronger. White-box attacks involve knowing the model, while blackbox attacks do not. The Carlini-Wagner L2 attack involves joint optimization of loss maximization and minimizing the distance of the adversarial example to the original example. Evaluating defenses against adversarial attacks is challenging due to gradient masking or obfuscation, which can reduce the effectiveness of gradient-based attacks but may not be robust to adversarial examples. When studying adversarial defense strength, analyzing white-box attacks is crucial. White-box attacks should be at least as strong as black-box attacks. Better defense against white-box attacks may indicate gradient masking. Testing with iterative searches like PGD can reveal if gradient masking is occurring. The test confirms that iterative attacks with small step sizes outperform single-step attacks like FGSM. BID1 discussed the BPDA attack for cases where defenses use non-differentiable transformations. Additional experiments were conducted using the identity function version of BPDA on Fashion MNIST FGSM (\u03b5 = 0.3). Fortified networks with the BPDA version of the attack achieved an accuracy of 89.88\u00b10.30 on Fashion MNIST FGSM (\u03b5 = 0.3), which corresponds to a weaker attack. All autoencoders in the fortified layers used a single hidden layer with tied weights. The experiments used the same attacks at training and test time to generate adversarial examples. The study focused on training an RNN language model with fortified networks to detect when it's given outputs from its own model. The model was trained on the Text8 dataset using a single-layer LSTM with fortified layers between hidden states and output. Results showed that as the number of sampling steps increased, the reconstruction error also increased, indicating that outputs move off the manifold. Various hyperparameters were tested to demonstrate the effectiveness of fortified networks. The study demonstrated the effectiveness of fortified networks in improving performance against adversarial attacks by varying noise levels and reconstruction loss weighting. Using generative models as a defense was considered due to the observation that adversarial examples often lie off the data manifold. The study explored the use of generative models as a defense against adversarial attacks, building on prior research that showed the effectiveness of training against adversarial examples lying on the data manifold. The method proposed considering the manifold in the space of learned representations, using fortified blocks in experiments with CNNs on CIFAR-10 dataset. Different baselines were considered by adding or removing fortified blocks in the architecture. In experiments with CNNs on CIFAR-10 dataset, different baselines were considered by adding or removing fortified blocks in the architecture. One motivation for considering the manifold in the space of learned representations is the simpler statistical structure of the representations, making it easier to model and detect unnatural points. Learning the distribution directly in the visible space is challenging and requires a high capacity model. The use of Deep Image Prior instead of a generative model like a denoising autoencoder was proposed to address challenges in the visible space. Results against white-box attacks on Fashion MNIST and blackbox MNIST attacks with adversarial training were reported, showing comparable performance to previous works. An experiment with adversarial training on Fashion-MNIST using PGD for various \u03b5 values aimed to confirm the success of unbounded adversarial attacks. The use of Deep Image Prior instead of a generative model like a denoising autoencoder was proposed to address challenges in the visible space. Unbounded attacks against Fortified Networks succeed with a sufficiently large \u03b5, evidence against gradient masking. The model may be limited in the types of adversarial attacks it's resistant to and provides no defense against attacks within the range of a convolutional network. DefenseGAN and the Invert-and-Classify approach use an iterative search procedure at inference time. Our approach uses small denoising autoencoders during both training and testing, presenting challenges for evaluation due to possible gradient masking issues. Black-box attacks outperforming white-box attacks can indicate gradient obfuscation, as seen in previous work. Our method shows similar defense quality against both types of attacks. The method involves using autoencoders for defense against black-box and white-box attacks. BID18 used a loss function in the hidden states space, while BID9 matched hidden layer activations to improve robustness. BID15 proposed matching logit values for original samples. The approach involves training a denoising autoencoder in the hidden states of a discriminator in a generative adversarial network to encourage the generator to produce points that are easy to reconstruct. This method aims to denoise adversarial examples and point back to the original samples. The DAE output is encouraged to denoise adversarial examples, pointing back to the original sample's hidden state. Adversarial Spheres study adversarial examples in classifying hollow concentric shells, proving their existence on the data manifold. Training with L adv helps map back from off-manifold points and hard-to-classify points. Fortified Networks is a method for enhancing deep neural networks by introducing Denoising Autoencoders (DAEs) between hidden layers. The DAE reconstruction error at test time serves as a signal for distribution shift, indicating potential adversarial attacks or domain shift. Fortified networks are practical, efficient, and can help protect against adversarial examples in critical applications. Fortified networks improve robustness to adversarial examples by adding fortified layers, with minimal computational cost. They have shown effectiveness in defending against attacks on datasets like MNIST, Fashion MNIST, and CIFAR10, using various attack parameters and methods in both black-box and white-box settings. The experimental setup involved using the Cleverhans library for attacks, with convolutional models having specific layer configurations. The study utilized convolutional and fully-connected Denoising Autoencoders (DAEs) with different activation functions. White-box attacks were conducted using PGD attacks on convolutional DAEs and FGSM attacks on fully connected DAEs. The models were trained using the Adam optimizer with a learning rate of 0.001. Black-box attacks were performed using a fully-connected substitute model to target a fortified convolutional network trained for 50 epochs. The study conducted white-box attacks on convolutional and fully-connected Denoising Autoencoders using PGD and FGSM attacks. A fortified convolutional network was targeted with a black-box attack using a substitute model trained with adversarial training. Jacobian data augmentation was used during training of the substitute model, with attacks of different epsilon values and more attack iterations performed."
}