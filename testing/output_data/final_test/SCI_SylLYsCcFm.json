{
    "title": "SylLYsCcFm",
    "content": "Analogical reasoning in neural networks for visual data involves inducing capacity through careful data selection and presentation, focusing on contrasting abstract relational structures in input domains. This method enables complex analogy making and generalization. Neural networks have capacities for complex analogy making and generalization, allowing for flexible mapping of relations between different domains, similar to how Roman scientists used water wave principles to understand acoustics. This ability to align relational structures between domains, regardless of perceptual differences, is a key aspect of human intelligence and creativity. In this work, neural network architectures demonstrate the ability to learn analogies with generality and flexibility, dependent on a method of training called learning analogies by contrasting abstract relational structures. Neural network architectures can learn analogies using the learning analogies by contrasting abstract relational structures (LABC) method. This approach enables training simple models to apply abstract relations to new domain mappings and unfamiliar domains. The model optimizes stimulus representation and cross-domain mapping jointly, exploring interactions between perception, representation, and inter-domain alignment. The model in the study must identify a relation on a particular domain in the source sequence and apply it to a different domain to find the correct answer panel. Structure Mapping Theory (SMT) distinguishes between analogy and similarity in human analogical reasoning. The High-Level Perception (HLP) theory of analogy posits that analogy is a function of tightly-interacting perceptual and reasoning processes, where stimulus representations and their alignment are mutually dependent. For example, when making an analogy between the sea and acoustics, certain perceptual features are represented while others are ignored. In this work, flexible analogy making in neural networks is induced by combining ideas from SMT and HLP. The key insight, LABC, enhances network performance by focusing on comparing inputs at a more abstract level of relations rather than attributes. This approach organizes training data to emphasize the inference and mapping of relational structures for better problem resolution. In experiments involving greyscale visual scenes, models must analyze source and target sequences to determine the best answer panel by analogy, based on a relation instantiated in the source sequence. In visual analogy tasks, candidates must use analogical reasoning to select the correct answer panel based on relations instantiated in the source sequence across different domains like line type, color, shape type, etc. The correct answer shares the same relation as the source domain, while incorrect candidates are consistent with target domain attributes but not relations. The dataset for visual analogy tasks involves a panel with attributes taking 10 possible values, defined by a relation, domain, and values. Despite a small number of factors, the space of possible questions is vast, allowing for increasing levels of abstraction. The dataset for visual analogy tasks involves a panel with attributes taking 10 possible values, defined by a relation, domain, and values. Questions require increasing degrees of abstraction and analogy-making, from simple cases to full analogy questions involving different domains in source and target sequences. The model used a CNN and RNN to evaluate candidate answers based on source and target sequence embeddings. The correct answer choice reflects relational structures in the target sequence. Model details can be found in appendix 7.1. Learning Analogies By Contrasting (LABC) involves selecting incorrect candidate answers from the same domain as the correct answer to prevent trivial matching and encourage the model to consider the source sequence for inferring the correct structure. This method aims to reduce the likelihood of models finding perceptual correlations to arrive at the correct answer consistently during training. The LABC regime involves training models to contrast abstract relational structures by ensuring incorrect answers are perceptually and semantically plausible. This method encourages analogical reasoning by completing the same relation in the target domain as observed in the source domain during training. In experiments, 600,000 training questions were generated along with 10,000 validation questions and 100,000 test questions. The process of analogy-making involves comparing two domains. Models were tested on analogies with unfamiliar domain transfers. Networks can learn to apply relations by analogy involving novel domain transfers. The ability to apply relations by analogy involving novel domain transfers relies on learning by contrasting. The model trained by contrasting achieves 83% accuracy for test questions with contrasting candidate answers. Humans can use analogies to understand unfamiliar domains. Two domains were held out from the model's training data to test its ability to generalize knowledge to completely novel domains. The model trained by contrasting can apply relations by analogy to novel domains, exploiting perceptual similarity between training and test domains. It can make sense of unfamiliar target domains in test questions, with accuracy boosted by LABC. Accuracy in the LABC condition is lower than in Experiment 1 but higher than random answer candidates. The model trained with contrasting candidates can apply relations by analogy to novel domains, improving accuracy in unfamiliar target domains. Interleaving random-answer and contrasting candidates during training helps recover deficits in performance on perceptually plausible test questions. Interpolation and extrapolation tasks reveal the model's ability to resolve more challenging questions involving darker, larger, or more-sided shapes. The study found that models trained with random candidates perform poorly on challenging contrasting test questions, indicating overfitting to a non-human-like reasoning strategy. A 'source-blind' model trained in the normal regime achieved high accuracy but lacked analogical mapping skills. In contrast, the source-blind model in the LBAC condition had lower accuracy. LBAC training resulted in a modest improvement in extrapolation to novel input values. The study found that models trained with contrasting and random candidate answers outperformed the normal model on test questions. LABC training improved generalization across different network architectures, confirming its effectiveness regardless of specific model design. The study confirmed that LABC training improves model generalization across different architectures, showing better performance on mixed test sets. It enhances models' ability to generalize beyond their training data distribution, especially in novel domain mappings and unfamiliar target domains. Additionally, LABC training results in moderate improvements in extrapolating to perceptual input outside the training range. In analyzing neural activity in models trained with LABC compared to those that were not, it was found that LABC encourages the model to represent relations more explicitly, leading to better generalization by analogy to novel domains. This contrasts with models trained normally, where relation-based clusters overlap more. This suggests that LABC training improves the model's ability to generalize across different architectures and unfamiliar target domains. In a symbolic analogy task based on feature-based stimuli, the construction of incorrect answer candidates can be learned in a proposal model trained jointly with a model that contrasts the candidates. This widens the potential applications of LABC to settings where the abstract relational structure is not clearly understood. Inputs are D-dimensional vectors of discrete features, each corresponding to a domain, and abstract relational structures are simulated using common mathematical functions operating on sets. The task involves studying generalization by aligning structures from different domains in a symbolic analogy task. The model structure is similar to previous experiments, with candidates and context processed independently to produce scores. The candidates are trained using a cross-entropy loss. See appendix 7.2 for more details. The task involves training candidate answers C by sampling functions randomly and populating C with the sampled functions adhering to relational structures. This training regime, known as LABC-explicit SMT, requires knowledge of the underlying data structure. Models trained with LABC achieved accuracies of just under 90% in the visual analogy task, outperforming models trained without LABC. Models trained with LABC achieved accuracies of just under 90%, outperforming models that do not use back-door solutions. By generating random vectors as candidates, models can embed all possible functions and choose the correct answer through process of elimination. This approach does not require analogical reasoning and allows the model to ignore the relation instantiated in the source set. This results in better performance at test time, as the model is forced to be more discerning during training. The model trained with LABC achieved high accuracies by generating random vectors as candidates and choosing the correct answer through process of elimination, resulting in better performance at test time. The model must be able to flexibly apply functions in novel ways, as demanded in the test set. The model trained with LABC achieved high accuracies by generating random vectors as candidates and choosing the correct answer through process of elimination, resulting in better performance at test time. This method improves performance from chance (25%) to approximately. The top-k method improved performance from chance to 77% by exploiting random sampling. However, for more difficult problems, random generation may not be suitable. Excluding randomly generated candidates that satisfied a certain condition decreased performance to 43%. Another method, LABC-adversarial, used a generator model instead of random generation. The generator model proposed candidate vectors to improve the analogy model's test performance from chance to approximately 62%. This method shows links between LABC, GANs, and self-play as automated methods to approximate LABC. Our experiments demonstrate the effects of adversarial training on out-of-distribution generalization in analogy-making. Training neural networks with contrastive examples at the relational level improves their ability to make analogies with visual and symbolic inputs, aligning with the SMT of human analogy-making. Our model, trained by LABC to reason better by analogy, shows improved ability to extrapolate to a wider range of input values. Neural networks can generalize to data outside the training distribution with careful learning, emphasizing the importance of data and learning methods. Learning was once considered difficult with neural networks, but advancements in training objectives, models, and optimization have solved this issue. Insights from this research can help develop general approaches for abstract reasoning. Previous work on analogical reasoning used symbolic stimuli, but newer methods show analogies can be made using non-parametric operations on word representations. This study demonstrates flexible analogy making in neural networks learning from raw data. This study demonstrates that even basic neural networks can exhibit strong analogical reasoning and generalization, showcasing the potential for flexible analogy making in neural networks learning from raw data. The selection of negative examples in machine-learning contexts can significantly impact generalization, with methods like self-play playing a crucial role in enhancing learning algorithms involving negative examples. The power of methods like self-play BID32 is explained, where a model challenges itself with increasingly difficult learning tasks. Analogies are used to illustrate how a single concept can be applied in different scenarios, suggesting a flexible cognitive system. Analogies are crucial for replicating human-like cognitive processes in machines. Past AI research on analogy should be revisited with modern tools and computing power. The CNN used in the study had 4 layers with 32 kernels per layer, downsampling the image by half. Each question panel was 80x80 pixels and greyscale, presented one at a time to the CNN. The study utilized a CNN with 4 layers and 32 kernels per layer to process 80x80 pixel greyscale question panels. The CNN generated embeddings for source, target, and candidate sequences, which were then inputted into an RNN with 64 hidden units. The RNN produced scalar scores for each candidate, which were used to determine the model's answer through a softmax function. The model was trained using cross-entropy loss and the Adam optimizer with a learning rate of 1e-4. The experiment involved using 16-dimensional vectors with binary tags to test domain-transfer generalization. The model was designed to easily identify relevant dimensions at test time, focusing on the effect of LABC. The experiment used 16-dimensional vectors with binary tags to test domain-transfer generalization. A Relation Network replaced the RNN core for processing dimensions i and j. The model outputted scores for different candidate vectors, trained using cross entropy loss function with batch sizes of 32 and Adam optimizer. The experiment utilized a Relation Network with cross entropy loss function and batch sizes of 32. A ResNet-50 processed panels simultaneously, while a parallel relation network model achieved the highest generalization accuracy of 95% on the domain-transfer test set. The experiment achieved a high accuracy of 95% on the domain-transfer test set using a Relation Network model. The model trained with LABC showed good generalization behavior and learned configurations at an abstract semantically-meaningful level. Test set performance varied between good and poor solutions, indicating the importance of finding configurations that generalize well. Training with LABC data yields higher performance on test questions with semantically-plausible candidates, showcasing the model's ability to specialize to specific problem types. This specialization effect outweighs the marginal improvement seen in models trained in the normal regime. The benefits of LABC training are particularly evident in cases where the exact details of test questions are unknown, suggesting potential transferability to a wider range of learning and reasoning problems. The importance of teaching concepts by contrasting with negative examples is established in cognitive science and educational research. Training modern neural networks with this principle can replicate human-like cognitive processes and improve generalization. Selecting plausible negative examples based on abstract principles can lead to more robust representations. Trained networks can resolve analogies effectively. Trained networks can resolve analogies quickly, similar to human visual reasoning. Approaches like LBC focus on improving input embeddings for classification using methods like LMNN and triplet loss. These methods aim to minimize distances within the same class while maximizing distances between different classes. The goal of LBC is to induce out-of-distribution generalization by improving abstract understanding of the problem, focusing on the quality of data in incorrect classes. It can be used in conjunction with previous approaches like LMNN or triplet loss. LBC shares similarities with generative adversarial active learning approaches. Active learning approaches like BID38 do not explicitly consider the effects of the quality of incorrect samples on out-of-distribution generalization. There are no experiments testing abstract generalization using networks trained with generative samples."
}