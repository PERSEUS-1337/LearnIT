{
    "title": "BJgqqsAct7",
    "content": "Modern neural networks are highly overparameterized, with the capacity to overfit to training data, yet they often generalize well. Trained networks can be compressed to smaller representations, leading to state-of-the-art generalization guarantees. The compressibility of models that tend to overfit is limited, as an increase in overfitting requires more bits to describe a trained network. This paper connects the empirical observations of network compression and generalization guarantees, providing the first non-vacuous generalization guarantees for realistic architectures on the ImageNet classification problem. The text discusses the performance of deep neural networks despite overparameterization and their ability to generalize well. Various approaches have been explored to explain the generalization performance of neural networks, including adapting classical measures and modifying training procedures. However, these approaches have not significantly improved performance in practice. Empirical evidence suggests that compressing trained neural networks can lead to equivalent models with smaller sizes. A new generalization bound based on the compressed size of neural networks has been established, showing effectiveness in the deep-learning regime. This measure is different from other complexity measures and has shown to improve generalization bounds in practical problems. The complexity of a learning problem is defined as the compressed size minus remaining structure. This concept links generalization to explicit compression and corrects for irrelevant information in neural network representations. Accounting for this can reduce complexity and improve generalization guarantees. Generalization bounds can be evaluated by compressing trained networks and substituting the effective compressed size into the bound. Using off-the-shelf neural network compression schemes with this recipe yields state-of-the-art bounds, including the first non-vacuous bounds for modern convolutional neural nets. A model's overfitting tendency sets an absolute limit on its compressibility, with the entropy of the classifier being lower bounded by the expected degree of overfitting. Increased overfitting leads to worse compressibility, as shown empirically through randomization tests. The relationship between small model size and generalization is a variant of Occam's razor, contradicting the use of highly overparameterized models in deep learning. The study of generalization and compression in deep learning has been disjoint, with compression primarily motivated by computational and storage limitations. Occam type arguments remain powerful in the deep learning regime, linking compression and generalization. The study discusses the problem of generalization in deep learning, emphasizing the need to control generalization error by bounding models that are plausible outputs of real-world data training. The PAC-Bayesian framework is used to encode prior beliefs about plausible models, with a focus on developing a bound within this framework. In the PAC-Bayes framework, the challenge lies in articulating a distribution that encodes plausible model outputs. Good compression assigns short codes to probable models, reflecting their plausibility. Statistical learning theory is used to learn a classifier from data examples, aiming to predict labels from features using a chosen hypothesis and loss function. The quality of prediction is measured using a loss function, with the overall quality of a hypothesis determined by the risk under the data generating distribution. Training data is used to estimate the true risk through empirical risk, and the learner's task is to choose a hypothesis from a set of possible hypotheses that minimizes the empirical risk. This can lead to overfitting to the training data. In this paper, the focus is on image classification using a deep neural network and the 0-1 loss function. The PAC-Bayesian framework is utilized to establish bounds on generalization error by comparing a pre-specified random classifier (prior) with the classifier of interest (posterior). The goal is to control overfitting by measuring the discrepancy between these classifiers. The PAC-Bayesian theory establishes bounds on generalization error by comparing a random classifier (prior) with the classifier of interest (posterior). The quality of the bound depends on the difference between the prior encoding plausible models and the actual output of the learning procedure. The PAC-Bayesian theory establishes bounds on generalization error by comparing a random classifier (prior) with the classifier of interest (posterior). The challenge lies in finding a good PAC-Bayes prior \u03c0 with a small and computable KL(\u03c1, \u03c0) value. Previous work has focused on identifying properties of real-world networks that explain good generalization behavior, with generalization bounds being loose relative to true generalization error. These bounds serve to provide qualitative evidence of the property's impact on generalization. The present paper proposes compressibility as a key signature of performant real-world deep nets and provides qualitative evidence for this thesis through a generalization bound. Previous work has explored the connection between compression and generalization in machine learning, showing that noise-stability implies the existence of a simpler network with similar performance. The present paper introduces compressibility as a crucial factor for high-performing deep neural networks and offers evidence through a generalization bound. It contrasts with previous work by utilizing existing compression algorithms to establish non-vacuous bounds, linking generalization and compression. The paper also highlights the difference in compression schemes used and raises questions about the properties of deep networks that allow for sparse compression. The paper discusses the importance of compressibility in deep neural networks and presents a generalization bound based on the insensitivity of deep networks to weight perturbations. It references previous studies using a PAC-Bayes approach to analyze the tolerance of deep nets to noise and the difference between deterministic and stochastic networks. The paper emphasizes the role of compression algorithms in establishing non-vacuous bounds and raises questions about the properties of deep networks that allow for sparse compression. The curr_chunk discusses the generalization guarantees for deep neural networks, particularly focusing on compressibility and perturbation robustness. The bound provided is the first non-vacuous guarantee for the ImageNet classification task, applicable to both fully connected and convolutional networks. The effectiveness of the work relies on good neural network compression algorithms. The curr_chunk discusses neural network compression algorithms and a simple Occam's razor type bound for generalization. It emphasizes the importance of choosing a PAC-Bayes prior that assigns greater probability mass to models with short code length. This approach is compatible with most forms of compression and is embedded in the PAC-Bayesian framework. The framework combines the compressible structure of trained networks with different properties. A non-random classifier is considered using the PAC-Bayes posterior \u03c1 as a point mass at the output of the training procedure. The PAC-Bayes bound reduces to computing KL(\u03c1, \u03c0). The number of bits required to represent hypothesis h is denoted by |h| c. A prior \u03c0 c exists such that it relies on the quality of the chosen coding, independent of lossy compression. The code c reflects explicit structure like sparsity imposed by compression. The text discusses the compressible structure of trained networks and the use of a prior \u03c0c that relies on the quality of chosen coding, independent of lossy compression. It mentions the explicit structure imposed by compression, such as sparsity, and the computation of KL-divergence to obtain results. The text also suggests a pragmatic solution for selecting a model size and considering m to be uniform over all possible lengths. Compression schemes often fail to exploit structure in the hypothesis space H, leading to suboptimal generalization bounds. Trained neural networks show tolerance to discretization and added noise in weights, making quantization a crucial step in compression strategies. A PAC-Bayes bound is constructed to reflect this structure, requiring a compression scheme outputting a triplet (S, C, Q) where S represents non-zero weight locations, C is a codebook, and Q denotes quantized values. Most advanced compression schemes can be formalized accordingly. Compression schemes can be formalized by defining weight values and utilizing quantization. The generalization error of a stochastic estimator can be bounded by applying random noise to network weights. A compression scheme outputs a triplet (S, C, Q) where S represents non-zero weight locations, C is a codebook, and Q denotes quantized values. A PAC-Bayes prior can be constructed to optimize compression strategies. Compression schemes involve defining weight values and using quantization. The generalization error of a stochastic estimator can be bounded by adding random noise to network weights. A compression scheme outputs a triplet (S, C, Q) where S represents non-zero weight locations, C is a codebook, and Q denotes quantized values. A PAC-Bayes prior can be constructed to optimize compression strategies. The KL-divergence of a distribution with a unnormalized measure is discussed, with the construction of the prior and proof deferred. Examples are presented combining theoretical arguments with neural network compression schemes. Other approaches to bounding generalization error for deep neural networks often yield vacuous bounds, except for BID10. Neural networks struggle to provide accurate bounds for realistic problems, except for BID10 which retrain the network to optimize generalization. Applying generalization bounds to compressed neural net models leads to non-vacuous bounds on realistic problems. The Occam bound shows a strong connection between compressibility and generalization. Confidence bounds are reported based on the effective compressed size of networks, achieved by combining PAC-Bayes bound Theorem 2.1 with Theorem 4.3. A small technical modification involves choosing the prior variance layerwise through grid search. The experiment involves pruning and quantizing the LeNet-5 network on the MNIST dataset. Dynamic Network Surgery is used to prune all but 1.5% of the network weights, followed by quantization with a 4-bit codebook. Gaussian noise is added to each non-zero coordinate for the stochastic classifier. The experiment involves adding Gaussian noise to non-zero coordinates before each forward pass. The noise has a standard deviation of 5% of the weight difference in the filter. This results in a slight decrease in classification performance. The training error bound is 46% with 95% confidence. The compressed model size is 6.23 KiB. ImageNet is a dataset with 1.2 million natural images categorized into 1000 classes, more complex than MNIST. Models like AlexNet and VGG-16 have millions of parameters, making non-vacuous bounds challenging with current compression techniques. Motivated by computational restrictions, there is interest in designing more parsimonious architectures with fewer parameters. By combining neural net compression schemes with parsimonious models, non-vacuous bounds on models with better performance than AlexNet are demonstrated. For example, SqueezeNet architecture achieves better performance than AlexNet with a model size of 0.47 MiB. Applying a na\u00efve Occam bound yields a non-vacuous bound on test error of 98.6%. The study focuses on compressing a network from scratch using MobileNet 0.5 BID18, which outperforms SqueezeNet in its uncompressed form. Pruning techniques from Zhu & Gupta (2017) are applied, with 67% of parameters pruned to achieve a validation accuracy of 60%. Weight quantization and a stochastic classifier with Gaussian noise are used. Biases and batch normalization parameters are ignored due to their minimal impact on the model. Top-1 accuracy is considered for prediction performance evaluation. The study focuses on compressing a network from scratch using MobileNet 0.5 BID18, outperforming SqueezeNet. Pruning techniques are applied, with 67% of parameters pruned to achieve a validation accuracy of 60%. Weight quantization and a stochastic classifier with Gaussian noise are used. Biases and batch normalization parameters are ignored. Top-1 accuracy is considered for prediction evaluation. The final \"effective compressed size\" is 350 KiB. Compression results imply generalization bounds, showing overfitting limits compressibility. The text discusses the relationship between overfitting and the ability of an estimator to distinguish between training and testing data. It presents a theorem related to discrete sample spaces and hypothesis sets, with a non-negative function denoted as 'g'. The proof is deferred to supplementary material. Empirical experiments are conducted to study this effect. In empirical experiments, the randomization test of BID39 is used to study the effect of model capacity on generalization error. The ResNet BID15 architecture is fitted with varying levels of label randomization on the CIFAR-10 dataset, showing that models with worse generalization require more bits to describe. The CIFAR-10 dataset labels undergo randomization, leading to 100% training accuracy regardless of the level of randomization. Networks can achieve 50% sparsity with no loss in accuracy, even with completely random labels. However, higher compression levels result in faster accuracy decay, indicating that network size influences generalization error. Despite the large model capacity in deep learning, good generalization performance is observed without modifications in the training and compression pipeline. The standard engineering pipeline of training and compressing a network shows demonstrable generalization guarantees without heavy regularization or modifications. The connection between compression and generalization raises important questions about limitations and potential improvements in compression rates for practical value. The construction of the prior \u03c0 and the bound on the KL-divergence are described in this section. The prior is expressed as a weighted mixture over all possible triplets, with the code length of the triplet determining the weight. The prior is defined as a mixture over all triplets, with S and C values representable by the code, and Q values in a chosen finite subset of real numbers. The proof of Theorem 4.3 involves the compression algorithm outputting a triplet (\u015c,Q,\u0108) with a normal posterior \u03c1 centered at w(\u015c,Q,\u0108) and variance \u03c3 2. The KL-divergence is computed, with a mixture term that is computationally challenging due to its independence across coordinates. The mixture can be expressed as a weighted sum of univariate normal distributions. The proof of Theorem 4.3 involves decomposing the KL-divergence over coordinates and using a stochastic estimator \u03c1. The bound requires selecting parameters like prior variances, which can be adjusted for data-dependent parameter selection. Theorem A.1 extends the bound to account for discrete parameter priors. The proof involves decomposing the KL-divergence over coordinates and using a stochastic estimator. Parameters like prior variances are selected for data-dependent parameter selection. The baseline model for LeNet-5 is trained using stochastic gradient descent with momentum and no data augmentation. The batch size is 1024, learning rate decays every 125 steps, and a penalty of 0.005 is applied. Training lasts for 20000 steps. The pruning process involves using Dynamic Network Surgery BID12 with a threshold selected per layer based on the mean and standard deviation of coefficients. The pruning probability starts at 1.0 and decays to 10^-3. Training is done for 30000 steps with the ADAM optimizer, and weights are quantized using a 4-bit codebook initialized with k-means. MobileNets utilize depthwise separable convolutions with each layer consisting of a depthwise and pointwise convolution. The pruning process involves using Dynamic Network Surgery with a threshold set for each weight based on quantiles of absolute values. The target sparsity for each layer is scaled linearly between 65% and 75% of the number of elements in the layer. Stochastic gradient descent with momentum is used for training, starting at 10^-3 and decaying by 0.05 every 2000 steps. Training is done with a minibatch size of 64 for a total of 300000 steps, with a tuning of the pruning schedule. We tune the pruning schedule to reach target sparsity after 200000 steps. We quantize weights using codebooks with 6 bits for most layers and 5 bits for the last fully connected layer. Cluster assignment is initialized using k-means and trained for 20000 steps with stochastic gradient descent. Batch normalization moving average parameters are modified for faster adaptation. Gaussian noise is added to pointwise and fully connected layers for noise robustness. Gaussian noise is added to pointwise and fully connected layers for noise robustness during training. The noise levels were carefully chosen to minimize training performance degradation while improving generalization. The top-1 training accuracy decreases from 67% to 65% with noise applied. The entropy of the model is lower bounded by the entropy of its \"memory\". The quality of the model as a discriminator between training and testing sets is quantified by the average proportion of false positives and true negatives. Theorem C.1 states that if the quantities of a classifier are significantly different from a random classifier, then it must have high entropy. The proof involves sampling pairs from a distribution, viewing the classifier as a random quantity, and considering a sequence of pairs with the same distribution as if they were sampled from a specific procedure. The distribution of a Bernoulli random variable does not depend on i due to the assumption that \u0125 is measurable with respect to the sample. The distribution of Bi conditional on the event where L0i = 0 is computed, and the binary entropy function is denoted as hb(p). The chain rule for entropy is applied to derive the final result."
}