{
    "title": "B1lnjo05Km",
    "content": "Deep neural networks can learn complex data representations, but these are often difficult to interpret visually due to high dimensions and arbitrary node ordering. To address this, a new approach called \\textit{Graph Spectral Regularizations} is proposed, which imposes graph structures on latent layers using graph filters like low pass and wavelet filters. This method leverages human pattern recognition to visualize higher dimensions effectively. The graph-structured layer with wavelet-like filters reveals topological features, enforces consistent node ordering in capsule nets, and forms localized receptive fields for image interpretation. This approach helps clarify the mapping between latent layer neurons and output space, enabling coherent image representations for image-processing techniques like convolutions. The introduction of Graph Spectral Regularizations aims to address the challenge of neural networks being black boxes to users by filtering activations on a predefined graph, enabling better interpretation of the latent encoding. The introduction of a Laplacian smoothing regularization in neural networks enforces smoothly varying activations and reconstructs data, useful for learning features with specific topologies. It aligns features in capsule networks and encodes transformations of digits. The data features have a recognizable topology, and a filter is designed to extract the data's topology by encouraging the graph structure layer to learn data-shape features. Spatially localized Gaussian filters are used to detect circular and linear topologies not reflected by observed features. A single-cell protein expression dataset depicting T cell development in the thymus shows a Y-shaped topology in the graph structured layer. The graph structured layer reveals a Y-shaped topology reflecting T cell biology, and when imposed on a 2D grid, creates a \"pseudo\" image for analysis by convolution layers. The convolution and max pooling functions may obviate the need for a spectral bottleneck. Contributions include a framework for graph structure on latent layers, Laplacian graph smoothing regularization, and spatially localized graph regularizations. Spatially localized graph regularizations using a spectral bottleneck based on a dictionary of Gaussian Kernels for recognizing data topology. The paper discusses applications of graph spectral regularizations and demonstrates improved interpretability in artificial neural networks. The text discusses the lack of clear topological structure in neural network representations, especially in unsupervised and exploratory tasks. The text discusses the challenge of interpreting neural network representations in unsupervised tasks due to their lack of structure. A new approach is proposed to impose a graph topology on the neurons in hidden layers to create human interpretable patterns. The text proposes imposing a graph topology on neurons in hidden layers to control spectral properties of latent representations, enabling recognizable patterns. Three ways are suggested to utilize this structure: spectral regularizations for signal smoothness, sparse graph wavelet dictionaries for encoding inputs, and bottleneck creation for sparse dictionary atoms. The text discusses the use of graph structures in neural networks to control spectral properties of latent representations. It explores different design choices such as spectral regularizations, sparse graph wavelet dictionaries, and bottleneck creation for sparse dictionary atoms. The graph structure defined by the weighted adjacency matrix provides a notion of locality and extends traditional signal processing and harmonic analysis notions. The text discusses the use of graph Laplacian in neural networks, providing a discrete version of manifold Laplace operators. The eigenvectors of the Laplacian are considered graph Fourier harmonics, converging to discrete Fourier harmonics on a ring graph. This leads to the definition of the graph Fourier transform using a matrix \u03a8. When applied to a neuron-activation signal \u03bd, the graph Fourier coefficients are obtained as \u03bd = \u03a8 T \u03bd associated with the graph frequency \u03bb j. The graph Fourier transform, defined by matrix \u03a8, allows for analysis of neuron activations in both neuron-domain and spectral domain representations. The Fourier coefficients of the activations provide a new way to apply regularizations in the spectral domain. The graph Fourier transform allows for analysis of neuron activations in both neuron-domain and spectral domain representations. Regularizations in the spectral domain directly enforce spectral properties of the neuron activation signal, providing stability to the optimization process. In the spectral domain, spectral L2 regularization can be weighted by functions of squared-frequencies, using weights \u00b5 j = \u00b5(\u03bb j). This regularization cannot be directly applied in the neuron domain, but can be achieved by considering the matrix form and graph Fourier transform. The L2 spectral regularization term in the neuron domain is defined by a matrix T that can be computed in advance from the neuron graph structure and spectral weights in \u00b5. Different weighted norms can be used for spectral regularization, with Laplacian smoothing focusing on L2 spectral regularizations with nonnegative weights. These spectral regularizations guide the latent representation. Spectral regularization in neural networks guides latent representations by penalizing high frequency components, promoting smoothness. Laplacian smoothing, using nonnegative weights, is a common choice for this regularization, resulting in a quadratic loss term. This approach produces interpretable latent representations in hidden layers. The graph Fourier transform uses a dictionary of representative atoms to represent signals, capturing spatial or spectral patterns. To include spatial locality, bandlimited filters like graph wavelets or translated Gaussians are considered. The dictionary coefficients computed by \u03d5 provide a dictionary-based extension of the Fourier coefficients computed by the graph Fourier transform, allowing for spectral regularization. Additionally, the dictionary coefficients can serve as an information bottleneck in neural networks, inspired by sparsity. Utilizing a new information bottleneck inspired by sparse representations in compressive sensing, the neural network is forced to project its neuron activation signal onto a single dictionary atom chosen adaptively based on the input. This bottleneck splits the graph-structured layer into two parts before and after the bottleneck, filtering the signal to pass on only one atom from the dictionary to subsequent layers. The activations from this filtered signal are then forwarded to the next layers in the network. In Section 3, a spectral bottleneck using graph-translated Gaussians as dictionary atoms forces individual inputs to be projected onto local regions of the neuron graph. This organizes the latent representation into \"receptive fields\" capturing trends in the input data based on the graph structure. Topological Inference involves graph spectral regularization on data with a hierarchical cluster structure, demonstrating the use of Laplacian Smoothing Regularization. In Section 3, a spectral bottleneck using graph-translated Gaussians organizes the latent representation into \"receptive fields\" capturing trends in the input data based on the graph structure. A graph of six nodes with three pairs of connected nodes detects large scale clusters, with each pair of nodes encoding one of the two Gaussian substructures. The model with Laplacian smoothing regularization extracts the hierarchical topology of the data. In the model with Laplacian smoothing, the structure of the data is clear, distinguishing between different subclusters. Capsule networks use 10 capsules of 16 nodes to represent digits semantically. Training on the MNIST dataset reveals meaningful differences in digit representation such as scale, skew, and width. The capsule net trained with Laplacian smoothing regularization enforces a consistent feature ordering, making the encoding more interpretable. This allows for the generation of data with specific properties, as demonstrated in FIG0. The Laplacian smoothing regularization enforces consistent feature ordering in capsule nets, making encoding more interpretable. It also introduces spectral bottleneck regularization to align features with data topology. The addition of kernel-shaped filters for activations in regularization helps encode different parts of the graph in layer features. Spatially-localized filter regularizations are beneficial for learning characteristic features of data topology, as demonstrated with biological mass cytometry data on T cell development. The spectral bottleneck encodes branches in specific nodes, revealing protein features that characterize different parts of the cell progression. Activation heatmaps highlight major distinctions in the data topology. The activation heatmap reveals key differences between branches in the cell progression. Node 18 acts as a switch between lineages, while nodes 6-12 show varying levels across branches. The architecture diagram shows the spectral bottleneck layer's role in identifying linear structures in the data. Adding Laplacian smoothing and a spectral bottleneck layer to the autoencoder reveals a clear one-dimensional structure in the data, showing differentiation levels in T cells. The spectral bottleneck offers meaningful features for characterizing data topology and can be applied to higher-dimensional datasets. Heatmaps of embedding layer activations sorted by branch and trajectory labels provide insights into cell progression. The data includes T Cell developmental trajectories with gene correlation plots showing interpretable embeddings split into CD4+ and CD8+ branches. Graph spectral regularization enhances visualization, creating meaningful representations compared to unstructured activations without regularization. Using Laplacian smoothing on a 2D lattice graph enhances visual distinguishability. The embedding is treated as an image, allowing for the use of standard convolutional architecture with 3 layers of 3x3 2D convolutions and 2x2 max pooling. This compresses digit representations into specific areas, enabling quick visual categorization. The high dimensional embedding layer is segmentable with receptive fields for each digit, making it suitable for classification. The classification penalty and graph-structured layer induce spatial localization without the need for a spectral bottleneck layer or localized filter regularization. The text discusses the use of convolutions and max pooling to induce spatially localized features in a 2D grid. It also mentions segmenting the embedding space by class to localize portions associated with each class. Additionally, it shows the top 10% activation on the embedding of sample images and highlights regions correlating with the semantic description of the digit type. The dataset analysis includes graph smoothing regularization and heatmaps displaying activation strength of datapoints. Different values of \u03b1 affect the heatmap's meaningfulness, with lower \u03b1 resulting in activations of lower period. A linear dataset is simulated by sequential feature activation, generating labels y by sampling numbers between zero and ten. Gaussian noise is added to 60,000 data points. Laplacian smoothing and spatially-localized analysis are conducted. In the analysis, Laplacian smoothing and spatially-localized representations are applied to images of a rotating teapot. A graph spectral layer with a ring topology of 20 nodes is used, with varying levels of smoothness coefficient showing different activation patterns. Leaky relus with a coefficient of 0.2 are utilized for most layers. The autoencoder used in the analysis has five fully connected layers with specific widths. Laplacian smoothing is applied by adding a penalty term to the loss function. A spectral bottleneck layer is created by transforming the activation vector into the spectral domain and then back to the neuron domain. Low frequency filters are encouraged over high frequency filters in the network. To encourage learning of low frequency filters over high frequency, Laplacian smoothing is applied in a spectral bottleneck. The classifier used includes two convolution and max pooling layers followed by a dense layer with Laplacian smoothing. Cross entropy loss is used for training. The architecture includes convolutions and max pooling layers after the Laplacian smoothing layer. The Laplacian smoothing layer is followed by convolution and max pooling layers to construct a 2D image."
}