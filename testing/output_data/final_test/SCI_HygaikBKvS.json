{
    "title": "HygaikBKvS",
    "content": "The text discusses combining actor-critic reinforcement learning with large-scale experience replay to address challenges in efficient learning and stability. It proposes solutions for accelerating hyper-parameter sweeps by sharing experiences among agents. The analysis focuses on bias-variance tradeoffs in V-trace and suggests mixing replayed experience with on-policy experience for improved performance. The proposed solution demonstrates state-of-the-art data efficiency on Atari among agents trained up until 200M environment frames. Value-based and actor-critic policy gradient methods are leading techniques in reinforcement learning. Experience replay agents store and reuse experiences to improve data efficiency and generalize prioritized sweeping to non-tabular settings. Experience replay is a technique that generalizes prioritized sweeping to non-tabular settings and reduces the number of interactions required by deep reinforcement learning algorithms. It can include expert trajectories to simplify exploration and is often combined with Q-learning for off-policy learning. Actor-critic algorithms with replay maintain Q-functions to learn from off-policy experiences. Off-policy actor-critic learning with experience replay can be achieved without surrogate Q-function approximators using V-trace. Mixing off-policy replay experience with on-policy experience is crucial for convergence to a locally optimal solution. A trust region scheme can mitigate bias and enable efficient learning in a strongly off-policy regime where agents share experience through a commonly shared replay module. In Section 2, the paper revisits pure importance sampling for actor-critic agents and V-trace, which allows for bias-variance trade-off in estimates. Proposition 2 states that off-policy V-trace may not converge to a locally optimal solution, even with the optimal value function. Mixing on-policy experience into experience replay alleviates this issue. A trust region scheme in Section 4 enables efficient learning in a strongly off-policy regime where agents share experience replay. The trust region in policy space is defined to improve return estimation. State-of-the-art data efficiency is demonstrated across 57 Atari games and DMLab30. Sharing experience between agents leads to more efficient hyper-parameter sweeps. The best agent from hyper-parameter sweeps with and without experience replay is presented. The study compares state-of-the-art agents on Atari games and DMLab-30, using hyper-parameter sweeps and V-trace importance sampling for actor-critic agents. LASER (LArge Scale Experience Replay) agent is proposed for improved performance. In this section, the text discusses how V-trace controls variance in actor-critic agents, leading to biased estimates and a failure mode affecting policy gradient. Solutions for these issues are presented in Section 4, with experiments showing the impact of combining on-policy data with experience replay for improved data efficiency. The text discusses the impact of capacity in experience replay on data efficiency. Sharing experience between agents in a hyper-parameter sweep fails, but a trust region estimator mitigates the issue. Combining population-based training with trust region estimation further improves performance. In reinforcement learning, the agent selects actions, receives rewards and observations, and aims to find an optimal policy that maximizes cumulative returns. Off-policy learning involves finding or evaluating a policy from data generated by a different policy, with techniques like experience replay and decoupling acting from learning. In reinforcement learning, decoupling acting from learning causes experience to lag behind the latest agent policy. Learning multiple general value functions or options from a single stream of experience is useful. On-policy n-step bootstraps provide more accurate value estimates with larger n, but the variance also increases with n. Multi-step importance sampling can be used to achieve benefits similar to n-step returns in the off-policy case, but it adds another potential source of variance to the estimate. Importance sampling estimates the expected return V \u03c0 from trajectories sampled from \u00b5 = \u03c0, using a previously estimated value function V as a bootstrap. Multi-step formulations of the expected return involve temporal difference errors and policy updates. Techniques like Tree Backup and Q(\u03bb) address high variance in importance sampling estimates, while RETRACE uses full returns in the on-policy case. V-trace reduces variance in importance sampling by trading off variance for a biased estimate of the return. It uses clipped importance sampling ratios to approximate the return and fully recovers the Monte Carlo return when on policy. The V-trace value estimate V\u03c0 is biased and does not match the expected return of the policy \u03c0, but rather a related implied policy defined by equation 3 that depends on the behavior policy \u00b5. This bias increases with the difference between \u03c0 and \u00b5, leading to potentially significant differences in the estimated values. The V-trace policy gradient is biased as it does not converge to a locally optimal value function V* and estimates values poorly, leading to significant differences in estimated values. The V-trace policy gradient does not converge to a locally optimal value function V* and can result in biased Q-estimates, leading to distorted policy gradients. The policy may be biased if the Q-function is distorted too much, affecting the selection of optimal actions. The regret is the difference between the maximum and 2nd largest value. The more distorted the Q \u03c9, the larger the regret compared to the optimal policy. Mitigation is provided by Proposition 3, showing that the V-trace policy gradient will converge to the true on-policy gradient with a sufficient proportion of on-policy experience. Choosing the right \u03b1 value in policy learning is crucial for accurately selecting the best action at a given state. The gap between action values impacts the amount of on-policy data needed, with \u03b1 potentially being as small as zero for off-policy learning. The ratio of state visitation between policies also influences the required data. Experimental evidence in a simulated environment supports the effectiveness of \u03b1 = 1/8 for stable learning. The use of imperfect Q-function estimates to determine \u03b1 remains a topic for future research. The use of \u03b1 = 1/8 facilitates stable learning and improves data-efficiency by incorporating off-policy replay experience. Mixing on-policy data with the V-trace policy gradient reduces bias and provides regularization to state-action values. This approach has theoretical grounding and helps mitigate bias and variance issues in V-trace. Mixing online data with replay data is also supported as a way to reduce sensitivity to replay memory size. The trust region scheme proposed addresses bias and variance issues in V-trace by adaptively selecting suitable behavior distributions for estimating state-value. A behavior relevance function is introduced to classify relevant behavior, and a trust-region estimator computes expectations only on relevant transitions, improving state-value estimates. Experimental validation is provided, and a behavior relevance function based on Kullback Leibler divergence is proposed. In off-policy learning, behavior policies are indexed by training iteration or agent. Sampling is done based on the selected behavior policy, extending to multiple agents. Transitions are processed using sampled behavior policies, with the expectation of off-policy returns described by an equation involving a bootstrap V. The off-policy returns at state s t are described by the expectation of sampling from a given behavior policy \u00b5 z. The key idea of the proposed solution is to compute the return using a method that reduces variance while avoiding biased estimates. The proposed solution aims to compute return estimates for a policy \u03c0 by using a subset of suitable behaviors \u00b5 z determined by a behavior relevance function \u03b2(\u03c0, \u00b5, s) and a threshold b. This approach helps control properties of V \u03c0 mix by restricting expectations on subsets of behaviors, reducing variance without introducing bias in the estimated return value. The trust region estimator for regular importance sampling (IS) and V-trace uses a behavior relevance function \u03b2(s) to reject return estimates with high bias. It defines the estimator as the conditional expectation with \u03bb-returns G, ensuring Monte-Carlo bootstraps are constrained to relevant behavior. The estimator shows similar contraction as a 1-step bootstrap but is faster due to its adaptive nature. The trust region estimator for regular importance sampling and V-trace uses a behavior relevance function to reject biased return estimates. It shows similar contraction properties as a 1-step bootstrap but is faster due to its adaptive nature. The V-trace estimators contract at a speed specific to the algorithm and behavior, allowing for control over bias and shrinkage. The choice of parameters enables the selective creation of V-trace targets and control over bias, improving estimation accuracy. The behavior relevance function \u03b2 KL is used to control bias and greediness in the policy gradient estimate. A suitable choice of \u03b2 can improve the trust region V-trace return estimator by moving the return estimate closer to V \u03c0 and enhancing shrinkage. The behavior probabilities \u00b5 z and target policy \u03c0 are evaluated and saved during agent execution, allowing for the determination of bias in V z. The target policy \u03c0 is represented by the agent's neural network, and using equation 3, \u03c0 \u00b5 can be computed. For large or infinite action spaces, a Monte Carlo estimate of the KL divergence can be computed. Separate behavior relevance functions for the policy and value estimate are defined. Transitions are rejected entirely for all estimates, and Monte-Carlo bootstraps stop once they reach undesirable state-behavior pairs. This censoring procedure is computed from state-dependent \u03b2(\u03c0, \u00b5, s) to ensure that the choice of bootstrapping is independent of sampled actions. Experiments are presented to support claims, including the use of uniform experience replay in Section 5.2. Experiments in Section 5.2 show that uniform experience replay is as effective as prioritized experience replay, while being easier to implement. Using fresh experience before adding it to the replay is better than purely off-policy learning, as shown in Section 5.3. Sharing experience without a trust region performs poorly, but Off-Policy Trust-Region V-trace solves this issue, as indicated in Section 5.4. Section 5.5 demonstrates that sharing experience can lead to state-of-the-art performance on Atari games and save memory through a single experience replay. The V-trace distributed reinforcement learning agent is used for updates on mini-batches of trajectories in the environment. In DeepMind Lab, the multi-task suite DMLab-30 is considered for training agents on a mixture of tasks simultaneously. Agents are augmented with multi-task Pop-Art normalization and PixelControl. A PreCo LSTM is used for updates instead of a vanilla LSTM. In DeepMind Lab, agents are trained on DMLab-30 with multi-task Pop-Art normalization and PixelControl. Updates are done using a PreCo LSTM instead of a vanilla LSTM. Mini-batches of trajectories are used, with entropy cost computed only on online data for better results. Experience is mixed with online data in each minibatch, and multiple agents share experience via a common replay buffer. Agents draw uniform samples from the replay buffer for hyper-parameter sweeps on DMLab-30 and Atari. Sweeps consist of 10 agents on DMLab-30 and 9 agents on Atari with different learning rate and entropy cost combinations. A single-agent LASER experiment is also presented with a 87.5% replay ratio and 15M replay. Episodes are stored in the replay buffer and replayed using the most recent network parameters. Prioritized experience replay uses recent network parameters to recompute LSTM states for efficient learning. It introduces new hyper-parameters and design choices, such as priority metric and bias correction. Uniform replay is parameter-free and easily shared between agents. Actor critic prioritized replay showed little benefit on DMLab-30. Priorities are computed from agent-specific metrics like TD-error. Performance degrades significantly without online data in the batch, validating difficulties of learning purely off-policy. Best results are obtained with experience replay of 10M capacity and 87.5% ratio. Larger ratios are more data-efficient, utilizing more replayed experience per training step. Hyper-parameter sweeps without trust region are surpassed by the baseline. State-of-the-art results are achieved when experience is shared with trust-region in a PBT sweep, showing parallel exploration benefits and memory savings. Increasing the \u03c1 threshold to reduce bias in V-trace is not a viable solution, as shown in Figure 4. The proposed agent is applied to Atari for sample-efficient learning. The study presents LASER, an off-policy actor-critic agent utilizing shared experience replay for data efficiency in reinforcement learning. Results show superior performance compared to prior work, achieving 448% in 200M steps on Atari games. The study introduces LASER, an off-policy actor-critic agent that achieves state-of-the-art data efficiency on 57 Atari games with 200M environment steps. It also shows competitive results on DMLab-30 and Atari under regular conditions by proposing mixing replayed experience and on-policy data, along with a trust region scheme to enable learning in strongly off-policy settings. Increasing the clipping constant \u03c1 in V-trace reduces bias but not suitable for sharing experience replay between multiple agents. The study introduces LASER, an off-policy actor-critic agent that achieves state-of-the-art data efficiency on Atari games. It proposes mixing replayed experience and on-policy data, along with a trust region scheme for learning in off-policy settings. The use of our trust region scheme is motivated by the negative impact of sharing experience replay in a certain way. Increased clipping thresholds lead to worse performance, highlighting the importance of variance reduction. The experiments involve investigating the use of uncorrected LSTM states with different replay modes, such as prioritized and uniform experience replay. The study introduces LASER, an off-policy actor-critic agent that achieves state-of-the-art data efficiency on Atari games by mixing replayed experience and on-policy data. Prioritized experience replay can be sensitive to hyperparameters \u03b1, \u03b2, and a third parameter. Implementing random access for recurrent memory agents like those using an LSTM is a critical practical consideration. Prioritized agents using LSTM for memory may face challenges with representational mismatch, especially when sharing experiences between multiple agents. Solutions like burn-in windows or constant starting states can only partially mitigate this issue. In our implementation, we advocate for uniform sampling to address the challenges of representational mismatch when sharing experiences between agents. We uniformly sample an episode and replay it using the most recent network parameters to recompute LSTM states. This approach is cost-efficient and exact, requiring only one additional forward pass for each learning step. Refreshing LSTM states at all times may not be viable for prioritized experience replay, as shown in our investigation. In Figure 4, the study compares different agents with and without experience replay. The uniform replay agent is more data efficient and performs better than the baseline agent. The best prioritized replay agent does not outperform the uniform replay agent. Therefore, the study uses uniform replay with full state correction for all investigations. Performance is evaluated by averaging episode returns and normalizing scores based on human expert and random agent benchmarks. In the multi-task setting, agent performance is defined as the median normalized score across all levels trained on. Population based training requires comparisons between algorithms at the sweep level. Averaging across episodes, then taking the median across games, and downsampling to 100M env steps reduces variance. DMLab-30 sweeps are repeated 3\u00d7, except for \u03c1 = 2 and \u03c1 = 4. Atari sweeps with 57 games are summarized by the median of human-normalized scores. Algorithm 1 presents the pseudocode for LASER with trust region, focusing on the single agent case without LSTM. Replayer Threads are used for LSTM state recomputation by sampling entire episodes from replay and slicing them into trajectories of length T. The Learner Thread initializes the LSTM with the transmitted state during unrolling. The Learner Thread initializes the LSTM with the transmitted state when unrolled over the trajectory. Initialize parameter vectors \u03b8. Actor Thread samples trajectory unroll using the latest \u03c0 k, computes trust-region V-trace return, and performs gradient update to \u03b8. Five propositions are stated in the paper, including the bias of V-trace value estimate and policy gradient. Proposition 2 discusses the bias in the V-trace policy gradient, showing that it does not converge to the optimal policy in certain scenarios. It presents a tabular counterexample where the V-trace policy gradient estimates a different policy than the optimal one. The optimal Q-function Q * is scaled by \u03c9(s t , a t ) = min 1,\u03c1 \u00b5(at|st) \u03c0(at|st) \u2264 1, resulting in implied state-action values Q * ,\u03c9 that penalize certain actions. An example with an MDP illustrates how choosing \u00b5 adversarially can corrupt the optimal state-action value, leading to V-trace distortion. The Atari per-level performance of various agents at 50M and 200M environment steps is displayed in Table 2. LASER scores are computed by averaging the last 100 episode returns before 50M or 200M environment frames. Episodes are terminated after 30 minutes of gameplay. Rainbow scores are obtained from Hessel et al. (2017)."
}