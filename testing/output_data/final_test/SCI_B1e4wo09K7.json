{
    "title": "B1e4wo09K7",
    "content": "Representations learnt through deep neural networks are informative but opaque. A new probabilistic modelling approach learns two separate deep representations: an invariant representation encoding class information and an equivariant representation encoding symmetry transformations of data points within the class manifold. This approach is transparent, easy to implement, and applicable to various data types. The method shows promising results in representation learning and competitive performance. Representation learning is a crucial aspect of deep learning, providing both qualitative representation and competitive performance in various settings. Transferable representations can be efficiently leveraged for new tasks, human interpretation of machine learning models, and model control. Having interpretable data representations within a model is often preferred for better understanding and performance. In deep learning, interpretable data representations are preferred for better understanding and model control. Some practitioners modify model objectives or generative models to achieve this, while others incorporate data symmetries into neural network architecture to learn meaningful latent variables. In deep learning, interpretable data representations are important for model control. The EQUIVAE approach presents a probabilistic model for data with distinct classes, using 2 latent variables to capture global class information and smoothly interpolate within each class. This approach offers flexibility without specifying the symmetry group of the manifold, allowing for control over model parameters and representation dimensions. The EQUIVAE approach requires some labeled data for model control and flexibility. It disentangles sources of variation using a clamping technique and does not need modifications to the training algorithm or penalty terms to separate information in latent variables. The EQUIVAE approach utilizes multiple data points from the same class to reconstruct a single data point, creating an invariant representation that encodes class-level information. It uses a deterministic latent for the invariant representation and a stochastic latent for the smooth equivariant representation. This approach eliminates the need to explicitly remove class-level information from the equivariant latent. EQUIVAE provides a non-trivial representation of global information using a deterministic latent, allowing for direct evaluation on unlabelled data. This invariant representation can be reused in downstream tasks, offering more information than a simple class-label distribution prediction. The encoding procedure in EQUIVAE is inspired by using images from known coordinates to reconstruct new scenes. In contrast to using known coordinates to reconstruct scenes, BID9 explores using multiple data points in generative modeling to capture uncertainty. They propose a generative model with latent variables to describe data from distinct classes with manifold structures. The generative model proposed by BID9 uses latent variables to describe data from different classes with manifold structures, aiming to capture uncertainty and rich information relevant to the class of the data point. The conditional dependency of p \u03b8 on the deterministically calculable representation r n of the global properties of class y n is ensured by providing samples from the whole class-y n manifold for r n to learn an invariant representation. This technique, inspired by Generative Query Networks, excludes the data point at hand x n from the expectation value. The text discusses the challenges of sampling labelled data and the impact on the assumption of i.i.d. data generation. It mentions simplifying notation for mathematical clarity and the purpose of providing information needed for learning an invariant representation. The bias from non-i.i.d. generation is considered insignificant. The approach in Equation 2 aims to learn a global-class embedding by providing necessary information about the class manifold. During training, values of m are sampled uniformly between 1 and a small maximal value. This allows the r y embedding to work well for various values of m, including m = 1. At inference time, unlabelled data points can be immediately embedded via f \u03b8inv (x), which is useful for downstream tasks. Variational inference is used to approximate the integral over the equivariant latent in Equation 1. The variational distribution over the equivariant latent is inferred from r y (and x), not y, to capture intra-class variations. Providing both r y and x to q \u03c6cov (v|r y , x) offers flexibility in the variational distribution. This leads to a lower bound on log p(x, y) in Equation 1. The inference of r y from multiple same-class data points enhances the global properties representation. EQUIVAE is designed for semi-supervised learning, with a focus on learning a representation that stores global-class information. An objective function for unlabelled data is needed alongside the labelled-data objective function. Variational inference is used with a variational distribution to marginalize over the label in unlabelled data points. The inference of r y from multiple same-class data points enhances the global properties representation. The EQUIVAE model focuses on semi-supervised learning by inferring labels for unlabelled data using variational inference. The lower bound objective for semi-supervised learning is computed by sampling from the discrete distribution q(y|x). The evidence lower bound objective for semi-supervised learning in EQUIVAE involves adding log q \u03c6y-post (y|x) to L lab without any hyperparameter tuning, except for choosing latent dimensionality and m max. Experiments are conducted on MNIST and SVHN datasets, which are appropriately modelled with EQUIVAE. The EQUIVAE generative model requires labelled data to force the representation to capture common information of a class. In unsupervised training, the equivariant latent is unused, leading to a deterministic autoencoder. In supervised learning, the invariant-equivariant properties are studied, while semi-supervised learning is discussed separately. The experimental setup for EQUIVAE involves training on labelled data sets like MNIST and SVHN, with convergence achieved in approximately 40 epochs and 90 epochs respectively. The equivariant latent representation captures non-trivial information, but struggles to distinguish between digit classes when visualized in 2 dimensions. The equivariant latent representation in the experimental setup for EQUIVAE shows uniformity in class-coloured labels, indicating similarity across all MNIST digits in rotations, stretches, and stroke thickness. Invariant representation vectors are well separated for each class, with some spread due to the small number of complementary samples used during training. The model in FIG0 had m randomly selected between 1 and 7 during training, with m = 5 used for visualization. Outlier points are exaggerated by dimensional reduction. SVHN results are similar to FIG0, with slightly less uniformity in the equivariant latent. All visualizations use data from the validation set. Test set was reserved for computing accuracy values. Hyperparameter tuning focused on number of epochs, range of m values, and dimensionality of latents. Architecture was fixed at the outset. In the context of the fixed architecture with ample capacity, reconstructed images in various latent-variable configurations are shown in FIG1. Samples from the equivariant prior p(v) are displayed with fixed invariant representations for each class y. Interpolations between digits of the same class with fixed invariant representations are smooth as expected. The equivariant representation v, as a stochastic latent variable, smoothly covers the trajectory between images. Latent pairs are used to reconstruct images, with the equivariant latent controlling all stylistic aspects and the invariant latent controlling only the central digit. An EQUIVAE trained on MNIST shows clear results with 2-dimensional invariant and equivariant latents. The EQUIVAE trained on MNIST uses 2-dimensional invariant and equivariant latents to visualize the latent space. Reconstructions with fixed values for y and evenly spaced v show similar stylistic variations across different digits. The validation set images show significant distance between clusters without dimensional reduction. The invariant latent captures relative similarity of base digits, supporting the assertion that it represents unique features. The invariant latent representation in EQUIVAE captures global class information, while the equivariant representation captures local, smooth, intra-class information. The invariant representation can predict the class label as well as a dedicated classifier, demonstrating its effectiveness in learning class information. The invariant representation r y is computed from x using f \u03b8inv (x). It can be passed into a neural classifier or compared to cluster means from the training set for class probabilities. Classifying test-set images with this distance metric performs similarly to a neural classifier. The distance-based technique is a more direct approach to classification than using p(y|x). The benchmark classifier, based on the invariant representation f \u03b8inv (x), performs well on MNIST and SVHN datasets. Uncertainty bands are calculated using standard error on the mean. Results from relevant literature show that BID19 performs slightly worse than EQUIVAE but within error bars. BID19 trains for 100 times more epochs than EQUIVAE. BID19 trains for 100 times more epochs than EQUIVAE and uses over 10 times more parameters in their model. They train on a training set of 50,000 MNIST images compared to our 55,000. We cannot compare to BID28 as they do not provide fully supervised results on MNIST. Test-set classification error rates are presented in TAB1 for varying numbers of labelled data. EQUIVAE outperforms the benchmark classifier on MNIST and SVHN datasets with different latent spaces and m max values. The experiments were run 5 times to calculate the mean and error. EQUIVAE shows competitive performance compared to BID28, converging rapidly in 20-35 epochs without hyperparameter tuning. EQUIVAE is effective for learning invariant and equivariant representations of data with discrete classes of continuous values. The invariant representation encodes global information about the class manifold, while the equivariant representation learns transformations covering instances on the class manifold. Invariant latents achieve 99.18% accuracy on MNIST and 87.70% on SVHN with a simple distance metric. The equivariant latent learns smooth transformations. The curr_chunk discusses the experimental setup for implementing EQUIVAE, using standard neural networks with minimal hyperparameter tuning. The models have fewer than 1 million parameters and converge quickly. The deterministic class-representation vector is parametrized using a 5-layer convolution network. The experimental setup for implementing EQUIVAE involves a 5-layer convolution network with specific filter and hidden unit configurations. The approximate posterior distribution is parametrized as a diagonal-covariance normal distribution. The EQUIVAE implementation involves a convolution network with specific configurations. The generative model is based on DCGAN-style transposed convolutions and assumes a Bernoulli distribution for MNIST. The invariant and equivariant representations are processed through dense networks before being combined for image output. The implementation involves a 5-layer convolution network with specific configurations for image output. In semi-supervised experiments, an initial embedding for x is provided using a (5-CNN, 1-dense) encoding block, concatenated with stop grad(f \u03b8inv (x)), and passed through a 2-layer dense dropout network. The use of stop grad(f \u03b8inv (x)) allows access to a relevant representation without passing gradients through. The implementation involves a 5-layer convolution network for image output. In supervised experiments, data sets are preprocessed and normalized. Activation functions are rectified linear units. Adam BID13 is used for training with a batch size of 32. In this appendix, the derivations of log-likelihood lower bounds are detailed for EQUIVAE when labelled data is available. The data set is denoted as DISPLAYFORM0, decomposed into labelled instantiations D y lab. The goal is to maximize the log likelihood of generating both labelled and unlabelled data. The text discusses constructing a lower bound on the log likelihood of labelled data using a variational distribution. It involves simplifying notation and utilizing Jensen's inequality. The text discusses constructing a lower bound on the log likelihood of unlabelled data using variational inference. It involves marginalizing over labels and utilizing a variational distribution over both y n and v n."
}