{
    "title": "B1xeyhCctQ",
    "content": "The gradient of a deep neural network (DNN) w.r.t. the input provides information to explain the output prediction in terms of input features. In a linear model, the gradient corresponds to the weights, while the bias is often overlooked in attribution methods. However, the bias in a DNN also contributes significantly to prediction correctness. A backpropagation-type algorithm called \"bias back-propagation (BBp)\" is proposed to attribute a DNN's bias to its input features. The \"bias back-propagation (BBp)\" algorithm attributes the bias of each layer in a DNN to its input nodes, combining bias terms of previous layers until reaching the input layer. This process, along with gradient backpropagation, allows for a full recovery of the locally linear model. BBp provides interpretable explanations by decomposing DNN output attribution into gradient and bias components, offering complementary insights beyond gradient-based methods. Deep neural networks (DNNs) are powerful for various tasks but are often seen as black boxes due to their complex architecture. Understanding DNN behavior is crucial, especially in critical applications. Gradients can help explain DNN behavior by reflecting the contribution of each input dimension to the output. Gradients provide attribution information for each input dimension in a DNN, aiding in understanding aspects like adversarial examples and defense methods. For linear models, the gradient recovers the weight vector, while for piecewise linear DNNs, it represents the weights of the local linear model. The bias term of the locally linear model is often overlooked and not explicitly studied. The bias term in neural networks, often overlooked, plays a crucial role in determining the target label's output probability. Understanding its nature could provide valuable attribution information complementary to the gradient. The bias term in neural networks is crucial for determining output probability. A bias attribution framework called \"bias backpropagation (BBp)\" distributes bias scalar to input data dimensions, revealing its importance. BBp uses a recursive rule to assign bias attribution layer by layer, enhancing understanding of DNN mechanisms. Bias backpropagation (BBp) distributes bias scalar to input data dimensions, revealing its importance in neural networks. The sum of attributions by BBp recovers the bias term in the local linear model of the DNN. Bias attribution complements gradient-based methods, highlighting essential features. Understanding DNN mechanisms is crucial for explaining their decisions and turning them into glass boxes. BID10 visualized behaviors of convolutional networks by investigating gradients of predicted class output. Deconvolution BID15 and guided backpropagation BID11 modify gradients with constraints. BID5 extended to higher order gradient information with Taylor expansion. BID0 studied Taylor expansion on DNNs with local renormalization layers. BID8 proposed DeepLift, separating positive and negative attribution. BID12 declared two axioms for attribution methods and developed integrated gradient method. CAM BID16 localizes attribution based on activation of convolution filters. The focus of this paper is on investigating the importance of bias terms in attribution methods for deep neural networks. Previous work has utilized gradient information but has not explicitly studied bias terms. Unlike some methods that implicitly consider bias terms, this approach can be applied to any piece-wise linear DNN, not just convolutional networks. The paper explores the significance of bias terms in attribution methods for deep neural networks. It presents a formalization of DNN output and structure, excluding the last softmax layer. The DNN architecture discussed can represent fully-connected networks and convolution operations as matrix multiplications. The paper focuses on DNNs with piecewise linear activation functions like ReLU, leaky ReLU, PReLU, and hard tanh. It discusses how average-pooling can be represented as a matrix multiplication, max-pooling as an activation function, and batchnorm as a linear operation combined into the weight matrix. Residual networks are represented by appending an identity matrix to the weight matrix to preserve input values. The paper discusses DNNs with piecewise linear activation functions such as ReLU, leaky ReLU, PReLU, and hard tanh. It explains how these activation functions create linear models at each input point, making the DNN equivalent to a linear model. The activation pattern is defined as the index of the interval containing the input value, extending to vectors and tensors. The paper discusses DNNs with piecewise linear activation functions like ReLU, explaining how these functions create linear models at each input point. This makes the DNN equivalent to a linear model, with each local linear model applied to one data point. By eliminating ReLU functions, a local linear model representing one piece of the DNN can be produced. The DNN model is linear, with each linear region uniquely determined by ReLU patterns. Attribution of DNN output to input features assigns portions to each feature, summing up to the output. The linear model can be decomposed into a linear transformation and a bias term. The attribution of the linear transformation \u2202x is straightforward, while the attribution of the bias term bx is more challenging due to its scalar nature. Bias attribution studies are scarce possibly because bias is considered less important than the weight vector in deep model decisions. The final bias scalar bx of each local linear model in deep models is a result of a complex process involving modification based on activation functions and weight matrices. Understanding this process is crucial as bias plays a significant role in producing accurate predictions, as shown in empirical studies. The bias term in deep models plays a significant role in producing accurate predictions, with the main component often leading to the final predicted label. Ignoring the bias term can result in misleading input feature attributions. Additionally, the bias component changes the geometric shape of piecewise linear DNNs, making it an essential component that should be studied. The bias term in deep models is crucial for accurate predictions and can contain key information often overlooked in explanations. A method for bias attribution is introduced to find a vector \u03b2 of the same dimension as the input data point x. A backpropagation-type algorithm is developed to attribute the bias layer by layer in a bottom-up manner in the neural net structure. The output f(x) of a piecewise linear DNN can be represented as a linear model of the input x of any layer > 2. The bias attribution \u03b2[p] on input node x[p] can be computed to recover the bias in the linear model. Various options are discussed to compute the attribution scores \u03b1 for layer - 1. The bias attribution algorithm recursively computes bias terms for each input dimension in a piecewise linear DNN. Starting from the last layer, the process backpropagates bias terms and attributions to lower layers, eventually obtaining bias attribution \u03b2[p] for each input dimension. The algorithm is detailed in Algorithm 1. The bias attribution algorithm computes bias terms for each input dimension in a piecewise linear DNN. It backpropagates bias terms and attributions to lower layers, obtaining bias attribution \u03b2[p] for each input dimension. Two options are discussed to compute attribution scores in \u03b1 [p], where bias attribution serves as a compensation for the weight or gradient term to achieve the desired output value. If DISPLAYFORM0 is negative, additional negative bias is applied to achieve the desired output x [p]. The bias attribution algorithm computes bias terms for each input dimension in a piecewise linear DNN. It backpropagates bias terms and attributions to lower layers, obtaining bias attribution \u03b2[p] for each input dimension. Positive components lead to a larger bias term, while negative components result in a smaller bias term. The logistic function is used to attribute bias, with a temperature parameter T controlling the sharpness of the attribution. Non-zero components are considered for attribution. The bias attribution algorithm in a piecewise linear DNN computes bias terms for each input dimension. It backpropagates bias terms and attributions to lower layers, obtaining bias attribution \u03b2[p] for each input dimension. Positive components lead to a larger bias term, while negative components result in a smaller bias term. The logistic function with a temperature parameter T controls the sharpness of the attribution. Non-zero components are considered for attribution, with the goal of achieving the target value x[p]. The offsets of each component to the average target value introduce the bias term, compensating components based on their distance from the target. The method computes s [p, q] by assigning less compensation to components close to the target. Different designs for the \u03b1 [p, q] function are proposed, with the attribution function being valid under certain conditions. The proposed options can be applied to piecewise-linear deep neural networks, allowing for specialized attribution functions for specific activation functions. The importance of bias terms and the information encoded in the bias are evaluated. The importance of bias terms in neural networks is evaluated by comparing networks trained with and without bias on various datasets. Results show that bias terms carry significant information and contribute to correct predictions. Bias attribution results are presented using different attribution scores, showing the importance of bias in network performance. Based on gradient information, BBp is tested on STL-10 and ImageNet datasets using specific network architectures. Bias attribution is shown to highlight meaningful features in the input data, capturing information not covered by gradient attribution. The bias attribution method highlights key features in input data that are not captured by gradient attribution. It provides cleaner explanations and focuses more on specific characteristics, such as body shape and beak, compared to gradient attribution. The bias attribution tends to attribute in a more concentrated way and complements the information provided by the gradient method. The columns in the output display the label of each image, the attribution of gradient and bias normalized to the color range, and the top 10% data features with the highest attribution magnitude. The bias attribution method highlights key features in input data not captured by gradient attribution, providing cleaner explanations. It complements the gradient method by focusing on specific characteristics, such as body shape and beak. The bias attributions offer explanations complementary to the information provided by the gradient, showing stronger attribution on certain features in images from ImageNet. The bias attribution method highlights key features in input data not captured by gradient attribution, providing cleaner explanations. It complements the gradient method by focusing on specific characteristics, such as body shape and beak. Similarly, for the \"folding chair\" of ImageNet, BBp shows clearer parts of the chair, while the gradient attribution shows less relevant features such as the background wall."
}