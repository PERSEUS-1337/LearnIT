{
    "title": "S1lvWeBFwB",
    "content": "Random Partition Relaxation (RPR) is a method for quantizing convolutional neural network parameters to binary and ternary values. It involves quantizing weights, relaxing random partitions to continuous values for retraining, and then quantizing again. RPR shows improved accuracies for binary and ternary weight networks compared to state-of-the-art methods, particularly with GoogLeNet. Recent research has focused on reducing the computation workload and model size of deep neural networks (DNNs) to make them more accessible for low-power and mobile applications. Efforts have been made to compress models to a smaller size for over-the-air updates and to improve energy efficiency through specialized hardware accelerators. Recent research has focused on reducing the computation workload and model size of deep neural networks (DNNs) for low-power and mobile applications. Specialized hardware accelerators can be integrated into system-on-chip devices for energy efficiency. Quantizing neural networks is crucial for storing more weights in on-chip memory and reducing repeated memory accesses. Complex network compression schemes are not suitable due to lengthy decompression processes and high energy consumption. Extreme network quantization simplifies multiplications in convolution and linear layers by quantizing parameters. BinaryConnect and XNOR-net are examples of successful binary neural networks, with XNOR-net showing improved accuracy using dynamic normalization and reporting results for a deeper ResNet topology. Trained ternary quantization (TTQ) was introduced by Zhu et al. in 2017, allowing weights to be scaled values of {\u03b1 1 , 0, \u03b1 2 } instead of {\u22121, 0, 1}. Incremental network quantization (INQ) developed by Zhou et al. in 2017 improved accuracy for ternary weight networks (TWNs) by quantizing parameters step-by-step without inaccurate gradients or stochastic forward passes. Last year, Leng et al. presented a different approach to training quantized neural networks using the alternating direction method of multipliers (ADMM). ADMM optimizes the loss function and enforces the quantization constraint by projecting continuous values to their closest quantization levels. While ADMM achieves state-of-the-art results, it requires optimization using the extragradient method. Quantization networks (QNs) were introduced as a different approach to training neural networks, using L2-norm gradient clipping and smoothed multi-step functions. They aim to optimize network parameters to minimize a non-convex function, diverging from standard DNN toolkits. Quantization networks optimize parameters to minimize a non-convex function by restricting some parameters to quantization levels. This results in a mixed-integer non-linear program, with common quantization levels being symmetric uniform or exponential. Convolutional and linear layer weights are typically quantized, except for the first and last layers, as their quantization has a significant impact on final accuracy. The impact of quantization levels on final accuracy is significant, especially in convolutional and linear layers. Practical optimization algorithms for mixed-integer non-linear programs are approximate, with approaches like annealing and introducing proxy gradients to address the optimization problem. ADMM has been proposed as a promising method to tackle this issue. For RPR, an alternating optimization approach is proposed to tackle the MINLP problem. The parameters are partitioned and optimized in a sequence until all constrained parameters are quantized. Standard optimization algorithms like SGD can be used for this process. Starting with a warm-start initialization method, gradient descent on the partition of W q can lead to faster convergence. Smaller freezing fractions can be used for quicker convergence, but starting from a pre-trained network allows for faster quantization and better local optima. After warm-start initialization, gradient descent on W q partition can speed up convergence. Pretrained model parameters are re-scaled to minimize distance between continuous and quantized values. Optimization time is minimal compared to overall compute time. Experiments on ImageNet with ResNet-18, ResNet-50, and GoogLeNet demonstrate RPR performance. The performance of RPR is demonstrated on ImageNet with ResNet-18, ResNet-50, and GoogLeNet by training them as binary weight and ternary weight networks. Results are not reported on CIFAR-10 and with AlexNet on ImageNet due to overparametrization. Preprocessing and data augmentation methods vary widely in related work. During testing, simpler preprocessing methods were found to work better, such as resizing the image to have a shorter edge of 256 pixels, random crops of 224\u00d7224 pixels, and random horizontal flips. The networks were trained using the Adam optimizer with initial learning rates identical to the full-precision baseline models. A freezing fraction of FF = 0.9 was used during an initial training phase until stabilization of the validation metric. During training, a freezing fraction FF = 0.9 was used until validation metric stabilization. The FF was then increased gradually to 1.0, with the learning rate adjusted accordingly. Quantizing a network with RPR requires a similar number of training epochs as training the full-precision model. For example, quantizing GoogLeNet to ternary weights with FF = 0.9 required 37 epochs, followed by 45 epochs of increasing FF before optimizing continuous parameters for 30 additional epochs. Results and comparisons to related work are provided in Table 1. Our method, compared to ADMM-based method, outperforms XNOR-net BWN, TWN, and INQ for ResNet-18 accuracy. ADMM algorithm's optimization procedure differs from SGD. TTQ and Quantization Networks achieve higher accuracy than RPR but introduce trained quantization levels with hardware implementation challenges."
}