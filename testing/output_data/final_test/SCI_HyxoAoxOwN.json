{
    "title": "HyxoAoxOwN",
    "content": "Aspect extraction in online product reviews is a key task in sentiment analysis and opinion mining. This work proposes a weakly supervised approach for training neural networks for aspect extraction using a small set of seed words. The approach involves distilling the seed words to the parameters of a neural network through a bag-of-words classifier. Regularization is shown to encourage the effective use of seed words in aspect extraction. The proposed distillation approach for aspect extraction in online product reviews outperforms previous weakly supervised methods by up to 34.4% in F1 score. This method encourages the neural network student to consider non-seed words for classification, leading to better performance compared to the teacher model. Aspect extraction is crucial in sentiment analysis, opinion mining, and summarization tasks. In this work, deep neural networks have outperformed traditional aspect extraction methods. The focus is on classifying segments of online product reviews into aspect classes without ground truth labels. This is important for both sellers and customers, especially in the context of large retail stores like Amazon. Fully supervised neural approaches are not feasible in this scenario. Neural networks can be effectively trained with weak supervision using a small set of seed words as descriptive keywords for each aspect. This approach helps in mapping learned topics to aspects of interest without the need for fully supervised neural methods. Our proposed approach aims to improve the utilization of seed words in weakly supervised networks for aspect extraction. We focus on learning vector representations for segments of words to predict aspect classes of interest, utilizing word embeddings for segment embedding. The proposed approach aims to enhance the use of seed words in weakly supervised networks for aspect extraction by learning vector representations for word segments to predict aspect classes. Word embeddings are utilized for segment embedding, and various neural network architectures such as RNNs and CNNs are used for classification. Unsupervised neural topic models like Aspect Based Autoencoder (ABAE) reconstruct embeddings for segments without the need for aspect labels. The Multi-seed Aspect Extractor (MATE) enhances weakly supervised networks for aspect extraction by considering a distinct set of seed words for each aspect. The model learns informative aspect representations by initializing aspect embeddings to the weighted average of seed word embeddings. In contrast to MATE, a weakly supervised approach for aspect extraction leverages seed words as a stronger signal for supervision throughout training. This approach uses individual seed words for supervision, adopting the knowledge distillation paradigm to train a simpler network to imitate the predictions of a complex network. In this work, a simple bag-of-words classifier acts as the teacher, encoding domain knowledge with seed words. The student, a more complex neural network, is trained to distill this knowledge from the teacher. The teacher is trained without labels, using predictive seed words for aspects. The weight matrix and bias vector are initialized for logistic regression. The weight matrix and bias vector of a logistic regression classifier are initialized using seed words for aspect extraction. The teacher assigns higher scores to aspects with seed words present in a segment, but non-seed words are also considered important. The student network, an embedding-based neural network, is trained to consider both seed and non-seed words for aspect extraction. The student network is an embedding-based neural network that classifies segments into aspects. It uses word2vec embeddings or BERT embeddings for the EMB function and a softmax classifier for the CLF function. The network is trained to imitate the teacher's predictions by minimizing cross entropy and can generalize by associating aspects with non-seed words. Regularization and dropout techniques are used to encourage this behavior. The student network, trained to imitate the teacher's predictions, replaces seed words with \"UNK\" during training to associate aspects with non-seed words. The OPOSUM dataset is used for training and evaluation, containing Amazon reviews from various domains with aspect labels available for validation and test sets. In experiments, the same pre-processing procedure as in BID0 is used for aspect extraction in Amazon product reviews. Models compared include ABAE and MATE-* from previous studies. Hyperparameters are tuned on the validation set, and micro-averaged F1 scores are reported on the test set. The weakly supervised autoencoder MATE-* outperforms ABAE in aspect extraction using seed words for supervision. Teacher-BOW, leveraging seed words effectively, performs better than MATE-* models. Evaluation results for aspect extraction are reported in Table 1 across 6 domains. Student-W2V outperforms Teacher-BOW and Student-BOW in aspect extraction by effectively utilizing non-seed words, even without explicit training on seed words. This approach leads to more accurate predictions compared to the teacher model and previous best performing models, showing a 17.5% improvement. Our distillation approach, Student-W2V, outperforms MATE-weighted-MT by 17.5% without using weights for seed words or a multitask objective. Student-BERT achieves the best performance among all models. Future work will focus on improving methods for handling noisy seed words and interactive learning approaches for better seed word learning."
}