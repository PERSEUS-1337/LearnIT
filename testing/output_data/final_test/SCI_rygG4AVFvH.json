{
    "title": "rygG4AVFvH",
    "content": "Achieving faster execution and shorter compilation time in neural networks can drive diversity and innovation. The current methods for executing neural networks are limited by hand-optimized libraries, traditional compilation heuristics, or genetic algorithms. These methods are time-consuming and suboptimal due to costly hardware measurements. A solution called CHAMELEON uses reinforcement learning to quickly adapt to new design spaces for code optimization, accelerating the search process and improving performance. CHAMELEON uses domain knowledge inspired logic to improve samples, providing a 4.45\u00d7 speed up in optimization time over AutoTVM and a 5.6% improvement in inference time for modern deep networks. The complexity of tensor operations in DNNs calls for automated compilation frameworks like STOKE, TVM, and TensorComprehensions to match hand-optimized libraries' success. The optimization time for neural network code can be lengthy, hindering innovation. AutoTVM and other frameworks aim to automate this process but face challenges with long compilation times. To address this, new approaches like CHAMELEON are being developed to improve optimization speed and efficiency for deep networks. The current approaches for optimization in neural network code face challenges with inefficient search and suboptimal solutions. Greedy sampling methods neglect distribution of candidate solutions, leading to redundant hardware measurements and prolonged optimization time. This work aims to address these issues. This work presents an Adaptive approach to reduce compilation time and automate optimization for diverse tensor operations in next-generation DNNs. It includes an Adaptive Exploration module using reinforcement learning and an Adaptive Sampling algorithm with clustering to reduce hardware measurements. Real hardware experimentation with modern DNNs shows improved performance. The combination of VGG-16 and ResNet-18, named CHAMELEON, achieves a 4.45\u00d7 speedup over AutoTVM. CHAMELEON is available on GitHub and focuses on the life-cycle of deep learning models, from design and training to deployment on various devices. The paper focuses on accelerating the deployment cycle of deep learning models by reducing compilation time without compromising performance. It implements CHAMELEON using Vasilache et al. and TVM, emphasizing the deployment stage over accuracy optimization. The frontend compiler translates the model and applies target-independent and target-dependent optimizations. The paper discusses CHAMELEON, an optimizing compiler for deep neural networks that accelerates deployment by reducing compilation time. Target-independent passes transform the DNN model without specificity to the target hardware, while target-dependent passes optimize the program considering the hardware architecture. CHAMELEON falls under the category of optimizing compilers that adapt to different design spaces for swift optimization. Optimizing compilers for deep neural networks use hardware measurements to configure optimization based on fitness. Factors for tiling and binding include the number of filters, height, and width of feature maps. Configuration is defined by assignment to knobs \u0398, reducing the problem to tuning output code template \u03c4. The design space D \u0398 is defined by knobs in an optimizing compiler for deep neural networks. It uses a search algorithm and real hardware measurements to find the best configuration \u0398 * \u2208 D \u0398. Three variables determine the compiler's effectiveness: a diverse design space, an effective search algorithm, and a mechanism to reduce costly hardware measurements. Knobs for convolution on a GPU optimize data reuse, shared memory usage, and minimize bank conflicts. The design space for optimizing compilation of deep neural networks includes various techniques like tiling and unrolling, creating a vast design space with numerous possibilities. Challenges remain in developing efficient search algorithms and reducing hardware measurement costs. Despite advancements in research, optimizing compilation for DNNs can still be time-consuming, but recent studies have significantly reduced training times to just a few hours or even minutes for large models like ResNet-50 on ImageNet. The current solutions for optimizing compilation time for deep neural networks are crucial, especially as deep learning pipelines are being integrated into major industry players. Long compilation times can hinder the deployment of DNNs on various hardware, creating a bottleneck in enabling intelligence on a wider range of platforms. With the exploration of different neural topologies, deeper networks, and new operations to achieve higher performance, the need for frequent network optimization is increasing. The primary goal of this work is to reduce optimizing compilation time for deep neural networks to meet industry needs for expedited DNN compilation. The inefficiency of simulated annealing leads to long optimization times, with a significant portion spent on measuring real hardware feedback. The current approach for optimizing deep neural network compilation suffers from inefficient sampling mechanisms that waste hardware measurement budget and incur overhead. Greedy sampling based on cost models leads to overfitting and neglects non-uniform solution distribution and invalid configurations. This hinders swift optimization for emergent deep neural networks. Developing a new framework, CHAMELEON, aims to optimize deep neural networks by improving the search algorithm to adapt to the design space and enhancing the sampling algorithm to better adapt to solution distribution. This involves applying reinforcement learning for Adaptive Exploration and implementing Adaptive Sampling to replace greedy sampling. The optimizing compiler CHAMELEON uses Adaptive Exploration and Adaptive Sampling to optimize code templates for deep neural networks. It iteratively optimizes code for different configurations, sampling configurations with a cost model proxy for hardware measurements to reduce the number of candidate configurations. The sampled configurations are then passed to the code generator for runtime measurements on real hardware. The current approach leverages real hardware measurements for fitness evaluation and updates the cost model to enhance exploration in iterations. It aims to avoid reliance on stochastic guarantees of simulated annealing, enabling faster convergence and performance gains in optimizing code for deep neural networks. Adaptive Exploration uses Reinforcement Learning to accelerate convergence and improve performance by maximizing fitness of explored configurations in the design space. It employs an actor-critic style RL with Policy and Value Networks sharing information to learn dependencies among knobs in the design space. The RL-based Adaptive Exploration Module uses networks to learn dependencies among design space knobs and potential gains of configuration modifications. It iterates through initial configurations, taking multiple search steps for each, updating configurations based on agent actions, and evaluating them using a cost model to improve fitness. The Adaptive Exploration Module improves by reducing costly hardware measurements through repeated episodes and iterations. Candidate configurations cluster in non-uniform subregions of the design space, with many achieving similar runtime. Adaptive Sampling adapts to the design space's shape and distribution, maintaining optimization performance. The Adaptive Sampling module leverages clustering algorithms to find representative configurations, using centroids for sampling. It iterates over different numbers of clusters and centroids, considering the tradeoff between performance and hardware measurements. A Threshold-based Meta-Search method is used to automatically determine the number of clusters based on the decreasing trend in L2 loss. The threshold setting in the compiler helps determine the point of diminishing returns, reducing hardware measurements while improving candidate configurations. However, redundancy among candidate configurations remains a critical issue. The current greedy sampling algorithm faces limitations in exploring different regions of the design space due to overfitting of the cost model. Automated approaches for black-box optimization are prone to generating invalid configurations, hindering exploration and leading to errors during memory accesses. The current greedy sampling algorithm faces limitations in exploring different regions of the design space due to overfitting of the cost model. Automated approaches for black-box optimization are prone to generating invalid configurations, hindering exploration and leading to errors during memory accesses. To overcome these limitations, a Sample Synthesis method is devised to analyze redundant samples and generate new configurations by combining the most frequent knob settings. This approach aims to converge to a better overall solution by combining the strengths of different knobs. The integration of Adaptive Sampling and Sample Synthesis is presented in Algorithm 1 for architecture exploration in adaptive exploration. The text discusses the use of Proximal Policy Optimization (PPO) for reinforcement learning in architecture exploration. The actor-critic networks are optimized through architecture exploration to find a tradeoff between network size and optimization performance. K-means Clustering is used for adaptive sampling to determine centroids of configurations. Hyperparameter tuning is crucial in machine learning tools. The hyperparameters used for evaluation are detailed in Table 7 (in appendix), with tuning taking several days. The same values as the AutoTVM paper were used for a fair comparison. The Adaptive Exploration module parameters were also utilized. The hyperparameters for the Adaptive Exploration module in CHAMELEON were tuned offline before deployment and remain unchanged during use. Integration into TVM allows for component evaluation and comparison with AutoTVM. Evaluation includes convolution layers from AlexNet, VGG-16, and ResNet-18, followed by end-to-end assessment on various layers. The CHAMELEON system integrates an Adaptive Exploration module with pre-tuned hyperparameters for efficient evaluation of convolution layers from AlexNet, VGG-16, and ResNet-18. End-to-end assessment on different layers shows that the reinforcement learning agent in CHAMELEON requires significantly fewer search steps compared to simulated annealing for finding optimal configurations. The CHAMELEON system integrates an Adaptive Exploration module with pre-tuned hyperparameters for efficient evaluation of convolution layers from AlexNet, VGG-16, and ResNet-18. The reinforcement learning agent in CHAMELEON requires fewer search steps compared to simulated annealing for finding optimal configurations. Adaptive sampling helps reduce hardware measurements by 1.98\u00d7 with simulated annealing and 2.33\u00d7 with reinforcement learning, with the latter being more effective due to the agent's ability to localize the search to meaningful samples. The CHAMELEON system integrates Adaptive Exploration and Adaptive Sampling, showing significant improvement over AutoTVM in simulated annealing and reinforcement learning tasks. Adaptive sampling focuses on regions with higher likelihood of better performance, leading to nonuniform sampling. This approach brings an average improvement of 13.5% and 19.0% in simulated annealing and reinforcement learning, respectively. CHAMELEON system integrates Adaptive Exploration and Adaptive Sampling, outperforming AutoTVM in optimization time and output code performance. Adaptive Sampling reduces hardware measurements significantly, resulting in better code with shorter optimization time. CHAMELEON achieves 1.17\u00d7 better performance with 4.82\u00d7 shorter optimization time compared to AutoTVM. Our Adaptive Exploration and Adaptive Sampling techniques in CHAMELEON outperform AutoTVM in optimization time and output performance for deep neural networks. CHAMELEON achieves a 4.45\u00d7 optimization time speedup and up to 6.4% improvement in output code performance compared to AutoTVM. CHAMELEON offers a unique solution for optimizing compilers for neural networks, enabling Reinforcement Learning and Sampling. It outperforms AutoTVM in optimization time and output performance, achieving a 4.45\u00d7 speedup and up to 6.4% improvement. Other related works use genetic algorithms, simulated annealing, black box optimization, and profiling-based compilation passes for code optimization. CHAMELEON is based on reinforcement learning for Adaptive Exploration and reduces the number of measurements through Adaptive Sampling. It leverages clustering to focus measurements. There is a growing body of studies on using reinforcement learning for hyper-parameter optimization in various objectives, including neural networks design automation. Our work focuses on optimizing compilers using reinforcement learning, while previous studies have used reinforcement learning for tasks such as designing deep neural network models, quantization levels selection, and neural network compression. Active learning involves updating the model based on changes, while passive learning selects a subset of training data independently of the model. The Adaptive Sampling algorithm for CHAMELEON aims to reduce the number of samples for hardware measurement during optimization in optimizing compilers. It introduces Reinforcement Learning to optimize compilers for neural networks and utilizes domain-knowledge inspired Sample Synthesis to enhance sample quality. Experimental results show that CHAMELEON significantly reduces compilation time for deep models. CHAMELEON reduces compilation time significantly and improves code quality, showing potential for optimizing deep learning models."
}