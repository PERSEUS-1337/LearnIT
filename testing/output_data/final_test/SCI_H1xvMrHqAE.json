{
    "title": "H1xvMrHqAE",
    "content": "Search engine is a crucial component in web and mobile applications, facing challenges with verbose queries. A vector space search framework using a deep semantic matching model based on BERT was explored for document retrieval. A fast k-nearest-neighbor index service was deployed for online serving, improving retrieval performance and search quality, especially for tail queries. The search engine system consists of three main modules: query understanding, retrieval, and ranking. The query understanding module parses the query string into a structured query object. It includes tasks like word segmentation, query correction, term importance analysis, query expansion, and query rewrite. The index module then retrieves candidate documents based on the parsed query. The retrieval stage in a search engine system involves parsing the query string, retrieving candidate documents using an inverted index, ranking the documents with a relevance model, and returning the top-N documents to users. The final retrieval performance is heavily dependent on query understanding modules like word segmentation, which can impact the accuracy of the search results. The algorithm's segmentation error can lead to no document retrieval, damaging user experience. A novel retrieval system maps queries and documents into low-dimensional embeddings for better performance. In this paper, a document retrieval framework is presented as a supplement to traditional retrieval systems. A new architecture is designed to retrieve documents without a traditional query understanding pipeline, utilizing deep learning techniques like BERT for end-to-end retrieval in the latent space. The paper presents a novel end-to-end document retrieval framework using BERT architecture and training techniques for performance enhancement. The proposed method avoids performance decay in query understanding tasks and shows improvements in document retrieval and search ranking. In Section 2, the paper presents a novel end-to-end document retrieval framework using BERT architecture and training techniques. Sections 3, 4, and 5 detail the proposed methods, offline and online experiments. Various methods for search query understanding, such as query correction, term weighting, expansion, and reformulation, are discussed. Vector search engines have also been widely applied in information seeking tasks like image search and recommendation systems. Various embedding techniques have been developed to map text into low-dimensional numerical vectors, capturing semantic meaning. Researchers have described different neural model architectures for text relevance matching, categorized into representation-based models like DSSM and interaction-based models. These models aim to encode text for relevance scoring. The curr_chunk discusses the term correlation matrix between query and documents, semantic matching similarity, and the use of representation models and interaction-based models in search engine systems. It highlights the challenges of using representation-based models for document retrieval and mentions BID38's architecture for transforming text into a sparse representation. The curr_chunk discusses the challenges of using representation-based models for document retrieval and the effectiveness of unsupervised pretraining models like ELMo, GPT-2, and BERT in NLP tasks. These models show great power in capturing complex semantic meanings of natural language. The curr_chunk discusses the utilization of a pre-trained BERT-Base(Chinese) 2 model on a 12 layer transformer architecture for semantic representation. The model outperformed other state-of-the-art models in deep relevance matching and achieved success in various NLP tasks. The proposed semantic retrieval framework includes offline and online components for model training, document embedding inference, and semantic index building. The curr_chunk explains the use of pre-trained BERT models for semantic ranking and matching in a retrieval framework. Two models, BERT(rep) and BERT(rel), are developed to compute the final score of a query-document pair. The models leverage the BERT model for obtaining embeddings and concatenating query-document pairs for scoring. The curr_chunk discusses the use of BERT models for semantic ranking and matching in a retrieval framework. Two models, BERT(rep) and BERT(rel), are compared for computing query-document scores using embeddings and interaction mechanisms. Training involves supervised learning with a pairwise max margin hinge loss function. The loss function for one query involves model scores, labels, and a margin parameter to push pairs apart. Negative sampling is used to enhance model performance by training on human annotated data and negative samples. Negative sampling has been successful in various tasks like neural language modeling and e-commerce list embedding. The proposed method aims to improve document retrieval by augmenting more irrelevant documents as negative samples during training. This approach helps the model distinguish between relevant and irrelevant documents, making it more robust to noise. Instead of selecting negative samples uniformly from the corpus, a new method involves training a baseline model with human annotated data, encoding documents and queries, clustering them, and then selecting negative samples from the same cluster as the query. This strategy aims to provide more informative hard negative samples for the model. The proposed method aims to improve document retrieval by augmenting irrelevant documents as negative samples during training. Negative sampling is done from the same cluster as the query, named N EG cluster, and globally sampled data, named N EG global, is also included. The model is fine-tuned using this augmented dataset to obtain the final model. The model is fine-tuned using augmented data from N EG cluster and N EG global to improve document retrieval. A vector index is built using faiss for efficient k-nearest neighbor search. An online semantic index server is developed for concurrent online service, with GPU server inference speed 2 times faster than tfserving. In this section, offline experiments are conducted to demonstrate the performance of the proposed semantic retrieval methods. The model is trained with 1 epoch using Adam with a learning rate of 10^-5. The dataset consists of 36159 queries and 1181229 query-doc pairs for training, and a test dataset with 2703 queries and 84244 query-doc pairs. The relevance score is graded as 0, 1, or 2 representing bad, fair, and excellent respectively. The dataset includes 2703 queries and 84244 query-doc pairs. Various models are evaluated for ranking and retrieval performance, such as ClickSim, K-NRM, Match Pyramid, and DSSM. The proposed model utilizes word vectors pretrained on document title corpus and three fully connected layers for text encoding. The model utilizes three fully connected layers for text encoding with dimensions of 300, 300, and 128. The evaluation metric used is Recall, which measures the retrieval performance by calculating the rate of relevant documents retrieved for a given query. To evaluate recall performance offline, semantic indexes are built for the model and baseline model using document title representations. Top k documents are retrieved using knearest-neighbor search based on query embeddings. Comparisons are made on recall measures and enhancement using semantic indexes. The study compared the recall enhancement of adding a semantic index to a lexical inverted index. The experiment used a term-based inverted index engine to build both indexes and calculated recall using all human annotated data. BERT(rep) outperformed the baseline model DSSM in recall, with a significant improvement in recall rate from 54.9% to 69.4% after adding the model to the lexical index. Our proposed model significantly improves recall rate from 54.9% to 69.4% when used as a supplement to the lexical index. The model's ranking quality is measured through Normalized Discounted Cumulative Gain (NDCG), showing superiority over state-of-the-art deep relevance matching models. The BERT(rep) model and BERT(rel) model outperform other baselines significantly. The BERT(rep) feature ranks first in the ranking function, accounting for 34% of importance. Negative sampling methods were used for training data enhancement, with the best performance achieved using 10 N EG global and 10 N EG cluster respectively. After adding negative samples, the average negative sample size for a given query increased from 19.9 to 39.9. Model performance improved with different kinds of negative samples, with N EG cluster enhancing NDCG@3 by 0.8%. The pooling layer in the BERT model used reduce-mean of the last layer as BERT(rep) model's output. Training models with pooled output from different layers showed that the layer closest to the last obtained higher NDCG measure. After adding negative samples, the average negative sample size for a given query increased from 19.9 to 39.9. Model performance improved with different kinds of negative samples, with N EG cluster enhancing NDCG@3 by 0.8%. The pooling layer in the BERT model used reduce-mean of the last layer as BERT(rep) model's output. Training models with pooled output from different layers showed that the layer closest to the last obtained higher NDCG measure. Comparing results of different layers, an attention layer calculates weights across layers for a weighted sum of each layer's embedding, with mean average pooling of the last layer performing best. In an online a/b test, 40% of online traffic was randomly distributed to four groups - 2 control and 2 experimental. The Clicked Search Rate (CSR) metric showed a 0.65% improvement in the experimental groups compared to the control groups. Performance was focused on long-tail queries, with a 1.05% increase in the Tail query part. The metric increased by nearly 1.05% in the Tail part, highlighting good cases after system deployment. Results ranked top 6 for query \"\u9001\u5916\u5356\u4e0d\u8ba4\u8bc6\u8def\" at TAB8, showing better relevance with semantic index retrieval. Multiple ways to express \"\u4e0d \u8ba4 \u8bc6 \u8def\" in Chinese captured by semantic index. In this paper, an architecture for semantic document retrieval is presented, utilizing a deep representation model for query and document embedding. A semantic index is built using a fast k-nearest-neighbor vector search engine, enhancing retrieval performance in offline and online experiments. Future work aims to explore a more comprehensive framework incorporating additional signals for semantic retrievals."
}