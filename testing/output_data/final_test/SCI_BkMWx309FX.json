{
    "title": "BkMWx309FX",
    "content": "Recent studies have highlighted the vulnerability of reinforcement learning models in noisy settings, where observed rewards may be unreliable due to various sources of noise. In response to this challenge, a robust RL framework has been developed to enable agents to learn in noisy environments by utilizing perturbed rewards and estimating a reward confusion matrix. This framework draws upon techniques from supervised learning with noisy data. The solution involves estimating a reward confusion matrix and defining unbiased surrogate rewards, with proven convergence and sample complexity. Experiments on DRL platforms show policies based on estimated surrogate rewards achieve higher rewards and faster convergence than existing baselines. Customizing reward functions is crucial for real-world reinforcement learning applications, but designing credible functions in noisy environments is challenging. In noisy environments, rewards can be affected by various types of randomness, including inherent noise from sensors, application-specific noise from human feedback, and adversarial noise that can mislead machine learning models. This poses challenges for designing credible reward functions in real-world reinforcement learning applications. In noisy environments, rewards can be affected by various types of randomness, including inherent noise from sensors, application-specific noise from human feedback, and adversarial noise that can mislead machine learning models. This poses challenges for designing credible reward functions in real-world reinforcement learning applications. To address this issue, a specific noisy reward model called perturbed rewards is introduced, where rewards observed by RL agents are generated according to a reward confusion matrix. This framework enables an RL agent to learn in a noisy environment with observing only perturbed rewards, using an unbiased reward estimator aided robust framework. Our solution framework utilizes existing reinforcement learning algorithms, such as DRL methods like Q-Learning, Cross-Entropy Method, Deep SARSA, Deep Q-Network, Dueling DQN, and others. The main challenge is dealing with biased rewards in RL or DRL, which can lead to errors amplifying over time. We propose an efficient method to estimate reward confusion matrices without assuming knowledge of the true reward distribution or adversarial strategies. The authors propose a reward robust RL method using surrogate rewards in Q-Learning, achieving comparable performance with true rewards in extensive experiments on OpenAI Gym games. In some cases, the method even outperforms using true rewards, attributed to a specific noise generation model. The authors propose a reward robust RL method using surrogate rewards in Q-Learning, achieving comparable performance with true rewards in extensive experiments on OpenAI Gym games. The method adds exploration through noise, ensuring convergence to the optimal policy even with noisy observations of rewards. Extensive experiments show robust performance even at high noise rates. Robust Reinforcement Learning algorithms aim to train policies that can withstand perturbed observations in noisy environments. These algorithms focus on noisy vision observations rather than rewards, with some recent works exploring uncertainty in models. In the context of training robust RL algorithms with uncertainty in models, previous studies have focused on learning with biased data. This work aims to adapt these approaches to reinforcement learning, establishing the extension both theoretically and empirically. The goal is to provide practical insights for implementing RL algorithms in noisy environments by defining the problem of learning from perturbed rewards. In this paper, the Markov Decision Process (MDP) problem and reinforcement learning (RL) problem are formulated with perturbed (noisy) rewards. The RL agent interacts with an unknown environment to maximize collected rewards, where the environment is formalized as M = S, A, R, P, \u03b3. The agent's goal is to learn the optimal policy in a sequential decision-making setting similar to learning with noisy data in supervised learning. The agent aims to learn the optimal policy in a Markov Decision Process (MDP) with perturbed rewards. It evaluates state preferences based on the current policy and calculates the expected cumulative reward. RL algorithms typically learn a state-action value function, known as the Q-function, to estimate rewards when the agent does not observe them perfectly. In a Markov Decision Process (MDP) with perturbed rewards, the RL agent observes a perturbed version of the reward instead of the actual reward directly. The reward generation follows a certain function, and noise rate parameters characterize the noise in binary reward settings. When there are multiple outcomes, the reward is generated accordingly. In a Markov Decision Process (MDP) with perturbed rewards, the RL agent observes a perturbed version of the reward instead of the actual reward directly. The reward generation follows a certain function, and noise rate parameters characterize the noise in binary reward settings. M outcomes in total, denoting as [R 0 , R 1 , \u00b7 \u00b7 \u00b7 , R M \u22121 ].r t will be generated according to the confusion matrix C M \u00d7M where each entry c j,k indicates the flipping probability for generating a perturbed outcome. We focus on settings with finite reward levels and provide discussions on handling continuous rewards with discretizations. The paper does not assume knowing the noise rates, instead, it will estimate the confusion matrices. An unbiased estimator for binary rewards in the reinforcement learning setting when the error rates are known is introduced, inspired by previous works. The method extends to multi-outcome and continuous reward settings, establishing an unbiased approximation of the true reward using noise rates. A surrogate reward is constructed, replicating results for binary rewards in a RL setting. The property guarantees convergence in supervised learning, where the empirical surrogate risk converges to its expectation. In the multi-outcome and continuous reward settings, surrogate rewards are used to approximate the true reward, ensuring convergence in supervised learning. The reward signal is discretized into intervals to reduce quantization error, with the method remaining similar to the multi-outcome setting. The text discusses the use of surrogate rewards in reinforcement learning algorithms, specifically focusing on Q-Learning. It mentions the trade-off between quantization error and reward confusion matrix, and provides a convergence guarantee for the Q-learning algorithm with surrogate rewards. The text introduces a generative model for an MDP setting to simplify the analysis of convergence guarantees for Q-Learning with surrogate rewards. Theorem 1 shows that agents will converge to the optimal policy despite noise in observing rewards, thanks to the unbiasedness of surrogate rewards. The phased Q-Learning algorithm uses generative models to estimate transition probabilities and update the value function. It takes m samples per phase and uses surrogate rewards for updating. The sample complexity of Phased Q-Learning is discussed for convergence guarantees. The Phased Q-Learning algorithm uses generative models to update the value function with surrogate rewards. The sample complexity is analyzed for convergence guarantees, with the number of samples needed being related to the noise level in the learning environment. The Phased Q-Learning algorithm utilizes generative models for updating the value function with surrogate rewards. The variance of the surrogate reward is bounded when the confusion matrix C is invertible, and it is always higher than the variance of the true reward. This is detailed in Theorem 3, which provides a bound on the variance of the surrogate reward. The variance of the surrogate reward in rewardr is bounded, with a trade-off between bias and variance by tuning a linear combination of R and R. Estimating confusion matrices is challenging without ground truth reward information, but efficient algorithms have been developed for supervised learning settings. The algorithms refine error rates based on rewards. This approach is similar to aggregating crowdsourcing labels. In reinforcement learning, the agent predicts true rewards using historical data and accuracy. Estimated reward confusion matrices are generated based on predicted rewards and accuracy for each state-action pair. The algorithm updates the confusion matrix indefinitely as more observations arrive. It initializes the value function Q(s, a) arbitrarily and uses majority voting to predict true rewards. The final definition of surrogate reward replaces known reward confusion with the estimated one. The proposed Reward Robust RL method introduces an estimated surrogate reward, denoted as \u1e59, to replace known reward confusion in Eqn. FORMULA4. This algorithm is versatile and can be applied with any existing RL algorithm by substituting rewards with the estimated surrogate rewards. Experimental results on various games with different noise settings demonstrate the effectiveness of this approach. The method is tested on classic control games (CartPole, Pendulum) and seven Atari 2600 games (AirRaid, Alien, Carnival, MsPacman, Pong, Phoenix, Seaquest) to evaluate performance under different environments and reward types. The study explores reinforcement learning algorithms on various games with different noise levels. Different policies are trained based on random initialization to reduce variance. Performance is tested with true rewards, noisy rewards, and surrogate rewards under symmetric and asymmetric noise settings. Two types of random noise are tested: rand-one and rand-all. The study evaluates reinforcement learning algorithms on different games with varying noise levels. Five algorithms are tested on CartPole, with the goal of preventing the pole from falling. Surrogate rewards are generated to adapt the algorithms to noisy settings without assuming the true reward distribution. The noise rate is increased to assess the effectiveness of the estimator in producing meaningful rewards. In Pendulum, the models converge slower with noisy rewards due to larger biases, but always reach the best score with the help of surrogate rewards. Surrogate rewards can even lead to faster convergence in some cases, outperforming learning with true rewards. Adding noise and then removing bias introduces implicit exploration, suggesting that even in settings with true rewards, adding noise may be beneficial. The goal in Pendulum is to keep a frictionless pendulum standing up, with continuous rewards ranging from -16.28 to 0.0. In Pendulum, rewards are continuous and range from -16.28 to 0.0. The rewards are discretized into 17 intervals and surrogate rewards are estimated using multi-outcome extensions. Two popular algorithms, DDPG and NAF, perform well with surrogate rewards under different noise levels. The algorithm achieves consistently good scores on CartPole and Pendulum, as well as seven Atari 2600 games. Atari games are validated using the PPO algorithm, covering various environments with rewards clipped into {-1, 0, 1}. Results for PPO on Pong-v4 in symmetric noise setting are shown. Surrogate estimator consistently helps PPO converge to optimal policy. Average scores of PPO on selected Atari games with different noise levels are presented, showing significant improvements with surrogate rewards. Handling unknown C is challenging due to large state-space in Atari games. The text discusses the challenges of handling large state-space in Atari games and the lack of focus on perturbed and noisy rewards in reinforcement learning studies. It adapts ideas from supervised learning to address these issues. The text discusses the challenges of handling large state-space in Atari games and the lack of focus on perturbed and noisy rewards in reinforcement learning studies. It adapts ideas from supervised learning to address these issues. In the following proofs, we solve a set of functions to obtain the surrogate reward based on the confusion matrix and probabilities of occurrence for surrogate and true rewards. The text discusses challenges in handling large state-space in Atari games and the lack of focus on perturbed and noisy rewards in reinforcement learning. It adapts ideas from supervised learning to address these issues. The surrogate reward is obtained based on confusion matrix and probabilities of occurrence for surrogate and true rewards. The proof involves establishing Theorem 1 with an auxiliary result (Lemma 3) from stochastic process approximation for convergence proof in Q-Learning. The procedure of Phased Q-Learning is described in Algorithm 2, where the surrogate reward is bounded when confusion matrices are invertible. Lemma 4 states that the surrogate reward is also bounded when the true reward is in the range [0, R max]. The surrogate reward is bounded when confusion matrices are invertible, with the determinant denoted as det(C). The determinant of the adjugate matrix of C is also bounded, leading to a bound on r t by M det(C) \u00b7 R max from Lemma 4. The Phased Q-Learning algorithm can converge to the near optimal policy within finite steps using proposed surrogate rewards. The policy error is bounded within a small value by choosing appropriate parameters. The probability of failure in any condition is smaller than a certain threshold. The Phased Q-Learning algorithm can converge to the near optimal policy within finite steps using proposed surrogate rewards. The error rate is set to be less than \u03b4, and after m|S||A|T calls, the value function converges to the optimal one for every state s with high probability. The upper bound for the error in discounted MDP setting is derived, and experiments are conducted using OpenAI baselines and kerasrl framework with OpenAI Gym environments. A variety of state-of-the-art reinforcement learning algorithms are tested in OpenAI Gym environments with different levels of noise. Symmetric and asymmetric noise are explored, with symmetric noise having equivalent probabilities of corruption for each reward choice. In the study, noise levels in reinforcement learning algorithms are tested in OpenAI Gym environments. Symmetric noise has equivalent corruption probabilities for each reward choice, while asymmetric noise includes rand-one and rand-all perturbation types. Weight of noise is defined using confusion matrices, with a focus on reward perturbation models. In a study on reinforcement learning algorithms, noise levels were tested in OpenAI Gym environments using symmetric and asymmetric noise. The reward perturbation model involved constructing a simple MDP with a confusion matrix for reward generation. Q-Learning was evaluated on different noise ratios, showing better results with surrogate rewards and confusion matrix estimation. Time-variant noise experiments were also conducted, demonstrating the robustness of the estimation module. The study tested noise levels in OpenAI Gym environments using symmetric and asymmetric noise. Q-Learning showed better results with surrogate rewards and confusion matrix estimation. Time-variant noise experiments demonstrated the robustness of the estimation module against varying noise levels. The models were trained using Adam optimizer with a learning rate of 1e-3 for 10,000 steps. For training models in various environments, different optimizers and learning rates were used. The exploration strategy employed was Boltzmann policy. Specific details for DQN, Dueling-DQN, DDPG, and NAF training were provided, including update rates, memory sizes, and optimizer settings. Pre-processing steps and network architecture for Atari Games were adopted from Mnih et al. (2015), with a specific input size and layer configuration outlined. The training models in various environments used different optimizers and learning rates. The exploration strategy was Boltzmann policy. Specific details for DQN, Dueling-DQN, DDPG, and NAF training were provided. Pre-processing steps and network architecture for Atari Games were adopted from Mnih et al. (2015). Each hidden layer is followed by a rectified nonlinearity. The rewards in Atari games are discrete and clipped into {-1, 0, 1}. Agents attempt to get higher scores in the episode with binary rewards 0 and 1. Reward Robust Q-Learning algorithm can be extended to other RL algorithms by plugging confusion matrix estimation steps and computed surrogate rewards. The curr_chunk discusses the estimation of confusion matrix and surrogate rewards based on observed noisy rewards. It utilizes majority voting to predict true rewards for state-action pairs. The framework incorporates Expectation-Maximization (EM) idea for estimation, inspired by previous works. The posterior probability of true reward is computed using Bayes' theorem. The curr_chunk discusses updating the confusion matrix and posterior probabilities of true rewards using Bayes' theorem. It suggests using online EM algorithms for perturbed-RL problems and updating the confusion matrix based on inference probabilities. This approach requires more computation compared to majority voting policy and can be applied in robust reward algorithms. The curr_chunk discusses the application of EM algorithms in robust reward algorithms, specifically replacing Eqn. (4) with Eqn. (9) for estimating the reward confusion matrix. It also mentions the possibility of different noise distributions for states but states that the algorithm is still applicable. Theorem 1 is highlighted for producing unbiased estimations of true rewards. The Phased Q-Learning algorithm with generative model calls produces a policy ensuring small error in value estimation. The variance of surrogate rewards is bounded, but they may decrease RL algorithm stability. Techniques like correlated sampling can help reduce variance in surrogate rewards. The BID2 method proposed a reward estimator for compensating stochastic corrupted reward signals, but it is not effective in general perturbed-reward settings. Integrating their method with a robust-reward RL framework could be beneficial, as surrogate rewards ensure unbiasedness in reward expectation. Experiment results show that models with only variance reduction techniques suffer from biases under noisy rewards and struggle to converge to optimal policies. The noise in reward signals poses challenges for optimal policy convergence. However, variance reduction techniques for surrogate rewards lead to faster convergence and better performance. Integrated algorithms outperform as noise rate increases, especially with large variance. Surrogate benefits from variance reduction techniques, particularly with high noise rates. Displayed are learning curves from reward robust RL algorithms on CartPole game with various reward types. The study conducted experiments in CartPole to validate robust reward algorithms in unknown noise environments. The proposed estimation algorithms successfully obtained approximate confusion matrices and showed fast convergence. The results are promising as they do not rely on additional knowledge about noise. The study conducted experiments in CartPole to validate robust reward algorithms in unknown noise environments. Results show fast convergence without assuming additional knowledge about noise or true reward distribution. Noise rates ranged from 0.1 to 0.9, with experiments repeated 10 times and plotted for analysis."
}