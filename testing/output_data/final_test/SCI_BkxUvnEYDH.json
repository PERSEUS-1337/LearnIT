{
    "title": "BkxUvnEYDH",
    "content": "Developing agents that can learn to follow natural language instructions is a growing research area. To address the ambiguity of instructions, a framework utilizing formal language programs is proposed. This framework can perceive different circumstances and instruct a multitask policy to fulfill subtasks of an overall task. Experimental results in a 2D Minecraft environment show the framework reliably accomplishes program instructions and efficiently learns multitask mechanisms. The text discusses the efficiency of a proposed modulation mechanism for learning multitask policies using natural language instructions. It highlights the importance of instructions in accomplishing complex tasks and improving task performance through practice. Instructions help in estimating progress, recognizing the current state, and taking corresponding actions, such as in cooking a gourmet dish by following recipes. Machines can learn to follow comprehensive instructions like humans, with natural language instructions being flexible and expressive. While expert demonstrations are widely studied, they can be expensive and less flexible compared to language instructions. Language, however, can be ambiguous even to humans due to its lack of structure and clarity. Utilizing programs written in a formal language as a structured and unambiguous representation to specify tasks, which include control flows, environmental conditions, and corresponding subtasks. This approach offers expressiveness by describing diverse situations and the required subtasks to be executed. The text discusses the use of programs as task specifications, introducing a new problem of learning to fulfill tasks specified by written programs. A modular framework, program guided agent, is proposed to decompose and execute tasks specified by programs and interact with the environment to accomplish them. The text introduces a program interpreter, perception module, and policy to fulfill subtasks extracted from a program. A modulation mechanism is used to instruct the policy with symbolically represented subtasks. The framework utilizes a rule-based program interpreter, learning perception module, and policy to handle tasks specified by programs in a Minecraft-inspired 2D gridworld. Our proposed framework in a Minecraft-inspired 2D gridworld environment demonstrates superior generalization ability by learning from simpler tasks and generalizing to complex tasks. Extensive analysis on end-to-end learning models using program instructions and natural language descriptions is conducted, showing that our learned policy modulation mechanism improves learning efficiency compared to other methods. Leveraging natural languages to specify tasks has been investigated in prior works. Prior works have explored using natural languages to specify tasks in various applications, but language descriptions can be ambiguous. Instead, the proposal suggests using precise and structured programs to define tasks, especially when language descriptions are inadequate. Expert demonstrations offer an alternative way to provide instructions for tasks that are difficult to describe in languages. Prior works have explored learning from video demonstrations or expert trajectories. However, demonstrations can be expensive to obtain and less expressive about complex task behaviors, better captured by control flow in programs. Program induction methods are used to acquire programmatic skills and achieve better generalization. Program induction methods aim to mimic task behaviors demonstrated in specifications, while program synthesis methods explicitly synthesize and execute programs for task performance. The framework focuses on executing programs directly and can be used to evaluate program synthesis frameworks in real-world scenarios. The curr_chunk discusses symbolic planning and programmable agents in the context of learning to ground symbolic concepts and follow control flow. It mentions executing programs with reinforcement learning in programmable hierarchies of abstract machines and contrasts it with training agents to execute declarative programs. The modular framework consists of modules for executing programs. The modular framework for learning environments involves modules for perceiving and interacting with the environment through imperative programs. Programs are defined using a Domain Specific Language (DSL) with perception and action primitives, as well as control flow. The text discusses how programs in a modular framework for learning environments are defined using a Domain Specific Language (DSL) with perception and action primitives, as well as control flow. It also introduces the concept of deterministic functions that output desired behaviors based on a history of states. Additionally, it mentions the use of finite-horizon discounted Markov Decision Processes (MDPs) in a shared environment. The proposed modular framework uses a set of programs to specify instructions for a task. It includes transition probability distribution, reward function, initial state distribution, and discount factor. The program interpreter executes the program by querying the perception module and instructing a policy to fulfill goals or subtasks. The perception module responds to queries, guiding the policy on which paths to choose. The framework involves a program interpreter querying the perception module to determine which paths in the program to choose. It aims to comprehend and fulfill instructions specified by a program, executing control flow, inferring paths, and interacting with the environment to accomplish subtasks. The framework involves a modular design with three key modules: a program interpreter, a perception module, and a policy module. The program interpreter reads and executes programs, the perception module responds to environment queries, and the policy module performs low-level actions to fulfill subtasks. Each module learns to interact with the environment effectively. The proposed framework includes a program interpreter, a perception module, and a policy module. The program interpreter executes programs using a rule-based parser. Program tokens are categorized into subtasks, perceptions, and control flows. The interpreter keeps track of progress by parsing program lines. The program structure includes a line parser and executor, transforming programs into a tree structure. The executor traverses the tree to execute the program, querying perception and instructing the policy. The interpreter is rule-based, not a learning module, and requires symbolically represented queries for path determination. The program employs a perception module to map queries and observations to responses. A multitask policy guides the agent in executing subtasks by taking low-level actions. The policy is trained using symbolic goals provided by the program interpreter. The policy for executing subtasks is trained using actor-critic reinforcement learning with a goal vector and environment state. A modulation mechanism is proposed to effectively learn diverse tasks by encoding goals and modulating state features for action prediction and value estimation. The modulation mechanism in the network activates state features related to the current goal and deactivates others. Training modules include a perception module and a policy, with details on training objectives and optimization methods discussed. Training the perception module involves supervised learning to predict perception output using binary cross-entropy loss optimization. More detailed training information and architectures can be found in section E.4.1. Loss is represented as a sequence of symbols, and training the perception module involves optimizing losses like categorical cross-entropy. A multitask policy takes a state and goal as inputs, modulating state features using the goal to predict actions. Different methods are experimented with for feeding state and goal in learning a multitask policy. The training curves show that modulating state and goal for learning a multitask policy is more efficient than concatenating them in raw or latent space. The policy is trained using Advantage Actor-Critic (A2C) for gridworld environments with discrete action spaces. A2C computes policy gradients and updates the policy based on the advantage function. The policy update rule involves maximizing policy entropy to improve exploration, with the learning rate denoted as \u03b1. Experiments aim to assess the framework's ability to learn tasks specified by programs and its generalization to complex tasks. Various end-to-end learning models are compared in terms of learning from programs and natural language instructions. The efficiency of learned modulation for multitask policies is evaluated in a Minecraft-inspired gridworld environment. In a Minecraft-inspired gridworld environment, a discrete environment is constructed for performing subtasks. The agent can navigate, interact with resources and obstacles, build tools, and sell resources for rewards. Programs are sampled and split for training and testing, with a focus on generalizing to more complex instructions. The study focuses on constructing a harder testing set by conditioning branches on average. Annotators were asked to provide natural language translations of programs. During training, programs are randomly sampled along with an environment state for execution. The policy receives a reward only when the entire program is completed. No explicit curriculum is introduced like in previous studies. The study explores a curriculum where the policy learns to solve simpler programs first, leading to a better understanding of subtasks and eventually completing more complex programs. The perception module is pre-trained in a supervised manner. Different end-to-end learning models like LSTM and Transformers are experimented with for encoding programs. The study investigates encoding programs using a Tree-RNN model and training them using A2C. The proposed framework shows satisfactory test performance with a negligible drop in the generalization gap. The modular design allows the perception and policy modules to focus on their specific tasks. End-to-end learning models, on the other hand, suffer significant performance issues. The study compares end-to-end learning models with models learning from programs, showing that models with explicit structures can generalize better to complex instructions. Seq-LSTM performs best on the test set but worst on the test-complex set. Transformer has smaller generalization gaps due to its multi-head attention mechanism. Tree-RNN achieves the best generalization performance by leveraging the explicit structure of programs. In Figure 5 (a), Seq-LSTM and Transformer show a performance drop with increasing instruction length. Seq-LSTM excels with shorter instructions but struggles with longer ones, while Transformer performs consistently. Tree-RNN outperforms both by leveraging program structure. In terms of instruction diversity (shown in Figure 5 (b)), all models experience a performance drop. Transformer handles diverse instructions better due to its semantic learning ability, while Seq-LSTM performs consistently across different levels of diversity. Tree-RNN shows the most consistent performance. The study compares different methods for multitask policy learning, including using a modulation mechanism to improve performance. Results show that the proposed policy modulation mechanism is more sample efficient and achieves better task completion. Additionally, the use of structured programs in a formal language is suggested for specifying tasks instead of natural language instructions. The study introduces a modular framework, program guided agent, to comprehend and execute tasks using structured programs instead of natural language instructions. It employs a policy modulation mechanism for efficient multitask learning. Experimental results in a 2D Minecraft environment show the framework reliably fulfills program instructions and generalizes well to complex tasks. The study presents a modular framework called program guided agent for learning from programs and natural language descriptions. The program interpreter described consists of a parser and program executor to transform the program into a tree-like object for execution. Each line of the program is represented as a tree node in the program tree. The program interpreter transforms each line of the program into a node in a tree structure. Nodes contain the original line, check if they are leaf nodes, and list their child nodes. The interpreter decides whether to call a policy or perception module based on the node type. The program executor uses a pre-order traversal to execute the program tree. Different domains require different DSLs, and the design aims to accommodate this variability. The DSL design principle focuses on creating a domain-specific language that is intuitive, modular, and hierarchical. It includes control flows, perceptions, and actions, with actions being domain-specific. The goal is to easily adapt the DSL to different domains. In multitask reinforcement learning, previous works have used hierarchical approaches where an RL agent is trained to accomplish subtasks to achieve a main task. Different methods such as predefined policy sketches, learning controllers, and subtask graphs have been proposed to guide the agent towards the goal. The framework described in this paper focuses on determining which branches in a program should be executed for hierarchical tasks. Our work focuses on determining the optimal subtask to execute in a hierarchical reinforcement learning framework, different from programmable agents. Previous works have explored hierarchical reinforcement learning and explicitly specifying sub-policies for meta-controllers to utilize. In contrast to programmable agents, our work focuses on hierarchical reinforcement learning to determine optimal subtasks. We propose using imperative programs for task representation and a modular framework to leverage program structure for addressing complex tasks. The text proposes a modular framework to leverage program structure for addressing complex tasks, while also mentioning future research opportunities to bridge the gap between natural language instructions and programs. Various works have shown the effectiveness of predicting affine transforms based on different conditions in various tasks such as visual question answering, image synthesis, style transfer, recognition, reading comprehension, and few-shot learning. These works often include extensive ablation studies to compare learned modulation techniques with traditional methods of merging information from input and condition domains. Recently, some works have applied similar modulation techniques to reinforcement learning frameworks for tasks like following language instructions and metareinforcement. In this work, an ablation study is conducted in a 2D Minecraft environment to explore the effectiveness of learning to modulate input features with symbolically represented goals for reinforcement learning. Various modulation variations are presented, and the study aims to verify if this mechanism is effective in more complex domains like robot manipulation or locomotion. The environment used in this work includes objects like wood, gold, and iron. The agent's actions involve crafting (mining, placing, building a bridge, selling) and motor actions (moving in four directions). Crafting actions can only be done on the current grid cell. The agent's actions involve crafting with wood and gold, mining at the current location, and selling items at merchants. During training, valid environments are randomly initialized, and at test time, 20 valid environments are pre-sampled. The agent's observation space includes an environment map and inventory status. The agent's inventory map is a 10x10x9 grid representing objects like wood, iron, gold, agent, wall, goal, river, bridge, and merchant. The agent's inventory status includes location coordinates and counts of wood, iron, and gold. The goal of the subtask is represented as a 1-D vector of size 10. The goal vector represents subtasks and resources, with the last two entries indicating goal locations. Environment maps depict resources as block objects, with the agent as a female human character and the merchant as an alpaca. River grids with wooden bridges are shown, surrounded by brick walls. The environment map depicts various elements such as wood, gold, iron, river, bridge, and merchants represented by blocks with corresponding textures. The boundaries are shown as brick walls. Ground truth perception information is provided to all baselines during training. During training, ground truth perception information is fed to baseline models to infer subtasks. The models utilize this information to generate program sets by sampling tokens and constructing them according to DSL grammar. During testing, the perception module predicts answers to queries, impacting the framework's performance. The training set consists of 32 tokens and 4.6 lines on average, while the more complex test set has 65 tokens and 9.8 lines. Plotted statistics of essential properties for the datasets are shown in Figures 11, 12, and 13. The maximum indent of a program is the depth of its scope or the height of its transformed program tree. Recurring procedures include while and loop. Program sets were chunked into subsets for annotators to provide natural language translations after understanding the DSL syntax. The annotators provided diverse translations for natural language instructions, which were then cleansed for errors. An average of 27, 28, and 61 words were used for train, test, and test-complex sets respectively. The total vocabulary size was 448 words. Qualitative results showed ambiguity in language leading to alternative interpreted programs. The framework and learning baselines were implemented in TensorFlow. The perception module in TensorFlow takes a query q and state s as input, outputting a response h. Queries have a size of 6 \u00d7 186, with zero-padding for shorter queries. The state map s is encoded by a CNN with four layers, while the state inventory s inv is encoded by a two-layer MLP. Each token in the query is encoded by a two-layer MLP as well. The perception module in TensorFlow processes a query q and state s to generate a response h. The query tokens are encoded by a two-layer MLP with a channel size of 32, followed by ReLU nonlinearity. The resulting features are concatenated and processed by another two-layer MLP with a channel size of 32. The encoded features are then concatenated and processed by a three-layer MLP with channel sizes of 128, 64, and 32. Finally, a linear fully-connected layer produces an output indicating the truthfulness of the query response. The perception module in TensorFlow processes a query q and state s to generate a response h. The query tokens are encoded by a two-layer MLP with a channel size of 32, followed by ReLU nonlinearity. The resulting features are concatenated and processed by another two-layer MLP with a channel size of 32. The encoded features are then concatenated and processed by a three-layer MLP with channel sizes of 128, 64, and 32. Finally, a linear fully-connected layer produces an output indicating the truthfulness of the query response. In contrast, a state s is input to a model that outputs an action distribution a, encoded by a four-layer CNN encoder for the state map s map and a two-layer MLP for the agent inventory status s inv. The goal g is encoded by a two-layer MLP with a channel size of 64, resulting in a feature vector denoted as f g. Linear fully-connected layers predict modulation parameters for the state CNN encoder and produce modulation parameters for modulation fc. The state map is encoded by a four-layer CNN with channel sizes of 32, 64, 96, and 128. Each convolutional layer has kernel size 3 and strides 2, followed by ReLU nonlinearity. The feature maps are modulated and flattened to a feature vector. The state inventory is encoded by a two-layer MLP with a channel size of 64. Modulated features are used to produce an action distribution and predicted value using separate MLPs with channel size of 64 for both layers. The end-to-end learning models utilize a mechanism to remember completed subtasks from instructions. A memorization mechanism is augmented using the memory of another LSTM network, which takes encoded states as inputs. Attention scores are computed after each action, pooling the outputs of input encoders. The agent policy network learns to perform task conditioning on the attention-pooled latent instruction vector. The framework can utilize RGB inputs with similar performance and generalization ability. Detailed failure analysis is conducted on the execution traces of the model to understand how it works or fails. The framework can utilize RGB inputs with similar performance and generalization ability. Detailed failure analysis is conducted on the execution traces of the model to understand how it works or fails. The analysis includes identifying subtasks that commonly cause task failure, average time steps for successfully executed subtasks, and additional visualizations on completion rates of different end-to-end learning models. In failure analysis, the first failed subtask is identified as the initial subtask that results in a program's failure. The first failure rate, representing the likelihood of a specific subtask being the first to fail, is calculated. Results from running the full model on a complex test set show that subtasks in goto and place categories are more prone to being the first failed subtask compared to build_bridge, mine, and sell categories. Subtasks in goto and place categories often involve navigating to grid cells near the world's border. In failure analysis, subtasks near the border of the world have a higher first failure rate. Tasks in build_bridge, mine, and sell categories are completed faster. Subtasks in goto and place categories near the border take more time and are more likely to fail. Failure analysis focuses on completion rates of program executions with different conditioning variables. Plots in Figure 9 show that execution failure is more common with larger program sizes, loops, while statements, and subtasks. The effect of program features on completion rates varies across different models. For Seq-LSTM, more if and else statements or larger indent values lead to more failures, while for Transformer and Tree-RNN models, larger values result in fewer failures due to their design handling hierarchical structures better. The analysis focused on the impact of program features on completion rates across different models. It introduced an algorithm to estimate the number of branches in a program, highlighting the varying effects of if and else statements on performance. The results did not show a clear trend, attributed to certain assumptions made during the calculation process. The analysis did not reveal a clear trend due to possible inaccuracies in the metric or its suitability. A2C agents were trained with specific hyperparameters on a Nvidia Titan-X GPU. Natural language instructions can be ambiguous, affecting task performance. Ambiguous natural language instructions can lead to impaired performance in executing tasks. Ambiguity arises from unclear scope modifiers and conditional statements, causing potential failures in model execution."
}