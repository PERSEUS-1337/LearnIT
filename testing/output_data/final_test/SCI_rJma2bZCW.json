{
    "title": "rJma2bZCW",
    "content": "The statistical properties of the endpoint of stochastic gradient descent (SGD) are studied by approximating SGD as a stochastic differential equation (SDE) and analyzing its Boltzmann Gibbs equilibrium distribution. Three factors - learning rate, batch size, and variance of loss gradients - control the trade-off between the depth and width of the minima found by SGD. Wider minima are favored by a higher ratio of learning rate to batch size in the equilibrium distribution. The experiments show that the endpoint of SGD is similar under simultaneous rescaling of batch size and learning rate, leading to flatter minima. Dynamics are also similar under the same rescaling, allowing for exchange in a cyclical learning rate schedule. High noise levels improve generalization, but the similarity breaks down if the learning rate is too large or the batch size is too small. Deep neural networks (DNNs) have shown good generalization ability and achieved state-of-the-art performances in various domains like image and speech recognition. Training DNNs involves minimizing a loss function using SGD and its variants, updating parameters based on the negative loss gradient. The complex structure of DNN loss functions with multiple minima and saddle points makes SGD challenging. SGD converges to different regions of parameter space depending on optimization hyper-parameters and initialization. Recent works have investigated how SGD impacts generalization in DNNs, suggesting that wide minima generalize better than sharp minima. The ratio of learning rate to batch size is correlated with the sharpness of minima, not just batch size alone. SGD tends to find wider minima at higher noise levels in gradients, correlating with better generalization. Approximating SGD as a stochastic differential equation, the learning rate to batch-size ratio influences the trade-off between depth and sharpness of minima, favoring flatter minima with a high ratio. Scaling the learning rate linearly with batch size (up to a limit) leads to identical performance in DNNs. High learning rate to batch size ratio results in wider minima and better validation performance. Multiplying learning rate and batch size by the same scaling factor shows similar training dynamics. One can exchange learning rate and batch size for cyclic learning rate schedule. The relationship between SGD and sampling a posterior distribution via stochastic Langevin methods has been discussed in various papers. Stochastic gradient descent (SGD) dynamics can be divided into three phases: weights diffuse in the first phase, gradient magnitude dominates noise in the second phase, and weights approach the optimum in the final phase. Observations suggest that diffusion behavior in the last phase minimizes mutual information between input and hidden representation. Our analysis of SGD dynamics focuses on the stationary distribution over the entire parameter space, comparing the probability of reaching different minima. We introduce a novel approach using the Fokker-Planck equation to derive the temperature of the Gibbs distribution in terms of learning rate, batch size, and gradient covariance. Our work explains the importance of the ratio of learning rate to batch size in SGD dynamics. It shows that rescaling both parameters leads to minima of similar width despite using different batch sizes. This is in line with previous studies that emphasized the significance of the learning rate to batch size ratio in training. The importance of the learning rate to batch size ratio in SGD dynamics is emphasized. The study focuses on the Boltzmann-Gibbs equilibrium distribution with isotropic noise, linking the noise in SGD to the width of its endpoint. Empirical verification shows a correlation between the width and height of minima with the learning rate to batch size ratio. This work builds on previous research on the significance of noise in SGD. Our focus is on the impact of batch size and learning rate noise on the width and depth of final minima in SGD. We analyze the relative probability of ending optimization near a minimum based on loss value and Hessian determinant, considering local geometry, batch size, learning rate, and covariance of loss gradients. By deriving the equilibrium distribution of SGD under a stochastic differential equation treatment, we find a Boltzmann-Gibbs distribution due to isotropic covariance of loss gradients. The text discusses the theoretical setup for stochastic gradient descent (SGD) and the equilibrium distribution, which is a Boltzmann-Gibbs distribution. It outlines a model parameterized by \u03b8 and defines the loss function and gradient for N training examples. Stochastic gradients arise when considering a batch of random indices, leading to an unbiased estimate of loss and gradient. SGD with learning rate \u03b7 is defined by an update rule. Assumptions are made regarding the noise in the stochastic gradient being Gaussian. The text discusses the theoretical setup for stochastic gradient descent (SGD) and the equilibrium distribution, which is a Boltzmann-Gibbs distribution. It outlines a model parameterized by \u03b8 and defines the loss function and gradient for N training examples. By the central limit theorem (CLT), the noise in the stochastic gradient is assumed to be Gaussian with a symmetric positive-semidefinite covariance matrix. The discrete process of SGD is approximated by a Langevin equation with a normalized Gaussian time-dependent stochastic term. The continuous time approximation of SGD as a stochastic differential equation holds under the condition of a small learning rate. The Langevin equation allows for a general loss function with multiple local minima. The Langevin equation, a stochastic differential equation, has multiple local minima. Its equilibrium distribution, a Gibbs-Boltzmann distribution, provides insights into SGD behavior and convergence properties. The distribution is derived from the Fokker-Planck equation with detailed balance, governing parameter probability density evolution over time. Appendices provide the equation in the machine learning context and proofs of the stationary distribution of a Langevin system. The equilibrium distribution of a Langevin system is derived from the Fokker-Planck equation with detailed balance, assuming isotropic gradient covariance. It describes the probability of parameters being in a particular state asymptotically, influenced by noise in the system set by learning rate and batch size. The loss surface in SGD is affected by noise level n, while gradient variance C(\u03b8) is influenced by dataset and model priors. Investigating how different architectures and parameterizations impact gradient covariance is crucial. The assumption of fixed and isotropic gradient covariance in the parameter space simplifies analysis but may not reflect reality. Empirical evidence supports predictions based on noise, batch size, and learning rate relationships in practice. The probability of ending at a given minimum \u03b8 A in SGD optimization is derived in Appendix D. The unnormalized probability of ending near minima \u03b8 A is determined by the noise level n = \u03b7 S used in the process. This analysis categorizes minima based on their characteristics. In SGD optimization, the probability of reaching a specific minimum \u03b8 A depends on the learning rate, batch-size, and gradient covariance. The analysis categorizes minima based on loss depth and Hessian determinant. The probability is influenced by the noise level n = \u03b7/S, with a focus on the ratio of probabilities p A. In SGD optimization, the probability of reaching a minimum \u03b8 A is influenced by factors like learning rate, batch-size, and gradient covariance. The analysis shows that SGD favors wider minima over sharper ones towards equilibrium. Specifically, when two minima have the same loss value, SGD favors the one with a lower determinant of the Hessian. If two minima have the same curvature, SGD will favor the one with lower loss. In general, if one minimum has a higher loss, there is an upper bound on the noise level for it to be favored over the other. The noise level in SGD controls the optimization favoring wider over deeper minima. Increasing noise by adjusting the learning rate to batch size ratio determines the extent of this preference. Increasing the noise level in SGD favors wider over deeper minima, with a higher ratio of learning rate to batch size leading to wider minima. Empirical studies on a 4-layer MLP trained on Fashion-MNIST show that as the noise ratio increases, the norm of the Hessian at the minima decreases, indicating lower sharpness. Training three Resnet56 models on CIFAR10 using SGD with different batch sizes and learning rates, it was found that models with larger batch sizes or lower learning rates ended up in sharper minima. This aligns with the theoretical analysis that a higher ratio of learning rate to batch size favors wider minima. Models trained with similar noise levels ended up in minima of similar quality, as shown in further experiments with VGG-11 models. The experiment involves training VGG-11 models on CIFAR-10 with different learning rates and batch sizes. The models are trained with the same noise level but varying values of learning rate and batch size. Interpolation between model parameters shows that models with identical noise levels have qualitatively similar minima. The plots display train and test accuracy for VGG-11 architecture on CIFAR10 dataset, showing the impact of cyclic batch size schedule on model performance. The experimental study involves training VGG-11 models on CIFAR-10 with different learning rates and batch sizes. The results show that models with identical noise levels have similar minima, supporting the theoretical observation. The study focuses on the equilibrium endpoint and dynamical evolution of SGD in four experiments. In four experiments with the VGG-11 architecture on the CIFAR10 dataset, test accuracies with cyclic batch size and learning rate are similar. High \u03b7 S leads to better generalization in memorization tests on MNIST. Test accuracies with constant learning rate to batch-size ratio are also similar. This shows the interchangeability of learning rate and batch size. The endpoint test accuracies show the interchangeability of learning rate and batch size, consistent with theoretical calculations. Additional results on cyclical learning rate and batch size schedules are in Appendix F.4. The dynamical evolution phenomena is noted, with training and test accuracy curves showing similarity. Theoretical analysis does not explain this, but it is reported as an interesting observation. Loss curves are detailed in Appendix F.2, FIG4. In Appendix F.2, FIG4, the loss curves are examined in detail. It is observed that while epoch-averaged loss curves align well when swapping batch size for learning rate, the per-iteration loss is not consistent. Smaller batch sizes exhibit higher variance in per-iteration loss, attributed to the examples having greater variance from one iteration to the next. The key takeaway is that SGD's endpoint and dynamics remain approximately invariant when batch size and learning rate are scaled simultaneously, contrary to the common practice of scaling the learning rate with the square root of the batch size. The experiments in this section show that changing the learning rate and batch size to keep the ratio n = \u03b7/S constant results in the same equilibrium distribution. SGD with noise improves generalization while maintaining memorization levels. Experiments on the MNIST dataset with an MLP reveal the impact of random labels on training. The experiments evaluate the impact of changing learning rate and batch size ratios on performance. Increasing these values initially improves performance until a breaking point is reached. The study highlights that SGD with low noise levels leads to optimization towards a minimum with low generalization ability. SGD with larger noise steers away from sharp solutions, while memorization starts after reaching maximum generalization. Momentum runs exclude learning rates above 0.02 to prevent divergence. Learning curves are shown in FIG5 in Appendix F.3. High learning rates invalidate the first order approximation in Taylor's expansion, making the continuous limit of SGD update equation invalid. The use of high learning rates can invalidate the stochastic differential equation and the Fokker-Planck equation, leading to an invalid theory. Empirical results show that increasing the learning rate and batch size by a factor up to a certain limit results in similar learning dynamics and final performance. The breaking point occurs for smaller learning rate values with smaller dataset sizes. Additionally, halving the noise level by reducing the learning rate also affects the breaking point. These experiments suggest that high learning rates are the reason behind the breaking point. The experiments suggest that high learning rates are the reason behind the breaking point, as performance drops at higher \u03b2 when the base learning rate is halved. Theoretical assumptions include treating the learning rate as fixed throughout training, while in practice, it is annealed to a lower value. SGD prioritizes width over depth initially, then shifts to prioritize depth as noise decreases. Additional assumptions include isotropic covariance of gradients for deriving a closed form solution. In practice, mechanisms may drive covariance towards isotropy, potentially improving optimization. Existing methods like batch normalization or careful initialization may contribute to more equalized covariance. Theoretical analysis assumed an equilibrium distribution independent of intermediate dynamics, but in practice, this may not hold true. Without isotropic covariance, the system's solution will depend on the path taken. In experiments on memorization, noise levels affect the preference for wide minima over sharp ones. SGD initially learns true labels, then focuses on random labels, with noise maintaining generalization. Lower noise levels lead to better fitting of random labels but worse generalization on true labels. Noise plays a crucial role in SGD optimization of DNNs, impacting the trade-off between width and depth of minima. The role of noise in SGD optimization of DNNs is crucial, influencing the trade-off between the width and depth of minima. Factors like batch size, learning rate, and gradient variance strongly impact the properties of the final minima. Higher noise levels lead to wider minima, correlating with better generalization. The noise factor n = \u03b7/S determines the width and height of the minima towards which SGD converges, affecting the memorization phenomenon. However, there are limitations, such as when the learning rate becomes too large. Rescaling \u03b7 and S simultaneously is possible as long as the noise \u03b7/S remains constant. The Fokker-Planck equation is derived in this appendix to explain the probability distribution of parameter values in stochastic differential equations. The proof aims to provide intuition for the machine learning audience, using tensor index notation for brevity. Rescaling \u03b7 and S simultaneously is feasible as long as the noise \u03b7/S remains constant. The stochastic differential equation is formalized with a probability expression for a given noise function. A small step in time is considered to determine how parameters move, with expectations of the noise being normalized Gaussian. The text discusses the Chapman-Kolmogorov equation in the context of stochastic differential equations and normalized Gaussian noise. It explains the process of moving parameters over time and the integration of multiple paths to calculate probabilities. The text discusses the Fokker-Planck equation in the context of stochastic differential equations and normalized Gaussian noise, providing supplementary comments on its intuition and a rewritten form under certain conditions. In terms of rescaled time coordinate t \u03b7, the ratio between drift and diffusion terms is governed by \u03b7\u03c3. A higher ratio leads to more diffusive evolution, while a lower ratio allows the drift term to dominate. This ratio controls the evolution towards the stationary distribution in SGD convergence, with learning rate and batch size being interchangeable. The time to reach the stationary distribution depends on \u03b7 due to rescaling. The time to reach the stationary distribution in SGD convergence depends on the learning rate \u03b7. A higher learning rate leads to quicker convergence by a factor of 1/\u03b7. However, the first order SGD update equation is only valid for small enough \u03b7, as the approximation breaks down for high values. Learning rate and batch size are interchangeable up to a maximum value of \u03b7 where the approximation breaks. The equilibrium solution of the Fokker-Planck equation for an isotropic covariance is given by a specific formula. The equilibrium distribution is proven by solving the Fokker-Planck equation with detailed balance, using a probability current J. The stationary solution requires \u2207 \u03b8 \u00b7 J = 0, but the equilibrium solution demands detailed balance to occur. The stationary solution with detailed balance ensures zero probability currents, leading to entropy increasing with time. The equilibrium solution results in a desired probability distribution, with a finite loss function controlled by a non-negative constant. In this appendix, we derive the discrete set of probabilities of ending at each minima using Laplace's method to approximate the integral of the probability density near a minimum. The approach is common and involves approximating the loss function near a minimum using the Hessian matrix. This approximation is specific to the region near the minimum and differs from global approximations in other studies. In this appendix, we use Laplace's method to approximate the integral of the probability density near a minimum in order to derive the discrete set of probabilities of ending at each minima. The approach involves approximating the loss function near a minimum using the Hessian matrix, allowing for a general loss with multiple minima. The region must be sufficiently larger than det H A to validate the local assumption. The minima should be far apart for the approximation to be valid. The unnormalized probability can be considered for relative probabilities between minima. In practice, strict minima are unrealistic for deep neural networks with a large number of parameters. In deep neural networks, the endpoint of training is not expected to be a strict minimum but rather a point where the Hessian has positive eigenvalues in a few directions. Understanding which minima stochastic gradient descent (SGD) favors involves considering the distribution the iterate of SGD follows at equilibrium. In cases of minima with degenerate directions, a minimum with more volume is more probable. Experiments with Resnet56 architecture on CIFAR10 dataset show the impact of adjusting the \u03b7 S ratio. Adjusting the \u03b7 S ratio in experiments with Resnet56 architecture on CIFAR10 dataset reveals that multiplying the ratio by a factor up to 5 yields similar performances, but degrades for factors above 5. The theory may become unreliable when the discrete to continuous approximation fails, covariance in gradients is non-isotropic, batch size is comparable to training set size, or momentum is considered. The approximation breaks down in cases of highly non-isotropic gradients, leading to complex partial differential equations for the equilibrium solution. Our theory may become unreliable when the discrete to continuous approximation fails, covariance in gradients is non-isotropic, batch size is comparable to training set size, or momentum is considered. The theory does not involve the finite size of the training set, which is a drawback. When the batch size becomes large compared to the training set size, we expect the theory to break down. Additionally, the theory does not consider momentum, which is another drawback, especially in models where momentum is important. In experiments exploring the correlation between learning rate to batch-size ratio and sharpness of minima, a peak validation accuracy is observed at a learning rate to batch-size ratio of around 2 \u00d7 10 \u22123. Higher learning rate to batch-size ratio does not necessarily lead to higher validation accuracy, instead acting as a control. The experiment involved testing the correlation between learning rate to batch-size ratio and validation accuracy in a challenging setup with 20 layers and no Batch Normalization. Results showed a correlation between Hessian norm and learning rate to batch-size ratio, as well as validation performance. Models with large batch size or low learning rate ended up in a sharper minimum compared to the baseline model. In this appendix, experiments from Section 4.2 demonstrate the exchangeability of learning rate and batch size. The comparison between different cyclical training schedules shows that discrete schedules perform similarly or slightly better than triangular schedules. The discrete S schedule leads to wider minima for similar loss. The orange and blue lines in the learning rate to batch-size ratio plot have the same ratio throughout. The dynamics of cyclic schedules show similar behavior in terms of learning rate to batch-size ratio throughout training, supporting the theoretical result that this ratio governs the stationary distribution of SGD. Exchangeability is observed not only in the stationary distribution but also throughout training, especially in cyclic schedules. Similar behavior is seen in standard learning rate annealing schedules as well. In a memorization experiment with 0.0 momentum, learning curves were reported. Results showed a correlation between batch size and learning rate ratio and norm of Hessian. It was noted that cyclic learning rate (CLR) schedule leads to better generalization. The exchangeability between CLR and cyclic batch size (CBS) suggests that the generalization benefit of CLR comes from varying noise levels. Running VGG-11 on CIFAR10 with 4 training schedules further explored this concept. In experiments with varying noise levels, cyclical schemes show oscillation between sharp and wide parameter space regions. Discrete schedules perform similarly to triangular CLR schedule, reaching wider minima at the same loss value. This suggests that changing noise levels in cyclical schemes can lead to wider minima. Cyclical schemes in experiments show oscillation between sharp and wide parameter space regions, finding wider minima than baseline runs at the same loss level. The implications and comparisons with other learning schedules are left for future work."
}