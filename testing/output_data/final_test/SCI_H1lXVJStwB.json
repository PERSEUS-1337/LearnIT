{
    "title": "H1lXVJStwB",
    "content": "We introduce dynamic instance hardness (DIH) to aid in training machine learning models. DIH is a property of each training sample, computed as the running mean of its instantaneous hardness. It helps evaluate how well a model retains knowledge over time. For deep neural nets (DNNs), samples with high DIH early on tend to maintain high DIH later, allowing for effective reduction of training samples in future epochs. By focusing on historically challenging samples, using DIH during training speeds up the process without additional computation. The DIH curriculum learning framework focuses on historically challenging samples to improve model accuracy. It does not require additional inference steps and utilizes dynamic instance hardness for stability. The problem is formulated as finding a curriculum that maximizes a multi-set function, leading to significant performance improvements in DNNs compared to random mini-batch SGD. Empirically, DIHCL-trained DNNs outperform random mini-batch SGD and other curriculum learning methods in efficiency, convergence, and final performance. The concept of \"dynamic instance hardness\" (DIH) is introduced to measure the difficulty a model faces over time in learning each sample from a training set. Various metrics are proposed to measure DIH, all sharing the same form as a running mean over different instantaneous sample hardness measures. In this work, three different notions of instantaneous instance hardness are introduced: (A) the loss, (B) the loss change between two consecutive time steps, and (C) the prediction flip. These metrics provide information on the difficulty a model faces in learning each sample over time. In this work, three different notions of instantaneous instance hardness are introduced: (A) the loss, (B) the loss change between two consecutive time steps, and (C) the prediction flip. Dynamics is achieved by computing a running average over instantaneous instance hardness, with a discount factor \u03b3 \u2208 [0, 1]. The set of training samples selected at time t, denoted as S t, should be large early during training but can be significantly smaller as the hardness values decrease. The hardness values can vary dramatically between samples, with some being more memorable and retained more easily than others. The concept of sample hardness in training data is discussed, where some samples are more memorable and retained easily, while others are harder to learn. Once a sample's hardness value is established, it tends to maintain its relative position among all samples. This allows for early identification of sample hardness categories during training. The concept of sample hardness in training data allows for early identification of categories of hard samples, similar to strategies like the Leitner system for spaced repetition and boosting. Instance hardness has been studied before as the complement posterior probability and as an average over training history. In Toneva et al. (2019), instance hardness is studied as an average over training steps, leading to a novel curriculum learning strategy. Unlike their approach, our study generalizes this concept to running dynamics computed during training, eliminating the need for additional models or inference steps. Our study generalizes the concept of instance hardness to running dynamics computed during training, eliminating the need for additional models or inference steps. The observed trend is that samples become easier to learn over time, with easy samples remaining easy and hard samples becoming easier as training progresses. This leads to a formulation of DIHCL as an online learning problem that maximizes the quality of the curriculum by observing partial observations over time. DIHCL performs optimization in a greedy manner by selecting hard samples with higher probabilities. The model is updated based on the selected subset, leading to a speedup in training. To encourage exploration, DIHCL initially sweeps through the entire training set. Different options for DIH-weighted subset sampling introduce randomness in the selection process. DIHCL optimizes non-convex problems by introducing randomness in sample selection. Theoretical bounds on curriculum achievement are provided. Empirical evaluation on 11 datasets shows DIHCL's advantage in time/sample efficiency and test accuracy over other algorithms. Early curriculum learning work emphasizes the importance of optimized training sets fed into algorithms. Self-paced learning (SPL) and its variations aim to improve model performance by selecting samples with smaller losses and gradually increasing subset size over time. Self-paced curriculum learning combines human expertise with loss-adaptation, while machine teaching focuses on finding the optimal training subset for similar performance. Minimax curriculum learning emphasizes sample diversity. Minimax curriculum learning (MCL) emphasizes the importance of sample diversity in early learning and uses a minimax optimization approach. Compared to other methods, DIHCL improves efficiency by not requiring extra inference on the entire training set per step and uses a more stable hardness metric. The study also relates to overfitting in noisy data and suggests that simple patterns are learned mainly in the early stages of training. Our work is distinct from catastrophic forgetting and focuses on single task learning, showing that easy samples remain easy. It is related to online submodular function optimization, including maximization and bandit settings with noisy feedback. The work on instance hardness is also closely related to ours, where sample difficulty is considered. Our study focuses on the dynamics of Difficulty of Instance Hardness (DIH) during training of Deep Neural Networks (DNNs). We conducted an empirical study using a WideResNet on the CIFAR10 dataset with a modified cosine annealing learning rate schedule. Our approach differs from previous studies by analyzing DIH during training rather than after training completes. The study focuses on analyzing the dynamics of Difficulty of Instance Hardness (DIH) during training of Deep Neural Networks (DNNs) using a WideResNet on the CIFAR10 dataset with a modified cosine annealing learning rate schedule. It includes a monotone decreasing schedule and tests two types of instantaneous instance hardness. The training samples are divided into three groups based on prediction flips, and the dynamics of average prediction flips over each group are visualized. The dynamics of average prediction flips and loss variance in different sample groups are analyzed during training of Deep Neural Networks. Easy samples with small prediction flips are quickly learned and maintain small losses, while hard samples with large prediction flips have oscillating losses and are difficult to learn. Hard samples show consistency with the learning rate schedule, indicating the challenge of achieving good performance on them. During training of Deep Neural Networks, it is beneficial to focus more on learning hard samples frequently and reduce learning on easy samples. Empirical verification shows that as learning progresses, samples become less informative. Curriculum learning procedure can be modeled based on additional mathematical assumptions. The curriculum learning procedure maximizes a diminishing return function by selecting subsets of data points. Each subset is a mini-batch, and the goal is to find the best curriculum that maximizes the function. The function is an unknown arbitrary function in practice. The curriculum learning procedure aims to maximize a diminishing return function by selecting mini-batches of data points. The function is unknown and inaccessible for optimization, but DIH values can be used as a surrogate to optimize it based on partial observations. This approach turns the problem into an online optimization task where the goal is to find a curriculum that maximizes the function by selecting subsets of samples at each step. The curriculum learning procedure aims to maximize a diminishing return function by selecting mini-batches of data points. The objective is to find a curriculum that increases the probability of learning on hard samples and reduces learning on easy ones. The solution provides an approximate solution to the ideal but intractable optimization problem, with a tight bound to the global optimal solution. The DIH Curriculum Learning method aims to select subsets of data points based on instance hardness, optimizing model parameters and updating instance hardness dynamically. This approach reduces inference on easy samples and focuses on hard ones, improving learning efficiency. DIHCL selects subsets of data points based on instance hardness, updating model parameters dynamically. It focuses on hard samples to improve learning efficiency by training on more samples early on for an initial accurate estimate of instance hardness. The DIHCL method selects subsets of data points based on instance hardness to improve learning efficiency. It starts with an accurate estimate of instance hardness by training on more samples early on. The subset size is gradually reduced as training progresses, with a focus on the most difficult samples. Additionally, diversity of selected samples is encouraged in the earlier stages to further reduce training time. The diminishing return property for a function f can be achieved if certain properties are assumed, such as f being normalized and monotone. The function r t (i) is observed to be monotonically decreasing, indicating the possibility of a diminishing return function governing its behavior. The function r t (i) may be governed by a diminishing return function, enabling further analysis. Bounds for DIHCL-Greedy(Alg. 1) can be derived under certain assumptions. The solution of DIHCL-Greedy outperforms any solution S * 1:T , achieving a minimum loss term C f,m reflecting warm start phase. The 1 \u2212 e \u22121 factor in the bound on g is tight to constant factors due to hard cases provided in the Appendix. Incorporating a submodular function G can model data point interactions in DIH value. Adding randomness to the selection procedure in Alg. 1 improves performance by exploring samples with small r t (i) and achieving a good quality solution for non-convex models like DNNs. Adding randomness to the selection procedure in Alg. 1 improves performance by exploring samples with small r t (i) and achieving a good quality solution for non-convex models like DNNs. We propose three sampling methods to replace line 7 of Alg. 1, including DIHCL-Beta which utilizes Thompson sampling with a Beta distribution prior to balance exploration and exploitation. We propose three sampling methods to replace line 7 of Alg. 1, including DIHCL-Beta which uses a Beta distribution prior to balance exploration and exploitation. Mini-batch SGD with momentum of 0.9 and cyclic cosine annealing learning rate schedule are utilized. More details about the datasets and settings can be found in the Appendix. In DIHCL variants, \u03bb 1 = 1.0, \u03b3 \u03bb = 0.8, \u03b3 k = 0.4 are used to reduce S t by solving Eq. (5). A \"facility location\" submodular function is employed, utilizing a Gaussian kernel for similarity. Test set accuracy changes with increasing training batches in curriculum learning methods on 3 datasets are shown in Figure 3. Results for 8 other datasets and wall-clock time for training and submodular maximization in DIHCL with diversity and MCL are in the Appendix. Final test accuracy for each method is reported in Table 1. DIHCL and its variants demonstrate faster and smoother gains in test accuracy compared to baselines, particularly in the early stages of training. They also show higher final accuracy and improved sample efficiency. MCL can achieve similar performance but is less stable and requires more computation. SPL exhibits instability due to its use of instantaneous instance hardness for sample selection. Compared to MCL and DIHCL, SPL and random mini-batch curriculum methods require more epochs to reach peak accuracy. The curriculum method requires more epochs to reach peak accuracy as it focuses on easier samples, while DIHCL variants achieve the best accuracy on some datasets. DIHCL-Exp and DIHCL-Beta show advantages with stable running mean computed on loss and prediction flips. The diminishing return property of f is defined over an integer lattice. In the context of selecting mini-batches for training machine learning models, a function f is assumed to be normalized and monotone. The gains of items in the multi-set diminish as their counters increase. The function g reflects the observed gains from f as data is selected for training. Function g reflects the observed gains from function f as data samples are selected for training. It is permutation-variant for k > 1 and connects the solution of DIHCL-Greedy to the greedy solution with singleton gain oracle. The observed gains from function f are reflected in function g, which is permutation-variant for k > 1. It connects the solution of DIHCL-Greedy to the greedy solution with singleton gain oracle. Note that sets with subscript 0 are empty sets. A lower bound of g(S 1:T ) is derived in terms of g(\u015c 1:T |S 1:T \u22121 ). The inequality C f,m \u2265 mf (V ) \u2265 g(\u015c 1:m |S 1:m\u22121 ) holds due to the diminishing return property. The diminishing return property ensures Eq. 13 and Eq. 16 hold, while Eq. 15 is a result of the greedy step. The proof guarantees finding an element in the sequence history. For modular function f and non-zero evaluations for k elements in V, the bound equals g(S 1:T-1) + C f,m = g(\u015c 1:T |S 1:T-1). Sweeping all elements in the ground set precedes identifying non-zero-valued elements. A lower bound of g(\u015c 1:T |S 1:T-1) is compared to g(S* 1:T) for T < T. The proof shows that by rearranging equations, the gap to the optimal solution is reduced by at least 1/k at each time step. A hard case scenario is presented with a factor of 1/k, where the optimal solution can select all k elements from one group at a time, while the algorithm may only select one element from each group. This highlights the difference in performance between the algorithm and the optimal solution. The proof demonstrates that the algorithm's performance is at least 1/k worse than the optimal solution, as it can only select one element from each group at a time, while the optimal solution can select all k elements from one group. By combining Lemma 1 and Lemma 2, the first factor for the bound in Theorem 1 is obtained. The solution g(\u015c 1:T |S 1:T \u22121 ) is connected with selecting the entire ground set V at every step. The solution g(S * 1:T ) is bounded by g(V 1:T |V 1:T \u22121 ) by partitioning S * 1:T into two parts. The proof shows that the algorithm's performance is 1/k worse than the optimal solution, as it selects one element from each group at a time. By combining Lemmas 1 and 2, the bound in Theorem 1 is obtained. The solution g(S * 1:T ) is bounded by partitioning S * 1:T into two parts, where one part contains every element exactly once and the other part contains the remaining elements. The proof demonstrates that the algorithm's performance is slightly worse than optimal, selecting one element from each group at a time. By combining Lemmas 1 and 2, the bound in Theorem 1 is derived. The remaining element in S * t is guaranteed to be in its conditioning history S * 1:t\u22121, allowing for the use of inequalities to bound terms. This leads to the derivation of Eq. 34 from Eq. 33, ultimately resulting in the second factor k 2n for the bound in Theorem 1. The hard case example involves distributing n elements into k groups, resulting in a ratio of k/n for n < k^2 and 1/k for n \u2265 k^2. A hard case with a 1/k^2 factor is shown, where the optimal solution achieves k^2 while the greedy solution gets k. Theorem 1 holds true for any solution S*1:T and the optimal solution. Theorem 1 holds true for any solution S*1:T and the optimal solution for g and f can differ. A random sampling procedure similar to Mirzasoleiman et al. (2015) is used in DIHCL-Greedy-random. The solution of DIHCL-Greedy-random outperforms any solution S*1:T. The selection of every S t in DIHCL-Greedy-random is a greedy process. In step j, the selected item is denoted by v j. The expected gain is calculated considering the probability of intersection. The gap to the optimal solution is reduced by (1 - )/k for each selected item v j. The sample without replacement can be approximated by sampling with replacement when n is large. Noisy feedback from the gains of function f can be obtained, turning the problem into a multi-armed bandit problem. Utilizing the contextual bandit UCB algorithm proposed in Krause & Ong (2011) can lead to a \u221a T dependent regret when noise is assumed to form a martingale difference sequence. Contextual information becomes crucial under the noise setting, as the function has DR-property. However, calculating large kernel matrices for utilizing contextual information is not feasible for efficient curriculum learning. The regret bound for a function with DR property and bounded RKHS-norm gain is derived using the contextual bandit UCB algorithm. The maximum information gain is defined based on perfect information about the function, and a regret bound is obtained by utilizing historical sequences. Additionally, dynamic instance hardness in early stages can predict later dynamics. In Section 2, dynamic instance hardness in early stages can predict later dynamics by using DIH values computed on early training history. Figure 4 shows matrices quantitatively verifying this statement based on CIFAR10 training experiment results. The matrices measure overlapping percentages for samples with large/small DIH values between different epochs, demonstrating DIH's ability to predict forgettable and memorable samples accurately. The DIH metric is stable and consistent across training trajectory, with the ability to detect noisy data. It loses the capability to predict future DIH values and reflects only past history. An empirical study on training neural nets with noisy data by replacing ground truth labels with random labels is conducted. The study involves replacing ground truth labels with random labels during training to analyze the impact on prediction flip and loss. Results show that DIH can reflect past history but not predict the future, indicating potential for distinguishing noisy data or adversarial attacks. Additionally, a smaller CNN architecture is used to further investigate the training process. The study explores the use of DIH in training neural nets with random labels, showing its ability to reflect past history but not predict the future. Results suggest DIH may help distinguish noisy data or adversarial attacks. Comparisons between DIH and instantaneous loss reveal DIH's smooth and consistent measure of learning progress on individual samples. The study discusses the use of DIH in training neural nets with random labels, highlighting its ability to reflect past history but not predict the future. DIH provides a smoother and more consistent measure of learning progress on individual samples compared to the instantaneous loss. The text also includes a comparison of DIH and instantaneous loss on different datasets, showing the stability of DIH during training. The study compares DIH and instantaneous loss in training neural nets with random labels, showing DIH's smoother measure of learning progress. Test accuracy changes with training batches and wall-clock time for 11 datasets are reported in Figures 9-12."
}