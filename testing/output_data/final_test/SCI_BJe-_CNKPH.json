{
    "title": "BJe-_CNKPH",
    "content": "The attention layer in neural network models provides insights into the model's reasoning behind predictions, with recent debates on the interpretability of attention weights. This work aims to clarify when attention is interpretable through experiments on NLP tasks, validating the interpretability through manual evaluation. Attention is a weighted sum of vector representations used in various tasks like machine translation and language modeling. Attention in neural network models, used in tasks like machine translation and language modeling, provides interpretability and performance benefits. Studies show mixed results on the interpretability of attention weights, with some indicating they capture linguistic notions while others suggest they are not easily interpretable. In this paper, the authors aim to provide a comprehensive explanation of the attention mechanism in neural network models. They conducted experiments on text classification, Natural Language Inference (NLI), and Neural Machine Translation (NMT) tasks using models trained with neural attention and fixed attention weights. The results suggest that the attention mechanism in text classification does not significantly impact performance, challenging previous conclusions about the interpretability of attention in these models. The analysis focuses on the interpretability of attention in NLP tasks like text classification, NLI, and NMT. Attention is found to be crucial for these tasks, with manual evaluation used to examine attention weights. The study extends prior work by exploring diverse tasks and models with self-attention, providing a comprehensive understanding of attention mechanisms. The study explores the interpretability of attention in NLP tasks like text classification, NLI, and NMT. Attention weights are found to be interpretable under certain conditions, validated through manual evaluation on various task categories including single sequence tasks like sentiment analysis. The study investigates the interpretability of attention in NLP tasks such as text classification, NLI, and NMT. It includes experiments using various datasets for review rating, question answering, and generation tasks like neural machine translation. In this section, the study explores neural attention-based models for different NLP tasks using English-German datasets. The model architecture involves token representation with GloVe embeddings, Bi-RNN encoder for contextualized token representation, and attention mechanism for computing attention weights. The study explores neural attention-based models for NLP tasks using English-German datasets. The model architecture involves token representation with GloVe embeddings, Bi-RNN encoder for contextualized token representation, and attention mechanism for computing attention weights. Bahdanau et al. (2015) proposed a method for computing attention weights \u03b1 i for all tokens, followed by a dense layer and softmax for prediction. The hierarchical attention model (Yang et al., 2016) involves attention over tokens to obtain a sentence representation, followed by attention over sentences to obtain an instance representation h \u03b1, which is used for prediction. The study explores neural attention-based models for NLP tasks using English-German datasets. The model architecture involves token representation with GloVe embeddings, Bi-RNN encoder for contextualized token representation, and attention mechanism for computing attention weights. Bowman et al. (2015) use two separate RNNs for encoding question and paragraph sequences. Rockt\u00e4schel et al. (2016) propose a variant of attention where the encoder of y is initialized with the final state of x's encoder, allowing for conditional encoding. Attention over tokens of x is defined with a weighted sum of tokens fed to a classifier for prediction. The paper focuses on Neural Machine Translation (NMT) for generating target language sequences from a given source text. It utilizes RNN-based encoder and decoder models, with attention mechanism for computing attention weights. The encoder computes contextualized token representations using a Bi-RNN, while the decoder generates target words at each time step. Attention weights are computed using a feed-forward network proposed by Bahdanau et al. (2015) and Luong et al. (2015). The paper explores attention mechanisms in various models for different tasks, including single and pair sequence tasks using pre-trained BERT models, and neural machine translation using Transformer models. The focus is on analyzing attention weights to understand if attention serves as an explanation in these models. In a variety of models on multiple tasks, attention explainability is explored. Some studies suggest attention does not provide an explanation, while others show it encodes linguistic notions. The findings of both lines of work are claimed to be consistent, with attention seen as a gating unit in the network for single sequence tasks. The attention mechanism in neural networks can be interpreted as a gating unit for single sequence tasks, where input is transformed and computed with gating scores. However, for pair sequence and generation tasks, attention also depends on another text sequence and current hidden state, resulting in a different form of attention mechanism. Evaluation results on single sequence tasks show the performance of attention models and the change in accuracy for its variant. The evaluation results show that altering attention weights during inference leads to a greater performance degradation compared to varying them during training and inference. This suggests that weights learned in single sequence tasks may not truly represent attention and do not reflect the model's reasoning behind predictions. This challenges the explainability of attention weights in single sequence tasks. In this section, various attention mechanisms are compared for different tasks. Three variants are analyzed for each model: Uniform, Random, and Permute. Uniform assigns equal weights to inputs, Random samples weights from a uniform distribution, and Permute randomly permutes learned attention weights during inference. The evaluation results on single sequence datasets SST, IMDB, AG News, and YELP show that permuting during inference leads to a decrease in accuracy compared to the base model. The model becomes more robust to handle altered weights in the Train+Infer case of Uniform and Random attentions. However, in the Infer scenario, the degradation in accuracy increases. The reduction in performance from Permute is consistent across all datasets and models, supporting previous observations that alternating attention in text classification tasks does not significantly impact model output. The impact of altering attention weights on pair sequence and generation tasks is more significant than on single sequence tasks. Results show a substantial decrease in performance for pair sequence and generation tasks compared to single sequence tasks. The evaluation results on neural machine translation show a significant decrease in performance for pair sequence and generation tasks compared to single sequence tasks. The results validate the proposition that altering attention affects model output, especially when the attention layer cannot be modeled as a gating unit in the network. Visualizing the effect of permuting attention weights further reinforces this claim. In Figure 2, the relationship between maximum attention value and median induced change in model output is reported over 100 permutations for all task categories. For single sequence tasks, even with attention weights in the range [0.75, 1.0], the prediction change is small compared to pair sequence and generation tasks. The importance of attention weights is investigated by removing one weight and comparing the input with the highest attention weights to a randomly selected input. The impact of attention weights on model output is analyzed by comparing the Jensen-Shannon divergence when removing a high attention weight input versus a randomly selected input. Results show that for single sequence tasks, even with significant differences in attention weights, the change in model output is small. However, for pair sequence and generation tasks, there is a substantial impact on the model output. The importance of attention weights on the performance of self-attention based models is analyzed, showing a substantial effect on tasks like NMT. Altering weights in self-attention models impacts performance across tasks, unlike single sequence tasks. In Transformer models, changing weights in the first step of Decoder and Across has the most significant effect. The impact of altering weights in self-attention models is significant, especially in the first step of Decoder and Across, almost halting information flow from encoder to decoder. Manual evaluation shows interpretability of attention weights on single and pair sequence tasks, with original weights remaining interpretable. However, changing weights in pair sequence tasks affects predictions substantially. In single and pair sequence tasks, attention weights with original weights are interpretable. Cohen's kappa score shows high inter-annotator agreement. Attention mechanism in single sequence tasks acts as a gating unit, giving higher weights to tokens relevant to sentiment. In single sequence tasks, attention weights are interpretable and remain correct in the majority of cases. However, in pair sequence tasks, attention reflects the reasoning behind model output. This paper addresses the contradictory viewpoints on the explainability of attention weights in NLP. The analysis extends to various NLP tasks, showing interpretable attention weights correlated with feature importance. Validation through experiments and manual evaluation reinforces these findings."
}