{
    "title": "By0ANxbRW",
    "content": "The paper proposes a technique to compress Deep Neural Networks by minimizing model complexity and changes in the loss function, achieving competitive results. This is in response to the growing interest in implementing DNNs on resource-bound hardware, with compression algorithms reducing model sizes without sacrificing accuracy. Compression techniques aim to reduce the size of Deep Neural Networks by pruning and quantizing parameters, addressing the mismatch between computational requirements and available resources for resource-bound platforms. In this work, the researchers reduced model size significantly by pruning and quantizing parameters. They introduced a method that combines Bayesian encoding with penalization during training to achieve superior compression with minimal loss in accuracy. Unlike previous approaches, they used the k-means objective for parameter encoding, reducing computational complexity. Additionally, they applied a hard constraint on training loss to maintain accuracy, leveraging existing information about the loss function. The researchers proposed a solution for constrained optimization by iteratively minimizing the k-means objective while maintaining the loss constraint. This method allows for adjustable compression rates and achieved state-of-the-art compression on popular datasets with minimal loss of accuracy. The redundancy of DNNs to noisy parameters has been utilized for simplifying computations and hardware design, with some approaches focusing on parameter pruning to reduce computational complexity. In this paper, the focus is on compressing a trained DNN model using pruning and weight-sharing techniques proposed by BID3. Pruning eliminates unnecessary network connections based on magnitude, while weight-sharing encodes parameters into a small dictionary using the kmeans algorithm. These techniques aim to reduce computations and storage requirements, with an intermediate tuning step to offset accuracy loss. Later studies have largely ignored changes in the training loss function. Later studies have shown that considering changes in the training loss function during pruning or quantization can improve compression techniques. BID1 formulated pruning as an optimization problem, while BID0 augmented the k-means objective with the training loss Hessian. In contrast, BID14 introduced soft weight-sharing, minimizing both training loss and model size simultaneously by using a Bayesian prior and optimizing the mixing ratio with SGD. The proposed algorithm compresses a trained neural network model by minimizing complexity while constraining the loss function. It focuses on k-means encoding of weights and discusses the optimization process for the loss function during training. The algorithm aims to control the trade-off between accuracy and model size, using SGD optimization. Training neural networks involves minimizing a loss function that quantifies dissimilarity between model output distribution and ideal distribution. The algorithm compresses a trained neural network model by minimizing complexity through k-means encoding of weights. The optimization process for the loss function during training aims to control the trade-off between accuracy and model size using SGD optimization. The algorithm compresses a trained neural network model by minimizing complexity through k-means encoding of weights. It iteratively updates centroids to reduce model complexity and control changes in the loss function. This approach combines weight-sharing and soft weigh-sharing techniques to take advantage of learned information. The goal is to formulate a solution for minimizing model complexity using the k-means objective function. The k-means objective is used to represent model complexity by learning centroids and network parameters that minimize this objective. Centroids are correlated to weights, and without constraints, the solution reverts to one solved by BID3 for Deep Compression. To address accuracy changes, a constraint on the loss function is introduced based on the optimal value. The method for minimizing model complexity involves iteratively solving equations to compress the model by eliminating unnecessary centroids and reducing k. Each iteration finds the nearest centroid to each network parameter and solves a constrained optimization problem to find a displacement for the model. The method involves iteratively solving equations to compress the model by eliminating unnecessary centroids and reducing k. The constrained optimization problem finds a displacement for the model parameters to update for the next iteration, satisfying the KKT condition. The system has two solutions, with one valid when the constraint is met. To estimate the loss function, a linear Taylor expansion is used, and the displacement of the weights is calculated with a closed form solution. The algorithm involves calculating the displacement of model parameters iteratively using a closed form solution. A trust region radius is used to update the value of \u03c1 in each iteration. The displacement for each iteration is found based on a constrained optimization problem, with a trust region radius as input. If the displacement is larger than the trust region radius, the algorithm adjusts the value of \u03c1. The algorithm calculates the displacement of model parameters iteratively using a closed form solution and adjusts the trust region radius accordingly. After compression, the accuracy of the model is tested and increased by 20% if sufficient. Tested on MNIST, CIFAR-10, and SVHN datasets. The study involves training LeNet-5 on MNIST, a smaller VGG network on CIFAR-10 and SVHN datasets. The algorithm achieves state-of-the-art compression rates, with parameters clustering around centroids. Trade-off between accuracy and model size is analyzed, comparing the optimal model obtained through the analysis. The proposed compression algorithm encourages parameters to gravitate towards centroids, verified on MNIST with 256 random centroids. Centroids are optimized using Algorithm 6, reducing the accuracy gap between the current model and compressed version. Little uncompressed model accuracy degradation is observed, with meaningful parameter changes depicted in FIG1. Parameters approach 4 centroids, with 86% gathered around them. The compression algorithm results in a 112\u00d7 reduction in model size with only a 0.53% drop in accuracy. The trade-off between accuracy and compression is controlled by the loss bound, with smaller model sizes leading to increased errors. Most clusters are eliminated quickly as the model size decreases, but error starts to increase rapidly as the size becomes smaller. The model size reduction leads to increased errors, with clusters being eliminated less often due to merging. The proposed method achieves state-of-the-art accuracy for MNIST and similar reductions in model size for SVHN and CIFAR-10 datasets. The complexity is minimized while maintaining accuracy by using a k-means objective. The algorithm introduced in previous works maintains accuracy by incorporating learned information into the optimization process. It iteratively finds the optimal complexity and achieves state-of-the-art compression with minimal accuracy loss on various datasets."
}