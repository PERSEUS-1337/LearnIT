{
    "title": "HkgXteBYPB",
    "content": "Recently, neural-network based forward dynamics models have been proposed to learn the dynamics of physical systems. Long-term predictions suffer from accumulating errors, leading to diverging trajectories from the ground truth. A novel Monte Carlo sampling method is introduced to sample multiple plausible trajectories and stabilize long-term predictions using a shape preservation loss and recurrent training. Our model outperforms existing baselines in predicting long-term dynamics of physical interactions. Using Monte Carlo dropout, we can train reinforcement learning agents faster and achieve better solutions. Predicting object trajectories accurately is crucial for various applications like robot control and computer vision. Physics simulators, including traditional numerical solvers and learned prediction models, struggle with accuracy in challenging scenarios. Small errors in input or model can lead to significantly different object trajectories, as seen in the example of an orange ball falling on a blue wedge. Deterministic physics engines predict only one trajectory, but uncertain situations may have multiple possible outcomes. Uncertainty-aware neural networks in machine learning avoid deterministic point estimates by predicting distributions or randomly sampling in predictions. In dynamics predictions, Monte Carlo sampling based dropout on model weights is used to model uncertainty and sample multiple trajectories for an initial state. A state-invariant recurrent training mechanism stabilizes trajectories and reduces error accumulation over long-time horizons. Introducing a new shape loss on model predictions improves shape preservation and trajectory stability. The fully differentiable forward dynamics model can sample multiple, accurate, and stable outcomes. A differentiable forward dynamics model can generate multiple accurate trajectories over long-time horizons, improving sample efficiency in robotic control. Stochastic forward dynamics predictors ground random exploration in the environment, enhancing generalization abilities compared to model-free reinforcement learning. The proposed stochastic differentiable forward dynamics model improves sample efficiency and learning speed in control experiments. It generates multiple plausible trajectories using Monte Carlo based graph-convolutional dropout and enhances long-term prediction accuracy with a fully connected shape loss term. This model benefits model-free reinforcement learning agents in physical manipulation tasks. Recent advancements in deep learning have led to successful systems for solving physical dynamics prediction problems by learning from data. Various approaches, such as graph-based models with object-centric and relation-centric representations, have been proposed. While existing works focus on single-step prediction during training, a new stochastic differentiable forward dynamics model improves sample efficiency and learning speed in control experiments, benefiting model-free reinforcement learning agents in physical manipulation tasks. Our proposed recurrent training scheme on multiple step predictions shows lower long-term error in experiments compared to existing works in computer graphics. Previous studies have addressed simulating future object states under constraints using different methods, such as Markov chain Monte Carlo algorithms and applying random impulses to colliding bodies. However, these methods may diverge from real physical behavior and require expertise in choosing simulation parameters for convergence. Han et al. (2013) discuss factors that improve visual plausibility in scenarios, such as the number of simultaneous collisions and the homogeneity of colliding objects. They highlight how physical systems can exhibit non-intuitive behaviors due to unobservable environmental states, which challenges the goal of achieving visual plausibility in computer graphics. This motivates the search for more advanced methods in physics engines to sample probable states accurately. The search for advanced methods in physics engines to sample probable states accurately led to the development of techniques allowing neural networks to incorporate uncertainty in model predictions. MeanVariance-Estimation (Nix & Weigend, 1994) predicts a normal distribution instead of point estimates in the output space. While this method showed promise in splash prediction with visually pleasing results, it led to incorrect shape predictions during test time due to lack of space-time consistency between particles in real objects. Stochastic regularization is proposed as a solution for addressing highly uncertain situations like collisions or force applications. Stochastic regularization is a growing field with practical applications in various research areas. Gal & Ghahramani (2016) proposed using dropout during training and inference to approximate Bayesian inference, allowing visualization of predicted trajectories. Injecting noise into neural networks has been successful as a regularization method and in training RL agents. Computational cost increases with the number of samples. In model-free RL, challenges like credit assignment and reward sparsity are addressed through reward shaping, improving sample efficiency. Designing shaping functions typically requires expert knowledge and constraints on task-solving. Predicting trajectories as a reward relaxation method with a single parameter - dropout rate, offers advantages. Hierarchical Relation Network (HRN) architecture introduced parametric noise for better exploration and rewards in action prediction networks. Stochastic methods show potential to enhance training efficiency in reinforcement learning. The stochastic forward dynamics model is based on the deterministic HRN, which propagates effects through object hierarchies for predicting particle positions. Gray blocks in the architecture represent graph convolutional effect propagation modules. HRN uses a tree-like graph structure to efficiently propagate effects, with edges describing shape and material properties within object graphs. The hierarchy is built by clustering particles based on their states, with nodes representing different levels of objects. The architecture takes past hierarchical physics graphs to predict particle positions. HRN uses hierarchical physics graphs to predict the next state of the scene graph based on external forces, collisions, and particle history. The Force, Collision, and History Modules compute effects on particles using graph convolutions, which are then combined in the Hierarchical Effect Propagation Module to predict the next graph state. The Effect Propagation Module in HRN predicts future particle velocity by estimating the influence between particles using hierarchical graph convolutions. It uses embedding vectors to describe the influence of one particle on another, and is implemented as a fully-connected neural network. This module takes into account the current particle state and predicts per-particle future velocity. The HRN model predicts particle velocity by considering position, velocity, and mass, along with embedding vectors. It uses a Monte Carlo dropout for stochasticity and introduces a shape loss term for improved predictions. The model generates realistic trajectories for training a model-free agent for manipulation tasks. Stable long-term predictions are essential for planning tasks. The HRN model's predictions are accurate for complex physical scenarios, but objects can fall apart quickly due to unrealistic deformations between groups. Introducing a fully-connected shape constraint improves shape preservation by imposing pairwise distances to be the same across all possible node combinations. The HRN model improves shape preservation by imposing pairwise distances to be the same across all node combinations. To reduce prediction errors, the model is trained recurrently in a state-invariant way, optimizing weights based on its own errors during training. This approach significantly reduces error accumulation during inference time. Our new approach involves using randomly sampled dropout masks on graph-convolution kernels to generate physically plausible trajectories. This method allows for the inference of multiple plausible trajectories by randomly sampling dropout masks for each generated trajectory during test time. The HRN architecture allows for dropout to be applied at different locations, resulting in physically plausible long-term predictions. Dropout is specifically applied to the force and collision modules, leading to consistent shapes in trajectories. This method improves the efficiency of exploration for model-free reinforcement learning agents. During training, a model-free policy is trained on a stochastic physics predictor for physical manipulation tasks. A reward relaxation method is introduced to accelerate training by rewarding the agent as soon as one of the trajectories leads to the goal. The dropout rate controls the level of reward relaxation, with higher dropout rates leading to quicker rewards for the agent. This method directs the agent towards the reward faster in early training stages. In experiments, a gradual reduction of the dropout rate during policy training improves convergence and efficiency compared to a fixed rate. Recurrent training and new loss functions enhance prediction quality for long-term trajectories. Monte Carlo sampling based dropout method generates high-quality trajectories, aiding in training a model-free policy for physical manipulation tasks more efficiently. Our models are evaluated on complex scenarios involving deformations and collisions. Training on various examples, we compare mean squared error on positions, velocities, and shape loss to assess performance. Our model significantly reduces long-term prediction errors in deformation and collision tasks compared to the HRN baseline. The recurrent training procedure works best with sequence lengths between 4 and 6 time steps, with gradual increases during training to prevent convergence issues. Visualizations show clear improvements in predicted trajectories over the HRN baseline. Our method outperforms the HRN baseline in predicting trajectories, preserving shapes and resolving collisions better. Monte Carlo sampling allows for generating multiple plausible trajectories from our predictor. The proposed method uses dropout to generate multiple trajectories with uncertainty in force applications and collisions. It shows realistic trajectories in collision scenarios by applying dropout to force and collision modules. The visualizations demonstrate physically and visually plausible predicted trajectories. The HRN model's modularity allows for targeted stochasticity via dropout, introducing uncertainty in force outputs. Our proposed sampling method utilizes dropout to introduce uncertainty in force and collision predictions, capturing trajectory distributions from single mode to multi-modal. Dropout rates between 0.05 and 0.3 enable fast convergence during training and visually plausible sample trajectories during inference. Inference dropout rates differing significantly from training rates can lead to biased predictions. Additional results on the effect of dropout rate on state distribution width can be found in supplement Figure 10. Our method utilizes dropout to sample multiple physically plausible trajectories in various scenarios, including deformations, collisions, falling on a wedge, and interactions between different materials. Stochastic simulation allows for multi-modal predictions, maintaining high performance in forward simulation despite the introduced graph-convolutional dropout. In various scenarios, reinforcement learning agents are trained in stochastic physical environments using Proximal Policy Optimization. One scenario involves moving cubes towards a goal region by applying forces, with uncertainty in force application. A constant dropout rate of 0.1 is used in the force module of the physics predictor during training. The agent is trained in a physics predictor with a 0.1 dropout rate. The ball hitting tower task is challenging for the RL agent, requiring collisions to push a cube out of a tower. Comparisons are made between deterministic and stochastic environments, showing faster learning in stochastic environments. In the \"ballhits-tower task\", comparisons are made between deterministic and stochastic environments. The agent finds shorter policies earlier in training in stochastic environments, indicating more efficient exploration. In the cube moving task, introducing action space noise leads to faster learning. Training in stochastic physical environments outperforms deterministic environments, allowing for better exploration and finding of shorter policies. The most effective learning method is in a stochastic environment with dropout rate annealing, enabling initial fast exploration without introducing too much stochasticity when precision is needed. The agent applies strong forces later in training, reducing randomness and improving learning pace. Stochastic HRN predicts future trajectories accurately, requiring minimal expert knowledge but careful design decisions like dropout rates. High dropout rates during inference can lead to unrealistic dynamics. During training, investigating dropout rate scheduling can improve convergence of the dynamics model, enhancing performance for reinforcement learning tasks. Optimizations such as non-linear annealing schedules, delayed dropout rate annealing, and finding suitable starting values can accelerate reinforcement learning. Further improvements for the physics predictor are crucial for scenarios with multiple materials, collisions leading to insufficient position prediction, and generalization to new scenes. The proposed sampling method generates physically plausible trajectories. The proposed sampling method produces physically plausible trajectories in single-and multi-object scenarios across various materials. Quality of roll-outs, like shape prediction, remains high despite introduced noise. Model-free reinforcement learning experiments show agents in stochastic environments explore better and learn faster. Dropout rate annealing effectively balances randomness without compromising benefits of stochasticity for exploration. Stochastic neural physics engines outperform conventional engines. In Figure 10, the effect of dropout rate on trajectory distribution width is shown in physics engines. Dropout is activated in the force module, comparing rates of 0.5, 0.3, and 0.1. Figure 11 displays stochastic predictions in a \"ball hitting tower\" scenario with dropout in force and collision modules. Prediction variance correlates with dropout rate strength. Highly complex behavior is observed in the scenario, with cubes falling off the tower at different rates."
}