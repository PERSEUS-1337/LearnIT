{
    "title": "SJf_XhCqKm",
    "content": "Driven by the need for parallelizable hyperparameter optimization methods, this paper explores open loop search methods like grid search, random search, low discrepancy sequences, and other sampling distributions. The paper proposes using k-determinantal point processes in hyperparameter optimization via random search to promote diversity. They introduce an approach to transform hyperparameter search spaces for efficient use with k-DPP and a Metropolis-Hastings algorithm for sampling from k-DPPs. Experiments show benefits in training supervised learners with limited budgets. Hyperparameters include regularization strength, model choices, procedural elements, and data preprocessing. Hyperparameter optimization is crucial for successful machine learning applications. Searchers suggest hyperparameter configurations for training models, with open loop and closed loop search strategies. Open loop searchers depend only on past configurations, while closed loop searchers consider past configurations and validation losses. Closed loop methods, such as Bayesian optimization and reinforcement learning, rely on validation loss feedback to suggest hyperparameter configurations. While closed loop methods have been shown to identify good configurations faster than open loop methods like random search, there is a renewed interest in embarrassingly parallel open loop methods due to the longer training times of modern deep learning models and the availability of cloud resources charged by CPU-hours. This paper explores open loop methods, focusing on diversity-promoting techniques like determinantal point processes (DPPs) and their connection to Gaussian processes (GPs). A sampling algorithm is introduced to support various dimensions and structures, showing superior performance in synthetic experiments compared to other open loop methods. The open-loop method outperforms other approaches in hyperparameter optimization experiments, especially when hyperparameter values significantly impact performance. Comparing to closed-loop Bayesian optimization, the open-loop method is more efficient, with open-source implementations available. Sequential model-based optimization techniques like Bayesian optimization adaptively sample hyperparameter spaces. These methods choose points in the hyperparameter space, train and evaluate models, and sample new points based on performance. While advantageous for fast evaluations, they can be cumbersome with limited time or budget. Some studies show that simple open-loop methods can outperform Bayesian optimization with a moderate number of hyperparameters. Parallelizing Bayesian optimization methods is challenging but crucial. While some algorithms can sample multiple points at once, the sequential nature of Bayesian optimization limits full parallelization. Research has explored using k-DPPs to optimize hyperparameters in parallel Bayesian optimization trials. The approach involves sampling a diverse set of points from a k-DPP over a relevance region for hyperparameter optimization tasks. It outperforms other methods and has better regret bounds. Sampling from a k-DPP is shown to be more effective than maximization for optimization tasks. The focus is on k-DPP sampling for fully-parallel optimization methods. Configuration evaluation methods have shown promising results. Configuration evaluation methods allocate resources adaptively to different hyperparameter settings, initially choosing a set of hyperparameters to evaluate and partially training models for them. After a fixed training budget, the best performing models are allocated more resources, resulting in high-quality models. These methods can be used as a replacement for initial hyperparameter assignments, and Gaussian Processes are commonly used for hyperparameter optimization due to their expressive power. Sequential sampling from a k-DPP with kernel K is an old hyperparameter optimization algorithm with information theoretic justification. The MCMC algorithm in Algorithm 2 allows for drawing samples with appealing properties from any space where uniform samples can be drawn. Sequential sampling from a k-DPP with kernel K allows for drawing samples with appealing properties from spaces where uniform samples can be drawn. Drawing k-DPP samples by sequentially sampling points proportional to the posterior variance is efficient, requiring only a single additional point for each increase in sample size. Recent trends have sparked interest in open loop methods for batch Bayesian optimization algorithms, where function evaluations are hallucinated using the prior or drawn based on posterior variance when no results are available. Open loop optimization methods like BID14's approach without hallucinated observations are equivalent to uniform sampling, while their approach with hallucinated observations is similar to sequentially sampling according to the posterior variance. SMAC BID12's open loop optimization is akin to Latin hypercube sampling followed by sampling uniformly among candidate points. Recent studies show that uniform sampling is competitive with closed loop methods for hyperparameter optimization tasks like deep neural networks. BID3's study compares random search and grid search in open loop methods. The paper compares random search and grid search for hyperparameter optimization, highlighting that random sampling is generally preferred due to promoting more diversity in relevant dimensions. Grid search aligns configurations with axes, leading to duplicates in irrelevant dimensions. However, grid search has an advantage in one dimension with maximum spacing between points. In hyperparameter optimization, random search is preferred over grid search for promoting diversity in relevant dimensions. Grid search aligns configurations with axes, leading to duplicates in irrelevant dimensions but has an advantage in maximizing spacing between points. The spread of a sequence is quantified using concepts like star discrepancy and dispersion. Star discrepancy is important in numerical integration as it bounds the integration error through the Koksma-Hlawka inequality. Low discrepancy sequences like the Sobol sequence are used for optimization tasks and Bayesian optimization schemes. Previous work on hyperparameter optimization focused on low discrepancy sequences for improved optimization performance. The optimization performance goal is to be close to the true optimum, not just have low discrepancy. Dispersion, measured by the radius of the largest Euclidean ball containing no points, helps in finding the optimum. The optimization error can be bounded based on the dispersion of the point set. The optimization error is bounded based on the dispersion of a point set x with dispersion d k (x). Dispersion can be computed efficiently and is at least \u2126(k \u22121/d). Low discrepancy does not imply low dispersion. Previous work shows that low-discrepancy sequences are also low-dispersion sequences, but they may behave differently. Uniformly drawn samples are not low dispersion. Optimal dispersion in one dimension is achieved with an evenly spaced grid. Optimal dispersion in one dimension is achieved with an evenly spaced grid, but it's unknown how to achieve this in higher dimensions. Finding a set of points with optimal dispersion is as difficult as solving the circle packing problem with k equal-sized circles. Dispersion is bounded from below, but it is uncertain if this bound is sharp. The dispersion of different sampling methods is compared in one and two dimensions, including the Sobol sequence and samples drawn from a k-DPP using an RBF kernel. The regular structure of the Sobol sequence leads to large plateaus in one dimension. The k-DPP samples show similar dispersion to the Sobol sequence but without plateaus. In experiments, k-DPP samples bias towards corners, beneficial for small search space bounds. Low-discrepancy sequences are limited to [0, 1] hypercube, not suitable for hyperparameter search with tree structure. Further study on k-DPP performance in real-world scenarios is conducted. In this study, the focus is on the k-DPP and its performance in real-world hyperparameter tuning problems. DPPs and k-DPPs are reviewed, with B representing the domain for sampling hyperparameter assignments. The DPP defines a probability distribution over subsets of Y, where elements are more likely to be chosen if they are dissimilar. L-ensembles are used to define the parameters of a DPP. The definition of L admits a decomposition to terms representing the quality and diversity of elements in Y. Discrepancy is a global measure depending on all points, while dispersion only depends on points near the largest \"hole\". Hexagonal tiling finds optimal dispersion in two dimensions. Each dimension of the d-dimensional Sobol sequence has plateaus. Featurized representation of y i is denoted by \u03c6 i \u2208 R d. Similarity kernel K is discussed. In future work, closed loop methods may use q i to encode evidence about hyperparameter quality. DPPs have support over all subsets of Y, including \u2205 and Y itself. k-DPPs are distributions over subsets of Y of size k, with known exact sampling algorithms for discrete items and continuous hyperrectangles. Sampling from k-DPPs defined over other base sets is not well-studied. Algorithm 1 presented by BID1 is a Metropolis-Hastings algorithm for sampling from discrete domains. Algorithm 2 is a generalization of the MCMC algorithm that allows sampling from any base set, including those with discrete, continuous, or mixed dimensions, as well as tree structures. This is the first algorithm to enable sampling from a k-DPP defined over spaces other than strictly continuous or discrete, utilizing the expressive capabilities of the posterior variance of a GP in these regimes. Algorithm 2 extends the MCMC algorithm to sample from base sets with mixed dimensions, including continuous and discrete ones. It directly samples points from the base set B and computes principal minors of L as needed for computations. Algorithm 2 extends the MCMC algorithm to sample from base sets with mixed dimensions, including continuous and discrete ones. It directly samples points from the base set B and computes principal minors of L as needed for computations. The algorithm computes the score for each item q i = \u03a8(\u03b2 i ) with probability p: \u03b2 = \u03b2 10. It requires less computation and space compared to Algorithm 1, saving space and computation whenever k log(N) < N. The feature vector \u03c6 i encodes hyperparameter attributes, assigning values to different hyperparameters. Values for \u03c3 2 determine the properties of models in hyperparameter search spaces. When \u03c3 2 is small, points interact little, and when \u03c3 2 is large, points are encouraged to be far apart. Real-world hyperparameter search spaces are often tree-structured, with each layer in a neural network adding new hyperparameters to tune. Binary hyperparameters like regularization are encoded using one-hot encoding. In hyperparameter optimization, different settings can be assigned to regularization strength based on whether it is \"on\" or \"off.\" Higher-level design decisions, such as the type of model used, can also be considered as hyperparameters. Experiments focus on optimizing hyperparameters for a convolutional neural network in text classification for binary sentiment analysis. Neural network for binary sentiment analysis on the Stanford sentiment treebank using the CNN-non-static model with skip-gram vectors. Hyperparameter optimization includes L2 regularization and dropout rates, with a budget of 20 evaluations. The study explores hyperparameter optimization for a neural network model in sentiment analysis. Different learning rate ranges are considered, with the best models found using k-DPP-RBF with fewer iterations. Comparison is made with a Bayesian optimization technique, BO-TPE, which sequentially evaluates points based on past performance. The study compares hyperparameter optimization techniques for a neural network model in sentiment analysis. BO-TPE, despite utilizing additional information, performs the worst due to its exploration/exploitation tradeoff. The variance of the best hyperparameter settings is lower when sampled uniformly or from a DPP. BID28 analyzed the stability of convolutional neural networks for sentence classification with respect to a set of hyperparameters. The study analyzed the impact of six key hyperparameters on neural network performance for sentiment analysis. Results show that k-DPP-RBF outperforms other methods even in spaces with good model values. Comparison against Spearmint BID23 reveals the importance of learning rate. The study compared the performance of different hyperparameter optimization approaches, including k-DPP-RBF, batch Spearmint, and sequential Spearmint. Results showed that k-DPP-RBF had the fastest average time to find the best result, while sequential Spearmint took the longest. The final average best-found accuracies were 82.61 for k-DPP-RBF, 82.65 for Spearmint with 2 batches of size 10, 82.7 for Spearmint with 10 batches of size 2, and 82.76 for sequential Spearmint. In hyperparameter optimization experiments, k-DPP-RBF outperformed other open loop methods, with sequential Spearmint taking significantly longer to find the best solution for only a slight increase in accuracy. The study demonstrated the effectiveness of k-DPP samples in various metrics, even for large values of k, and an open-source implementation of the method is available. The expected star discrepancy of sequences chosen uniformly at random from [0, 1] d is at least 1 k, with bounds depending on absolute constants. Sobol sequences start to suffer as d grows large relative to k. A formulation of a DPP with star discrepancy between Sobol and random for all k is studied theoretically. Experimental results were not included. This work motivates looking at DPPs for hyperparameter optimization. The Koksma-Hlawka inequality bounds numerical integration error by the product of star discrepancy and variation. Finding points with low star discrepancy can improve integration approximations. Voronoi diagram over a point set can help measure dispersion. Hyperparameter space on [0, 1] d can be used to measure optimization performance. The Koksma-Hlawka inequality bounds numerical integration error by star discrepancy and variation. Low discrepancy sequences like Sobol, uniform random, and DPP are compared in terms of distance to the center and origin in a hypercube. DPP tends to favor points in the corners due to how they are sampled. This behavior is also common in Bayesian optimization schemes. The DPP tends to outperform uniform random and Sobol sequences in terms of distance to the origin in a hypercube. This behavior is also observed in Bayesian optimization schemes."
}