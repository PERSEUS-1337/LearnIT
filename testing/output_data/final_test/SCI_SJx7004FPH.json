{
    "title": "SJx7004FPH",
    "content": "State-of-the-art language models achieve high performance by pre-training on large text datasets and fine-tuning for specific tasks. Using BERT as an example, researchers show that fine-tuned models closely resemble pre-trained ones in parameter space. This allows for computational efficiency by only fine-tuning critical layers and a fraction of parameters. Fine-tuning involves learning a binary multiplicative mask on pre-trained weights, achieving specific tasks, saving memory by storing binary masks, and saving compute with sparse operations. Overparameterized deep neural networks show that increasing dimensionality does not make optimization harder with proper network architecture. The optimization of deep neural networks occurs in a low-dimensional parameter subspace, with training only marginally altering the high-dimensional parameter configuration. Pre-training provides a common initialization for supervised learning tasks, with fine-tuning involving learning a binary mask on pre-trained weights. Pre-training deep neural networks helps with supervised learning tasks by providing a common initialization. Fine-tuning involves adjusting pre-trained weights for specific tasks, but as models get larger, computation becomes more expensive due to increased memory and compute requirements. In this study, the focus is on exploring the relationship between fine-tuned and pre-trained parameters using BERT and the GLUE benchmark tasks as examples. The findings show that fine-tuned and pre-trained parameters are close in parameter space, with good fine-tuned models being similar to the pre-trained one with a small number of differences. The study explores the relationship between fine-tuned and pre-trained parameters using BERT and the GLUE benchmark tasks. It shows that good fine-tuned models are similar to the pre-trained one with a small number of differences. Additionally, by enforcing fine-tuned parameters to be L 0 -close to the pre-trained ones, memory can be saved, and task-specific parameter configurations can be found within a sparse L 0 -vicinity of large pre-trained language models like BERT. The study discusses fine-tuning parameters in relation to pre-trained models like BERT. By enforcing fine-tuned parameters to be L 0 -close to pre-trained ones, memory can be saved, and task-specific configurations can be found within a sparse L 0 -vicinity. In a study related to fine-tuning pre-trained models like BERT, sparsification plays a role in gradient-based learning. The authors applied techniques from multi-task visual object classification to larger language models, exploring the trade-off between L 0 -closeness and sparseness. Iterative pruning was used during fine-tuning to create high-performance sparse models. During fine-tuning in 2017, high-performance sparse models were created by enforcing parameter sparsity using a straight-through estimator. This technique was also used by Mallya et al. (2018) and Zhou et al. (2019). The fine-tuning procedure involves training a task-specific last layer unique to the task t \u2208 T in a pre-trained network F \u03b8 : x \u2192 F (x; \u03b8). In the case of BERT, the network consists of modules such as E for the embedding layer, P for the final pooling layer, and transformer blocks denoted as B. The B transformer block in the BERT BASE model collects learnable parameter matrices. Various functions such as scaled dot-product attention, dropout, layer normalization, and activation functions are utilized. Fine-tuning on the GLUE benchmark tasks is performed, excluding the WNLI set. Evaluation performances are reported for different tasks using F1 scores, Spearman correlations, and accuracy scores. For fine-tuning procedures on the GLUE benchmark tasks, various techniques are employed such as L0-close fine-tuning, sparse fine-tuning using iterative pruning, and Supermask. Parameters are selectively fixed or pruned to achieve desired sparsity levels during optimization. Iterative pruning ensures parameters are L0-small. Supermask training involves reparameterizing the model with a binary mask to search for sparse and L0-close fine-tuned networks. The mask is optimized through a straight-through estimator technique. The study explores fine-tuning hyperparameters for sparsity control in model initialization. Initial sparsity influences final sparsity levels, ranging from 1% to 89%. Fine-tuning procedures for GLUE tasks involve 10^2 to 10^4 parameter update steps. Comparison of fine-tuned and pre-trained parameters is done using L1-distances and angular distances. The study compares fine-tuned and pre-trained parameters for GLUE tasks using L1-distances and angular distances. Results show higher closeness between fine-tuned and pre-trained parameters than between random initializations, indicating minimal parameter updates during fine-tuning. Parameter distance scales with the number of fine-tuning iterations, with minimal changes observed in parameter subspaces for each layer. During fine-tuning, there is variability in parameter matrices across different layers, with deeper blocks being less L1-close but more angular-close. Effective fine-tuning can be achieved by optimizing only a fraction of layers while keeping others fixed at pre-trained values, resulting in models close in parameter space. Experiments excluded key projection layers in self-attention, encoder stacks, and word embedding layer, showing feasibility of this approach. During fine-tuning, excluding certain layers and using supermask training can significantly reduce the number of parameters to fine-tune, leading to a 40% decrease in computational cost. This approach allows for sparsity in parameters, resulting in improved model performance across tasks. The supermask training approach allows for sparsity in parameters, leading to improved model performance across GLUE tasks. Layer-wise sparsity trends show that certain layers are consistently sparser than others, and fine-tuning only a fraction of sensitive layers can still achieve high performance. The supermask training approach enables sparsity in parameters, enhancing model performance across GLUE tasks. Analysis of pre-trained weights zeroed by supermasks reveals higher magnitudes compared to next smallest entries, indicating a unique learning mechanism. Comparison with magnitude-based pruning shows differences in weight magnitudes, with overlap percentages between supermask and magnitude-based pruning masks. The study found good fine-tuned parameters among various supermasks, with up to 40% sparsity without significant performance degradation. Performance drops as the mask becomes sparser, but there are still good supermasks at the dense end. Fine-tuning is achieved with less than 3% of pre-trained weights pruned for several tasks. The study found that supermasks with all-dense initialization can successfully learn tasks without significant degradation from baseline. For complex tasks like MNLI and QQP, pruning 12-13% of pre-trained weights is sufficient, while simpler tasks like MRPC and RTE only require 1-2% pruning. Fine-tuning can be precise, indicating the presence of good solutions within a sparse L0-neighborhood of pre-trained parameters. The study found that supermasks can learn tasks without degradation from baseline by pruning pre-trained weights. There are good solutions within a sparse L0-neighborhood of parameters, with unique masks for each task. Overlaps in supermasks across tasks are not significantly larger than chance. The study shows that optimizing only the most sensitive layers and learning to sparsify parameters are effective techniques for producing efficient fine-tuned networks for specific language understanding tasks. Freezing parameter weights in the pre-trained model and optimizing only the task-specific last layer leads to low performance. The study emphasizes the importance of fine-tuning layers in the pre-trained model, not just the task-specific last layer. Iterative pruning during fine-tuning outperforms supermask training at higher sparsity levels, achieving binary masks up to 50% sparse without significant performance degradation. The study highlights the significance of fine-tuning layers in the pre-trained model, emphasizing iterative pruning during fine-tuning for better performance at higher sparsity levels. Supermask training requires a higher learning rate compared to typical training, with a noticeable performance degradation at smaller learning rates. The study emphasizes the importance of fine-tuning layers in pre-trained models and iterative pruning during fine-tuning for improved performance at higher sparsity levels. Supermask training requires a higher learning rate and shows performance degradation at smaller learning rates. Initial sparsity levels influence the final sparsity levels of supermasks across GLUE tasks, with a correlation observed in the shift of sparsity levels during training. The study highlights the importance of fine-tuning layers in pre-trained models and iterative pruning for better performance at higher sparsity levels. Parameter and angular distance scale with the number of fine-tuning steps in GLUE tasks."
}