{
    "title": "SJgubJrKPr",
    "content": "Conventional deep reinforcement learning often requires significant time and effort to learn effective policies, especially in complex environments. To address this, macro actions, sequences of primitive actions, are incorporated into the action space to speed up exploration and learning. Previous research has focused on developing macro actions by mining frequently used action sequences, but this may only reinforce existing behavior. Our approach proposes constructing macro actions using a genetic algorithm to speed up learning in deep reinforcement learning. Extensive experiments show that these macro actions improve performance and are transferable across different methods and environments. A comprehensive ablation analysis validates our methodology. Traditional deep reinforcement learning (DRL) has shown superhuman performance in various tasks, but struggles with decision-making in temporally-extended frameworks like humans. This leads to the need for vast amounts of data in complex environments with sparse rewards, making exploration challenging. Finding an optimal policy remains difficult in real-world environments due to their complexity and sparse rewards. In temporally-extended frameworks of deep reinforcement learning (DRL), researchers have explored new mechanisms to efficiently deal with complex environments. High-level controllers select temporal-extended policies called \"options\" that interact with the environment for a certain number of timesteps. Developing effective options either requires domain knowledge or is limited to simple environments. In DRL, researchers explore constructing macro actions as an alternative to developing options for complex environments. Macros are open-loop policies composed of primitive actions, chosen by the agent without further decision-making. Some DRL studies aim to create macros from agent experience, offering the advantage of constructing desired macros without supervision. Some DRL studies focus on constructing macros without supervision, but this can lead to biased macros. Action repeat is a method where primitive actions are repeated in a macro before the agent makes another decision. Allowing agents to perform macros with diversified actions can improve performance. The authors discuss the need for constructing macros in research, highlighting the challenges of human supervision in deriving macros for training efficiency. They aim to develop a methodology for constructing macro actions without human intervention, emphasizing the vast space of possible macro actions. In this study, the authors introduce the concept of the macro action space, which consists of good and bad macros that impact an agent's performance in reaching a target state efficiently. They propose an evaluation method to determine the effectiveness of macros in a specific task environment by augmenting the action space and analyzing the agent's performance results. The authors introduce the concept of the macro action space, proposing a systematic method for macro construction using genetic algorithms. This approach aims to eliminate dependency on past policies and human supervision, producing diversified macros through mutation. The method involves three phases: macro construction by GA, action space augmentation, and evaluation of the augmented action space. The authors propose a systematic method for constructing macro actions using genetic algorithms, aiming to diversify macros without relying on past policies or human supervision. They validate their approach on Atari 2600 and ViZDoom, showing its complementarity to existing DRL methods and favorable performance against baselines. The choice of macro significantly impacts agent performance, with some macros showing transferability across environments or DRL methods. A comprehensive ablation analysis justifies various aspects of the approach, defining it as a framework. The authors propose a framework for constructing macro actions using genetic algorithms, defining macro action space, introducing an augmentation method, and evaluating macro effectiveness. They investigate macro action transferability and provide implementation details in sections 2-5 of the paper. The paper introduces the concept of macro actions, defined as a sequence of primitive actions, and discusses deep reinforcement learning where an agent interacts with an environment under a policy. The goal is to train the agent to learn an optimal policy to maximize expected return. The paper introduces macro actions in deep reinforcement learning, emphasizing the use of provided macros over primitive actions. The framework consists of DRL methods, macro action construction methods, action space augmentation methods, and evaluation methods. Proximal Policy Optimization (PPO) and Advantage Actor Critic (A2C) are selected as DRL methods. Pseudocodes for macro construction, action space augmentation, and evaluation are presented. The paper introduces macro actions in deep reinforcement learning, focusing on the use of provided macros over primitive actions. It includes DRL methods like Proximal Policy Optimization (PPO) and Advantage Actor Critic (A2C), along with pseudocodes for macro construction, action space augmentation, and evaluation. The algorithm aims to construct a list of top-performing macros based on the environment, DRL algorithm, and evaluated macros. The implementation details of the genetic algorithm (GA) for constructing macro actions are presented in this section. Algorithm 1 is formulated based on the action space augmentation method, macro evaluation method, append operator, and alteration operator for mutating macros. The four phases of GA are highlighted in the algorithm. The four phases of the genetic algorithm (GA) for constructing macro actions are outlined. The initialization phase sets essential parameters, the fitness phase evaluates actions, the selection phase picks top performers, and the mutation phase updates the population. Experimental results of Algorithm 1 are presented to validate the proposed methodology. In this section, the experimental setups are walked through, comparing the effectiveness of Algorithm 1's constructed macro actions across different generations. The compatibility and transferability of these macros with two DRL methods are investigated, along with their transfer to more challenging environments. A set of ablation analysis is presented, focusing on key results. The environments used in the experiments and baseline descriptions are provided, with all macros constructed by Algorithm 1 unless specified otherwise. In this section, the experimental setups compare the effectiveness of Algorithm 1's constructed macro actions across different generations. The environments used in the experiments include Atari 2600 and ViZDoom, with specific games selected for evaluation. The results are presented with curves generated based on five random seeds and a 95% confidence interval. In ViZDoom, the methodology is evaluated on tasks like \"Dense\", \"Sparse\", \"Very Sparse\", and \"Super Sparse\" to analyze macro compatibility and transferability. Baselines PPO and A2C are used for training agents, with the addition of the intrinsic curiosity module in some tasks. The Super Sparse task features extended rooms and corridors with longer distances between the agent's spawn point and the goal. ViZDoom setups are used to validate the hypothesis that macro actions from an easy environment can be transferred to complex ones. Macro actions are constructed from the Dense task and evaluated in Sparse, Very Sparse, and Super Sparse tasks. The performance of the best macros improves with generations, indicating the effectiveness of the evaluation method. The evaluation method of Algorithm A2 is effective for Algorithm 1. Best macros are tested with A2C and PPO for DRL baselines, leading to improved performance. Augmented action spaces benefit baseline DRL methods significantly, as shown in learning curves and evaluation results for Atari games. Macros enhance DRL methods like PPO and A2C, improving performance and learning speed. Transferability of macros among similar environments is investigated using Algorithm 1 with A2C and ICM on Dense to construct a macro D. Training on Dense allows the agent to adapt to different reward scenarios easier. The study investigates the transferability of constructed macros to harder environments by adapting to different reward sparsity settings. The macro action (2, 2, 1) corresponds to two forward moves followed by one right turn, which is beneficial due to the map layouts favoring forward moves and right turns. The gap between 'Curiosity+m D' and 'Curiosity' widens as the reward signal sparsity increases, with 'Curiosity+m D' converging faster than 'Curiosity' in the Super Sparse task. The agent's actions mainly consist of straight moves, with left turns being rare and only used when deviated from optimal routes. Genetic Algorithm (GA) was chosen as the macro construction method after comparing it with three others, showing superiority in most cases. Performance of GA was compared with randomly constructed macros, with GA showing better results. After comparing Genetic Algorithm (GA) with other methods, GA was chosen for macro construction due to its superiority. The macro with the highest evaluation score is selected as \"Random\". Results show that 'A2C with Random' is not significantly better than 'A2C with A2C' and 'A2C with D'. Comparing 50 randomly constructed macros with those constructed by Algorithm 1, our approach tends to result in higher mean scores. Further analysis is provided in the appendices. In comparing different action repeat macros, 'A2C with Repeat' performs significantly worse than 'A2C with A2C', 'A2C with D', and 'A2C with Random'. Additionally, replacing randomly constructed macros with handcrafted ones did not significantly impact the outcome of Algorithm 1. The study compared different action repeat macros in BeamRider, finding that handcrafted macros like (1, 1, 1) and (1, 1, 1, 1) did not significantly impact the performance of the A2C algorithm. The methodology presented aims to improve the learning efficiency of Deep Reinforcement Learning methods without the need for human intervention. The proposed methodology allows for various combinations of DRL methods, action space augmentation, evaluation methods, and macro action construction. Results show that the macro actions created are complementary to DRL methods and can be transferred across different environments. A comparison with other macro construction methods was conducted to validate the design decisions. This research sets the stage for future studies on macros and their practical applications. The text chunk summarizes the hyperparameters used, experimental results, and computing infrastructure. The authors implemented ICM along with A2C, finding their implementations to be stabler. Detailed hyperparameters for each method are provided in tables. The study utilized customized machines and virtual machines from Google Cloud Platform, with specifications listed in Table A6. Up to 320 virtual CPU cores, 8 graphics cards, and around 1 TB memory were used in experiments. Performance comparison of macros constructed via Algorithm 1 and random construction method was illustrated in Figure A4, showing mean scores achieved by A2C agents. Results were presented for the top 50 performers for Seaquest and Dense tasks."
}