{
    "title": "rJgjYyaio7",
    "content": "In this work, an iterative algorithm is proposed for computing word vectors based on Generalized Low Rank Models, which generalizes Skip-gram and GloVe methods. Experimental results show that multiple iterations of the algorithm improve results over GloVe on word analogy tasks. Word embeddings are low-dimensional vector representations of words or phrases used in various NLP tasks. Word embeddings are vector representations of words or phrases used in NLP tasks. Skip-gram and GloVe are two methods for creating word embeddings by processing a corpus into a co-occurrence matrix and outputting word vectors. Skip-gram specifically uses a count co-occurrence matrix to find word embeddings based on word context within a certain token distance. GloVe processes a corpus into a harmonic co-occurrence matrix X, defining word vectors as rows of \u00db. An objective function is introduced to find a low rank factorization related to the co-occurrences X. In this paper, the objective functions are derived from a model-based perspective, introducing an iterative algorithm. Problem (1) arises from running the algorithm in full-convergence mode for a Multinomial model, while problem (2) is a step of the algorithm for a Tweedie model. The algorithm bridges the gap between Skip-gram and GloVe, offering new methods for word vector discovery. Previous approaches focused on explaining word embedding methods by analyzing the co-occurrence matrix X and objective function J, leading to the development of new methods discussed in Section 4.1. The related work involves using the co-occurrence matrix from Skip-gram. Early approaches for word embeddings used singular value decomposition to find low-dimensional embeddings. BID7 showed that this is equivalent to using an objective function that is invariant to orthogonal transformation. The co-occurrence matrix and loss function for Skip-gram can be derived from problem (1). The objective function in problem (1) is related to a Multinomial distribution. Skip-gram with negative sampling (SGNS) maximizes the true positive rate plus k times an approximation of the true negative rate. BID19 interprets SGNS as using a shifted PMI matrix but does not determine the objective function explicitly. Later studies identified the co-occurrence matrix and loss function for Skip-gram. Later studies by Li et al. [2015] and Landgraf and Bellay [2017] identified the co-occurrence matrix and loss function for Skip-gram. They found a different co-occurrence matrix not dependent on k, while the loss function does depend on k. SGNS was found to be related to a low-rank matrix X. Landgraf and Bellay [2017] explained the probabilistic interpretation of the loss function and used it to recover the shifted PMI matrix. Their approach viewed the co-occurrence matrix entries as random variables. The paper introduces an objective function based on the likelihood of co-occurrence matrix entries as random variables. It reviews iteratively reweighted least squares (IRLS) and generalized low rank models. Generalized linear models (GLMs) are discussed as a flexible generalization of linear regression for estimating unknown coefficients. The standard approach to estimate \u03b2 is maximum likelihood estimation using Fisher scoring, which produces a sequence of estimates. Each iteration of Fisher scoring is equivalent to minimizing a weighted least squares objective. Principal components analysis is a method for finding a low-rank matrix related to X. A maximum likelihood estimator is used to create a low-dimensional embedding of the data, enabling interpretability and noise reduction. To extend this method to non-normal data, a generalized low rank model is introduced, allowing for model-based low-dimensional embeddings of non-normal data from an exponential dispersion family. The generalized low rank model for X is a matrix with rank at most d, related to a low-rank factorization. An iterative algorithm inspired by IRLS is used to find word vectors in this model. The method utilizes an iterative algorithm inspired by IRLS to find word vectors in a generalized low rank model. The three steps involve choosing a co-occurrence matrix, selecting a dispersion family, and running IWLRLS with specified inputs to output word vectors. The method involves choosing a co-occurrence matrix and specifying a distribution for it. The model is explicit, providing interpretation for its output. The choice of distribution determines the link function. The third step involves running IWLRLS, a generalized version of IRLS, which is a \"computational\" choice fixed in advance. The method includes choosing co-occurrence matrices and distributions to recover common methods for finding word vectors. An improved estimator is proposed for Skip-gram. The proposed method involves choosing a harmonic co-occurrence matrix and the Tweedie distribution to improve the estimator in Skip-gram. The method includes running IWLRLS to recover GloVe, with the Tweedie distribution being justified by its connection to Poisson and Inverse Gamma distributions. The Tweedie distribution is chosen to improve the estimator in Skip-gram by modeling co-occurrence of words as Poisson and Inverse Gamma distributions. The weight H and pseudo-response Z are determined using the cumulant generating function provided. The Tweedie distribution with power p = 1.25 is used in IWLRLS to initialize the algorithm, resulting in GloVe without regularization. By extending GloVe for multiple iterations and considering different co-occurrence matrices, the algorithm can capture word pairs even when x ij = 0. The IWLRLS algorithm uses the Tweedie distribution with power p = 1.25 to initialize, resulting in GloVe without regularization. By considering different co-occurrence matrices, it can capture word pairs even when x ij = 0. The algorithm chooses the count co-occurrence matrix and proposes a Gaussian distribution for the entries of X. In Section A.1, it is shown that the cumulant generation function from the normal distribution is \u03c8(\u03b8) = 1/2 \u03b8^2, ensuring convergence in one iteration for the IWLRLS algorithm. The IWLRLS algorithm uses the Tweedie distribution with power p = 1.25 to initialize, resulting in GloVe without regularization. It proposes a Gaussian distribution for the entries of X and converges in one iteration. Another choice could have been a Multinomial distribution for the entries of X, with a proposed link function as the multi-logit. The Poisson trick BID2 can reduce estimation in a Multinomial model to a Poisson model. Maximum likelihood estimators in a Multinomial model can also be used in a Poisson model with independent responses. The choice of Multinomial model implies a Poisson model with a systematic component. Bias terms for both rows and columns may be appropriate due to the symmetry of the co-occurrence matrix. The Poisson estimator with a non-restricted systematic component is introduced to address bias terms in the co-occurrence matrix. A Poisson distribution is proposed as a plausible model, with a canonical link function g(\u00b5) = log \u00b5. BID1 proposes an estimator similar to IWLRLS, with variations in the derivation process. The Negative-Binomial distribution is also considered as an alternative model. The Negative-Binomial distribution is commonly used as an alternative for the Poisson in the presence of over-dispersion. It produces the same weight and pseudo-response as the Poisson. Landgraf and Bellay [2017] showed that a maximum likelihood estimator from this model with canonical link g(\u03c0) = log \u03c0 1\u2212\u03c0 is identical to a SGNS estimator. In Section 4.1, the IWLRLS algorithm is introduced to compute word vectors like those from GloVe or SGNS, followed by quantitative evaluation experiments on English word analogy and word similarity tasks to demonstrate algorithm performance. In Section 6.1, the analogy similarity task is introduced to evaluate word vectors, followed by presenting algorithm results with different distributions in Sections 6.2. Parameter configurations, training procedures, and results of IWLRLS in various scenarios are provided in Sections B.1-B.5, showcasing improvement and robustness. The word analogy task, consisting of semantic and syntactic subsets, is introduced following previous work by Pennington et al. (2014). The analogy similarity task evaluates word vectors by finding the closest vector in the embedding space using cosine similarity. Results of the IWLRLS algorithm for different models are presented, including Tweedie, Multinomial, and Poisson. Various techniques such as weight truncation, regularization terms, and bias terms are applied to improve the models. The effectiveness of multiple iterations of the IWLRLS algorithm is demonstrated with results for the one-step estimator, early-stopped estimator, and full-likelihood estimator in the Tweedie and Multinomial models. The full-likelihood result is the iteration with maximum accuracy on the analogy task, while the early-stop algorithm performs best in total accuracy between the one-step and full-likelihood iterations. For both models, the full-likelihood result is after 3 iterations and the early-stopped result is after 2 iterations. The study compares the accuracy of different estimators in the Poisson, Multinomial, and Tweedie models. The full-likelihood algorithm generally outperforms the one-step estimator, with small differences in accuracy. The early-stopped estimator shows similar accuracy to the full-likelihood algorithm in the Poisson model. In the Multinomial model, the early-stopped algorithm performs slightly better than the one-step estimator. Overall, the study presents a model-based methodology for word vector extraction from a corpus. The methodology presented in the study introduces a model-based approach for extracting word vectors from a corpus. It unifies existing word embedding methods and introduces new extensions to Skip-gram and GloVe. Experimental results show improvements in word analogy similarity tasks. The hope is that this methodology can lead to better word embeddings and enhance performance in various downstream tasks. The distribution of the response in generalized linear models is discussed, focusing on exponential dispersion families. The exponential dispersion family is defined by the density function of a random variable y. The natural parameter \u03b8, dispersion parameter \u03d5, and cumulant generating function \u03c8 are key components. The normalizing constant c(y; \u03d5) ensures the density integrates to one. The mean \u00b5 of y with natural parameter \u03b8 is equivalent to specifying \u03b8. Classical distributions like Poisson, Normal, Binomial, and Gamma fall under this family. The Tweedie distribution, a member of the exponential dispersion family, is defined by the relationship between its mean and variance. This relationship is based on a result from J\u00f8rgensen (1987) stating that distributions within this family are characterized by their mean-variance relationship. The Tweedie distribution has a power parameter p and is defined through its mean and variance relationship. The Tweedie distribution with power parameter p is part of the exponential dispersion family, characterized by the mean-variance relationship. It includes distributions like Normal (p=0), Poisson (p=1), and Gamma (p=2). We focus on p values between 1 and 2, known as compound Poisson-Gamma distributions. The Tweedie distribution, also known as compound Poisson-Gamma distributions, has positive mass at zero to capture zero-inflation in some co-occurrence matrices. It is characterized by the mean-variance relationship and is part of the exponential dispersion family. The distribution is generalized to a multivariate setting where the density function satisfies a specific equation over its support. The multivariate exponential dispersion family includes distributions with natural parameter \u03b8, dispersion parameter \u03d5, and cumulant generating function \u03c8. Independent draws from the same family follow a specific density function. The Multinomial distribution is another example within this family, characterized by parameters s and \u03c0. The multinomial distribution is in the multivariate exponential dispersion family. The linear model assumes a normal distribution for the response variable y with covariates x. Generalized linear models relax the normality and linearity assumptions. Generalized linear models with link function g connect the distribution of the response variable y to covariates x through a systematic component \u03b7 i. The canonical link function is often used for its computational properties, but alternative choices are available. These models are commonly used in various fields for non-normal distributions. The generalized linear model, specifically logistic regression, is a linear model with the logit link function. It can be seen as a neural network with activation function g. Models are trained on the English text8 corpus with a vocabulary size of 11,000. Different model configurations are experimented with, including power parameter, penalty tuning parameter, and co-occurrence processing steps. The co-occurrence matrix is used with a penalty tuning parameter \u03bb = 0.002. Word vectors are estimated as rows of DISPLAYFORM0 with a dimension of d = 150. Models are trained for up to 50 epochs with a fixed learning rate of 0.18. Multiple iterations of the IWLRLS algorithm are evaluated with different distributions, including Tweedie distribution and weight truncation to match the GloVe objective function. The weight function in the model is adjusted for GloVe objective function, with results presented for an early-stopped skip-gram and Poisson estimator. Observations show increasing accuracy on the analogy task with more steps. The Tweedie model performs best in the first two steps, while the Multinomial model excels in steps 3-5. The Poisson model performs best after 9 iterations, similar to early-stopped algorithm at 3 iterations. Early-stopped and one-step versions of the algorithm can perform comparably. The effect of the power parameter p in a Tweedie generalized low rank model is examined. Results show that high values of p perform poorly, while values below 1.5 perform similarly. A value of p = 1.25 is found to be the best choice, accounting for zero-inflation in the data. Future work aims to improve the estimation of p as part of the algorithm. In the algorithm, the power parameter p is set to 1.25 with a Tweedie distribution. Different strategies for handling large entries in the co-occurrence matrix X are explored, including inputting X directly, truncating weights, or using a minimum value for each entry. Weight truncation slightly outperforms no adjustment in the algorithm. Weight truncation slightly outperforms no adjustment in handling large entries in the co-occurrence matrix. Alternative approaches such as using a more robust distribution or link function may provide further improvements, especially with larger corpora like a full Wikipedia corpus. Mimicking weight truncation in GloVe can be achieved by regularizing the large values of the co-occurrence matrix using different strategies. Further exploration is needed to determine the best approach for regularizing word vectors. Regularizing word vectors with penalty DISPLAYFORM0 and \u03bb = .002 reduces noise and preserves dimension. The penalty helps symmetrically distribute singular values of \u00dbV T to matrices \u00db and V, improving embedding quality. Levy et al. [2015] argued that a symmetric distribution of singular values optimally works on semantic tasks. Experimenting with penalty in Equation FORMULA0 improves results by reducing noise. Regularization within the IWLRLS algorithm with Tweedie distribution and p = 1.25 improves results. Multinomial model outperforms Poisson model in Experiment 1, despite Poisson model having an additional bias term. Experimenting with a single bias term in the Tweedie model yields overall improvements. The Tweedie model with a systematic component without the bias term performs slightly better than the model with both bias terms. Further study on the impact of bias terms and other systematic components is planned for future work."
}