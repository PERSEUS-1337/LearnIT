{
    "title": "HJlnC1rKPB",
    "content": "Recent trends in vision research have seen attention mechanisms being considered as a replacement for convolutional layers in CNNs. A study by Ramachandran et al. (2019) demonstrated that attention can achieve state-of-the-art performance in vision tasks. This work shows that attention layers can perform convolution and are as expressive as convolutional layers. Numerical experiments confirm that self-attention layers attend to pixel-grid patterns similarly to CNN layers. Recent advances in Natural Language Processing (NLP) are largely attributed to the rise of transformer-based architectures like GPT-2, BERT, and Transformer-XL. These models can learn the underlying structure of text and generalize across tasks by attending to every word in their input sequence simultaneously, thanks to the attention mechanism. Self-attention, a key component in transformer-based architectures, captures the similarity between words in a sequence through attention scores. Researchers have explored incorporating self-attention in vision tasks, initially by adding it to CNNs through channel-based attention or non-local relationships. Recent advancements involve replacing convolutional layers with self-attention layers in CNNs, leading to improved performance in image classification and object detection. Studies have shown that combining attention and convolutional features can achieve state-of-the-art results, even under the same computation and model size constraints. Self-attention layers in transformer-based architectures have shown competitive image classification accuracy under the same constraints as convolutional layers. The question of how self-attention layers process images compared to convolutional layers remains open, with the role of positional encodings emphasized. The self-attention layer in transformer-based architectures processes input tokens by mapping query tokens to output dimensions using attention scores and probabilities. This model is equivariant to reordering of input tokens, giving the same output regardless of their order. To address the issue of token order independence in transformer-based architectures, positional encoding is introduced for each token in the sequence. This allows the model to focus on different parts of the input by using multiple heads in self-attention, each with different query, key, and value matrices. The output of these heads is concatenated and projected to a specified dimension using new parameters. Convolutional layers are commonly used in neural networks for image processing. Self-attention can be adapted from 1D sequences to images by associating query and key pixels in the input tensor. The formulas are adjusted to handle 2D tensors by slicing them with a 2D index vector. The multi-head self attention layer output at pixel q can be expressed using absolute and relative positional encoding. Absolute encoding assigns a vector to each pixel, while relative encoding considers position differences between query pixels. The main idea of the introduced method is to consider the position difference between the query pixel and the key pixel in the multi-head self-attention layer. This allows the attention scores to depend only on the shift \u03b4 := k \u2212 q. The key weights are split into two types, pertaining to the input and the relative position of pixels. The section derives conditions for the layer to simulate a convolutional layer, with Theorem 1 stating that a multi-head self-attention layer can express any convolutional layer. The multi-head self-attention layer can simulate any convolutional layer by selecting parameters that mimic a convolutional layer. The attention scores of each head should focus on different relative shifts within a set of all pixel shifts in a kernel. The quadratic encoding, with learned parameters determining attention center and width, satisfies the conditions for this simulation. The encoding defined above efficiently encodes relative pixel positions with only 3 dimensions. The theorem covers general convolution operators, but questions arise regarding hyper-parameters of 2D convolutional layers, such as padding. The correct way to alleviate boundary effects is to pad the input image with zeros on each side. Strided convolution can be viewed as a convolution followed by pooling. Multi-head self-attention can simulate dilated convolution and is commonly used in text, audio, and time series data. Multi-head self-attention can simulate a 1D convolutional layer with N h heads and a kernel size of K = N h. The proof is based on Lemmas 1 and 2, showing the capacity to convolve input sequences. The construction matches the behavior of 1D self-attention in theory, but empirical testing is needed. The multi-head self-attention operator simplifies the effect of multiple heads by using learned matrices. Each head's value matrix and projection matrix are learned, allowing for a one-to-one mapping to a convolutional layer. The attention probability for each head follows specific conditions, leading to an equivalent expression of a convolutional layer. The rank of the learned matrix determines the expressiveness of the convolutional layer. The multi-head self-attention operator simplifies the effect of multiple heads by using learned matrices to map to a convolutional layer. The rank of the learned matrix determines the expressiveness of the convolutional layer, with advice to concatenate heads of dimension D h = D out for exact re-parametrization. There exists a relative encoding scheme with D p \u2265 3 for attention probabilities independent of the input tensor X. The multi-head self-attention operator simplifies the effect of multiple heads by using learned matrices to map to a convolutional layer. Setting W key = W qry = 0 leaves only the last term of eq. (8). The equation can be satisfied by proving the existence of v and {r \u03b4 } for which eq. (14) holds. The magnitude of \u03b1 is crucial for the exact representation of one pixel. The magnitude of \u03b1 is crucial for the exact representation of one pixel in self-attention. Practical implementations rely on finite precision arithmetic, where a constant \u03b1 can suffice. The aim is to validate theoretical results on self-attention performing convolution and to study the relationship between self-attention and convolution in image classification tasks. In a study of a fully attentional model with multi-head self-attention layers, attention probabilities learned tend to respect certain conditions, supporting a hypothesis. The model is compared to ResNet18 on the CIFAR-10 dataset, using a 2 \u00d7 2 invertible down-sampling on the input. Full attention cannot be applied to larger images due to the quadratic scaling of attention coefficient tensors with input size. The input image size limits full attention application. The fixed size representation is computed using average pooling and fed to a linear classifier. Implementation based on PyTorch Transformers. Test accuracy evolution shown for self-attention models vs. ResNet on CIFAR-10 dataset. ResNet converges faster, reasons unclear. Optimization potential in exploiting Gaussian attention probabilities' locality. Code released on Github with hyper-parameters listed. To exploit the locality of Gaussian attention probabilities and reduce FLOPS, embeddings with content-based attention were found harder to train. The focus is not on matching ResNet performance but on verifying if attention layers behave like convolutional layers with relative position encoding. Nine attention heads are trained at each layer to mimic 3x3 kernels in ResNet. Initial positions of the heads change during training, forming a grid around the query pixel. The study confirms that Self-Attention applied to images learns convolutional filters around the queried pixel. Attention heads in different layers focus on local patterns in the early layers and larger patterns in deeper layers. The attention heads do not overlap and maximize coverage of the input space. The study explores positional encoding in self-attention layers for images, using quadratic encoding. They implement a 2D relative positional encoding scheme and differ from previous implementations by using only a 2x2 invertible downsampling layer. The study investigates positional encoding in self-attention layers for images, utilizing a 2D relative positional encoding scheme with a 2x2 invertible downsampling layer. The attention scores are computed using both positional and content-based attention, revealing different patterns of attention across self-attention heads. The study explores the attention patterns of self-attention heads in a standalone model, showing a mix of position-based and content-based attention. Combining CNN and self-attention features was found to outperform using them separately. The study demonstrates that combining CNN and self-attention features outperforms using them separately. The attention patterns in the model show a mix of position-based and content-based attention, with localized patterns following the query pixel. This behavior is similar to convolving the receptive field of a convolutional kernel over an image, as shown in the attention probabilities at different query pixels. The interactive website allows exploration of different attention components for a better understanding of how MHSA processes images. The study explores how MHSA processes images by analyzing attention patterns in a model with 6 layers and 9 heads. A comparison between CNNs and transformers in Natural Language Processing and Neural Machine Translation shows transformers have a competitive edge. The study compares transformers and CNNs in Natural Language Processing and Neural Machine Translation, showing transformers have a competitive advantage. Recent research by Ramachandran et al. (2019) demonstrated transformers achieving similar accuracy as ResNets on images. The study also highlights the ability of transformers to capture long-term dependencies and shows that the class of functions expressed by a layer of self-attention includes all convolutional filters. The author presents a unified framework bridging attention and convolution using tensor outerproduct. They distinguish index-based convolution from content-based convolution and show that relative positional encoding allows content-based convolution to express any index-based convolution. Experimental results demonstrate this behavior is learned in practice. Self-attention layers applied to images can express any convolutional layer, and fully-attentional models combine local behavior with global attention. Fully-attentional models generalize CNNs by learning kernel patterns simultaneously with filters, similar to deformable convolutions. Future work includes applying insights from CNNs to transformers across various data modalities. Funding acknowledgments are also mentioned. Attention probabilities for a model with 6 layers and 9 heads are presented using learned relative positional encoding and content-content based attention. Figures 7 to 10 show average attention for different query pixels in single images. The query pixel is on the horse head and the building in the background. The Multi-Head Self-Attention operator is reworked to make the effect of multiple heads transparent. Matrices W (h) val and W out are learned, and each pair of matrices is replaced by a learned matrix W (h) for each head. The output of a convolution at pixel q is rewritten. The attention mechanism is extended to non-isotropic Gaussian distributions over pixel positions, with each head parametrized by a center of attention and a covariance matrix. The attention scores are obtained through a dot product between the head target vector and the attention coefficient. The attention mechanism is extended to non-isotropic Gaussian distributions over pixel positions, with each head parametrized by a center of attention and a covariance matrix. The model was trained using a generalized quadratic relative position encoding to explore attending to non-isotropic groups of pixels, resulting in unseen patterns in CNNs. Despite learning non-isotropic attention probability patterns, there was no performance improvement observed. The study explored non-isotropic Gaussian distributions for attention in CNNs but found no performance improvement. Some attention heads showed unusual patterns, prompting the question of their usefulness. Heads with low eigen-values or condition numbers were pruned to determine their impact on the model. In the study, attention heads with low eigen-values or high condition numbers were pruned, leading to a reduction in parameter size and FLOPS without sacrificing performance. The model recovered its accuracy after continued training with a smaller learning rate. Additionally, increasing the number of heads from 9 to 16 revealed two main types of attention patterns. The self-attention layer in the model uses localized heads in the first few layers, similar to convolutional layers. As the layers progress, less-localized attention heads become more common."
}