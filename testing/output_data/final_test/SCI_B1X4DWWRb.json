{
    "title": "B1X4DWWRb",
    "content": "Predictive models that generalize well under distributional shift are crucial for machine learning applications, such as estimating treatment effects from observational data and unsupervised domain adaptation. Overcoming distributional shift often involves heuristic methods or assumptions that may not hold in practice. In this work, a bound on generalization error under design shift is devised using integral probability metrics and sample re-weighting. The idea is combined with representation learning to improve existing results. An algorithmic framework inspired by the bound is proposed for causal effect estimation in artificial intelligence. The goal is for agents to learn how to act by accurately predicting and optimizing outcomes, involving estimating counterfactuals in situations where experimentation is impractical. Adjusting for biased observational data distribution is necessary in many applications like patient treatment in hospitals. In this work, domain adaptation and treatment effect estimation are posed as prediction across shifting designs, adjusting for biased observational data to predict counterfactual outcomes under different treatments. The focus is on making causal statements about the policy while separating it from the domain. The text discusses predicting outcomes under different treatments across shifting designs, addressing distributional shift by learning shift-invariant representations or performing sample re-weighting. The focus is on making causal statements about the policy while separating it from the domain. Representation learning through deep neural networks aims to create distributionally similar activations across designs, reducing error from distributional shift. Re-weighting methods, like importance sampling, assign higher weight to representative samples from the source design to correct for distributional shift in causal inference, domain adaptation, and reinforcement learning. Re-weighting methods aim to correct for distributional shift in causal inference by assigning higher weight to representative samples from the source design. Optimal weights are often unknown, but can be learned through various methods such as estimating feature or treatment densities, minimizing distributional distance metrics, or using matching techniques to find similar units in the target design. Our key algorithmic contribution combines shift-invariant representation learning and re-weighting methods to minimize re-weighted empirical risk and distributional shift between designs. This allows for principled control of estimator variance through regularization of the re-weighting function. The text discusses the combination of shift-invariant representation learning and re-weighting methods to minimize empirical risk under distributional shift between designs. This approach aims to control estimator variance through regularization of the re-weighting function, providing a framework for prediction under design shift without requiring access to importance sampling weights or a well-specified model. The proposed neural network architecture learns a representation of the input and a weighting function to improve balance across changing settings. It is applied to predict causal effects from observational data, achieving state-of-the-art results on a benchmark. The goal is to accurately predict outcomes of interventions in different contexts drawn from a target design. The outcome of interventions has a stationary distribution given the context, similar to the covariate shift assumption often used in domain adaptation. The target design involves a policy mapping observations to interventions and a domain of contexts. Data is available from the source domain with labeled samples. The source design includes a policy for historical treatment administration. The focus is on observational settings where treatments are administered non-randomly based on X, leading to covariate shifts between treated and control populations. The goal is to determine the causal effect of an intervention T on Y, conditioned on X. Examples include predicting the return of an advertising policy based on historical results from a different policy applied to a different customer population. In observational settings, treatments are administered non-randomly based on X, causing covariate shifts between treated and control populations. The focus is on determining the causal effect of intervention T on Y, conditioned on X. Assumptions are made to ensure causal identifiability, including consistency, ignorability, and overlap. Ignorability assumes no hidden confounders, with all variables causing both T and Y assumed to be measured. Under ignorability, variables causing both T and Y are assumed to be measured. Potential outcomes equal conditional expectations, allowing us to predict Y by regression. We aim to learn accurate predictors under a design that deviates from the data-generating process. Expected risk is used to measure the ability of predictors to predict outcomes under this design. Under \u03c0, DISPLAYFORM1 is an appropriate loss function like squared loss or log-loss. Estimating (1) under p \u03c0 is challenging without direct observation. Importance sampling can help by re-weighting empirical risk under \u00b5. However, large variance can occur when p \u03c0 is large and p \u00b5 is small. Knowing p \u00b5 (x, t) and w * is rare in practice, but any re-weighting function w with specific properties can yield a valid risk under p w \u00b5. In this work, re-weighting functions are learned from observational data to minimize risk under a different distribution. The classical setting for estimating treatment effects involves binary treatments and fixed domains across designs. The effect of an intervention is measured by the conditional average treatment effect (CATE). Predicting outcomes for unobserved units typically involves prediction of treatment effects. In a clinical setting, knowledge of \u03c4 is necessary to assess which medication should be administered to a certain individual. Estimating CATE from observational data requires overcoming distributional shift with respect to the treat-all and treat-none policies. Regression methods for counterfactual estimation are consistent under certain assumptions, but may suffer from bias due to model misspecification. Importance sampling, used in propensity-score methods, re-weights samples to address distributional shift. However, a drawback is the assumption of known design density. Others have proposed learning sample weights to minimize distributional distance. Some have proposed learning sample weights to minimize distributional distance between samples, but without considering which data aspects are important for outcome prediction. Shalit et al. (2017) suggested learning representations for counterfactual inference, but their generalization bounds are loose. These approaches do not utilize treatment/domain assignment probabilities, which can be estimated from data. Our approach involves designing a predictor based on unlabeled samples from a target distribution and labeled samples from a source distribution. By combining representation learning, distribution matching, and re-weighting, we achieve a tighter bound than previous work. The predictors we consider are compositions of functions that incorporate a representation of the input and a hypothesis. We provide bounds on the risk under distributional shift, showing that the gap between target risk and re-weighted source risk is small when the target and source distributions are close or when the true outcome is a simple function of the input and treatment. The integral probability metric (IPM) distance measures distance between distributions using a normed vector space of functions. Examples include the Wasserstein distance and Maximum Mean Discrepancy. A valid re-weighting of distributions can lead to a tighter bound on risk under distributional shift. A valid re-weighting of distributions can lead to a tighter bound on risk under distributional shift. Importance sampling weights can result in a tight bound in expectation, but the design densities and their ratio are generally unknown. Weighting functions can achieve a tighter bound than uniform weighting, trading off bias and variance in finite samples. Representation learning aims to reduce distributional shift by learning representations that minimize the source-target error gap. This approach has been applied in domain adaptation, algorithmic fairness, and counterfactual prediction. The goal is to focus on common information between source and target distributions, such as recognizing human features in varying conditions. Re-weighting should only be done based on predictive features, as proposed in Section 5. In Section 5, re-weightings based on learned representations are proposed to address distributional shift. The setup involves learning invertible representations \u03a6 : X \u2192 Z and considering re-weighted distributions p w \u03c0,\u03a6. Hypotheses operating on the representation space are denoted by G, and the relationship between expected target risk and re-weighted empirical source risk is explored. Theorem 1 presents a bound for re-weighted distributions based on learned representations to address distributional shift. It involves a twice-differentiable, invertible representation \u03a6, hypothesis h(\u03a6, t), and a reproducing kernel Hilbert space H. The bound guarantees a certain probability with a specific constant C. The proof involves applying generalization bounds to Lemma 1. Theorem 1 utilizes finite-sample generalization bounds with Lemma 1 and the representation \u03a6. It highlights implications such as non-identity feature representations, non-uniform sample weights, and variance control affecting the lower bound. Minimizing uniform weights results in biased hypotheses, even in the asymptotical limit. Importance sampling weights can reduce bias but may lead to high variance. Theorem 1 emphasizes the importance of minimizing the bound with respect to weights for improved tightness. The factor B \u03a6, which measures joint complexity, is sensitive to the scale of \u03a6 and can be normalized to prevent issues. Shalit et al. (2017) used a hyperparameter \u03b1 for B \u03a6, but selecting its value without counterfactual labels is challenging. A heuristic for adaptively choosing \u03b1 based on complexity measures of observed loss is explored in experiments. The term C Theorem 1 applies to unsupervised domain adaptation with a single potential outcome of interest. The text discusses the prediction of conditional average treatment effects using a fixed domain and binary treatment. Proposition 1 provides a bound on the error in predicting treatment effects. The proof involves the relaxed triangle inequality and the law of total probability. By applying Theorem 1 separately, bounds on the risks under constant policies can be obtained. The framework is evaluated in treatment effect estimation in Section 6.2. In Section 6.2, the framework is evaluated for treatment effect estimation by minimizing a bound. A joint learning approach is proposed to minimize risk under the target design, improving on previous work by alleviating bias and increasing flexibility in balancing methods. The training objective involves hyperparameters and a regularizer for the hypothesis. Theorem 2 states that in a reproducing kernel Hilbert space with weak overlap, the minimizers of the optimization problem converge to the representation and hypothesis that minimize the counterfactual risk. However, implementing this optimization is challenging in practice due to difficulties in adjusting weights and ensuring invertibility of the representation. In practice, the implementation deviates from theory by fitting re-weighting based on imbalance and variance, without enforcing invertibility. The objective is split into two, using only the IPM term and regularizer to learn the re-weighting function. This allows for early stopping and hyperparameter selection based on predictive information, unlike existing methods like BID8 and BID13. An example architecture for treatment effect estimation is shown in FIG2. In treatment effect estimation, the balance parameter \u03b1 is set adaptively to improve prediction accuracy. A synthetic domain adaptation experiment demonstrates the benefits of using a learned reweighting function to minimize weighted risk. In treatment effect estimation, the balance parameter \u03b1 is set adaptively to improve prediction accuracy. A synthetic domain adaptation experiment highlights the benefit of using a learned reweighting function to minimize weighted risk over using importance sampling weights for small sample sizes. The experiment involves fitting linear models to logistic outcomes and comparing different methods for minimizing weighted source risk. Our proposed method uses MMD with an RBF-kernel of \u03c3 = 1.0 as IPM, with \u03b1 = 10. Comparing to exact importance sampling weights (IS) and clipped IS weights (ISC), we observe in FIG4 that our method performs well at small sample sizes. Clipped weights alleviate issues with exact IS weights but do not preserve relevance ordering like our method does. True domain densities are only known to IS methods. Our framework is evaluated in the CATE estimation setting to predict the expected difference between potential outcomes. Our task is to predict the expected difference between potential outcomes conditioned on pre-treatment variables. We compare our results to various methods including OLS, Random Forests, Causal Forests, BID5, and CFRW. Our implementation, RCFR, uses neural networks for parameterizing representations and weighting functions. The RBF-kernel maximum mean discrepancy is used as the IPM. The RBF-kernel maximum mean discrepancy is used as the IPM in our implementation, RCFR. We compare results using uniform weights or learned weights with different balance parameter settings. Hyperparameters are chosen based on empirical loss on a held-out sample. Re-weighting has varying effects based on the imbalance penalty, with smaller weights improving error for moderate penalties and larger weights helping for large penalties. Our proposed method achieves state-of-the-art results by adaptively choosing \u03b1 and using nonuniform sample weights. The behavior of our model varies with hyperparameters on the IHDP dataset, showing marginal gains for moderate \u03b1 with the IPM penalty and large gains for large \u03b1 with non-uniform re-weighting. The proposed method achieves state-of-the-art results by adaptively choosing \u03b1 and using nonuniform sample weights. It combines representation learning and sample re-weighting to predict outcomes of interventions under shifts in design, balancing source and target designs. Existing reweighting methods are sensitive to metric choice, while this method learns weights to improve accuracy. In this work, the authors propose a framework that combines representation learning and sample re-weighting for causal effect estimation. They emphasize the importance of measuring and adjusting for distributional shift in a relevant representation space. Challenges include optimizing the full objective and relaxing the invertibility constraint on representations. Variable selection methods are not covered, as they may induce a non-invertible representation. The authors propose a framework combining representation learning and sample re-weighting for causal effect estimation. They highlight the need to address distributional shift in a relevant representation space. Challenges include optimizing the full objective and relaxing the invertibility constraint on representations. Variable selection methods are not discussed as they may lead to a non-invertible representation. The importance of using predictive attributes when measuring imbalance is emphasized for future work. The expected risk under distribution p \u03c0 is bounded by the expected risk under p \u00b5 and a discrepancy measure. A valid re-weighting of p \u00b5 exists for hypotheses f with loss f such that f / f H \u2208 H. The first inequality is tight for importance sampling weights, while the second is not tight for general f unless p \u03c0 = p \u00b5. Importance sampling weights are defined as w IS (x, t) = \u03c0(t;x) \u00b5(t;x) for any h \u2208 H. The loss under distribution q can be bounded based on the weighted loss under p. Theorem A1 (Generalization error of re-weighted loss BID6): For a loss function h of any hypothesis h \u2208 H, with pseudo-dimension d and weighting function w(x) such that E p [w] = 1, we have a simpler form. Theorem A2 (Estimation of IPMs from empirical samples): Let M be a measurable space with a measurable kernel k and reproducing kernel Hilbert space H induced by k. The text discusses learning invertible representations in a kernel Hilbert space induced by a kernel function. It introduces the concept of re-weighted distributions and empirical forms, as well as sets of hypotheses operating on these representations. Theorem 1 is restated and proven in this context. The text discusses learning invertible representations in a kernel Hilbert space induced by a kernel function. Theorem 1 is restated and proven in this context, involving labeled and unlabeled samples, squared loss, hypothesis space, re-weighting, and capacity measures with logarithmic dependence on sample size. The text discusses learning invertible representations in a kernel Hilbert space induced by a kernel function. The expected variance in potential outcomes is analyzed, along with a bound for functions with Lipschitz constant at most 1. The proof involves minimizing a certain function and showing the relationship between different terms. The text addresses re-weighting regularization in CATE estimation on IHDP data, showing the impact of different regularization strengths on error. The results highlight the importance of balancing uniformity and source prediction error for accurate CATE estimation."
}