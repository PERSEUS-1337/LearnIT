{
    "title": "S1vuO-bCW",
    "content": "Deep reinforcement learning algorithms require a lot of experience to be collected by the agent in real-world applications like robotics. A new autonomous method for safe and efficient reinforcement learning is proposed in this work, involving learning a forward and backward policy to reset the environment for subsequent attempts. This approach aims to reduce the need for manual intervention during the learning process. Deep reinforcement learning algorithms have shown success in automating complex behaviors in various settings, but real-world results lag behind simulated accomplishments due to the challenge of scaffolding for learning. A bad policy can lead to unrecoverable states, hindering further learning progress in tasks like autonomous driving and robotic manipulation. Real-world tasks often require manual resets of the environment after each episode in reinforcement learning. This can be costly and time-consuming, especially when the environment breaks. To avoid these manual resets, task designers may incorporate negative rewards to prevent failures. To prevent manual resets caused by environmental failures, task designers use negative rewards to deter dangerous actions. Scaling to complex environments requires writing numerous rules to avoid specific actions. Our method aims to automatically learn safety rules, reducing the need for manual resets and enabling longer RL experiments with multiple agents. We propose the concept of \"leaving no trace\" to teach agents not only how to perform tasks but also how to undo them, ensuring safety by focusing on reversible actions. In this work, two policies are learned to alternate between task completion and environment resetting, reducing the need for manual resets. The value-based reset policy ensures the agent only visits reversible states, preventing irreversible actions. This approach can be applied to both deterministic and stochastic MDPs. The main contribution of the work is a framework for continually learning a reset policy in conjunction with a forward task policy in MDPs. The reset policy automates environment resetting between episodes, enhances safety by reducing unrecoverable states, and balances exploration with safety through uncertainty in value functions. Experiments show a decrease in the need for manual resets. Our method reduces the need for manual resets during learning of robotic skills by incorporating safe exploration techniques. It extends previous work on safe exploration in small MDPs to solve complex, high-dimensional tasks. Additionally, it balances exploration with safety through uncertainty in value functions. Our work focuses on defining safety as guaranteeing that an agent can reset, rather than just guaranteeing some reward. We learn a reset policy to undo actions of a forward policy, preemptively predicting failures. Unlike other approaches, our reset policy is learned simultaneously with the forward policy, enhancing safety during complex tasks. Our approach involves learning a reset policy simultaneously with the forward policy, using uncertainty estimation via bootstrap for safety. Unlike other methods like BID11, we focus on curriculum generation by engaging the reset controller in increasingly distant states. This approach does not require explicit goal setting, allowing the reset policy to abort an episode when necessary. The training dynamics of our reset policy resemble reverse curriculum generation, with the reset policy acting as a learner. Our method involves learning a reset policy alongside the forward policy, using uncertainty estimation for safety. Unlike other approaches, we focus on curriculum generation by involving the reset controller in distant states without explicit goal setting. The training dynamics of our reset policy resemble reverse curriculum generation. Our work aims to avoid manual resets in RL training by learning a reset policy that ensures the distribution over final states is close to the initial state distribution. This allows the agent to continually learn without querying the initial state distribution. Our method involves jointly learning a forward policy and reset policy, using early aborts to avoid manual resets. The forward policy aims to maximize task reward, while the reset policy resets the environment. Both policies have the same state and action spaces but different reward objectives. The reset policy reward approximates the initial state distribution. The design used for experiments was simple, involving negative distance to a start state and reward shaping. Two assumptions are made about the task environment to ensure it can be solved without manual resets. The assumption for solving tasks without manual resets is crucial in robotics, with many tasks meeting this criteria. However, games like Ms. Pacman violate this assumption. Another assumption is that the initial state distribution is unimodal and narrow, ensuring the reset policy's final state is close to the initial distribution. Detecting violations of this assumption is straightforward by comparing the distributions. Our method utilizes off-policy actor-critic as the base RL algorithm, allowing for sharing of experience between forward and reset policies. The reset policy transitions from the final state back to an initial state, reducing the need for costly manual resets. In challenging domains, where quick resets are not possible, the value function of the reset policy will be low. This value can be used to signal early aborts. The value function of the reset policy can signal early aborts in challenging domains where quick resets are not possible. This allows for exploration in a 'safe' subspace of the MDP, restricting visits to safe states. Early aborts, as proven in Appendix A, are a dynamic safety constraint that restricts the forward policy to safe states based on true Q-values. They prevent the agent from entering unsafe states, making learning easier. Experimental analysis is done in Section 6.3, and handling of Q-value estimates is discussed in Appendix B. A hard reset resamples the state from the initial distribution, available to external agents but not the learned agent. Early aborts reduce the need for hard resets but may not eliminate them entirely. Identifying states for reset is challenging, with a set of states defined for rewarding reset policies. An irreversible state is determined if safe states are not visited within a set number of episodes, with increasing this number reducing hard resets. However, remaining in an irreversible state may occur with a higher number of episodes. In an irreversible state, increasing N means staying in that state for more episodes. The setting of this parameter should consider the cost of hard resets. The algorithm involves running a forward policy and reset policy alternately. Q-value accuracy affects task reward and safety. Q-values may not be good estimates of the true value function. To improve reset policy, train ensemble of Q-functions with uncertainty estimates. Use different random initializations for each Q-function. Propose three strategies for early aborts based on Q-values: Optimistic Aborts, Realist Aborts, Pessimistic Aborts. The text discusses three strategies for early aborts based on Q-values: Optimistic Aborts, Realist Aborts, and Pessimistic Aborts. It also mentions the impact of these strategies on exploration and hard resets, with an empirical test in Appendix C. Additionally, a didactic example is presented to illustrate how cautious exploration reduces the number of hard resets in a gridworld scenario. In a gridworld scenario, early aborts are triggered based on Q-values, with most aborts occurring near the initial state. A harder environment is presented where the forward policy is blocked by the reset policy from reaching an absorbing goal state. Increasing Q min can decrease training time without increasing the number of steps needed to solve the task, leading to greater efficiency in real-world experiments. In real-world experiments, increasing Q min can reduce training time even if more steps are needed to learn. Various complex control environments are used to evaluate the approach, with some being reversible while others are not. Evaluation is done separately from training by running the policy for a fixed number of steps to compute average reward per step. Our method is compared to a nonepisodic approach on ball in cup, showing that only our method learns to catch the ball without hard resets. The \"forward-only\" approach fails even on reversible environments, highlighting the effectiveness of our method in lifelong RL settings. Our method outperforms the \"forward-only\" approach on the ball in cup task by automatically resetting the environment after each attempt, allowing for better learning without hard resets. This approach achieves equal or better rewards than the status quo with fewer manual resets. Our approach achieves the same rewards as the status quo with fewer manual resets, showing faster learning in the pusher task and significantly higher rewards in the cliff cheetah task. Increasing the early abort threshold helps avoid hard resets and aids learning by preventing wasted exploration time. Increasing the early abort threshold helps avoid hard resets and aids learning by preventing wasted exploration time. Lowering the early abort threshold increases the number of hard resets, supporting the hypothesis that early aborts prevent them. Adjusting Q min values during learning on pusher and cliff cheetah tasks showed that increasing the threshold reduced the number of hard resets needed for achieving similar rewards. Our algorithm includes a mechanism for requesting a manual reset if the agent reaches an unresettable state. Increasing the number of reset attempts reduces hard resets significantly without affecting the reward on certain tasks. Our approach uses an ensemble of value functions to reduce hard resets during learning. Varying the ensemble size from 1 to 50 showed a decrease in hard resets without affecting the reward. The method induces a curriculum for tasks like peg insertion with sparse rewards. Our algorithm reduces hard resets during learning by starting with the peg in the hole and running the forward policy until an early abort occurs. As the reset policy improves, early aborts occur further from the hole, increasing the task difficulty. Comparing to an \"insert-only\" baseline, our method is the only one that solves the task, as shown in FIG0. Our method reduces the number of manual resets required for learning tasks by automating resets between trials and using early aborts to avoid unrecoverable states. It learns a forward and reset policy simultaneously, balancing exploration and recoverability. Experimental results show that our algorithm not only reduces manual resets but also enables agents to learn policies they couldn't solve otherwise. Our algorithm reduces manual resets needed for learning tasks, avoids unsafe states, and induces a curriculum. It can be applied to various tasks with minimal manual resets. However, it has limitations such as the inability to accurately predict consequences early on and treating all manual resets equally. Differentiating costs for manual resets based on task importance is not explored in this paper. The paper discusses the need to incorporate reset costs into the learning algorithm for real robots, where manual resets are costly. It highlights the challenge of automatically identifying resets in the real world and proves that learning true Q-values for the reset policy can keep the forward policy safe for certain types of MDPs. The paper discusses incorporating reset costs into the learning algorithm for real robots. It proves that learning true Q-values for the reset policy can keep the forward policy safe in certain types of MDPs. The reliability of the abort condition depends on the accuracy of the learned Q-values, which can be partially mitigated with the Q-function ensemble. The paper discusses incorporating reset costs into the learning algorithm for real robots, proving that learning true Q-values for the reset policy can keep the forward policy safe in certain types of MDPs. The assumptions in a stochastic MDP are used to establish a lower bound for the value of any state for the reset policy. The paper proves that a reset policy in a stochastic MDP can guarantee a minimum expected reset reward at every time step. It also shows that Q-values can be maintained or increased with optimal actions. The paper demonstrates that a reset policy in a stochastic MDP ensures a minimum expected reset reward at each time step and that Q-values can be preserved or enhanced with optimal actions. Additionally, it discusses the safety of states in transitions and the expectation of safe states in the next state. The paper discusses the safety of states in transitions and the expectation of safe states in the next state. In deterministic MDPs, Leave No Trace only visits safe states if the initial state is safe. In practice, Leave No Trace may visit unsafe states but less frequently than existing approaches. The proofs show states are safe in expectation, not with high probability. Using an ensemble of Q-functions helps reduce unsafe actions compared to using a single Q-function. Early aborts were introduced assuming access to true Q-values, but in reality, learned Q-values may over/under-estimate Q-values. When learned Q-values over/under-estimate the reset Q-value, the agent may mistakenly perceive unsafe states as safe and vice versa. This can lead to incorrect reset actions and affect the generalization of the Q-function across states. In continuous tasks, the agent may act optimistically based on the largest predicted Q-value. The ensemble method helps the agent act optimistically by choosing the largest predicted Q-value, leading to efficient exploration in gridworld. In continuous control environments, the ensembling method has little impact on performance, suggesting its benefit lies in producing less biased predictions in novel states. The training dynamics of the algorithm are visualized by showing the number of steps taken by the forward policy before an early abort occurs. The reset policy improves over time, leading to fewer early aborts in various tasks. The reset policy improves over time, leading to fewer early aborts in various tasks. Increasing the safety threshold Q min causes early aborts to occur sooner, especially early in training. In the cliff cheetah task, the agent learns rudimentary policies for running forwards and backwards after 200 thousand steps. The sensitivity of the method to Q min is highlighted in cliff walker and pusher environments. If Q min is too small, early aborts will never occur. Automatically tuning the safety threshold based on the real-world cost of hard resets is a promising area for future research. Increasing the safety threshold leads to earlier early aborts, prompting the agent to explore cautiously. Experiments on cliff cheetah, cliff walker, and pusher environments compared different approaches and the effects of varying the early abort threshold. In this section, additional information on continuous control environments is provided. The ball in cup, cliff cheetah, cliff walker, pusher, and peg insertion environments are discussed, with modifications made to some environments for the experiments."
}