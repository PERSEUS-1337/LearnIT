{
    "title": "SJe4SJrFDr",
    "content": "In this paper, the authors propose a method called Adversarial Neural Pruning (ANP) to obtain robust deep neural networks by sparsifying latent features sensitive to adversarial perturbations. They define vulnerability in the latent feature space and use a Bayesian framework to prioritize features based on their impact on both original and adversarial loss. Regularizing the features' vulnerability during training further improves robustness. The method is validated on multiple benchmark datasets, showing its effectiveness as a defense mechanism against adversarial attacks. The authors introduce Adversarial Neural Pruning (ANP) to enhance test accuracy and achieve state-of-the-art robustness in deep neural networks. ANP addresses the challenge of obtaining sparse and robust networks simultaneously, crucial for ensuring adversarial robustness on lightweight devices. Deep neural networks have shown remarkable success in various AI tasks, motivating their use in safety-critical domains like medical imaging and autonomous driving. Deep neural networks have been successful in tasks like medical imaging and autonomous driving but are vulnerable to adversarial attacks. The development of defenses against these attacks has led to a continuous cycle of new attacks and defenses. Various defense mechanisms have been proposed, followed by successful attacks designed to overcome them. The vulnerability of deep neural networks to adversarial attacks is mainly attributed to distortion in the latent feature space. If perturbations at the input level are suppressed in the latent feature space, clean and adversarial samples cannot be distinguished, preventing misclassification. Some latent features may amplify perturbations, leading to distortion, while others remain unaffected. The paper proposes a method to address distortion in latent features of a network caused by adversarial perturbations. By learning a Bayesian pruning mask to suppress highly distorted features, the model aims to enhance its robustness against adversarial attacks. The vulnerability of deep neural networks to such attacks is attributed to distortion in the latent feature space, which can be minimized through regularization or more effective means. The paper introduces the adversarial neural pruning (ANP) method to enhance model robustness against adversarial attacks by selectively dropping highly vulnerable latent features while preserving robust ones. This approach aims to overcome limitations of naive sparsification methods that prune both vulnerable and robust features, ultimately improving network defense mechanisms without significant architectural modifications. The paper introduces the adversarial neural pruning (ANP) method to enhance model robustness against adversarial attacks by selectively dropping highly vulnerable latent features while preserving robust ones. ANP effectively suppresses distortion in the latent feature space, resulting in significantly improved adversarial robustness across multiple datasets with less memory and computational requirements. The contribution of the paper includes formally describing vulnerable and robust latent features and demonstrating the effectiveness of ANP in enhancing DNNs' robustness. The paper introduces the ANP method to enhance DNN robustness by selectively pruning vulnerable features while preserving robust ones. ANP achieves state-of-the-art robustness on CIFAR-10 and CIFAR-100 datasets with reduced memory and computation. It also improves accuracy for clean inputs compared to baseline adversarial training. The ANP method enhances DNN robustness by selectively pruning vulnerable features while preserving robust ones. This helps regularize models and improve accuracy on non-adversarial samples. The method enables a robust and lightweight network, beneficial for resource-limited devices. Our proposed method minimizes distortion in various networks like Lenet-5-Caffe for MNIST and VGG-16 for CIFAR-10 and CIFAR-100 datasets. Visualization shows reduced distortion in adversarial training and further suppression with our method. The vulnerability of features in different layers is analyzed for robustness enhancement in DNNs. The vulnerability of features in different layers is analyzed for robustness enhancement in DNNs. The vulnerability of a feature in a layer can be measured by the distortion caused by an adversary. A feature is considered (\u03b5, \u03b4)-robust if the difference between the clean feature and its adversarial perturbation is less than \u03b4. Conversely, a feature is (\u03b5, \u03b4)-vulnerable if this difference is greater than or equal to \u03b4. The vulnerability of an entire network can be measured by assessing the vulnerability of its individual features. The vulnerability of features in different layers is analyzed for robustness enhancement in DNNs. To measure the vulnerability of an entire network, the sum of the vulnerability of all latent feature vectors is computed. Adversarial training is used as a data augmentation method to train the network on clean and adversarial examples. Our proposed method achieves minimum distortion across all datasets, showing vulnerability in the latent feature space despite adversarial training. Incorporating adversarial search in training process to minimize vulnerability and improve model robustness. Regularizing adversarial training loss with vulnerability suppression loss (VS) to minimize distortion of latent features. Empirical findings show VS loss pushes decision boundary, increases smoothness of model output, and loss surface. Incorporating Adversarial Neural Pruning (ANP) to enhance model robustness by suppressing vulnerability in the latent space through weight and activation pruning. ANP combines adversarial training with Bayesian pruning methods to achieve robustness while reducing distortion in latent features. Incorporating Adversarial Neural Pruning (ANP) to enhance model robustness by suppressing vulnerability in the latent space through weight and activation pruning. ANP combines adversarial training with Bayesian pruning methods to achieve robustness while reducing distortion in latent features. Using Projected Gradient Descent (PGD) to generate adversarial examples with l \u221e -bounded perturbations. Introducing Adversarial neural pruning for Beta Bernoulli dropout based on Algorithm 1, extendable to any sparsification method. Our proposed method extends to any sparsification method. Adversarial Beta Bernoulli Dropout sets dropout rates by generating masks from a beta-Bernoulli prior. The goal is to compute the posterior distribution using an approximate variational distribution. Weight decay regularization is applied, and the Kumaraswamy distribution is used for \u03c0. Stochastic Gradient Variational is used for training iterations. Using the Stochastic Gradient Variational Bayes (SGVB) framework, the final loss is calculated with log-likelihood and regularization terms. Adversarial Beta Bernoulli Dropout (ANP) is introduced for sparsification. MNIST dataset with 60,000 handwritten digit images is used for training. The curr_chunk discusses different datasets used for training, including MNIST, CIFAR-10, and CIFAR-100, with details on the number of images per class and the base networks used for each dataset. The curr_chunk discusses different models and algorithms used for evaluation, including Standard, Bayesian Pruning, Adversarial Training, Adversarial Neural Pruning, Adversarial Training with vulnerability suppression, and Adversarial Neural Pruning with vulnerability suppression. Evaluation includes clean accuracy, vulnerability metric, and accuracy on adversarial examples. All implemented using Tensorflow. The curr_chunk discusses the implementation of models and algorithms using the Tensorflow library, listing hyper-parameters for result reproduction. It includes evaluation results for CIFAR-10, CIFAR-100, and MNIST datasets under various attack parameters, showcasing improved robustness with AT-VS compared to the original AT model. The standard Bayesian pruning method demonstrates the best generalization and marginal improvement in robustness. ANP-VS outperforms all baselines, achieving 68% reduction in vulnerability with 2% improvement in adversarial accuracy under both white box and black box attacks on CIFAR-10 and CIFAR-100 datasets. Evaluation includes total adversarial perturbation of (0.03), perturbation per step of (0.007), 10 total attack steps for training, and 40 total steps with random restarts for evaluation. Results show AT-VS improves robustness of AT by 5% and vulnerability by 52%. ANP-VS achieves state of the art robustness for CIFAR-10 and CIFAR-100 with significant improvements in adversarial accuracy and vulnerability reduction compared to standard adversarial training. Different numbers of PGD steps and epsilon values were considered, showing that robustness may decrease with increased PGD steps. Our adversarial neural pruning method, ANP-VS, outperforms all competing methods in achieving better robustness across various l \u221e -epsilon values and PGD iterations. This indicates that even with increased attacker resources, the impact on our model is minimal. ANP can reduce memory footprint by 88% while maintaining robustness, making it suitable for lightweight and robust network deployment on memory-limited devices. ANP outperforms adversarial training up to 80% sparsity for CIFAR-10 and CIFAR-100, but robustness decreases after that due to model capacity reduction. Comparing ANP with PAT, which combines bayesian pruning and adversarial training, shows PAT slightly improves robustness but loses clean accuracy. This highlights the effectiveness of ANP as a defense mechanism. Further analysis of individual components is provided in the appendix. The appendix contains an analysis of individual components in adversarial neural pruning, including the effectiveness of pretrained models and bayesian components. A series of ablation experiments were conducted, showing improvements in model robustness. Vulnerability analysis was also performed, visualizing the vulnerability of the latent-feature space for various datasets. The vulnerability of latent features in CIFAR-10 is analyzed, showing standard model vulnerability decreasing with adversarial training and further reduced by half with adversarial neural pruning. Proposed method aligns better with human perception, resulting in interpretable gradients. Adversarial neural pruning zeros out distortions for most features, while adversarial training reduces distortion levels. Loss landscape visualization is also discussed. Loss landscape visualization was discussed, showing that the loss surface becomes smoother with sparsity and adversarial training. Adversarial robustness in neural networks has been extensively studied, with various defense mechanisms proposed. Various defense mechanisms against adversarial attacks on neural networks have been proposed, including adversarial training where the network is trained to optimize maximum loss using projected gradient descent. Previous work has focused on robust and vulnerable features at the input level, while our work defines vulnerability at the latent feature level, directly impacting model prediction. Sparsification methods for neural networks are crucial due to increased deployments on resource-limited devices. Various pruning methods have shown high compression rates with minimal accuracy loss. Group sparsity and Bayesian approaches have also been effective in reducing network size while maintaining accuracy. The relationship between sparsity and robustness in model prediction has been explored, providing theoretical motivation and connections to classical techniques. In contrast to previous studies, the authors propose a novel adversarial neural pruning technique to enhance robustness in deep neural networks. They focus on sparsity and robustness, aiming to minimize loss on adversarial examples and improve model efficiency. The study highlights the varying degrees of vulnerability and robustness in latent features of deep networks when exposed to adversarial perturbations. The study introduces an adversarial pruning technique to enhance model robustness by minimizing vulnerability in latent features. By pruning out vulnerable features and minimizing vulnerability, the model's robustness is increased. A Bayesian formulation is proposed for training the pruning mask adversarially, leading to improved model robustness and more interpretable latent features. In this section, the Equation 7 for beta Bernoulli dropout is proven for the Bayesian neural network. The posterior distribution is approximated using an approximate variational distribution of known parametric form. Weight decay regularization is applied to compute a computationally efficient point estimate for the parameters. The Kumaraswamy distribution is used for \u03c0, and z k is sampled using continuous relaxation. The text discusses reparametrization with continuous relaxation using Stochastic Gradient Variational Bayes (SGVB) to minimize the KL divergence between the variational distribution and the true posterior. It extends the idea of Adversarial Neural Pruning to variational information bottleneck. The text extends the idea of Adversarial Neural Pruning to variational information bottleneck, aiming to reduce redundancy between layers in a neural network while maximizing mutual information for accurate predictions of adversarial examples. The layer-wise energy is defined with a coefficient \u03b3 determining the strength of the bottleneck. The text discusses the coefficient determining the bottleneck strength in a neural network for adversarial examples. It introduces the concept of adversarial examples for bounded perturbations and uses variational bounds to optimize the distribution parameters. The text describes experimental settings for training neural networks, including pretraining with standard procedures and sparsification methods. Results show high accuracy for Lenet 5-Caffe on MNIST and VGG-16 on CIFAR-10 and CIFAR-100. The Adam optimizer with adjusted learning rates is used for training. In the study, Ba (2015) used a learning rate for weights 0.1 times smaller than for variational parameters. Alpha/K was set to 10^-4 for Beta-Bernoulli Dropout, pruning neurons/filters with drop probabilities below 10^-3. Results were validated based on compression ratio and model complexity metrics: Memory, xFLOPs, and Sparsity. The study evaluated the performance of Adversarial Neural Pruning (ANP) on a compressed network, measuring values across 3 trials with different seeds. Results showed ANP outperformed base adversarial training for robustness against adversarial attacks, with improved memory and computation efficiency. ANP can be extended to enhance performance with any sparsification method. Additionally, the feature vulnerability histogram for various datasets indicated standard Bayesian pruning eliminated some distortions. Adversarial neural pruning successfully defends against adversarial attacks by reducing distortion levels in features. Adversarial training shows varying degrees of vulnerability in latent features, with some being robust and others vulnerable. Future work will focus on improving suppression methods. Future work will focus on exploring more effective ways to suppress perturbations at the intermediate latent features of deep networks."
}