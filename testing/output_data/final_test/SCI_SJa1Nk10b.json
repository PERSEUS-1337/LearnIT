{
    "title": "SJa1Nk10b",
    "content": "We present an approach for anytime predictions in deep neural networks (DNNs) by using anytime predictors to quickly produce coarse results and refine them within a set computational budget. This method addresses the computational challenges of DNNs by adjusting to varying test-time budgets. We introduce anytime neural networks (ANNs) as a general augmentation to feed-forward networks, emphasizing the importance of high final accuracy. By optimizing final predictions in small anytime models, we can significantly speed up large models without sacrificing accuracy. This speed-up is achieved through simple weighting of anytime losses that oscillate during training. The accuracy in visual recognition tasks has improved with complex convolutional neural networks. Applications requiring latency-sensitive responses are increasing, making it challenging to choose between slow predictors with high accuracy and fast ones. In the context of visual recognition tasks, the challenge of choosing between slow predictors with high accuracy and fast predictors with low accuracy can be resolved by using an anytime predictor in neural networks. This predictor produces a fast initial prediction and refines it as budget allows, ensuring valid results at any test-time budget. Previous methods have focused on appending auxiliary predictions and losses in feed-forward networks for anytime predictions, but have not fully utilized the total weightings for the final prediction, resulting in large anytime models being as accurate as smaller non-anytime models. Our novel approach involves oscillating weightings of losses to speed up small anytime models with near-optimal final predictions, outperforming larger models on various datasets and models. By using exponentially increasing depths in ANNs, we dedicate early predictions to smaller networks while only slightly delaying large networks, resulting in near-optimal late predictions but slightly less accurate early predictions. The initial feature map x0 is set to x, and subsequent feature transformations generate intermediate features. Each feature map xi can produce an auxiliary prediction \u0177i using a prediction layer gi. This augmented network is called an Anytime Neural Network (ANN). The parameters of the full ANN are denoted as \u03b8. The optimization of losses in an ANN is typically done in a weighted sum, where the weight scheme for the losses is crucial. The proposed SIEVE weight scheme aims to address issues with existing weight schemes, leading to improved performance in anytime models. The proposed SIEVE weight scheme aims to improve anytime models by adjusting weights in the final loss to prevent slow errors, enhance neighborhood losses, and avoid zero sum gradients. It assigns uneven weights to early losses and normalizes weights for each predictor in the network. The proposed SIEVE weight scheme adjusts weights in the final loss to prevent slow errors, enhance neighborhood losses, and avoid zero sum gradients. It assigns uneven weights to early losses and normalizes weights for each predictor in the network. Additionally, alternating ANNs (AANNs) speed up anytime models by oscillating weights during training iterations. EANNs leverage high weights in the final layer to improve early predictions of large ANNs by forming a sequence of networks with exponentially growing depths. The EANN sequentially computes ANNs and outputs anytime results if current result is better than previous ones. Small anytime models with SIEVE can outperform large ones with CONST, costing only a fraction of extra budgets. Comparisons on ResNets show networks with CONST have double the depths as those with SIEVE. In experiments with various network architectures, SIEVE leads to the same error rates as CONST but at double the speed. MSDNets are modified DenseNets for anytime predictions, improving final predictions without sacrificing early ones. EANNs outperform parallel OPT by sequentially computing ANNs for anytime results. EANNs reduce early errors of ANNs but reach final error rate later. ANNs with accurate final predictions using SIEVE and EXP-LIN 2 outperform CONST and LINEAR in the long run."
}