{
    "title": "SJJySbbAZ",
    "content": "We propose using Optimistic Mirror Decent (OMD) for training Wasserstein GANs to address limit cycling behavior. OMD has faster regret rates in zero-sum games, which is the context of WGANs. It effectively tackles the limit cycling problem in training WGANs by converging to an equilibrium in bi-linear zero-sum games. Qualitatively, OMD dynamics outperform GD dynamics even with adaptations like gradient penalty or momentum. We apply OMD WGAN training to generate DNA sequences in bioinformatics. We apply OMD WGAN training to generate DNA sequences in bioinformatics, achieving smaller KL divergence compared to GD variants. Additionally, Optimistic Adam, an optimistic variant of Adam, improves performance in WGAN training on CIFAR10 with higher inception scores than Adam. GANs have been successful in fitting generative models in complex spaces like image distributions, framing the task as a zero-sum game between a Generator and a discriminator. The training process of Generative Adversarial Networks (GANs) involves a generator creating samples from noise to approximate the underlying data distribution, while a discriminator distinguishes between real and generated samples. GANs are effective in generating visually appealing samples but are challenging to train due to issues like instability. Training involves a zero-sum game using Stochastic Gradient Descent for both players. The Stochastic Gradient Descent algorithm for both players in Generative Adversarial Networks can lead to oscillatory behavior instead of convergence to an equilibrium. No variant of GD in the class of Follow-the-Regularized-Leader algorithms can converge to an equilibrium in terms of the last-iterate, but are bound to converge to limit cycles around the equilibrium. Training GANs, specifically Wasserstein GANs, using Optimistic Mirror Descent can lead to faster convergence rates in zero-sum games compared to gradient descent. This approach aims to make the last iterate of training close to the equilibrium, leveraging the predictability of the opponent's strategy. Optimistic Mirror Descent (OMD) achieves faster convergence rates than gradient descent in zero-sum games, including normal form games. OMD offers better worst-case guarantees and can converge to an equilibrium in terms of the last iterate, providing stability and improved performance in GAN training. This theoretical result is significant for solving zero-sum games in machine learning applications. In contrast to Gradient Descent (GD), Optimistic Mirror Descent (OMD) avoids limit cycles and converges pointwise, showing better performance in distribution learning tasks. OMD is particularly effective in learning to generate DNA sequences with the same cellular function, essential for understanding the functional landscape of the human genome. In a simulation study, OMD outperforms GD variants in learning DNA sequence distributions. Optimistic Adam algorithm improves GAN training for images, surpassing Adam in inception score on CIFAR10. The goal is to learn a generative model of data point distributions. The text discusses using a deep neural network to approximate a distribution via a Generative Adversarial Network (GAN) training strategy. It mentions the faster convergence rates of a specific algorithm called Optimistic Mirror Descent (OMD) compared to Gradient Descent (GD) in learning games. Replacing GD with OMD in training WGANs offers faster convergence rates due to the favorable properties of OMD in learning games. OMD's update rule is a small adaptation to GD, parameterized by a predictor of the next iteration's gradient. This predictor can be the last iteration's gradient, an average of a window of last gradients, or a discounted average of past gradients. The simple modification in the GD update rule for OMD is different from existing adaptations in GAN training, such as Nesterov's momentum or gradient penalty. The intuition behind OMD can be better understood when GD is viewed through the Follow-the-Regularized-Leader formulation. In the context of training WGANs, replacing Gradient Descent (GD) with Online Mirror Descent (OMD) leads to faster convergence rates. OMD augments GD by adding a predictor of the next iteration's gradient. The predictor can be the last iteration's gradient or other alternatives. In practice, the true distribution is replaced with an empirical distribution. Replacing the true distribution with an empirical distribution in training WGANs can lead to faster convergence rates. Even if computing the gradient of the expected loss is impractical, using unbiased estimators can still result in small loss. By evaluating gradients at a single sample or a small batch, we can simplify the process and improve efficiency. The WGAN example involves a generator learning parameters from a multivariate normal distribution. The goal is for the generator to converge to the true distribution. The WGAN loss function is simplified, and the equilibrium is reached when the generator chooses the true parameter value. In a zero-sum game, the equilibrium is for the generator to choose \u03b8 = v and the discriminator to choose w = 0. Gradient Descent (GD) dynamics lead to a limit cycle, while Optimal Mirror Descent (OMD) dynamics converge to v in terms of the last iterate. OMD stability extends to Stochastic Gradients with a decent batch size. In the appendix, various modifications to the GD dynamics are explored to improve stability, such as adding gradient penalty and Nesterov momentum. While these changes narrow the limit cycle band, they still result in a non-vanishing limit cycle unlike OMD dynamics. It will be formally proven that OMD dynamics converge to equilibrium for a large class of zero-sum games. Optimistic Mirror Descent (OMD) dynamics converge to equilibrium in zero-sum games, showing final-iterate convergence for min-max solutions in bilinear functions. The convergence result extends to games with linear terms in players' strategies. Initialization for OMD iteration involves specifying initial values for x and y. In Optimistic Mirror Descent, we initialize x and y values based on the column space of matrix A. The dynamics converge to min-max solutions in zero-sum games, with \u03bb \u221e \u2264 1. The equilibrium solutions are pairs (x, y) in the null space of A T and A. Theorem 1 (Last Iterate Convergence of OMD): In Optimistic Mirror Descent, the dynamics converge to min-max solutions in zero-sum games with \u03bb \u221e \u2264 1. The OMD dynamics satisfy certain conditions for convergence, with the equilibrium solutions being pairs (x, y) in the null space of A T and A. The OMD dynamics converge to min-max solutions in zero-sum games with certain conditions for convergence. Applying OMD to generate DNA sequences from observed distributions, where sequences with the same function are viewed as samples from a distribution modeled by a PWM. Training GANs from DNA sequences sampled from a PWM distribution serves as a practically motivated problem. In experiments, 40,000 DNA sequences were generated based on a position weight matrix. WGANs with different optimization methods were trained and evaluated using KL divergence between generated samples and true distribution. Discriminator and generator were CNNs. The analysis utilized convolutional neural networks (CNN) for modeling DNA-protein binding. Two model selection methods were explored to compare optimization strategies: selecting the iteration with the lowest discriminator loss on the test set and using the model after the last training epoch. 50 independent models were trained for each learning rate and optimizer to compare strategies based on KL divergences. The study compared optimization strategies for DNA-protein binding using CNNs. Different variants of gradient descent and OMD were explored, along with training schemes for the discriminator and generator. The analysis focused on the distribution of KL divergences across 50 runs. The study compared optimization strategies for DNA-protein binding using CNNs, focusing on the distribution of KL divergences across 50 runs. Optimism in gradient descent is discussed, emphasizing the predictability of gradients from regularized algorithms. Different learning rates were tested for various algorithms, with WGAN trained with Stochastic OMD achieving lower KL divergence than competing SGD variants. Optimistic Adam outperforms other SGD variants in terms of KL divergence. SOMD with 1:1 generator-discriminator training ratio shows better performance than the 1:5 ratio. Optimistic WGAN training is applied to generate images after training on CI-FAR10. Optimistic Adam, a modified version of the Adam algorithm, is used for training WGANs on images. The algorithm is denoted as Optimistic Adam and shows promising results for training WGANs on CIFAR10 images. Optimistic Adam, a modified version of the Adam algorithm, is used for training WGANs on images. It outperforms Adam in terms of inception score, achieving high scores after few training epochs. The algorithm involves updating biased estimates of first and second moments, and correcting the bias to optimize training. In one iteration, Optimistic Adam outperforms the 1:5 generator-discriminator training scheme using vanilla Adam. Training the discriminator only once between generator iterations leads to poor performance. Even training the discriminator 5 times between generator training, as proposed by BID1, results in worse performance than Optimistic Adam with a 1:1 ratio. The same hyperparameters were used for all methods compared, including learning rate, betas, gradient penalty coefficient, and batch size. Various modifications of GD training were tested, such as Adagrad, Momentum, and Nesterov momentum. In the experimental results, various modifications of Gradient Descent training were tested, including Adagrad, Momentum, and Nesterov momentum. The Wasserstein GAN introduces a gradient penalty to ensure the discriminator approximates all 1-Lipschitz functions of the data. This penalty aims to bound the gradient of the discriminator function with respect to x, with weight-clipping being one approach but potentially introducing instability during training. Gulrajani et al. (2017) propose an alternative method by adding a penalty to the loss function of the zero-sum game. The authors propose a regularized WGAN loss by adding a penalty to the loss function of the zero-sum game. This penalty is based on the 2 norm of the gradient of D w (x) with respect to x. The gradient penalty modified WGAN ensures the discriminator approximates all 1-Lipschitz functions of the data. Additionally, different modifications of Gradient Descent training such as Adagrad, Momentum, and Nesterov momentum were tested in the experimental results. Nesterov's momentum is affected by gradient penalty, leading to an update rule for asymmetric training. Training the discriminator more frequently than the generator can reduce cycling, but completely solving the discriminator's problem may not be feasible due to stochastic training and finite samples. Asymmetric training can help reduce cycling in GD dynamics with various modifications like gradient penalty, momentum, and Nesterov momentum. Training the generator less frequently than the discriminator can bring the problem closer to convex minimization, but may not completely eliminate cycles. In another example, the benefits of using OMD over GD are demonstrated. The data distribution is a mean zero multi-variate normal with an unknown co-variance matrix. The discriminator is a set of all quadratic functions, and the generator is a linear function of random input noise. The WGAN game loss associated with these functions is then expanded. The covariance matrix is written as \u03a3 = U U T, simplifying the loss. The equilibrium is for the generator to choose V = \u03c3 and the discriminator to pick W = 0. Update rules for mean GD dynamics and OMD dynamics are provided in matrix form. Regularization is added to the game's loss due to non-convexity and multiple optimal solutions. The regularization of the game's loss involves adding 2 regularization terms to stabilize dynamics. OMD can stabilize dynamics for convergence pointwise, as shown in the weights and covariance matrix of the generator's distribution. OMD demonstrates robustness under stochastic dynamics compared to GD, where gradients are replaced with unbiased estimates. The text discusses the use of Optimistic Mirror Descent (OMD) in achieving last iterate convergence to min-max solutions for bilinear functions. It presents the proof of Theorem 1, showing that OMD converges to min-max solutions for a specific problem. The regularization of the game's loss stabilizes dynamics, with OMD demonstrating robustness under stochastic dynamics by replacing gradients with unbiased estimates. The text introduces the initialization and notation for the iterative process in Optimistic Mirror Descent (OMD) to converge to min-max solutions for bilinear functions. It establishes the necessary conditions for meaningful iterations and presents a claim regarding matrices and vectors. Lemma 2 states the dynamics of Eq. (31) and (32) for any initialization x0 and y0. Theorem 3 considers the same dynamics with specific conditions on the matrix A and its transpose. The proof involves showing the inductive step for the theorem, utilizing lemmas provided in Appendix D.3 to establish the basis for induction at the end of the proof. The proof involves showing the inductive step for the theorem, utilizing lemmas provided in Appendix D.3 to establish the basis for induction at the end of the proof. For all i \u2265 0, t \u2265 0: Given these lemmas, we show our inductive step. For t \u2265 4: where we used Lemmas 4, 5, and 6, induction hypothesis, and small enough \u03b7. This completes the proof of our inductive step. It remains to show the basis of the induction, namely that H(i, 3) holds for all i \u2208 N. From Lemma 4 we have: where we used initialization and Lemma 5. The proof involves utilizing Lemmas 5 and 6 for inequalities, showing the distance from the equilibrium space, and proving Corollary 7. The solutions to the min-max problem exhibit last iterate convergence under certain conditions. OMD exhibits last iterate convergence in the same sense as in Corollary 7. For large enough t, the last iterate of OMD is within O \u221a \u03b7 \u00b7 \u03b3 \u2206 0 0 distance from the equilibrium points of the game. When (30) is infinite or undefined, the OMD dynamics travels to infinity. The proof of Theorem 8 involves considering functions of the form x T Ay + b T x + c T y and making variable substitutions for x t and y t. OMD exhibits last iterate convergence, showing that for large enough t, the last iterate is within a certain distance from the equilibrium points. When (30) is finite, OMD exhibits convergence. The proof involves considering functions of the form x T Ay + b T x + c T y and making variable substitutions for x t and y t. OMD exhibits last iterate convergence, with the last iterate being within a certain distance from equilibrium points for large enough t. The OMD dynamics diverges linearly when (30) is infinite or undefined. The GD update step for f(x, y) = xTy is considered, showing that the running iterate diverges from the equilibrium for any \u03b7 > 0. GD diverges from the equilibrium. Proof of Claim 1: DISPLAYFORM2 is proven analogously. Proof of Lemma 2: First, note the scaled update rule: DISPLAYFORM4. Taking the norm of both sides and using Claim 1, we get: DISPLAYFORM5. Expanding the inner products and using Claim 1 again, we have: DISPLAYFORM6. Multiplying by 2\u03b7 and substituting yields: DISPLAYFORM7. Update step for time t \u2212 2 is well-defined for all t \u2265 1. Define X + and Y + as: DISPLAYFORM8. This allows us to freely use the expansion for all t \u2265 2: x t\u22122 = x. Proof of Lemma 4: To prove this, consider the inequality."
}