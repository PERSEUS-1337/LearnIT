{
    "title": "ryeYpJSKwr",
    "content": "Transferring knowledge across tasks to improve data-efficiency is a key challenge in global optimization algorithms. A novel transfer learning method is proposed within the Bayesian optimization framework, utilizing Gaussian processes for generalization capabilities. By meta-training an acquisition function using reinforcement learning on related tasks, the method learns to extract structural information for improved efficiency. Experiments on sim-to-real transfer tasks and simulated functions demonstrate the algorithm's ability to automatically identify structural properties of objective functions. The algorithm automatically identifies structural properties of objective functions from available source tasks or simulations, performs well with both scarce and abundant data, and adapts its performance accordingly. Bayesian optimization is a data-efficient global black-box optimization method that uses a Gaussian process surrogate model to generalize information from individual data points. It is highly relevant for tasks like tuning hyperparameters in machine learning or optimizing system designs where gradient information is not available. Transfer learning is crucial for optimal data-efficiency in optimization tasks, especially in scenarios where optimizations are repeated in similar settings. Acquisition functions play a key role in guiding the exploration-exploitation trade-off during optimization using Gaussian processes. Transfer learning is essential for efficient optimization tasks, particularly when optimizations are repeated in similar settings. To enhance global black-box optimization in the framework of Bayesian Optimization (BO), it is beneficial to perform transfer learning to maintain the generalization capabilities of the underlying Gaussian Process surrogate model. A novel method is proposed to encode the task structure in a specialized Acquisition Function (AF) through meta-learning a neural AF using reinforcement learning on a set of training tasks. This approach is applicable to standard BO settings without assuming access to the objective. The proposed approach introduces a novel transfer learning method for Bayesian Optimization without requiring access to objective function gradients. It incorporates implicit structural knowledge through neural Acquisition Functions (AFs) to improve data-efficiency on new tasks. The method includes a metalearning procedure for training neural AFs in a black-box optimization setting and demonstrates its efficiency on various tasks. In the field of meta-learning optimization, research focuses on learning local optimization strategies for tasks like training neural networks. This includes learning update rules and controllers for gradient descent. Some approaches involve training neural networks using reinforcement learning or supervised learning. Finn et al. (2017) also contribute to this area of research. In the field of meta-learning optimization, research focuses on learning local optimization strategies for tasks like training neural networks. Approaches include training neural networks using reinforcement learning or supervised learning. Finn et al. (2017) propose methods for initializing machine learning models through meta-learning to solve new tasks with few gradient steps. Chen et al. (2017) address meta-learning global black-box optimization, assuming access to gradient information and using a supervised learning approach with a recurrent neural network. In the field of meta-learning optimization, research focuses on increasing data-efficiency via transfer learning methods such as incorporating data from similar optimizations on source tasks into the current target task. Various approaches include using Gaussian Processes (GPs) with ranking algorithms, standardization, multi-task GPs, GP noise models, and regression on prediction biases. To address the cubic scaling behavior of GPs, alternatives like Bayesian neural networks, task-specific embedding vectors, and adaptive Bayesian linear regression have been proposed. Neural Processes introduced by Garnelo et al. (2018) offer a recent solution in this area. In the field of meta-learning optimization, research focuses on increasing data-efficiency via transfer learning methods. Various approaches include using Gaussian Processes (GPs) with ranking algorithms, multi-task GPs, and regression on prediction biases. Neural Processes introduced by Garnelo et al. (2018) offer a recent solution in this area. Other approaches combine individual GPs for source and target tasks in an ensemble model with adjusted weights based on GP uncertainties and dataset similarities. The proposed approach by Wistuba et al. (2018) introduces a new transfer acquisition function (TAF) that combines source and target Gaussian Processes (GPs) without using a surrogate model. The TAF is a weighted combination of predicted improvements from source GPs and expected improvements from the target GP, with weights adjusted based on GP uncertainty predictions or a ranking-based approach. This method combines knowledge from both tasks in a new AF, with weighting determined in a meta-learning phase and automatically regulated during optimization on the target task. Unlike other methods, it does not store and evaluate multiple source GPs during optimization. The MetaBO method encodes knowledge from source datasets directly in network weights of a learned neural AF, allowing for incorporation of large amounts of source data. It aims to find a global optimum of an unknown bounded real function without gradient information. Performance assessment is based on simple regret, where the best input vector found by an algorithm up to a certain step is considered. The method relies on the framework of Bayesian Optimization and is trained without storing and evaluating multiple source GPs during optimization. The proposed method in MetaBO relies on Bayesian Optimization (BO) framework and reinforcement learning. BO involves specifying a prior belief about the objective function and building a probabilistic surrogate model at each step. Gaussian process is typically used as the surrogate model, with a mean function and variance function to determine the next iterate. An acquisition function is defined to output a score value at each point in the domain. The AF outputs a score value at each point in D to define the next iterate. Well-known general-purpose AFs include probability of improvement (PI), GP-upper confidence bound (GP-UCB), and expected improvement (EI). Reinforcement learning allows an agent to learn goal-oriented behavior via trial-and-error interactions with its environment. The agent explores the environment using a probabilistic policy, receiving rewards and aiming to maximize future rewards. A global black-box optimization method is used to identify and exploit structural properties of objective functions efficiently within the framework of Bayesian optimization. The MetaBO framework utilizes a GP surrogate model for optimization, replacing hand-designed AFs with meta-trained neural AFs. During training, the AF corresponds to an RL policy, sampling actions from a categorical distribution. At test time, the AF output determines the next action in the optimization domain. The MetaBO framework uses neural acquisition functions (AFs) parametrized by a vector \u03b8 to represent the policy \u03c0 \u03b8. These AFs are trained via transfer learning to incorporate additional information for improved sampling strategies in Bayesian optimization. The neural acquisition function in MetaBO is extended with additional features to evaluate sample locations, including the input location, optimization step, and budget. This scalable neural AF can be used in any BO framework and allows for gradient-based optimization strategies. The neural acquisition function in MetaBO is extended with additional features for evaluating sample locations, such as input location, optimization step, and budget. It can be used in any Bayesian optimization framework and supports gradient-based optimization strategies. The resulting neural AF is fully defined after the training phase, eliminating the need to calibrate any AF-related hyperparameters. The class of objective functions F for learning a neural acquisition function \u03b1 \u03b8 includes various physical configurations of experiments or loss functions for training machine learning models on different datasets. Approximations to F can be obtained to capture the most relevant properties for data-efficient optimization on related tasks. During meta-training, the proposed algorithm uses cheap approximations to understand the structure of the objective function F and adapt \u03b8 for a data-efficient optimization strategy. The algorithm utilizes RL (PPO) as the meta-algorithm since gradients of F are often unavailable. The goal is to shape the mapping \u03b1 \u03b8 (x) to identify promising sampling points for optimization. The meta-algorithm PPO uses a stochastic policy \u03c0 \u03b8 to explore the state space, sampling actions a t based on the current state s t. The state s t corresponds to functions \u00b5 t and \u03c3 t, which are evaluated on a set of points \u03be and fed through a neural AF \u03b1 \u03b8 to generate logits for a categorical distribution, shaping the policy architecture. The meta-algorithm PPO utilizes a neural acquisition function \u03b1 \u03b8 to adjust \u03b8 based on promising input points \u03be i. To address the challenge of evaluating points in higher dimensions, an adaptive approach is taken where \u03b1 \u03b8 is evaluated on a static Sobol grid before maximizing local points. This adaptive local set enables the agent to exploit the current state effectively. The adaptive local part of the set enables the agent to exploit what it has learned so far by picking promising points according to the current neural AF, while the static global part maintains exploration. The final characteristics of the learned AF are controlled through the choice of reward function during meta-training, emphasizing fast convergence to the optimum by setting negative simple regret. This choice normalizes the functions and does not penalize explorative evaluations. Knowledge about the true maximum is only needed during training, and cases where it is not known do not limit the applicability of the method. The training loop involves RL meta-training iterations with policy updates and recording episodes to approximate gradients. Each episode involves selecting a function from the training set, fixing an optimization budget, and updating the optimization history based on actions and rewards. The GP is conditioned on the updated history for further iterations. The GP is conditioned on the updated optimization history to obtain the next state. MetaBO-50 outperformed EI and TAF in early stages and over wide ranges of the optimization budget. MetaBO was trained on a wide range of function classes and compared with EI and TAF for transfer learning. MetaBO outperformed EI and TAF in early stages and over wide ranges of the optimization budget by learning robust neural AFs with strong early-time performance and on-line adaption to target objectives. TAF's applicability is restricted to a small number of source tasks, while MetaBO benefits from learning from the whole set of available source data. MetaBO-50 outperformed EI and TAF in early stages and over wide ranges of the optimization budget by utilizing structural knowledge acquired during meta-learning. It showed superior performance on unseen functions and surpassed both flavors of TAF-50 due to its ability to learn advanced sampling strategies. MetaBO outperforms EI and TAF by utilizing structural knowledge acquired during meta-learning. It learns advanced sampling strategies beyond a prior over x \u2208 D and a standard AF. MetaBO does not require the user to choose a set of source tasks but learns from the whole set by randomly picking a new source task at each iteration. The full version of MetaBO performs comparably to MetaBO-50 on global optimization benchmark functions. MetaBO's performance is comparable to MetaBO-50, especially in complex experiments like simulation-to-real tasks. The number of source tasks influences MetaBO's performance. Sample efficiency is crucial for optimizing real-world systems, where MetaBO shows promise in improving data efficiency. MetaBO was evaluated on a 4D control task using a Furuta pendulum. BO was applied to tune feedback gains of a controller, with a sensitive cost function. MetaBO showed efficient sampling strategies in hyperparameter optimization tasks, outperforming benchmark methods. MetaBO outperformed benchmark methods in a 4D control task using a Furuta pendulum. The optimization domain is discrete, allowing tasks to be solved exactly with zero regret. A simple numerical simulation of the pendulum was used to meta-learn the neural AF, with training distribution generated by sampling physical parameters around hardware measurements. MetaBO learned a sophisticated sampling strategy to adapt online to target objective functions, resulting in strong optimization performance. The MetaBO architecture adapts online to target objective functions, yielding strong optimization performance. In contrast, TAF does not adjust source and target task weighting, leading to excessive exploration on complex objective functions. Comparison with MetaBO-50 shows the benefit of incorporating large amounts of source data. The task requires significant source data for efficient solving, as shown in simulation and hardware evaluation. MetaBO outperformed baseline AFs by learning a neural AF that generalizes well. MetaBO learned a neural AF that generalizes well from simulated objectives to hardware tasks, resulting in data-efficient optimization. It consistently yielded stabilizing controllers in less than ten BO iterations, outperforming benchmark AFs. The method's generalization performance to functions outside the training distribution is also explored. The distribution obtained from simulation can be modeled to contain true system parameters with high confidence. Hyperparameter Optimization involved two problems on RBF-based SVMs and AdaBoost using precomputed results from training on multiple datasets. MetaBO, compared to TAF, showed broad applicability with similar source data usage and achieved promising results. MetaBO demonstrated superior data efficiency in learning neural AFs compared to EI and TAF. It performed on par with general-purpose AFs like EI on function classes without specific structure. The approach injects prior knowledge into Bayesian optimization using neural AFs, as shown in real-world optimization tasks. Our method using neural AFs consistently outperforms popular general-purpose AF EI and state-of-the-art solution TAF in real-world optimization tasks. It is applicable to various practical problems with both scarce and abundant data, generalizing well beyond training distribution for robust performance on unseen problems. Future work aims to tackle multi-task multi-fidelity settings, showcasing MetaBO's sample efficiency impact. Experimental results demonstrate that MetaBO's neural AFs learn representations surpassing standard AFs with a prior over x \u2208 D. In experiments with simple one-dimensional toy objective functions, MetaBO learns search strategies involving nongreedy steps to identify specific instances of function classes. The optimal strategy involves evaluating smaller and wider bumps that overlap to encode information about the function's position. MetaBO learns non-greedy optimization strategies by evaluating smaller and wider bumps that encode information about the function's position. This allows for the determination of the global optimum in two steps, going beyond a simple combination of a prior over x with a standard AF. MetaBO's neural AFs learn representations that go beyond a simple prior over x by using a convex combination of a Gaussian Mixture Model (GMM) with n comps components fitted on the best designs from each of the M source tasks. This approach, GMM-UCB, is similar to TAF but uses more principled methods to adaptively determine weights between its prior and posterior parts. The study focused on improving performance on source tasks and expected improvement on the target task. EPS-GREEDY samples from the best designs of source tasks and uses standard EI. Parameters for GMM-UCB and EPS-GREEDY were optimized through grid search. GMM-UCB tested different values for w and number of GMM-components, while EPS-GREEDY evaluated on linearly spaced points in [0.0, 1.0]. MetaBO uses a sophisticated sampling strategy in the Rhino-1 task, optimizing with GP mean, standard deviation, and ground truth function. It learns non-greedy evaluations and maximizes performance by querying the objective function strategically. MetaBO efficiently finds the global optimum by using a sophisticated sampling strategy in the Rhino-2 task. Each episode involves sampling a wide bump and placing a sharp peak, with optimization steps visualized in Figure 6. The GP mean, standard deviation, ground truth function, and neural AF are all utilized for optimization. MetaBO efficiently finds the global optimum using a sophisticated sampling strategy, outperforming baseline methods GMM-UCB and EPS-GREEDY on benchmark functions. MetaBO outperforms GMM-UCB and EPS-GREEDY on benchmark functions with optimal parameter configurations and 50 source tasks. Its primary use case is transfer learning to speed up optimization on similar target functions. Generalization performance on unseen functions provides insights into task nature. In a study of MetaBO's generalization performance, neural AFs were evaluated on 100 test distributions with different translations and scalings. The color on the heatmap indicates the number of optimization steps needed to reach a regret threshold, with white tiles showing thresholds not reached within 30 steps. Regret thresholds were set individually for each function based on the 1%-percentile of function evaluations on a Sobol grid. The study evaluated neural AFs on test distributions with disjoint ranges of physical parameters using MetaBO. The number of steps required to reach a regret threshold was plotted against different intervals of physical parameters. The intended use case of MetaBO is on systems inside a certain range of parameters. MetaBO is designed for systems within a specific training distribution. It can handle a large amount of source data without storing it in GP models, unlike existing methods like TAF. This feature is advantageous for tasks with abundant source data, eliminating the need to manually select representative tasks. MetaBO eliminates the need for manual selection of representative tasks by handling a large amount of source data efficiently. Experimental results show its effectiveness even with the same amount of source data as baseline methods, especially on tasks like the Branin function. The Furuta stabilization task, however, requires a larger amount of source data for reliable performance. MetaBO requires a constant budget for optimization tasks like the Branin function and Furuta pendulum stabilization. The number of source tasks influences the number of steps needed to reach a certain performance level. The dashed red line represents the convergence point of meta-training. Regret thresholds are set for each task to determine performance levels. MetaBO's performance was tested with a regret threshold of 1.0, simulating stabilization on the real system. Experiments were conducted on general function classes without specific structure, serving as a sanity check. Two experiments were performed using different Gaussian process kernels with varying lengthscales. The results showed that MetaBO performs similarly to general-purpose optimization strategies when no special structure is present in the objective functions. MetaBO was tested on general function classes with different Gaussian process kernels. The Matern-5/2 kernel was used for one experiment to avoid model mismatch. The neural AFs produced by MetaBO performed slightly better than benchmark AFs on various dimensional tasks without retraining. The resulting AFs can be applied to functions of different dimensionalities, with simple regret reported for each function in the test set. MetaBO-50 outperformed EI in early optimization stages and also outperformed both versions of TAF over various optimization budgets. MetaBO was trained on the same source tasks as TAF but can learn from the entire set of available tasks. MetaBO learns robust neural AFs with strong early-time performance and on-line adaption to target objectives, yielding stabilizing controllers in less than ten BO iterations. TAF-ME, TAF-R, and EI explore heavily in comparison. MetaBO benefits from learning from the whole set of available source data, while TAF's applicability is limited to a small number of tasks. Detailed experiment settings and hyperparameters are provided for reproducibility. In our experiments, we used MetaBO hyperparameters consistently and Gaussian Process Surrogate Models with GPy implementation. We tuned hyperparameters offline using type-2 maximum likelihood. Our method is compatible with other hyperparameter optimization techniques, but we did not use them for a fair comparison. Baseline AFs included the parameter-free version of EI and TAF as per Wistuba et al. For TAF, we evaluated ranking-based and product-of-experts versions following Wistuba et al. Our method is compatible with maximizing AFs using gradient-based techniques. We used a hierarchical gridding approach for evaluations and training of MetaBO. For experiments with continuous domains, a multistart Sobol grid is used for initial evaluation, followed by local searches around maximal evaluations. The trust-region policy gradient method Proximal Policy Optimization is used to train the neural AF. If the true maximum of the objective function is unknown, the reward is defined with respect to an approximate maximum. The reward signal is defined with respect to an approximate maximum for experiments on general function classes and the simulation-to-real task on the Furuta pendulum. For global optimization benchmark functions and HPO tasks, the exact global optimum is known, and a logarithmic transformation of the simple regret is used as the reward signal. Multi-layer perceptrons with relu-activation functions and four hidden layers are used to represent the neural AFs. To reduce gradient estimate variance for PPO, a separate neural network is used to learn a value function V \u03c0 (s t ) = V \u03c0 (t, T). The same network architecture is used for learning value functions as for neural AFs. Training MetaBO involves ten CPU-workers for data recording and one GPU for policy updates, taking between 30 min to 10 h for neural AF training depending on function complexity. Training a neural AF for a given function class took between 30 min to 10 h on a moderately complex architecture. The task involved stabilizing a Furuta pendulum for 5 s using a linear state-feedback controller. A penalty term was added to the cost function if the system couldn't be stabilized or if the motor voltage exceeded safety limits. The simulation used for training MetaBO was based on the nonlinear dynamics equations of the pendulum and did not include effects like friction and stiction. The study focused on training a neural AF for stabilizing a Furuta pendulum using a linear state-feedback controller. Effects like friction and stiction were not considered in the simulation. The cost function was approximated using a linear quadratic regulator (LQR) controller, and hyperparameter optimization was done through 7-fold cross-validation and grid search."
}