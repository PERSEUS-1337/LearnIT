{
    "title": "Bklfsi0cKm",
    "content": "The output of a CNN with a prior over weights and biases is a Gaussian Process in the limit of infinitely many filters. The equivalent kernel for a CNN can be computed efficiently with few parameters, achieving a record 0.84% classification error on MNIST with a 32-layer ResNet. Bayesian inference is a successful framework for learning in uncertain situations, but it is challenging to apply in CNNs due to the large number of parameters. Other methods like Gaussian Processes are more suitable for Bayesian inference in CNNs, allowing for exact computation of posterior uncertainty. Combining CNNs with exact probabilistic computations in GPs can be achieved through deep convolutional kernels or convolutional GPs. The former uses CNN parameters to parameterize a GP kernel, while the latter defines a GP kernel induced by a CNN. These approaches aim to address the challenges of overconfidence and overfitting in CNNs when inferring posterior uncertainty. The GP kernel induced by a CNN allows for efficient computation by tracking patch variances instead of covariances and computing variances simultaneously at every location. This approach addresses challenges of overconfidence and overfitting in CNNs when inferring posterior uncertainty. The study demonstrates the performance boost from incorporating translation-invariant structure to the GP prior, achieving a record low 0.84% error rate on the MNIST benchmark without gradient computation or data augmentation. The focus is on 2D convolutional NNs, with implications for nD convolutions, dilated convolutions, and upconvolutions. The network's activations are defined recursively from the first linear transformation of inputs. The outputs are the last activations, with pseudo-weights and prior distribution over functions defined. The filters and biases in the CNN are independent Gaussian random variables. Weight variance is adjusted to maintain activation variance. The output of the CNN defines a Gaussian Process indexed by the inputs. The multivariate Central Limit Theorem is applied to each layer to show convergence as the number of channels increases. When the number of channels approaches infinity in a CNN, the feature maps form a vector with strong correlations within each map. Different maps are independent and identically distributed. When the number of channels in a CNN approaches infinity, the feature maps exhibit strong correlations within each map, while different maps are independent and identically distributed. The goal is to show that as the number of channels increases, the properties hold at the next layer, where all feature maps become independent and identically distributed multivariate Gaussian random variables. By applying the multivariate Central Limit Theorem, it is demonstrated that the feature maps at the next layer are multivariate Gaussian random variables. The feature maps in a CNN exhibit strong correlations within each map as the number of channels increases. However, different maps are independent and identically distributed. To show that these properties hold at the next layer, where all feature maps become independent and identically distributed multivariate Gaussian random variables, the output feature maps need to be shown as iid. By demonstrating that the weights are independent with zero-mean, any correlations arising through shared random variables can be eliminated. This allows for the computation of a computationally efficient kernel for the CNN described. The CNN described in the previous section efficiently computes the kernel due to the shared convolutional filter's rich covariance structure in the feature maps. It is found that for every layer, only the \"diagonal covariance\" of the activations is needed, simplifying the computation to consider mean and covariance functions for joint Gaussian distribution parameters. For computing the mean and covariance in a CNN, the network can be represented in index notation. The mean function is easy to compute, with zero mean for certain variables. Surprisingly, it is possible to efficiently compute the covariance function by only considering the \"diagonal\" covariance. This simplifies the computation for joint Gaussian distribution parameters. The \"diagonal\" covariance simplifies computation in a CNN by only considering covariance at specific locations in the outputs and inputs, propagating backwards through the network. The \"diagonal\" covariance simplifies computation in a CNN by only considering covariance at specific locations in the outputs and inputs, propagating backwards through the network. The weights are independent for different channels, allowing for efficient computation of elementwise covariance in closed form for Gaussian activations. The algorithm simplifies computation in a CNN by tracking the diagonal covariances of activations, reducing computational cost to be close to that of a forward pass in a CNN with 1 filter per layer. This efficiency is achieved by considering specific locations in the inputs and outputs, allowing for closed-form computation of elementwise covariance for Gaussian activations. The algorithm simplifies computation in a CNN by tracking diagonal covariances of activations, reducing computational cost. The required variances and covariances can be efficiently computed as a convolution. Adding skip connections between activations of different layers preserves Gaussian Process behavior. Changing the NN recursion leads to a different kernel recursion, equivalent to pre-activation shortcuts. Residual connections performed best in empirical evaluations. The algorithm simplifies computation in a CNN by tracking diagonal covariances of activations, reducing computational cost. Adding skip connections between activations of different layers preserves Gaussian Process behavior. Residual connections performed best in empirical evaluations. Our kernel is evaluated on the MNIST handwritten digit classification task, approximating classification as multi-output regression. Training involves exact conjugate likelihood GP regression with noiseless targets. The predictions in \"ConvNet GP\" and \"Residual CNN GP\" are optimized by random search for kernel hyperparameters. Hyperparameters include \u03c3 2 b , \u03c3 2 w, number of layers, convolution stride, filter sizes, nonlinearity, and frequency of residual skip connections. The models are not retrained on the validation set after hyperparameter selection. MNIST classification results are shown in Table 1, with the number of randomly sampled kernels for hyperparameter search indicated. \"ConvNet GP\" and \"Residual CNN GP\" are random CNN architectures with fixed filter sizes, while \"ResNet GP\" is a modified version. The \"ResNet GP\" is a modified version of the architecture by BID11, using stochastic gradient descent for hyperparameter tuning. The ResNet GP, despite not being optimized, outperformed other architectures with an initial 3x3 convolutional layer and a final dense layer instead of average pooling. Removing pooling allowed for efficiency gains. The 32-layer ResNet GP outperformed other architectures, including the NNGP in BID18, and convolutional GPs. Results did not reach state-of-the-art for methods with parametric neural networks. Probability density plots showed good match around 100 channels in the first layers. The 32-layer ResNet GP outperformed other architectures in BID18, showing a good match around 100 channels in the first layers. The moment propagation equations only require the Gaussianity assumption for propagation through the relu, making it computationally efficient. The ResNet kernel was found to be more expensive to compute than to invert. Van der Wilk et al. adapted GPs for image classification by defining a prior on functions that takes an image and outputs a scalar. Their approach is inspired by convolutional NNs but their convolutional kernel is expensive to evaluate. The current chunk discusses the comparison of empirical and limiting probability densities, the use of quantile-quantile plots for testing Gaussianity, and the performance of CNN kernels in terms of computation cost and accuracy. It also contrasts different approaches to defining kernels in Gaussian Processes, with a focus on deep kernel learning. Deep kernel learning improves Gaussian Processes by preprocessing inputs with a deep neural network before computing the kernel. This approach, with over 10^6 parameters, is comparable to directly applying a neural network to image classification tasks. While both convolutional neural networks (CNNs) and deep kernel learning outperform GP kernels, the latter has significantly fewer parameters. Borovykh (2018) also suggests that CNNs exhibit GP behavior, but in a different context. Deep Bayesian CNNs with infinitely many filters are equivalent to a Gaussian Process with a recursive kernel. The kernel for the GP equivalent to a CNN outperforms previous GP approaches in handwritten digit classification. The focus is on finding good random initializations to avoid vanishing or exploding gradients. The variance is computed for a single training-example, while output covariances for different training/test examples are needed to obtain the GP's kernel. In handwritten digit classification, Gaussian Process CNNs outperform previous GP approaches. The equivalence between CNNs and GPs is of practical relevance for various domains. Key technical issues in the proof arise from different approaches to taking limits. In handwritten digit classification, Gaussian Process CNNs outperform previous GP approaches. The equivalence between CNNs and GPs is of practical relevance for various domains. Key technical issues in the proof arise from different approaches to taking limits. Both our argument and BID18's argument are valid if limits are taken \"inside\" the network. However, BID20 argues for taking limits \"outside\" the network, specifically by taking all layers to infinity simultaneously. This approach presents additional difficulties in reasoning about convergence. The section discusses extending proofs from BID21 to show convergence of convolutional networks to a Gaussian process as n \u2192 \u221e, with mean zero and covariance given by specific equations. The proof involves using the Cram\u00e9r-Wold device and has three main steps. The section extends proofs from BID21 to demonstrate the convergence of convolutional networks to a Gaussian process as n \u2192 \u221e. The proof involves using the Cram\u00e9r-Wold device and has three main steps, including proving convergence of scalar random variables to a Gaussian with specified variance, deriving an adapted CLT for exchangeable random variables, and satisfying moment conditions for the exchangeable CLT. The section extends proofs from BID21 to demonstrate the convergence of convolutional networks to a Gaussian process as n \u2192 \u221e. Convergence in distribution of random vectors is equivalent to convergence on linear projections to real-valued random variables. Projections converge to a Gaussian by defining summands and applying the exchangeable CLT. The exchangeable CLT is applied to show that summands are exchangeable with respect to the index j. By mirroring Eq. 29 in BID21, a recursion is applied to demonstrate convergence in distribution to N 0, \u03c3 2 * under certain conditions. The exchangeable CLT is applied to show that summands are exchangeable with respect to the index j, leading to convergence in distribution to N 0, \u03c3 2 * under certain conditions. Conditions b) and c) are addressed by extending Lemma 20 in BID21 to show convergence in moments for activation functions with unbounded but linearly enveloped nonlinearities. The bars display the proportion of training examples in each bin for test points with that label."
}