{
    "title": "ryeot3EYPB",
    "content": "In this paper, the authors address the domain gap in semi-supervised cross-lingual document classification by combining masked language modeling pre-training with Unsupervised Data Augmentation (UDA). They show that closing the domain gap is crucial for cross-lingual tasks and achieve a new state-of-the-art performance. Recent advances in Natural Language Processing have enabled high-accuracy systems for language tasks, but collecting data for new tasks in multiple languages is inefficient. Cross-Lingual Understanding (XLU) focuses on learning models in one language and applying them to others, often in a zero-shot setting. The labeled data can be used to train a high quality model in the source language, then rely on general domain parallel corpora and monolingual corpora to transfer to the target language. Transfer methods can use machine translation models or language universal representations like cross-lingual word embeddings, contextual word embeddings, or sentence embeddings. Recent work has shown reasonable zero-shot performance for crosslingual document classification and natural language inference using these techniques. Existing work in Cross-Lingual Understanding (XLU) focuses on aligning languages assuming perfect translation systems. However, real-world applications require bridging domain and language gaps. Tasks differ across languages, impacting sentiment expression and topic coverage. In this paper, the focus is on addressing language and domain transfer challenges in Cross-Lingual Understanding (XLU). The approach combines cross-lingual methods with unsupervised domain adaptation and weak supervision techniques for cross-lingual document classification. The use of masked language model pre-training on unlabeled target language corpora is highlighted as a method to improve weakly supervised models like BERT. The study explores methods to enhance weakly supervised models like BERT for cross-lingual tasks. Unsupervised data augmentation (UDA) and domain adaptation techniques are utilized to address challenges in Cross-Lingual Understanding (XLU). The value of additional labeled data in the source language is found to be limited due to train-test discrepancies, prompting the use of self-training for domain adaptation. By combining self-training technique with domain adaptation, error rates in cross-lingual document classification can be reduced by 44% over a strong XLM baseline, setting a new state-of-the-art. Cross-lingual document classification was introduced in 2003 and has since been extended to various NLP tasks like dependency parsing and question answering. Cross-lingual methods gained popularity with the development of cross-lingual word embeddings in 2013. Recently, methods have been proposed to align word embedding spaces of different languages, including seq2seq training of cross-lingual sentence embeddings and pre-training on masked language modeling with multilingual BERT and XLM. XLM is used as the baseline representation in experiments due to its state-of-the-art performance on the XNLI benchmark. Domain adaptation, related to transfer learning, has a history in machine learning and natural language processing. Domain adaptation for NLP is closely linked to transfer learning and semi-supervised learning. While pre-training language models on general domain text has shown good transferability, performance can suffer when the target domain is significantly different. Further pre-training on in-domain text, especially using unsupervised domain data when available, has been found to be beneficial. In the study of weakly supervised learning in language processing, Xie et al. (2019) introduced an unsupervised data augmentation technique for few-shot learning. This technique is extended to facilitate cross-lingual and cross-domain transfer. The problem discussed in this paper is formally defined, focusing on vanilla zero-shot cross-lingual document classification with a labeled dataset in a source language (English) and no data available for the task in the target language. In the context of weakly supervised learning, a cross-lingual classifier is trained on labeled source data and applied to target language data using general purpose resources. Access to a large unlabeled corpus in the target language is assumed, aiming to improve model adaptation and generalization. This semi-supervised XLDC approach, in a zero-shot setting, utilizes domain-specific unlabeled data for better performance. Transfer of knowledge across languages in the vanilla zero-shot setting can be done through translation systems or multilingual embedding systems. In weakly supervised learning, a cross-lingual classifier is trained on labeled source data and applied to target language data using general purpose resources. The XLM model is utilized as a multilingual embedding system for language irrelevant representations. The domain-mismatch problem is addressed by exploring different adaptation approaches for the classifier to the target distribution. To fully adapt the classifier to the target distribution, different approaches leveraging unlabeled data in the target language are explored. Masked Language Model pre-training BERT and its derivations are used for general domain corpora. Fine-tuning the XLM model in the target language can lead to performance issues in the source language. To address this, the translate-train method is used in combination with MLM pre-training. The second approach involves utilizing the Unsupervised Data Augmentation (UDA) algorithm to leverage unlabeled data. UDA generates augmented samples using a predefined function, such as a paraphrase generation model or a noising heuristic. The UDA loss encourages the classifier to make label-consistent predictions for original and augmented sample pairs, helping the model adapt better to the target domain in a cross-lingual setting. In a cross-lingual setting, augmented samples can be generated through translation. Translating back into the target language for augmentation works best. Data augmentation can also be done using unlabeled data from the source domain. The UDA algorithm helps explore prior information on the target domain but still faces train-test discrepancy. The goal is to maximize classifier performance on real samples in the target language. The UDA method involves training on samples from source, target, and augmented domains, with a focus on diversity in augmented samples. However, mismatches between source and target domains, as well as artifacts in augmented samples, can impact performance. To address this, the self-training technique is proposed to improve classifier performance in a cross-lingual setting. In 2013, a classifier based on the UDA algorithm is trained to score unlabeled data from the target domain. A new XLM model is fine-tuned using soft classification loss with pseudo-labeled data. This process results in a new classifier trained solely on the target domain, showing improved generalization ability compared to the teacher model. The method is tested on cross-lingual sentiment and news classification tasks in French, German, and Chinese languages. The text discusses datasets used for cross-lingual sentiment analysis, including French, German, and Chinese datasets from Amazon reviews and Dianping. Training samples are merged for each language, with varying numbers of unlabeled samples. The Chinese datasets include Amazon-cn and Dianping, with training data sourced from Amazon-en and Yelp respectively. The training sample size for Chinese datasets is 2000. For the news classification task, the MLDoc dataset is used with 4 categories and 250 training samples each. The unlabeled data is sourced from the RCV2 dataset with varying sample sizes per language. Languages not supported by the XLM model are ignored in the benchmark datasets. Pre-processing scripts, augmented samples, pretrained models, and experiment settings are required for the above datasets. The pre-training strategies for the sentiment classification task involve fine-tuning an XLM with MLM loss for each target domain with large unlabeled corpus. In contrast, for the MLDoc dataset, unlabeled data from all languages is integrated as the training corpus to preserve language universality in the task. Models compared include Fine-tune (Ft) by fine-tuning the pre-trained model with the source-domain training set. The pre-training strategies for sentiment classification involve fine-tuning XLM models with different methods like UDA and self-training. Results of these methods are compared on both the original XLM model and the XLM ft model. Details on implementation and hyper-parameter tuning are provided in Appendix A.1. The results of cross-lingual sentiment classification and news classification tasks are summarized in tables 1 and 2. The comparison with monolingual baselines shows a performance gap without unlabeled data. The model performance of cross-lingual settings and monolingual baselines, even with state-of-the-art pre-trained cross-lingual representations, shows significant improvements using UDA algorithm and MLM pre-training with unlabeled data. Ft(XLM f t) model using MLM pre-training consistently improves sentiment classification with larger unlabeled data, while UDA method is more helpful with limited unlabeled samples in MLdoc dataset. The combination of UDA and MLM methods in UDA(XLM f t) model outperforms either method alone, with self-training technique observed in sentiment classification task. The self-training technique consistently improves sentiment classification results in both XLM and XLM ft based classifiers, alleviating train-test distribution mismatch and providing better generalization. It also achieves the best results overall in the MLdoc dataset, closing the performance gap with cross-lingual and monolingual baselines by utilizing unlabeled data in the target language. The framework reaches new state-of-the-art results, improving over vanilla XLM baselines by 44% on average. The experiment results show that using only English samples for semisupervised learning lags behind using unlabeled data from the target domain, highlighting the importance of target domain information in the XLDC task. Additionally, evidence is provided for the train-test domain discrepancy in the UDA method, indicating that adding more labeled data in the source language does not necessarily improve target task accuracy. In the cross-lingual setting, model performance peaks at around 10k training samples per category but worsens with larger training sets. Conversely, in the monolingual setting, model performance improves consistently with more labeled data. To address this issue, the self-training technique is proposed to maximize performance in the target domain by abandoning data from the source and augmentation domains. Different augmentation strategies are explored for their impact on final performance. The main experiment uses an augmentation strategy where samples are translated into English and back to the original language. Two additional strategies are explored: using English samples directly and translating unlabeled English data into the target language. These strategies are denoted as t2s and s2t, respectively. The experiment explores different augmentation strategies for sentiment and news classification. Results show that t2t is the best approach, while leveraging unlabeled data from other domains may not consistently improve performance but can be valuable in certain cases. Additional ablations on translation systems and hyper-parameters are included in the appendix. In this paper, the authors address the domain mismatch challenge in cross-lingual document classification by proposing a framework that combines cross-lingual transfer techniques with domain adaptation methods. Their results show that by reducing the domain discrepancy, they can significantly improve the performance of cross-lingual transfer models in document classification tasks. The authors demonstrate significant improvements in cross-lingual document classification by reducing domain discrepancies. The study showcases advancements in this area and suggests potential applications in other cross-lingual tasks. The experiments utilize PyTorch and Pytext packages, with Adam optimizer and grid search for learning rates. In the UDA paper (Xie et al., 2019), annealing strategies are introduced with a batch size of 128 in the Ft and UDA+Self method. The batch size is 16 for labeled data and 80 for unlabeled data in the UDA method. The length of samples is set as 256, with input tokens exceeding this length cut off. The best hyper-parameters are reported, and the augmentation process involves sweeping the temperature for beam search diversity in translation. Different temperatures and sampling spaces are used for various language settings. The results are based on Facebook production translation models, and reproducibility will be ensured by releasing the augmented data. The baseline approach for semi-supervised learning involves training a model with English unlabeled data and augmented samples, then testing it on different target domains. Two input strategies are experimented during testing: using original test samples and translating them into English. Results show that the performance using original and translated samples is similar. Utilizing unlabeled data from the English domain slightly outperforms training only with labeled data, but still falls behind using unlabeled data from the target domain. A translate-train method is applied to maintain multilingual ability when fine-tuning XLM on the target language. An ablation study tests different input strategies, including using original English data and translate-train. The study compares using original English data, translate-train strategy, and a combination of both for training in sentiment and news classification tasks. Results show that original data performs best in most cases, but translate-train method is better in some instances like MLDoc-ru. It is recommended to try both approaches in practice. The temperature of the softmax distribution in translation sampling affects the trade-off between quality and diversity. Different values of temperature influence the final performance of translation systems. The best temperature varies for different language pairs. The appendix includes the best temperature values for the translation systems used in the study. MLM pre-training offers better improvements over UDA method but is more resource intensive, requiring 500 GPU hours to converge. Ft method takes 3.2 GPU hours and UDA method takes 16.8 GPU hours to converge. The method addresses domain mismatch by applying it to the monolingual cross-domain document classification problem. Results show a clear gap between cross-domain and in-domain results, with significant performance boost by leveraging unlabeled data from the target domain. The combined approach achieves the best results. The combined approach achieves the best results, almost completely matching the in-domain baselines."
}