{
    "title": "S1fduCl0b",
    "content": "In this work, a lifelong learning approach to generative modeling is explored, focusing on incorporating newly observed streaming distributions into the model. A student-teacher architecture is used to retain all distributions seen so far without the need for past data or models. A novel cross-model regularizer helps the student model leverage information from the teacher model, reducing catastrophic interference when learning over streaming data. The approach focuses on reducing interference when learning over streaming data by incorporating newly observed distributions into the model. Deep unsupervised generative learning leverages unlabeled data to build models that approximate the true data distribution. It has various applications like image denoising, super-resolution, and clustering. However, there is a need for efficient lifelong learning of deep generative models in a sequential setting, especially for scenarios with streaming data and limited resources. In a sequential setting with streaming data, the goal is to learn a generative model that can summarize all distributions seen so far. Retaining previously learned knowledge is essential to integrate it into future learning and accumulate additional distributions in the current model. In a sequential setting with streaming data, the goal is to learn a generative model that can summarize all distributions seen so far. To accumulate additional distributions in the current model, a student-teacher architecture is used, where the student model learns from the currently observable distribution and synthetic data samples from previous distributions. When a new distribution shift occurs, the student becomes the teacher, and a new student is instantiated. A regularizer in the learning objective function of the student brings its posterior distribution close to that of the teacher, allowing for building upon and extending the teacher's knowledge. The regularizer in the student's learning objective function brings its posterior distribution close to that of the teacher, allowing for faster convergence and extension of knowledge. The lifelong generative models are built over Variational Autoencoders (VAEs), which learn the posterior distribution of a latent variable model using an encoder network. Sampling from the prior in VAEs can be problematic due to limitations in the generative process. To address limitations in the VAE generative process, the latent variable vector is decomposed into a continuous and discrete component. The discrete component summarizes discriminative information while the continuous component accounts for sample variability. This approach aims to improve sampling and balance distributions in the posterior. The sampling strategy in the proposed approach involves independently sampling the discrete and continuous components to preserve distributional boundaries and overcome previous limitations. This method, along with a regularizer, enables learning and remembering individual distributions from the past, allowing for consistent sampling. Previous work has focused on Gaussian mixture models or variational methods, while our approach is more aligned with online or sequential learning of generative models in a streaming setting. Our work introduces a novel approach to lifelong learning with generative models, specifically Variational Autoencoders (VAEs). Unlike previous methods that aim to adjust the model to reflect the current data distribution while forgetting past distributions, our approach focuses on retaining all previously observed distributions within the model. This is the first attempt to bring VAEs into a lifelong setting where distributions are sequentially seen, learnt, and remembered. The use of encoder and decoder neural networks in VAEs helps learn parameters of the posterior and likelihood, but training neural networks in a sequential manner can lead to catastrophic interference. The problem of catastrophic interference arises when training neural networks sequentially, causing bias towards recent samples and forgetting older ones. Various methods have been proposed to address this issue, including distillation, privileged information, transfer learning, and methods like Progressive Neural Networks and Deep Block-Modular Neural Networks. These approaches typically require storing previous models or data, unlike our method. Our method addresses catastrophic interference in neural networks without the need to store previous models or data. Unlike the EWC method, which utilizes the Fisher Information matrix to control parameter changes, our model does not make distributional assumptions about model parameters. Instead of constraining parameters, we restrict the posterior representation of the student model to prevent interference. In our method, we prevent catastrophic interference in neural networks by restricting the student model's posterior representation to be close to the teacher's previous distributions. We aim to approximate the true data distribution with a model that fits the data well, using a latent variable model and prior knowledge integration. The joint distribution can be factorized using conditional distributions P \u03b8 (x|z) or P \u03b8 (z|x). The marginal likelihood P \u03b8 (x) is often intractable, leading to the intractability of the posterior distribution P \u03b8 (z|x). Variational inference approximates the posterior with a tractable distribution Q \u03c6 (z|x) in VAEs, using an encoder to minimize the reverse KL divergence between Q \u03c6 (z|x) and P \u03b8 (z|x). In VAEs, the goal is to maximize the ELBO by using a powerful model Q \u03c6 (z|x) to approximate the true posterior P \u03b8 (z|x). By sharing variational parameters across data points, VAEs avoid per-data optimization loops. The standard setting in generative modeling aims to maximize the marginal likelihood P \u03b8 (x) for data generated from multiple distributions. Latent variable models like VAEs capture complex structures in a mixture distribution by conditioning observed variables on latent variables. In a sequential setting, data samples originate from different components of the generative distribution, and the goal is to build an approximation of the true mixture by observing data from one component at a time. In a sequential setting, data samples come from different components of the generative distribution. A dual model architecture is proposed for lifelong generative learning, with a teacher model preserving memory of previous tasks and passing knowledge to the student model. The student learns distributions over new data while incorporating knowledge from the teacher. The relationship between the teacher and student generative models is shown in Figure 2. The teacher model retains past distributions for knowledge transfer to the student model, which updates parameters based on new data. The student learns from a mix of synthetic samples from the teacher and real samples from the current distribution. The mean \u03c0 of the Bernouli distribution controls the sampling proportion of past distributions to the current one. Once a new distribution is signalled, the student model becomes the new teacher and a new student is initiated with the latest weights. Each new student learns a new approximate posterior Q \u03c6 (z|x) using input data mix and latent variable posterior induced by the student model. The text discusses how the student model's latent variable posterior is brought closer to the teacher model's posterior to prevent catastrophic interference. This is achieved by minimizing the KL divergence between the student's and teacher's posteriors over synthetic data, reusing the teacher's encoder model. This reparameterizes the student model's posterior while maintaining the same learning objective as a standard VAE. The teacher's decoder generates synthetic samples representative of past distributions by sampling latent variables and decoding them. A unimodal prior distribution leads to undersampling and distant posterior means. The teacher's decoder generates synthetic samples representative of past distributions by sampling latent variables and decoding them. To address the issue of distant posterior means caused by a unimodal prior distribution, the latent variable z is decomposed into a discrete component to summarize discriminative information about true generating distributions. The teacher's decoder generates synthetic samples by sampling latent variables. The latent variable z is decomposed into discrete and continuous components to represent discriminative information about true generating distributions. Uniform multivariate categorical and standard normal priors are used for the discrete and continuous components, respectively. When generating synthetic data, the teacher samples from these priors to condition the decoding step. The student models aim to maximize the ELBO from equation FORMULA1, with additional terms for cross-model consistency and mutual information. Training instances are sampled from different distributions, either generated by the teacher model or from the active training set. The learning objective includes maximizing mutual information between discrete representation and data to prevent the encoder from favoring continuous representation. The training set consists of the currently active distribution (\u03c9 = 1), with a focus on the consistency regularizer and the \u03bb hyper-parameter controlling the mutual information regularizer. Experiments were conducted to explore the benefits of the proposed method in lifelong learning, with a specific emphasis on distributional intervals. In this experiment, the focus is on the performance benefit of an augmented objective formulation compared to a simple ELBO objective. Two models with identical architectures are trained, one using the augmented objective and the other using the standard ELBO objective. The model's ability to distinguish distributional boundaries and variations is also demonstrated using Fashion MNIST dataset. The study uses Fashion MNIST dataset to simulate sequential learning, treating each object as a different distribution. Performance is evaluated by computing ELBO over the test set after distributional transitions. Results show an average test ELBO over ten repetitions, indicating the model's ability to distinguish distributional boundaries and variations. The study leveraged InfoGAN for experiments over MNIST, showing the benefits of the new objective formulation in reducing catastrophic interference. The batch VAE outperformed the model due to simultaneous access to all distributions during training. Samples generated from the model with consistency were contrasted with those without consistency. Our model separates 'style' from distributional boundaries, benefiting consistent sampling. The augmented objective aids in preserving previously learned knowledge in lifelong learning settings. Comparing models with and without consistency, we measure their ability to recall information. The models' ability to recall previously learned information is measured by the consistency between student and teacher models over the test dataset. Using the MNIST dataset with rotated digit samples, the model with augmented objective outperforms the one with simple ELBO objective in preserving knowledge for lifelong learning. The model with augmented objective shows lower negative test ELBO and faster learning speed in retaining and transferring knowledge across different datasets. The model demonstrates the ability to transfer knowledge across different datasets by training a student model on SVHN data and then using a teacher model to aid in learning over a mix of teacher-generated synthetic SVHN samples and true MNIST data. The final student model reconstructs samples from both datasets. The architecture can transition between complex distributions while preserving knowledge learned from previous datasets. Data is generated from an interpolation of a 2-dimensional continuous latent space, showing a common continuous structure for two distributions. In this work, a novel method for learning generative models over streaming data is proposed based on lifelong learning principles. The method involves a dual student-teacher architecture to preserve past knowledge and aid in future learning. The proposed method augments the standard VAE's ELBO objective with terms for teacher-student knowledge transfer. The augmented VAE's ELBO objective supports lifelong learning by retaining past knowledge and reducing catastrophic interference. Future work aims to extend the architecture to GAN-like learning for improved generative abilities. Transfer to GANs is not straightforward due to differences in objective functions. The consistency regularizer in VAE can be seen as a transformation of the standard regularizer, preserving the same learning objective. It scales the mean and variance of the student posterior by the teacher's variance and includes an extra 'volume' term. The analytical form of the regularizer is presented for categorical and isotropic gaussian posteriors. The consistency regularizer in VAE transforms the standard regularizer, scaling the student posterior by the teacher's variance and adding a 'volume' term. Variational inference approximates the posterior distribution with a tractable distribution Q \u03a6 (z|x) and optimizes parameters \u03a6 to match it with P \u03a6 (z|x) using stochastic gradient descent. Variational inference minimizes the reverse KL divergence between the variational posterior distribution Q \u03a6 (z|x) and the true posterior P \u03b8 (z|x) using gradient descent. The evidence lower bound (ELBO) is derived as the objective function to optimize, with the reparameterization trick used to remove dependence on the stochastic variable z for backpropagation. This trick allows the gradient operator to be moved into the expectation by introducing a distribution P ( ) not dependent on the data or computational graph. In this section, extra details of the model architecture are provided. Two different architectures were used for experiments: a standard deep neural network with two layers of 512 for mapping to the latent representation and a fully convolutional architecture with strided convolutional layers in the encoder for the transfer from SVHN to MNIST. The final projection layer maps the data to a [C=|z d |, 1, 1] output which is then reparameterized. The decoder utilizes fractional strides for the convolutional-transpose. The decoder in the model architecture uses fractional strides for convolutional-transpose layers, reducing filters by half at each layer. The architecture includes batch norm and ELU activations, optimized with Adam. Weight transfer involves re-initializing momentum vectors and Batch Norm layers. The model's latent variable combines discrete and continuous distributions using Gumbel-Softmax reparameterization. The code is available online under an MIT license. The model architecture allows for the addition of new layers and parameters over time, with no restrictions on the model's architecture. The dimensionality of the discrete latent representation can also grow to accommodate new distributions by padding smaller distributions with zeros. The teacher-student model transfers weights and handles expanding latent representations. New distributions copy weights except those surrounding the projection, which are reinitialized. The choice between reverse or forward KL divergence depends on the true posterior distribution. Reverse KL is preferred for multi-modal distributions, while forward KL is better for image data. Experiment validation involves training models with different posterior regularizers. Our method evaluates sample complexity using standard VAEs and early stopping. The number of required real and synthetic samples is determined by monitoring the negative ELBO on the validation set. As more distributions are assimilated into the model, the number of required real samples decreases rapidly. This procedure is demonstrated in experiment 5.1. In this section, an experiment on MNIST is conducted to evaluate the performance benefit of the consistency regularizer in the learning process. The regularizers help disambiguate distributional boundaries and inter-distributional variations, separating MNIST digits from their variants. Sequential learning is simulated using MNIST, treating each digit as a different distribution and progressing through them sequentially. In an experiment on MNIST, the performance of generative models with consistency and mutual information regularizers is evaluated by computing the ELBO over the test set at every interval. The regularizers help disambiguate distributional boundaries and inter-distributional variations. Samples generated from the final student model are compared with and without the regularizers. The model learns to separate 'style' from distributional boundaries, as seen in the digit '2'. Larger images for the ELBO from experiment 5.2 are provided, along with reconstructions from the rotated MNIST problem. Effects on reconstructions without using the mutual information regularizer are shown in FIG0. The network utilizes a larger continuous representation to model discriminative aspects of the observed distribution."
}