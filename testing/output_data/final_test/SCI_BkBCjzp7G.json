{
    "title": "BkBCjzp7G",
    "content": "The iterative two-pass decomposition flow accelerates existing CNNs by determining proper ranks for low rank approximation. The CP-decomposition helps prevent instability, making the decomposition of deeper networks systematic. Experimental results show VGG16 can be accelerated with a 6.2x speedup while maintaining only a 1.2% accuracy drop. Deep learning, particularly convolutional neural networks (CNNs), has significantly improved recognition accuracy in computer vision tasks. However, the larger network size and computational complexity pose challenges for deployment on edge devices or cloud servers. Research has focused on accelerating CNN models, with approaches like using Fast Fourier Transform to speed up convolution operations. Fast Fourier Transform and Winograd's minimal filtering algorithms are used to accelerate convolution operations in CNN models. Pruning removes unnecessary weights in filters to reduce computation. Quantization decreases precision for hardware accelerators, while new models are designed for more efficient computation. New models tend to achieve greater speedup compared to accelerating pre-trained models. Our approach aims to accelerate existing CNNs by using low-rank approximation to represent tensors. Previous studies have shown speedups in text recognition and image classification tasks. Inspired by CP-ALS decomposition, our work aims to address instability issues and fine-tuning challenges in network layers. The network was successfully decomposed using the Tucker Decomposition method. Nonlinear unit considerations and spatial dimension decomposition techniques led to a 5\u00d7 speedup with a 2% accuracy loss for VGG16. Fine-tuning was then applied to further improve the accuracy to 1%. An iterative fine-tuning approach was proposed in BID0 to overcome CP instability, gradually transforming the dense network into a decomposed form layer by layer with less accuracy drop. The proposed two-pass decomposition method prevents instability in CP decomposition and improves accuracy effectively. An iterative flow aids in decomposing deeper networks systematically, with a rank selection algorithm determining target ranks. The approach can be applied to various deep convolutional networks, achieving a 6.2\u00d7 speedup for VGG16 with only a 1.2% accuracy loss on ImageNet 2012 validation set. Convolutional layer size can be reduced by 85%. The low rank approximation technique using CP decomposition accelerates convolutional layers in CNNs by factorizing tensors into a sum of rank-one tensors. This method can reduce the size of convolutional layers by 85% and speed up ResNet50 by 1.35\u00d7 with a 1.51% accuracy loss and 48% model size reduction. The CP decomposition technique accelerates convolutional layers in CNNs by factorizing tensors into a sum of rank-one tensors. This method reduces the size of convolutional layers by 85% and speeds up ResNet50 by 1.35\u00d7 with a 1.51% accuracy loss and 48% model size reduction. The R decomposition is expressed with input features X, output features Y, filter weights W, and specific convolutional layer configurations. The complexity of decomposing a convolution layer with rank R can lead to a speedup, but may also result in training instability and slow accuracy improvement in deeper networks. Using a small learning rate and fixing part of the decomposed layer during training can help mitigate these issues. The Two-Pass Decomposition method is introduced to address CP Instability and improve accuracy loss in deeper networks. It involves four steps: 1) CP decomposition of the original filter tensor, 2) restoration of the decomposed tensor, 3) optimization by replacing the original filter tensor with the restored one, and fine-tuning filter weights, and 4) decomposing the optimized filter tensor again. The Two-Pass Decomposition method aims to address CP Instability and enhance accuracy in deeper networks. The optimized filter tensor is decomposed again to prevent instability, with the restored dense form leading to smoother training results and less decomposition error. Experimental results comparing two-pass decomposition with original CP-ALS decomposition on VGG16 show improved accuracy. The comparison between two-pass CP decomposition and original CP decomposition shows that the former retains high accuracy with smaller ranks, allowing for network speedup while maintaining accuracy. A Rank Selection algorithm is proposed for determining the rank for each layer in deeper networks. The two-pass decomposition technique is applied iteratively to group layers in a CNN. The Rank Selection algorithm optimizes target ranks for decomposition, followed by fine-tuning the network with decomposed layers to improve accuracy. The proposed iterative two-pass decomposition flow optimizes rank configuration for CNN layers to improve speedup. The fitness of a convolutional layer is defined by the product of fitness and operation complexity, aiming to enhance decomposition quality. The proposed Rank Selection algorithm optimizes rank configuration for CNN layers based on fitness and complexity. A linear approximation is used to predict fitness, with layers ranked for speedup or accuracy. Two-pass decomposition prevents accuracy degradation iteratively. The approach involves iterative two-pass decomposition of target layers in the VGG16 model, with each iteration consisting of five steps. The process includes fine-tuning the network to optimize weights and fixing decomposed layers in subsequent iterations to improve accuracy. The VGG16 model, based on machrisaa (2016), is used for iterative two-pass decomposition. ImageNet 2012 training set is utilized, achieving 89.9% accuracy with AdaDelta Optimizer in Tensorflow. Speedup is measured on a single-thread 2.7GHz Intel Core i5 CPU. Convolutional layers, except Conv1 1, are decomposed using Baseline rank configuration from BID25. Rank-Selection configuration achieves a 6.2\u00d7 speedup compared to the theoretical 8.4\u00d7. Different decomposition sequences are compared using two grouping schemes in Table 2. In this experiment, two grouping schemes are evaluated for the VGG16 model decomposition. The In-Order scheme groups layers based on connection order, while the Fitness-Based scheme groups layers by fitness sorting. Results show that the Rank-Selection configuration significantly improves accuracy. The In-Order scheme outperforms the Fitness-Based scheme for the Baseline configuration, but differences are less clear with Rank-Selection. The experiment compared two grouping schemes for VGG16 model decomposition. The Fitness-Based configuration outperformed the In-Order scheme, achieving the highest speedup with the lowest accuracy drop. Additional fine-tuning was applied to reduce accuracy drop further. The CP decomposition effectively compresses filter size, reducing convolutional layers by 85% in VGG16. Fully connected layers remain large, resulting in a 9% reduction in overall network size. ResNet50 achieves 92.02% accuracy with iterative decomposition, excluding 1x1 convolutional layers. After iterative two-pass decomposition, the model size reduces by 48% (98MB\u219251MB) with a speedup of 1.35\u00d7. Freezing layers in fine-tuning may lead to optimization getting stuck in local minima, but our approach efficiently decomposes layers while preventing gradient explosion. In-Order grouping scheme performs better for Baseline rank configuration, especially with smaller rank configurations. Fitness-Based approach decomposes layers with smaller ranks first, leading to quick accuracy drop in former layers. The Rank-Selection configuration in the former layers of small ranks shows that the Fitness-Based scheme does not significantly affect accuracy due to higher ranks and fitnesses. The iterative two-pass decomposition flow accelerates deep CNNs, preventing CP instability. VGG16 can be accelerated by 6.2 times with only a 1.20% accuracy drop and 85% size reduction, while ResNet50 can be sped up by 1.35 times with a 1.51% accuracy drop. The future works for ResNet50 include improving the grouping scheme, decomposing order, and rank selection with non-linear fitness estimation. Accelerating 1\u00d71 convolutional layers will also be considered for further improvement on advanced CNNs. Initial fitnesses of baseline rank configuration and fitness-based configuration are shown in APPENDIX A FIG6, with the fitness-based grouping scheme based on the sorting of the baseline rank configuration."
}