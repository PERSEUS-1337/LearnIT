{
    "title": "B1g8VkHFPH",
    "content": "Fine-tuning from pre-trained ImageNet models is the standard for computer vision tasks. This paper re-evaluates common practices for setting hyper-parameters in fine-tuning, emphasizing the importance of momentum and the sensitivity of hyper-parameters to dataset and domain similarity. Transfer learning with fine-tuning pre-trained models is crucial for avoiding overfitting in deep neural networks. Common practices for setting hyper-parameters in fine-tuning need to consider dataset and domain similarity. Reference-based regularization may not be effective for dissimilar datasets, challenging traditional fine-tuning approaches. ImageNet classification models have achieved impressive results for tasks like object detection and segmentation, becoming the standard for computer vision problems. Fine-tuning with default hyperparameters and a small learning rate is common practice to preserve learned knowledge and features. Fine-tuning ResNets with default hyperparameters like learning rate 0.01, momentum 0.9, and weight decay 0.0001 is common practice. However, optimal hyperparameters for fine-tuning on different tasks are not well understood. While some studies have explored learning rate and weight decay, the momentum coefficient is often overlooked. Fine-tuning pre-trained networks is believed to outperform training from scratch, but setting hyperparameters for fine-tuning remains a challenge. Fine-tuning pre-trained models with optimal hyperparameters remains a challenge, as the difference in loss landscape and optimization strategies between fine-tuning and training from scratch can impact performance. Re-examining common practices for fine-tuning through extensive hyperparameter search is necessary to determine the effectiveness of frozen hyperparameters. Our work focuses on understanding the effectiveness of hyperparameters for fine-tuning pre-trained models. Optimal hyperparameters are dataset-dependent and vary based on the similarity between the source and target domains. Using optimization schedules from ImageNet training may not guarantee good performance, leading to unsatisfactory results in some tasks. Our study explores the impact of hyperparameter selection on fine-tuning pre-trained models. We found that adjusting momentum values can significantly affect performance, with zero momentum being more effective for similar tasks and nonzero momentum for different tasks. The effective learning rate, which combines learning rate, momentum, and batch size, plays a crucial role in fine-tuning success. This concept is less explored in existing literature but is essential for optimizing fine-tuning performance. In transfer learning, adjusting momentum values can impact performance, with zero momentum being effective for similar tasks and nonzero momentum for different tasks. The optimal effective learning rate depends on the similarity between the source and target domains. Regularization methods designed to keep models close to the initial model may not apply for dissimilar datasets, especially for nets with Batch Normalization. Fine-tuning the whole network usually results in better performance than using it as a static feature extractor. In transfer learning, researchers have examined the necessity of fine-tuning for good performance. Studies show that transfer learning effectiveness is negative when domains are mismatched, even if they seem similar. Pre-training on a similar source domain benefits transfer learning, as demonstrated by estimating domain similarity with ImageNet. Fine-tuning ImageNet models shows a strong correlation between top-1 accuracy and transfer accuracy. Several studies have questioned the necessity of ImageNet pre-training for various tasks, including object detection and fine-grained object classification. Some researchers have found that training from scratch can be just as effective as fine-tuning, especially with large target datasets. Additionally, transfer learning has been shown to have minimal performance improvement in medical imaging applications but can significantly speed up convergence. Literature also exists on hyperparameter selection for training neural networks, focusing on batch size, learning rate, weight decay, and momentum. In the context of hyperparameter tuning for neural networks, recent studies have proposed an automatic tuner for momentum and learning rate in SGD, showing faster convergence than Adam. While there are correlations between hyperparameters like batch size and learning rate, most tuning methods are designed for training from scratch rather than fine-tuning tasks in computer vision. Limited works have performed large-scale grid search for optimal performance due to computational costs. In this section, the focus is on momentum, effective learning rate, and regularization in the fine-tuning process of neural networks. Momentum accelerates SGD convergence by accumulating a velocity vector. Nesterov momentum SGD iteratively updates the model parameters with hyperparameters like learning rate, batch size, momentum coefficient, and weight decay. In fine-tuning neural networks, key factors include the learning rate, batch size, momentum coefficient, and weight decay. Image classification datasets are evaluated for tasks like object recognition using source domains like ImageNet, Place365, and iNaturalist. Data augmentation techniques such as random mirror, cropping, and color jittering are applied to improve performance. The hyperparameters for fine-tuning neural networks include learning rate, momentum, and weight decay. ResNet-101-V2 is used as the base network, pre-trained on ImageNet. DenseNets and MobileNet are also considered. The default hyperparameters are batch size 256, learning rate 0.01, momentum 0.9, and weight decay 0.0001. Training involves 300 epochs for fine-tuning and 600 epochs for scratch-training to observe convergence behavior. The learning rate is decayed by a factor of 0.1 at epoch 150 and 250. Momentum 0.9 is widely adopted for training from scratch and fine-tuning. To investigate momentum's influence on fine-tuning, optimal values were searched on the Birds dataset. Surprisingly, momentum zero outperformed nonzero momentum in fine-tuning performance. The optimal learning rate increases when momentum is disabled. Disabling momentum works better for Dogs, Caltech, and Indoor datasets, while momentum 0.9 works better for Cars, Aircrafts, and Flowers. Datasets like Dogs, Caltech, Indoor, and Birds have high overlap with ImageNet, while Cars and Aircrafts struggle with fine-tuning from pre-trained models. Similarity to Birds and Dogs is 0.562 and 0.620, while similarity to Cars, Aircrafts, and Flowers is 0.560. The relative order of similarity to ImageNet is Dogs, Birds, Cars, Aircrafts, and Flowers aligning well with the transition of optimal momentum value from 0.0 to 0.9. Fine-tuning from different source domains like Place365 and iNaturalist shows better results than ImageNet for specific datasets. Disabling momentum improves performance when source and target domains are similar, while large momentum works better for fine-tuning on different domains. The Dogs dataset, being close to ImageNet, benefits less from momentum during fine-tuning compared to faraway target domains like Cars and Aircrafts. Decreasing momentum during the final stage of training can aid in finer convergence, as observed in early work. Reducing momentum from 0.99 to 0.9 in the last 1000 parameter updates is recommended for finer convergence. Large momentum helps escape saddle points but can hinder final convergence near optima, suggesting a reduction at the end of training. Momentum SGD should use a smaller step size than vanilla SGD at the final stage of step size annealing. Our work shows that disabling momentum can aid in final convergence, especially when fine-tuning on similar domains. The effective learning rate with momentum set to 0.9 is significantly higher than with momentum at 0.0, impacting optimal learning rates. It is important to consider both momentum and learning rate together, as they are coupled and can affect performance differently. The importance of momentum tuning is highlighted when the effective learning rate is held fixed. Different effective learning rates result in varying performance, emphasizing the significance of the effective learning rate for optimal performance. Changing momentum may have a similar effect to changing the learning rate, as they both impact the effective learning rate. The optimal effective learning rate and weight decay depend on the similarity between source and target domains. ELR is critical for fine-tuning performance, with factors affecting the optimal ELR being of interest. The relationship between ELR and domain distance is important for fine-tuning, encapsulating the effects of learning rate and momentum. Varying hyperparameters showed that a smaller \u03b7 works better for similar source and target domains, while a larger ELR is needed for training from scratch. The relationship between weight decay and effective learning rate is well-studied, showing that the optimal weight decay value is inversely related to the learning rate. The optimal effective weight decay is larger when the source domain is similar to the target domain. This information can help in reducing hyperparameter search ranges. The text discusses the use of source models for domain similarity score calculation and proposes a strategy for ELR selection based on similarity scores. It also mentions the inadequacy of standard L2 regularization in transfer learning and the use of reference-based regularization to retain pre-trained knowledge. The text discusses the limitations of standard L2 regularization in transfer learning and proposes a reference-based regularization approach to retain pre-trained knowledge. The text discusses the limitations of standard L2 regularization in transfer learning and proposes a reference-based regularization approach to retain pre-trained knowledge. L2 regularization drives weights towards zero, preserving functionality, while L2-SP norm constrains weights to original scale, avoiding increased learning rate during fine-tuning. This approach may explain why L2-SP provides better performance in similar domain fine-tuning. The text discusses the limitations of standard L2 regularization in transfer learning and proposes a reference-based regularization approach to retain pre-trained knowledge. In contrast to L2 regularization, L2-SP norm avoids increased learning rate during fine-tuning by constraining weights to their original scale. Experimental results show that L2 regularization performs as well as or better than L2-SP on datasets like Birds, Cars, Aircrafts, and Flowers, suggesting that reference-based regularization methods may not generalize effectively for fine-tuning on dissimilar domains. The default hyperparameter setting may work well for random initialization, but for fine-tuning, the choice of hyperparameters is dataset dependent and influenced by the similarity between the target and source domains. The rarely tuned momentum value can hinder performance when the domains are close, connecting with previous theoretical works on decreasing momentum and effective learning rate. The optimal effective learning rate depends on the similarity of the domains, allowing for significant reduction in computational costs. The influence of momentum on fine-tuning was investigated by searching for the best momentum values on the Birds dataset. Zero momentum outperformed nonzero momentum in most configurations, and disabling momentum led to an increase in optimal learning rate. Performance comparisons were made for datasets similar to ImageNet, showing a decrease in validation error when turning off momentum. After investigating the influence of momentum on fine-tuning, it was found that disabling momentum led to a decrease in validation error for datasets similar to ImageNet. The optimal learning rate generally increased 10x when changing from 0.9 to 0.0, in line with the rule of effective learning rate. Similar observations were made on DenseNet-121 and MobileNet-1.0, showing that the optimal effective learning rate is related to the similarity to the source domain. Additionally, the optimal effective learning rates for different datasets varied when fine-tuned from ImageNet, confirming findings on ResNet-101. In Appendix C, the correlation between EMD based domain similarity and optimal ELR is consistent across different architectures. EMD calculation is introduced in (Cui et al., 2018) where ResNet-101 is trained on JFT dataset for feature extraction. Features are averaged by category and denoted as g(s i ) for source domain and g(t j ) for target domain. The EMD between source domain S and target domain T is calculated based on the average feature vectors of labels. Each label is assigned a weight corresponding to the percentage of images with that label. The domain similarity is determined by solving the EMD optimization problem, with domain similarities listed in Table 5 for seven datasets using a pre-trained ResNet-101 model. The model was trained on JFT dataset for feature extraction. The study compares domain similarities using pre-trained ResNet-101 model for feature extraction on different datasets like Caltech and Indoor. Due to unavailability of JFT dataset, the model is used directly as feature extractor. The approach aims to capture transfer learning process better for various source models. The study compared domain similarities using different ImageNet pre-trained models. Consistent patterns were found across architectures, with Caltech and Dogs having the highest similarity scores, followed by Birds and Indoor. Cars, Aircrafts, and Flowers were the most dissimilar datasets. The domain similarity score showed correlation with the scale of optimal ELR, where more similarity led to smaller optimal ELR. The scores provided reasonable predictions about the scale of optimal ELR, reducing search ranges. The study compared domain similarities using different ImageNet pre-trained models and found consistent patterns. The domain similarity score correlated with the scale of optimal ELR, leading to smaller optimal ELR for more similar datasets. Kornblith et al. (2018) suggested adjusting the batch normalization momentum parameter for fine-tuning, based on the dataset size and number of steps per epoch. The study explored the effect of batch normalization (BN) momentum on model performance. They fine-tuned a pre-trained model with different BN momentum values (0.0, 0.95, 0.99) but did not observe significant performance differences. The experiments were conducted using an ImageNet pretrained ResNet-101-V1 model with a batch size of 64. The model is fine-tuned with batch size 64 in 9000 iterations, using momentum 0.9. Grid search was performed on learning rate and weight decay. Data augmentation is crucial for increasing data quantity and diversity, acting as a regularization method. Data augmentation methods like random mirror flipping, rescaled cropping, and color jittering have been effective for training ImageNet models. The impact of data augmentation on fine-tuning varies by dataset, with advanced cropping working better for Cars, Aircrafts, and Flowers but worse for Dogs. Data augmentation methods have a significant impact on convergence behavior, with simpler methods converging quickly while random resized cropping converges slower. Default hyperparameters and data augmentation can lead to overfitting on the Dogs dataset, but disabling momentum can improve performance. Random resized cropping adds extra variance to the gradient direction, and disabling momentum is more effective in this case. Comparing data augmentation methods with different momentum settings, random resized cropping consistently outperforms random cropping on various datasets. Random resized cropping outperforms random cropping on datasets like Cars, Aircrafts, and Flowers. Using momentum significantly improves performance for both methods. Advanced data augmentation with default hyperparameters leads to overfitting on Dogs and Caltech datasets. Random resized cropping with zero momentum solves this issue and performs better than random cropping. Disabling momentum for random cropping still results in better performance for Dogs but decreases for other datasets. Increasing the learning rate for random cropping and disabling momentum shows improved performance. Transfer learning from ImageNet subsets shows varying performance on different datasets. Fine-tuning from ImageNet-Natural pre-trained models performs better on Birds and Dogs, while Caltech-256 and Indoor benefit more from ImageNet-Manmade pretrained models. The performance gap between the two subsets is more significant for Birds and Dogs compared to Cars and Flowers. Surprisingly, fine-tuning from ImageNet-Manmade yields worse performance on Cars and Indoor datasets. Fine-tuning results on both subsets do not exceed pre-trained models with full ImageNet. Scratch. Results on subsets do not surpass pre-trained models with full ImageNet. Scratch training can outperform fine-tuning with better hyperparameters. After slight tuning, scratch training performance gets close to fine-tuning results, even surpassing in some cases like Cars and Aircrafts. Previous studies also found that datasets like Cars and Aircrafts do not benefit much from fine-tuning. Results of ResNet-101 DELTA and Inception-v3 are compared with fine-tuning and scratch training on subsets. Training ResNet-101 from scratch on each subset with standard hyperparameters and training schedules. Top-1 errors of training ResNet-101 on each source dataset are illustrated in Table 8. Training from scratch on each dataset with different learning rates and weight decay is shown in Figure 12. Training ResNet-101 on different datasets with varying learning rates and weight decay parameters. Weight decay of 0.0005 consistently outperforms 0.0001. Parameters searched include \u03b7 and \u03bb with fixed momentum and batch size."
}