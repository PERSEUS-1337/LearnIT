{
    "title": "SJICXeWAb",
    "content": "Recent work has demonstrated a distinction in the capabilities of depth-2 and depth-3 neural networks. The separation is achieved by constructing functions and input distributions that can be well-approximated by depth-3 networks but not by depth-2 networks, although these results are not robust and require careful selection of functions and input distributions. In recent work, a distinction has been shown between the capabilities of depth-2 and depth-3 neural networks. The expressive power of depth-2 and depth-3 sigmoidal neural networks over various input distributions is explored, with a focus on understanding the functions that can be represented and approximated by neural networks of bounded size, depth, width, and weights. The study reveals that depth-2 sigmoidal neural networks with small width and weights can be well-approximated by low-degree multivariate polynomials. This research contributes to understanding the success of deep neural networks in various domains. The power of depth in neural networks is a key focus in understanding their expressive capabilities. Depth-2 neural networks are shown to be universal approximators, able to represent arbitrary continuous functions on bounded domains. Parameters such as number of neurons, width of hidden layers, and weight magnitude play a role in determining the representability of natural signals like images and speech. Deeper networks can be seen as representing deeper hierarchies, highlighting the importance of depth in neural network architectures. The power of depth in neural networks has been investigated to understand its effect on expressive power. A study shows a separation between depth-2 and depth-3 networks, where a depth-3 network can represent a bounded function with a polynomially bounded size, while a depth-2 network would require an exponentially large size to approximate the same function within a small constant. This separation holds for various activation functions like ReLUs and sigmoids. The study explores the expressive power of depth in neural networks, showing a separation between depth-2 and depth-3 networks. Depth-3 networks can approximate a bounded function with polynomial size, while depth-2 networks would need exponentially large size. This separation holds for various activation functions like ReLUs and sigmoids. The study shows a separation between depth-2 and depth-3 neural networks in terms of expressive power. Telgarsky (2016) demonstrates a separation between depth-2k 3 + 8 and depth-k ReLU networks. BID11 also shows that univariate functions on a bounded interval require larger neural network sizes for uniform approximation with constant depth compared to deep networks. Deep networks can approximate constructed functions well, unlike shallow networks, which struggle with approximation using a carefully defined distribution. Liang & Srikant (2017) is distribution-independent and deals with uniform approximation. The extent to which deeper networks are more expressive than shallow ones remains unclear. Large classes of functions and distributions may show the separation between deep and shallow networks, shedding light on practical applications of neural networks. BID17, BID16, and BID18 demonstrate that even functions computed by depth-2 neural networks can be challenging to learn using gradient descent algorithms for various distributions, focusing on learnability rather than expressive power. Our depth separation results apply to neural networks with bounds on the magnitudes of the weights. Small weights are natural in training neural networks to avoid overfitting, achieved through weight decay and early stopping. Keeping weights low also helps maintain the Lipschitz constant of the function computed by the network. The importance of keeping weights low in neural networks is to maintain the Lipschitz constant of the function. Adversarial examples, which are misclassified inputs due to tiny perturbations, suggest a high Lipschitz constant. Regularizing training by penalizing high Lipschitz constant can help improve generalization error and eliminate adversarial examples. This can be achieved by enforcing an orthonormality constraint on weight matrices. The study explores the impact of enforcing an orthonormality constraint on weight matrices in neural networks, leading to better resilience against adversarial examples. Additionally, it demonstrates that Lipschitz constant alone may not fully explain generalization, showing that certain functions can be well-approximated by depth-3 sigmoidal neural networks with bounded size and weights in d-dimensions. This separation holds for a wide range of input distributions with a density of at least 1/poly(d) on a small ball in Bd. In this section, it is shown that sigmoid neurons can be well-approximated by low-degree polynomials, allowing for depth-2 sigmoidal neural networks to be approximated by low-degree multivariate polynomials. The Chebyshev polynomial approximation method is used to closely approximate the minimax polynomial to a given function. The bias term in the activation function is dropped for simplicity, as it does not affect the generality of the results. The activation function of a sigmoid neuron can be approximated by low-degree polynomials using Chebyshev polynomials. The coefficients in the Chebyshev expansion of the sigmoid neuron are bounded, allowing for a low-degree polynomial approximation. This observation was previously discussed in a paper by Shalev-Shwartz et al. (2011). A depth-2 sigmoidal neural network with bounded weights and width can be approximated by a low-degree polynomial. This is crucial in some results, such as a weaker version of Daniely's separation result for neural networks. The logarithmic dependence in the bound does not hold for a ReLU neuron. A depth-2 sigmoidal neural network with bounded weights and width can be represented by a low-degree polynomial. Proposition 4 provides a multivariate version of this concept, emphasizing the importance of approximating a sigmoid neuron with a low-degree polynomial. A depth-k sigmoidal neural network with polynomial width and bounded weights can be well-approximated by a low-degree multivariate polynomial. The growth of the degree of polynomial approximation depends on the widths of hidden layers, allowing for a depth separation result when weights are bounded. A depth-k sigmoidal neural network with polynomial width and bounded weights can be approximated by a low-degree polynomial. This approximation has a polynomial degree in d when n and B are polynomial in d and k is constant. Daniely proves that y cannot be approximated by a depth-2 neural network with polynomial size and exponentially bounded weights, but can be approximated by a depth-3 ReLU neural network with polynomial size and bounded weights. This shows a separation between depth-2 and depth-3 ReLU neural networks. Daniely's proof involves harmonic analysis on the unit sphere and requires a uniform distribution. The proof of separation between depth-2 and depth-3 sigmoidal neural networks involves showing that a Lipschitz function can be computed by a depth-2 network with bounded weights. This demonstrates the expressive power difference between the two network depths. The Lipschitz property allows for approximation by a depth-3 neural network, but being far from low-degree polynomials prevents approximation by depth-2 networks. Modifying the function to G(x) = sin(\u03c0N x^2) maintains the lower bound for any distribution over B^d with a radial line segment of length at least 1/poly(d). This inapproximability by depth-2 networks can also be shown with L2-norm for a large enough N = poly(d). The Lipschitz property allows for approximation by a depth-3 neural network, while being far from low-degree polynomials prevents approximation by depth-2 networks. G(x) can be well-approximated by a depth-3 sigmoidal neural network of polynomial size and weights. This is similar to Daniely's construction for ReLU networks. By Lemma 6, there exists a function f : [\u22121, 1] \u2192 R computed by a depth-2 sigmoidal neural network for approximating sin(\u03c0d 3 t) over [0, 1]. The composition of depth-2 neural networks can result in a depth-3 neural network. A lower bound is shown for functions computed by depth-2 sigmoidal neural networks. A polynomial of degree D cannot match the sign of sin(\u03c0N t) on all points in S, leading to a contradiction. The proof technique for sigmoid neurons with biases carries over from the case without biases. By considering a new weight vector and input with an additional dimension, the input lies on a hyperplane slice. The ideas in the proofs generalize without technical modifications, with details deferred to the full version. Lower bounds under the L2-norm are shown for the class of densities on B d. The theorem provides a technical condition on probability densities on B d for a lower bound to hold. An example is given with a convex set K in B d where points are at least r away from the boundary, satisfying certain conditions. The function G(x) = sin(\u03c0N x 2 ) is considered, with \u00b5 being a probability density over B d with specific conditions on a subset C. The induced probability density on affine lines in the intersection \u2229 C is (\u03b1, \u03b2)-uniform. A function F computed by a neural network cannot \u03b4-approximate G under L 2 -norm. A lower bound on L 2 -error of approximating G with a multivariate polynomial P is shown under a specific distribution given by \u00b5 on B d. The affine line u is defined as {x = u + tv : t \u2208 R}, where IC(u, t) = 1 if u \u2208 B d\u22121 and u + tv \u2208 C for some interval I of length at least r. The distribution induced by \u00b5(x) along any line segment of length at least r in the intersection \u2229 C is (\u03b1, \u03b2)-uniform. The function sin(\u03c0N(u^2 + t^2)) alternates its sign as u^2 + t^2 takes values that are successive integer multiples of 1/N. The text discusses the alternating sign behavior of the function sin(\u03c0N(u^2 + t^2)) on segments of length 1/N. It also presents a proof showing the L2 separation between depth-2 and depth-3 neural networks under probability density \u00b5. The text discusses the approximation of depth-2 sigmoid neural networks by multivariate polynomials and the limitations of approximating functions in L2 by polynomials. The text discusses the approximation of depth-2 sigmoid neural networks by multivariate polynomials. Proposition 2 guarantees polynomials of degree O(B log(B/\u03b4)) that approximate \u03c3(w_i t) well. The polynomial p(t) = \u2211(a_i * p_i(t)) has degree O(B log(nB^2/\u03b4)). The text discusses approximating depth-2 sigmoid neural networks with multivariate polynomials. By induction, each F j (x) can be 1-approximated by a d-variate polynomial Q j (x) of degree O(nB) k\u22122 log(k\u22122) in each variable. The output of the network is bounded by |F j (x)| \u2264 nB, leading to |Q j (x)| \u2264 2nB. A polynomial q(t) approximates \u03c3(Q j (x)), and a polynomial p approximates \u03c3(w_i, q). Polynomial p of degree at most O(nB log(nB/ )) approximates \u03c3(w_i, q) with small error. P(x) = \u03a3 a_i p(w_i, q) for x in Bd. P(x) is a d-variate polynomial of degree in each variable."
}