{
    "title": "BJxbOlSKPr",
    "content": "In this work, a differentiable product quantization (DPQ) framework is proposed to compress embedding layers used in NLP applications. DPQ offers significant compression ratios (14-238x) with minimal performance impact on various language tasks. The embedding layers in NLP applications, such as language modeling and machine translation, consume large storage and memory due to large vocabulary sizes. Efforts have been made to reduce the size of embedding layers, with recent advancements in compressing techniques like differentiable product quantization (DPQ) offering significant compression ratios without compromising performance on language tasks. In recent advancements, efforts have been made to reduce the size of embedding layers in NLP applications. Chen et al. (2018b) and Shu and Nakayama (2017) proposed methods to encode symbols/words with discrete codes, but with limitations. Chen et al. (2018b) suggests learning codes in an end-to-end fashion for better task performance, while Shu and Nakayama (2017) uses fixed codes. A novel differentiable product quantization (DPQ) framework is proposed in this work to address these limitations. The proposed differentiable product quantization (DPQ) framework aims to obtain discrete codes in an end-to-end fashion, allowing for more flexible model designs and achieving better task performance and compression efficiency. Compared to existing methods, it offers a new perspective on obtaining discrete codes and avoids the need for cumbersome distillation procedures. The proposed DPQ framework replaces the original embedding layer to learn compact discrete embeddings with higher compression ratios and equivalent performance. Results are achieved through end-to-end training without the need for distillation procedures. The goal is to learn a compact embedding function with a substantially smaller number of bits than the original full embedding table. Discrete codes are derived from the quantization process, making it flexible and differentiable. The proposed differentiable production quantization (DPQ) function enables end-to-end learning of discrete codes by transforming continuous spaces into a discrete bottleneck. This involves using a discretization function to map a continuous vector into a K-way D-dimensional discrete code, and a reverse-discretization function to map the discrete code back into a continuous embedding vector. The general DPQ mapping is T(\u00b7) = \u03c1 \u2022 \u03c6(\u00b7), allowing for a compact embedding layer to be obtained via DPQ. The DPQ function involves transforming raw embeddings into discrete codes using a discretization function. The final symbol embedding matrix is constructed using a reverse-discretization function. The codebook and parameters needed for reverse-discretization are stored for compact inference. The discretization and reverse-discretization functions are specified using product keys and values. The DPQ function transforms raw embeddings into discrete codes using a discretization function. The KD codebook C is computed using a Key matrix K. Columns of K and Q are split into D groups to compute D dimensional KD codes separately. During training, differentiable product quantization approximates the raw embedding table. At inference, only the codebook C and the Value matrix V are needed to construct the embedding table. The dist(\u00b7, \u00b7) function computes distance measure between vectors to determine discrete codes. The reversediscretization function uses a single Value matrix V to compute final continuous embedding vectors. Columns of V are split into D groups, indexed by the code in each dimension, and concatenated to form the final embedding vector. This simplification reduces computation overhead and eases optimization. The proposed method simplifies the computation overhead and optimization by using a learned hash function to generate KD codes for inference. The DPQ embedding reduces storage complexity by only requiring the codebook and Value matrix, making it more compact than the original full embedding table. Inference complexity is also minimized as only indexing and concatenation are used, resulting in negligible extra computation and memory footprint compared to regular embeddings. The DPQ embedding method introduces sparsity into the embedding matrix to achieve compactness without reducing rank. The matrix H is full rank under specific constraints, allowing for good compression ratio while maintaining full rank. Designs of the discretization function are not specified in the current discussion. The DPQ embedding method introduces sparsity into the embedding matrix to achieve compactness without reducing rank. Designs of the discretization function, such as the distance function and gradient computation, are not specified. Two DPQ instantiations are introduced, with the first one (DPQ-SX) approximating the arg max operation with a differentiable softmax function. The DPQ embedding method introduces sparsity into the embedding matrix to achieve compactness without reducing rank. The softmax function is used to compute discrete codes, with different temperatures set during forward and backward passes to enable a pseudo gradient. The DPQ-VQ instantiation uses a centroid-based approximation for gradient computation. The DPQ embedding method introduces sparsity into the embedding matrix to achieve compactness without reducing rank. It uses a centroid-based approximation for gradient computation, allowing the direct passing of gradients through centroids. This enables the computation of discrete codes in the forward pass. The DPQ embedding method introduces sparsity into the embedding matrix to achieve compactness without reducing rank. It uses a centroid-based approximation for gradient computation, allowing the direct passing of gradients through centroids. The Eq. 7 approximates gradient for the Query matrix but does not update the centroids. Regularization terms like arithmetic mean or Exponential Moving Average can be used to update centroids. DPQ-SX approximates the one-hot vector with softmax, while DPQ-VQ approximates the continuous vector using centroids. DPQ-VQ and DPQ-SX differ in how they approximate the gradient for the non-differentiable arg min function. The DPQ embedding method introduces sparsity into the embedding matrix to achieve compactness without reducing rank. DPQ-SX is more flexible than DPQ-VQ, as it does not constrain the distance metric or tie the Key/Value matrices. DPQ-SX back-propagates through the whole distribution of K choices, while DPQ-VQ only back-propagates through the nearest centroid, making it more scalable. Experiments were conducted on ten datasets across three tasks: language modeling, neural machine translation, and text classification. The DPQ embedding method introduces sparsity for compactness without reducing rank. DPQ-SX is more flexible than DPQ-VQ, back-propagating through the whole distribution of K choices. Experiments were conducted on ten datasets for language modeling, neural machine translation, and text classification. The models were evaluated based on task performance metrics and compression ratios, with further compression achievable through 'subspace-sharing'. DPQ-SX and DPQ-VQ were compared against baseline models using regular full embeddings. DPQ-SX and DPQ-VQ outperform baseline models in various tasks with comparable or better performance and compression ratios ranging from 14\u00d7 to 163\u00d7. DPQ-SX shows superior results in 6 out of 10 datasets. The DPQ method can further compress sub-word representations, demonstrating its potential for learning very compact embedding layers. Comparisons with other embedding compression methods were also conducted. The DPQ method achieves good compression ratios and perplexity scores on language modeling tasks using LSTMs with different model sizes. The E2E-dist. method shows similar compression ratios and perplexity scores as the full embedding baseline, but requires extra distillation guidance during training. DPQ variants, particularly DPQ-SX, achieve competitive perplexity scores and offer significantly higher compression ratios compared to alternatives. Key hyper-parameters include code size (K) and code length (D). A small K and large D configuration is better, with adjustments in K and D impacting task performance and compression ratio. Increasing K or D improves task performance but lowers compression ratios. Adjusting K and D can optimize task performance and compression ratio. DPQ variants achieve competitive perplexity scores and higher compression ratios. Adjusting code size (K) and code length (D) impacts task performance and compression ratio trade-off. Decreasing D has a more significant effect on DPQ-VQ than on DPQ-SX due to the nearest neighbour approximation becoming less exact. DPQ incurs slightly higher computational cost during training but no extra cost at inference. The KD codes learned end-to-end via DPQ show well-utilized centroids and a changing codebook that converges to < 20% throughout training. Modern neural networks with many parameters and redundancies have attracted research efforts for model compression. Most techniques focus on shared weights in convolutional and dense layers, but compressing embedding layers presents different challenges. Existing work on compressing embedding layers includes methods by Shu and Nakayama (2017) and Chen et al. (2018b). Our new formulation for compressing embedding layers leverages product quantization, allowing for two types of instantiations with different gradient approximation. The product keys and values in our model enhance efficiency in training and inference, achieving better compression ratios without the need for extra distillation processes. Unlike traditional quantization techniques, our approach can be trained end-to-end and utilizes multiple orthogonal subspaces/groups for quantization. In this work, a novel differentiable product quantization framework is proposed for compressing embedding layers. Two instantiations of the framework are provided, surpassing existing compression methods by achieving up to 238\u00d7 compression of the embedding table without loss in performance. The DPQ framework can compress the embedding table up to 238\u00d7 without performance loss. The algorithm pseudo-code for the DPQ embedding layer during forward training/inference pass is provided. The DPQ framework compresses the embedding table without performance loss. The output embedding matrix H is full rank due to the re-parameterization. Training the Transformer Model on WMT'19 En-De dataset involves a vocabulary of 32k sub-words generated using SentencePiece tokenizer. The Transformer model in the DPQ framework utilizes a context window size of 256 tokens and is trained with a batch size of 2048 sentences for 250k steps. The SM3 optimizer with momentum 0.9 and a quadratic learning rate warm-up schedule is used. The code distribution heat-maps for the Transformer model on WMT'19 En-De show that DPQ-VQ has evenly distributed code utilization, while DPQ-SX has a more concentrated and sparse code distribution. The codebook in the DPQ framework changes during training, with some codes not being used. DPQ-SX shows tighter clusters of similar words, while DPQ-VQ has further neighbors. Semantically related words share common codes in more dimensions. Figure 7 displays heatmaps with varied K and D, showcasing data for different days of the week and various political figures and cities. Figure 7 displays heatmaps of task performance and compression ratio, with the option of subspace-sharing improving the compression ratio. Figure 8 shows trade-off curves for different DPQ variants, highlighting the task-dependent effect of subspace-sharing on LM and NMT tasks. In this work, discrete codes are computed as an outcome of product quantization, allowing for generalization beyond a fixed set of vocabulary. Two variants with different approximation techniques are derived, making the framework efficient compared to previous methods. Our DPQ method is efficient with small memory footprint and reduced approximation errors compared to traditional approaches like scalar quantization and product quantization. Unlike previous methods, DPQ can be trained end-to-end without distillation loss. DPQ utilizes discrete codes and product quantization to generate high-rank embedding tables with sparse factorization. Unlike traditional compression techniques like pruning and low-rank factorization, DPQ is end-to-end differentiable, allowing neural nets to adapt to quantization errors. Comparisons on PTB language modeling task show DPQ's efficiency with small memory footprint and reduced approximation errors compared to scalar quantization and product quantization. The proposed method DPQ improves compression ratio and task performance compared to traditional techniques like scalar quantization and low-rank factorization. DPQ achieves better accuracies and compression ratios in text classification tasks, supporting end-to-end compact embedding learning. The proposed method DPQ improves compression ratio and task performance compared to traditional techniques like scalar quantization and low-rank factorization. In experiments, DPQ outperforms the reconstruction baseline in accuracy and compression ratios on WMT19 (En-De) translation task based on Transformer. The reconstruction baseline shows significantly degraded performance due to small approximation errors accumulating in the embedding layer. Our method DPQ improves compression ratio and task performance compared to traditional techniques like scalar quantization and low-rank factorization. In experiments, DPQ outperforms the reconstruction baseline in accuracy and compression ratios on WMT19 (En-De) translation task based on Transformer. The ablation study on PTB language modeling task shows that DPQ-SX (with untied K,V) performs the best, followed by DPQ-SX (tied K,V) and DPQ-VQ. It is important to demonstrate that DPQ can achieve competitive performance on BERT, with BERT-base pre-trained on 512-token sequences for 1M iterations with batch size 1024. In experiments, DPQ-SX with D=128 and K=32 is used for pre-training BERT, replacing the embedding layer. The DPQ model performs similarly to full embedding in downstream tasks, with a compression ratio of 37\u00d7 on the embedding table, saving 24M parameters in the BERT-base model."
}