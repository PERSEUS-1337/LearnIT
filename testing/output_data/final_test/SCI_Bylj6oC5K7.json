{
    "title": "Bylj6oC5K7",
    "content": "In this paper, logit regularization techniques are proposed as an adversarial defense for neural networks. The effectiveness of this defense mechanism is demonstrated against white-box and black-box attacks, improving defense against PGD-based models. Neural networks can be vulnerable to adversarial examples, which are data intentionally crafted to deceive the model. These examples pose a security threat by allowing attackers to control a model's behavior. Current research focuses on creating adversarial examples through small data transformations, but the full extent of this threat depends on the attacker's creativity. Performance on adversarial examples can be significantly worse than on normal data. Performance on adversarially chosen examples can be much lower than on unperturbed data, with white-box accuracy for CIFAR-10 image classification dropping below 50%. Current defenses include adversarial training, input transformation, and generative models to detect out-of-sample data. Adversarial logit pairing is a robust method that improves model performance by regularizing logits towards zero, resulting in increased robustness to adversarial examples. Other alternatives for logit regularization, such as label smoothing, can also significantly enhance model robustness, with potential improvements of over 40% against projected gradient descent attacks. Adversarial logit pairing improves model robustness by regularizing logits towards zero, enhancing defense against adversarial attacks. It sets a new state-of-the-art for PGD-based adversaries on CIFAR-10, requiring minimal computational overhead. The study of adversarial examples has gained prominence in the deep learning community, with a focus on enhancing defense mechanisms. In the current renaissance of deep learning, BID5 introduced adversarial training with a weighted loss between original and adversarial examples. The method, known as the \"fast gradient sign method\" (FGSM), involves a single signed gradient adjustment to create adversarial examples. BID11 extended this method into a multi-step attack by iteratively adjusting the perturbation applied to input examples. BID12 improved upon previous attacks by randomly initializing the search process for adversarial perturbations, making it the strongest attack currently available. It approximates the strongest adversary found with current first-order methods, but adversarial training with this approach can be costly. BID9 introduced adversarial logit pairing (ALP) to improve on PGD-based adversarial training by encouraging similar logits for original and adversarial examples. They also studied a baseline version called \"clean logit pairing\" which paired unperturbed examples together, showing promising results. The study introduced adversarial logit pairing (ALP) to improve adversarial training by aligning logits of original and adversarial examples. They also explored \"clean logit squeezing\" which regularizes the model's logits effectively. During training, the model's weights are updated based on the gradient of the loss term with respect to the logits. Adversarial examples aim to move the model's predictions away from the correct label, resulting in updates that make original logits smaller and adversarial logits larger for the correct category, and vice versa. This is considered in the context of the adversarial training loss, which already encourages higher adversarial logits for the correct label. The main effect of adversarial logit pairing is to update the logits of the original example to be smaller for the correct category and larger for all incorrect categories. This has a similar effect to regularizing model logits, such as \"logit squeezing\" or label smoothing. Incorporating the scale of the logits in the logit pairing term implies that the model would always attempt to update the scale of the logits. The model trained with Adversarial Logit Pairing (ALP) updates logits towards zero, counterbalanced by adversarial training. Experimentally verifying smaller logits with ALP compared to standard training can provide insights for adversarially robust methods. The logits for a model trained with ALP are smaller than those trained with PGD, indicating logit regularization. Further experiments show that adversarial robustness can be improved by regularizing the logits. Regularizing the logits in a model trained with adversarial logit pairing can recover some improvement, but too much regularization can be harmful. However, adding regularization to a model already trained with ALP does not lead to any improvement and can hurt performance. This suggests that one key improvement from logit pairing is due to logit regularization. The results do not diminish the effectiveness of ALP, but rather aim to understand how it works and explore potential generalizations or improvements. Label smoothing is a method of regularizing logits by replacing the one-hot training distribution of labels with a softer distribution. It was introduced as a form of regularization to prevent models from being too confident about training examples, leading to improved generalization. This technique can be easily implemented as a preprocessing step on the labels and does not significantly affect model training time. In a study comparing the effects of label smoothing on model performance, it was found that disabling label smoothing in a model trained on ImageNet improved adversarial robustness by 1%. However, a different effect was observed in a separate experiment, where using only label smoothing resulted in a model nearly as robust as those trained with adversarial techniques, but in significantly less time. This benefit of label smoothing did not lead to a significant loss in accuracy on unperturbed test data. Adding label smoothing significantly improves robustness to adversarial attacks, with a notable increase in accuracy. Models trained with label smoothing show a smaller dynamic range of logits and a more consistent logit distribution, contributing to their adversarial robustness. This effect is observed across different levels of label smoothing. Recently, a new form of data augmentation was discovered that combines different training examples together, altering their appearance and labels. This method involves element-wise weighted averaging of two input examples, resulting in softer target labels during training and encouraging linear behavior between examples. This type of data augmentation has been found to improve robustness to adversarial attacks, such as FGSM attacks on ImageNet. In experiments using VH-mixup, a form of data augmentation, researchers observed increased robustness to FGSM attacks but no significant improvement against PGD-based adversaries. Robustness to a 5-step PGD adversary slightly increased, while robustness to a 10-step PGD adversary remained unchanged. The interaction of logit regularization methods with adversarial logit effects is still unclear. In adversarial logit pairing, the logit pairing term is implemented as an L2 loss, with the goal of making logits more similar and regularizing them towards zero. Different similarity metrics can be used, such as Jensen-Shannon divergence or cross entropy between distributions induced by the logits. Adversarial logit pairing improves adversarial robustness by explicitly decomposing it into logit pairing and regularization terms. The strength of the pairing loss is set as a constant fraction of the adversarial loss, leading to a 1.9% improvement over ALP and a 5.6% improvement over standard PGD-based adversarial training on CIFAR-10. In experiments on CIFAR-10, a ResNet BID6 model with weight decay of 2 \u00b7 10 \u22124 and momentum optimizer with strength of 0.9 was used. Standard data augmentation was applied, and the learning rate peaked at 0.1 before decaying by a factor of 10 at 100 and 150 epochs. Adversarial examples had a maximum L \u221e norm of .03, with PGD-based attacks using a step size of 0.0078. CleverHans library in TensorFlow was used for constructing adversarial attacks. In experiments on CIFAR-10, a ResNet model with weight decay and momentum optimizer was used. Adversarial attacks were constructed using the CleverHans library in TensorFlow. The focus is on making a model robust to PGD-based attacks by combining logit regularization methods and training on adversarial examples. Label smoothing, VH-mixup, and logit pairing formulation were used to enhance robustness. LRM (\"Logit Regularization Methods\") achieved the highest level of adversarial robustness on CIFAR-10 for all PGD-based attacks. This comes at the cost of performance on the original test set, as clean test images are at the center of feasible adversarial examples. The tradeoff between adversarial and non-adversarial performance is notable. The tradeoff between adversarial and non-adversarial performance can be pushed further, with the optimal value depending on the application. Black-box performance is evaluated by generating adversarial examples with one model and testing them on a separate model. The success of a black-box attack depends on the similarity of training procedures between the models and the strength of the source model. Using LRM as the source results in a stronger black-box attack. In white-box and black-box analyses, label smoothing was surprisingly effective, damaging black-box defenses of PGD and ALP. Label smoothing was robust in black-box attacks, outperforming adversarially-trained models. Further investigation showed mixed behavior, with a model trained with label smoothing being less robust to stronger white-box attacks. In this study, the researchers observed a decrease in accuracy when increasing the number of iterations in PGD attacks. Label smoothing was found to make the adversarial optimization problem more challenging, potentially explaining its effectiveness against black-box attacks. The exact mechanism behind this remains unclear, suggesting a need for further research. Additionally, logit regularization was shown to enhance neural network robustness against adversarial examples. The study explores the effectiveness of logit regularization in improving adversarial defense. Different forms of logit regularization were investigated, leading to a new method for adversarial logit pairing. Combining these techniques resulted in a stronger defense against white-box attacks and a stronger attack against defenses. Label smoothing was also found to be effective against black-box attacks but vulnerable to highly-optimized white-box attacks. Future research is expected to further explore logit regularization. Future research is expected to push the limits of logit regularization to improve defenses against adversarial examples, potentially incorporating techniques from other areas. Insights into training adversarially-robust models without the need for multi-step adversarial training are also sought to scale up defenses to larger datasets efficiently."
}