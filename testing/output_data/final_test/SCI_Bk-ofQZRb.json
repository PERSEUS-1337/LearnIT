{
    "title": "Bk-ofQZRb",
    "content": "Temporal Difference Learning with function approximation can be unstable, but previous work has proposed stable alternatives. TD-learning with neural networks often requires using a target network that updates slowly. This study introduces a constraint on the TD update to minimize changes to target values, which can be applied to any TD objective with nonlinear function approximation. The technique is validated through deep Q-learning without a target network, and it prevents Q-learning from diverging in Baird's counterexample. Learning techniques like TD(\u03bb), Q-learning, and Sarsa have limitations and do not converge on simple examples. Residual gradients are proposed as a solution to this issue by splitting the TD error over the current and next state. However, these methods are slower to converge and may not reach the desired fixed point. An alternative objective function is suggested to address this problem by projecting the TD target onto the linear function approximator basis, ensuring convergence to the desired fixed point. In this work, a constraint is proposed on parameter updates to minimize changes to target values when using deep networks for value function approximation. This approach aims to freeze the target values that the current predictions are moving towards, offering an alternative to traditional techniques like Q-learning and Sarsa. The current work proposes a constraint on parameter updates to minimize changes to target values in deep networks for value function approximation. This approach aims to freeze the target values that current predictions are moving towards, providing an alternative to traditional techniques like Q-learning and Sarsa. The method is validated on Baird's counterexample and a gridworld domain, using a multi-layer perceptron for value function parametrization without the need for a target network. The focus is on TD(0) methods such as Sarsa, Expected Sarsa, and Q-learning, which minimize the TD error. The TD methods like Sarsa, Expected Sarsa, and Q-learning minimize the TD error. The update in Q-learning is determined by the choice of \u03c0, with the target being max a q(s t+1 , a). Using function approximation in TD-learning minimizes the squared TD error. The gradient of the TD loss with respect to state s t and parameters \u03b8 t is used to update the parameters through gradient descent with step size \u03b1. Bootstrapping is a key characteristic of TD methods, where the update at each step uses the prediction at the next step as part of its target. When using function approximation in TD-learning, updates to the value of one state can affect nearby states that share features. This allows for generalization across states in large state spaces, but updating the value of the state visited on the next step may not always have the desired effect on the TD-error. When using function approximation in TD-learning, updates to the value of one state can affect nearby states that share features. However, updating the value of the state visited on the next step may not always have the desired effect on the TD-error. Previous works have addressed this indirectly, such as residual gradients and methods minimizing MSPBE. Residual gradients update the parameters of the next state in the opposite direction of the current state, splitting the error between the two states. MSPBE methods remove error components not in the span of the current state's features. When using function approximation in TD-learning, updates to the value of one state can affect nearby states that share features. BID5 propose a solution by projecting the error on the tangent plane to the function at the point of evaluation. Instead of modifying the objective, the update is constrained to minimize the change in values of the next state while minimizing the TD-error. The parameters are updated based on gradients to minimize the TD error and change the value the most in the next state. The parameters are updated based on gradients to minimize the TD error and change the value the most, ensuring minimal impact on the next state. This technique can be applied to improve behavior in residual gradients and Q-learning. Our method shows fast learning like TD and convergence similar to residual methods. Baird's Counterexample has 6 states and 7 parameters, with values calculated within each state. Comparisons with Q-learning and Residual Gradients are made, showing convergence differences. Applying gradient projection with TD error leads to convergence for Q-learning and residual gradients, but not to ideal values of 0. Our method constrains the gradient to prevent updates to certain weights, resulting in values that do not converge to the true values of 0 but also do not blow up. Residual gradients eventually converge to the ideal values of 0. The Gridworld domain used is a (10\u00d710) room with specific parameters set. A deep network with 2 hidden layers is used for approximating Q-values, with results being similar for any goal on the grid. Our method uses 2 hidden layers with 32 units each to approximate Q-values. We employ a softmax policy and calculate target values. The comparison of DQN and Constrained on the Cartpole Problem is shown in Figure 4. The agent's performance is cut off after averaging above 199 over 100 episodes. Implementation is based on OpenAI baselines for Deep Q-learning with a network of 2 hidden layers. In this paper, a constraint is introduced on the updates to the parameters for TD learning with function approximation. Constrained DQN, using RMSProp, learns faster with less variance compared to regular DQN. The constraint ensures targets in the Bellman equation do not move when updates are applied to the parameters. In this paper, a constraint is introduced on the updates to the parameters for TD learning with function approximation. The constraint ensures targets in the Bellman equation do not move when updates are applied to the parameters. This added constraint prevents parameters from exploding in Baird's counterexample when using TD-learning. Constrained DQN can learn faster and with less variance compared to regular DQN, as demonstrated on a Gridworld domain and the Cartpole domain. In this paper, a constraint is introduced on the updates to the parameters for TD learning with function approximation to prevent parameters from exploding. The approach shows less variance on the Cartpole domain and aims to scale to larger problems like the Atari domain. Future work includes proving convergence of TD-learning with this added constraint."
}