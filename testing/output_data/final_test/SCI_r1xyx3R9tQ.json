{
    "title": "r1xyx3R9tQ",
    "content": "Machine learning research has explored prototypes as examples representative of the behavior to be learned. Five methods for identifying prototypes were evaluated, showing different but meaningful interpretations. Human study confirmed the metrics' alignment with intuition. Disagreements in metrics offer insights into data and algorithms, impacting data-corpus construction, efficiency, interpretability, and more. The \"train on hard\" curriculum approach can enhance accuracy but is less effective with mislabeled or ambiguous examples. In machine learning research, prototypes are sought as representative examples for learning behavior. Different metrics are used to identify prototypes in training and test data, with recent work shedding light on their importance. Characteristics of prototypes and outliers are studied, aiming to define prototypicality properties effectively. The study evaluates prototypicality metrics based on adversarial robustness, retraining stability, ensemble agreement, and differentially-private learning. Distinct sets of prototypical and outlier examples are found in datasets like MNIST, Fashion-MNIST, CIFAR-10, and ImageNet. Predictive stability under retraining correlates with adversarial distance. The study evaluates prototypicality metrics based on adversarial robustness, retraining stability, ensemble agreement, and differentially-private learning. There is overall agreement on prototypes and outliers, with differences in metrics identifying uncommon submodes and misleading examples. Training models on prototypes leads to faster learning, interpretability, and adversarial robustness, while training on outliers can achieve higher accuracy. Desirable properties for prototypicality metrics include independence from the learning task. A desirable prototypicality metric should be applicable to all machine-learning tasks, independent of modeling approaches, aligned with human intuition, and cover all data modes evenly. A prototypicality metric for machine-learning tasks should provide coverage for imbalanced data modes, offer stable ranking, apply to both training and test data, and predict test accuracy based on prototypical examples. A prototypicality metric for machine-learning tasks should provide coverage for imbalanced data modes, offer stable ranking, apply to both training and test data, and predict test accuracy based on prototypical examples. Different metrics can provide valuable insights into model behavior and improve interpretability, even if they do not perfectly satisfy all properties. Various metrics for identifying prototypes may meet the desired criteria, including early methods based on concept similarity introduced by Zhang (1992). The Appendix contains a survey of related work on prototypicality metrics. Two metrics based on learning order and gradient magnitude were evaluated in experiments, but they showed high variance and low signal. These metrics did not predict test accuracy as desired. The Appendix discusses related work on prototypicality metrics, including two metrics that did not predict test accuracy as desired. Instead, five new metrics are defined and applied, ranking examples by their relative ordinal number based on adversarial robustness. This includes measuring the distance to the decision boundary using an adversarial example attack. The curr_chunk discusses an adversarial-example attack based on iterative gradient descent to find directions that increase the model's loss within an \u221e -norm ball. It introduces the concept of prototypicality and compares the adv metric based on the 2 norm with the \u221e norm. Additionally, it mentions Holdout Retraining as a metric to assess model performance on prototypical examples regardless of training data. The curr_chunk explains the process of training a model on a dataset, fine-tuning it on a holdout dataset, and computing the prototypicality of an example. The choice of metric for prototypicality is not crucial, with the paper using symmetric KL-divergence. The paper uses the symmetric KL-divergence for measuring prototypicality, which differs from previous work in important ways. The holdout retraining metric is conceptually simpler, more stable numerically, and computationally efficient. Ensemble Agreement (agr) is used to rank examples' prototypicality based on agreement within an ensemble of models trained on different subsets of the data. The paper introduces a method to measure examples' prototypicality using the symmetric KL-divergence between models' outputs. Privacy-preserving training ensures proper classification of well-represented examples but may reduce accuracy on rare cases due to attenuated gradients. The paper introduces a method to measure examples' prototypicality using differential privacy in training. Outliers are more impacted by noise and attenuation, while common signals are preserved in models trained with reasonable accuracy. The priv metric ranks examples based on their tolerance to noise, with higher tolerance indicating more prototypicality. The five metrics for evaluating prototypicality exhibit good coverage and stability in ranking across training and test data, providing a balanced view of distinct data modes even with data skew. Each metric avoids strictly ranking one class as more prototypical than others within the same class. The metrics for evaluating prototypicality show good coverage and stability in ranking across training and test data, providing a balanced view of distinct data modes even with data skew. The metrics are widely applicable and give consistent results despite changes in hyperparameters or model architecture. Strong predictive accuracy is observed when training on prototypes. Correlation coefficients between the metrics are shown in FIG0. The metrics for evaluating prototypicality provide good coverage and stability in ranking across training and test data, showing strong predictive accuracy when training on prototypes. The correlation coefficients between the metrics are overall strongly correlated, with some unexpected correlations between adversarial robustness and retraining distance. This new finding may be of independent interest and significance. The ret metric defined in Section 2.1 is easily computable for any ML model or task. A subjective visual inspection shows that the metrics rank examples in a way that aligns with human intuition, with clear distinctions between prototypes and outliers. An outlier \"9\" mislabeled as a three and an atypical dress-like \"shirt\" memorized during training were identified. Results of a human study on Mechanical Turk workers selecting best or worst examples from training data images show clear distinctions between prototypes and outliers in various datasets. The study validates how metrics correlate with human perception. In an online human study using Amazon's Mechanical Turk service, evaluators chose the most or least representative images for each output class in MNIST, Fashion-MNIST, and CIFAR-10 datasets. Over 100,000 images were assessed by 400 evaluators who picked either the best or worst images from a 3x3 grid of random images. Each evaluator focused on one output class, such as MNIST digits or CIFAR-10 cars. The study included \"Gold Standard\" questions to identify unreliable workers. Evaluators showed high agreement on the worst image but low agreement on the best image. Human evaluators correlated with metrics, preferring low-prototypicality images as worst and high-prototypicality as best. Notable differences were found between metrics and datasets in human perception. Further investigation is warranted due to discrepancies in human perception of prototypicality metrics. Visual correlation in a scatter plot can be informative, as shown in FIG3 (a) for MNIST models. Computing the Jaccard distance reveals substantial disagreement between metrics, as seen in the results at the 25% threshold for Fashion-MNIST in FIG3. The results at the 25% threshold for Fashion-MNIST show substantial disagreement between metrics. Examples that are prototypical in one metric but outliers in others are considered, combining adv and ret prototypes into boundary and ensemble metrics. Memorized exceptions, like the unusual \"shirt\" image, are analyzed by intersecting top prototypical images with bottom-half outliers. The set includes atypical \"shirt\" images, T-shirt-like, and pullover-like images, which can be misleading. The curr_chunk discusses the presence of misleading T-shirt-like and pullover-like images in Fashion-MNIST, labeled as memorized exceptions due to their ambiguity. It also mentions the use of priv metric for differential-private learning to prevent memorization of rare examples. This suggests the identification of uncommon submodes in learning. The curr_chunk discusses discovering uncommon submodes in learning tasks by intersecting outlier examples on the priv metric with top prototypes in boundary and ensemble metrics. It also introduces canonical prototypes as the intersection of top prototypical examples in all metrics, ensuring coverage without spurious examples. The examples in MNIST and CIFAR-10 demonstrate the effectiveness of these approaches. In Figures 4 and 5, the metrics' prototype and outlier sets are clustered using t-SNE and HDBSCAN BID8 for interpretability. Different data projection and clustering methods could offer insights into ML datasets. Prototype metrics can enhance sample complexity, accuracy, or robustness of models. In the context of enhancing sample complexity, accuracy, or robustness of models, two experiments were conducted on three datasets to compare training on outliers versus prototypes. The results varied across datasets, with training on least prototypical examples yielding highest accuracy on MNIST and training on nearly-the-most prototypical examples giving highest accuracy on CIFAR-10. The study compared training on outliers versus prototypes in three datasets. Training on least prototypical examples resulted in highest accuracy on MNIST, while training on nearly-the-most prototypical examples gave highest accuracy on CIFAR-10. Outliers in CIFAR-10 and Fashion-MNIST were found to be misleading or erroneously labeled. About 10% of outliers were memorized exceptions. Inserting 10% label noise decreased model accuracy by 10%. Removing erroneous and misleading outliers is crucial for achieving high accuracy on small training data. When the amount of training data is not fixed, the best strategy varies depending on the dataset. Training on the k-least prototypical examples is optimal for MNIST, while training on prototypical examples is better for Fashion-MNIST and CIFAR-10 when k is small. However, as the dataset size increases, training on outliers becomes more accurate, especially for Fashion-MNIST and CIFAR-10. Training only on the most prototypical examples yields high test accuracy. Prototype-training is better for Fashion-MNIST and CIFAR-10 with limited data, while outlier-training is superior with more data. Data augmentation and regularization techniques can help recover utility loss from training with more prototypical examples. Training on increasingly more prototypical examples improves model accuracy. Training on prototypes can lead to simpler decision boundaries and high accuracy on prototypical test examples, even with a small fraction of the most prototypical training data. This approach may result in models that are more robust to adversarial examples compared to training on outliers. Training on prototypes can lead to simpler decision boundaries and high accuracy on prototypical test examples, even with a small fraction of the most prototypical training data. Prior work has shown that discarding outliers from the training data can improve robustness to adversarial examples. Models trained on prototypical examples are more robust to adversarial attacks compared to those trained on outliers. This paper explores the impact of training models on fixed-sized subsets of data with varying prototypicality, showing that models trained on prototypical examples exhibit increased robustness. The study explores metrics for identifying prototypes and their utilization during training. Models trained on prototypes have simpler decision boundaries and are more adversarially robust, although training solely on prototypes may result in lower accuracy compared to outliers. Further research on metrics for identifying prototypes and methods for their use during training is deemed crucial for future work. The analysis discusses the utilization of prototypes in training models, highlighting various metrics and techniques for identifying prototypes. Different approaches such as selecting prototypes based on maximum mean discrepancy or using autoencoders to project data onto a reduced manifold are explored in the literature. These methods aim to simplify decision boundaries and enhance adversarial robustness in models. The curr_chunk discusses the use of prototypes in improving model interpretability without requiring architectural modifications. Different interpretability approaches are compared, such as using prototypes to summarize the dataset and explain decisions, or generating saliency maps to visualize neural network learning. These methods aim to enhance model interpretability and decision-making processes. Curriculum learning seeks to optimize the order in which training data is presented to improve model performance and address dataset limitations. It suggests presenting easy-to-classify samples early and gradually introducing complex samples as learning progresses. Curriculum learning involves optimizing the training data order to enhance model performance. Various methods, such as leveraging sample distance or predicting loss, are used to dynamically generate the curriculum during training. Training on easy samples is beneficial in noisy datasets, while training on hard examples is more effective in clean data. This contradicts self-paced learning and hard example mining approaches. Different strategies have been proposed, such as assigning weights to training examples based on alignment of logits and gradients, or training on points with high prediction variance or close to the decision threshold. The variance and average are estimated by analyzing a sliding window of prediction probabilities during training epochs. Prototypes and coresets offer a more compact way to describe datasets, with clustering algorithms utilizing both to handle high dimensionality. Coresets are defined based on a specific metric, while prototypes are independent of machine-learning aspects. Wang et al. (2018) took a different approach. In a different approach, Wang et al. (2018) presented training examples from MNIST, FashionMNIST, and CIFAR10, showcasing outlier and prototypical images grouped by class. MNIST results were obtained using a CNN with specific architecture and training parameters, while Fashion-MNIST used the same setup. CIFAR results were achieved with a ResNetv2 model trained on batches of 32 points using Adam optimizer. The CIFAR results were obtained using a ResNetv2 model trained on batches of 32 points with the Adam optimizer for 100 epochs. Data augmentation and training script were adapted from a specific source. Ensemble models were created with different random initializations when needed. Three matrices report the accuracy of models trained on MNIST, Fashion-MNIST, and CIFAR-10 datasets with varying degrees of prototypicality. The matrices show that training on outliers can lead to good performance on prototypes in MNIST, Fashion-MNIST, and CIFAR-10 datasets. The best performance is achieved by training on examples that are neither prototypical nor outliers. The matrices demonstrate that training on outliers can result in good performance on prototypes in MNIST, Fashion-MNIST, and CIFAR-10 datasets. The highest performance is attained by training on examples that are not prototypical or outliers. The current chunk contains numerical values and is under review as a conference paper at ICLR 2019."
}