{
    "title": "SJlp8sA5Y7",
    "content": "While deep neural networks have excelled in prediction tasks, they struggle with sequences of probability distributions. The Distribution Regression Network (DRN) addresses this by encoding distributions efficiently, but it lacks the ability to model time dependencies. To overcome this, the Recurrent Distribution Regression Network (RDRN) is proposed in this paper. The Recurrent Distribution Regression Network (RDRN) is introduced in this paper as a model that utilizes a recurrent architecture to capture time dependencies in distribution sequences. RDRN outperforms neural networks and DRN in prediction performance while maintaining a compact network structure. Deep neural networks, such as CNN and RNN, have achieved state-of-the-art results by designing architectures tailored to specific data types. CNN uses local filters and max pooling to extract features and reduce image representation size, while RNN's recurrent architecture with shared weights models time dependencies. The Recurrent Distribution Regression Network (RDRN) is a model designed to capture time dependencies in distribution sequences, addressing the challenge of representing sequences of probability distributions. Conventional neural networks struggle to compactly represent distributions, requiring decomposition into separate nodes. Unlike convolutional neural networks, fully-connected multilayer perceptrons fail to capture distribution data effectively. The Distribution Regression Network (DRN) BID8 has solved the problem of capturing image data effectively, using a novel representation of encoding distributions in a single node. DRN achieves superior performance with fewer parameters compared to MLP. However, it does not address the need to model time dependencies in distribution sequences. To tackle this, we propose the Recurrent Distribution Regression Network (RDRN), which represents distributions in hidden states, containing richer information with fewer weights. The Recurrent Distribution Regression Network (RDRN) captures time dependencies better than DRN, resulting in superior prediction performance by using compact distribution representations. RDRN's ability to model time-varying distributions has applications in various fields such as astrophysics, biological physics, animal population studies, and weather forecasting. Various machine learning methods have been proposed for distribution data, including the Triple-Basis Estimator (3BE) for function-to-function regression. The 3BE uses basis representations of functions and learns a mapping from Random Kitchen Sink basis features, showing improved accuracy and speed compared to instance-based learning methods. Improved accuracy and speed compared to instance-based learning method BID15, BID8 proposed the Distribution Regression Network (DRN) which encodes an entire distribution in a single node for better accuracies with fewer parameters. For predicting the future state of a time-varying distribution, BID9 introduced the Extrapolating the Distribution Dynamics (EDD) method using reproducing kernel Hilbert space (RKHS) embedding to model time dependencies in the distribution sequence. The Extrapolating the Distribution Dynamics (EDD) method models distribution evolution between time steps, showing effectiveness for some synthetic data variants but struggles with non-linear dynamics. EDD's limitation includes single-input regression and inability to learn from multiple trajectories. It aims to predict future distributions in a time-varying sequence, assuming univariate distributions and the possibility of multiple inputs at each time step. Our proposed Recurrent Distribution Regression Network (RDRN) addresses the limitations of existing models by combining compact distribution representations with modeling of time dependencies. Unlike recurrent neural networks that lack efficient distribution representation, and feedforward architectures like DRN that do not capture time dependencies, RDRN offers a solution by being a recurrent extension of DRN. This neural network model is designed to effectively handle distribution sequences by incorporating both compact representations and time dependencies. The Distribution Regression Network (DRN) proposed by BID8 is designed for distribution-to-distribution regression tasks, utilizing a unique approach where each network node encodes an entire distribution. Unlike conventional neural network architectures, DRN's forward propagation is tailored for propagating distributions, inspired by statistical physics. This novel network consists of multiple fully-connected layers similar to MLP, with each node representing a distribution and connections having real-valued weights. The Distribution Regression Network (DRN) encodes probability distributions at each node in the network, computed using incoming node distributions, weights, and bias parameters. The probability density function of each node is calculated by marginalizing over conditional probabilities and incoming probabilities. The network architecture includes fully-connected layers with real-valued weights and bias terms. The Recurrent Distribution Regression Network (RDRN) is introduced as a recurrent extension of the Distribution Regression Network (DRN). RDRN takes in a distribution sequence as input and predicts the distribution at a future time step. The network architecture includes multiple distributions in the hidden state at each time step. The hidden state at each time step in the Recurrent Distribution Regression Network consists of multiple distributions. The weights U and W are shared across time steps, while V represents the weights between the final hidden state and the output distribution. The bias parameters for the hidden state nodes are also shared. The hidden state distributions at t = 0 represent the 'memory' of past time steps and can be initialized with prior information. The propagation involves multiple distributions per time step in the data and hidden layers. The Recurrent Distribution Regression Network involves multiple hidden state distributions at each time step. The energy function includes weights for input-hidden and hidden-hidden connections. The hidden node distributions are computed based on the previous time step and current input data distribution. The Recurrent Distribution Regression Network involves hidden node distributions normalized at each time step. The output distribution is computed using weight vector V and bias parameters at the final time step. The cost function is measured by Jensen-Shannon divergence between label and output distributions, optimized through backpropagation. Parameter initialization follows a uniform distribution method. Integration is done numerically by partitioning the distribution into bins. Experiments were conducted on four datasets. The Recurrent Distribution Regression Network (RDRN) compares with DRN by concatenating input distributions for all time steps at the input layer. Benchmark methods include RNN, MLP, and 3BE. RNN and EDD take inputs sequentially over time, while DRN concatenates distributions for all time steps. DRN consists of fully connected layers with nodes encoding entire distributions, optimized using JS divergence. The Recurrent Distribution Regression Network (RDRN) compares with DRN by concatenating input distributions for all time steps at the input layer. RDRN is optimized using JS divergence. The RNN architecture discretizes the distribution into bins and uses separate input nodes. The final hidden state is transformed by the hidden-output weights and processed by a softmax layer to obtain the output distribution. The cost function is the mean squared error between the predicted and output distribution bins. The Multilayer Perceptron (MLP) input layer consists of input distributions for all time steps, each discretized into bins represented by separate nodes. The Multilayer Perceptron (MLP) has T \u00d7 q input nodes and consists of fully-connected layers with a final softmax layer, optimized with mean squared error. The Triple-Basis Estimator (3BE) represents distributions with sinusoidal basis coefficients and Random Kitchen Sink basis functions. Extrapolating the Distribution Dynamics (EDD) learns from a single trajectory of distribution and uses radial basis function kernel for RKHS embedding. The first experiment predicts output distribution from multiple time steps of past distributions with a Gaussian distribution varying sinusoidally in mean. The dataset consists of input distributions with a sinusoidally varying mean in the range [0.2, 0.8] and constant variance of 0.01. Predicting the next distribution requires multiple past time steps, with a history length of 3 time steps being optimal. The regression performance is measured using L2 loss, with lower values being better. 20 training data points were sufficient for RDRN and DRN. The regression accuracy of RDRN and DRN was evaluated using 20 training data points, with RDRN showing the best performance. The model successfully predicted the next distribution at t=4 by considering multiple time steps in history and inferring the direction of movement well. In contrast, neural network counterparts like RNN and MLP exhibited overfitting due to excessive use of nodes. The next experiment models heat flux at the sea surface as a time-varying one-dimensional distribution using the stochastic Ornstein-Uhlenbeck process. The diffusion and drift coefficients are determined from real data measurements. The process is described by a time-varying Gaussian distribution with specific parameters. The energy balance climate model uses diffusion and drift coefficients D = 0.0013, \u03b8 = 2.86. Initial distribution is a delta-function at position y. Sampling y \u2208 [0.02, 0.09] generates Gaussian distributions at different time points. Regression task predicts distribution at t 0 + 0.02 using data from previous time points. RDRN shows best regression accuracy, followed by DRN, MLP, and RNN. Time-dependent architectures perform better than feedforward. The recurrent models explicitly capture time dependencies in the architecture and outperform feedforward models, especially with more time steps. RDRN and DRN have significantly fewer model parameters compared to other methods, making them more compact. RDRN is useful for tracking distribution drift in image datasets. The next experiment uses the CarEvolution dataset BID20 for domain adaptation, consisting of 1086 images of cars manufactured from 1972 to 2013, split into intervals of 5 years each. The experiment involves creating data distributions from DeCAF(fc6) features of car images using kernel density estimation. Regression task aims to predict future feature distributions based on past distributions. Training set consists of 5 intervals, while test set has 1 interval. Performance is measured by negative log-likelihood of test samples. The regression results in Table 2a show that RDRN has the best prediction performance, followed by DRN. RNN struggled with optimization due to high input dimensions. EDD has the fewest parameters but may be too restrictive for the dataset. RDRN is effective for predicting price movements in stock markets, with correlations between indices providing a basis for future stock return predictions. The study found that previous day stock returns of Nikkei and Dow Jones are good predictors of FTSE return. Predicting the entire distribution of stock returns is more useful for portfolio selection. Regression task involves predicting future returns based on past distribution. RDRN architecture with 3 nodes per time step showed the best prediction performance. The study utilized 3 days of distribution returns and one layer of hidden states with 3 nodes per time step for forward prediction of 1 and 10 days ahead. RDRN and DRN outperformed other methods in regression results, with RDRN performing slightly below DRN for 1 day ahead prediction but better for 10 days ahead. Results were visualized by comparing predicted and labeled distributions in FIG4, showing correlation coefficients. Regression deteriorated for all methods in the 10 days ahead prediction. The regression performance of RDRN and DRN is the best for 10 days ahead prediction, with RDRN showing strength in predicting longer time steps. RNN exhibits regression to the mean, limiting output distributions around zero. Conventional neural networks lack suitable representations for probability distributions, posing challenges in learning from distribution sequences. The Distribution Regression Network (DRN) addresses the challenge of neural networks lacking suitable representations for distributions by encoding each node with a distribution, leading to improved accuracies. However, a second challenge is modeling time dependencies in distribution sequences, which both RNN and DRN address separately. The proposed Recurrent Distribution Regression Network (RDRN) extends DRN with a recurrent architecture, allowing for explicit distribution representation in each node and shared weights across time steps. RDRN outperforms RNN, DRN, and other methods in forward prediction on distribution sequences by incorporating distribution representation in each node and shared weights across time steps."
}