{
    "title": "rJgv9J3NKH",
    "content": "In this preliminary work, the generalization properties of infinite ensembles of infinitely-wide neural networks are studied. The model family allows tractable calculations for information-theoretic quantities. Analytical and empirical investigations are reported to find signals correlating with generalization. Research focuses on understanding deep neural networks' ability to generalize, viewing them through the lens of information theory. Recent papers have measured information-theoretic quantities in deterministic neural networks. In this study, the generalization properties of infinite ensembles of infinitely-wide neural networks are explored. The approach involves utilizing stochasticity in the choice of initial parameters to generate an ensemble of predictions. By characterizing the generalization properties of the ensemble, insights into the generalization of individual draws can be gained. The evolution of infinitely-wide neural networks is described by the neural tangent kernel (NTK), which is constant in time and can be computed based on the network's architecture. The output of these networks forms a conditional Gaussian distribution, with mean and covariance functions that can be calculated. This simple form allows for bounding various properties. For more details, refer to the appendix. The simple form of infinitely-wide neural networks allows for bounding various information-theoretic quantities, such as mutual information between the representation and targets, inputs, and training set. The Fisher information metric is constant and flat in parameter space, while the Euclidean distance the parameters move is small. The Fisher metric provides a finite distance for parameter movement, while the mutual information tends to infinity, making generalization bounds vacuous. The Gaussian Information Bottleneck characterizes the tradeoff between information in inputs and targets. Infinite neural networks were trained on Gaussian data to measure mutual information estimates, showing performance close to optimal. We trained a three-layer FC network with ReLU and Erf activation functions, analyzing test set loss based on initial weight variance. Networks underfit at high \u03c3 w, show signs of overfitting at lower variances, especially with Erf non-linearity. Training loss goes to zero for all networks. Performance on the information plane is shown in fig. 2, with x-axis representing complexity of learned representation and y-axis showing relevant information. Curves depict trajectories of the networks. The MI estimates are calculated for networks with different weight variances, showing trajectories of representation over time. As time approaches infinity, the MI between networks' output and input becomes finite and small. Despite individual networks having rich representations, the ensemble compresses the input strongly due to random initialization. Overfitting occurs at late times, especially for more complex representations. Optimal early stopping helps achieve a balance between prediction and compression. The models achieve a near optimal trade-off in prediction versus compression by varying the initial weight variance. The MNIST dataset is used for a binary regression task on the parity of the digit. Results show that both ReLU and Erf networks exhibit overfitting at different initial weight variances. The Erf network shows overfitting at higher representational complexities. The Erf network exhibits overfitting at higher representational complexities, challenging existing claims about information theory and generalization in deep neural networks. Infinite ensembles of infinitely-wide neural networks provide a model family for studying generalization. Despite their simplicity, they can achieve good generalization performance, laying the groundwork for further empirical and theoretical studies in this area. Understanding generalization in the NTK limit may shed light on generalization in deep neural networks. The NTK limit sheds light on generalization in deep neural networks by providing a closed form expression for the evolution of predictions over time. It is governed by the neural tangent kernel, which converges to a fixed value as the network width increases to infinity. Calculating the infinite-width kernel for wide classes of neural networks is tractable. The behavior of infinitely-wide neural networks trained with gradient flow and squared loss is a time-dependent affine transformation of their initial predictions. The distribution of outputs of the ensemble of networks at initialization is Gaussian, conditioned on its input, and remains Gaussian at all times. The NNGP kernel is another kernel used in neural networks. The NNGP kernel corresponds to the expected gram matrix of the outputs in a neural network. It can be computed tractably and is a function of the network architecture. In experiments with a jointly Gaussian dataset, optimal representation can be analytically solved. The mutual information between x and y can be determined using covariance calculations. The SVD decomposition allows us to compute information-theoretic quantities of interest in neural networks. Marginalizing out stochasticity in the output helps in computing the expected loss of the ensemble. The expected log loss includes contributions from square loss and covariance trace. The mutual information between the network's output and the input can be determined through these calculations. In neural networks, the SVD decomposition helps compute information-theoretic quantities. The mutual information between the network's output and targets can be approximated using variational lower bounds. Estimating the mutual information between the input and output conditioned on the dataset requires knowledge of the marginal distribution. Minibatch lower bound estimates are upper bounded by the log of the batch size. Variational upper bounds on the mutual information between network representations and the training dataset can also be estimated. In neural networks, the SVD decomposition helps compute information-theoretic quantities. The mutual information between the network's output and targets can be approximated using variational lower bounds. Estimating the mutual information between the input and output conditioned on the dataset requires knowledge of the marginal distribution. For infinitely wide networks, the trace of the Fisher information is the same as the trace of the NTK, which is a constant and does not evolve with time. The parameters of infinitely wide neural networks do not change significantly over the course of training, as indicated by the relative Frobenius norm. Instead of focusing on norm changes, the length of the parameters' path in parameter space can be analyzed using the Fisher information metric. This reparameterization-independent distance measure provides insight into the trajectory of parameters during training. The residual norm at initialization projected along \u0398e \u2212\u03c4 \u0398 remains positive and finite even as t \u2192 \u221e. By considering its expectation over the ensemble, we can use Jensen's inequality to bound the expectation of trajectory lengths. Simplifications arise from the Gaussian distribution of z 0 (X) \u223c N (0, K) at initialization, shedding light on the loss function over time on the Gaussian dataset."
}