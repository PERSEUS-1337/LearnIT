{
    "title": "Sy_MK3lAZ",
    "content": "In this paper, a parametrized deep Q-network (P-DQN) is proposed for a hybrid action space in the context of designing Game AI for King of Glory. The algorithm combines DQN and DDPG without approximation or relaxation, offering a more efficient and robust solution compared to existing approaches. The success of deep reinforcement learning (DRL) in games like Go has led to advancements in artificial intelligence (AI) for games. DRL has been applied in various games to achieve super-human performances. However, most existing DRL methods are limited to environments with either finite and discrete actions or continuous actions. In the context of advancements in artificial intelligence for games, deep reinforcement learning (DRL) has shown success in games like Go. However, existing DRL methods are typically limited to environments with either discrete or continuous actions. In a new approach, the reinforcement learning problem is considered with a discrete-continuous hybrid action space, where actions are defined hierarchically. This approach aims to apply existing DRL approaches on this hybrid action space. In a new approach, the reinforcement learning problem is considered with a discrete-continuous hybrid action space. Two ideas are proposed: approximating A by a finite discrete set or relaxing A into a continuous set. The continuous relaxation can increase the complexity of the action space and lead to over-parametrization. In this paper, a novel DRL framework called parametrized deep Q-network learning (P-DQN) is introduced to work on discrete-continuous hybrid action spaces without approximation or relaxation. The method extends the DQN algorithm to handle continuous parameters within actions by defining a deterministic function and an action-value function. The algorithm combines the advantages of both DQN and DDPG for improved performance. P-DQN algorithm is applied to King of Glory (KOG), a popular online MOBA game with over 200 million users. It outperforms BID8's method using DDPG for hybrid action spaces. Reinforcement learning environments are modeled as Markov decision processes (MDP). The text discusses the Markov Decision Process (MDP) framework, where the initial state probability distribution is denoted by p0, the reward function by r(s, a), and the discount factor by \u03b3. An agent interacts with the MDP by selecting actions and observing rewards and next states based on policies. The state-value and action-value functions are defined for policies, with the optimal functions denoted as V* and Q*. Reinforcement learning algorithms aim to find a policy that maximizes the expected total discounted reward. These algorithms can be categorized into value-based methods, which estimate Q* and output the greedy policy, and policy-based methods, which directly optimize the policy. Q-learning is based on the Bellman equation and updates the Q-function iteratively in the tabular setting. When the state space is too large to store all states, function approximation for Q* is used. Deep Q-Networks (DQN) approximates Q* with a neural network. DQN is trained with techniques like experience replay and stochastic gradient descent. Policy-based methods directly model the optimal policy using a distribution of states and actions executed according to a policy. The objective of policy-based methods is to find a policy that maximizes the expected reward. Stochastic policy gradient methods aim to maximize J(\u03c0 \u03b8) via gradient descent. The actor-critic methods combine value-based and policy-based perspectives to achieve superhuman performance in games like Go. Continuous Q-learning is used to achieve superhuman performance in games like Go by rewriting the action value function as Q(s, a) = V(s) + A(s, a), where V(s) is the state value function and A(s, a) is the advantage function. These functions are approximated by neural networks, and the action value function is updated in each iteration. The continuous Q-learning updates \u03b8 v and \u03b8 a using the least squares loss function in each iteration. Policy-based methods can be adapted to continuous action spaces with deterministic policies. The deterministic policy gradient theorem states that the DPG algorithm and DDPG algorithm are proposed. Deep reinforcement learning combines reinforcement learning with deep learning. Deep reinforcement learning, combined with recent advancements in deep learning, has led to a surge in research in the field. Various algorithms, such as DQN, Double DQN, dueling DQN, bootstrap DQN, asynchronous DQN, and averaged-DQN, have shown remarkable success in challenging applications. Policy-based methods like REINFORCE and actor-critic methods, including A3C, have also demonstrated state-of-the-art performance. The A3C algorithm has shown state-of-the-art performance on the Arcade Learning Environment benchmark. Other methods for deep reinforcement learning on continuous action spaces include the deterministic policy gradient algorithm and the DDPG algorithm. Policy optimization methods like natural gradient descent and trust region optimization have also been proposed. Additionally, recent work has focused on reinforcement learning with a structured action space, combining finite actions with continuous parameters. Recent advances in game AI have utilized deep reinforcement learning to build AI bots for various computer games, such as Atari Games, Texas Hold'em, and Doom. Notably, AlphaGo achieved super-human performance by defeating the human world champion Lee Sedol. Real-time strategy (RTS) games and MOBA games present more complex challenges due to their multi-agent nature and large state and action spaces. Various algorithms have been proposed to handle parametrized actions, including DDPG and frameworks for updating parameters alternately. The proposed framework introduces a hybrid discrete-continuous action space for multi-agent games with huge state and action spaces. Current research in this area is limited, with most work focusing on specific scenarios rather than full-fledged RTS or MOBA games. The framework considers a MDP with a parametrized action space consisting of discrete actions associated with continuous parameters. This hybrid approach allows for a combination of discrete and continuous components in the actions taken. The framework introduces a hybrid discrete-continuous action space for multi-agent games with a MDP. The action space consists of discrete actions with continuous parameters, allowing for a combination of both components in actions taken. The Bellman equation involves solving for the largest Q value based on the selected discrete action and associated continuous parameter. The computational complexity arises from taking the supremum over the continuous space Xk. The Bellman equation involves finding the largest Q value for a hybrid discrete-continuous action space in multi-agent games. A deep neural network is used to approximate the Q value, while a deterministic policy network approximates the continuous parameter. The proposed P-DQN differs from previous work in handling discrete-continuous hybrid action spaces. The key differences between BID8 and P-DQN are: BID8 parametrizes discrete action types as continuous values, turning the action space into a continuous one, while P-DQN is an off-policy method that can use historical data. The networks of P-DQN and DDPG in BID8 differ in how they select actions: P-DQN maximizes Q values for discrete actions, while DDPG chooses the discrete action with the largest continuous parameterization. The weights of the value network and deterministic policy network are updated using gradient descent to minimize the mean-squared Bellman error. The n-step target y t is defined for fixed n \u2265 1, with a least squares loss function for \u03c9. The loss function for \u03b8 is defined to minimize Q[s, k, x k (s; \u03b8); \u03c9]. The parameters are updated using gradient-based optimization methods, with gradients given by \u2207 x Q(s, k, x k ; \u03c9) and \u2207 \u03c9 Q(s, k, x k ; \u03c9). Stochastic gradient methods are used to update the parameters, aiming to minimize the loss function \u0398 t (\u03b8) when \u03c9 t is fixed. Online two-timescale update rules are employed to achieve this goal. The text discusses the use of a two-timescale update rule BID3 in fashion, updating network weights \u03c9 and \u03b8 with step sizes \u03b1t and \u03b2t respectively. It involves selecting actions, observing rewards, and updating parameters using stochastic gradient methods for optimization. The P-DQN algorithm with experienced replay is presented, requiring a distribution \u00b5 for exploration. Actions are sampled randomly from \u00b5 or taken greedily. Additive noise can be added for exploration, and experience replay is used to reduce sample dependencies. The P-DQN algorithm incorporates prioritized replay and asynchronous gradient descent for efficient training in a distributed framework. Local processes generate transition trajectories and compute gradients independently, which are then aggregated to update global parameters, reducing variance and improving algorithmic stability. Experience replay is not needed in this distributed setting. The asynchronous P-DQN algorithm in Algorithm 2 outlines the process for each local process to fetch parameters from the server, compute gradients, and update global parameters. RMSProp BID10 is used for network parameter updates, known for its stability. King of Glory is a MOBA game where two teams fight with hero units and towers guarding their bases. The asynchronous P-DQN algorithm in Algorithm 2 details the control of a powerful hero unit in the MOBA game King of Glory. It involves exploration parameters, action space distribution, and updating global parameters for gameplay. The goal in the MOBA game King of Glory is to destroy the opposing team's base by advancing levels, obtaining gold, purchasing equipment, and improving unique skills. Computer-controlled units spawn periodically to march towards the opposing base in three lanes. Players can kill units, destroy towers, and upgrade their heroes to enhance power. If a hero is killed, it will wait to respawn. In the game King of Glory, players aim to destroy the enemy base by advancing levels, obtaining gold, and improving skills. Each team can have one, three, or five players, with the five-versus-five mode requiring strategic collaboration. The solo mode, a one-versus-one match, focuses on control of a single hero in the middle lane. A typical solo game lasts 10 to 20 minutes, with players making quick decisions and various actions like attacking and purchasing items. The game poses challenges for reinforcement learning due to a large state space and different types of actions. In the solo mode of King of Glory, the game presents challenges for artificial intelligence due to its large state space, complex action space, undefined reward function, and real-time gameplay. The P-DQN algorithm was applied in experiments against the default AI hero Lu Ban. The game state is represented by a 179-dimensional feature vector manually constructed from the game engine output, including basic attributes of heroes and units. The first part discusses the basic attributes of heroes, units, and buildings in King of Glory, extracted directly from the game engine. The second part focuses on the relative positions of units and buildings, as well as the actions available to heroes. The study suggests that performance could be enhanced with more carefully engineered features. In King of Glory, heroes have unique skills such as throwing grenades, launching missiles, and calling airships. The goal is to destroy the opponent's base, but rewards are sparse and delayed, so manual design is used for training rewards. In King of Glory, heroes have unique skills like grenades and missiles to destroy the opponent's base. Manual design of rewards is used based on statistics like Gold difference and Health Point difference to encourage hero behavior. In King of Glory, heroes have unique skills like grenades and missiles to destroy the opponent's base. Manual design of rewards is based on statistics like Kill/Death ratio, Tower/Base HP difference, Tower Destroyed, and Winning Game to encourage hero behavior. In King of Glory, heroes have unique skills like grenades and missiles to destroy the opponent's base. Manual design of rewards is based on statistics like Kill/Death ratio, Tower/Base HP difference, Tower Destroyed, and Winning Game to encourage hero behavior. The overall reward in the game is calculated as a weighted sum of time-differentiated statistics, with coefficients set roughly inversely proportional to the scale of each statistic. The algorithm is not very sensitive to changes in these coefficients within a reasonable range. Default parameters of skills provided by the game environment are used in experiments, with simplifications not affecting the overall performance of the agent. In King of Glory, heroes have unique skills like grenades and missiles to destroy the opponent's base. Manual design of rewards is based on statistics like Kill/Death ratio, Tower/Base HP difference, Tower Destroyed, and Winning Game to encourage hero behavior. The overall reward in the game is calculated as a weighted sum of time-differentiated statistics, with coefficients set roughly inversely proportional to the scale of each statistic. Default parameters of skills provided by the game environment are used in experiments, with simplifications not affecting the overall performance of the agent. To deal with movement direction and action usability issues, a normalized two-dimensional vector is used, and multi-step targets are calculated. The network structure includes a feature vector of 179 dimensions, with value and policy networks as multi-layer fully-connected deep networks with 256-128-64 nodes in each hidden layer and Relu activation function. During training and testing, frame skipping parameter is set to 2, actions are taken every 3 frames (0.2 seconds), t max is set to 20 (4 seconds) to alleviate delayed reward. Exploration is encouraged using -greedy sampling with = 0.255. P-DQN algorithm learns faster than its precedent work in the setting. Performance of P-DQN and DDPG are compared based on episode lengths and reward sums. The performance of P-DQN and DDPG algorithms were compared in a learning environment. Actions were sampled with specific probabilities, and parameters were drawn from uniform distributions. Infeasible actions were replaced with a greedy policy. The training used 48 parallel workers with a learning rate of 0.001, while validation used 1 worker with deterministic sampling. Each algorithm ran for 15 million steps. The DDPG BID8 algorithm was implemented for comparison. The P-DQN algorithm outperformed the DDPG algorithm in learning speed and performance. P-DQN learned the value and policy networks faster, with consistent increases in game length and total rewards. P-DQN explicitly computes maximization over actions, making it suitable for hybrid action spaces. The deterministic policy network in P-DQN extends the algorithm to handle discrete and continuous action types efficiently. The P-DQN algorithm extends the classical DQN to handle hybrid action spaces of discrete and continuous types, making it more suitable for realistic scenarios. Empirical experiments show its efficiency in training AI for popular games like King of Glory."
}