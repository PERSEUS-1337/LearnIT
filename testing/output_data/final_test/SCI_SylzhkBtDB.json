{
    "title": "SylzhkBtDB",
    "content": "We investigate multi-task learning with a shared feature representation for all tasks, studying the impact of task data alignment on performance. Aligning tasks' embedding layers leads to performance gains in multi-task training and transfer learning, with a 2.35% GLUE score average improvement over BERT LARGE. Our study focuses on multi-task learning with a shared feature representation, emphasizing the importance of task data alignment for performance improvement. We introduce an SVD-based task re-weighting scheme that enhances the robustness of multi-task training on a multi-label image dataset. Multi-task learning leverages supervised data from related tasks to reduce the need for massive per-task training datasets, leading to more efficient learning across multiple tasks. While some cases show significant improvements compared to single-task learning, task interference can sometimes lead to decreased performance in certain tasks. In this work, the focus is on investigating task interference in multi-task learning and developing methods to improve training effectiveness. Previous studies have shown that multi-task learning is effective for similar tasks, but less is known for neural networks. The lack of analytic tools makes predicting task interference challenging. The work explores task interference in multi-task learning, utilizing alternating minimization and l2 regularization to learn shared and separate task parameters. It investigates an architecture with a shared module for all tasks and separate output modules. Task data similarity is noted to have a second-order effect after controlling for model similarity. The study explores task interference in multi-task learning, emphasizing the impact of task data similarity on model performance. By analyzing the similarities of task data and models separately, the interference of tasks can be more precisely attributed. The research delves into the theory of multi-task learning through a shared module in linear and ReLU-activated settings, focusing on the capacity of the shared module, task covariance, and the per-task weight of the training procedure. The study delves into task interference in multi-task learning, focusing on the capacity of the shared module, task covariance, and per-task weights. It introduces the concept of task covariance to measure data alignment and explores positive and negative transfers between tasks. The research also proposes conditions for positive task transfer and discusses algorithms for aligning task embedding layers. Our method aligns covariances of task embedding layers to improve performance on GLUE benchmark tasks with BERT LARGE model. It also enhances transfer learning between sentiment analysis tasks using LSTM model and implements an SVD-based task reweighting scheme for multi-task training, showing significant improvements on ChestX-ray14 image classification dataset. In multi-task learning models, three key components - model capacity, task covariance, and optimization scheme - determine the effectiveness compared to single-task learning. Evaluations show a 5.6 AUC score improvement for all tasks with an unweighted scheme. Theoretical insights are applicable to various settings and applications. In multi-task learning, tasks share input dimension and can have different output labels. The model includes a shared module B and separate output modules A i for each task. The objective is to minimize a loss function over B and A i 's. The activation function g is applied to X i B. The curr_chunk discusses re-weighting tasks during training in a multi-task learning setup with a shared module B and task-specific modules A i. It focuses on two models: a single-task linear model and a single-task ReLU model with different output label representations. The ReLU activation function is applied on X\u03b8 entrywise. Positive vs. negative transfer between tasks is analyzed based on model capacity, task covariances, and per-task weights. Regression tasks under squared loss are focused on, with synthetic experiments on classification tasks for validation.notations include the column span of a matrix X and its pseudoinverse X \u2020. The text discusses the importance of model capacity in multi-task learning. It suggests that the output dimension of the model should be smaller than the sum of capacities of the modules. If the output dimension is greater than the number of tasks, there will be no transfer between tasks. The text also presents an optimal solution for each task to minimize error, but highlights that this approach may hinder generalization, especially for tasks with limited data. The text discusses the importance of model capacity in multi-task learning, highlighting the potential overfitting of single-task learning compared to multi-task learning. It presents a proof of Proposition 1 and its extension to ReLU settings. The performance improvement of a target task with MTL vs. STL is shown in Figure 3, indicating positive transfer when the source task has the same covariance matrix and negative to positive transfer when the covariance differs. Task data similarity is quantified using regression tasks under a linear model without noise. The shared module's capacity is limited to enforce information transfer, with the shared module now being a d-dimensional vector. The text discusses the importance of model capacity in multi-task learning, highlighting the potential overfitting of single-task learning compared to multi-task learning. It presents a proof of Proposition 1 and its extension to ReLU settings. The shared module B is now a d-dimensional vector, and A 1 , A 2 are both scalars. Task similarity is quantified using regression tasks under a linear model without noise. The geometry between task alignment is formalized using the covariance matrices of X 1 and X 2. In the context of multi-task learning, the text explores the effects of task covariances and model cosine similarity on transfer between tasks. By analyzing the angle or cosine similarity between optimal solutions, positive and negative transfers can be observed. Varying task covariances and sample sizes can impact the type of transfer between tasks. The study examines transfer between tasks in multi-task learning by analyzing task covariances and model cosine similarity. Three tasks are used to measure transfer from source to target, with data generation involving random Gaussian matrices and orthogonal matrices. Task 1 involves a target, Task 2 is a source task for the red line, and Task 3 is a source task for the green line. The study analyzes transfer between tasks in multi-task learning by examining task covariances and model cosine similarity. Task 1 and Task 3 have different covariance matrices, indicating signals lying in different subspaces. The effect of transferring to the target task is small unless the source task has significantly more samples to estimate certain parameters. Logistic regression and ReLU-activated regression tasks show similar results. The task involves task embedding layers and alignment matrices for optimization. The study examines transfer between tasks in multi-task learning by analyzing task covariances and model cosine similarity. It quantifies the amount of data needed for positive transfer and shows that it depends on the condition numbers of tasks' covariances. Theorem 2 states conditions for positive transfer in linear regression tasks with parameters and samples. The study analyzes transfer between tasks in multi-task learning by examining task covariances and model cosine similarity. Theorem 2 quantifies the data required for positive transfer in linear regression tasks with parameters and samples, showing dependency on the condition numbers of tasks' covariances. The results for task 2 plateau when the sample size becomes large enough. The ReLU model analysis involves resolving challenges with the ReLU function under distributional input assumptions. The study by Du et al. (2017) discusses a covariance alignment method for multi-task training, introducing an alignment matrix before input passes through a shared module. They propose a covariance similarity score to measure task similarity and consider re-weighting tasks for optimization. In the study by Du et al. (2017), a covariance alignment method for multi-task training is discussed, introducing an alignment matrix before input passes through a shared module. They propose a covariance similarity score to measure task similarity and consider re-weighting tasks for optimization. This part focuses on the re-weighting of tasks to improve robustness in multi-task training, particularly in the presence of label noise. The approach involves up weighting certain tasks to reduce noise and enhance performance, with a motivating example provided. The text discusses an SVD-based task reweighting scheme for multi-task training to improve robustness in the presence of label noise. It introduces a method to derive the optimal solution in a linear model and extends the results to show that all local minima are global minima in the linear setting. The approach involves finding the best rank-r approximation and providing a rigorous proof based on Proposition 3. The text introduces a re-weighting scheme using SVD for multi-task training to address label noise. It computes task weights based on the SVD of label vectors, aiming to remove noise. Theoretical results are connected to practical problems, showing benefits of shared MTL module with smaller capacity than single-task models. Our proposed covariance alignment method enhances multi-task training on various settings, including the GLUE benchmarks and sentiment analysis tasks. The SVD-based reweighed scheme proves more robust than the standard unweighted scheme for multi-label image classification tasks with label noise. The datasets used include GLUE for natural language understanding and BERT LARGE as the model, along with six sentiment analysis tasks. The curr_chunk discusses various sentiment analysis tasks, including movie review sentiment, customer reviews polarity, and question type categorization. It also mentions the use of different models like LSTM for sentiment analysis and CheXNet for ChestX-ray14 classification. The GloVe embeddings and a shared module approach are utilized for all tasks. In multi-task training, Algorithm 1 is compared by training with and without an alignment procedure on task embedding layers. For transfer learning, an STL model is first trained on the source task and then fine-tuned on the target task with an added alignment module. In reweighted schemes, per-task weights are computed and used to reweight the loss function. In Algorithm 2, the loss function is reweighted using Gaussian likelihood for classification outputs. Comparison is made with reweighting techniques from Kendall et al. (2018) and unweighted loss as a baseline. Performance is measured on the GLUE benchmark using the GLUE score, including accuracy and correlation scores. Sentiment analysis tasks are evaluated based on sentiment prediction accuracy, while image classification tasks are assessed using the area under the curve (AUC) score. Results are averaged over five random seeds, and MTL experiment results are averaged over all tasks. Training procedures and setup details are provided elsewhere. Our method outperforms BERT LARGE by 2.35% average GLUE score for five tasks in multi-task training. It also outperforms BERT LARGE on 7 of the 10 task pairs in the particular setting of training two tasks. Additionally, our method is useful for transfer learning. Our method improves accuracy on multiple tasks by up to 2.5% through reweighting training for the same task covariates. Evaluation on the ChestX-ray14 dataset shows a 1.3% AUC score improvement over previous techniques. Model capacity analysis confirms that MTL model capacity should not exceed the total capacities of the STL model. In the sentiment analysis tasks, the optimal capacity for the shared module in an MTL model is found to be 100, which is smaller than the total capacities of all STL models. Confining the shared module's capacity is crucial for achieving peak performance. Task covariance similarity scores are used to study covariance alignment, showing that aligning covariances improves performance. Additional results on CNN/MLP are provided in Appendix C.5. After studying task covariance similarity scores, it was found that aligning covariances improves performance in sentiment analysis tasks. The results confirmed increased accuracy on 13 of 15 task pairs by up to 4.1% and an increase in similarity scores for all 15 task pairs. The optimization scheme was verified by testing the method on the ChestX-ray14 dataset, showing an improvement over the unweighted scheme by an average 2.4% AUC score. Our method improves over the unweighted scheme by an average 2.4% AUC score and techniques of Kendall et al. (2018) by an average 0.5% AUC score in the study of multi-task learning theory in linear and ReLU-activated settings. The work includes extensive synthetic and real-world experiments to verify the theory and practical implications, opening up future questions about extending guarantees to non-linear settings. The study explores optimization schemes for multi-task learning in neural networks, including hard parameter sharing vs soft parameter sharing architectures. The authors hope to inspire further research in understanding and improving multi-task learning. The study delves into optimization schemes for multi-task learning in neural networks, focusing on hard vs soft parameter sharing architectures. It also discusses domain adaptation and provides rigorous arguments and proofs in various sections. The text discusses singular values, condition numbers, norms, inner products, and the sine function in the context of neural network optimization for multi-task learning. It emphasizes the importance of the quality of the subspace in determining the shared module's capacity in comparison to single-task models. The quality of subspace B in equation 1 determines multi-task learning performance. In the linear setting, the optimal B is a rotation matrix containing {\u03b8 i } k i=1 in its column subspace. This allows for optimal selection of vectors within the column span of g B (X i ) to minimize L(v, y i ). In multi-task learning, optimizing an MTL model reduces to optimizing over the span of subspace B. This applies to linear regression, linear classification, and mixtures of regression and classification tasks. In the ReLU setting, if the shared module's capacity is larger than the total capacities of the single task models, all parameters can be placed in the shared module. Each final output layer A i can then select the optimal parameter for the i-th task without transfer between tasks through the shared module. In multi-task learning, optimizing an MTL model involves varying the cosine similarity between single task models. The objective is to find the best rank-r approximation subspace of a matrix C. This is achieved by maximizing over a subspace C \u2286 R d\u00d7r. The maximum is attained at the best rank-r approximation subspace. In multi-task learning, the optimal solutions for each task are orthogonal to each other, with minimum error. The MTL error is minimized by finding the best rank-r approximation subspace. Upper bounds on MTL solutions quality depend on task relatedness. A procedure for orthogonalizing task matrices is outlined. The procedure for orthogonalizing task matrices involves finding the optimal MTL solution with capacity r. Theorem 2 states conditions for linear regression tasks with bounded l2-norm and error margin. The optimal MTL solution is determined with a desired error margin. Theorem 2 provides conditions for positive transfers in Multi-Task Learning (MTL) when source and target models are close and the number of source samples is large. The error bound decreases with parameter c, but more data points are required. The assumption is that c is at most 1/3, which is related to label noise in task 2. The proof of Theorem 2 in Multi-Task Learning involves showing the angle between vectors B and \u03b8 1 is small, leading to a bound on the angle between B and \u03b8 2. The distance between vectors B A 2 and \u03b8 2 is bounded, considering the estimation error and signal to noise ratio of task two. This proof relies on the geometric fact that X has full column rank with condition number \u03ba. The proof involves bounding the angle between vectors B and \u03b8 2 in Multi-Task Learning, utilizing the full column rank property of X with condition number \u03ba. The angle is constrained by a lemma, ensuring the distance between B and \u03b8 2 is limited based on the estimation error and signal to noise ratio of task two. In the MTL model, the proof of Theorem 2 involves bounding the distance between vectors B and \u03b8 2 by utilizing the full column rank property of X with condition number \u03ba. The angle is constrained by a lemma, ensuring the distance is limited based on estimation error and signal to noise ratio of task two. The third term is bounded by 2 X 1 \u03b8 1 \u03c3 1 log 1 \u03b4 with probability 1 \u2212 \u03b4, leading to \u03ba 2 (X 1 ) \u2264 3\u03ba(\u03a3 1 ) and X 1 \u03b8 1 2 \u2265 m 1 \u00b7 \u03b8 1 \u03a3 1 \u03b8 1 /2 \u2265 m 1 /2. In the MTL model, Theorem 2 involves bounding the distance between vectors B and \u03b8 2 using the full column rank property of X with condition number \u03ba. The estimation error of BA 2 is obtained by extending Theorem 2 to the ReLU model, assuming task 1's input X 1 follows a Gaussian distribution. Theorem 9 formalizes the result for two tasks with X 1 drawn from a standard Gaussian distribution. Making distributional assumptions is necessary due to the complexity of optimizing a single ReLU function under worst-case inputs. The text discusses the generation of y_i using the ReLU model with specific conditions. It also mentions the optimal MTL solution and provides a proof with probability considerations. The proof involves bounding the angle between vectors B and \u03b8_1, utilizing the Hoeffding bound for ReLU(X_1\u03b8_1), and employing an epsilon-net argument for ReLU(X_1B). The text discusses bounding the angle between vectors B and \u03b8_1, utilizing Bernstein's inequality and union bound, and showing concentration with probability considerations. It also involves finding a set of unit vectors S and estimating a 2. In this part, the proof of Proposition 3 is presented, showing that all local minima are global minima for the reweighted loss in the linear case. The key is reducing the MTL objective to low rank matrix approximation and applying recent results that show no spurious local minima. Lemma 10 states that under certain conditions, all local minima of the objective function are global minima. The proof presented in the current text chunk demonstrates that the optimal solution for the reweighted loss in the linear case is equal to (X X)^-1 XQr, showing that all local minima are also equal to this solution. This is achieved by reducing the MTL objective to low rank matrix approximation and leveraging recent results on spurious local minima. The current text chunk discusses the limitations of applying Lemma 10 and Proposition 3 to linear models only, excluding ReLU models. It mentions the need for further research on characterizing optimization landscapes in non-linear models. The text also outlines the content of various appendices detailing datasets, models, training procedures, and experiments conducted to support the claims made. For synthetic experiments, 10,000 random data samples with dimension d = 100 are drawn from a standard Gaussian. Data is split into training and validation sets. Labels are generated using sigmoid function for classification tasks and ReLU activation for regression tasks. The number of data samples varies. Sentiment analysis aims to understand sentiment opinions expressed. The sentiment analysis task involves understanding sentiment opinions expressed in text through multi-label classification. Six sentiment analysis benchmarks are used, including Movie review sentiment, Sentence subjectivity, Customer reviews polarity, and Question type datasets. The TREC dataset by Li and Roth (2002) classifies questions into 6 types. The MPQA dataset by Wiebe et al. (2005) detects opinion polarity. The SST dataset by Socher et al. (2013) is an extension of the MR dataset. GLUE benchmark includes tasks like question answering, sentiment analysis, text similarity, and textual entailment. Five tasks - CoLA, MRPC, QNLI, RTE, and SST-2 are selected for validation. The focus is on providing insights into multi-task learning rather than achieving state-of-the-art results. The ChestX-ray14 dataset contains 112,120 frontal-view X-ray images of 30,805 unique patients with 14 different thoracic pathology labels. It is used for multi-label image classification tasks and is representative in the medical imaging and computer vision domains. Synthetic experiments utilize linear regression and logistic models. For synthetic experiments, linear regression, logistic regression, and a one-layer neural network with ReLU activation are used. For sentiment analysis, models include MLP, LSTM, and CNN with specific architectures and pre-trained GLoVe embeddings. In experiments, pre-trained GLoVe embeddings from Wikipedia 2014 and Gigaword 5 are fine-tuned. Shared modules in multi-task learning include embedding and feature extraction layers. BERT model is used for GLUE benchmark tasks with added classification/regression layers. BERT LARGE is utilized as the shared module for multi-task learning. DenseNet model is used for ChestX-ray14 dataset tasks. In experiments, pre-trained GLoVe embeddings are fine-tuned. Shared modules in multi-task learning include embedding and feature extraction layers. BERT model is used for GLUE benchmark tasks with added classification/regression layers. BERT LARGE is utilized as the shared module for multi-task learning. DenseNet model is used for ChestX-ray14 dataset tasks. Training procedures involve mini-batch SGD with task data sampling and joint updates for tasks with the same features. Accuracy is used as the metric for classification tasks. For the sentiment analysis experiments, data is split into training, dev, and test sets. The model setup follows Lei et al. (2018) protocol with default hidden dimension set to 200. Synthetic experiments involve grid search over learning rate and epochs, with best results chosen. Regression task reports Spearman's correlation score, while classification task reports accuracy. In model capacity experiments, the default hidden dimension is 200, but varied. Accuracy score on test set is reported as performance metric. GLUE experiments involve training alignment and output modules. BERT LARGE module is fixed during training to study effect of adding alignment modules. Grid search is used to tune learning rate and epochs, with specific values chosen for experiments. For the ChestX-ray14 experiments, the model is fine-tuned for 20 epochs using the suggested configuration by Rajpurkar et al. (2017), and the AUC score is reported on the test set. The effect of cosine similarity is demonstrated in synthetic settings for regression and classification tasks, with varying task similarity between different datasets. Performance gap between MTL and STL models is compared for both regression and classification tasks. In synthetic experiments, the performance gap between MTL and STL models is compared for regression and classification tasks, showing that higher task similarity leads to better improvement in the target task. ReLU-activated models are also considered, with similar results observed. In synthetic experiments, two sets of model parameters are used for tasks 1 and 2, with task 2's parameters being a combination of task 1's parameters. Data points are generated from Gaussian, and labels are calculated using ReLU activations. Multi-Task Learning (MTL) with ReLU activations is used to co-train the tasks, aiming to analyze the impact of different levels of similarity between the tasks. The results show the relationship between data size, cosine similarity, and alignment of solutions. In synthetic experiments, Algorithm 1 corrects negative transfer in linear and ReLU regression tasks by aligning covariances and improving transfer rates. Algorithm 1 corrects negative transfer in tasks with limited data. Cross validation is used to choose model capacities for LSTM models in sentiment analysis tasks. Model capacities for CNN and MLP models are also verified using SST and MR datasets. The experiment evaluated the robustness of Algorithm 2 in the presence of label noise by adding noise to labels of tasks from the ChestXray14 dataset. Results showed that the MTL model capacity performed better than the STL model capacities on all models. The reweighting scheme in Algorithm 2 outperformed other techniques, improving AUC scores by 2.4% over unweighted training and 0.5% over Kendall et al. (2018) on 20 task pairs. The experiment evaluated Algorithm 2's robustness to label noise on tasks from the ChestXray14 dataset. MTL model capacity outperformed STL model capacities, with the reweighting scheme in Algorithm 2 showing the best performance, improving AUC scores by 2.4% over unweighted training and 0.5% over Kendall et al. (2018) on 20 task pairs. Figure 13 displays 5 example task pairs from the evaluation."
}