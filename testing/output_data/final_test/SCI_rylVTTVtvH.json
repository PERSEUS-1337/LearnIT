{
    "title": "rylVTTVtvH",
    "content": "In recent years, graph neural networks (GNNs) have been used for representation learning on various irregular domains like social networks, financial transactions, neuron connections, and natural language structures. However, existing GNNs struggle with dynamic graphs. This paper introduces a novel technique for learning embeddings of dynamic graphs using a tensor algebra framework. The method extends the graph convolutional network (GCN) with a tensor M-product technique, showing promising results in edge classification tasks on real datasets. The proposed method demonstrates usefulness for edge classification on dynamic graphs, utilizing graph neural networks (GNNs) and graph convolution filters. Various GNN architectures have been explored for graph-related tasks, operating on static graphs in the contemporary literature. In real-world applications, dynamic graphs are common, with changing connections and features over time. Existing GNN models mostly focus on static graphs, but efficient methods for handling time-varying graphs and capturing temporal correlations are lacking. The goal is to predict properties of future graphs in a sequence of graphs with fixed nodes, adjacency matrices, and graph feature matrices. Tensor-based approaches are used to process high-dimensional data effectively, with applications in edge classification, node classification, and edge/link prediction. Tensor based approaches have been successful in image and video processing applications. Various tensor based neural networks have been explored, including methods based on tensor decomposition, tensor-trains, and tensor factorized neural networks. A new tensor framework called the tensor M-product framework has been proposed to extend matrix based theory to high-dimensional architectures. In this paper, a novel tensor variant of the graph convolutional network architecture, called TensorGCN, is introduced to capture correlation over time. TensorGCN is a novel approach that leverages the tensor M-product framework to capture correlation over time in dynamic graphs. It computes embeddings for tasks like link prediction and edge classification, drawing parallels between TensorGCN and spectral filtering of tensors. Experimental results demonstrate its effectiveness for edge classification. The method presented focuses on edge classification on dynamic graphs using graph convolution based on spectral graph theory. Previous works like Chebnet and GCN have paved the way for efficient computation and improved performance in graph neural networks. However, existing methods for fixed/static graphs cannot be directly applied to dynamic settings. Seo et al. (2018) introduced the Graph Convolutional Recurrent Network to address this challenge. The Graph Convolutional Recurrent Network by Seo et al. (2018) is not suitable for dynamic graphs with changing edges. Wang et al. (2018) introduced EdgeConv for static graphs, not dynamic ones. Zhao et al. (2019) developed T-GCN for fixed graphs with varying features. Methods for learning embeddings of dynamic graphs combine GNNs and RNNs, like the approach in Manessi et al. (2019) using LSTM. Manessi et al. (2019) utilize LSTM in conjunction with GNNs for handling time variations in architectures designed for semi-supervised node classification and supervised graph classification. Pareja et al. (2019) introduce EvolveGCN, a variant of GCN incorporating GRU and LSTMs to address dynamic graphs, currently considered state-of-the-art. Newman et al. (2018) propose a tensor NN using the M-product tensor framework for high-dimensional data on regular grids, differing from dynamic graph data considered in this study. In this paper, tensors are introduced as three-dimensional arrays of real numbers denoted by boldface Euler script letters. The framework discussed relies on the M-product, a new definition of the product of two tensors. The M-product framework allows for generalizing classical numerical methods from linear algebra and has applications in neural networks, imaging, facial recognition, and tensor completion. It was originally developed for three-dimensional tensors but has been extended to handle tensors of higher dimensions. Definition 3.1 describes the M-transform operation. The M-transform operation, denoted by X \u00d7 3 M, applies a mixing matrix M to a tensor X elementwise, placing it in a transformed space. The M-product, denoted by X Y, involves two tensors X and Y multiplied by an invertible matrix M to yield a new tensor in the original formulation using the Discrete Fourier Transform matrix. The tensor M-product framework extends matrix concepts to tensors, including diagonality, identity, transpose, and orthogonality. Definitions 3.4-3.7 define the identity tensor, tensor transpose, and orthogonal tensor. These concepts enable a tensor eigendecomposition to be defined. The tensor eigendecomposition of a tensor X involves eigendecomposing each symmetric frontal slice. Inspired by GCN for static graphs, our approach utilizes a tensor model with a tensor activation function \u03c3 operating in the transformed space. The proposed dynamic graph embedding involves defining an activation function \u03c3 for a tensor A, and computing Y = A X W. For time-varying graphs, the matrix M is chosen to be lower triangular and banded to ensure each frontal slice only contains relevant adjacency matrices. The proposed method involves using a lower triangular and banded matrix M to ensure each frontal slice contains relevant adjacency matrices. The weight tensor W is chosen to be shared across layers to reduce over-parameterization and improve performance. This allows for the use of an embedding Y for various prediction tasks like link prediction and edge and node classification. The proposed method involves using a model for edge classification based on TensorGCN, which connects spectral convolution of tensors and spectral graph theory. This approach reduces computational cost and improves performance by applying tensor activation functions elementwise in the transformed domain. The proposed method involves using TensorGCN for edge classification, connecting spectral convolution of tensors and spectral graph theory. It defines a tensor Laplacian and introduces a tensor graph Fourier transform, analogous to matrix operations. The proposed method introduces a tensor graph Fourier transform, defining a convolution operation for tensors similar to spectral graph convolution. It utilizes a polynomial approximation to avoid eigendecomposition for filter function computation. The method introduces a tensor graph Fourier transform and utilizes a polynomial approximation to avoid eigendecomposition for filter function computation. The approximation involves tensor powers of L and can be done using tensor polynomial analogs of Chebyshev polynomials. The degree-one approximation by Kipf and Welling (2016) involves parameter choice for the embedding model, with results presented for edge classification on datasets including Bitcoin Alpha, OTC transactions, Reddit body hyperlinks, and chess results. Bitcoin datasets represent user transaction histories with labeled trust levels, while the Reddit dataset is based on hyperlinks between subreddits. The dataset is built from hyperlinks between subreddits and chess player matches. Each node represents a subreddit or player, and edges indicate interactions or matches labeled with -1 for hostile or black victory, 0 for a draw, and +1 for friendly or white victory. The data is divided into graphs based on time windows, with features computed for each node-time pair. The adjacency tensor A is constructed accordingly. The adjacency tensor A is divided into training, validation, and testing slices. Sparse adjacency matrices are handled by adding entries from one slice to the following slices. The datasets for bitcoin and Reddit are skewed, with most edges labeled positively. Negative instances are more important to identify, so the F1 score is used for evaluation. When evaluating experiments on datasets, the F1 score is used for skewed datasets like bitcoin and Reddit, where negative edges are crucial. In contrast, the chess dataset, with more balanced classes, uses accuracy for evaluation. Embeddings are trained using a specific formula, and a sliding window approach is used to compute embeddings for validation and testing data. Initial trials with 2-layer architectures did not significantly improve performance. The experiments focused on training with a 1-layer architecture using cross entropy loss function, with a weighted approach for skewed datasets like bitcoin and Reddit. Implementation was done in PyTorch with preprocessing in Matlab, using specific parameters for edge life, bandwidth, and output features. The impact of symmetrizing directed graphs was also investigated. In the experiments, the impact of symmetrizing directed graphs was investigated. The method was compared with three others, including variants of WD-GCN, EvolveGCN-H, and a baseline GCN. Our method outperforms other methods on various datasets in edge classification experiments using dynamic graph embedding. Symmetrizing adjacency matrices may lead to lower performance. Future research directions include further exploration of the tensor Mproduct framework. Future research directions for the method include developing theoretical guarantees, exploring optimal structure and learning of the transform matrix M, using the method for other prediction tasks, and investigating deeper architectures for dynamic graph learning. The M-transform in Definition 3.1 is illustrated through unfolding, multiplying by M, and folding back into a tensor. The Bitcoin Alpha dataset is available at https://snap.stanford.edu/data/soc-sign-bitcoin-alpha.html. The datasets for Bitcoin OTC, Reddit, and chess are available at specific URLs. When partitioning data into graphs, multiple data points for an edge are combined into one with the sum of their labels. Gradient descent is used for training with specific parameters. The weight vector \u03b1 in the loss function is treated as a hyperparameter in the bitcoin and Reddit experiments. Different \u03b1 0 values are tested to find the best model performance on validation data. The trained model's performance on testing data is reported in Tables 3 and 4. The chess experiment follows a similar process but with a single \u03b1 0 value. The chess experiment results are computed for a single vector \u03b1 = [1/3, 1/3, 1/3]. The M-product framework defines sets R 1\u00d71\u00d7T and R N \u00d71\u00d7T as playing roles similar to scalars and vectors in matrix algebra. Propositions D.1 and D.2 clarify the algebraic properties of the M-product, showing that R 1\u00d71\u00d7T forms a commutative ring with identity. The set R N \u00d71\u00d7T with product forms a free module over the ring R 1\u00d71\u00d7T, similar to a vector space. The lateral slices of Q in the tensor eigendecomposition serve as a basis for R N \u00d71\u00d7T. The lateral slices of Q are linearly independent in R N \u00d71\u00d7T."
}