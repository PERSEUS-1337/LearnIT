{
    "title": "BygIjTNtPr",
    "content": "In this work, the training dynamics of generative adversarial networks (GAN) using stochastic gradients are analyzed. The study focuses on the convergence of simultaneous gradient descent (simGD) and its variants under convex-concavity assumptions. The research shows that simGD converges with stochastic sub-gradients under strict convexity and introduces optimistic simGD and anchored simGD methods, demonstrating their convergence with different types of gradients. Despite the challenges in training GANs, the empirical success of these networks is acknowledged. The global training dynamics of GANs, despite their empirical success, remain a major open problem. While the local training dynamics are reasonably well understood, analyzing convergence assumptions for linear gradients and full gradients provides limited insight into global convergence. This work investigates the global convergence of simultaneous gradient descent (simGD) in GAN training. This work investigates global convergence of simultaneous gradient descent (simGD) and its variants for zero-sum games with a convex-concave cost using stochastic subgradients. Specifically studying convergence of the last iterates instead of the averaged iterates. Section 2 presents convergence of simGD with stochastic subgradients under strict convexity in the primal variable. Section 3 introduces a generalization of optimistic simGD with separate optimism and learning rates. Section 4 presents anchored simGD. Anchored simGD is a new method that demonstrates convergence with stochastic subgradients in training GANs. The approach is guided by continuous-time ODEs and incorporates optimism and anchoring as discretizations of regularized dynamics. The experimental results in Section 5 show the benefits of optimism and anchoring in certain setups for GAN training. In this work, the focus is on training the model effectively by introducing optimism as a modification to address cycling behavior in simGD. Prior works assume linear gradients and use full gradients, while the concept of optimism dates back to the 1980s and has been studied independently in the mathematical programming community. In this work, the focus is on analyzing the convergence of Polyak-averaged iterates in solving convex-concave saddle point problems using stochastic subgradients. The study highlights the use of last iterates or exponentially averaged iterates in GANs, which differ from classical Polyak averaging techniques and contribute to the empirical success of GANs. In analyzing the training of GANs, classical techniques like stochastic approximation, control theoretic techniques, variational inequalities, and continuous-time ODE analysis have been utilized. The focus is on convex-concave cost functions with a saddle point assumption, where the subdifferential with respect to x and u is defined. The goal is to find the saddle point where the cost function is convex-concave. The text discusses the convergence of Simultaneous Stochastic Sub-Gradient Descent (SSSGD) for strictly convex cost functions in the context of training GANs. The operator G is shown to be monotone, and the dynamics of the system are analyzed in both discrete and continuous time. The focus is on finding the saddle point of convex-concave cost functions. The text discusses the dynamics of a deterministic continuous-time system with a saddle point z. The LaSalle-Krasnovskii invariance principle is mentioned, showing the constant distance of z(t) to z. The text also refers to a canonical counter example and the Dirac-GAN. The text discusses the convergence of Stochastic Subgradient Descent (SSGD) towards a saddle point of a strictly convex or concave function. The proof utilizes stochastic approximation techniques and shows convergence from discrete-time to continuous-time trajectories. The discrete-time process converges to a continuous-time trajectory satisfying a differential inclusion. The LaSalle-Krasnovskii invariance principle is used to show that cluster points are solutions. Theorem 3.1 of (Mertikopoulos et al., 2019) proves convergence in a mirror descent setup under the assumption of \"strict coherence\". Optimistic Simultaneous Gradient Descent with a starting point z0, learning rate \u03b1, and optimism rate \u03b2 is discussed. Optimism is a modification that remedies cycling behavior, with SimGD-O converging where simGD diverges. In this section, a continuous-time interpretation of SimGD-O as a regularized dynamics is provided, showing convergence for the deterministic setup. The Moreau-Yosida regularization of G with parameter \u03b2 > 0 is discussed, where G \u03b2 is an approximation of G that is better-behaved. Specifically, G \u03b2 is \u03b2-cocoercive, making it a regularized dynamics. The regularized dynamics of G \u03b2 is \u03b2-cocoercive. Reparameterizing the dynamics leads to convergence with stochastic gradients in machine learning. SimGD-OS experiments show that the learning rate must diminish faster than the optimism rate for convergence. The choice of \u03b1 k and \u03b2 k must diminish separately for convergence in the dynamics of G \u03b2. The finite difference approximation \u03b1 is unreliable with stochastic gradients. It is unclear if observed convergence holds generally in the nonlinear convex-concave setup. The following section is compatible with stochastic subgradients. Prior work shows convergence of SimGD-OS under strict convex-concavity. Anchored Simultaneous Gradient Descent is proposed for convergence in both deterministic and stochastic setups. Continuous-time illustration and convergence rates are provided for SimGD-A. The method involves a starting point, anchor rate, and diminishing steps for convergence. Monotonicity and Young's inequality are used to derive the convergence rate. Anchoring leads to faster convergence rates compared to optimism in continuous time. The method is inspired by Halpern's and James-Stein estimator. Convergence results are presented using deterministic and stochastic gradients. The proof involves a discretization of continuous-time analysis. In Section 2, Anchored Simultaneous Stochastic SubGradient Descent is proposed without assuming differentiability of L. The main contribution is Theorem 4, showing last-iterate convergence for convex-concave cost functions using stochastic subgradients. Experimental results demonstrate the effectiveness of optimism and anchoring for training GANs on MNIST and CIFAR-10 datasets using PyTorch. The approach combines Adam with optimism and anchoring, outperforming the baseline Adam optimizer. The study compares the benefits of anchoring and optimism in GAN training on MNIST and CIFAR-10 datasets. Anchoring is beneficial for MNIST, while optimism is beneficial for CIFAR-10. The Anchored SSSGD method is proven to converge under the assumption of convex-concave cost functions. Further research is needed to compare the effects of optimism and anchoring in practical GAN training scenarios. Theorems 1, 2, 3, and 4 discuss different convergence notions in GAN training. The choices made were based on what could be proven through analysis. The discrete-time analysis of SimGD-O in Theorem 2 bounds the squared gradient norm of the best iterate. Theorem 3 establishes a rate close to O(1/k) on the squared gradient norm of the last iterate for SimGD-O. Parameter choices for SimGD-O and Corollary 1 are nearly optimal, with \u03b1 = 0.124897/R and \u03b2 = 1.94431\u03b1 providing a factor of 135.771. There is a discrepancy in the rate between continuous-time analysis and SimGD-A in Theorem 3. Theorem 3 shows a slower rate in discrete time analysis compared to continuous time analysis. Errors accumulate during discretization, affecting the rate. Simple tests reveal divergence when p < 1/2. SSSGD-A and Theorem 4 involve the parameter \u03b5, with a belief that \u03b5 > 0 is not necessary. Optimal choices for SSSGD-A are seen with \u03b5 = 0 and p = 2/3 in Figure 2. In stochastic convex minimization, p = 2/3 is optimal (Moulines & Bach, 2011; Taylor & Bach, 2019). Theorems 2, 3, and 4 extend to monotone operators (Ryu & Boyd, 2016; Bauschke & Combettes, 2017) without modification. Theorems establish strong and weak convergence in infinite dimensional setups for monotone operators. The LaSalle-Krasnovskii principle in Theorem 1 is specific to convex-concave saddle functions and does not extend to monotone operators. Monotonicity of a point-to-set mapping A on R d is defined using inner product notation. The inequality requires nonnegativity in the set. A maximal monotone operator has no other maximal monotone operator contained within it. Zer(G) is the set of saddle-points when G is maximal monotone. Young's inequality holds for any a, b \u2208 R m+n and \u03b5 > 0. Lemma 1 states the convergence of a martingale sequence. The text discusses the analysis of a theorem involving a convex-concave function with a saddle point. It utilizes the differential inclusion technique and references previous works by Duchi & Ruan (2018) and Davis et al. (2019). The proof involves nonnegative random sequences and a zero-mean random variable. The technique by Duchi & Ruan (2018) and Davis et al. (2019) shows convergence from discrete-time to continuous-time trajectory satisfying a differential inclusion. The proof involves adapting the LaSalle-Krasnovskii principle to show convergence to a saddle point and demonstrating the time-shifted interpolated discrete time process converges to a solution of the differential inclusion. The proof involves adapting the LaSalle-Krasnovskii principle to show convergence to a saddle point in a differential inclusion scenario. The time evolution operator maps the initial condition to the point at time t, with 1-Lipschitz continuity. The LaSalle-Krasnovskii invariance principle is applied to differential inclusions, showing convergence to a saddle point. The proof involves strict convexity and the existence of cluster points. The LaSalle-Krasnovskii invariance principle is applied to differential inclusions, showing convergence to a saddle point. By strict convexity, it is concluded that the conditions hold for s = 0, leading to the convergence of the discrete-time process to a solution of the continuous-time differential inclusion. The LaSalle-Krasnovskii invariance principle is applied to differential inclusions, showing convergence to a saddle point. By strict convexity, it is concluded that the conditions hold for s = 0, leading to the convergence of the discrete-time process to a solution of the continuous-time differential inclusion. The noisy discrete time process is close to the noiseless continuous time process, and both converge to the same limit. Conditions of Lemma 6 are verified, ensuring bounded iterates and stepsizes. Lemma 2 implies that z_k is a bounded sequence, leading to G(z_k) also being bounded. The conditions for convergence are met, ensuring that cluster points of z_interp are solutions of the original discrete-time process z_k. Lemma 7 states that under certain conditions, the interpolated process z_interp and the original process z_k share the same cluster points. Continuous convergence from discrete convergence is discussed, showing that the solution to the differential inclusion converges to a saddle point. Lemma 7 implies that z_interp and z_k share cluster points. Theorem 2 shows convergence of SimGD-O to a saddle point of L. Lemma 9, 10, and 11 provide proofs for certain inequalities and calculations. These results are essential for understanding the discrete analog of certain theorems. Lemma 13 and 14 discuss boundedness and convergence of nonnegative sequences under certain inequalities. The proofs involve recursion arguments and theorems related to convex-concave functions with saddle points. The text discusses the analysis of Theorem 4, which involves proving convergence of a sequence towards a solution using fixed points and convergence properties. The key insight is defining \u03b6 k as a fixed point and showing convergence towards it. Lemma 17 states that \u03b6 k slowly converges to a solution, leading to the conclusion that the sequence converges as well. The text provides hyperparameters for the MNIST and CIFAR-10 experiments, detailing the architectures of the generator and discriminator models, along with batch sizes and learning rates. For the CIFAR-10 experiment, hyperparameters include batch size = 64, Adam learning rate = 0.0001, Adam \u03b2 1 = 0.0, Adam \u03b2 2 = 0.9, max iteration = 100000, GAN objective = \"WGAN-GP\", Gradient penalty parameter \u03bb = 1, n dis = 1, Optimizer options = \"Adam\", \"Optimistic Adam\", or \"Anchored Adam\", with optimism rate \u03c1 = 1, anchor rate \u03b3 = 1, and anchor refresh period T = 10000."
}