{
    "title": "SygAlRLvoX",
    "content": "We propose a new anytime neural network that allows partial evaluation by subnetworks with different widths and depths, leading to higher resource utilization and improved performance under various resource budgets. This scheme is effective for image classification tasks on resource-constrained devices like mobile devices and autonomous vehicles with real-time latency requirements. Despite existing network compaction techniques, the challenge remains due to continuously changing resource availabilities. In this work, a new anytime neural network is proposed that allows partial evaluation by subnetworks with different widths and depths, improving performance under various resource budgets. The network is effective for image classification tasks on resource-constrained devices like mobile devices and autonomous vehicles with real-time latency requirements. The proposed doubly nested anytime neural network allows for more efficient utilization of multiple heterogeneous resources by slicing along width and depth, generating diverse sub-networks. This increased flexibility enables higher resource utilization in devices with dynamically changing resource budgets. Sub-networks can be formed along the depth by appending an output generation stage to the final convolution layer, avoiding performance degradation. The proposed channel-causal convolution approach allows for easy extraction of sub-networks along any direction by ensuring unidirectional data flow. This method addresses the issue of interdependence between nodes at different horizontal locations, enabling efficient utilization of resources in devices with changing budgets. Our network utilizes separate fully connected (FC) layers for each sub-network, with each FC layer taking only a portion of activations from preceding convolution layers to produce the final output. By sharing FC layers between sub-networks at the same depth, computational and memory costs are kept similar even with more possible output locations. Loss functions are obtained for each sub-network based on the number of possible partitions and classes. Experimental evaluation was conducted on CIFAR-10 and SVHN datasets, following a similar architecture to the ResNet-32 model. Our network architecture includes one convolution layer, 15 residual blocks, and fully-connected layers for output generation. It allows for 16\u00d722 sub-networks with varying widths and depths. Resource usage increases with deeper or wider sub-networks, impacting computational and memory requirements. This diversity in resource usage sets our scheme apart from conventional methods. The proposed architecture allows for nontrivial nesting of sub-networks along the width direction, outperforming vertical slicing schemes. Resource-constrained devices can benefit from the architectural diversity enriched by the anytime prediction scheme, with future works focusing on adding adaptive conditioning to improve performance."
}