{
    "title": "SJeNA6EtDB",
    "content": "Modern deep neural networks (DNNs) require high memory consumption and large computational loads. DNN compression algorithms, including factorization methods, aim to deploy DNN algorithms efficiently on edge or mobile devices. Previous works struggle to measure ranks of DNN layers during training, inducing low-rank through implicit approximations or costly SVD processes. This work proposes SVD training, decomposing DNN layers with SVD and training on full-rank decomposed weights to improve training quality and convergence. This work introduces SVD training for DNN compression, adding orthogonality regularization to singular vectors and applying sparsity-inducing regularizers on singular values to encourage low-rank. Empirical results show significant rank reduction and computation load savings compared to previous methods. Model compression techniques for deep neural network (DNN) models have been extensively studied to reduce memory consumption and computation load. Methods include element-wise pruning and structural pruning to address the challenge of deploying modern DNN models on resource-constrained platforms. Model compression techniques for DNN models include quantization, pruning, and factorization methods. Quantization and element-wise pruning reduce memory consumption but require specific hardware. Structural pruning removes redundant filters or channels, but complex DNN structures like ResNet or DenseNet may need additional adjustments for valid pruning. Factorization approximates weight matrices with low-rank matrices to reduce computation load. Weight matrices of a layer can be approximated by multiplying low-rank matrices, maintaining input/output dimensions. Previous methods have shown feasibility but may degrade performance. Some techniques manipulate filter directions to reduce rank implicitly, but face challenges in training and compression rates. The nuclear norm regularizer directly reduces the rank of weight matrices. Our work proposes SVD training to achieve a low-rank DNN network without applying SVD on every step. The weight matrix is decomposed into left-singular vectors, singular values, and right-singular vectors for training. Techniques like singular vector orthogonality regularization are used to induce low-rank while maintaining high performance. The proposed method uses Singular Value Decomposition (SVD) training to reduce the effective rank of a DNN without applying SVD at every step. It includes techniques like singular value sparsification and pruning to achieve low-rank models. Ablation studies show that the method outperforms state-of-the-art factorization and pruning methods on various tasks and model structures. The method proposed involves using Singular Value Decomposition (SVD) to reduce the rank of a DNN without applying SVD at every step. It includes techniques like singular value sparsification and pruning to achieve low-rank models, outperforming state-of-the-art methods on various tasks and model structures. Other methods like channel-wise decomposition and spatial-wise redundancy are also used to reduce computation and size of convolution layers. Methods have been proposed to reduce the rank of weight matrices during training process in order to achieve low-rank decomposition with low accuracy loss. Wen et al. (2017b) induce low rank by applying an \"attractive force\" regularizer to increase the correlation of different filters in a certain layer. Ding et al. (2019) achieve a similar goal by optimizing with \"centripetal SGD,\" which moves multiple filters towards a set of clustering centers. Xu et al. (2018) explicitly estimate and reduce the rank of weight matrices without performing actual low-rank decomposition during training. Xu et al. (2018) propose reducing rank by adding Nuclear Norm regularizer to training objective, requiring SVD computation on each layer. Tai et al. (2015) suggest training network directly in low-rank form to avoid decomposition complexity, addressing gradient issues. Adding batch normalization between decomposed layers can cause issues with theoretical guarantees and inference efficiency. Manually choosing a low rank for decomposition may not lead to optimal compression and can make optimization harder. This work combines decomposed training and trained low-rank methods to address these challenges. The model undergoes full-rank SVD training, followed by singular value pruning for rank reduction, and finetuning for accuracy recovery. Training is done in spatial-wise or channel-wise decomposition to avoid time-consuming SVD. Orthogonality regularization and sparsity-inducing regularizers are applied during SVD training. The method achieves optimal compression rate by inducing low-rank through training without additional computations. In this work, the neural network is trained in its low-rank decomposition form, where each layer is decomposed into two consecutive layers via SVD. For fully connected layers, the weight matrix can be decomposed into three variables U, V, and s. The method aims to achieve optimal compression rate without the need for additional operations. The work focuses on decomposing the convolution layer using channel-wise and spatial-wise decomposition methods. Channel-wise decomposition reshapes K into a 2-D matrix and decomposes it with SVD into U, V, and s. Spatial-wise decomposition reshapes K and decomposes it into U, V, and s. The decomposed layers from the convolution layer can replicate the original layer's function. Training the decomposed model with U, s, V variables achieves similar accuracy as the original model. The forward pass involves converting U, s, V into consecutive layers, and backpropagation is done directly with respect to these variables. This allows direct access to the singular value s during optimization. The decomposed layers from the convolution layer can replicate the original layer's function by accessing the singular value directly without performing SVD on each step. Orthogonality regularization is added to U and V to address gradient vanishing/exploding issues during optimization. Sparsity-inducing regularizers are added to induce rank reduction in each layer. The orthogonality property of U and V may not hold in the decomposed training process, unlike in standard SVD procedures. In order to enable effective rank reduction through singular value pruning, an orthogonality regularization loss is introduced to U and V. This regularization forces the singular vectors in U and V to be close to orthogonal matrices, preventing high energy differences that could impact the result. Additionally, the decomposed training process doubles the number of layers, potentially exacerbating issues of exploding or vanishing gradients. The orthogonality loss introduced in the training process helps mitigate exploding or vanishing gradient issues by keeping singular vectors close to orthogonal matrices. This regularization also aids in reducing the rank of the network, making singular value vectors sparse. This approach is inspired by DNN pruning techniques and utilizes a differentiable sparsity-inducing regularizer. Incorporating a differentiable sparsity-inducing regularizer, such as the L1 norm, helps make singular value vectors sparse and reduces the rank of the network. This regularization technique is inspired by DNN pruning methods and aids in optimizing the weight matrix. The L1 norm of s shrinks all singular values simultaneously, but pruning close-to-zero singular values may harm neural network performance. To address this, the Hoyer regularizer induces sparsity and outperforms other methods by using the ratio of L1 and L2 norms. This regularizer is differentiable and scale-invariant, making it easy to optimize in the objective function. The Hoyer regularizer retains energy in top singular values, making it effective in training. The overall objective function includes training loss, orthogonality loss, and sparsity-inducing regularization loss on singular values. Regularization loss is applied to the singular values of each layer using L1 and L2 regularizers. The decay parameters \u03bbs and \u03bbo control the sparsity and orthogonality loss respectively. The low-rank decomposed network is achieved through full-rank SVD training, singular value pruning, and low-rank finetuning. Training at full rank helps the decomposed model reach the performance of the original model without capacity loss. In the previous section, regularization loss is applied to singular values using L1 and L2 regularizers to achieve a low-rank decomposed network. The current section discusses the importance of singular vector orthogonality and the reduction of FLOPs through pruning and decomposition techniques. In this section, ablation studies are conducted to analyze the impact of singular vector orthogonality regularization and singular value sparsity regularizers on ResNet models using the CIFAR-10 dataset. The proposed decomposed training method is then applied to various DNN models on CIFAR-10 and ImageNet ILSVRC-2012 datasets, exploring the accuracy-FLOPs trade-off with different hyperparameters. Results consistently outperform previous works, highlighting the significance of incorporating singular value orthogonality loss in the training process. The experiments on ResNet-56 and ResNet-110 models with channel-wise and spatial-wise decomposition on CIFAR-10 dataset show that orthogonality loss improves accuracy. Training without this loss results in a 2% accuracy drop. Factors affecting compression rate and model performance include decomposition method and regularizers for singular values. Channel-wise and spatial-wise decomposition methods are considered with L1 and Hoyer regularizers. In this section, the accuracy-compression tradeoff of ResNet models is explored by comparing channel-wise and spatial-wise decomposition methods with different regularizers. Results show that spatial-wise decomposition has an advantage in shallower networks, but both methods perform similarly in deeper networks like ResNet-110. Spatial-wise and channel-wise decomposition methods perform similarly in deep networks like ResNet-110. Deeper layers in modern DNNs exhibit more channel-wise redundancy, making it a dominant factor. Hoyer regularizer achieves higher speedup with low accuracy loss compared to L1 regularizer. The tradeoff tendency of L1 regularizer shows a larger slope than Hoyer regularizer. Hoyer regularizer achieves a higher compression rate under low accuracy loss. The Hoyer regularizer achieves a higher compression rate under low accuracy loss compared to the L1 regularizer. The L1 regularizer, on the other hand, may perform better for extremely high compression rates with higher accuracy loss. The difference lies in how the regularizers handle singular values during training, with the L1 regularizer making all values small and the Hoyer regularizer focusing more energy on larger values. This results in the Hoyer regularizer being able to remove more singular values without significantly impacting performance, leading to higher compression rates at low accuracy loss. The proposed SVD training framework is applied to various ResNet models on different datasets to compare accuracy-FLOPs tradeoff with previous methods. Comparison is made with low-rank compression methods like TRP and C-SGD, as well as filter pruning methods like NISP, SFP, and CNN-FCF. Results show spatial-wise decomposition achieves higher compression rates than channel-wise decomposition in shallower networks. Our proposed SVD training framework achieves higher FLOPs reduction with less accuracy loss compared to previous methods on different models and datasets. The Hoyer regularizer for singular values sparsity is used for better compression rates, outperforming the L1 norm. Multiple strengths of the regularizer are explored to analyze the accuracy-FLOPs tradeoff. The proposed SVD training framework effectively compresses deep neural networks through low-rank decomposition. It incorporates full-rank decomposed training and singular value pruning to achieve low-rank DNNs with minor accuracy loss. Orthogonality regularizers maintain valid SVD form, while sparsity-inducing regularizers explicitly induce low-rank layers. Thorough experiments validate the effectiveness of these techniques. The orthogonality regularization on singular vector matrices is crucial for the performance of decomposed training. Spatial-wise decomposition outperforms channel-wise in shallower networks, while both perform similarly in deeper models. Hoyer regularizer achieves higher compression rates compared to L1 regularizer with low accuracy loss. Applying the method to ResNet models on CIFAR-10 and ImageNet datasets shows improved accuracy-FLOPs tradeoff above previous methods' Pareto frontier. This work offers an effective approach for learning low-rank deep neural networks. The experiments were conducted on the CIFAR-10 and ImageNet datasets using TorchVision. Data was normalized and augmented before training with specific batch sizes. Different training epochs and learning rates were used for full-rank SVD training and low-rank finetuning on both datasets. The finetuning process involved a decay in learning rate by 0.1 every 30 epochs, starting at 0.01 and decaying at epoch 30. Pretrained full-rank decomposed models were used to initialize SVD training. SGD optimizer with momentum 0.9 and weight decay values were used. Experiment accuracy was based on the best testing accuracy achieved during finetuning. Different decay parameters were set for orthogonality regularizer and sparsity-inducing regularizer during SVD training on CIFAR-10 and ImageNet datasets. The energy threshold and \u03bb values were adjusted in experiments to find the optimal accuracy-#FLOPs tradeoff. Results of different decomposition methods and regularizers on CIFAR-10 dataset are shown in Table 2 and visualized in Figure 2. Spatialwise decomposition with Hoyer regularizer for ResNet-20 and ResNet-32 is compared in Figure 3. The results of low-rank compression methods like TRP and C-SGD, as well as filter pruning methods like NISP, SFP, and CNN-FCF, are compared with our method for ResNet-50 on the ImageNet dataset. Previous works on compressing CIFAR-10 and ImageNet models are used as baselines for comparison. Only the most recent works are listed to show the state-of-the-art Pareto frontier."
}