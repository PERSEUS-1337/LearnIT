{
    "title": "BkeoaeHKDS",
    "content": "We propose a method for deep representation learning by utilizing gradient-based features in a linear model that combines gradient features and network activations. Our model provides a local linear approximation to deep models without computing actual gradients, showing strong results across various tasks and datasets. The recent development of representation learning offers promises for improving data efficiency in training deep neural networks. Features can be learned through unsupervised learning, self-supervised learning, or transfer learning, enabling easy adaptation to different tasks. These learned features can be used with simple classifiers for various tasks. In the context of representation learning, simple classifiers can be built for different tasks using features. To enhance performance, gradient-based features from a pre-trained network are proposed, incorporating task-specific discriminative information. A novel linear model is designed to combine gradient-based and activation-based features, inspired by recent advances in deep model analysis. This model offers a local approximation of fine-tuning for improved task performance. Our model offers a local approximation for fine-tuning deep models by utilizing gradient-based features. It is efficient and scalable for training linear models, with applications in visual representation learning, natural language processing, and speech recognition. Empirical evaluations show that our model outperforms traditional activation-based features across various tasks, datasets, and architectures. Our novel representation learning method, utilizing gradient-based features, outperforms traditional activation-based features significantly. The method provides a local approximation of fine-tuning deep models and demonstrates strong results across various tasks, datasets, and architectures. The approach is efficient, scalable, and well-aligned with theoretical insights. Representation learning using deep models has been explored recently, including deep latent variable models and self-supervised learning tasks like predicting rotation angles and solving jigsaw puzzles. Representation learning using deep models has been explored recently, including tasks like predicting rotation angles, solving jigsaw puzzles, and colorizing grayscale images. Transfer learning relies on sharing learned feature maps from a large dataset, with successful models pre-trained on ImageNet. The method discussed focuses on maximizing existing tasks rather than proposing new ones, offering a generic framework applicable to any representation learning paradigm. The approach utilizes the Jacobian matrix of a deep network for feature representation in downstream tasks. Our work draws inspiration from Fisher vectors (FVs) for various tasks such as visual recognition and meta-learning. FVs have shown success with deep models and task embeddings. Our method differs from traditional FV approaches. Our method differs from traditional Fisher vector approaches by not being built around a probabilistic model, allowing exact gradient computation for scalable training. Neural Tangent Kernel (NTK) connects deep networks with kernel methods, showing that networks evolve as linear models in the infinite width limit under certain losses and gradient descent. Our method is the first attempt to apply theory to practical deep neural networks, specifically in binary classification. We evaluate the impact of pre-training on linear approximation theory, using a feed-forward network with a backbone and linear model. The linear model reduces to a kernel machine with Neural Tangent Kernel (NTK), mapping feature vectors to output dimensions. Linear classifiers map feature vectors into output dimensions. The focus is on ConvNets for classification tasks, but the method can extend to other networks and tasks. Representation learning assumes a pre-trained f\u03b8 with learned weights, not using discriminative signals from the task. This can be an encoder of a generative model or a ConvNet learned from proxy tasks or large-scale datasets like ImageNet. The method proposed assumes a partition of \u03b8 (\u03b8 1 , \u03b8 2 ) parameterizing the bottom and top layers of the ConvNet f. It utilizes gradient-based features \u2207\u03b8 as the Jacobian matrix of f\u03b8 with respect to the pre-trained parameters \u03b8 2 from the top layers of f, for training linear classifiers. The model for concreteness classification includes linear classifiers w1 and shared linear weights w2 for gradient features. The model subsumes a traditional linear model and adds a second term linear in gradient-based features. The output is normalized by a softmax function and trained with cross-entropy loss using labeled data. Our method involves pre-training a ConvNet f\u03b8, training linear classifiers \u03c9 using f\u03b8(x), and learning a linear model \u011d w1,w2 (x) using gradient-based and activation-based features. The linear model provides a local linear approximation to F (x; \u03b8 2 , \u03c9) and can be interpreted as the 1st-order Taylor expansion of F \u03b8,\u03c9 w.r.t. its parameters (\u03b8 2 , \u03c9) around a specific point. The approach involves pre-training a ConvNet f\u03b8, training linear classifiers \u03c9, and learning a linear model \u011d w1,w2 (x) using gradient-based and activation-based features to provide a local linear approximation to F (x; \u03b8 2 , \u03c9). The quality of the linear approximation can be theoretically analyzed when the base network is sufficiently wide and at random initialization, drawing inspiration from recent studies on neural tangent kernel approach. The linear approximation is applied on pre-trained models of practical sizes, leveraging the stability properties of the network output. This approach is useful due to the strong starting point provided by the representation learning network, making the pre-trained network parameters close to a good solution for downstream tasks. The pre-trained base network exhibits stability properties, with similar predictions to a fine-tuned network for a significant portion of data in downstream tasks. The network width needed for linearization decreases as data structure increases. Practical datasets are well-structured, and a sufficiently wide trained network can maintain the approximation. Our method aims to reduce data complexity by utilizing the bottom layers and linearizing the top layers. It only requires learning a linear classifier, making it efficient for training and achieving better performance than activation-based features. The linear approximation works well when pre-training tasks are similar to target tasks, outperforming fine-tuning in some cases. However, a challenge lies in the scalable training of the method, as a naive approach can be computationally expensive and infeasible. Our method aims to reduce data complexity by linearizing the top layers, making training efficient. Inspired by Pearlmutter (1994), we design a scalable training scheme for our model, with the key being the inexpensive evaluation of the Jacobian-vector product. This allows for training complexity similar to a linear classifier on activation-based features. The approach involves defining a linear function for JVP computation in neural networks, incorporating nonlinearity with activation functions like ReLU, handling batch normalization, and efficiently computing JVPs on the fly. This method enables training efficiency by linearizing top layers and updating parameters through standard back-propagation. Our approach involves updating parameters w1 and w2 through back-propagation with a single forward pass and a single backward pass. The complexity analysis compares our method to fine-tuning network parameters \u03b82, showing that our method requires twice as many linear operations but is slightly cheaper due to the removal of the bias term. Our method requires storing an additional \"copy\" of model parameters compared to fine-tuning, but the increase in computing and memory cost is minor due to the small size of \u03b8 2. The experiments and results are organized into two parts, including an ablation study on the CIFAR-10 dataset and evaluation on three representation learning tasks. In our experiments, we focus on tasks like unsupervised learning with deep generative models, self-supervised learning with a pretext task, and transfer learning from large-scale datasets. We use specific settings for training and evaluation, including NTK parametrization for \u03b8 2, batch normalization, a batch size of 128, learning rate of 0.001, and Adam optimizer. No weight decay or dropout is utilized. Inference is done on a single center crop of the image for all datasets, with the \"baseline\" referring to a linear classifier trained on the last activation. In this study, the model design is probed through ablations on CIFAR-10 using a pretrained encoder ConvNet from BiGAN. The success of the method depends on pre-training all three parameters, as highlighted in Table 1. Pre-training plays a central role in the materialization of the results. Pre-training is crucial for model performance. Using pre-trained parameters improves results significantly. Transitioning from pre-training to training the proposed classifier is key. The optimal size of parameters affects model performance. Our method suggests setting the top layer as \u03b8 2 for improved performance. The model initialized with pre-trained parameters closely matches the fine-tuned network with moderate width gradient features. However, a performance gap emerges with narrower layers towards the bottom. Starting with pretrained \u03b8 2 and \u03c9 is the best practice for our method. Our method involves starting with pretrained \u03b8 2 and \u03c9 for improved performance. The size of \u03b8 2 affects performance, with a smaller size being sufficient. Results are presented for unsupervised learning, self-supervised learning, and transfer learning tasks, comparing against baseline linear classifiers. Our model is compared against baselines and a fine-tuned network to showcase the advantage of gradient-based features. BiGAN and VAE models are used for representation learning tasks, with training following specific architectures. The linear model is trained on CIFAR-10/100 and SVHN datasets, with classification accuracy reported on test splits. Our models consistently outperform the baseline across three datasets with a relative improvement of over 10%. Performance saturates with gradient-based features only from the top layer in the BiGAN case. Self-supervised learning results show good agreement with mean average precision scores using a ResNet-50 pre-trained on the Jigsaw pretext task. Our method utilizes gradient-based features from three residual blocks in the last stage, achieving significant performance improvements over the linear classifier baseline on VOC07 and COCO2014 datasets. The experiments are summarized in Table 3, showing a boost of over 5% in mean average precision scores. Our method outperforms self-supervised learning methods and fine-tuned networks by a large margin, attributing the difference to the semantic gap between tasks. Transfer learning results show significant performance improvements using ImageNet pre-trained models with gradient-based features. Transfer learning from ImageNet using pre-trained AlexNet was employed in the study. Two settings were tested: one with only convolutional layers and another including fully connected layers. Gradient features were computed from specific layers, and weight rescaling was applied. Results showed a notable improvement in performance on VOC07 and COCO2014 datasets compared to the baseline. Our method showed 2% (5%) and 5% (2%) improvements on VOC07 and COCO2014 datasets compared to the baseline. It slightly outperformed finetuned networks in the first setting, attributed to significant overlap between pre-training and target tasks. The novel method for deep representation learning involves exploring per-sample gradients of model parameters and constructing a linear model for efficient training and inference. Our method demonstrated significant improvements over the baseline in deep representation learning through gradient-based features. Ablation studies were conducted to determine the impact of pre-training and optimal size of gradient features. Comparison with two baselines showed the effectiveness of our linear model. The full model is compared against two baselines, the activation model and the gradient model. Fine-tuning is used for validation. A BiGAN is trained on CIFAR-10, and its encoder is used for classification. Results are presented in Table 5, with hyperparameter settings explained. The gradient model and full model achieve 23.09% and 62.83% accuracy respectively. Different scenarios involve parametrizing convolutions and computing gradient features with random or pre-trained networks. Activation features are computed using a pre-trained network to isolate the impact of the gradient feature on model performance. The study highlights the importance of pre-training for the representation power of the gradient feature in object classification. Results show that the full model consistently outperforms the gradient model, especially as the dataset and base network complexity increase. The study emphasizes the significance of pre-training for the representation power of the gradient feature in object classification. The full model consistently outperforms baselines and fine-tuning with different datasets and networks. Gradient from the topmost layer of a pre-trained network is sufficient for performance gain, while further inflating the gradient feature may not always be beneficial. Our model outperforms standard logistic regressor on network activation by a large margin in all scenarios, including the challenging VOC07 and COCO2014 datasets. Predictions are averaged over ten random crops at test time for these datasets, using ResNet-18 as the base network in transfer learning. The results show that our model is as good as or better than fine-tuning, even when the dataset has low variability. Our model performs better than standard logistic regressor on network activation across various datasets, including VOC07 and COCO2014. It is as effective as fine-tuning, especially in scenarios with low dataset complexity or significant overlap between pre-training and target tasks. Further exploration is needed to identify additional scenarios where our method excels in representation learning."
}