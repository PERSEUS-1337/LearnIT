{
    "title": "SJe2so0qF7",
    "content": "Users and utility providers are increasingly interested in ensuring data privacy. A proposed framework allows users to control which data characteristics they share and keep private without requiring changes to existing machine learning algorithms. The paper explores privacy-preserving representations and the trade-off between utility and privacy when disclosing sanitized data. Learning architectures are presented to approach this trade-off in a data-driven manner, with potential collaboration between users and utility providers in various use-case scenarios. The framework allows utility providers to collaborate with the sanitization process in various use-case scenarios. It focuses on space-preserving transformations for accommodating privacy requirements with a single set of utility algorithms. Three use cases are implemented: subject-within-subject, gender-and-subject, and emotion-and-gender, demonstrating the ability to hide certain attributes while preserving others. The new paradigm of collaborative privacy environment focuses on preserving emotion detection in devices by delaying data understanding until a visual or sound trigger is detected at the sensor level. This approach aims to address privacy concerns related to algorithmic or data augmentation and unpredictable correlations. Studies in the domain of differential privacy have shown that assumptions about the data or the adversary are necessary to balance privacy and utility. Recent data-driven privacy approaches have also explored minimizing privacy sacrifices for utility, but integration of collaborative elements is lacking. In a collaborative privacy environment, Wu et al. (2018) explored designing systems where users share sanitized data with service providers to perform non-sensitive tasks while keeping sensitive information hidden. They formalized this concept using privacy and transparency definitions, deriving an information-theoretic bound on privacy-preserving representations. This bound's metrics are used to learn representations directly from data without prior knowledge of the joint distribution of observed data and latent variables. The curr_chunk discusses a framework for accommodating user-specific privacy requirements without prior knowledge of the joint distribution of data. It can incorporate constraints on service provider's utility inference algorithms to satisfy multiple privacy constraints simultaneously. The framework is applied to tasks like hiding gender information from facial images while preserving subject verification and designing a sanitization function for subject identification on a subset of users. New applications include blocking a simpler task while preserving a harder one and blocking a device from constantly listening to out-of-sample data. The curr_chunk addresses theoretical foundations and algorithms for handling sanitized and non-sanitized data. It details a problem statement, derives information-theoretic bounds, defines a trainable adversarial game, and discusses service-provider specific requirements. Examples of the framework are provided, concluding the paper. The scenario involves high-dimensional data depending on utility and secret latent variables, with a service provider aiming to estimate the utility variable. The curr_chunk discusses the concept of privacy and transparency in data representation. It introduces a privatizer agent that aims to learn a mapping preserving space while revealing information about a latent variable U but not about a sensitive attribute S. Privacy is defined as the distance between probability distributions, while transparency is defined similarly. The goal is to find a data representation that is private with respect to S and transparent with respect to U. The curr_chunk discusses the concept of transparency in data representation, defining it as the distance between probability distributions. It introduces a stochastic mapping that is transparent with respect to a utility variable U. The text also mentions deriving an information-theoretic bound between privacy and transparency, showing that a specific choice of metrics can be implemented as a loss function for learning privatization transformations from data. In the context of the Privacy Funnel, the goal is to design a data-driven implementation for learning privacy-preserving mappings. This involves analyzing the properties of a mapping Q:X\u2192Q to minimize information leakage from sanitized data while maximizing shared information with the utility variable U. The joint distribution P(X,U,S) is considered, and the relationship between entropies, mutual information, and information leakage is explored. The bounds developed here are related to privacy/utility definitions. The text discusses the trade-off between privacy and utility in data sanitization mappings. By maximizing shared information with the utility variable U, the censored information I(U ; X|Q(X)) increases as the sanitized information I(S; Q(X)) decreases. This trade-off is formalized in Lemma 3.1, which sets a lower bound on the performance of mappings Q(X). The lemma shows that the information leakage about the secret and the censured information on the utility variable cannot exceed the total information in the original observed variable X. This is related to the trade-off between privacy and transparency in data sanitization mappings. The text discusses the trade-off between privacy and transparency in data sanitization mappings. It introduces a trainable loss metric based on the reverse Kullback-Leibler divergence to define privacy. The proposed privatizer loss function allows for different points in the transparency-privacy trade-off space, controlled by a tradeoff constant \u03b1. A low \u03b1 value prioritizes transparency, while a high value prioritizes privacy. The text introduces a privatizer loss function with a trade-off parameter \u03b1, balancing transparency and privacy. It provides bounds on the level of transparency and privacy achievable for any \u03b1 value. The privatizer can optimize this utility-privacy formulation using a data-driven approach, even without knowledge of the joint distribution. The privatizer aims to predict attacks by learning estimators of posterior distributions. It utilizes parametric mappings to censor information about utility variables. An adversarial game is optimized to minimize a loss function balancing transparency and privacy. Details on algorithmic implementation are provided in Section 7.3.1. The proposed framework in Section 7.3.1 allows for collaboration with the utility provider while maintaining user privacy. It addresses scenarios where the utility provider wants to use the same algorithm for sanitized and non-sanitized data, optimizing Q\u03b8(x, z) to minimize performance impact. Another scenario involves the utility provider being the sole agent with access to sanitized data and having estimation algorithms for both utility and privacy. The service provider collaborates with the utility provider to optimize Q\u03b8(x, z) for both sanitized and non-sanitized data. The service provider ensures that the secret attribute cannot be inferred from the sanitized data. Key agents include the utility algorithm, secret algorithm, and privatizer, each with specific roles in preserving privacy and utility. The proposed framework involves a utility algorithm trained on raw data and a secret algorithm trained to infer the secret variable after sanitization. The privatizer architecture remains consistent across experiments, demonstrating its versatility. Additional experiments are conducted under known conditions. The collaborative privacy framework consists of three components: raw data fed into the secret and utility algorithms, privatized data fed into both tasks directly, and analysis of the subject-within-subject problem. The text discusses a framework for analyzing the subject-within-subject problem related to facial identification on mobile phones. It involves training a stochastic mapping algorithm on facial image data to differentiate between consenting and nonconsenting users. The framework utilizes the FaceScrub dataset and VGGFace2 algorithm for utility and secrecy inference. The implementation details of the stochastic mapping algorithm are provided in Section 7.3.2. The UNET Ronneberger et al. (2015) architecture details are discussed in Section 7.3.2. Table 1 displays the top-5 categorical accuracy of the utility network over sanitized data at different \u03b1 points in the privacy-utility trade-off. Figure 3 illustrates how images are sanitized, showing that the sanitization function preserves utility information while censoring secret variables, even for unobserved images. A phone with this filter at the sensor level would be unable to collect information on nonconsenting users. The left and center figures in Figure 3 show images of consenting and nonconsenting users, along with their sanitized versions. The identity of consenting users remains easily verified, while the identity of nonconsenting users is effectively censored. The right table shows the Top-5 accuracy performance of the subject detector after sanitization. The table on the right displays the Top-5 accuracy performance of the subject detector after sanitization at various sanitation levels \u03b1. Performance is shown for consenting users (CU), observed private users (OPU), and unobserved private users (UPU). The system recognizes CU but not nonconsenting users. The study focuses on gender recognition (U) and emotion (S) using the CelebA dataset and Xception networks as utility and privacy estimators. The study uses Xception networks as utility and privacy estimators to hide gender attributes while allowing subject verification. The mapping should prevent gender detection but still allow subject verification. The experiment focuses on gender and emotion detection on raw and sanitized data, with a pretrained FaderNet as the baseline for the mapping function. The study uses Xception networks to hide gender attributes while allowing subject verification. FaderNet is chosen as the baseline for the mapping function, trained to defeat a gender discriminator. The performance is improved by training a UNET mapping using a proposed loss function. Testing on the FaceScrub dataset shows sanitized images produce gender probabilities close to the dataset prior. The top-5 categorical accuracy of the subject verification task varies accordingly. The study introduces a new paradigm for achieving privacy and utility in subject verification tasks. By using a user-specific privacy filter, almost perfect privacy can be achieved without modifying the system infrastructure. Representative architectures and results suggest that a collaborative user-controlled privacy approach is feasible. The study introduces a new paradigm for achieving privacy and utility in subject verification tasks through a user-specific privacy filter. The proposed framework provides privacy metrics and bounds, with ongoing research focusing on privacy tails concentration and information theory metrics for worst-case scenario guarantees. Privacy is linked to fairness, transparency, and explainability, with future research exploring a unified theory of these topics. A unified theory of privacy, fairness, transparency, and explainability in subject verification tasks is a significant contribution to the ML community. The study presents a new paradigm for privacy and utility, with ongoing research focusing on privacy metrics and information theory for worst-case scenario guarantees. The framework introduces equations and relaxations to achieve privacy and utility goals. The study introduces a new paradigm for privacy and utility in subject verification tasks, focusing on privacy metrics and information theory for worst-case scenario guarantees. It presents equations and relaxations to achieve these goals, with experiments showing how close to the theoretical bound can be reached under known conditions. In the extreme case, observations X contain almost perfect information about U and S, achieved through a Gaussian Mixture Model with low \u03c3 = 0.05. Privacy filters are linear, with Figure 6 illustrating data distribution for different levels of codependence k and tradeoff \u03b1. Figure 7 shows optimized privacy filters closely approaching theoretical bounds. The linear filter can achieve effective privacy-preserving mappings close to theoretical bounds by adjusting the parameter \u03b1. It can reach optimal bounds when variables U and S are perfectly independent or codependent. However, for intermediate cases, the linear filter may not follow the optimal tradeoff line for higher levels of privacy. Figure 7 illustrates the best privacy-utility loss on the validation set for different levels of codependence and trade-off parameter \u03b1. The text discusses optimizing data-driven loss functions using an adversarial approach. It details the adversarial training setup and network architectures used for the experiments. Minimizing the objective functions is challenging, with distinct purposes for each loss term. The first three terms aim to estimate true distributions, ensuring good estimators for privacy-utility mappings. The final loss term aims to find the best sampling function by applying Stochastic Gradient Descent to individual loss terms. Algorithms are used to solve specific equations, and crossentropy loss is evaluated on different types of inferences. A sanitation loss is also evaluated using Stochastic Gradient Descent. The architecture used for implementing the privacy filter in experiments involved a fully convolutional network trained initially to copy the image and infer attributes. This was followed by training the network as described in Algorithm 1. The architecture of the privacy filter is based on UNET, with a single noise layer for injecting Gaussian noise and an auxiliary label softmax for initialization. Input image sizes are shown for experiments. The experiments in Section 7.2 use networks shown in Figure 9 and FIG0, while other networks in the results section are implemented as described in their respective papers."
}