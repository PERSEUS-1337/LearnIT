{
    "title": "S1eEmn05tQ",
    "content": "Using variational Bayes neural networks, an algorithm is developed to accumulate knowledge from multiple tasks into a rich prior for few-shot learning. The posterior provides good uncertainty estimates and can learn from diverse tasks, achieving state-of-the-art accuracy on Mini-Imagenet. New benchmarks highlight failure modes of existing meta learning algorithms. Recent advancements in scaling Bayesian neural networks and improving posterior approximations are also discussed. In Bayesian methods, choosing a prior that reflects both current knowledge and lack of knowledge is crucial, especially in cases with limited observations. It is important to accurately approximate the posterior distribution, particularly in scenarios with small sample sizes and multiple modes. Gaussian processes have traditionally been used for applications like Bayesian optimization, active learning, and reinforcement learning, but the RBF kernel may not be suitable for all tasks. Recent tools like deep Gaussian processes show promise, but their scalability for learning from multiple tasks needs improvement. This study introduces a simple and scalable method to learn expressive priors and posteriors for models across tasks, achieving state-of-the-art performance on mini-imagenet. Two new benchmarks are proposed to highlight failure modes of popular meta-learning algorithms, where this method excels. The approach is further extended to three levels of hierarchies for better classification. In Section 5, experiments are conducted on three benchmarks to understand the behavior of the algorithm. By using a variational Bayesian approach, a prior over neural network weights is learned across tasks. The algorithm simplifies the modeling of weight distributions, resulting in a scalable method known as deep prior. In a scalable algorithm called deep prior, a probability distribution p(w|\u03b1) over network weights is learned using a hierarchical Bayes approach across N tasks. The likelihood of sample i of task j given model parameterized by w j is considered. The posterior p(\u03b1|D) is estimated using maximum a posteriori, with a focus on the term p(w j |\u03b1) due to high dimensionality and intricate correlations among dimensions. In a scalable algorithm called deep prior, a probability distribution p(w|\u03b1) over network weights is learned using a hierarchical Bayes approach across N tasks. To handle the high dimensionality and intricate correlations among dimensions, an auxiliary variable z is used along with a deterministic function projecting noise to the space of w. Marginalizing z is intractable, so it is added to joint inference and marginalized at inference time. The full posterior is factorized for point estimation of \u03b1, with a likelihood function of a neural network with weight matrices generated from the function h \u03b1. In multi-task learning, a simpler architecture is proposed to jointly learn a common function and posterior distribution for each task. The Variational Bayes approach is used to address the intractable normalization factor issue. The joint Evidence Lower Bound (ELBO) is reduced to a sum of individual ELBO for each task by factorizing the posterior independently for all tasks. The Evidence Lower Bound for a task in multi-task learning is simplified by factorizing the posterior distribution into task-specific and task-agnostic components. This simplification removes the need to explicitly calculate the Kullback-Leibler divergence over the space of parameters, improving scalability. The proposed approach simplifies the likelihood function for scalability by using a deep network parameterized by \u03b1. The posterior distribution for each task can be modeled using N(\u00b5 j , \u03c3 j ) or more expressive options like Inverse Autoregressive Flow (IAF) or Neural Autoregressive Flow. Using a single IAF for memory and computational efficiency. The approach simplifies the likelihood function by using a deep network parameterized by \u03b1. It models the posterior distribution for each task using Inverse Autoregressive Flow (IAF) for memory and computational efficiency. The Monte-Carlo approximation is used to estimate the KL term and accelerate training through mini-batch procedures. The approach simplifies the likelihood function using a deep network parameterized by \u03b1 and models the posterior distribution for each task with Inverse Autoregressive Flow (IAF). It utilizes a Monte-Carlo approximation to estimate the KL term and accelerates training through mini-batch procedures. To approximate the gradient, n mb samples are taken across all tasks by concatenating datasets into a meta-dataset and adding j as an extra field. Sampling uniformly 5 n mb times with replacement from the meta-dataset, the term n j in Equation 7 balances the prior and observations for each task. The deep prior allows flexible knowledge transfer from multiple tasks, assuming task information is encoded in a low-dimensional variable z. In Section 5, the text discusses encoding task information in a low-dimensional variable z for regression tasks. However, for image classification, this approach is not ideal. To address this, a latent classifier is introduced at a third level of hierarchy to enhance algorithms like Prototypical Networks. The likelihood function is decomposed into p(y i |x i , v) using a latent variable v, such as Gaussian linear regression on the representation produced by the neural network. This factorizes the general form of the marginal likelihood. The text discusses encoding task information in a low-dimensional variable for regression tasks and introduces a latent classifier to enhance algorithms like Prototypical Networks. The likelihood function is decomposed into p(y i |x i , v) using a latent variable, such as Gaussian linear regression on the neural network representation. This factorizes the general form of the marginal likelihood, with closed form solutions available for linear regression with Gaussian prior. Sampling schemes were explored to reduce gradient variance on task-specific parameters, but no benefits were observed. Different factorizations of the marginal likelihood are proposed for training the latent classifier on subsets of the training set and evaluating on samples left out. The text discusses encoding task information in a low-dimensional variable for regression tasks and introduces a latent classifier to enhance algorithms like Prototypical Networks. A closed form solution is proposed for leave-one-out in prototypical networks, where prototypes are computed without certain examples in the training set. This efficient algorithm maintains the same complexity as the original one and provides a good proxy for evaluating samples. Hierarchical Bayes algorithms for multitask learning have a long history, with recent exploration of hierarchical Bayesian inference with neural networks. Our approach differs by using a discriminative approach focusing on model uncertainty, obtaining a posterior on z without explicitly encoding S j, and exploring more complex posterior families like IAF. These differences make our algorithm simpler to implement and scale to larger datasets. Other works consider neural networks with latent variables but do not delve into these specific aspects. Some recent works on meta-learning focus on transfer learning from multiple tasks. ModelAgnostic Meta-Learning (MAML) finds a shared parameter \u03b8 that allows for good predictions on test sets. A Bayesian version of MAML has been considered, along with a meta-learning approach where an encoding network generates model parameters for good performance on test sets. Few-shot learning algorithms have also gained recent interest. Recent interest in few-shot learning has led to algorithms capable of transferring knowledge from multiple tasks. These approaches aim to find a representation where a simple algorithm can create a classifier from a small training set. Using a neural network pre-trained on a standard multi-class dataset, researchers have been able to transfer prior knowledge to new classes. Through experiments, they aim to determine if deep prior can learn meaningful prior tasks, compete against state-of-the-art benchmarks, and identify situations where it may fail. To gain insight into the behavior of the prior and posterior, one-dimensional regression tasks are chosen, specifically using periodic functions to test the regressor's ability to extrapolate beyond observed points. For this study, periodic functions are used to test the regressor's ability to extrapolate beyond its domain. Datasets consist of (x, y) pairs sampled from sine waves with different phase and amplitude. A meta-training set of 5000 tasks is constructed with varying parameters. Evaluation is done on tasks with different domains, and the model uses densely connected layers after sampling from IAF. The model uses densely connected layers after sampling from IAF to concatenate z with x. It consists of 12 layers with residual connections and ReLU activation functions. The final layer projects to 2 outputs \u00b5 y and s, where s is used to generate heteroskedastic noise. The likelihood of the training set is expressed using p(y|x, z) = N (\u00b5 y (x, z), \u03c3 y (x, z)2). Results show examples of tasks with different sample sizes, with samples from the posterior distributions fading in the background. The model uses densely connected layers with residual connections and ReLU activation functions. It projects to 2 outputs \u00b5 y and s, where s generates heteroskedastic noise. Results show tasks with varying sample sizes, with posterior predictions becoming more accurate. The posterior predictions improve with more data points, but uncertainties remain. Experimenting with MAML BID11 did not yield satisfactory results, even after simplifying the task. The results show adaptation with 16 samples per task using a densely connected network with 8 hidden layers. Training involves two gradient steps and evaluation with 5 steps. Replicating regression results from BID11 was done to validate the implementation. Removing the KL regularizer from deep prior and reducing the posterior distribution to a deterministic one centered on \u00b5 j improved mean square error. The uncertainty provided by deep prior led to systematic improvement in an increasing dataset size. BID34 proposed using a subset of Imagenet for few-shot benchmarking. BID34 proposed a few-shot learning benchmark using a subset of Imagenet. Tasks are generated by sampling 5 classes with 5 training samples each. Meta-validation and meta-test are performed on unseen tasks/classes. 16 classes are isolated for meta-validation and 20 for meta-test. The training procedure follows the one suggested in BID28. The training procedure for few-shot learning tasks involves training on a fixed set of 1000 tasks to ensure diversity. Resampling 5x5 training and test sets from a fixed split of the tasks increases diversity. The deep prior model uses a ResNet with FILM layers between residual blocks to condition on the task. Predictions are made through a softmax layer. This architecture was found to be slow to train due to noisy last layer generation. The deep prior model, using a ResNet with FILM layers, was slow to train due to noisy last layer generation. Despite this, it achieved 62.6% accuracy on Mini-Imagenet. To enhance the model, task conditioning was combined with prototypical networks, eliminating the need to generate the final layer and improving training speed and generalization. The KL term acts as a regularizer, preventing overfitting and improving generalization. A deeper network with residual connections yields significant improvements, while task conditioning only provides minor benefits. The KL regularizer is crucial for achieving state-of-the-art results. The Synbols benchmark involves predicting font families and symbols with Prototypical Networks, achieving 88.3% accuracy. However, blending tasks reduces accuracy to 76.8%, which can be improved to 83.5% with task conditioning. This highlights the importance of task conditioning for learning representations suitable for heterogeneous benchmarks. Using a variational Bayes framework, a scalable algorithm called deep prior was developed for hierarchical Bayesian learning of neural networks. Results on the Harmonics dataset showed that the learned manifold across tasks exhibits meaningful prior properties. MAML struggles to adapt when tasks are too different, and algorithms based on a single image representation only work well when tasks share similar features. These findings led to achieving state-of-the-art results on Mini-Imagenet. When experimenting with the Harmonics toy dataset, issues with repeatability were observed, likely due to local minima. Investigating the multimodality of posterior distributions with small sample size, a simplified problem of a single sine function was used. The likelihood function was defined, and the posterior distribution for a bi-dimensional problem was visualized with a dataset of 2 points. The posterior distribution in FIG1 shows high multi-modality, with modes representing different functions. The number of modes varies with the dataset, but the IAF model struggles to capture more than two modes, even after adjustments and multiple restarts. The IAF model struggles to capture more than two modes, even with adjustments and multiple restarts. There is a thin path of density connecting the modes, which remains significant with longer training."
}