{
    "title": "SkgjKR4YwH",
    "content": "MixUp is a data augmentation scheme where training samples and labels are mixed using linear coefficients. Without label mixing, it becomes a more conventional scheme. Samples are moved towards other classes, known as directional adversarial training (DAT). Under certain conditions, MixUp converges to a subset of DAT. Untied MixUp (UMixUp) mixes training labels with different coefficients, converging to the entire class of DAT schemes. UMixUp is a generalization of MixUp and a form of adversarial training. UMixUp is a novel interpretation of MixUp, belonging to a class similar to adversarial training. It outperforms MixUp by experimenting with different datasets and loss functions. Deep learning applications often require complex networks with a large number of parameters, leading to the need for effective regularization techniques. Complexity curtailing methods like weight decay and dropout constrain models to learn in a subset of parameter space for better generalization. Data augmentation methods add transformed versions of training samples to increase the dataset size. Adversarial training, a recent augmentation scheme, aims to reduce misclassification of minimally perturbed training samples. Adversarial training schemes focus on perturbing training samples within a bounded region to improve model robustness. MixUp, a data augmentation scheme, generates new samples by mixing pairs of training samples. Despite its generalization performance, the working mechanism of MixUp is not well understood. Some suggest it imposes local linearity on the model using points outside of the data manifold. MixUp is seen as improving adversarial robustness, but not as adversarial training. A framework connects MixUp to a MixUplike scheme without label mixing, moving samples towards adversarial classes, similar to adversarial training. MixUp is a method that improves adversarial robustness and is connected to directional adversarial training (DAT). Untied MixUp (UMixUp) is introduced as an enhancement of MixUp that converges to the entire family of DAT schemes. Experiments show that UMixUp's classification performance surpasses MixUp. This research aims to better understand the working of MixUp. The study aims to establish DAT as analogous to adversarial training, UMixUp as a superset of MixUp converging to the entire family of DAT schemes, and empirically show that UMixUp's classification performance surpasses MixUp. Additionally, the paper extends the applicability of MixUp to models using any loss function termed target-linear. The paper extends the applicability of MixUp to models using any loss function called target-linear. It defines target-linearity and experiments with a new loss function called negative cosine-loss to show its potential. The set Y represents all possible labels for training samples in D. A neural network function F, parameterized by \u03b8, maps X to vector space Z. Function \u03d5 maps labels in Y to elements in Z. The model's prediction is denoted by F(x), with the true label t(x). The learning problem involves minimizing the loss function L with respect to parameters \u03b8. Target-linearity is defined for loss functions in this setting. Target-linear loss functions arise naturally in many settings. Two examples include the Cross-Entropy Loss and Negative-Cosine Loss, both defining specific loss functions for probability vectors and unit-length vectors, respectively. Theoretical development in this paper relies on two fundamental assumptions: target linearity and symmetric pair-sampling distribution. Target-linear loss functions like Cross-Entropy and Negative-Cosine Loss are key. MixUp in this study goes beyond standard methods, focusing on symmetric pair sampling. In MixUp, samples are combined using a mixing ratio \u03bb drawn from a symmetric distribution Q. Different schemes like DAT and UMixUp use varying \u03bb distributions and target label generation methods. UMixUp's label mixing ratio is a function of \u03bb, while Untied MixUp allows for any \u03b3(\u03bb) as the label mixing ratio. Untied MixUp is defined by its mixing policy P uMix and weighting function \u03b3, which can vary freely. A framework is established to compare MixUp, DAT, and Untied MixUp schemes based on their optimization problems. Sample pairs are drawn i.i.d. from Q in each scheme, with MixUp using P Mix as the mixing policy and DAT using P DAT as the adversarial policy. The overall loss functions are denoted as L m E, with UMixUp using any \u03b3(\u03bb) as the label mixing ratio. The main theoretical result of this paper establishes the relationship between DAT, UMixUp, and MixUp. Both UMixUp and MixUp converge to DAT as the number of mixed sample pairs increases. Prior to this, insight into DAT is provided, highlighting its similarity to adversarial training and regularization mechanisms. Adversarial training involves augmenting the training dataset by searching for approximations of true adversarials within bounded regions around each sample. The loss function in adversarial training maximizes the loss with respect to the true label of x. Baseline training learns correct classification over training data, while adversarial training improves generalization by moving the classification boundary. DAT combines intra-class mixing and inter-class mixing to smooth classification boundaries and perturb training samples towards adversarial classes for better generalization. Inter-class mixing dominates in many-class learning problems, with DAT primarily consisting of inter-class mixing. DAT, analogous to adversarial training, involves inter-class mixing with probabilistic movement within a bounded region. In adversarial training, a sample is moved within an L p -ball to maximize training loss, while in DAT, a second sample governs the direction of perturbation. In DAT, a second sample x governs the direction in which x is perturbed, connecting adversarial training and DAT. Inter-class mixed sample x 2 pushes the model's classification boundary closer to the ground-truth classifier, while intra-class sample x 3 smooths inner parts of the class region. Untied MixUp and DAT are equivalent when n tends to infinity, infusing both MixUp and UMixUp with the intuition of adversarial training. The Untied MixUp loss function, uMix, is related to the DAT loss function, DAT. As n tends to infinity, the overall loss of both DAT and UMixUp converges in probability to their respective expected losses. UMixUp converges to a subset of DAT, and DAT converges to a subset of UMixUp as n increases. Each configuration in P \u00d7 F defines an Untied MixUp scheme, and U maps a DAT scheme to an Untied MixUp. The Untied MixUp scheme is defined by configurations in P \u00d7 F. U maps a DAT scheme to an Untied MixUp scheme. Another map, D u, maps an Untied MixUp scheme to a DAT scheme. Lemmas 2, 3, 4, and 5 provide the basis for theorem 1. Lemma 2, 3, 4, and 5 provide the foundation for Theorem 1, showing that as n increases, both DAT and UMixUp converge towards their expected loss. The family of DAT schemes converges to a subset of UMixUp schemes, and vice versa. Ultimately, the family of UMixUp schemes converges to the entire family of DAT schemes as n increases. This equivalence between the two families indicates that there are DAT schemes that do not correspond to a MixUp scheme. The relationship between MixUp, DAT, and Untied MixUp schemes is illustrated in Figure 1. Image classification tasks on various datasets are performed using PreActResNet18 as the baseline classifier. Two target-linear loss functions, cross-entropy (CE) loss, and negative-cosine (CE) loss are implemented. The NC loss model maps labels to unit-length vectors of dimension d, with d chosen as 300 for specific datasets. Our implementation of MixUp and Untied MixUp improves upon the original authors' implementation by sampling \u03bb independently for each sample and creating two shuffled copies of the entire training dataset prior to each epoch for a closer approximation to i.i.d. sampling. Our implementation improvements for MixUp and Untied MixUp involve using mini-batches for smoother training. The main results of our experiments are presented in tables 1 to 4, with a focus on policy space exploration for MixUp and Untied MixUp. The search for Untied MixUp's optimal policy is limited to an ad hoc heuristic search due to its complexity. The experiments' main results are summarized in tables 1 to 4, showing that Untied MixUp schemes outperform MixUp counterparts in most cases. The confidence intervals for Untied MixUp are significantly different from MixUp schemes, with the baseline model designed for image classification tasks. The results demonstrate that Untied MixUp outperforms MixUp in image classification tasks, with significant improvements in regularization. Both MixUp and Untied MixUp are effective on NC loss models, validating their generalization to linear loss models. The lemma holds true because the denominator is only zero when p(\u03bb) = 0, so those \u03bb for which g(\u03bb) is undefined never get drawn in the DAT scheme."
}