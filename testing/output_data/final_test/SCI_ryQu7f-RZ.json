{
    "title": "ryQu7f-RZ",
    "content": "Several stochastic optimization methods like RMSProp, Adam, Adadelta, Nadam use gradient updates scaled by exponential moving averages of squared past gradients. However, in some cases, these algorithms fail to converge to an optimal solution due to the exponential moving average used. By providing an example in a convex optimization setting, it is shown that Adam does not always converge to the optimal solution. New variants of the Adam algorithm are proposed to address these convergence issues by incorporating \"long-term memory\" of past gradients. New variants of the Adam algorithm are proposed to fix convergence issues and improve empirical performance in training deep networks. These variants adjust the learning rate on a per-feature basis by scaling coordinates of the gradient with past gradients. ADAGRAD is effective for sparse gradients but deteriorates in other settings. In high dimensional problems like deep learning, ADAGRAD's performance deteriorates due to rapid decay of the learning rate in nonconvex settings with dense gradients. Variants like RMSPROP, ADAM, ADADELTA, and NADAM have been proposed to address this issue by using exponential moving averages of squared past gradients. These algorithms limit the reliance on past gradients to improve convergence, although they may not converge in some settings with rare large gradients in minibatches. In this paper, the analysis focuses on how exponential moving averages in optimization algorithms like RMSPROP and ADAM can lead to non-convergence issues. The study provides a simple convex optimization problem example where RMSPROP and ADAM fail to converge to an optimal solution. The analysis extends to other algorithms using exponential moving averages, such as ADADELTA and NADAM. The analysis in this paper focuses on the need for optimization algorithms to have \"long-term memory\" of past gradients for guaranteed convergence. New variants of the ADAM algorithm are proposed to address convergence issues by incorporating long-term memory of past gradients. These variants maintain the same time and space requirements as the original ADAM algorithm. The analysis introduces new variants of the ADAM algorithm that incorporate long-term memory of past gradients for guaranteed convergence. A preliminary empirical study shows promising performance on machine learning problems. The framework analyzes iterative optimization methods in the online optimization problem setting. The algorithm's regret at the end of T rounds is given by DISPLAYFORM0. Our aim is to devise an algorithm that ensures R T = o(T), converging the model's performance to the optimal one. The standard online gradient descent algorithm moves the point x t in the opposite direction of the gradient g t = \u2207f t (x t) while maintaining feasibility by projecting onto set F. The online learning problem is related to stochastic optimization, specifically empirical risk minimization (ERM). Online gradient descent and stochastic gradient descent (SGD) are used interchangeably. A generic adaptive framework is provided to understand the differences between adaptive methods. The generic adaptive framework encapsulates popular adaptive methods for online learning. The algorithm remains abstract, with unspecified averaging functions. Adaptive methods aim to choose these functions appropriately to improve empirical performance compared to standard stochastic gradient algorithms. Adaptive methods aim to improve convergence by choosing appropriate averaging functions. ADAGRAD BID2 uses averaging functions DISPLAYFORM4 and step size \u03b1 t = \u03b1/ \u221a t. Exponential moving average variants like RMSPROP, ADAM, NADAM, and ADADELTA are popular in deep learning, using an exponential moving average as function \u03c8 t. ADAM is a popular variant of adaptive methods in deep learning, using exponential moving average functions with recommended values for \u03b21 and \u03b22. RMSPROP is a variant of ADAM with \u03b21 = 0. In practice, a more aggressive choice of constant step size is effective. In deep learning applications, the momentum term from non-zero \u03b21 in ADAM significantly boosts performance. The discussion focuses on the fundamental flaw in exponential moving average methods like ADAM, showing examples of non-convergence in simple settings. The main issue lies in the change in the inverse of learning rate over time, which differs from SGD and ADAGRAD. The discussion highlights the flaw in exponential moving average methods like ADAM and RMSPROP, showing non-convergence examples in simple settings due to violations of positive definiteness. Specifically, ADAM can converge to highly suboptimal solutions in certain scenarios. Theorem 1 states that there is an online convex optimization problem where ADAM has non-zero average regret as T approaches infinity. The algorithm in BID3 uses an update that includes adding a small constant in the denominator to potentially address this issue. The selection of this parameter is crucial for the algorithm's performance. ADAM and RMSPROP algorithms may have non-zero average regret asymptotically, converging to a point that is the worst among all points in the set [-1, 1]. This issue is more pronounced in high-dimensional settings with large variance in gradients over time. Using a large \u03b22 in ADAM is recommended in practice, but there are scenarios where ADAM may still have non-zero average regret. Theorem 2 states that for constant \u03b21 and \u03b22 where \u03b21 < \u221a \u03b22, ADAM has non-zero average regret asymptotically. This implies that momentum or regularization via will not aid in convergence to the optimal solution. The condition \u03b21 < \u221a \u03b22 is typically met in practice. Theorem 3 extends this to stochastic convex optimization, showing that ADAM may not converge to the optimal solution in that setting. ADAM may not converge to the optimal solution in a stochastic convex optimization problem when using problem-dependent \u03b21 and \u03b22. This requires tuning a large set of parameters, defeating the purpose of adaptive methods. The example of non-convergence highlights issues with ADAM, which could slow down convergence in realistic scenarios. The analysis in BID3 relies on decreasing \u03b21 over time, extending the implications of the findings. In this paper, a new exponential moving average variant is developed with guaranteed convergence, aiming to preserve the practical benefits of ADAM and RMSPROP. The analysis focuses on the non-convergence of ADAM when \u03b21 is held constant, and modifications are made to ensure the algorithms satisfy additional conditions. AMSGRAD is a modified algorithm that uses a smaller learning rate compared to ADAM. It ensures positive semi-definite values for \u0393 t by adjusting \u03b2 1 and \u03b2 2 with time. By maintaining the maximum of all v t, AMSGRAD normalizes the running average of the gradient, resulting in a non-increasing step size. This approach avoids the issues of ADAM and RMSPROP, ensuring \u0393 t 0 for all t \u2208 [T]. AMSGRAD is a modified algorithm that uses a smaller learning rate compared to ADAM. It ensures positive semi-definite values for \u0393 t by adjusting \u03b2 1 and \u03b2 2 with time. By maintaining the maximum of all v t, AMSGRAD normalizes the running average of the gradient, resulting in a non-increasing step size. This approach avoids the issues of ADAM and RMSPROP, ensuring \u0393 t 0 for all t \u2208 [T]. In practice, a constant \u03b2 1t is typically used for AMSGRAD, which neither increases nor decreases the learning rate, potentially leading to a non-decreasing learning rate even with large gradients in future iterations. AMSGRAD is a modified algorithm that uses a smaller learning rate compared to ADAM. It ensures positive semi-definite values for \u0393 t by adjusting \u03b2 1 and \u03b2 2 with time. By maintaining the maximum of all v t, AMSGRAD normalizes the running average of the gradient, resulting in a non-increasing step size. This approach avoids the issues of ADAM and RMSPROP, ensuring \u0393 t 0 for all t \u2208 [T]. In practice, a constant \u03b2 1t is typically used for AMSGRAD, which neither increases nor decreases the learning rate, potentially leading to a non-decreasing learning rate even with large gradients in future iterations. Theorem 4 in AMSGRAD provides a bound on the regret, and a momentum decay of \u03b2 1t = \u03b2 1 /t can still ensure a regret of O( \u221a T ). The algorithm AMSGRAD adjusts \u03b2 values over time to maintain positive semi-definite values for \u0393 t. It normalizes the running average of the gradient, ensuring a non-increasing step size. AMSGRAD avoids the issues of ADAM and RMSPROP by ensuring \u0393 t 0 for all t \u2208 [T]. Theorem 4 in AMSGRAD provides a regret bound, and a momentum decay of \u03b2 1t = \u03b2 1 /t can ensure a regret of O( \u221a T ). The resulting algorithm is similar to ADAGRAD but normalizes with smoothed gradients. Empirical results on synthetic and real-world datasets for multiclass classification using logistic regression and neural networks are presented. The average regret and iterate values of ADAM and AMSGRAD algorithms are compared with \u03b2 values set to 0.9 and 0.99. ADAM's regret does not converge to 0 and its iterates converge to x = 1 with the largest regret. In contrast, AMSGRAD's average regret converges to 0 and its iterate converges to the optimal solution. The stochastic optimization setting is shown in FIG1 with a probability function. ADAM's iterate converges to x = 1, a highly suboptimal solution. ADAM's iterate x t converges to x = 1, a suboptimal solution. AMSGRAD outperforms ADAM on logistic regression with MNIST dataset. AMSGRAD shows better performance on train and test loss, and is more robust to parameter changes. For the multiclass classification problem on MNIST, a neural network with a 1-hidden fully connected layer was trained using AMSGRAD and ADAM optimizers. Parameters were set as \u03b2 1 = 0.9 and \u03b2 2 from {0.99, 0.999}, with a hidden layer of 100 ReLU units. A grid search was conducted to optimize performance. Additionally, the CIFAR-10 dataset was used with CIFARNET, a convolutional neural network. The CIFARNET model, a CNN with multiple layers, was used to train a multiclass classifier on a dataset of 60,000 labeled 32x32 images. The architecture includes 2 convolutional layers with 64 channels, 2 fully connected layers, max pooling, and dropout. Results show that AMSGRAD outperforms ADAM in terms of train loss and accuracy, with good performance on test loss as well. The alternative approach, ADAMNC, uses a non-constant \u03b21 and \u03b22 to achieve good convergence rates without changing the structure of ADAM. The algorithm is provided in the appendix and shows that appropriate selection of \u03b21t and \u03b22t can lead to good performance. Theorem 5 provides conditions for achieving a bound on regret when using ADAMNC. The text discusses the generalization of results when constraints are violated in the selection of parameters {(\u03b1 t , \u03b2 2t )}. A corollary shows that setting \u03b2 2t = 1 \u2212 1/t yields a momentum-based variant of ADAGRAD with improved regret rates. The study also explores different settings for \u03b2 1t to achieve a data-dependent regret of O( \u221a T ). In this paper, the study highlights flaws in exponential moving variants of ADAGRAD, such as RMSPROP and ADAM, leading to suboptimal solutions. Algorithms relying on a fixed window of past gradients for scaling updates face convergence issues. Proposed fixes modify the algorithms to incorporate long-term memory of past gradients, maintaining practical performance and sometimes showing improvements. The focus is on theoretical problems with popular exponential moving average variants of ADAGRAD. The paper discusses flaws in RMSPROP and ADAM, emphasizing the need to understand their behavior and potential pitfalls. It suggests design principles for better stochastic optimization and provides a proof for a specific function sequence. The paper discusses flaws in RMSPROP and ADAM, emphasizing the need to understand their behavior and potential pitfalls. It suggests design principles for better stochastic optimization and provides a proof for a specific function sequence. In the new coordinate system, the ADAM algorithm is executed for a sequence of functions with bounded gradients. The main claim is that iterates {x t } \u221e t=1 from ADAM updates satisfy x t > 0 for all t \u2208 N and x 3t+1 = 1 for all t \u2208 N \u222a {0}, proven by mathematical induction. The paper discusses flaws in RMSPROP and ADAM, emphasizing the need to understand their behavior and potential pitfalls. It suggests design principles for better stochastic optimization and provides a proof for a specific function sequence. In the new coordinate system, the ADAM algorithm is executed for a sequence of functions with bounded gradients. The main claim is that iterates {x t } \u221e t=1 from ADAM updates satisfy x t > 0 for all t \u2208 N and x 3t+1 = 1 for all t \u2208 N \u222a {0}, proven by mathematical induction. For some t \u2208 N \u222a {0}, if x i > 0 for all i \u2208 [3t + 1] and x 3t+1 = 1, the aim is to prove that x 3t+2 and x 3t+3 are positive and x 3t+4 = 1. By analyzing the gradients and step size choice, it is shown that x 3t+2 and x 3t+3 are positive, and x 3t+4 = 1 is proven by showing x 3t+4 \u2265 1. The text discusses flaws in RMSPROP and ADAM, emphasizing the need to understand their behavior. It provides a proof for a specific function sequence where iterates from ADAM updates satisfy certain conditions. The proof involves analyzing gradients and step size choices to show that specific values are positive and equal to 1. The proof generalizes the optimization setting used in Theorem 1, focusing on linear functions and specific conditions for the function sequence. The analysis shows that certain values are positive and equal to 1, emphasizing the flaws in RMSPROP and ADAM. The proof generalizes the optimization setting used in Theorem 1, focusing on linear functions and specific conditions for the function sequence. It is not hard to see that for large constant C depending on \u03b2 1 and \u03b2 2, x = -1 provides the minimum regret. The gradients have a specific form, and the analysis focuses on iterations t \u2265 T where regret is constant. The proof focuses on linear functions and specific conditions for the function sequence, extending the optimization setting from Theorem 1. It shows that for large constant C, x = -1 minimizes regret, with gradients following a specific form. The analysis concentrates on iterations t \u2265 T where regret remains constant. The proof focuses on linear functions and specific conditions for the function sequence, extending the optimization setting from Theorem 1. It shows that for large constant C, x = -1 minimizes regret, with gradients following a specific form. The analysis concentrates on iterations t \u2265 T where regret remains constant. The inequalities and bounds derived from the definitions and properties of variables lead to the conclusion that x t+C \u2265 min{1, x t + \u03bb/ \u221a t}. The proof focuses on linear functions and specific conditions for the function sequence, extending the optimization setting from Theorem 1. It shows that for large constant C, x = -1 minimizes regret, with gradients following a specific form. The analysis concentrates on iterations t \u2265 T where regret remains constant. The inequalities and bounds derived from the definitions and properties of variables lead to the conclusion that x t+C \u2265 min{1, x t + \u03bb/ \u221a t}. The proof involves a one-dimensional stochastic optimization setting over the domain [-1, 1], with specific probabilities for function selection at each time step. The expected function F(x) = \u03b4x; the optimum point over [-1, 1] is x = -1. The gradient at each time step equals C with probability p and -1 with probability 1 - p. For a large enough constant C, E[\u2206t] \u2265 0, implying ADAM's steps drift away from x = -1. Lemmas and bounds are used to analyze the optimization process. The text discusses bounding various terms in the optimization process using inequalities like Cauchy-Schwarz and Jensen's. By combining these bounds, a lower bound on E[\u2206t] is derived for ADAM's step. The analysis shows that for a sufficiently large constant C, the expression can be made non-negative. The text discusses the non-convergence of ADAM due to incorrect assumptions in the proof provided in a previous paper. The proof in the previous paper had issues with the positivity of a defined term, leading to problems in the convergence claim for ADAM. The proof addresses issues in lemmas 10.3 and 10.4, providing a corrected convergence proof for AMSGRAD. Using Lemma 4, the regret at each step is bounded by the convexity of function f t. Lemma 2 further supports the parameter settings in Theorem 4. The proof in Theorem 4 establishes parameter settings and conditions for AMSGRAD. It involves inequalities derived from the update rule of Algorithm 2 and Cauchy-Schwarz inequality. The proof concludes by bounding the quantity in Equation FORMULA8 using harmonic sum bounds. The proof of Theorem 4 establishes parameter settings and conditions for AMSGRAD, involving inequalities derived from the update rule of Algorithm 2 and Cauchy-Schwarz inequality. The bound on the harmonic sum is used to simplify Equation FORMULA8, with the regret of AMSGRAD bounded by O(G \u221e \u221a T) or the bound in Theorem 4. The regret of AMSGRAD is upper bounded by O(\u221aT) in the worst case scenario, as shown in the proof. The inequalities are derived from the convexity of function ft and the update rule of Algorithm 2. Lemma 3 provides an intermediate result for further bounding the inequality. Theorem 6 states that for any > 0, ADAM with a modified update and specific parameter settings can have non-zero average regret as T \u2192 \u221e for convex functions with bounded gradients on a feasible set. The proof involves a telescopic sum and rescaling the sequence of functions. The proof of Theorem 6 involves rescaling the function sequence and using a modified update for ADAM. The aim is to show that specific parameters lead to non-zero average regret for convex functions with bounded gradients on a feasible set. The proof follows a principle of mathematical induction to demonstrate the positivity of certain variables in the sequence. The proof of Theorem 6 involves rescaling the function sequence and using a modified update for ADAM to show non-zero average regret for convex functions with bounded gradients on a feasible set. The proof follows mathematical induction to demonstrate the positivity of variables in the sequence, with specific parameters leading to the desired outcome. The proof of Theorem 6 involves rescaling the function sequence and using a modified update for ADAM to show non-zero average regret for convex functions with bounded gradients on a feasible set. By mathematical induction, it is shown that ADAM suffers a regret of at least 2C - 4 every 3 steps, leading to a regret of (2C - 4)T/3. For the general case, a sequence of functions is considered, resulting in a regret of (2C - 4)\u221aT/3 asymptotically. Lemma 4 states that for any Q \u2208 S d + and convex feasible set F \u2282 R d, if u 1 = min x\u2208F Q 1/2 (x\u2212z 1) and u 2 = min x\u2208F Q 1/2 (x\u2212z 2), then Q 1/2 (u 1 \u2212 u 2) \u2264 Q 1/2 (z 1 \u2212 z 2). The proof involves properties of projection operators and inequalities. The lemma discusses properties of projection operators and inequalities in relation to a convex feasible set. It states that for any non-negative real numbers y1, ..., yt, certain conditions hold, leading to a specific inequality."
}