{
    "title": "BkUp6GZRW",
    "content": "This paper introduces the Dual Actor-Critic (Dual-AC) algorithm, derived from the Lagrangian dual form of the Bellman optimality equation. It involves a two-player game between the actor and a dual critic, updated cooperatively to optimize the same objective function. The algorithm effectively solves the minimax optimization problem using multi-step bootstrapping, path regularization, and stochastic dual ascent. It outperforms existing algorithms in various benchmarks in reinforcement learning. Value-function-based RL algorithms approximate the optimal value function to derive a good policy, utilizing the Bellman equation and bootstrapping for efficiency. However, the link between value function quality and policy quality is weak. Policy-search-based algorithms like REINFORCE search for optimal policy parameters using unbiased Monte Carlo estimates, incrementally updating parameters to enhance policy quality. Actor-critic methods combine policy-search and value-function-based approaches to improve policy quality with reduced sample complexity. The algorithm consists of an actor (policy) and a critic (value function), where the critic helps compute update directions for the actor. This integration reduces variance and sample complexity compared to pure policy-search algorithms. The Dual Actor-Critic algorithm proposes a new approach where the actor and a dual critic function are trained cooperatively to optimize the same objective function. This algorithm is derived by solving a dual form of the Bellman equation and can be seen as a two-player game between the actor and the critic. The algorithm proposed in the Dual Actor-Critic approach involves a two-player game between the actor and the dual critic, aiming to optimize the Bellman equation. It utilizes path regularization for numerical stability and extends the formulation to the multi-step case for improved bias/variance tradeoff. The algorithm is compared to existing ones and evaluated in various scenarios. The algorithm proposed in the Dual Actor-Critic approach involves a two-player game between the actor and the dual critic, aiming to optimize the Bellman equation. It is evaluated on locomotion tasks in the MuJoCo benchmark and compares favorably to state-of-the-art algorithms. The algorithm is based on a discounted MDP framework and utilizes linear programming to derive the Bellman optimality equation. In Appendix A.2, the duality view is extended to continuous state and action spaces. The reinforcement learning problem aims to find a policy that maximizes the total expected discounted reward. The optimal policy can be obtained from a linear program, and the dual form of the LP is often easier to solve. The optimal policy in reinforcement learning can be derived from a dual LP, which optimizes the policy directly. The optimal policy and value function are obtained through a saddle-point problem, providing a game perspective on the learning process. The learning process in reinforcement learning involves a game between the dual critic and the weighted actor, bypassing the traditional separation of policy evaluation and improvement. To solve the dual problem, stochastic mirror prox or primal-dual algorithms can be applied, but their use is limited beyond special cases. When state/action spaces are large or continuous, convergence guarantees for the one-step saddle-point problem with tabular parametrization no longer hold due to lack of convex-concavity. This leads to bias and numerical issues, impacting performance in practice. Notably, one-step bootstrapping in temporal difference algorithms can result in biased estimates when function approximation is used, introducing noise in policy parameter updates. The absence of local convexity and duality in nonlinear parametrization breaks necessary conditions for success in applying primal-dual algorithms to constrained problems. This results in biased stochastic gradient estimators and futile policy improvements when optimizing the saddle point problem without local convexity. Extending existing primal-dual algorithms for parametrized Lagrangian dual will also lead to biased estimators. In response to the issues of biased estimators and sample inefficiency in parametrized Lagrangian dual algorithms, techniques are introduced to address instability problems. These techniques include generalizing the minimax game to the multi-step case for better bias-variance tradeoff, using path regularization for local convexity and duality promotion, and implementing stochastic dual ascent for unbiased gradient estimates. Extending the minimax game to the multi-step setting allows for improved bias/variance tradeoffs in temporal-difference algorithms. In the multi-step setting, the optimal policy and value function are determined by a saddle point problem where the dual critic and weighted k-step actor compete for equilibrium. This competition is not straightforward due to the max-operator over distributions in the Bellman optimality equation. The competition in Theorem 3 is complex due to non-linear optimization and lack of convex-concave structure. Generalizing the duality to multi-step setting requires careful analysis. Function approximation leads to non-convex saddle-point problems, requiring methods like the augmented Lagrangian to stabilize solutions. The curr_chunk discusses the use of path regularization to promote local convexity and computation efficiency in optimization. It introduces a penalty function to the objective, controlled by a hyper-parameter, to enable local duality in the primal parameters. This regularization is motivated by the fact that the optimal value function satisfies a certain constraint. The approach of path regularization promotes local convexity and computation efficiency in optimization by utilizing rewards in the sample path to regularize the solution path of the value function V. It guides the search for the solution path by moving towards the optimal value function while staying close to the behavior policy \u03c0 b. This regularization also restricts the feasible domain of candidates V to be a ball centered at V \u03c0 b, enhancing local convexity and numerical robustness. The introduced path regularization promotes local convexity and computation efficiency in optimization by utilizing rewards in the sample path to regularize the solution path of the value function V. It ensures numerical robustness and controls side-effects, maintaining the optimal solution (V*, \u03b1*, \u03c0*). The local duality holds for Lr(V, \u03b1, \u03c0), with the valid range of \u03b7V keeping the optimal solution unchanged. The function approximation error remains an open problem for a general class of problems. The function approximation error is still an open problem for a general class of parametrization. The dual actor-critic algorithm is introduced to address this issue by incorporating stochastic update rules. The gradient estimators of the dual functions are derived using chain rule. The regularized dual function has gradients estimators for stochastic mirror descent algorithm. KL-divergence is used as the prox-mapping for the dual variables. The update rule emphasizes a trade-off between current policy and improvements based on samples, similar to TRPO. The Dual-AC algorithm utilizes a stochastic dual ascent approach to update the policy in each iteration, ensuring unbiased gradient estimators for better convergence. The V function in the Lagrangian is optimized to improve the policy. The V function in the Dual-AC algorithm serves as an interpolation between the value functions learned from on-policy and off-policy samples. Path regularization enforces V to be close to the value function of the behavior policy. Strategies for practical computation considerations are presented, including the update rule of V in each iteration. The update rule of V in the Dual-AC algorithm involves solving for V in each iteration by minimizing a loss function that depends on the behavior policy and a learning rate parameter. Setting a large learning rate can improve convergence speed. The algorithm utilizes the behavior policy from the previous iteration and can incorporate experience replay. Stochastic gradient descent is used for optimization, with options for other efficient algorithms. A k-step Monte Carlo approximation is employed for gradient estimation, with negligible truncation error as k increases. In the Dual-AC algorithm, V is updated in each iteration by minimizing a loss function with a learning rate parameter. The algorithm does not approximate the value or advantage function of the policy, but rather helps improve the current policy. The update rule of \u03b1 may face challenges with fixed initial sampling distributions. An assumption is made for \u03b1(s) to address this issue. The Dual-AC algorithm updates V in each iteration by minimizing a loss function with a learning rate parameter. The update rule of \u03b1 faces challenges with fixed initial sampling distributions, assuming \u03b1(s) to address this issue. Theorem 6 introduces a closed-form update for \u03b1 with a square-norm regularization term in the dual function. The update rule for \u03b8 \u03c1 involves the prox-mapping operator following the stochastic mirror descent algorithm for the regularized dual function. The update rule for \u03b8 \u03c0 in the t-th iteration simplifies to the natural policy gradient. This is a penalty version of trust region policy optimization, where conservative updates are made based on KL-divergence constraints. Approximating the KL-divergence with a second-order Taylor expansion allows for a closed-form update using the Fisher information matrix. Normalizing the gradient by its norm can improve performance. These practical tricks accelerate the stochastic mirror descent update. The dual actor-critic algorithm combines practical tricks with stochastic mirror descent updates to learn optimal value functions and policies in a unified framework based on the duality of linear programming. It extends the dual gradient method and is different from algorithms based on entropy-regularized Bellman equations. The algorithm extends the dual gradient method using stochastic gradient and Bregman divergence, converging with diminishing stepsizes and decaying errors. The update rules of \u03b1 and \u03c0 in the dual actor-critic are related to existing algorithms, reweighing samples and bearing similarities with TRPO and natural policy gradient. The Dual-AC algorithm is based on a stochastic dual ascent algorithm for solving a two-player game derived from Bellman optimality equation. It was evaluated on continuous control environments from OpenAI Gym using MuJoCo physics simulator. Dual-AC was compared with TRPO and PPO algorithms, with results reported for 5 random seeds. Details of the experiments, including setups and hyperparameters, are provided in Appendix C. To justify the analysis of instability sources in optimizing the parametrized one-step Lagrangian duality and its impact on the dual actor-critic algorithm, an Ablation study was conducted in various environments. The study compared Dual-AC and its variants, including Dual-AC without multi-step, path-regularization, and unbiased V, while also exploring the benefits of multi-step by varying k values. The Ablation study compared Dual-AC and its variants, including Dual-AC without multi-step, path-regularization, and unbiased V. Dual-AC with k = 10 and k = 50 showed consistent empirical performances on tasks like InvertedDoublePendulum-v1. The naive Dual-AC performed the worst, while Dual-AC found the optimal policy faster than other variants. Dual-AC w/o unbiased V had slower convergence due to sample inefficiency. Dual-AC w/o multistep and path-regularization couldn't reach the optimal policy, highlighting the importance of path-regularization in duality recovery. In this section, the Dual-AC algorithm was evaluated against TRPO and PPO on tasks like InvertedDoublePendulum-v1, Hopper-v1, HalfCheetah-v1, Swimmer-v1, and Walker-v1. The performance of Dual-AC improved with longer step lengths, as bias dominated variance in these MuJoCo environments. The results showed that path-regularization is crucial for duality recovery in Dual-AC. The proposed Dual-AC algorithm outperforms TRPO and PPO in various environments like Pendulum, InvertedDoublePendulum, Hopper, HalfCheetah, and Walker. It shows better learning speed and sample efficiency, especially in unstable dynamics. The advantage may stem from the different meaning of V in the algorithm, leading to improved policy improvement. In this paper, the Dual Actor-Critic algorithm is proposed to enhance sample efficiency by learning the optimal value function. The framework for actor and dual critic allows them to be optimized for the same objective function, but parametering them induces instability in optimization. The algorithm exploits a stochastic dual ascent algorithm for path regularization. The Dual Actor-Critic algorithm is proposed to enhance sample efficiency by learning the optimal value function. It achieves comparable performances with TRPO and PPO in some tasks but outperforms on more challenging tasks. The algorithm utilizes multi-step bootstrapping in a two-player game to address optimization instability. The Dual Actor-Critic algorithm enhances sample efficiency by learning the optimal value function. It achieves comparable performances with TRPO and PPO in some tasks but outperforms on more challenging tasks. The algorithm utilizes multi-step bootstrapping in a two-player game to address optimization instability. In this section, the linear programming and its duality are extended to continuous state and action MDP, with a focus on the strong duality for infinite constraints. The solvable MDP's optimal policy, \u03c0 DISPLAYFORM0, satisfies the KKT conditions and strong duality. The duality gap is computed to show strong duality holds. The k-step Bellman optimality equation leads to the \u03bb-Bellman optimality equation. The optimal policy, \u03c0 * (a|s), holds for arbitrary k \u2208 N. The k-step Bellman optimality equation leads to the \u03bb-Bellman optimality equation, where both T k and T \u03bb are shown to be monotonic. The optimal value function V * can be rewritten as the solution to an optimization problem, utilizing the Banach fixed point theorem. The optimization problem involves finding the optimal policy \u03c0* and value function V* by utilizing the KKT condition and complementary condition. The existence of a max operator over distribution space makes it non-linear programming, but Theorem 1 still applies to the dual variables. The optimal policy is denoted as \u03c0* and the value function as V*, with the condition implying \u03c1(s, a) = 0 if and only if a = a*. The optimization problem involves finding the optimal policy \u03c0* and value function V* using the KKT condition and complementary condition. The optimal policy \u03c0* and its corresponding value function V* are the solution to a saddle point problem. By decomposing \u03c1(s, a) = \u03b1(s)\u03c0(a|s) and applying strong duality, we can simplify the optimization process. The strong duality holds in optimization (8) as proven in Lemma 9. By switching min and max operators, we achieve the strong duality without the need for convex-concave property. The computation of P c is challenging due to the composition of max and condition expectation. The optimization for augmented Lagrangian method becomes more difficult with multi-step Lagrangian duality, as constraints are on distribution family P(S) and P(A). The local duality holds for L r (V, \u03b1, \u03c0), with (V * , \u03b1 * , \u03c0 * ) as the solution to Bellman optimality equation. The Hessian of L r (\u03b8 V * ) verifies the local duality, and the path regularization does not alter the optimum with appropriate \u03b7 V."
}