{
    "title": "ryga2CNKDH",
    "content": "Deep generative models have made significant progress, but evaluating and comparing them remains a challenge. Log-likelihood is a popular metric, and it can be efficiently estimated using annealed importance sampling for models like VAE and GAN. However, log-likelihood alone may not capture all performance aspects, so rate distortion curves are proposed for evaluation. These curves can be approximated using AIS at a similar computational cost. Generative models like VAEs, GANs, and AAEs are evaluated for lossy compression rates on MNIST and CIFAR10 datasets. Log-likelihood evaluation is challenging for implicit generative models like GANs. Alternative metrics such as Inception score and FID score are used for performance evaluation. Log-likelihoods are important measures of generative models, but they have limitations, especially for implicit generative models like GANs. Alternative metrics like Inception score and FID score are used for performance evaluation. Lossless compression metrics for GANs often give large bits-per-dimension, which may not reflect the true model performance. Wasserstein distance is a criterion of interest for generative models, but it is challenging to approximate. To address this, measuring lossy compression rates of deep generative models is proposed as a way to achieve a balance between different evaluation metrics. The text discusses measuring lossy compression rates of deep generative models to estimate the rate distortion function, which balances description length and fidelity of reconstruction. This approach allows for different tradeoffs and the use of perceptually valid distortion metrics like structural similarity (SSIM). The rate distortion curve can be computed by finding normalizing constants of unnormalized probability distributions over noise variables. This analysis generalizes the evaluation of log-likelihoods with Gaussian observation models. Annealed Importance Sampling is the most effective method for estimating log-likelihoods of generative models. Annealed Importance Sampling (AIS) is a method used to estimate log-likelihoods of generative models by interpolating between tractable and intractable distributions. It provides a stochastic lower bound on normalizing constants, producing an upper bound on the rate distortion curve. This approach allows for approximating the entire rate distortion curve at a similar computational cost to a single log-likelihood estimate. The study uses rate distortion approximations to analyze various generative models like VAEs, GANs, and AAEs. It reveals insights on their tradeoffs, showing that VAEs excel in lossless compression with larger code sizes but struggle in low-rate lossy compression. On the other hand, increasing GANs' capacity improves distortion at high rates without compromising quality at low rates, with code size impacting tradeoffs differently than depth. Different GAN variants with the same code size exhibit distinct effects on rate distortion tradeoffs. The study compares different GAN variants with the same code size, finding that code size is the main factor influencing performance differences. Annealed importance sampling (AIS) is a Monte Carlo algorithm that constructs intermediate distributions to estimate log partition functions. The Bidirectional Monte Carlo method uses forward AIS distribution as a proposal for estimating log Z. The Bidirectional Monte Carlo method uses forward AIS distribution as a proposal for estimating log Z, while running AIS in reverse provides an upper bound on log Z. The combination of AIS lower bound and upper bound is called BDMC, used for estimating log-likelihoods in implicit generative models. The BDMC gap measures the accuracy of partition function estimators. Shannon's rate distortion theory quantifies the trade-off between lossy compression rate R and distortion D. The rate distortion function R(D) defines the minimum bits per sample needed for lossy compression with average distortion less than D. It involves an optimization problem over the channel conditional distribution q(z|x) for data distribution p d (x). Generative modeling aims to learn a model distribution p(x) to approximate the data distribution p d (x) using latent variable z and a decoder network. Different models like VAEs and GANs handle this process in various ways, either explicitly parameterizing a conditional distribution or directly outputting x. In the context of generative modeling with VAEs and GANs, the focus is on evaluating lossy compression using coding schemes corresponding to trained generative models. The rate-prior distortion function is described as a variational upper bound on the true rate distortion function, modifying the standard rate distortion formalism to match generative model evaluation goals. The approach involves treating the VAE decoder as an implicit model and using squared error distortion or Gaussian negative log-likelihood as the distortion measure for both VAEs and GANs. The rate-prior objective adjusts the rate distortion formalism for generative models like VAEs, using KL divergence to describe the length of z. It upper bounds the standard rate and is tight when the variational output marginal equals the aggregated posterior. This approach has been used in algorithms like the Blahut-Arimoto algorithm. The rate-prior distortion function Rp(D) is defined as the minimum value of the rate-prior objective for a given distortion D. The optimization can be rewritten using Lagrange multipliers, allowing for independent optimization problems for each x. The rate distortion curve can be computed by sweeping over \u03b2. Rp(D) is non-increasing and convex, similar to properties of the rate distortion function. The rate-prior distortion optimization function Rp(D) is a non-increasing and convex function of D. Proposition 1b states that Rp(D) is a variational upper-bound on the rate distortion function R(D). The optimization has a unique global optimum expressed as q*. The geometrical illustration in Fig. 1a shows that Rp(D) curves are upper bounds on R(D), with Rp*\u03b2(D) tangent to both Rp(D) and a line with slope \u03b2 passing through the optimal solution. The rate-prior distortion function Rp(D) with NLL distortion of \u2212 log p(x|z) has properties such as the global optimum expressed as q*(c) and the relationship between rate distortion theory and log-likelihood. The relationship between rate distortion theory and generative modeling is highlighted by the optimal log-likelihood L* and corresponding optimal prior p*(z) found through solving the rate distortion problem. In a good generative model, the negative log-likelihood is close to the data entropy, and the variational gap between rate-prior distortion functions is tight. In this section, a method is presented to upper bound the rate distortion function R(D) using a single run of the AIS algorithm. A temperature schedule is fixed, and optimal channel conditionals and partition functions are used along the AIS Chain. The AIS Rate-Prior Distortion Curve is obtained by running multiple independent AIS chains for each data point. The AIS algorithm runs multiple independent chains to obtain the AIS distribution for each data point. The AIS rate-prior distortion curve upper bounds the rate-prior distortion function, and the AIS rate-prior distortion curve is estimated. The AIS distribution can be easily sampled from, but its density is intractable to evaluate. The estimated AIS rate-prior distortion curve upper bounds the true rate distortion curve in expectation. AIS is accurate for estimating partition functions. The AIS method is accurate for estimating partition functions, with AIS variance proportional to 1/M K for a large number of intermediate distributions. The Rate Distortion Tradeoff in the AIS Chain involves different values of \u03b2 corresponding to tradeoffs between compression rate and distortion in a generative model. \u03b2 = 0 indicates zero compression rate and high distortion. In the context of the Rate Distortion Tradeoff in the AIS Chain, when \u03b2 = 1, the optimal channel conditional is the true posterior of the generative model q(z|x) = p(z|x). As \u03b2 approaches infinity, the network prioritizes distortion over compression rate, leading to the optimal channel condition putting all its mass on z ML to minimize distortion. However, representing this delta function would require infinitely many bits, resulting in the rate going to infinity. Evaluation of Implicit Generative Models has been a challenge in the field, with measures like Inception score and Fr\u00e9chet Inception Distance being proposed. However, these methods have drawbacks such as sensitivity to pretrained ImageNet classifier weights and limited applicability to other domains. Another evaluation method, the Parzen window estimate, suffers from a large variance due to zero intermediate distributions. Another evaluation method for GANs involves measuring the generator network's ability to reconstruct samples from the data distribution. This metric is similar to the distortion obtained in the high-rate regime of a rate distortion framework. GILBO is another related work that allows direct comparison of VAEs and GANs without requiring a tractable posterior. Rate Distortion Theory and Generative Models have also been explored in similar works. The rate-prior distortion function is evaluated by fixing the neural network architecture and learning the distortion measure d(x, f(z)). In contrast, our rate distortion assumes a fixed distortion measure given by a trained generative model. We plot the rate-prior distortion curve for a specific generative model, not architecture. Additionally, we find the optimal channel conditional q*(z|x) using AIS, while previous work uses a variational distribution restricted to a variational family. See Appendix A for related works on compression schemes, distortion-perception tradeoffs, and precision-recall tradeoffs. In this section, rate distortion approximations are used to analyze the performance of different generative models like VAEs, GANs, and AAEs at various compression rates. Rate distortion curves for GANs trained on MNIST and CIFAR-10 are shown, with variations in noise vector dimension and decoder depth. Different code sizes were tested, including shallow and deep GANs with Gradient Penalty on MNIST. Increasing the code size in GAN experiments on MNIST and CIFAR-10 extends the curve leftwards, improving reconstruction ability. Increasing the depth pushes the curves down and to the left, enhancing distortion in high-rate and mid-rate regimes. This increased capacity enables the network to better utilize information in the code space. Increasing the depth or latent size in GAN experiments improves reconstruction ability and enhances distortion in high-rate and mid-rate regimes. However, in the low-rate regime, increasing depth does not improve distortion. VAEs achieve better distortions than GANs in the mid-rate to high-rate regimes, as VAEs are trained with the ELBO objective, encouraging good reconstructions. Increasing capacity reduces distortion at high rates but increases it in low rates. The performance drop of VAEs in the low-rate regime is due to the \"holes problem\" in their code space, where a large fraction is allocated to garbage images. This results in blurry samples and requires many bits to approximate the image manifold. In contrast, GANs do not face this trade-off and can train high-capacity models without sacrificing performance. The AAE was introduced to address the holes problem of VAEs. The AAE was introduced to address the holes problem of VAEs by directly matching the aggregated posterior to the prior and optimizing the reconstruction cost. AAEs can match the low-rate performance of GANs but achieve better high-rate performance due to their direct optimization of the reconstruction cost. They perform slightly worse than VAEs in the high-rate regime but slightly better in the low-rate regime, alleviating the holes problem to some extent. Log-likelihoods cannot distinguish different aspects of a generative model, such as the prior or observation model. The AAE addresses VAE's holes problem by matching the aggregated posterior to the prior and optimizing the reconstruction cost. AAEs can match GANs' low-rate performance but achieve better high-rate performance. They perform slightly worse than VAEs in the high-rate regime but slightly better in the low-rate regime. Log-likelihoods cannot distinguish different aspects of a generative model, such as the prior or observation model. Two manipulations damaging a trained VAE in different ways result in different behavior of the RD curves. The first manipulation alters the VAE's prior to a mixture prior, resulting in a \"poor\" generative model that generates garbage samples most of the time. The RD curves show that log-likelihood fails to distinguish between good and poor generative models, with similar performance at high rates but significant drop in performance at low rates. The RD analysis is not sensitive to the choice of metric, as seen in the comparison of GANs, VAEs, and AAEs with MSE distortion. Our second manipulation involves damaging the decoder by adding a Gaussian blur kernel after the output layer. This results in a drop in high-rate performance but an improvement in low-rate performance of the VAE. The RD curves provide a richer picture of generative model performance compared to scalar log-likelihoods. The experiments discussed above used pixelwise MSE as the distortion metric, but for natural images, more perceptually valid metrics like SSIM could be used. RD curves of GANs, VAEs, and AAEs on the MNIST dataset were compared using MSE on deep features of a CNN as the distortion metric. Qualitatively, the behavior of the RD curves with this metric closely matched that of pixelwise MSE. GANs with different depths and code sizes showed similar low-rate performance, with deeper and wider models pushing the RD curves down and to the left. VAEs generally had better high-rate performance but worse low-rate performance compared to GANs and AAEs. In this work, rate distortion approximations were studied for evaluating generative models like VAEs, GANs, and AAEs. Rate distortion curves provide more insights about the models than log-likelihood alone, with similar computational cost. VAEs with larger code size achieve better lossless compression rates but drop in performance at low-rate compression. On the other hand, expanding the capacity of GANs reduces distortion at high rates without compromising quality at low rates. Increasing the capacity of GANs by adjusting code size has different effects on rate distortion curves. Larger code size extends curves leftwards, while depth pushes them down. Different GAN variants with the same code size show similar rate distortion curves, indicating code size dominates algorithmic differences. Lossy compression provides a comprehensive view of generative model performance, offering insights for improvement. The use of compression terminology is justified by Shannon's fundamental result on rate. Practical compression schemes like bits-back encoding have been developed for lossless compression with generative models, achieving theoretical rates. However, there is currently no practical scheme for lossy compression with deep generative models. Other researchers have achieved variational rate distortion bounds for specific latent variable models using practical coding schemes. The variational posterior lacks a factorized form, making existing methods not directly applicable. The hope is that the variational approximation will lead to a practical lossy compression scheme. The work is related to incorporating a perceptual quality loss function in the rate-distortion framework. Incorporating a perceptual constraint in the rate-distortion framework improves reconstruction quality. The regularization term KL(q(z) p(z)) enforces matching the aggregated posterior to the prior. This optimization aims to minimize rate for a given distortion constraint. Incorporating a perceptual constraint in the rate-distortion framework improves reconstruction quality by matching the reconstruction distribution to the model distribution. The model balances rate and perceptual quality as the distortion constraint tightens, resulting in an elevated rate distortion curve. Existing evaluation metrics like IS or FID struggle to distinguish mode dropping from mode inventing behavior in generative models. In order to evaluate generative models, Lucic et al. (2018); Sajjadi et al. (2018) propose studying the precision-recall tradeoff. High precision indicates samples close to the data distribution, while high recall means the model can reconstruct any sample from the data distribution. Precision-recall curves help identify mode dropping and mode inventing behavior in generative models. The rate-prior distortion framework is an information theoretic extension of precision-recall curves, extending log-likelihood to rate distortion curves. In our framework, mode dropping affects high-rate distortion performance, while mode inventing impacts low-rate distortion performance. Empirical studies validate our implementation and AIS estimates, showing close agreement with analytical solutions. The BDMC gap for VAEs and AAEs was 0.127 nats, and for GANs. The largest BDMC gap for VAEs and AAEs was 0.127 nats, and for GANs was 1.649 nats, indicating tight AIS upper bounds. The rate-prior objective is a convex function of the channel conditional distribution q(z|x), with detailed proofs provided in the appendices. The convexity of the rate-prior term E p d (x) KL(q(z|x) p(z)) and the distortion term E q(x,z) [d(x, f (z))] leads to a convex optimization problem for the rate distortion function R p (D). The convexity of R p (D) is proven by showing that the perturbation function of any convex optimization problem is convex. The distortion is a linear function of q(z|x), leading to a convex optimization problem for the rate distortion function. The unique global optimum can be found by rewriting the equation. The minimum is obtained when the KL divergence is zero, and the optimal channel conditional is q*. The AIS algorithm computes importance weights for chains and selects a chain index based on these weights. It then returns a value based on the AIS distribution. The distortion and rate-prior of AIS are defined for estimation purposes. The AIS algorithm computes importance weights for chains and selects a chain index based on these weights to return a value from the AIS distribution. The proof of Eq. 40 shows that log\u1e90 AIS k (x) estimates the log partition function, and Eq. 49 demonstrates that R AIS k (x) upper bounds the AIS rate-prior R in expectation. The code for reproducing all experiments will be publicly available. The paper will be open sourced publicly, using MNIST and CIFAR-10 datasets in experiments. VAE and GAN architectures were used with different code sizes. Training details include epochs, learning rate, and optimizer. Gradient penalty was used for stability. Code sizes of d \u2208 {2, 5, 10, 100} were used in deep architectures. In the experiments, different GAN models were used with varying code sizes. The architectures included Deep-GAN2, Deep-GAN5, Deep-GAN10, and Deep-GAN100. Various GAN models such as DCGAN, GP-GAN, SN-GAN, and BRE-GAN were tested on the CIFAR-10 dataset. RD curves were computed with one AIS chain for different \u03b2 values. In the experiments, different GAN models were used with varying code sizes. The architectures included Deep-GAN2, Deep-GAN5, Deep-GAN10, and Deep-GAN100. Various GAN models such as DCGAN, GP-GAN, SN-GAN, and BRE-GAN were tested on the CIFAR-10 dataset. RD curves were computed with one AIS chain for different \u03b2 values. The experiments involved using different \u03b2 values for 1999 points, with specific values for \u03b2 min and \u03b2 max depending on the model dimensions. The number of intermediate distributions varied based on the model dimensions, with different parameters used for HMC and batch sizes for image processing. The P100 GPU is used for adaptive tuning of HMC parameters in experiments. The HMC parameters are adaptively tuned in a preliminary run to achieve a 65% acceptance probability. The final results are obtained by fixing the tuned parameters and starting a new chain with a different random seed. The AIS with the HMC kernel shows robustness against different random seeds in approximating the RD curve of VAE10. The AIS with the HMC kernel is robust against different random seeds for approximating the RD curve of VAE10. Experiments were conducted to validate the implementation and accuracy of AIS estimates. Comparison with the analytical solution of rate-prior distortion optimization on a linear VAE trained on MNIST was done. The decoder of the VAE is a simple linear function, and the observation noise is assumed to follow a normal distribution for numerical stability. The text discusses the application of the Woodbury Matrix Identity to simplify the matrix inversion in the context of VAE, GAN, and AAE experiments on the MNIST dataset. It also evaluates the tightness of the AIS estimate by computing BDMC gaps at different compression rates, showing that the AIS upper bounds are tight. Additionally, it visualizes high-rate and low-rate reconstructions of MNIST images for VAEs, GANs, and AAEs with different hidden code sizes. The qualitative and quantitative results of VAEs, GANs, and AAEs on MNIST images with different hidden code sizes are shown in Figures 10 and 11. High-rate reconstructions (\u03b2 max) for VAEs, GANs, and AAEs are visualized, with \u03b2 max values specified for different dimensional models."
}