{
    "title": "Byx4NkrtDS",
    "content": "The text discusses the importance of navigation in animal behavior and the internal representation of the external environment. A recurrent neural network is trained to control an agent in navigation tasks, with a focus on internal representations. Pre-training shapes the attractor landscape of the networks, influencing performance in the Q-learning phase. Our results show that in recurrent networks, inductive bias takes the form of attractor landscapes shaped by pre-training. Non-metric representations are useful for navigation tasks, which require a correct internal representation of the world. Artificial navigation relies on SLAM, based on maps. Navigation in artificial systems involves reward acquisition and exploiting environmental regularities. Different tasks require different internal representations, so we train recurrent neural networks with various statistical regularities. We use a two-phase learning approach, pre-training the networks to emphasize internal representation before fine-tuning with Q-learning for specific tasks. The network's readout weights are adjusted for specific tasks while maintaining internal connectivity. Performance on navigation tasks varies based on network structure, with different attractors encoding priors for tasks. Combining networks with different biases improves multiple-task learning. Pre-training can significantly impact a network's dynamics and reinforcement learning for navigation tasks. Recent studies have focused on using recurrent networks for navigation, with some emphasizing position as the desired output. However, in this paradigm, the goal is to obtain rewards rather than report position. Recent approaches in navigation tasks focus on reward acquisition as the goal, utilizing deep RL directly with rich visual cues. This contrasts with the emphasis on position reporting in previous studies. The pre-training method used is similar to unsupervised, followed by supervised training. End-to-end learning has become a dominant approach in recent years. The work also relates to neuroscience studies on neural representations for navigation beyond spatial maps. The study focuses on the ability of a pre-training framework to manipulate network dynamics and internal representations for navigation tasks. It systematically explores different internal models and spatial regularities using a toy navigation problem inspired by the Morris water maze. The goal is to understand the type of cognitive map expected in navigation tasks. The study involves a square arena where an agent must find a reward location based on input from neighboring positions. The agent is controlled by a RNN and moves in cardinal directions. The RNN outputs values for actions to update the agent's position. The study involves a square arena where an agent controlled by a RNN must reach a context-dependent reward location using sensory input and action feedback. Variants of the task introduce different statistical regularities like obstacles and bars in random positions. The agent's goal is to reach the reward location in the shortest time using only proximal input. The study involves a square arena where an agent must reach a context-dependent reward location using sensory input and action feedback. The tasks require finding a strategy to tackle uncertain elements and achieve goals, despite the simple game setting. The interaction between internal representation and statistical regularities is explored through different tasks. The approach taken focuses on end-to-end reinforcement learning with various hyper-parameters for different solutions. The study explores a different approach to reinforcement learning, emphasizing the importance of biological motivations in shaping an agent's cognitive abilities. Learning is divided into pre-training and task-specific phases, with pre-training focusing on modifying internal and input connectivity. The agent's actions are externally determined in a random arena environment. The study focuses on pre-training internal connectivity in reinforcement learning, emphasizing landmark memory and position encoding. Networks are trained with different hyperparameters to control these aspects. The objective function includes position, landmark memory, and action, with a regularizer term. Probability distributions are estimated from hidden states of the RNN. The study pre-trains networks with different hyperparameters to control position and memory encoding. A Q-learning algorithm with TD-lambda update is used for network outputs, with a focus on the readout matrix W o. Recursive least square method is employed for fast weight updates, leading to better convergence speed compared to stochastic gradient descent. The study analyzes test performance of networks on various tasks using principal component analysis. The first two components explain 79% of the variance, with the first component related to task difficulty and the second component indicating task nature. Negative coefficients suggest metric invariance in tasks like overcoming dynamic obstacles. The study examines network performance on different tasks using principal component analysis. Positive coefficients indicate topological invariance, with successful agents adapting strategies to overcome scaling and obstacle tasks. The ability to adjust trajectories and use shortcuts is crucial for task success. Other tasks vary between these extremes, incorporating elements of both topological and metric information. The study analyzes network performance on various navigation tasks, measuring metric and topological scores to evaluate different networks. Random networks show balanced performance, while PostNet networks lean towards the metric side and MemNet networks towards the topological side. The inductive bias from task agnostic pre-training influences network performance on navigation tasks. The underlying structures of different networks encode bias for specific tasks, with RNNs playing a key role. RNNs are nonlinear dynamical systems that contain fixed points and special areas of phase space to understand their dynamics. Trained networks converge to approximate fixed points but are not required to maintain the same position indefinitely. We detect areas of phase space with slow points in RNNs by driving the agent in the environment and relaxing dynamics towards approximate fixed points. MemNet has slower points compared to PosNet, while RandNet does not show slow points. The resulting manifold of slow points for PosNet is depicted in Figure 4C. Pretraining in PosNet creates a smooth representation of position along a manifold, while MemNet represents landmark memory as 4 fixed points. Despite the dominance of position representation in PosNet, landmark memory still influences it. The decoding accuracy of trajectories improves as the agent encounters walls, showing integration of path integration and landmark memory. Pretraining biases are implemented by distinct attractor landscapes. The pre-training biases in PosNet and MemNet create distinct attractor landscapes. PosNet encodes a metric representation of space modulated by landmark memory, while MemNet robustly encodes landmark memory but sacrifices position encoding. RandNet lacks clear structure and relies on short-term memory. PosNet smoothly encodes position on a manifold, while MemNet encodes memory with four discrete points. Memory information is present in PosNet, not just position. The analysis compared the memory information in PosNet with position encoding. Different networks were used, and a simple measure of representation components was employed. By driving the agent in an empty arena with a single wall, the variance explained by position and wall identity was determined using GLM. Results were similar to those from previous tasks but much quicker to compute. Correlating these measures with task performance revealed their correspondence to metric-topological aspects. The study showed that networks have different abilities to handle environmental regularities based on their dynamics. They proposed a hierarchical system with a representation layer and a selector layer to combine advantages from different dynamics. The selector layer assigns reliability values to modules and the most reliable module makes the final decision for the agent's control. Our work explores how internal representations for navigation tasks are implemented by the dynamics of recurrent neural networks. Pre-training networks in a task-agnostic manner can shape their dynamics into discrete fixed points or a low-D manifold of slow points, corresponding to landmark memory and spatial memory. These dynamical objects serve as priors for the network's representations during Q learning for specific tasks, enhancing its performance on various navigation tasks. Both plane attractors and discrete attractors are found to be useful in this context. In tasks outside of reinforcement learning, line attractors underlie network computations. A single recurrent network has a trade-off between adapting to different tasks due to attractor landscapes. Position requires a continuous attractor, while stimulus memory needs discrete attractors. Learning multiple tasks can be achieved by considering multiple modules optimized for different dynamical regimes. By considering multiple modules optimized for different dynamical regimes, a modular system can learn multiple tasks more flexibly than a single-module network. Pre-training alters network connectivity, resulting in connectivity between random and designed networks. Even untrained networks can perform tasks using Q-learning of the readout. Studying connectivity changes due to pre-training may improve the understanding of random network statistics. When testing the agent on a task, trials are conducted for each possible initial position with randomly initialized hidden states. The time taken for the agent to reach the target is measured and normalized by an approximate optimal strategy. This approach could potentially accelerate learning in other domains, similar to curriculum learning. The study compared two training approaches for an agent navigating an arena to reach a reward: classic deep Q learning and a method adapted from RDPG. The deep Q learning method outperformed the alternative in most tasks. The study compared deep Q learning and adapted RDPG for agent navigation tasks. Deep Q learning was found to be more effective and was used for benchmarks with LSTM and vanilla RNN. The end-to-end approach failed in topological tasks but performed well in metric tasks, except for the implicit context task. The critical question is whether networks trained end-to-end develop a representation for better performance in similar tasks. The study compared deep Q learning and adapted RDPG for agent navigation tasks, finding deep Q learning more effective. Pre-training networks showed better performance in subsequent Q-learning tasks, especially in the implicit context task. Recurrent neural networks behave differently in various phase spaces, with approximate fixed points influencing global dynamics. In the study comparing deep Q learning and adapted RDPG for agent navigation tasks, deep Q learning was found to be more effective. Recurrent neural networks exhibit different behaviors in various phase spaces, with approximate fixed points impacting global dynamics. Slow points can be detected in multiple ways, and stable fixed points can be simulated until convergence. The agent's movement in the environment is recorded to search for the relevant manifold. The study compares deep Q learning and adapted RDPG for agent navigation tasks, finding deep Q learning more effective. Recurrent neural networks show different behaviors in phase spaces, with approximate fixed points affecting global dynamics. Slow points are detected and stable fixed points simulated until convergence. The agent's movement in the environment is recorded to search for the relevant manifold, while decoding accuracy for position using PosNet is analyzed. Pre-training an LSTM network with similar parameters as PosNet1 and MemNet1 showed qualitatively similar behavior. MemNet exhibited slow regions instead of discrete points, suggesting discrete attractors may appear with longer relaxation times. Slow points revealed by relaxation were helpful in analyzing dynamics of different recurrent networks. Pre-training focused on decoding landmark memory or agent position, using various hyperparameter variants. The agent explored an empty arena with random actions and a probability p of changing direction. Table 1 displays protocols and hyperparameters for LSTM PosNet and MemNet. The networks were pre-trained with an l2 regularizer and a learning rate. The weights were taken from a standard reservoir computing literature. The weights for the networks in reservoir computing literature are crucial for training success. The results of combining PosNet 1 and MemNet 25 are obtained from a modular network. Connectivity and dynamics in the networks are influenced by the norm of internal connectivity, with an advantage to residing on the edge of chaos. The study analyzed the norms of matrices during pre-training, showing different trends for PosNet and MemNet. Singular value decomposition revealed a low-rank structure in the internal connectivity changes induced by pre-training, partially correlated to network inputs. This low-rank perturbation was not constrained during pretraining. The low-rank structure in network connectivity changes is linked to inputs, influencing dynamics and behavior. Matrix norms and low-rank components impact landmark memory and path integration trade-off. Removing leading ranks from \u2206W affects network encoding and performance, highlighting the importance of the low-rank component. Different networks like PosNet, MemNet, and RanNet develop diverse strategies for metric and topological tasks, with examples of their trajectories in basic, bar, and scaling tasks."
}