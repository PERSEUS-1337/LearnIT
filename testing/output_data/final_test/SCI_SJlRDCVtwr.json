{
    "title": "SJlRDCVtwr",
    "content": "The universal approximation property of neural networks motivates their use in real-world problems. Neural networks can efficiently learn complex patterns from large datasets using the backpropagation algorithm. Despite their widespread use, neural networks are not fully understood, leading to ongoing research on their interpretability. In contrast, topological data analysis (TDA) relies on algebraic topology and other mathematical tools to analyze complex datasets. In this work, a connection is made between Topological Data Analysis (TDA) and neural network training using a universal approximation theorem. Simplicial Complex Networks (SCNs) are introduced for regression tasks, offering an interpretable deep learning model. The performance of SCNs is verified on regression problems, showcasing their ability to approximate continuous functions with one hidden layer and a finite number of neurons. Neural networks can approximate continuous functions, known as the universal approximation property. Various theoretical universal approximators include multivariate polynomials and reproducing kernel Hilbert space (RKHS) with universal kernels. Gaussian process regression with an appropriate kernel can also approximate any continuous function with precision. Neural networks are preferred over multivariate polynomials and Gaussian processes due to practical limitations of the latter approaches. Polynomial interpolations may lead to overfitting and poor generalization, while Gaussian processes become computationally intractable for large datasets. Neural networks, with efficient gradient computation using backpropagation, can be trained on large datasets in a reasonable time and generalize well in practice. Topological Data Analysis (TDA) is a growing field in data analysis. Topological Data Analysis (TDA) is a growing field that uses statistical and algorithmic methods to analyze the topological structures of data known as point clouds. TDA methods assume a point cloud in a metric space with a distance between samples and build a topological structure to extract geometric information from the data. These models are not trained with gradient-based approaches and are limited to predetermined algorithms, which may be challenging in high-dimensional spaces. In this work, a novel class of neural networks called Simplicial Complex Networks (SCNs) is introduced, leveraging the geometrical perspective of Topological Data Analysis (TDA). SCNs preserve the universal approximation property and can be trained using a forward pass and backpropagation algorithm. Unlike conventional neural networks, SCNs do not require activation functions or architecture search. This contribution aims to develop deep models that are interpretable and robust to perturbations. The paper introduces Simplicial Complex Networks (SCNs), a novel class of neural networks based on Topological Data Analysis. SCNs are interpretable, robust to perturbations, and do not require activation functions or architecture search. The paper is organized into sections explaining SCNs, related works, experiments, limitations, and conclusions. The approach involves learning a subdivision of the input space into small simplexes and a piece-wise linear mapping for regression tasks. The simplicial approximation theorem allows the approximation of continuous functions using simplicial mappings. A simplicial complex is a set of simplexes satisfying specific conditions, and a simplicial mapping is a mapping between two simplicial complexes where the images of the vertices are preserved. The Barycentric Subdivision (BCS) method breaks a d-simplex into smaller simplexes of the same dimension. The Simplicial Approximation Theorem states that for two simplicial complexes X and Y with a continuous function f: X \u2192 Y, there exist large k and l, and a simplicial mapping g representing the k-th and l-th BCS of X and Y. Each application of BCS multiplies the number of simplexes by (d + 1)!. The Barycentric Subdivision (BCS) method multiplies the number of simplexes by (d + 1)! with each application. However, this approach is limited in practice as it subdivides spaces independently of data. In contrast, data-driven methods in Topological Data Analysis (TDA) such as Vietoris-Rips and Cech complexes are based on deterministic approaches using pair distances and closed ball criteria. The Barycentric Subdivision (BCS) method expands simplexes by (d + 1)! with each iteration, but it lacks data-driven features. In contrast, data-driven methods in Topological Data Analysis (TDA) like Vietoris-Rips and Cech complexes use pair distances and closed ball criteria. The modified BCS framework automates subdivisioning and can be integrated into machine learning algorithms. Algorithm 1 demonstrates generating a random simplex from the set of all simplexes in BCS, offering a unique perspective on simplex identification. This approach transforms BCS into a data-driven method, with Figure 2 illustrating the process. The Barycentric Subdivision (BCS) method expands simplexes in a data-driven approach by allowing arbitrary values for weights and subdivision depth. This modification automates subdivisioning and integrates into machine learning algorithms, offering a unique perspective on simplex identification. The Barycentric Subdivision (BCS) method allows arbitrary values for weights, removing deterministic restrictions. This enables learning weights through optimization. To make the process data-driven, simplexes are sampled based on data likelihood. Identifying the simplex containing a sample is done by sampling from data and using stochastic optimization for subdivision updates. The Barycentric Subdivision (BCS) method allows arbitrary weights for flexibility in optimization. Simplexes are sampled based on data likelihood, and identifying the simplex containing a sample involves a forward pass using convex combinations of vertices. The process iteratively updates the subdivision to locate the sample's simplex, allowing for parameter updates through backpropagation and gradient descent. The Barycentric Subdivision (BCS) method allows for flexible optimization with arbitrary weights. The process involves automatic subdivisioning using backpropagation and gradient descent. Any continuous function from a simplicial complex can be approximated with a simplicial mapping from BCSs of the input space to BCSs of the output space. In this section, SCNs are used to parameterize a class of simplicial mappings with trainable values on vertices of the input space subdivision. The mapping is defined linearly on the d-simplexes of the subdivision using evaluations at its vertices, resulting in a piece-wise linear simplicial mapping. Parameters of this mapping are optimized for function approximation or regression tasks. The output of the SCN at a given input x is calculated using the values of its mapping at the vertices of a d-simplex. SCN's output at x is calculated using the values of its mapping at the vertices of \u03c3 and added vertices during locating x. Simplicial mapping at the m-th added vertex is defined recursively using its preceding vertices. The value of the simplicial mapping at h m is defined as a convex combination of f(u i) with the same weights, added with a bias which is a function of h m. The simplicial mapping holds the universal approximation property as long as each b m is an arbitrary function of the h m. The simplicial mapping in an SCN is defined using a simple model for biases and weights. Parameters to be learned include bias function parameters and weights for input space subdivision. These parameters can be updated using backpropagation. The ordering of vertices for inputs to certain layers is not specified, referred to as the policy on vertex ordering. The architecture of an SCN is determined by specifying depth, network policy, and bias functions. Training involves taking the derivative of the loss function over mini-batches and updating parameters using gradient descent. After updating weights, a projection is used to ensure their summation is 1, which can be avoided by using logits and the Softmax function. Persistent homology methods are commonly used in TDA to extract robust features. These features are utilized in various architectures for tasks such as music tagging, multi-label classification, and time series classification. TopologyNet introduces an architecture using persistent homology for 3D biomolecular structures to extract suitable features. TopologyNet introduces a persistent homology method for 3D biomolecular structures to extract features for convolutional layers. A trainable layer is proposed by Hofer et al. (2017) to learn representations from the persistence diagram of the data, achieving state-of-the-art results in a specific task of social network graph classification. However, existing methods align with common neural network architectures, losing the TDA geometrical perspective. Single-model architectures like SCNs can be trained with forward and backward passes without specific persistent homology for extracting topological geometry from inputs. Persistent homology is utilized to extract topological geometry from inputs, allowing for generalization to other domains. Regression tasks are performed to assess the performance and complexity of functions that SCNs can learn, using mean squared error as the loss function. SCNs with fewer parameters can sometimes estimate functions better than neural networks with more parameters. An experiment demonstrates approximating the sum of three Gaussian functions with an SCN, showcasing the resulting simplicial mapping and learned subdivision. In a more general experiment, SCNs are compared to neural networks with fully-connected layers and ReLU activations in learning complex structures. The experiment highlights the limitations of specific activation functions for neural networks. SCNs can learn intricate structures even with constant parameters as bias functions. Models are trained using Adam Kingma & Ba (2014) with a learning rate of 0.001 and the same mini-batch size. The comparison is shown in Figure 4, with more details and a binary classification experiment on the MNIST dataset in the appendix F. To backpropagate through the SCN's architecture, only d out of d + 1 vectors and their function evaluations need to be stored. In training very deep SCNs, storing d + 1 vectors and their function evaluations is sufficient for gradient computation of network parameters. It is not necessary to store weights of the network and function evaluations at previous nodes. However, in some cases, bias functions for hidden nodes may output high norm vectors leading to non-smooth outputs and potential overfitting. In deep SCNs, weight drifting can occur when input weights converge to degenerate values, leading to redundant layers. To prevent overfitting, layers with bias values close to zero can help adjust the model's capacity. The assumption that inputs lie in a d-simplex is made for presentation purposes, but samples can actually lie in any given simplicial complex. In experiments, simple policies in SCNs show proper function approximation. SCNs are universal approximators even with random policies and fixed weights in one dimension. Sensitivity to bias function choice is higher than network policy. Topological data analysis methods used to create neural network architectures with universal approximation property. Topological data analysis methods are utilized to create neural network architectures with strong theoretical analysis. SCNs are seen as a step towards interpretable deep learning models, with experiments mainly on synthetic data. Future work includes exploring practical applications and using common neural network architectures as bias functions for continued backpropagation. The text provides a topological proof of an approximation theorem used in the main text. It introduces a simplicial map g that approximates f as desired, extending the definition of g for all elements of X. The text introduces a simplicial map g that approximates f, proving its continuity. For each x in X and simplex \u03c3 in X containing x, f(x) and g(x) lie in the simplex with vertices {g(v)}. Linear regression reformulation is discussed for an SCN without hidden nodes. Linear regression is formulated with a data matrix X and output vector y, minimizing ||Xw - y|| by representing coefficients as a convex combination of v0, ..., vd in matrix C. The linear regression problem can be reformulated with a data matrix X and output vector y, where coefficients are represented as a convex combination of vertices. SCNS does not require input separation into blocks for extracting outputs from previous layers. The linear regression problem can be reformulated with a data matrix X and output vector y, where coefficients are represented as a convex combination of vertices. SCNS does not require input separation into blocks for extracting outputs from previous layers. In an SCN, knowing the last d hidden nodes, their function values, and an array indicating the order of vertices removed during the forward pass is enough to extract all previous function values, hidden nodes, and weights. The text discusses the use of SCNs for linear regression, where coefficients are represented as a convex combination of vertices. SCNs do not require input separation into blocks for extracting outputs from previous layers. The experiments involved a binary classification experiment on MNIST as a proof of concept. The neural network used in the experiments had two fully connected layers with 300 and 2 hidden neurons respectively. The neural network used in the experiments had two fully connected layers with 300 and 2 hidden neurons respectively, with ReLU activations. The SCN model had a depth of 4 with 4 constant bias functions. A learning rate of 0.001 and mini-batches of size 100 were used. The network policy was random, assigning preceding nodes randomly to the next convex combination weights. Inputs for the experiments were in a 2-simplex for the sum of Gaussians experiment and in a 1-simplex for the one-dimensional case. An experiment on the MNIST dataset was conducted to demonstrate the practical use of SCNs. In a primary experiment on the MNIST dataset, SCNs were used for binary classification of zeros and ones. The data was reduced to 20 dimensions using PCA. Comparisons were made between SCN, logistic regression, and a neural network. Despite not performing as well as the neural network, adding a single hidden node to logistic regression improved accuracy by around 7%. Adding a single hidden node to logistic regression improved accuracy by around 7%, showing potential for enhancement in its architecture. Scaling up the SCN architecture to higher dimensional spaces is considered as a future direction."
}