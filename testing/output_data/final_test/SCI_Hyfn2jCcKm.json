{
    "title": "Hyfn2jCcKm",
    "content": "Recently, Approximate Policy Iteration (API) algorithms have achieved super-human proficiency in two-player zero-sum games like Go, Chess, and Shogi. These algorithms use a slow policy (tree search) and a fast policy (neural network) to iterate. However, the Rubik\u2019s Cube poses a challenge as it has only one solved state and episodes may not always end. Autodidactic Iteration is introduced as an API algorithm that addresses the issue of sparse rewards by training on a state distribution, allowing rewards to propagate. This algorithm can solve the Rubik\u2019s Cube and the 15-puzzle without human data, achieving a 100% success rate on randomly scrambled cubes. The Rubik's Cube presents unique challenges for AI and machine learning due to its large state space. While traditional methods have been developed to solve the cube, recent advancements have focused on computing the minimal number of moves needed to solve it. Developing reinforcement learning algorithms for the Rubik's Cube could offer insights into sparse-reward environments. The Rubik's Cube and its solutions are deeply rooted in group theory, raising questions about the applicability of machine learning to complex symbolic systems. It is part of a larger family of combination puzzles with varying dimensions and geometries, increasing complexity. God's numbers for the 4x4x4 cube are unknown, highlighting the challenges posed by larger cubes. The Rubik's Cube and its variations present challenges for machine learning due to their complexity. Deep reinforcement learning methods, such as Approximate Policy Iteration (API), are used to address these challenges. DeepCube utilizes API, ADI, and MCTS to train a DNN and solve cubes. Dual Policy Iteration has shown success in two-player zero-sum games like Go and Chess. Dual Policy Iteration (DPI) algorithms like AlphaZero and ExIt update policies using a fast neural network and a slow tree search. The fast policy is trained on data from the slow policy, improving it. DPI has solved the Rubik's Cube with reinforcement learning and is successful in environments with many states and few reward states. Long-term goals include applying DPI to tasks like generating sentences or code. In our work, we introduce Autodidactic Iteration (ADI) to address challenges in applying DPI algorithms like AlphaZero and ExIt to sparse-reward environments like the Rubik's Cube. ADI trains a neural network value and policy function through an iterative process, overcoming biases in the value function and ensuring convergence to the optimal policy. DeepCube, a solver for the Rubik's Cube, combines a neural network with Monte Carlo Tree Search (MCTS) to estimate optimal value functions and train a policy network. The Rubik's Cube, created by Erno Rubik in 1974, has gained worldwide popularity with various human-oriented algorithms for solving it. Algorithms for solving the Rubik's Cube have been developed since 1981 to find the upper bound of moves needed, known as God's Number. It was proven in 2014 that any valid cube can be solved optimally in 26 moves in the quarter-turn metric. Algorithms like Kociemba two-stage solver use group theory to solve the Rubik's Cube efficiently. Heuristic search algorithms like IDA* combined with pattern databases have also been successful. Recent advancements in compute power have enabled bidirectional brute-force search to find optimal solutions in up to 28 hours. There have been attempts to train deep neural networks for solving the Rubik's Cube. Recent advancements in compute power have enabled bidirectional brute-force search to find optimal solutions in up to 28 hours. Attempts have been made to train deep neural networks using supervised learning with hand-engineered features as an alternative heuristic. However, these algorithms take a long time to run and often fail to find solutions within reasonable time constraints. Evolutionary algorithms have also been used, but they can only reliably solve cubes up to 5 moves away from the solution. In contrast, the Rubik's Cube is solved using API, a general purpose reinforcement learning algorithm with various strategies such as stochastic policies, conservative approaches, and learned critics. Despite theoretical optimality guarantees, these algorithms rely on random exploration. Dual Policy Iteration BID33 algorithms like AlphaZero and Exit have achieved success in two-player zero-sum games by using two policies: a fast policy trained via supervised learning and a slow policy guided by the fast policy in tree search. These algorithms utilize a forward dynamics model, eliminating the need for random exploration and improving sample efficiency compared to traditional API algorithms. The ADI approach addresses the challenge of learning a stable value function with nonlinear approximation for combinatorial optimization, particularly in the context of the Rubik's Cube. It ensures unbiased training by sampling states to propagate the goal state's learning signal during training. This method is similar to AlphaZero and other Dual Policy Iteration algorithms, which have shown success in two-player zero-sum games. The Rubik's Cube consists of 26 smaller cubes classified by sticker count. Each sticker is uniquely identifiable based on the cubelet type and other stickers. Using one-hot encoding for the 54 stickers, we focus on one sticker per cubelet to reduce dimensionality. Ignoring redundant center cubelets, we store 24 possible locations for edge and corner cubelets, resulting in a 20x24 state representation. The Rubik's Cube environment consists of a set of 4.3 \u00d7 10 19 states S with one special state, s solved, as the goal state. Moves are represented using face notation, with each move corresponding to a letter indicating which face to rotate. A clockwise rotation is denoted by a single letter, while a letter followed by an apostrophe signifies a counter-clockwise rotation. The agent uses Autodidactic Iteration to train a network for solving the Rubik's Cube. This algorithm combines value and policy networks and utilizes MCTS. Unlike AlphaZero and ExIt, Autodidactic Iteration prevents divergence of the value function by sampling states during training. The sampling distribution starts from the solved state and takes random actions. The sampling distribution for training the deep neural network in Autodidactic Iteration starts from the solved state and weights cubes closer to the solved state more heavily. The network outputs move probabilities and values to guide the Monte Carlo Tree Search algorithm. Training samples are generated from the solved cube, ensuring positive rewards for shallow searches. Targets are created using a depth-1 breadth-first search from each training sample. The Autodidactic Iteration algorithm uses a depth-1 breadth-first search to estimate values for each child state. Training samples are generated by scrambling the cube multiple times, and targets are created based on the maximum value and reward of each child state. The Autodidactic Iteration algorithm uses a depth-1 BFS to estimate values and policies for each child state. Training involves setting value and policy targets based on maximal values and rewards, then training the neural network with RMSProp optimizer. Correct sampling of training data is crucial for convergence, especially for the Rubik's Cube. The Autodidactic Iteration algorithm uses a depth-1 BFS to estimate values and policies for each child state, training the neural network with RMSProp optimizer. Training on true value data can help prevent value function divergence, especially important for the Rubik's Cube. Autodidactic Iteration Initialization: \u03b8 initialized using Glorot initialization. Training the approximate value function on a subset of support states weighted by their proximity to the solved state to prevent divergent behavior. This novel state sampling procedure improved accuracy in predicting targets for states farther away. The novel state sampling procedure improved accuracy in predicting targets for states farther away during an asynchronous Monte Carlo Tree Search augmented with a trained neural network to solve the cube from a given starting state. The tree policy in the Monte Carlo Tree Search involves selecting actions iteratively from the root node until reaching an unexpanded leaf node. Virtual loss and exploration hyperparameters guide action selection, while preventing revisiting states. Upon reaching a leaf node, its children are added to the tree, with memories initialized and values computed for each child state. The Monte Carlo Tree Search involves selecting actions iteratively from the root node until reaching an unexpanded leaf node. The value is backed up on all visited states in the simulated path. Memories are updated for 0 \u2264 t \u2264 \u03c4. The maximal value encountered along the tree is stored, not the total value, due to the deterministic nature of the Rubik's Cube. The simulation continues until the solved state is reached or the computation time limit is exceeded. The tree T is extracted and converted into an undirected graph to find the shortest predicted path to the solution. Alternatively, the last sequence may be used, but it produces longer solutions. The DeepCube solver is compared against two other solvers: Kociemba and Korf Iterative Deepening A*. Kociemba relies on human domain knowledge and finds solutions quickly but may produce longer solutions. Korf's algorithm always finds the optimal solution but takes longer due to exploring many states. DeepCube is also compared against variants of itself. The Naive DeepCube and Greedy algorithms are compared to Kociemba using 640 randomly scrambled cubes. Kociemba solved all cubes in under a second, while DeepCube had a median solve time of 10 minutes. DeepCube matched or beat Kociemba in 55% of cases despite having higher variance in solution length. Additionally, DeepCube was compared against Naive DeepCube to evaluate the benefit of performing BFS on the MCTS tree, showing a slight but consistent improvement. Performing BFS on the MCTS tree provides a slight but consistent performance gain over the MCTS path, resulting in shorter solution lengths. Comparisons with Korf were not possible due to its slow runtime, so DeepCube's solutions were evaluated against Korf on cubes closer to solution. All solvers could reliably solve these cubes within an hour, and the length of solutions found by different solvers was compared. DeepCube performs consistently for close cubes compared to Kociemba and matches Korf's performance. DeepCube's median solve length is 13 moves, matching Korf's optimal path in 74% of cases. However, DeepCube struggles with a few cubes resulting in longer solutions. Korf's solver requires more nodes than our MCTS algorithm, which explores far fewer tree nodes. DeepCube's MCTS algorithm expands an average of 7,823 nodes, solving fully scrambled cubes faster than Korf. It addresses the issue of finding a terminal state in the Rubik's Cube environment by selecting a state distribution for the training set to propagate rewards. DeepCube uses a depth-1 BFS algorithm to update the policy, which is similar to TD(0) with function approximation. It ensures exploration through the selection of the starting state distribution in order to train a deterministic policy for solving the Rubik's Cube. Dual Policy Iteration can find a solution path in environments with large state spaces like the Rubik's Cube. Future work includes applying Autodidactic Iteration to other problems such as robotic manipulation and path finding. By combining neural networks with symbolic AI, algorithms can reason about problems instead of just using pattern recognition. DeepCube, utilizing neural networks and symbolic AI, can distill complex environments into knowledge and reason to solve problems. It teaches itself to reason and solve complex environments with reinforcement learning. DeepCube learned Rubik's Cube strategies similar to expert \"speed-cubers\", using patterns like aba \u22121 for manipulating cubelets effectively. DeepCube generated solutions for fully scrambled cubes by analyzing triplet frequencies, finding that aba \u22121 conjugations were most common. The distribution of frequencies showed conjugations appeared more often. Strategies learned by DeepCube prioritize completing a 2x2x2 corner halfway through the solution. The solver completes a 2x2x2 corner halfway through the solution, matching corner and edge cubelets before placing them in their correct locations. Advanced human \"speed-cubers\" employ a similar strategy. The network architecture for f \u03b8 uses elu activation on all layers except for the outputs, resulting in more efficient training. The neural network for f \u03b8 outputs a scalar value v and a vector p representing move probabilities. Trained using ADI for 2,000,000 iterations on a 32-core Intel Xeon E5-2620 server with three NVIDIA Titan XP GPUs. Implemented parallel MCTS with 32 workers to speed up solver 20x compared to single core."
}