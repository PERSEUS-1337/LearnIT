{
    "title": "HyEi7bWR-",
    "content": "Unitary Recurrent Neural Networks (uRNNs) have been developed to address the issue of vanishing or exploding gradients in Recurrent Neural Networks (RNNs). A novel update scheme using a skew-symmetric matrix and the Cayley transform is proposed to maintain orthogonal recurrent weight matrices. This approach overcomes limitations by scaling the recurrent weight matrix with a diagonal matrix of ones and negative ones. The proposed scaled Cayley orthogonal recurrent neural network (scoRNN) achieves superior results with fewer trainable parameters in experiments. The scaled Cayley orthogonal recurrent neural network (scoRNN) outperforms other unitary RNNs with fewer parameters. RNNs excel in sequential learning tasks but face challenges with vanishing or exploding gradients during training, hindering their ability to learn time-based dependencies in long input sequences. The current preferred RNN architectures, such as LSTMs and GRUs, introduce gating mechanisms to control information retention. The unitary evolution RNN (uRNN) maintains a unitary weight matrix throughout training and outperforms LSTMs on various tasks. Orthogonal and unitary RNN schemes have gained popularity and complexity, with methods to expand capacity to include all unitary matrices. In this paper, the focus is on expanding the capacity of unitary RNNs to include all unitary matrices. Different approaches like using Givens rotations and a gating mechanism have been explored. Methods involving complex valued recurrent weights are used to address the vanishing and exploding gradient problem. The network considers RNNs with a recurrent weight matrix from the set of all orthogonal matrices, parametrized with a skew-symmetric matrix through a scaled Cayley transform to avoid singularity issues. The method presented in this paper focuses on achieving superior performance on sequential data tasks using real matrices for unitary RNNs. The approach involves a simple gradient descent update with fewer trainable parameters and hidden sizes compared to LSTMs. The results indicate that state-of-the-art performance can be achieved without the complexity of optimization along the Stiefel manifold and using complex matrices. The RNN uses parameters W, b, V, and c with input sequence x and output sequence y. The hidden layer state h is recursively computed using an activation function. Orthogonal matrices have desirable properties for RNNs to prevent vanishing gradients. The use of orthogonal or unitary matrices in RNNs helps prevent vanishing and exploding gradients. BID0 introduces a parametrization of the recurrent matrix W using simpler matrices like diagonal matrices, Householder reflection matrices, Fourier transform matrices, and a permutation matrix to create a unitary product. BID21 presents the full-capacity uRNN with a multiplicative update step to reach all unitary matrices of order n by moving along the Stiefel manifold. The Cayley transform provides a bijection between orthogonal matrices without -1 eigenvalues and skew-symmetric matrices, allowing for parametrization in machine learning. The Cayley transform allows for parametrization of orthogonal matrices without -1 eigenvalues using skew-symmetric matrices in machine learning. Gradient descent algorithms like RMSprop or Adam can be used to train parameters, but difficulties arise when representing matrices with eigenvalues close to -1. Diagonal scaling can help overcome these challenges. The scaled Cayley transform allows for parametrization of orthogonal matrices using skew-symmetric matrices in machine learning. The proposed network, the scaled Cayley orthogonal recurrent neural network (scoRNN), is based on this theorem. The recurrent weight matrix W is parametrized through a skew-symmetric matrix A, resulting in n(n\u22121)/2 trainable weights. The scoRNN operates identically to the set of equations given in Section 2.1, updating the skew-symmetric matrix A using gradient descent algorithms during training. The scaled Cayley transform allows for parametrization of orthogonal matrices using skew-symmetric matrices in machine learning. The proposed network, the scaled Cayley orthogonal recurrent neural network (scoRNN), updates the skew-symmetric matrix A using gradient descent during training. The number of \u22121s on the diagonal of D, called \u03c1, is a manually chosen hyperparameter. Gradients of A are found by backpropagating through the Cayley transform, ensuring that A (k+1) will be skew-symmetric and W (k+1) will be orthogonal. The scoRNN and full-capacity uRNN can optimize an orthogonal recurrent matrix W using different update schemes. The scoRNN performs an additive update in the direction of steepest descent, proving more resistant to loss of orthogonality during training. It also maintains stable hidden state gradients with minimal computational costs compared to standard RNNs. The scoRNN architecture is similar to the standard RNN but with a three-layer process for applying the recurrent weight. The modReLU function, using real-valued functions and weights, outperformed other activation functions in the real case. It is defined as sign(z)\u03c3 ReLU (|z| + b), where b is a trainable bias. The modReLU activation function in scoRNN replaces the computation of ht with a specific formula. It allows for both positive and negative activation values, crucial for state transition in orthogonal RNNs. Initialization of parameter matrices, especially the recurrent matrix A, significantly impacts performance. A unique initialization method inspired by BID5 is used, setting most entries of A to 0 except for specific 2x2 blocks along the diagonal. The Cayley transform of the A matrix in scoRNN initializes eigenvalues uniformly along the right half-circle, reflecting \u03c1 of them across the imaginary axis with the scaling matrix D. Optimal hyperparameters were found using grid search for scoRNN, while other models used settings from BID21 and BID0 or underwent a grid search. This experiment tests an RNN's ability to reproduce a sequence seen many timesteps earlier, with 10 input classes represented by digits 0-9. The RNN is trained to output the first ten elements of a sequence after seeing a 'marker' class 9, with information propagation critical to avoid gradient issues. A baseline strategy is to output random elements from classes 1-8 after seeing the marker. Gated RNNs like LSTMs often converge to this strategy. The study compares the performance of different RNN models with varying hidden units. The best results were obtained with the scoRNN model using a specific parameter setting. The full-capacity uRNN and scoRNN quickly converged to zero entropy solutions, outperforming the restricted-capacity uRNN and LSTM models. The study examined the performance of different RNN models with varying hidden units. The scoRNN model showed smooth convergence, outperforming the baseline but slower than the full-capacity uRNN. A variation of the adding problem involved passing two sequences into the RNN, each with specific characteristics. The study compared different RNN models with varying hidden units. The scoRNN model outperformed the baseline but was slower than the full-capacity uRNN. Test set MSE results for different sequence lengths were provided. The networks showed improvement in MSE towards zero after a few epochs. The study compared different RNN models with varying hidden units, showing improvement in MSE towards zero after a few epochs. The best settings for the scoRNN were identified for different sequence lengths. Experiments were conducted on classifying samples from the MNIST dataset using pixel sequences fed into the RNN. The study compared different RNN models on the MNIST dataset, with experiments conducted on classifying pixel sequences. The scoRNN machines were trained with RMSProp optimization, varying learning rates for input, output, and recurrent parameters. Optimal values for \u03c1 were found to be n/10 for unpermuted MNIST and n/2 for permuted MNIST, reflecting different types of dependencies in each dataset. Each experiment used 55,000 training images and 10,000 testing images, with machines trained for 70 epochs. The study compared different RNN models on the MNIST dataset, with machines trained for 70 epochs. Test set accuracy was evaluated at the end of each epoch. The 170 hidden unit scoRNN performed similarly to the 512 hidden unit uRNNs with fewer parameters. The 2170 restricted-capacity uRNN had comparable performance to the 360 hidden unit scoRNN for unpermuted MNIST but performed worse for permuted MNIST. Orthogonal and unitary RNNs did not outperform LSTM in the unpermuted case. The 360 and 512 hidden unit scoRNNs outperform the unitary RNNs on permuted MNIST, with the 512 hidden unit scoRNN achieving a test-set accuracy of 96.6%. Speech prediction was also conducted on the TIMIT dataset using audio files downsampled to 8kHz and processed with a short-time Fourier transform. The machines used input data from amplitudes to predict the next frame in a sequence. Different models were trained with adjusted hidden layer sizes and optimized with Adam and RMSprop. The loss function used was mean squared error, showing that scoRNN models outperformed LSTM and unitary RNN models in MSE on validation and testing sets. The scoRNN models outperformed LSTM and unitary RNN models in MSE on validation and testing sets. The scoRNN predictions achieved better scores on the signal-to-noise ratio metric SegSNR, but slightly worse on human intelligibility and perception metrics STOI and PESQ. The recurrent weight matrix in the scoRNN architecture is parameterized with a skew-symmetric matrix through the Cayley transform. The full-capacity uRNN maintains a unitary recurrent weight matrix through a multiplicative update scheme, but rounding errors can cause it to become less unitary over time. In contrast, the scoRNN's orthogonality is not affected by roundoff errors, as shown in the experiment on the unpermuted MNIST dataset. The vanishing/exploding gradient problem is caused by rapid growth or decay of the gradient of the hidden state as we move earlier in the sequence. Hidden state gradients in the scoRNN and LSTM models were examined on the adding problem experiment with sequence length T = 500. LSTM gradient norms decrease steadily, while scoRNN gradients decay by less than an order of magnitude, remaining near 10 \u22122 for all timesteps. The scoRNN hidden state gradients decay slightly during 300 training iterations, allowing easy information propagation. The architecture is similar to a standard RNN but requires additional memory for the skew-symmetric matrix A. The recurrent weight matrix is formed from A using the Cayley transform, with minimal computational cost compared to forward and backward propagations. Real run-time comparisons with other models on the unpermuted MNIST experiment are included in TAB2. The models on the unpermuted MNIST experiment were compared in terms of speed. The LSTM model was the fastest, with hidden sizes not significantly affecting time per epoch. The scoRNN model with n = 170 was approximately 1.5 times faster than the restricted-capacity uRNN with n = 512, and twice as fast as the full-capacity uRNN with n = 116. The relationship between speed and hidden parameters was also observed in other models with similar parameter sizes. The scoRNN model with \u2248 137k parameters takes 11.2 minutes per epoch, compared to 25.8 minutes for the full-capacity uRNN model."
}