{
    "title": "rJxiXT2XcV",
    "content": "In this paper, a new logic-based framework for explanation generation is proposed. The goal is to explain decisions by identifying subsets of a knowledge base that entail a formula. Two types of explanations are defined, and a cost function is used to compare them. An algorithm for propositional logic is presented and evaluated in various scenarios. In the context of explainable AI, researchers are classifying ML algorithms for explainability and proposing new algorithms that prioritize explainability over accuracy. The term \"explainability\" is used broadly across different AI subareas. Researchers in the automated planning community focus on generating explanations for plans found by planning agents to make them easily understood and accepted by users. They address challenges such as inconsistent user models, learning user models, and different forms of abstraction. The common approach in these works is the use of automated planning methods. In this paper, the focus is on generating explanations for plans using a knowledge representation and reasoning framework. Two types of explanations are defined, and a logic-based algorithm is presented to compute these explanations. The performance of the algorithm is evaluated in various knowledge bases and planning domains. Our approach offers a generalizable solution for explanation generation in planning domains and knowledge bases. KB L represents well-formed knowledge bases, BS L denotes possible belief sets, and ACC L assigns acceptable sets of beliefs. This approach can be applied beyond planning problems as long as they can be modeled using a logical KR language. The set of propositional theories over P is denoted by KB L, BS L represents possible belief sets, and ACC L maps theories to their models. A negation operator \u00ac exists, with contradictory formulas \u03d5 and \u00ac\u03d5. Sub-theories and subsumption relationships are defined within theories. Rules in a rule system \u03a3 L have a form with body and head, stating that if the body is true, then the head is also true. Knowledge bases can derive conclusions using rules. In propositional logic, a proof from a knowledge base (KB) for a formula \u03d5 involves a sequence of rules where each rule's body is contained in the KB and the final rule's head is \u03d5. This proof is called a bounded planning problem, where a solution is found by creating a propositional formula representing the initial state, goal, and action dynamics for a specified number of time steps. This approach is used in classical planning problems. The formula represents a solution to a planning problem encoded as a conjunction of initial state, goal facts, action schemes, explanatory frame axioms, and complete exclusion axioms. A plan can be extracted by satisfying the formula with truth assignments for each time step. Explanation Generation Problem: Given two knowledge bases KB 1 and KB 2 and a formula \u03d5 in a logic L, the goal is to identify an explanation (a set of formulas) \u2286 KB 1 such that KB 2 \u222a |= \u03d5. The notion of a support of a formula w.r.t. a knowledge base is defined, where a \u2286-minimal support of \u03d5 is a support that has no proper subtheory. The text discusses the concept of minimal support and general support in explanation generation for knowledge bases. It defines model-theoretic and proof-theoretic explanations, using an example to illustrate the concept of minimal and general support. The text discusses m-explanations and p-explanations in the context of knowledge bases in a monotonic logic. It highlights cases where m-explanations may be insufficient and introduces the concept of p-explanations for persuading other agents about the correctness of their knowledge base. The text introduces p-explanations in the context of knowledge bases in a monotonic logic. It explains how a proof-theoretic explanation for a formula \u03d5 from KB 1 for KB 2 is a proof from KB 1 for \u03d5. The relationship between m-explanations and p-explanations is discussed, emphasizing the importance of sound and complete rule systems in identifying p-explanations. The text discusses the relationship between m-explanations and p-explanations in the context of knowledge bases in a monotonic logic. It introduces a preferred relation among explanations based on a cost function mapping pairs of knowledge bases and sets of explanations to nonnegative real values. The cost function C x L characterizes complexity measurements of explanations. It induces a preference relation over explanations, with preferred and strictly preferred explanations defined. Algorithms 1 and 2 compute most-preferred explanations for a formula \u03d5 and two knowledge bases KB 1 and KB 2 using the cost function C x L. The algorithms discussed in this section rely on an algorithm for checking entailment and computing minimal explanations with respect to a cost function and knowledge base. The implementation is focused on propositional logic and different cost functions, with a SAT solver used for entailment checking. Two algorithm implementations are discussed for finding minimal explanations. The algorithm discussed focuses on finding minimal explanations with respect to a cost function and a knowledge base. It utilizes a priority queue to prioritize potential explanations based on their costs and checks for valid m-explanations. If no valid explanations are found, the algorithm returns nil. The algorithm prioritizes minimal explanations based on cost and knowledge base, using a priority queue. It checks for valid m-explanations and returns nil if none are found. Algorithm 3 returns the most preferred m-explanation, while Algorithm 4 computes the most-preferred p-explanation or returns nil if none exists. The algorithm prioritizes minimal explanations based on cost and knowledge base using a priority queue. It checks for valid p-explanations and extends the proof if needed. If no valid p-explanations are found, the algorithm returns nil. Proposition 6 states that Algorithm 4 returns the most preferred p-explanation for a given formula from KB 1 to KB 2. The framework can be used to generate explanations for planning problems by encoding the formula and optimal plan into the knowledge base. The algorithm prioritizes minimal explanations based on cost and knowledge base using a priority queue. It checks for valid p-explanations and extends the proof if needed. If no valid p-explanations are found, the algorithm returns nil. Proposition 6 states that Algorithm 4 returns the most preferred p-explanation for a given formula from KB 1 to KB 2. The framework can be used to generate explanations for planning problems by encoding the formula and optimal plan into the knowledge base. In CNF clauses, the optimal plan of the specific planning problem is defined in terms of KB and plan optimality. An optimal plan explanation is given for a knowledge base KB and a plan \u03c0 n = a 0 , a 1 , . . . , a n\u22121. The formula \u03c6 that is sought to explain is that no plan of lengths 1 to n\u22121 exists, and that a plan of length n exists, making that plan an optimal plan. A second knowledge base KB 2 can be used to compute a model-or proof-theoretic explanation. The implementation of Algorithm 3 is empirically evaluated to find m-explanations on synthetically generated benchmarks in propositional logic. The algorithm was implemented in Python and evaluated using different cost functions on random knowledge bases with clauses in Horn form. The KBs were constructed by randomly choosing clauses from KB 1 and generating symbols for each KB. The algorithm prioritizes minimal explanations based on cost and knowledge base using a priority queue. The algorithm generates clauses using symbols from KB 1, with each symbol used at most once. The clauses have a premise and conclusion, and additional symbols are added as facts. The formula to explain is a randomly chosen conclusion. Runtimes and explanation costs increase with the size of KB 1. The costs of explanations increase as the search space expands, with costs dependent on KB 1 size. Experiments were conducted on BLOCKSWORLD using FAST-DOWNWARD BID2 to find optimal solutions. The planning problem is translated into a SAT problem with horizon h, forming KB 1. The KB 1 size affects the costs of explanations in BLOCKSWORLD experiments using FAST-DOWNWARD BID2. KB 2 is constructed by randomly selecting 25% of clauses from KB 1. The goal is to explain the absence of plans of lengths 1 to h-1 and the existence of a plan of length h. Results show similar trends to random knowledge bases, with runtimes for different cost functions being close due to only one valid explanation per problem instance. Larger problem experiments were omitted due to timing out after 6 hours. The curr_chunk discusses the broad area of explainable AI, focusing on related work from the KR and planning literature. It compares the notion of explanation to diagnosis, highlighting the difference in identifying reasons for inconsistency versus support for a formula. The curr_chunk discusses the development of explanation capabilities in knowledge-based systems and decision support systems, focusing on m-or p-explanations for a given formula to a second theory. It proposes the notion of an optimal explanation with respect to the second theory. The curr_chunk discusses the use of argumentation for explanation and compares it to the problem of computing a most preferred explanation. It is more general and does not focus on a specific question like previous works. The curr_chunk discusses the importance of human-aware planning and the need for explanations in planning processes. It highlights the use of argumentation over different models to incorporate human beliefs into the planning process. The necessity for plan explanations arises when the model of the agent and the model the human thinks the agent has diverge. An explainable planning agent must account for model differences and maintain an explanatory dialogue with the human to agree on the same plan, known as model reconciliation. The agent computes the optimal plan in his model and explains it in terms of model differences to move the human's model. The agent's explanations aim to align the human's model with its own by considering completeness, conciseness, monotonicity, and computability. The proposed approach enforces completeness and conciseness through cost functions but does not address computability. The model reconciliation approach aims to update the model of the human to ensure feasibility and optimality. It involves identifying explanations that are minimally complete and minimally monotonic, with all subsuming explanations also considered valid supports. Explanation generation is a crucial problem in the field of explainable AI. Previous work in automated planning domains has mainly utilized automated planning approaches. The model reconciliation approach focuses on updating the human model to ensure feasibility and optimality, considering minimal and monotonic explanations. Subsuming explanations are also valid supports in this context. In this paper, a logic-based framework for explanation generation is proposed, defining model-and proof-theoretic explanations and using cost functions to compare preferences. Empirical results show the approach's effectiveness beyond planning problems, with potential for more complex scenarios like persuading others of incorrect knowledge bases."
}