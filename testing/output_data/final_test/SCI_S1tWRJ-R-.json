{
    "title": "S1tWRJ-R-",
    "content": "The text discusses the importance of incorporating prior knowledge into learning to achieve good performance with small noisy samples. A framework for learning multiple tasks simultaneously is developed using a modular deep feedforward neural network with shared and private branches. This allows for commonalities and differences between tasks to be learned in a data-driven fashion. The architecture for learning through standard algorithms like stochastic gradient descent is discussed, focusing on meta-learning for domain adaptation and transfer learning. Numerical experiments show the effectiveness of learning in different setups, highlighting task-oriented representations in the network. Inductive learning requires regularity assumptions beyond the data, based on prior knowledge from similar problems. Incorporating prior knowledge into learning has gained success in deep learning schemes. Various aspects of prior knowledge are captured in meta-learning settings like multi-task learning. The core idea is shared representations to benefit from what has been learned from other tasks. In unsupervised and semi-supervised learning setups, datasets from different domains are compressed based on shared and unique features using a deep neural network. While joint representations can aid supervised learning, it is crucial for representations to include information about the output identity for optimal performance. Pre-training with unlabeled data may not always be beneficial. The proposed joint encoding-classification scheme combines labeled and unlabeled data for multiple tasks, allowing for the extraction of task-specific and shared representations. This work offers a modular setup for unsupervised, supervised, and transfer learning, enabling efficient end-to-end transfer learning with minimal labeled examples needed. Previous related work falls into generative and non-generative models for learning input representations. The Deep Domain Confusion (DDC) algorithm in BID21 focuses on unsupervised and supervised domain adaptation by optimizing classification accuracy and domain invariance through an adaptation layer and domain confusion loss. It aims to minimize the discrepancy between source and target representations to leverage prior knowledge effectively. Another algorithm in BID6 enhances deep learning with a domain classifier connected to the feature extractor. The Deep Reconstruction Classification Network (DRCN) in BID7 addresses unsupervised domain adaptation by jointly learning a shared encoding representation of the source and target domains. It balances classification loss of the source data with the reconstruction cost of the target data, allowing the target representation to benefit from the source supervised data. Additionally, theoretical papers have explored the benefits of this approach. Recent work in the generative approach includes extensions of Generative Adversarial Networks (GAN). The Coupled Generative Adversarial Network (CoGAN) framework aims to generate corresponding representations from different domains, while the Adversarial Discriminative Domain Adaptation (ADDA) approach focuses on adapting models for unlabeled target domains using a domain adversarial loss function. The model is adapted for the target domain using a domain adversarial loss function, similar to GAN setup. Various works focus on extracting shared and task-specific representations, requiring inputs of the same dimension. Multi-modal learning involves extracting meaningful features from different sources by maximizing correlation between learned representations. Multi-view representation learning algorithms rely on paired examples from two views, unlike transfer learning or domain adaptation which use unpaired samples. While GANs are powerful for multi-task learning, they can be difficult to train. This approach offers a non-generative perspective, allowing parallel training of multiple tasks in an end-to-end fashion. The approach described allows for unsupervised, supervised, and transfer learning within a single architecture. It can operate with inputs from different domains and is similar to BID4 in separating common and private branches, using multiple loss functions to optimize intermediate representations. Our framework focuses on penalizing reconstruction and classification errors, avoiding internal constraints and enabling multi-task learning. Unlike DSN, it can use labeled targets for supervised transfer learning. The proposed DSN architecture is computationally costly and struggles with small datasets, unlike our more efficient approach using joint autoencoders (JAE) for multi-task learning through unsupervised extraction. Joint autoencoders (JAE) is a method for multi-task learning by extracting shared and task-specific features. It utilizes unlabeled samples and a reconstruction loss function to improve reconstruction. Tasks are related, and similarities are exploited to enhance reconstruction, considering differences in aspects like color and grayscale images. The method of Joint autoencoders (JAE) for multi-task learning involves extracting shared and task-specific features using unlabeled samples and a reconstruction loss function. Weight sharing forces common branches to learn common features, while private branches capture unique features. The framework supports flexible sharing options. The framework for Joint autoencoders (JAE) allows for flexible sharing options, including sharing multiple layers or partially shared layers. The resulting network, called a JAE, can be trained with backpropagation on all reconstruction losses simultaneously. Inputs can be different, branches may have varying architectures, bottleneck sizes can differ, and more than one layer can be shared. Shared layers do not have to be bottlenecks. The Joint autoencoders (JAE) framework allows for flexible sharing options, including sharing multiple layers or partially shared layers. Weight sharing can occur between subsets of tasks and at different levels for different tasks. Additionally, the framework supports various learning scenarios, such as adding supervised losses to the autoencoders for each task. The framework supports various learning scenarios, including unsupervised domain adaptation, semi-supervised learning, transfer learning, and multi-task learning. Two strategies are described to improve supervised learning by exploiting shared features through common-branch transfer. The framework supports various learning scenarios such as unsupervised domain adaptation, semi-supervised learning, transfer learning, and multi-task learning. One strategy involves using shared features through common-branch transfer in supervised learning. Another approach combines supervised and unsupervised training by extending the JAE architecture with new layers and loss functions for each task. The network is trained using all losses simultaneously, making it suitable for transfer learning with non-uniform labeled sample sizes or semi-supervised learning with similar sample sizes. Our approach allows sharing weights in deeper layers of a neural net, while leaving the shallow layers un-linked. By forcing shared-branch nets to share deep weights, shallow layers must transform data from different domains into a common form. Sharing deep layers provides a performance boost for similar domains. Sharing deep layers in neural networks provides a performance boost for similar domains, with no price for relying on \"deep similarities\". Experiments in Keras over Tensorflow show advantages for different domains. Results on MNIST and CIFAR-10 datasets demonstrate improved unsupervised learning. Joint autoencoder outperformed baseline by 4% in L2 reconstruction error. The joint autoencoder (JAE) outperformed the baseline by 4% in mean squared error (MSE). Comparing autoencoders (AEs) to a JAE with the same total number of parameters as the baseline showed a 1.4% improvement. Visualization of activations showed shared layers were more mixed than private layers, indicating extraction of shared features. Digits reconstruction examples were displayed in FIG4. The Fisher criterion quantitatively shows the separation between private and shared branches in the joint autoencoder (JAE). Private branches capture fine details specific to each subset, while shared branches capture the general shape of the digit. Private branches have a higher variance compared to shared branches, indicating a difference in how they learn from the dataset. The joint autoencoder (JAE) outperforms private autoencoders in separating datasets, especially for visually similar image classes like deer-horses, showing a consistent advantage in performance. This improvement is not due to increased network capacity but rather the effective utilization of shared branches in capturing general shapes of the data. The JAEs outperform standard AEs by a factor of \u221a2 in terms of reconstruction MSE on CIFAR-10 objects. An extension of unsupervised JAEs to variational autoencoders showed promising results on the MNIST dataset, with a 12% improvement over VAEs of the same size. The study compared the performance of two JAE-based transfer learning methods on the MNIST dataset. The common-branch transfer method achieved 92.3% and 96.1% classification precision for different transfer tasks, while the end-to-end approach outperformed with 96.6% and 98.3% scores. The influence of shared layer depth on transfer performance was also investigated. The study compared two JAE-based transfer learning methods on the MNIST dataset, with the end-to-end approach outperforming the common-branch method. The influence of shared layer depth on transfer performance was also examined, showing that deeper layers improved performance for dissimilar tasks like MNIST-USPS. Early sharing is not possible when input dimensions differ. The end-to-end JAE-with-transfer algorithm was found to outperform other domain adaptation methods with little to no target samples. In transfer learning tasks involving USPS\u2192MNIST and SVHN\u2192MNIST, 2000 samples from MNIST and 1800 samples from USPS are used. The unsupervised JAE training utilizes both the source and target samples, with weakly-supervised performance studied using a small number of target samples. Results show that JAE achieves comparable or superior performance with limited supervision compared to existing methods. In transfer learning tasks, models with varying numbers of parameters are used for different domain adaptation tasks. The SVHN\u2192MNIST task is considered the most challenging, but good results are achieved due to the abundance of unsupervised training data. Knowledge transfer from the source to the target is demonstrated in the MNIST\u2192USPS transfer task. Target test improvement is attributed to source dataset reconstruction training. The test improvement by 2% is solely due to the source dataset reconstruction training passed to the target via the shared bottleneck layer. Multi-task learning is demonstrated by transferring knowledge from SVHN to MNIST and USPS simultaneously, resulting in 94.5% classification accuracy for MNIST and 88.9% accuracy for USPS. The smaller USPS dataset benefits relatively more from the presence of the larger tasks. The approach presented allows for transfer learning between multiple tasks by sharing representations for common aspects while maintaining private branches for task-specific features. It enables self-organization to solve tasks based on prior exposure to similar tasks, requiring standard gradient-based optimization for learning. The method allows for transfer learning between tasks by sharing representations for common aspects and maintaining private branches for task-specific features. It demonstrates efficacy in domain adaptation and transfer learning, showing improved functionality through modular prior structures imposed on the system. The training process utilizes simple learning rules with images scaled to [0, 1]. ADAM optimizer is used with specific parameters, and Keras default Xavier initialization is applied. Shared layers are indicated in red and connected bidirectionally. Various layer types and operations are defined, such as convolution, ReLU, max-pooling, fully-connected, and merge. SoftMax is used for outputs. For MNIST reconstruction experiments, a CNN-based autoencoder and JAE are employed. Mini-batch size is set to 256 with 10 epochs, and JAE losses are equally weighted. Transfer learning from SVHN to MNIST is performed with a mini-batch size of 64 and 10 epochs. The reconstruction losses are weighted lower than the classification losses to prevent early overfitting in a challenging classification task. Deconvolution layers with 2x2 upsampling are used, and the classification task is designed to avoid overfitting."
}