{
    "title": "ByloJ20qtm",
    "content": "Automated program repair using neural networks is a growing research area, focusing on variable-misuse bugs. A new approach combines localization and repair using multi-headed pointer networks, outperforming traditional methods. The availability of large corpora of source code has led to interest in developing neural representations of programs for program analyses. Different representations based on token sequences, program parse trees, program traces, and graphs have been proposed for tasks like repair, optimization, and synthesis. A recent study introduced the problem of variable misuse (VARMISUSE) to find and predict correct variables in program locations where misuse occurs. The programmer made a mistake by copying line 5 to line 6 without renaming the variable. BID0 proposed a solution using graph neural networks to predict correct variables in program locations with misuse. The corrected version of the program involves training a model on a synthetic dataset to predict the correct variable for each slot in a program. At inference time, the model generates predictions for each slot, highlighting high probability predictions that differ from the existing variable. The enumerative strategy for identifying VARMISUSE bugs has technical drawbacks, such as losing shared context among predictions and limitations in training with synthetic bugs at specific slot positions. During inference, the model encounters situations where the bug location does not coincide with the slot position. The enumerative approach for identifying VARMISUSE bugs has technical drawbacks, such as losing shared context among predictions and limitations in training with synthetic bugs at specific slot positions. The model encounters situations where the bug location does not coincide with the slot, leading to a drop in prediction accuracy of 4% to 14%. This mismatch between training and test distributions hampers the model's performance. The enumerative approach for identifying VARMISUSE bugs has drawbacks in producing one prediction per slot in a program. To address this, BID0 manually selects a threshold for reporting bugs based on predicted probabilities. However, setting a suitable threshold is challenging due to the risk of false positives and false negatives. To overcome these limitations, a new model is proposed that jointly learns program classification, bug localization, and repair. The key insight is that in a program with a single VARMISUSE bug, a variable token can only be a buggy variable or a correct occurrence. The text discusses a pointer model for identifying and repairing bugs in programs, based on the observation that a program with a bug likely contains repair information elsewhere. This model utilizes pointer networks to learn distributions over input tokens and classify the location of bugs in the program. The text introduces a joint prediction model using multi-headed pointer networks for classifying, localizing, and repairing bugs in programs. Results show higher accuracy and true positive scores compared to an enumerative approach. The model is compared to a graph neural network for program repair tasks. Limited to syntactic inputs, the pointer network outperforms the previous model by BID0. The text presents a neural network model that localizes and repairs faults in programs, outperforming a graph-based model in syntactic inputs. Future work will explore joint bug location and repair prediction, as well as comparing the model's performance with semantics included. Key contributions include solving the variable-misuse problem and demonstrating the effectiveness of pointer networks. The text discusses a neural network model that localizes and repairs faults in programs, demonstrating the effectiveness of pointer networks in solving the VARMISUSE problem. It contrasts this approach with an enumerative method proposed by BID0 and a neural model for semantic code repair proposed by BID5. Our model uses multi-headed pointers for classification, localization, and repair in programs. DeepFix and SynFix repair syntax errors using neural program representations, with DeepFix using an attention-based model and SynFix using a Python compiler for error localization and repair computation. In contrast, we use pointer networks for fine-grained localization and repair of variable misuses, addressing semantic bugs. The DeepBugs BID19 paper introduces a learning-based approach to identify name-based bugs by representing program expressions with a small set of features and training a classifier to predict buggy program expressions. Unlike previous approaches, our model embeds the full input program and performs localization and repair of VARMISUSE bugs in addition to classification. Our model implements a pointer mechanism for representing repairs that often requires pointing to variable uses in other parts of the program that are not present in the same buggy expression. Unlike the Sk p model, which needs to predict full program statements for repairing bugs, our approach can generate VARMISUSE repairs using a pointer mechanism. Additionally, it would be difficult for models like DeepBugs to predict repairs that include variables defined two or more lines above the buggy variable location. Automated program repair (APR) is a key area of research in software engineering. Traditional APR approaches differ from end-to-end learning-based solutions like ours in several ways. Our solution, along with other learning-based repair methods, does not require a correctness specification but learns to fix errors directly from source code examples. Our learning-based solution for automated program repair does not require a correctness specification, instead fixing errors directly from source code examples. It classifies, localizes, and repairs bugs, focusing on common error types to generalize across programs. The APR community has designed repair benchmarks like ManyBugs, IntroClass BID9, and Defects4J BID12 for evaluating techniques guided by test executions. Our learning-based solution for automated program repair focuses on common error types to generalize across programs. We use pointer-network models to predict bug locations and repairs in VARMISUSE bugs, training the model end-to-end on correct program examples. In this paper, the focus is on learning to localize VARMISUSE bugs in program source code. The VARMISUSEREPAIR problem is defined for identifying and fixing bugs in programs using pointer-network models trained on correct program examples. The VARMISUSEREPAIR problem aims to predict if a program contains VARMISUSE bugs or identify the location and repair tokens for the bug. It involves variables defined in the program and their uses, with the goal of fixing bugs using pointer-network models trained on correct program examples. The VARMISUSE bug location is identified by the token, while the repair token corresponds to the correct variable in the program. The example illustrates how variables, literals, and keywords are included in T, sources, object name, subject name, and result in V. Variable uses are defined as occurrences in a load context. The broader definition includes any variable use, load or store, for comparison with BID0. The pointer network model is defined using a trainable embedding matrix and LSTM to obtain hidden states for program tokens. A masking vector is used to consider only variable token states, and attention is performed over the hidden states. The pointer network model utilizes trainable projection matrices and attention mechanisms to compute probability distributions over program tokens, with the option of using masking on hidden states and attention values. Training is done on a dataset of synthetic Python programs containing both buggy and non-buggy code. The training process involves collecting source code for program definitions, creating binary vectors to identify bugs and correct variables, and ensuring a balance between buggy and non-buggy examples in the dataset. The training process involves balancing buggy and non-buggy examples in the dataset by setting a special token index for non-buggy programs. Different loss functions are used for training the location and repair pointer distributions, with the joint model optimizing the additive loss. Experimental evaluation includes three research questions and the use of two datasets for benchmarks. In our experimental evaluation, we address three research questions using the ETH-Py150 dataset, which contains 150K Python source files. The dataset is divided into training (100K files), validation (10K files), and test (50K files) subsets. We extract unique top-level functions from each partition, resulting in 394K (training), 42K (validation), and 214K (test) unique functions. Each function is analyzed for VARMISUSE slots and repair candidates, generating bug-free and buggy examples for each slot. More details on data generation can be found in the appendix. The enumerative model creates a smaller evaluation set by sampling 1K test files, resulting in 12,218 test examples. Bug-free and buggy examples are constructed per function by randomly selecting a single slot location. The evaluation results on the ETH dataset use this filtered evaluation set. The enumerative model accepts a program with a hole that identifies a slot, while the joint model accepts a complete program. Training, validation, and test datasets for the enumerative approach are constructed by inserting a hole at variable-use. MSR-VarMisuse dataset consists of 25 C# GitHub projects split into train, validation, seen test, and unseen test partitions. The seen test partition contains files from the same projects as train and validation, while the unseen test partition contains disjoint projects. ETH-Py150 dataset contains Python examples with function-level scope, while MSR-VarMisuse contains entire C# files with variable load and store uses. Repair candidates differ between the two datasets. The ETH-Py150 dataset is used for experiments targeting Python, while MSR-VarMisuse is used for comparison. The average number of repair candidates per slot is 9.26 in ETH-Py150 and 3.76 in MSR-VarMisuse. A joint model is compared to an enumerative repair model, with the enumerative approach creating variants of a program for each slot and predicting repairs using a trained model. The trained model M r predicts repairs for program variants, filtering predictions based on threshold value \u03c4 and maximum predictions k. Results show metrics like True Positive and Classification Accuracy. The table presents results on prediction permissiveness based on threshold value \u03c4 and maximum predictions k. Higher \u03c4 values lead to higher true positive rates but lower classification accuracy, while lower \u03c4 values improve localization and repair accuracy. The joint model achieves a maximum localization accuracy of 71% and localization+repair accuracy of 65.7%. The joint model achieves a maximum localization accuracy of 71% and localization+repair accuracy of 65.7%, showing a significant improvement in accuracy compared to other thresholds and k values. The network efficiently performs both tasks without explicit enumeration, maintaining high true-positive and classification rates. Additionally, the joint model is more efficient for training and prediction tasks compared to other models. The joint model achieves high accuracy in localization and repair tasks without explicit enumeration, showing significant improvement compared to other models. The effect of incorrect slot placement is quantified, with the repair-only model used to predict bug localization. Two datasets are generated to introduce VARMISUSE bugs away from the prediction problem slot. The study introduces two datasets, AddBugAny and AddBugNear, to analyze the impact of VARMISUSE bugs on model accuracy. Results show a significant drop in accuracy for both datasets, with the Near dataset being more adversarial. Repair predictions on unlikely fault locations can significantly impair repair in the presence of bugs, impacting the overall enumerative approach. The approach deviates from the state-of-the-art VARMISUSE model by using a pointer network on top of an RNN encoder and performing separate but joint bug localization and repair. The approach uses a pointer network for bug localization and repair on syntactic program information. Results show higher accuracy compared to a previous study, even with a smaller training dataset. A new dataset from software projects was collected for realistic evaluation. The study collected before-after pairs of function versions to identify variable misuses, marked as VARMISUSE bugs. A dataset of 41672 non-buggy examples and 4592 buggy examples was generated from 4592 snapshot pairs. The pointer model was trained on a dataset excluding files with buggy snapshots. The joint model achieved high accuracies in localizing and repairing variable misuses, with a true positive rate of 67.3% and a localization+repair accuracy of 15.8%. In contrast, the enumerative baseline approach had lower accuracies, with a true positive rate of 41.7% and a localization+repair accuracy of 4.5%. The approach presented in the paper focuses on jointly learning to localize and repair bugs, utilizing insights from the VARMISUSE problem. The joint model significantly outperforms an enumerative approach in predicting repairs for bugs in the original program. Future research aims to explore joint localization and repair using different models like graph models and combining pointer and graph models with more semantic information about programs."
}