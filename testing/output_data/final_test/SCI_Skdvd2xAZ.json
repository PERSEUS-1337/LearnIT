{
    "title": "Skdvd2xAZ",
    "content": "We leverage second-order optimization for neural networks to create a Kronecker factored Laplace approximation for weight posterior. This method allows for uncertainty estimation in production models without retraining. Compared to Dropout and diagonal Laplace, our approach provides better uncertainty estimates on out-of-distribution data and is more robust to adversarial attacks. It is computationally efficient, requiring only two square curvature factor matrices per layer. The Bayesian framework of statistics helps avoid overconfidence in neural network parameters by treating them as unknown quantities and integrating over all possible values. This is crucial for real-world applications where mistakes can have fatal consequences, such as driving a car or diagnosing a disease. In the Bayesian framework, the posterior distribution over neural network parameters is usually intractable due to their size and nonlinearity. Recent adaptations to modern architectures rely on approximations to make the posterior tractable, but these approaches assume independence between weights, leading to underestimation of uncertainty. The approach in BID6 for approximating the posterior distribution over neural network weights involves using stochastic regularizers not commonly found in recent architectures. By combining insights from second-order optimization and Laplace approximation with Kronecker factored covariance matrices, a computationally efficient matrix normal posterior distribution over weights is obtained. This method allows for obtaining uncertainty estimates from existing models after training. Our method is inspired by recent Kronecker factored approximations of the curvature of a neural network for optimization. We focus on the Hessian and denote a feedforward network with input x and output h L. The network parameters are optimized with respect to an error function E(y, h L) for targets y. The error function E(y, h L) for targets y is optimized in neural networks using second-order methods. Traditional methods use a curvature matrix C to generate parameter updates, but it is infeasible for modern networks due to their large number of parameters. Recent work exploits Kronecker factored diagonal blocks of curvature matrices for optimization. The Kronecker factorization in neural networks allows for efficient computation and storage of covariance and pre-activation Hessian matrices. This method reduces the size of matrices to be computed and stored, making it feasible for modern networks with a large number of parameters. Additionally, the inverse of a Kronecker product is equal to the Kronecker product of the inverses, simplifying the inversion process. The Kronecker factorization in neural networks simplifies computation by modeling diagonal blocks of weights and assuming independence between Q \u03bb and H \u03bb. Kronecker factored optimizers efficiently approximate E [H \u03bb ] by passing back expectations at every layer or utilizing the Fisher identity to propagate vectors instead of matrices. This approach reduces memory and computational requirements. A scalable Laplace approximation for neural networks involves using a stochastic rank-one matrix for each data point to approximate the diagonal blocks of the Hessian. The Laplace approximation is obtained by taking the second-order Taylor expansion around a mode of a distribution, which can be found using standard gradient-based methods for neural networks. The Laplace approximation for neural networks involves approximating the posterior over weights as Gaussian by integrating over a Gaussian functional form for \u03b8. The posterior mean for predicting on unseen data is approximated by averaging predictions of Monte Carlo samples from the approximate posterior. The diagonal of the Fisher matrix is used as an approximation for the Hessian matrix w.r.t. all weights jointly. The Laplace approximation for neural networks involves approximating the posterior over weights as Gaussian. Diagonal approximations to the curvature of a neural network have been successful for weight pruning and transfer learning. Modeling weights with a Normal distribution with diagonal covariance is common, but some approximations are needed for computational efficiency. Independent weights across layers are assumed, preserving sufficient curvature information for competitive optimization. The Laplace approximation for neural networks involves approximating the posterior over weights as Gaussian with diagonal covariance. The Hessian of every layer is Kronecker factored over the entire dataset, allowing for more efficient inversion and sampling. The Laplace approximation for neural networks involves approximating the posterior over weights as Gaussian with diagonal covariance. The Hessian of every layer is Kronecker factored over the entire dataset, allowing for more efficient inversion and sampling. The resulting posterior for the weights in layer \u03bb is calculated once and does not require approximation of E [H \u03bb ]. Augmenting the data, such as randomised cropping of images, may be advantageous. The Hessian decomposes into terms depending on data log likelihood and the prior, with L2-regularisation corresponding to a Gaussian prior. The Hessian is approximated by adding a multiple of the identity to each Kronecker factor from the log likelihood. The Laplace approximation for neural networks involves approximating the posterior over weights as Gaussian with diagonal covariance. The precision of the Gaussian prior on the weights can be optimized for predictive performance without retraining the network. Setting N larger than the dataset size can be seen as including duplicates of data points as pseudo-observations. Adding uncertainty to the precision matrix has a regularizing effect on the approximation to the true Laplace and the Laplace approximation itself. Recent approaches to approximating the posterior of a neural network involve formulating an approximate distribution and optimizing the variational lower bound. Some methods assume independence between weights, leading to underestimation of uncertainty. Dropout is used to approximate the posterior with a mixture of delta functions, while ensembles of networks are suggested for estimating uncertainty. A scalable approximation method involves a factorization of the covariance into a Kronecker product, resulting in a more efficient matrix normal distribution. The Laplace approximation method is compared to Dropout for uncertainty estimates in neural networks. The experiments use Theano and draw one sample per data point for the Laplace approximations. Hyperparameters are set using a grid search over 20 validation points. The regularised Laplace approximations provide a good fit to the HMC predictive posterior, with slightly higher uncertainty near the training data. The diagonal and full Laplace approximations require stronger regularisation compared to the Kronecker factored one. The full Laplace approximation overestimates uncertainty without regularisation, leading to a bad predictive mean due to underdetermined Hessian of the log likelihood. This is common in deep learning where the number of parameters is much larger than the data points. Restricting the covariance structure is crucial for computational efficiency and precise estimation of approximate covariance. A network with two layers is trained to classify MNIST digits using Dropout and L2-regularization. Images in the notMNIST dataset are classified using this network. We classify images in the notMNIST dataset using a network trained with Dropout. Uncertainty is compared by predicting digit classes through deterministic forward pass, sampling Dropout masks, and sampling weight matrices from matrix normal distribution. Additionally, a network with fully factorised Gaussian approximate posterior is used as a baseline. The model is trained on the variational lower bound using the reparametrisation trick with 100 samples for the stochastic process. Our model, trained on the variational lower bound using the reparametrisation trick with 100 samples, measures uncertainty through the entropy of the predictive distribution. Averaging probabilities of multiple passes yields higher uncertainty predictions compared to a deterministic pass. The Kronecker factored Laplace approximation makes few predictions with absolute certainty. The Laplace approximation assigns high uncertainty to most letters, performs similarly to Dropout, and makes accurate predictions on the MNIST test set. Variational factorised Gaussian posterior has low uncertainty. Bayesian models may be more robust to adversarial attacks compared to neural networks. For experiments on adversarial attacks, a fully connected net trained on MNIST is used. The sensitivity of different prediction methods is compared for two types of attacks. The untargeted Fast Gradient Sign method is employed, which adjusts the input based on the gradient of the predicted class probability. This method generates examples away from the data manifold. The Kronecker factored Laplace approximation shows higher uncertainty than other prediction methods as images move away from the data on the MNIST test set. Both diagonal and Kronecker factored Laplace maintain higher accuracy than MC Dropout. Deterministic forward pass is robust in accuracy but has lower uncertainty, confidently predicting false classes. Targeted attacks aim to force the network to predict a specific class, '0', excluding data points already predicted as '0'. The targeted attack using the Kronecker factored Laplace approximation shows higher uncertainty compared to other methods, but is more robust. It achieves 100% accuracy on the target class after almost 50 updates, while other methods are fooled after about 25 steps. The targeted attack using the Kronecker factored Laplace approximation exhibits smaller uncertainty than Dropout and is more robust to prediction changes. This method is applied to a state-of-the-art convolutional network architecture, specifically wide residual networks, showing competitive performance on CIFAR100. The study compares the Bayesian Dropout method to a new approach incorporating batch normalization into curvature backpropagation algorithms. The accuracy of prediction methods is similar, and the distribution of predictive uncertainty on the test set is compared in FIG5. The study compares the distribution of predictive uncertainty on the test set, focusing on correct and incorrect classifications. The network shows higher uncertainty on misclassifications regardless of training methods. Dropout and Laplace approximation increase uncertainty in predictions, but Kronecker factored Laplace approximation can be scaled to modern convolutional networks. The study compares the distribution of predictive uncertainty on the test set, focusing on correct and incorrect classifications. Dropout and Laplace approximation increase uncertainty in predictions, but Kronecker factored Laplace approximation can be scaled to modern convolutional networks while maintaining good classification accuracy. Stronger regularization was needed for Laplace approximation on wide residual networks due to the block-diagonal approximation becoming inaccurate on deep networks with a high parameter-to-data ratio. Further research is needed to observe the behavior of Laplace approximations on larger datasets like ImageNet. The Laplace approximation for the posterior of a neural network provides uncertainty estimates comparable to Dropout. It allows for obtaining principled uncertainty estimates from models trained in a maximum likelihood/MAP setting. Possible extensions include automatically determining hyperparameters and using model evidence for Bayesian model averaging on ensembles of neural networks. Active learning could be a challenging application for this approach. The text provides a derivation of the factorization of the diagonal blocks of the Hessian in a neural network, along with a recursive formula for calculating H. It explains the gradient and per-sample Hessian of a given layer, as well as the pre-activation Hessian. The Hessian can be calculated recursively using diagonal matrices and the first and second derivatives of the transfer function. The matrix normal distribution is a multivariate distribution over an entire matrix, parameterized by covariance matrices U and V, and a mean matrix M. Samples can be efficiently drawn from this distribution using a formula involving matrices A, Z, and B. The sample Z corresponds to a sample from a normal distribution of length np reshaped to a n \u00d7 p matrix, making calculations more efficient. While Q \u03bb's square root is calculated during the forward pass, H requires an additional backward pass. Approximating E [H] is not essential for the Kronecker factored Laplace approximation, as the curvature only needs to be calculated once. For large datasets like ImageNet, calculating the curvature for every data point individually would be impractically slow. By using data augmentation, the curvature of the network can be estimated, effectively increasing the dataset size. The minibatch approximation is used in experiments to demonstrate practical applicability. E [H] can be calculated exactly with a minibatch size of one using KFRA BID3. KFAC BID27 stochastically approximates the Fisher matrix and cannot calculate the curvature factor exactly. Adversarial experiments show the curvature per datapoint with and without data augmentation. The Laplace approximation with curvature estimated from data augmentation performs better than without augmentation, requiring less regularization and achieving higher uncertainty on adversarial attacks. It is particularly effective for large datasets. The Laplace approximation with curvature estimated from data augmentation is effective for large datasets and can be applied to minimize regularization and improve uncertainty on adversarial attacks. The difference between approximating the activation Hessian over a minibatch and calculating it exactly is negligible. The curvature factors correspond to precision matrices with parameters to estimate, growing linearly in the number of layers and quadratically in the dimension of the layers. Once calculated, the curvature factors only need to be done once and their Cholesky decomposition can be used. When sampling weights from the matrix normal distribution, the curvature factors are calculated once and their Cholesky decomposition is used to solve linear systems. Weight samples are reused for each minibatch to save computation time. Another approach is to sample a fixed set of weight matrices from the approximate posterior to create an ensemble of networks, which can be evaluated in parallel for faster processing. Additionally, distilling the Laplace network's predictive distributions into a smaller feedforward network can further speed up the process. The Laplace network can be distilled into a smaller feedforward network, as shown in BID0 using HMC for posterior samples. Different Laplace approximations are compared without hyperparameter tuning, showing high uncertainty away from the data. Regularization can optimize hyperparameters to match the true posterior uncertainty. In BID3, the Hessian of a neural network is often underdetermined due to a small number of data points compared to parameters. The Kronecker factored and diagonal approximations show smaller variance than the full Laplace approximation. The diagonal Laplace approximation places more mass in low probability areas, leading to higher variance in regression. Greater regularization is needed for the diagonal approximation to achieve acceptable predictive performance. This section presents accuracy values from different predictions. The accuracy values from different prediction methods on feedforward networks for MNIST and wide residual network for CIFAR100 are presented. Results for MNIST are in TAB0 and for CIFAR in TAB1. MC Dropout and Laplace approximation do not significantly affect classification accuracy. The wide residual network on CIFAR100 has n=3 block repetitions and a width factor of k=8, trained with specific hyperparameters for 200 epochs. Learning rate is decayed every 50 epochs. L2-regularization is used as in the original study. The authors made modifications to the architecture by using 2x2 convolutions instead of 1x1 convolutions for downsampling. They applied L2-regularization with a factor of 5x10^-4 and used a Laplace approximation for batch normalization parameters. Curvature backpropagation through convolutions required adjustments when calculating curvature factors. When backpropagating curvature for residual networks, instead of passing back two curvature matrices for each summand, the authors simply add the curvature matrices after each residual connection to avoid exponential growth in computation time/memory requirements."
}