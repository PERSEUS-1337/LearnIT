{
    "title": "H1eH_nsBlQ",
    "content": "We propose a multitask learning setup to reduce distant supervision noise by using sentence-level supervision. This method improves sentence encoding and identifies which sentences express the relationship between entities. A novel neural architecture combining attention and maxpooling increases AUC by 10% and outperforms previous results on the FB-NYT dataset. The development of small datasets with sentence-level annotations like ACE 2004, BioInfer, and SemEval 2010 was motivated by early work in relation extraction using fully supervised methods like BID2. Annotating text with relations is challenging, especially with a large number of relation types of interest. The distant supervision approach to relation extraction uses a knowledge base and text corpus to generate labeled sentences for training relation classifiers. Sentence-level supervision can help reduce noise in distantly supervised models by identifying relevant sentences expressing a relation. Various model architectures are experimented with to combine supervision methods. The paper proposes a multitask learning setup to reduce noise in distant supervision for relation extraction. They introduce a maxpooled attention architecture to combine evidence from different sentences mentioning an entity pair. The contributions include leveraging sentence-level annotations for supervision and releasing a library for relation extraction. The paper introduces a multitask learning setup to improve distant supervision for relation extraction. They utilize sentence-level annotations for supervision and provide a library for relation extraction as open source. The paper introduces a multitask learning setup to enhance distant supervision for relation extraction by utilizing sentence-level annotations for supervision. The paper introduces a multitask learning setup to enhance distant supervision for relation extraction by utilizing sentence-level annotations for supervision. It modifies neural architectures to incorporate sentence-level supervision, with a sentence encoder and bag encoder components. The model outputs a vector representing the sentence encoding and the probability that two entities are related given the sentence. The bag encoder predicts relations between entities by combining bag-level and sentence-level supervision in a multi-task learning framework. The model uses a maxpooled attention architecture to weigh sentences in the bag and predict relation types. The model uses a sentence encoder to translate word sequences into fixed-length vectors and a bag encoder to predict relations between entities. Input representation includes word embeddings and position embeddings for entity mentions in sentences. Improvements in sentence encoding include randomly initializing position embeddings around the mean value to facilitate learning. Adding a dropout layer before the sentence encoder reduces overfitting. Word composition is illustrated using a convolutional neural network with multiple filter sizes, different from previous work that used Piecewise CNN. In contrast to previous work using Piecewise CNN, the model described utilizes a sentence encoder to map sentences to fixed-length vectors and a bag encoder to produce fixed-length vectors for relation types. The sentence encoder emphasizes the middle segment of the sentence, while the bag encoder concatenates encoded sentences with entity embeddings to output probabilities for each relation type. The model learns parameters in white boxes, while gray boxes do not have learnable parameters. The model described in the current chunk utilizes sentence and bag encoders to compute sentence and bag-level annotations for relation types. The sentence encoder uses a CNN architecture to encode sentences, while the bag encoder combines information from multiple sentences to predict relation probabilities. The model learns parameters for sentence encoding, while bag-level annotations do not have learnable parameters. The model utilizes sentence and bag encoders to compute annotations for relation types. Maxpooled attention is proposed as a new form of attention that combines characteristics of maxpooling and soft attention. It learns weights for sentences to focus on important information. Maxpooled attention differs from soft attention in two key aspects. Firstly, each sentence is given a probability independently, unlike soft attention where sentences compete for probability mass. This is achieved by normalizing weights with a sigmoid function. Secondly, maxpooled attention uses weighted maxpooling instead of weighted average, which is more effective for picking useful features. Maxpooling is more effective for picking useful features from different sentences. The unnormalized maxpooled attention weight for s j is computed using entity embeddings, specifically pretrained GloVe vectors for improved coverage. The entity vector is the average of GloVe vectors of its words, improving results significantly. Better entity representation can further enhance performance. The model uses bag encoding g and entity pair encoding m to predict relations through multilabel classification. Training is done on bag-level labels obtained with distant supervision. The model is trained on bag-level labels obtained through distant supervision using binary crossentropy loss. Sentence-level annotations are also used to improve the model's ability to predict the usefulness of a sentence. The sentence encoder module improves encoding and attention weights for sentence contributions. Training includes sentence-level and bag-level losses with a weighted sum. Datasets, metrics, and model comparisons are discussed. In the multilabel classification setting, models are compared using precision-recall curves on the FB-NYT dataset. The area under the PR curve (AUC) is used for early stopping and hyperparameter tuning, focusing on high-precision extractions. The FB-NYT dataset lacks a validation set, so the test set is used for tuning, with 3-fold cross-validation in BID13. 90% of the training set is used for training, and the remaining 10% for validation, along with pretrained word embeddings. The model is trained on a training set with 90% of the data, using 300-dimensional GloVe vectors. OOV words are assigned random vectors. The model is implemented in PyTorch and AllenNLP BID6, trained on P100 GPUs, with each run taking five hours on average. Early stopping with patience = 3 is used, and Adam optimizer with default parameters. Each configuration is run three times. The model is trained using PyTorch with Adam optimizer and default parameters. Different configurations are run three times, and the PR curve for the best validation AUC is reported. The baseline model includes position embedding initialization, CNN sentence encoding, entity embeddings, and maxpooling. Additional features like maxpooled attention and sentence-level annotations are also tested. Comparison is made with models from literature, including those using attention mechanisms and soft labels during training. The model extends BID13 by using soft labels during training. The AUC of the baseline is comparable to previous work, but adding maxpooled attention and sentence-level supervision improves performance. Results show improvements with additional bags and sentence loss. Independent changes to the baseline configuration are indicated in Table 1. The improved results are a combination of additional supervision and model enhancements. Controlled experiments quantify the contribution of each modification. In Table 1, experiments were conducted to quantify the impact of different modifications on the model. Removing distance-based initialization of position embeddings led to a significant drop in AUC. Replacing the CNN encoder with PCNN BID27 resulted in inferior performance compared to CNN, contrary to previous findings. The experiments in Table 1 showed that removing distance-based initialization of position embeddings led to a drop in AUC. Comparing PCNN with CNN, our model performed better due to multiple filter sizes and improved entity position representation. Entity embeddings in the baseline model were found to be valuable for predicting relations, encoding entity type, compatibility, and bias. Different aggregation methods were compared, with maxpooling showing better performance than soft attention. Our proposed maxpooled attention combines the ability to learn different weights for sentences with the capability to pick out useful features from multiple sentences. It outperforms soft attention and traditional attention methods, especially in cases where the bag size equals 1. The comparison in Table 1 highlights the advantages of using maxpooled attention for sentence-level annotations. Our reimplementation of BID13 attention differs from the original paper, utilizing unnormalized attention weights for improved performance. The unnormalized attention weights of BID13 are implemented as a feedforward layer with output size = 1, yielding better results. Adding sentence-level annotations as additional bags has little effect, but integrating sentence-level supervision through multitask learning significantly improves performance. Supervision through multitask learning improves model performance by better filtering noisy sentences. Tuning \u03bb balances losses, with the right value impacting results significantly. Qualitative analysis shows model assigning weight differently from attention model. The term 'distant supervision' was coined to identify sentences in a text corpus where related entities are mentioned, using relation instances in a knowledge base. Researchers have extended this approach for relation extraction, addressing noise through multi-instance learning. Neural models have been used for relation extraction, with BID8 using multi-instance learning to develop graphical models for entity pairs. BID27 proposed a neural implementation of multi-instance learning for distantly supervised relation extraction, while BID11 addressed the limitation of neglecting information in multiple sentences mentioning an entity pair. BID11 addresses the limitation of neglecting information in multiple sentences mentioning an entity pair by maxpooling vector encodings. Results suggest maxpooling is more effective than attention for multi-instance learning. BID26 proposed leveraging dependencies between different relations in a pairwise ranking framework. The question of combining fully supervised and distantly supervised signals in relation extraction has been mostly ignored in the literature. In contrast to previous methods, our neural model combines both distantly supervised and supervised sentences for relation extraction. We propose two methods to enhance performance: incorporating sentence-level supervision and using maxpooled attention. These approaches aim to improve sentence encoding and reduce noise in the extraction process. The neural model combines distantly supervised and supervised sentences for relation extraction. Two methods are proposed to enhance performance: incorporating sentence-level supervision and using maxpooled attention. Experiments show a 10% improvement in AUC on the FB-NYT dataset."
}