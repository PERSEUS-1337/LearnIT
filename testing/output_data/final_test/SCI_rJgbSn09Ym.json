{
    "title": "rJgbSn09Ym",
    "content": "In this paper, a particle-based simulator is proposed for complex control tasks, addressing challenges faced by traditional rigid-body physics engines. By combining learning with particle-based systems, the simulator can adapt quickly to new environments with unknown dynamics. This approach allows robots to achieve complex tasks effectively. Robots can achieve complex manipulation tasks in dynamic environments using a particle-based simulator. Different objects exhibit distinct dynamics, posing challenges for traditional rigid-body simulators. Developing accurate forward dynamics models is crucial for robot learning in dynamic scenes. Developing accurate forward dynamics models is crucial for robot manipulation of real-life objects. A differentiable, particle-based simulator called DPI-Nets is proposed for complex control tasks, focusing on capturing dynamic interactions of particles. This approach can be combined with perception and control algorithms for robot manipulation of deformable objects. The DPI-Nets simulator enables gradient-based control algorithms for robot manipulation of deformable objects. It captures complex behaviors and allows robots to successfully manipulate objects with varying physical properties. The system reconstructs shapes from visual input and uses trajectory optimization to achieve desired outcomes. The particle-based representation provides a strong bias for learning dynamics. DPI-Nets utilize a strong inductive bias for learning dynamics, enabling quick adaptation to new environments. The model can characterize novel objects with unknown physical parameters and assist robots in successful object manipulation. Key features include multi-step spatial propagation, hierarchical particle structure, and dynamic interaction graphs that capture meaningful interactions among particles. Dynamic graphs allow neural models to focus on learning interactions, crucial for high simulation accuracy. DPI-Nets improve simulation accuracy and manipulation success rates by adapting to non-rigid objects. They outperform other interaction networks and are applied to control tasks for deformable objects and fluids. The model can handle scenarios with unknown physical parameters and has shown generalization ability in real-world experiments. Differentiable simulators for deformable objects have been less studied, with recent works proposing SPNets for position-based fluids and HRN for particle dynamics of deformable shapes. In contrast, we introduce state-specific modeling and dynamic graphs for accurate forward prediction across different states of matter, showcasing the use of learned dynamics models for control tasks. Our approach involves learning dynamics models for control tasks, which can be used in simulation and real-world scenarios. It complements recent work on interaction graphs and can be enhanced with a perception module for visual input. Model-predictive control with deep networks is utilized, focusing on abstract state transition functions for training policy networks. Our model extends previous approaches by learning a general physics simulator. Our model extends previous approaches by learning a general physics simulator that integrates into trajectory optimization algorithms for control. It can better generalize to novel testing scenarios and directly optimizes trajectories for continuous control. Compared to other methods using interaction networks, our dynamic particle interaction network simulates and controls multiple rigid bodies under partial observations. Our dynamic particle interaction network simulates and controls deformable, particle-based objects using dynamic graphs to handle complex object interactions. The physical system is represented as a directed graph where vertices represent objects and edges represent relations. The goal is to create a learnable physical engine to capture physical interactions using function approximators. The physical engine captures physical interactions using function approximators \u03c6. It predicts future states from the current interaction graph G t+1 = \u03c6(G t). Interaction networks (IN) perform object-and relation-centric reasoning about physics. INs define object and relation functions to model objects and relations in a compositional way. Propagation networks handle instantaneous propagation of forces, overcoming limitations of INs. Propagation networks were introduced to handle the instantaneous propagation of forces in the graph G by using multi-step message passing. They utilized fast training of RNNs to encode shared information and reuse it during propagation steps. The propagation process involves object and relation propagators, with the goal of accurately modeling various types of objects in physical simulations. The system extends existing models to allow particle-level deformation in object interactions. A dynamic graph is built on particles from all objects, with edges generated over time for efficiency. Relations are environment and task-specific, often considering neighbors within a set distance. Building a static, complete interaction graph is less efficient due to limited interactions between particles. In physical systems, particles interact with a limited set of neighbors. Dynamic graphs are used to handle discontinuous functions like contact. Hierarchical modeling is employed for long-range dependence, clustering particles into non-overlapping clusters with roots for efficient propagation. Hierarchical modeling is used for long-range dependence in physical systems, clustering particles into non-overlapping clusters with roots for efficient propagation. Specifically, a multi-stage propagation paradigm is employed, involving propagation among leaf nodes, from leaf nodes to root nodes, between roots, and from root to leaf for final prediction. The interaction graph and propagation rules on particles are defined for different types of objects, including rigid bodies and elastic/plastic objects. For rigid bodies, a hierarchical model is used to predict a rigid transformation, while for elastic/plastic objects, the particle state with the resting position is included to indicate the place of restoration after deformation. For fluid simulation, density and incompressibility are enforced by considering a small neighborhood for each particle. Hierarchical modeling is not needed for fluids, as edges are dynamically built to connect fluid particles to their neighbors. Good performance is achieved even when tested on data outside training distributions. Two directed edges are generated for interactions between different materials. Model-based methods offer advantages over model-free approaches, such as generalization and sample efficiency. When an accurate model is difficult to specify, a data-driven approach using neural networks to approximate dynamics becomes useful. Control inputs can be optimized by minimizing the loss between simulated and target results. Online system identification can be performed by minimizing the difference between model predictions and reality. An algorithm outline can be found in Section A. Model predictive control using shooting methods involves determining control inputs to minimize the distance between the actual outcome and the specified goal. A dynamic particle interaction network does forward simulation by taking the dynamics graph at time t as input and producing the graph at the next time step. Loss functions can be used to update control inputs through stochastic gradient descent. The shooting method in trajectory optimization uses loss functions to update control inputs through stochastic gradient descent. Model-Predictive Control (MPC) stabilizes trajectories by forward simulation and updating control inputs to compensate for prediction errors. DPI-Nets estimate attributes like mass and stiffness with SGD updates to minimize the distance between predicted and actual future states. The method is evaluated on four different environments with various objects and interactions. The environments for simulation results include FluidFall with falling fluids, BoxBath with flushing fluids, FluidShake with moving fluids, and RiceGrip with object manipulation using elastic and plastic deformation. Various parameters are varied for training and evaluation to test the model's generalization ability. The model learns interactions between a gripper and \"sticky rice\" in simulation environments like FluidShake and RiceGrip. Control tasks involve determining box speed and selecting grip configurations. The implementation details for dynamics learning in different environments are presented. In FluidFall, an interaction graph is dynamically built connecting particles within a certain distance. In BoxBath, the rigid cube is modeled using multi-stage hierarchical propagation. Directed edges are constructed between particles based on distance. In FluidShake and RiceGrip models, directed edges are added between particles based on distance. External particles represent walls or fingers, and edges are dynamically generated over time. The models use propagation networks for multi-stage effect propagation. Our DPI-Net model outperforms HRN BID19 in simulations across four environments (FluidFall, BoxBath, FluidShake, and RiceGrip). Detailed results and training information can be found in the sections provided. The model utilizes a 6-dim vector for particle velocity and resting position to handle deformations. Hyperparameters are consistent across environments, with DPI-Net performing equally well in environments without hierarchy. DPI-Net outperforms HRN BID19 in modeling fluids like BoxBath and FluidShake due to dynamic graphs. Comparison with baselines shows that Interaction Networks struggle with larger environments due to memory limitations and inefficiency. Dynamic graphs are crucial for modeling fluids effectively. DPI-Net outperforms HRN BID19 in modeling fluids like BoxBath and FluidShake due to dynamic graphs and state-specific modeling. HRN BID19 aims to learn a universal dynamics model for all states of matter, making it less effective in scenarios with multiple states. Without hierarchy, capturing long-range dependence becomes challenging, leading to performance drops in environments with hierarchical object modeling like BoxBath and RiceGrip. DPI-Net's performance is also highlighted in scenarios outside the training distribution, such as those with more particles. DPI-Net performs well on out-of-sample scenarios by leveraging inductive bias. Ablation studies test sensitivity to hyperparameters in RiceGrip, showing that fewer roots capture motion better. Longer propagation steps increase training difficulty. Larger neighborhood improves results but slows computation. The study justifies using different motion predictors for objects in various states, showing that state-specific predictors perform better than a unified model. They leverage dynamic particle interaction networks for control tasks in simulation and real-world scenarios, optimizing control sequences using a shooting method with model gradients. Online system identification is also utilized for further optimization. The study utilizes dynamic particle interaction networks for control tasks, optimizing control sequences with a shooting method using model gradients. Online system identification is used for further improvement. Comparisons with baselines show that the model-based control algorithm outperforms others significantly. The study demonstrates the effectiveness of dynamic particle interaction networks for control tasks, outperforming RL in real-world scenarios. The robot successfully manipulates deformable objects using DPI-Nets, adapting to unknown physical parameters. The learned policy in RiceGrip does not generalize well to real-world environments due to domain discrepancies. Our study shows that a learned particle dynamics model can approximate diverse object interactions, aiding in complex manipulation tasks. The system utilizes standard robotics and deep learning tools, with potential applications in household and manufacturing settings. Robot learning with particle-based representations offers generalizability and expressiveness, laying the foundation for dynamic scene understanding. The control algorithm involves updating gradients for simulation and forward propagation to improve performance. Our model's performance on fluids, rigid bodies, and deformable objects with a larger number of particles than in the training set is evaluated using MSE. The model shows good generalization to cases with twice as many particles as seen during training, indicating strong extrapolation performance. The gripper is randomly sampled within a circle of radius 0.5 with its orientation perpendicular to the line connecting the origin. The close distance is uniformly sampled between 0.7 to 1.0. 90% of the generated data is used for training, and 10% for validation. Models are implemented in PyTorch and trained using Adam optimizer with a learning rate of 0.0001. Batch size is 1, weights are updated every 2 forward rounds. Neighborhood d is 0.08, propagation step L is 2 for all environments.\u03c6 LeafToLeaf uses L = 2 for hierarchical modeling. In the FluidFall environment, propagation networks with different L values are used for LeafToLeaf, LeafToRoot, RootToRoot, and RootToLeaf interactions. The model is trained for 13 epochs to output 3D velocity for rollouts. In the BoxBath environment, hierarchical modeling with 8 roots for a rigid cube utilizes separate motion predictors for fluids and rigid bodies. The model is trained for 5 epochs. FluidShake environment uses a single propagation network and is trained for 5 epochs. In the RiceGrip environment, four propagation networks are used with 30 roots for \"rice\" modeling, trained for 20 epochs. N sample is set as 20 for control sequences evaluation based on Chamfer distance. In FluidShake, control sequence is box speed along x axis, optimized with RMSprop for 10 iterations using a learning rate of 0.003. Model-predictive control is then applied to the FleX. In the RiceGrip environment, grip configurations are optimized using model-predictive control with a learning rate of 0.003. The control sequence is applied to the FleX physics engine, with N fill chosen as 768. In the RiceGrip environment, grip configurations are optimized using model-predictive control with a learning rate of 0.003. The control sequence is applied to the FleX physics engine, with N fill chosen as 768. Physical parameters are estimated online, and RMSprop optimizer is used to optimize control inputs for 20 iterations before applying model-predictive control in the real world."
}