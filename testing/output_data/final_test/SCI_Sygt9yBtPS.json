{
    "title": "Sygt9yBtPS",
    "content": "Image paragraph captioning involves automatically generating descriptive sentences for images. Current deep learning models for this task consist of an image encoder and a language model decoder. However, the limitation of word-level guiding signals for the image encoder hinders success in generating long text. To address this, a Text Embedding Bank (TEB) module is proposed in this paper, utilizing the paragraph vector model to learn fixed-length feature representations for image paragraph captioning. The Text Embedding Bank (TEB) module benefits paragraph captioning by providing global supervision for visual feature extraction and distributed memory for the language model. It significantly improves performance on the Visual Genome dataset. This interdisciplinary task combines computer vision, natural language processing, and artificial intelligence, leveraging large datasets for advancements in the field. Recent works have shown promising results in generating high-level scenes for images and videos, but they lack the detail needed for real-world applications like video retrieval, medical report generation, blind navigation, and video subtitling. To address this, paragraph captioning has emerged as a task to provide coherent and detailed descriptions by capturing fine-grained entities. Some works have achieved high performance in paragraph captioning, particularly on the Visual Genome dataset. The Visual Genome dataset has pushed captioning performance to new heights, but paragraph-length caption models still lag behind single-sentence models. To tackle the challenges of paragraph captioning, a TEB module is proposed for integrating with existing image captioning models. This module maps paragraphs to fixed-length vectors with meaningful distances, aiding in fine-grained image understanding and long-term language reasoning. The TEB module integrates with image captioning models to improve image feature extraction and long-term language reasoning. It provides global supervision for better regularization of the image encoder during training, addressing the long-term dependency problem of RNNs. This integration was tested on the Visual Genome dataset for paragraph captioning. The state-of-the-art methods on the Visual Genome dataset for paragraph captioning achieved new state-of-the-art results. The Neural Image Caption (NIC) model uses deep neural networks with a pre-trained CNN as the visual model and a RNN as the language model to generate image captions. The model predicts one short simple sentence for each natural image by optimizing the difference between predicted and ground truth words using softmax with cross entropy loss. In Xu et al. (2015), an attention mechanism improved the performance of generating one-sentence captions for natural images. DenseCapJohnson et al. (2016) introduced a fully convolutional localization network to describe objects in images, utilizing visual features from localized regions in the RNN model. However, DenseCap lacks coherence in paragraph generation due to the absence of semantic relationships between sentences. Krishna et al. (2017a) explored dense video captioning by adapting the proposal module for events in videos. Recently, advancements in video captioning have included replacing RNN/LSTM language models with CNNs for parallel computing, introducing methods to reduce computation costs, and exploring paragraph captioning for generating fine-grained and coherent descriptions. Hierarchical recurrent neural network architecture is widely used in paragraph captioning to generate multiple sentences for video captioning by capturing strong temporal dependencies. Krause et al. (2017) uses a hierarchical recurrent network to build relationships between sentences, with regional features passed to a sentence RNN to generate topic vectors for controlling the ending of new topic generation. This hierarchical RNN and DenseCap offer two ways of generating new topics, essential for multiple sentence generation. The IU Chest X-ray dataset is used for automatic report generation using co-attention and hierarchical LSTM. The Diversity model improves sentence diversity with a repetitive penalty. The TEB module overcomes limitations in optimizing image encoding. GANs, like SeqGAN, improve real text generation by addressing sequential and discrete properties of text. The TEB module proposed by Yu et al. (2017) enhances paragraph captioning by describing rich content in images. It integrates with existing image captioning pipelines and utilizes word vectors for paragraph generation. The framework maps each word to a unique vector in a matrix W. The framework maps each word to a unique vector in a matrix W, indexed by the word's order in the vocabulary. The objective is to maximize the average log probability using a multi-class classifier like softmax. The framework is implemented in a neural network and trained using stochastic gradient descent through back-propagation. After training, the framework using recurrent neural networks can map words with similar meanings to close positions in the vector space. This allows for analogy questions answering through simple vector algebra manipulation. Inspired by word vector framework, the paragraph vector also contributes to predicting the next word. Each word is still mapped to a unique vector in the paragraph vector framework. The paragraph vector framework integrates with the word vector framework by mapping each word and paragraph to unique vectors in matrices W and D. These vectors are fused as features to predict the next word, with the paragraph vector acting as a super word representing the topic of the paragraph. This distributed memory model compensates for the lack of logical connections between sentences or paragraphs in recurrent neural networks. The TEB module consists of three boxes: a green box for visual feature extraction, a yellow box for paragraph generation using a RNN, and a red box for mapping paragraphs to a fixed-length vector called TEB. The TEB' is supervised by TEB through an L1 loss to regulate visual feature extraction. The visual features and TEB' are fed into the RNN for paragraph generation, supervised by the ground truth paragraph through a word-level loss. The TEB module integrates a paragraph vector as TEB for image paragraph captioning, with hyperparameters including a vector size of 512 and a sliding window size of 50. The paragraph vector model is trained for 1000 epochs before generating the TEB. Visual features are converted to the same dimension as TEB, and a weight of 0.1 is applied to TEB' in the concatenation with visual features. The TEB module is integrated with the Diversity model and a transformer model for image captioning. SCST and repetitive training are used. Results show quantitative improvements with the \"Diversity + TEB\" and \"Transformer\" models. Replacing LSTM with Transformer in the Bottom-UP and Top-Down model, the \"Transformer + TEB\" model includes the TEB module, improving baseline models significantly. The \"Diversity + TEB\" model achieves state-of-the-art results on visual genome data. The qualitative results show a man in a white basket, wearing a black shirt and hat, holding a hot dog. Other people are also present in the park. The curr_chunk describes a scene with a baby elephant walking on grass near water, with white tusks and a long trunk. The surroundings include trees, a calm river, and a hill in the distance. The elephant is grey and stands by the water, with yellow grass and short trees nearby. The curr_chunk describes a white toilet in a bathroom with white tiles, a white wall, and wires on the bottom left side of the toilet bowl. The toilet lid is up, and the toilet bowl is cleaning. Additionally, it introduces the Text Embedding Bank (TEB) module for visual paragraph captioning, providing global and parallel deep supervision for generating detailed image descriptions. The TEB module offers global and parallel deep supervision for fine-grained image understanding and long-term language reasoning, leading to significant improvements in state-of-the-art results."
}