{
    "title": "BJeYYeaVJ7",
    "content": "There is mounting evidence that pretraining is valuable for neural network language understanding models, but the impact of pretraining objectives on the linguistic information learned is not clear. Comparing four objectives - language modeling, translation, skip-thought, and autoencoding - shows that language models perform best in inducing syntactic and part-of-speech information. Language modeling may be the best pretraining task for transfer learning requiring syntactic information. A randomly-initialized, frozen model can also perform well on auxiliary tasks, but this effect diminishes with more training data. Representation learning with deep recurrent neural networks has transformed natural language processing by replacing expert-designed linguistic features. Researchers have explored the properties of representations learned by networks through training auxiliary classifiers with hidden states of pretrained models. Deep LSTM RNNs trained on tasks like machine translation can identify syntactic and semantic information in input sentences. This raises questions about how training tasks impact latent learning in models. In this study, the impact of training tasks on latent learning in models is investigated by varying the training task and amount of data while keeping the data source and model architecture constant. Models trained on tasks like English-German translation, language modeling, skip-thought, and autoencoding are compared using datasets from the 2016 Conference on Machine Translation. The study aims to determine how different tasks and data sizes affect the learning of syntactic properties in models. The study investigates the impact of training tasks on latent learning in models by comparing models trained on various tasks like translation, language modeling, skip-thought, and autoencoding. The focus is on evaluating syntactic properties through tasks like part-of-speech tagging and CCG supertagging. The study compares models trained on different tasks like translation, language modeling, skip-thought, and autoencoding. Bidirectional language models outperform other models for POS and CCG tagging, even with small training data. Randomly initialized LSTMs underperform trained models when using all labeled data, but perform significantly worse with reduced training data. The study compares models trained on various tasks like translation, language modeling, skip-thought, and autoencoding. Bidirectional language models excel in POS and CCG tagging, even with limited training data. Randomly initialized LSTMs perform better than trained models in predicting neighboring word identities from hidden states. This suggests that trained models excel in tagging tasks due to learning representations that align with POS and CCG tagging, rather than recovering neighboring word identity information well. The study examines how different tasks influence the learning of syntactic structures in machine translation models. Previous research has shown that translating into morphologically poorer languages can slightly improve encoder representations. Additionally, deep LSTMs have been found to learn hierarchical syntax across various tasks. By controlling for model size and training data, direct comparisons can be made between tasks in inducing syntactic structures. Transfer Learning of Representations in sentence-level pretraining has evolved from sentence-to-vector models to newer models incorporating LSTMs pretrained on data-rich tasks like translation and language modeling, achieving state-of-the-art performance. Skip-thought and InferSent techniques have been successful in training sequence-to-sequence models for predicting surrounding sentences, with InferSent showing improved performance with labeled data. Neural models for tasks like translation and language modeling have achieved state-of-the-art results. The performance of these models heavily relies on the amount of training data used. Studies show that for smaller datasets, statistical machine translation approaches outperform neural ones. Neural models perform well for image classification with small data amounts. Training auxiliary classifiers on randomly initialized RNNs follows reservoir computing tradition. Echo state networks have been used for speech recognition and language modeling. English-German dataset from 2016 ACL Conference on Machine Translation is utilized. In the same WMT shared task, 58 million ordered sentences are used to analyze the impact of training data volume on learned representations. Four corpus sizes are examined: 1, 5, 15, and 63 million sentences. Different sampling methods are used to create corpora of 1 million and 15 million sentences. Language model LSTM states are initialized with the final state after reading the previous sentence. Word-level representations are used, and vocabularies are limited to the 50k most frequent tokens in both English and German. In training models for language translation, vocabularies in English and German are limited to the 50k most frequent tokens. Models are trained using OpenNMT-py with specific modifications such as larger LSTMs, bidirectional encoders, and validation-based learning rate decay. Models include 1000D two-layer encoder-decoder LSTMs with 500D embeddings, trained with and without attention. Language models consist of a single 1000D forward model and a bidirectional model. All models are randomly initialized with a uniform distribution. The models are trained with OpenNMT-py using specific modifications like larger LSTMs and bidirectional encoders. Training includes validation-based learning rate decay and models are randomly initialized with a uniform distribution. Evaluation is done on the validation set every epoch, and the model with the lowest validation perplexity is selected for auxiliary task evaluations. Performance is measured in terms of perplexity and BLEU score. Translation uses beam search with a beam size of 5, and for CCG supertagging, CCG Bank is used. CCG supertagging provides detailed syntactic information for each word in a sentence, aiding in parsing. The dataset consists of 50k sentences with 1327 tag types. Smaller classifier training sets are created by sampling 10% and 1% of the sentences to study the impact of auxiliary task data volume. Truecasing and vocabulary restriction are applied using a truecase model trained on WMT. The word-conditional most frequent class serves as a baseline. The classifier for word identity task uses LSTM hidden state to predict word identity at a different time step. MLP classifiers with a 1000D hidden layer are trained for this task using WSJ data. The training procedure follows the same process as pretraining the encoders. In this section, we discuss the POS and CCG tagging results, showing that accuracies increase with more data for LSTM encoders. BiLM and translation encoder representations perform best for all pretraining dataset sizes and tasks, with BiLMs outperforming models trained on larger datasets. BiLMs consistently outperform translation encoders for transfer learning of syntactic information, with a significant performance gap in favor of BiLMs for both POS and CCG tagging tasks. This suggests that bidirectional context information is more crucial for identifying syntactic structure than part of speech. The best performing BiLMs and translation models are more robust to decreases in classifier data. When training on less auxiliary task data, POS tagging performance drops less than CCG supertagging performance. For the best model (BiLM trained on 63 million sentences), CCG accuracy drops 9 percentage points with 1% auxiliary task data, while POS accuracy only drops 2 points. Skip-thought models consistently underperform BiLMs and translation encoders but improve with more pretraining data. Skip-thought models without attention are similar to language models but have separate encoder and decoder weights. Language models share weights between encoder and decoder, making them regularized versions of skip-thought. Randomly initialized LSTM encoders perform well, only slightly behind BiLM on POS and CCG tagging. In the 1% classifier data regime, untrained encoders perform worse than all trained models and even the word-conditional most-frequent class baseline. The randomly initialized baseline excels in tagging tasks with ample auxiliary task training data by memorizing word configurations and associated tags. Untrained LSTM representations excel at capturing neighboring word identity information compared to trained models. Autoencoder models trained on autoencoding do not consistently improve with more training data, as they are prone to learning identity mappings. When trained on limited auxiliary task data, autoencoders outperform randomly initialized encoders. However, when trained on all auxiliary data, untrained encoders perform better than autoencoders. Autoencoders learn some useful structure in low auxiliary data regimes but do not capture syntactically rich features. In high auxiliary data regimes, random encoders outperform models that do not capture syntactically rich features. The second layer of an autoencoder without attention on POS tagging performs poorly. Randomly initialized embeddings perform as well as word-conditional most frequent class baseline on tagging tasks. Trained embeddings only consistently outperform randomly initialized ones with smaller amounts of classifier data. Trained embeddings consistently outperform randomly initialized ones, with the first layer performing better than the second in most models. This pattern holds even for untrained models, indicating that POS information is stored in the lower layer due to the deep LSTM architecture. For CCG supertagging, the first layer also outperforms the second in untrained models, but the behavior is mixed in trained models. The best-performing layer seems to be independent of absolute performance on the supertagging task. The results on word identity prediction show that randomly initialized LSTMs outperform trained models due to useful forgetting during training. Untrained encoders perform better on word identity prediction but worse on POS and CCG tagging, indicating trained models capture substantial syntactic features. The first layer outperforms the second layer in predicting immediate neighbors of a word for both trained and untrained models. Our findings suggest that depth in recurrent neural networks plays a crucial role in predicting distant neighboring words. Bidirectional language models (BiLMs) outperform translation and skip-thought encoders in extracting syntactic information for POS tagging and CCG supertagging, even with less training data. Our results show that BiLMs consistently outperform translation encoders in syntactic information extraction across different data sizes. Untrained models perform well in neighboring word identity prediction, indicating that trained encoders struggle with tagging tasks due to memorization of word identity. Trained and untrained LSTMs store local and distant word information differently in their layers. Depth in LSTMs allows them to capture larger context information, with bidirectional language models like ELMo showing more useful features for transfer learning. Randomly initialized LSTMs can preserve input contents better than trained models. Further research is needed on models trained on different tasks and how training tasks affect syntactic information learned. In the word identity prediction task, randomly initialized LSTM encoders with up to 4 layers show different encoding patterns. Lower layers focus more on nearby neighboring word information, while upper layers focus on distant neighboring word information. Training on 1% of auxiliary task data yields best performance for POS tagging at 81.8% and CCG supertagging at 62.3%. The best overall performance is highlighted for each task and training dataset size."
}