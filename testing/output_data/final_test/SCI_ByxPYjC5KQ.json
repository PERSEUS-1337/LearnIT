{
    "title": "ByxPYjC5KQ",
    "content": "Generative Adversarial Networks (GANs) are popular for learning complex distributions, but their generalization properties are not well understood. This paper analyzes GANs' generalization in practical settings, showing that discriminators trained on discrete datasets with the original GAN loss lack generalization capability. A zero-centered gradient penalty is proposed to improve the discriminator's generalization by pushing it towards the optimal discriminator. Experiments confirm the theoretical analysis on synthetic and large scale datasets. Recent research has focused on improving the stability of GANs, with studies showing that GANs trained with Two Timescale Update Rule converge to local equilibria. However, the generalization of GANs at local equilibria is not extensively discussed. High capacity discriminators are shown to have better generalization capabilities in detecting lack of diversity and teaching the generator to approximate the target distribution. High capacity discriminators trained with the original GAN loss tend to overfit to mislabeled samples, leading to collapsed equilibria. BID3 measures generalization by estimating the number of modes in the model distribution. Experiments show more modes than training examples, indicating some level of generalization. Poor generalization is attributed to the mismatch between discriminators and the optimal discriminator. A zero-centered gradient penalty is proposed to enhance generalization in high capacity GANs. Our zero-centered gradient penalty improves the generalization capability of high capacity discriminators in GANs. It pushes the discriminator towards the optimal one, leading to convergence to equilibrium with good generalization. Comparing with other gradient penalties, our 0-GP shows superior performance on real/fake samples. Gradient penalties are commonly used in GANs literature to enhance stability and convergence. However, our work focuses on improving generalization capability through gradient regularization. Higher capacity discriminators are necessary for better approximation of the target distribution, as shown by previous studies. The tradeoff between generalization and discrimination in GANs has been explored, with the importance of discriminator set highlighted for guaranteed generalization. Our work focuses on enhancing generalization in GANs by utilizing rich discriminators while maintaining discriminative power. Various techniques have been proposed to prevent mode collapse in GANs, such as using mixed-batches and weak divergences. The introduction of Wasserstein GAN and the importance of divergences have also been highlighted in previous studies. The method prevents mode collapse by utilizing distributional information in a mini-batch. VEEGAN BID24 uses the generator's inverse to map data to the prior distribution, detecting mode collapse through mismatch. Our method helps GANs discover unseen regions of the target distribution, improving sample diversity. The training process in GANs involves bringing the generator's distribution closer to the real data distribution. The game reaches equilibrium when the two distributions match. An optimal discriminator leads to convergence of the generator's distribution to the real data distribution. The performance of a discriminator in GANs is crucial for the convergence of the generator's distribution to the real data distribution. If the discriminator is too good at distinguishing real and fake samples, the generator's learning is hindered. This mismatch between the discriminator's performance on training and held-out datasets can impact the convergence of the generator's distribution. The discriminator in GANs is pushed towards a specific distribution D* during training, regardless of the distance between distributions. The original GAN may not converge to the target distribution even with sufficient data, as the discriminator can overfit to mislabeled examples in the training dataset. The discriminator in GANs can overfit to mislabeled examples, leading to poor generalization and mode collapse behavior. Training on a larger dataset may not prevent overfitting, causing the discriminator to guide the model distribution towards real samples instead of the target distribution. This results in the need for many iterations to find the empirically optimal discriminator using gradient descent. The discriminator in GANs can overfit, leading to poor generalization and mode collapse. Limiting discriminator updates per generator update can prevent overfitting. An -optimal discriminator is defined to address the vanishing gradient problem. Goodfellow et al. proposed non-saturating loss for the generator to overcome this issue. An -optimal discriminator can be constructed as a one hidden layer MLP with fewer parameters than a shallow one. Even when the generator produces realistic samples, a perfect discriminator can be easily found using gradient descent. The experiment showed that an -optimal discriminator can be found using gradient descent. The norm of the gradient decreases as fake samples get closer to real samples. Alternating gradient descent with the same learning rate for discriminator and generator cannot maintain the optimality of the discriminator. In GANs trained with Two Timescale Update Rule (TTUR), the ratio between the learning rate of the discriminator and generator increases as the iteration progresses. In GANs, the discriminator can learn much faster than the generator, maintaining optimality. The gradient explodes when the fake datapoint approaches the real one, affecting the generator's parameters. The gradient explodes in the generator's parameters as the model distribution gets closer to the target distribution. This occurs due to the maximization of the discriminative power of the discriminator, especially when trained with TTUR, leading to persistent gradient exploding throughout the training process. Without TTUR, gradient exploding may occur sporadically but does not persist. The gradient exploding in the generator's parameters occurs when the sigmoid function used in neural network discriminators saturates, leading to vanishing gradients. This phenomenon is especially prominent near the decision boundary, causing mode collapse in the generator. The discriminator is updated using the average of gradients of real and fake datapoints in the mini-batch, with fake datapoints close to real ones causing the gradient to explode and dominate the update, resulting in mode collapse. To improve the generalization capability of empirical discriminators, we aim to push them towards the theoretically optimal discriminator D*. This involves ensuring that D* approaches 1/2 for all inputs from the real and generated distributions, leading to a decrease in discriminative power as the distributions become more similar. To achieve this, we enforce two requirements on the empirical discriminator: setting the gradient to 0 for sampled datapoints and ensuring consistency between real and fake datapoints. The discriminator's objective is to enforce two requirements: setting the gradient to 0 for sampled datapoints and ensuring consistency between real and fake datapoints. This is done to prevent gradient exploding and push the gradient towards 0 for every datapoint on every path in the distributions. The discriminator enforces two requirements by using a zero-centered gradient penalty. Finding a path from a fake to a real sample within the distribution supports is challenging. The objective involves a straight line approximation connecting a pair of samples. A larger \u03bb value pushes the gradient (\u2207D)x towards 0. The discriminator's \u03bb value controls the tradeoff between discrimination and generalization, with larger values pushing the gradient (\u2207D)x towards 0. BID13 proposed forcing the gradient w.r.t. datapoints to be 0 for GAN training convergence. An optimal discriminator D* exists for discrete datasets, found by gradient descent, but may have exploding gradients near real datapoints. Discriminators in FIG0, 2c, and 2d show poor generalization with small gradients on training datapoints and large gradients elsewhere. The one-centered GP (1-GP) enforces a Lipschitz constraint on the discriminator and prevents gradient exploding, making the original GAN more stable. It forces the norm of gradients w.r.t. datapoints on the line segment connecting x and y to be 1, ensuring convergence and stability in training. The discriminator trained with 1-GP can overfit to training data and lack generalization capability. BID13 proposed using zero-centered GP on real and/or fake samples (0-GP-sample) to make GANs convergent. This penalty is based on the convergence analysis for Dirac GAN, ensuring that the gradient of the discriminator w.r.t. fake datapoints is 0 when the generator distribution matches the real distribution. The GP prevents the generator from oscillating and ensures convergence. Discriminators trained with original GAN loss focus on specific regions, hindering the discovery of real datapoints. Training with Eqn. 6 helps balance this issue. The discriminator trained with Eqn. 6 balances between maximizing L and minimizing the GP, encouraging gradients in different regions of the real data space to have the same norm. This reduces mode collapse and allows for gradient norms to vary in different regions. The discriminator trained with Eqn. 6 balances between maximizing L and minimizing the GP, encouraging gradients in different regions of the real data space to have the same norm. This reduces mode collapse and allows for gradient norms to vary in different regions. The code for testing gradient penalties in preventing overfitting is available at https://github.com/htt210/GeneralizationAndStabilityInGANs. Results show that 1-GP helps improve generalization, but 0-GP-sample does not. We increased the number of discriminator updates per generator update to 5 to test the effect of GPs in preventing overfitting. GAN with our 0-GP can still learn normally and produce recognizable digits after only 1,000 iterations, confirming the effectiveness of our GP in preventing overfitting. Testing different gradient penalties on synthetic datasets showed that GANs with other GPs fail to learn the distribution and exhibit mode collapse, while GAN with our 0-GP is successful. GAN with 0-GP successfully learns the distribution and avoids mode collapse issues. It can generate data points on the circle with good generalization capability. In contrast, GANs without 0-GP exhibit mode jumping behavior, indicating poor generalization. GAN-0-GP behaves similarly to Wasserstein GAN by first learning the overall distribution structure before focusing on modes. Results on MNIST dataset and other synthetic datasets are provided in the appendix. After 1,000 iterations, GAN-0-GP shows robustness to hyperparameter changes and mode collapse. The code for the experiment is adapted from BID13 with \u03bb = 10 for all GANs. WGAN-GP's critic was updated 5 times per generator update. TTUR was used for convergence with learning rates of 0.0001 and 0.0003 for the generator and discriminator. Higher \u03bb values improve sample diversity, with \u03bb = 50 showing similar samples. More details and samples are available in appendix D. The study shows that higher \u03bb values improve sample diversity in GANs. When \u03bb = 50, similar samples are observed, indicating better generalization. GAN-0-GP outperforms GAN-0-GP-sample and WGAN-GP in producing high-quality samples across 1,000 classes. The proposed zero-centered gradient penalty addresses the poor generalization issue in GANs by guiding the discriminator and generator towards a generalizable equilibrium. The zero-centered gradient penalty proposed in the study improves generalization and convergence in GANs. Experiments confirm its effectiveness in enhancing stability and generalization. The study proposes a zero-centered gradient penalty to improve generalization and convergence in GANs. Experiments show its effectiveness in enhancing stability and generalization. In higher dimensional space, the probability of sampling close datapoints decreases exponentially, making it easier to separate real and fake samples. A MLP with 1 hidden layer is used as the -optimal discriminator, with weight matrices W1 and W2. The output is computed using the softmax function, with a becoming the output as k approaches infinity. The softmax function is used in the network, with a becoming a one-hot vector as k approaches infinity. The discriminator and generator are linear in a case where both datasets contain a single datapoint. The learning rate is the same for both, and the discriminator is -optimal at step t. The norm of gradient w.r.t. \u03b8 D decreases as t increases. The norm of gradient w.r.t. \u03b8 D decreases as t increases and vanishes when the two empirical distributions are the same. To maintain D's -optimality as x \u2212 y (t) decreases, \u03b8 D has to increase. The gradient w.r.t. \u03b8 G grows as the distributions become more similar. G learns faster than D due to the same learning rate \u03b1. The number of gradient steps for D to reach the next -optimal state increases as x \u2212 y (t) \u2192 0, leading to infinity. Gradient descent with fixed updates cannot maintain D's optimality. GANs trained with different gradient penalty on swissroll dataset, with GAN-1-GP showing better learning. In the experiment, GAN-1-GP showed better learning on the swissroll dataset compared to other GANs. However, GAN-1-GP had a bad gradient field pattern and was sensitive to changes in hyperparameters and optimizers. It also failed to learn the scaled-up version of the distribution. The ImageNet dataset with 1000 classes was used, with images of size 64 \u00d7 64. The experiment used code from BID13 and ResNets with 5 residual blocks for the Generator and Discriminator. All GANs had the same architectures and hyperparameters. The configuration for WGAN-GP5 was specified."
}