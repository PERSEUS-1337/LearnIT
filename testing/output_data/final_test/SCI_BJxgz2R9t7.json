{
    "title": "BJxgz2R9t7",
    "content": "Recent efforts in combining Representation Learning with Formal Methods, known as Neuro-Symbolic Methods, have led to applying neural architectures to solve combinatorial optimization problems. A neural framework is proposed in this paper to solve the Circuit Satisfiability problem, utilizing a rich embedding architecture and an end-to-end differentiable training procedure. Experimental results demonstrate the framework's superior generalization performance compared to NeuroSAT. Neuro-Symbolic methods combine classical symbolic techniques with Deep Learning to solve combinatorial optimization problems. By extracting common structures from problem instances, Statistical Learning can produce meta-algorithms that outperform hand-crafted algorithms. There are two main approaches to realizing this idea in practice. In practice, there are two main approaches to combining classical symbolic techniques with Deep Learning for solving combinatorial optimization problems. One approach involves importing the solver algorithm template and having Deep Learning learn optimal heuristics within this template, while the other approach allows Deep Learning to figure out the entire solution structure from scratch. The latter approach is attractive as it enables learning optimal decision heuristics and search strategies beyond the greedy strategy, but training such models can be challenging. Training models for solving combinatorial optimization problems using Deep Learning can be challenging. One alternative method proposed is using latent representations learned for binary classification of the SAT problem to create a neural SAT solver model. However, this approach may result in poor generalization and sub-optimal models. In response, a neural Circuit-SAT solver framework is proposed in this paper, which learns the entire solution structure from scratch. The proposed training strategy for building neural models for combinatorial optimization problems learns the entire solution structure from scratch. Unlike typical Policy Gradient methods, it is differentiable end-to-end and directly trains the model towards the end goal. This strategy includes an Explore-Exploit mechanism for better optimization, although it is not a Reinforcement Learning approach. Additionally, there is a shift towards using structure-aware architectures like neural graph embedding to represent problem instances, as classical architectures like RNNs or LSTMs may ignore the inherent structure in the problems. Neural graph embedding methodologies focus on propagating local information on a graph, especially for Directed Acyclic Graphs (DAGs) like Boolean circuits. The proposed rich embedding architecture considers the sequential propagation of information in DAG-structured problems. The proposed rich embedding architecture implements sequential propagation for DAGs, harnessing structural information in input circuits. Contributions include a general graph embedding architecture, a neural Circuit-SAT solver, and a training strategy that is end-to-end differentiable. Experimental results demonstrate superior performance in generalizing to new problem domains. Deep learning on graph-structured data is a hot topic in the Machine Learning community. Models can be categorized based on assumptions about the underlying graph structure. Some methods assume all datapoints share the same structure, while others allow for varying structures. Various methods like Spectral CNN, Graph CNN, and Graph Neural Network operate in both spatial and frequency domains. In this paper, the authors extend the single layer DAG-RNN model for DAG-structured data to a deep version with Gated Recurrent Units. They apply Machine Learning, specifically deep learning, to logic and symbolic computation, bridging the gap between Machine Learning and classical Computer Science. Some works have shown the effectiveness of neural networks in modeling symbolic expressions, while others have attempted to learn in this domain. Some researchers have explored using neural networks to model symbolic expressions and learn approximate algorithms for solving symbolic NP-complete problems. One approach incorporates the graph structure of NP-hard problems to develop efficient search heuristics, while another introduces the NeuroSAT framework for learning to solve the Boolean Satisfiability problem without biasing towards greedy search. In this paper, a deep learning framework is proposed for the Circuit-SAT problem, a more general form of the SAT problem. Unlike previous approaches, this model is directly trained to find SAT solutions without needing to see them in the training data. The framework is designed for DAG-structured data but can be applied to general graphs with an explicit ordering. The framework proposed in this paper is designed for DAG-structured data but can be applied to general graphs with an explicit ordering. It defines a Directed Acyclic Graph (DAG) and a reversed DAG, along with a d-dimensional vector function \u00b5 G defined on the nodes of the DAG. The notation \u00b5 G induces the DAG structure G and a vector function on it. An example DAG function with d = 3 is shown in Figure 1(a). G d represents the space of all possible d-dimensional functions \u00b5 G. The embedding function E \u03b2 maps d-dimensional DAG functions to a q-dimensional space, preserving the DAG structure. The fixed pooling function P aggregates the embedded DAG function in the new space. The pooling function P aggregates the embedded DAG function to produce an aggregated version. The classification function C \u03b1 is then applied to generate the final prediction output. The embedding of graph-based data into vector spaces has been a recent focus in Machine Learning. The proposed framework builds upon the DAG-RNN framework by incorporating ideas from GGS-NNs, Deep RNNs, and sequence-to-sequence learning. The proposed framework extends the DAG-RNN framework by incorporating ideas from GGS-NNs, Deep RNNs, and sequence-to-sequence learning. Assigning input feature vectors to nodes defines a DAG function in the framework. The update rule for the state vector at each node involves a GRU function applied on the input vector and the aggregated state of its direct predecessors. The aggregator function is a tunable deep set function with free parameters that is permutation invariant. The main difference from DAG-RNN is the use of GRU instead of simple RNN logic and a more complex aggregation logic. The proposed framework extends the DAG-RNN framework by incorporating ideas from GGS-NNs, Deep RNNs, and sequence-to-sequence learning. It uses simple RNN logic instead of GRU and fixed summation for aggregation. The state vector is computed for all nodes in one pass using an update logic equation. Reversed layers are introduced to process the input DAG in reverse order. This one-layer embedding can be seen as a generalization of GRU-NNs on sequences to DAGs. The framework extends DAG-RNN by incorporating ideas from GGS-NNs, Deep RNNs, and sequence-to-sequence learning. Reversed layers are introduced to process input DAGs in reverse order, providing valuable information for the learning task. Stacked L-layer versions with their own parameters can be sequentially applied to generate the final output. The framework extends DAG-RNN by incorporating ideas from GGS-NNs, Deep RNNs, and sequence-to-sequence learning. Stacked L layers can be sequentially applied T times in the recurrent fashion to generate the final embedding.\u03b2 = \u03b2 1 , ..., \u03b2 L , H is the list of parameters and Proj H : G q L \u2192 G d is a linear projection adjusting the output dimensionality of E stack. In experiments, letting T > 1 significantly improves model accuracy. The proposed framework, Deep-Gated DAG Recursive Neural Networks (DG-DAGRNN), improves model accuracy by allowing T > 1 during training. The Circuit Satisfiability problem is a fundamental NP-complete problem in Computer Science, where the goal is to find a satisfying assignment for a Boolean expression. The Circuit Satisfiability problem involves finding a solution for a Boolean expression, which can be represented as a DAG function \u00b5 G with nodes for variables and logical gates. The framework aims to learn a Circuit-SAT solver from data, ensuring that Boolean sub-expressions are not redundantly represented. The Circuit-SAT problem involves finding a solution for a Boolean expression represented as a DAG function. The approach can be binary classification or directly solving the Circuit-SAT problem. Unlike previous models, the focus is on solving SAT problems rather than classification. The model can be adapted for SAT classification. Learning to solve SAT problems is more challenging than classification. The NeuroSAT framework proposes a post-processing procedure to decode a solution from latent state representations. The NeuroSAT framework proposes a post-processing procedure to decode a solution from the latent state representations of Boolean literals. However, it is not optimized for actually finding SAT assignments. In contrast, this paper introduces a new strategy using the DG-DAGRNN framework to directly generate satisfying assignments for circuits if they are SAT. The paper introduces a new strategy using the DG-DAGRNN framework to generate satisfying assignments for circuits directly without needing to see any actual SAT assignment during training. The components of the training strategy include an embedding function and a classification function. The paper introduces a new strategy using the DG-DAGRNN framework to generate satisfying assignments for circuits directly without needing to see any actual SAT assignment during training. The components of the training strategy include an embedding function and a classification function. The solver or policy network is referred to as F \u03b8. The soft evaluation function R G is defined as a DAG computational graph that shares the same topology G with the circuit \u00b5 G, with And nodes replaced by smooth min function, Or nodes by smooth max function, and Not nodes by N (z) = 1 \u2212 z function. Smooth min and max functions are defined with temperature \u03c4. The soft evaluation function in the framework evaluates a soft assignment to circuit variables, with a value above 0.5 indicating a satisfying solution. The evaluator network, R G, has no trainable parameters and each graph example induces a unique evaluation network. The framework encodes logical operators using smooth min and max functions. The framework uses smooth min and max functions to encode logical operators, improving gradient back-propagation efficiency. The satisfiability function combines the solver network and evaluator network to produce a real number output representing circuit satisfaction. The loss function is defined to push the solver network towards higher satisfiability values. In practice, training on only satisfiable circuits speeds up the process as unsatisfiable circuits confuse the solver network. Including unsatisfiable examples can be done for unsupervised learning if the model has enough capacity. The loss function has higher gradients for satisfiability values close to 0.5. The gradient vector in backpropagation is dominated by examples close to the decision boundary, pushing easier examples to the SAT region with a safety margin. The learning scheme is a variant of Policy Gradient methods, with the solver network as the policy function and the evaluator network as the reward function, where the reward function is fully known and differentiable. The reward function in the mathematical form is fully known and differentiable, allowing training using backpropagation to maximize total reward. Smooth min and max functions are used in the evaluator network to ensure gradients flow back through the active path, forcing the solver network to adapt accordingly. To improve training efficiency, smooth min and max functions are used in the evaluator network to allow gradients to flow through all paths in the input circuit. A high temperature value is initially used to explore all paths, gradually annealing towards 0 as training progresses. At test time, non-smooth versions of the functions are used for prediction. Our model evaluates a test circuit \u00b5 G using S \u03b8 (\u00b5 G ) = R G F \u03b8 (\u00b5 G ). If s > 0.5, the circuit is classified as SAT and the SAT solution is provided by F \u03b8 (\u00b5 G). Otherwise, it is classified as UNSAT. The model never produces false positives and allows for variable recurrences T to improve accuracy at test time. Comparing to NeuroSAT, our framework does not assume input in CNF. In hardware verification, input problems are often in circuit format, which can be converted to CNF but may complicate the problem for SAT solvers. Efforts have been made to develop SAT solvers that work directly with the circuit format, preserving structural information for better solver performance. Our neural framework for learning a SAT solver can directly work with the circuit format, unlike NeuroSAT. While we assume input problems in CNF for fair comparison, we propose pre-processing methods to convert CNF into a circuit, allowing for the injection of structural information. This approach enables encoding problem-specific heuristics into the circuit structure, enhancing solver performance. Our method can build a target circuit from SAT problems, utilizing circuit structure for optimization. Training requires a large sample size, limited by GPU memory, but can generalize well to larger out-of-sample SAT problems. Our method utilizes circuit structure for optimization and can generalize well to larger out-of-sample SAT problems. The NeuroSAT model is trained on a dataset of 300K SAT and UNSAT pairs with Boolean variables ranging from 3 to 10. The models have roughly 180K tunable parameters, with two DAG embedding layers each with an embedding dimension of q = 100. The NeuroSAT model utilizes circuit structure for optimization and can generalize well to larger SAT problems. It is trained on a dataset of 300K SAT and UNSAT pairs with Boolean variables ranging from 3 to 10. The model consists of two DAG embedding layers with an embedding dimension of q = 100, a 2-layer MLP classifier with hidden dimensionality 30, and an aggregator function A(\u00b7) with two 2-layer MLPs each with hidden dimensionality 50. Training used the Adam optimization algorithm with specific hyperparameters and dropout mechanism. The model only includes satisfiable examples in the test data. The main performance metric measured is the percentage of SAT problems in the test set that each model can find a SAT solution for. Our method converges faster than NeuroSAT as the number of iterations increases. The sequential propagation mechanism in DG-DAGRNN is more effective in decoding structural information for SAT problems compared to NeuroSAT's synchronous propagation mechanism. Additionally, both models were evaluated on test datasets with larger numbers of variables. Our method outperforms NeuroSAT on test datasets with larger numbers of variables, showing better generalization to out-of-sample, larger problems. This is attributed to NeuroSAT being trained towards the SAT classification problem, which may result in picking up features harmful for out-of-sample examples. Our framework does not suffer from this issue. Our neural approach to SAT solving outperforms NeuroSAT on larger test datasets, showing better generalization. It is more parallelizable, solving a set of 10,000 examples in 8 minutes compared to MiniSAT's 114 minutes. Our neural approach to SAT solving demonstrates extreme parallelization on GPU, showing superior generalization compared to NeuroSAT. We tested the models on graph k-coloring decision problems, a different domain from k-SAT, which is reducible to SAT. Incorporating graph topology information enhances our model's circuit representation. Incorporating graph topology information enhances our model's circuit representation for the graph k-coloring problem. Two test datasets were generated, including random graphs and trees paired with random color numbers to create graph k-coloring instances. SAT instances were kept in the dataset, with random edges added to graphs until they became UNSAT. Our method and NeuroSAT were evaluated on two test datasets for the graph k-coloring problem. Dataset-1 has higher coverage of graph distributions, while Dataset-2 contains harder SAT examples. Our method solved 48% and 27% of SAT problems in Dataset-1 and Dataset-2, respectively. Surprisingly, NeuroSAT could not solve any SAT graph k-coloring problems in either dataset, contradicting previous results. Different CNF formulations may be the reason for this discrepancy. NeuroSAT is sensitive to different CNF formulations for graph k-coloring problems. Despite training on larger datasets with UNSAT examples, NeuroSAT failed to decode any SAT problems in the test sets. This is attributed to its dependence on learning a good SAT classifier that captures the essence of SAT vs. UNSAT. NeuroSAT restricts training to SAT-UNSAT pairs to avoid learning superficial classification features. In this paper, a neural framework is proposed for efficiently learning a Circuit-SAT solver. The methodology relies on a rich DAG-embedding architecture for learning useful representations of input circuits and an efficient training procedure that directly trains the architecture to solve the SAT problem without requiring SAT/UNSAT labels. Our proposed neural framework for learning a Circuit-SAT solver directly trains the architecture to solve the SAT problem without needing SAT/UNSAT labels. The embedding architecture harnesses structural information in the input DAG distribution, allowing for better model performance and generalization to out-of-sample test cases. The neural framework for learning a Circuit-SAT solver directly trains the architecture without needing SAT/UNSAT labels. It shows superior generalization when transferred to a new domain like graph coloring. Future efforts will focus on extracting high-level knowledge from the learned SAT solver algorithm to aid classical SAT solvers. While these neural models have room for improvement, the preliminary results are promising and motivate further exploration in this direction. Special thanks to Leonardo de Moura and Nikolaj Bjorner from Microsoft Research for their valuable feedback. In the classification problem, the DG-DAGRNN framework is customized to classify input circuits as SAT or UNSAT. The classification function is a MLP with ReLU activation for hidden layers and Sigmoid for the output layer. The embedding function is a multi-layer recursive embedding. The last layer of the embedding is a forward layer for final prediction. The training set is used to minimize cross-entropy loss via backpropagation. The training set is used to minimize cross-entropy loss via backpropagation through the entire network. A proof by contradiction shows that a soft assignment with R G (a) > 0.5 leads to a contradiction. Converting CNF to a circuit can be done in various ways, with one approach based on the Cube and Conquer paradigm for solving CNF-SAT problems. By recursively reducing variables in a CNF formula, the complexity of the SAT problem can be decreased. This process can be applied up to a fixed level to convert the CNF formula into a circuit. The graph k-coloring problem can be reduced to the SAT problem using Boolean CNF representation. The Muldirect approach from Velev (2007) is used for this conversion. The Boolean variables x ij represent node colors in a graph with N nodes and maximum k colors. The CNF encoding ensures each node takes at least one color and neighboring nodes have different colors. A satisfiable solution corresponds to a valid coloring solution. Multiple valid solutions can be generated from one SAT solution. The graph structure in the coloring problem provides valuable information for circuit generation. The graph structure in the graph coloring problem contains valuable information that can be encoded as heuristics into the circuit structure. One useful heuristic is the node degrees, where higher node degrees indicate more constrained variables. Sorting graph nodes based on their degrees can provide a meaningful variable ordering for building circuits."
}