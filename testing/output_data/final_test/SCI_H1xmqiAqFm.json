{
    "title": "H1xmqiAqFm",
    "content": "Deep convolutional neural networks (CNNs) are more robust to class-dependent label noise than class-independent label noise, as revealed in a study investigating their behavior. This study found that CNNs can memorize corrupted labels but still learn similar representations to situations with no noise. Deep convolutional neural networks (CNNs) excel in supervised image classification tasks and can transfer learned representations to other tasks like object detection and semantic segmentation. However, recent CNNs with more parameters than training samples can memorize all data, even with incorrect labels, potentially degrading performance. Research is ongoing to determine if CNNs are robust or fragile to label noise. To investigate if CNNs are robust or fragile to label noise, controlled experiments with noisy labels are needed. Natural noise, such as annotators' mislabeling, is present in datasets, while synthetic noise simulates this by replacing ground truth labels. Previous research has proposed robust training methods for dealing with label corruption. Previous research has mainly used MNIST or CIFAR-10/100 datasets for studying label noise in CNNs. To address the limitations of these datasets, simulated noise on ImageNet-1k with possible mislabeling is proposed. By exploiting ImageNet-1k's conceptual hierarchy, clusters are created to generate class-conditional label noise for training networks with and without corrupted labels. In this study, CNNs trained on ImageNet-1k with synthesized noise show better performance than uniformly synthesized noise. Models trained on class-dependent label noise are more robust to adversarial perturbation. Even with 80% of labels corrupted, CNNs can still learn useful representations for transfer learning. In this section, methods to synthesize label noise are described using a noise transition matrix. Class-independent and uniform noise generation methods are discussed, where labels are replaced with others with specific probabilities. To synthesize label noise, a noise transition matrix is used. Natural noise is class dependent, with similar-class labels stochastically swapped using transition matrices based on inter-class distance. Each class in ImageNet is a leaf of a conceptual tree, and label noise is generated by clustering. To generate label noise, 1,000 ImageNet-1k categories are clustered using Ward's hierarchical clustering to create a block diagonal transition matrix. The transition matrix N is used to replace labels with probabilities based on inter-class distances. The number of clusters can be adjusted for different levels of noise. The study investigates the impact of label noise on CNNs by training them with different types of noise settings and generating adversarial perturbations. Internal representations are analyzed using various methods. The study uses a conceptual distance matrix to generate label noise, referred to as class-dependent noise. The matrix shows a correlation between conceptual and visual similarity in ImageNet-1k classes. The study investigates label noise impact on CNNs by training with different noise settings and analyzing internal representations. Networks like ResNet-50, VGG-19, and DenseNet-121 are used with specific training parameters. Categorical cross entropy is the loss function, and ImageNet-1k dataset is utilized for the ILSVRC 2012 task. In the experiments, networks like ResNet-50, VGG-19, and DenseNet-121 are trained on the ImageNet-1k dataset with label noise. The dataset consists of 1.2 million training images and 50,000 validation images. Input images are standardized, randomly cropped to 224px \u00d7 224px, and horizontally flipped during training. Evaluation involves resizing images to 256px \u00d7 256px and center-cropping to 224px \u00d7 224px. The study examines the impact of label corruption on network performance, showing that learning with class-dependent noise is more challenging than with class-independent noise. In experiments with ResNet-50, VGG-19, and DenseNet-121 on ImageNet-1k, the impact of label noise is studied. Results show that CNNs are more robust to class-dependent noise compared to class-independent noise. Adversarial perturbation is also explored using Generative Adversarial Perturbation. The study explores the impact of label noise on CNNs using ResNet-50, VGG-19, and DenseNet-121 on ImageNet-1k. Adversarial Perturbation BID28 is used to create perturbations, with a focus on class-dependent noise. The generator is trained on 3200 images in the training set for 10 epochs and evaluated on the validation set. The generator is optimized using Adam with a learning rate of 0.0002. The study investigates the impact of label noise on CNNs using ResNet-50, VGG-19, and DenseNet-121 on ImageNet-1k. Models trained on class-independent noise are more vulnerable to adversarial perturbations compared to models trained on no noise or class-dependent noise. The study uses canonical correlation analysis, transfer learning, and adversarial perturbation to analyze the learned features and differences in behavior between models trained on different types of noise. The study compares the robustness of models trained on class-dependent noise versus class-independent noise to adversarial perturbations. Results show that models trained on class-dependent noise are more robust. CNNs trained on class-independent label noise learn unnatural representations compared to clean data. The study analyzed the distance between feature matrices of different models trained on various settings using images from ImageNet-1k dataset. Results show that models trained on class-independent noise learn different representations compared to clean or class-dependent noise, particularly in the final output of the network. The study found that class-independent noise disrupts the last layer structure of the network, leading to lower convergence compared to class-dependent noise. Transfer learning performance is degraded by class-independent noise compared to class-dependent noise, as shown in tests on ImageNet-100 and WikiArt using ResNet-50 models. Transfer learning is used to evaluate learned representations by training networks on ImageNet-1k and replacing the final fully connected layer. Two datasets, ImageNet-100 and WikiArt, are used to investigate transferability to similar and different domains. ImageNet-100 is a 100-class subset with no overlapped categories from ImageNet-1k. WikiArt is a different-domain dataset from ImageNet-1k. In experiments, a small amount of training data was used as validation data to select hyper-parameters. Adam optimizer was utilized for faster convergence, training networks for ten epochs on ImageNet-100 and 30 epochs on WikiArt. Results showed class-dependent label noise in the source data degrades performance on the target data. Transferred models had better test accuracy than models trained from scratch, in both feature transfer and full fine-tuning settings. In the full fine-tuning setting, models pre-trained on data with 40% class-independent label noise perform worse than those pre-trained on class-dependent label noise. Transferring models from 80% class-independent label noise results in worse performance on both datasets compared to training from scratch. The difference in learned representation between class-dependent and class-independent label corruption may explain these results. Class-dependent noise is more informative and prevents loss values from becoming too large, allowing the network to learn the sample's cluster. The network can learn which cluster the sample belongs to based on the label, with class-dependent noise providing more information than class-independent noise. The penalty for misclassification is larger when the prediction is weaker, leading to worse solutions when the label is corrupted by class-independent noise. Our study on label noise in image classification tasks found that class-dependent noise is more beneficial than class-independent noise. This has implications for quality control in data annotation, as inexperienced annotators may introduce class-dependent noise while lazy or malicious annotators may introduce class-independent noise. Administrators should exclude such workers based on our results. Our study found that class-dependent noise is more beneficial than class-independent noise in image classification tasks. Networks trained on class-independently noisy data learn different representations compared to clean or class-conditionally noisy data. While plain CNNs can be robust against real noise, mislabeling still degrades network performance, posing a challenge in avoiding label noise effects. To address the issue of label noise, the ImageNet 100 dataset was created by selecting synsets with over 1200 images from ImageNet 10k. Overlapped classes with ImageNet 1k, descendant classes of \"psychological feature,\" and non-leaf classes were removed. Abstract classes were excluded, resulting in a final selection of 100 classes with 50 images each for validation. Twenty categories were used as TAB0, with some classes being merged."
}