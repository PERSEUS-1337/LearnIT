{
    "title": "SyfXKoRqFQ",
    "content": "Neural networks can converge faster with Ada-Boundary, an adaptive-batch selection algorithm that speeds up training by focusing on samples near the decision boundary. Ada-Boundary outperforms other strategies, improving training time by up to 31.7% compared to the state-of-the-art and 33.5% compared to the baseline. Neural networks have achieved high performance in various fields like computer vision and natural language processing. However, the training process using stochastic gradient descent on mini-batches is computationally expensive due to slow convergence. Different approaches, such as maintaining individual learning rates and batch normalization, have been used to speed up convergence. Recent studies have focused on designing sampling schemes based on sample importance to improve training accuracy. MNIST dataset contains moderately hard samples, while CIFAR-10 dataset has many very hard samples. The CIFAR-10 dataset has many \"too hard\" samples that can cause overfitting in models. Curriculum learning, inspired by human learning, gradually increases the difficulty level of training samples to speed up training. Deep learning studies focus on giving higher weights to harder samples during training. Selecting batches of hard samples can help converge faster than randomly selected batches. The hardness of a sample can be judged by criteria like the rank of the loss computed from previous epochs. The question of whether selecting \"hard\" batches always speeds up DNN training is partially answered. The CIFAR-10 dataset contains \"too hard\" samples that can lead to overfitting. Curriculum learning gradually increases sample difficulty to speed up training. Deep learning studies suggest giving higher weights to harder samples. Selecting hard sample batches can accelerate convergence. The question of whether hard batch selection always speeds up DNN training is partially answered. In easy datasets like MNIST, hard samples provide useful training information. A new sampling scheme is needed to support both easy and hard datasets. The Ada-Boundary adaptive batch selection strategy proposed in this paper accelerates training and generalizes better to hard datasets. The Ada-Boundary strategy accelerates training by selecting samples near the decision boundary, which are moderately hard, without requiring human intervention. This approach considers the learning progress of the model and generalizes better to hard datasets. The Ada-Boundary strategy selects samples near the decision boundary based on learning progress, accelerating training without human intervention. It improves convergence speed, reduces execution time by 14.0-33.5%, and enhances test error compared to random batch selection. Ada-Boundary accelerates training by selecting samples near the decision boundary based on learning progress. It reduces execution time by 14.0-33.5% and improves test error by 7.34-14.8% in the final epoch. Compared to the state-of-the-art hard batch selection BID17, Ada-Boundary achieves a smaller execution time by 18.0% and a smaller test error by 13.7% in the CIFAR-10 dataset. The main challenge for Ada-Boundary is evaluating sample proximity to the decision boundary using a novel distance measure and sampling probability computation method based on the measure. The softmax distribution of a sample x i over labels is used to determine the correctness and confidence of a neural network's prediction. The distance from a sample x i to the decision boundary is defined by a directional distance function, which considers the direction and magnitude of the distance based on correctness and confidence. The correctness is determined by comparing the highest probability label to the true label, while confidence is measured by the standard deviation of the softmax distribution. The cross-entropy loss may not capture prediction probability for false labels due to one-hot true label vector formulation. Our distance function is bounded and advantageous compared to the loss. The rank-based approach by BID17 sorts samples by importance measure for sampling probability in mini-batch selection. The rank-based approach by BID17 sorts samples by importance measure for sampling probability in mini-batch selection, with the sampling probability exponentially decaying according to rank. The selection pressure parameter affects the probability gap between samples, leading to skewed selection if the true sample distribution is imbalanced. The quantization method BID6 BID2 is used to incorporate the impact of distance into batch selection. The quantization index q increases as a sample moves away from the decision boundary, reflecting the difference in actual distances. The index is bounded by the total number of samples N. The quantization-based method Ada-Boundary ensures a balanced distribution of samples, even with skewed true sample distribution. The algorithm requires a warm-up period during early training stages to confirm quantization indexes for each sample. Randomly selected mini-batch samples are used for warm-up, and their quantization indexes are updated. After the warm-up epochs, the algorithm computes the sampling probability. After a warm-up period, the algorithm computes sampling probabilities for mini-batch samples and updates quantization indexes using the latest model state. Three heuristic sampling strategies are presented in Appendix A: Ada-Easy focuses on samples far from the decision boundary, Ada-Hard is similar, and Ada-Boundary ensures balanced sample distribution. Ada-Easy, Ada-Hard, and Ada-Uniform are three sampling strategies for mini-batch samples. Ada-Easy focuses on samples far from the decision boundary in the positive direction, Ada-Hard focuses on samples far from the decision boundary in the negative direction, and Ada-Uniform samples uniformly over the distance range. Ada-Boundary(History) is a history-based variant that updates quantization indexes using the previous model. In this section, experiments were conducted on three benchmark datasets: MNIST, Fashion-MNIST, and CIFAR-10. Different models were used for each dataset, with batch normalization applied. Hyper-parameters included a learning rate of 0.01 and batch sizes of 128. Training epochs varied for each model. The experimental setup included training epochs of 50 for LeNet-5 and 70 for WideResNet-16-8, selection pressure set to 100, and warm-up threshold \u03b3 set to 10. Results using DenseNet (L = 25, k = 12) BID8 on CIFAR-100 4 and Tiny-ImageNet 5 are discussed in Appendix C. The experimental setup included training epochs of 50 for LeNet-5 and 70 for WideResNet-16-8, selection pressure set to 100, and warm-up threshold \u03b3 set to 10. Results using different adaptive batch selections were implemented using TensorFlow and executed on a single NVIDIA Tesla V100 GPU. Performance gain over baseline and state-of-the-art strategies was measured using three metrics, with tests repeated five times for robustness. Source code is available for reproducibility. Reduction in number of epochs and running time to achieve the same error percentage using different batch selection strategies. Gain err was 42.0% with Ada-Boundary compared to random batch selection. Ada-Easy was excluded due to slower convergence speed. The convergence analysis of five batch selection strategies showed that Ada-Boundary achieved the fastest convergence speed in training loss and test error for Fashion-MNIST. Ada-Hard and Ada-Boundary were the fastest for CIFAR-10 in training loss, but Ada-Boundary was the fastest in test error. In the easiest MNIST dataset, adaptive batch selections accelerated convergence speed compared to random batch selection. As training difficulty increased to Fashion-MNIST and CIFAR-10, only Ada-Boundary significantly outperformed random batch selection in convergence speed. Ada-Boundary outperforms online batch selections in TAB3 and significantly reduces training time by up to 30%. Curriculum learning and self-paced learning are other methods used to determine sample importance during training. Recently, Bayesian optimization was used to optimize a curriculum for training dense word representations. The right curriculum arranges data samples by difficulty and introduces dissimilar samples. Hard-example mining algorithms aim to select challenging samples, but are task-specific. Ada-Boundary follows the philosophy of curriculum learning and outperforms online batch selections, reducing training time by up to 30%. Ada-Boundary, similar to BID17, focuses on selecting hard samples for training by computing sampling probabilities based on loss rank. It outperforms BID17 in online batch selection and is considered state-of-the-art. In contrast to previous works like BID1, our main contribution is training faster using confusing samples near the decision boundary. Additionally, we propose a novel adaptive batch selection algorithm, Ada-Boundary, which selects the most appropriate samples based on the learning progress of the model. Ada-Boundary is a novel adaptive batch selection algorithm that accelerates training by selecting samples near the decision boundary. Extensive experiments with CNN models on benchmark datasets showed that Ada-Boundary significantly speeds up training and generalizes well on hard datasets. It saves time in training deep neural networks, especially with large and complex data, and can benefit from advancements in hardware technologies. Our future work includes applying Ada-Boundary to different types of DNNs like RNN and LSTM, exploring the relationship between DNN power and Ada-Boundary improvement. Ada-Easy and Ada-Hard can be modified by adjusting quantizers in the algorithm. Ada-Uniform can be implemented using a specific computation method. Ada-Uniform can be implemented by using F \u22121 (x) to compute the sampling probability in Line 9 of Algorithm 1, where F (x) is the empirical sample distribution according to the sample's distance to the decision boundary. Experimental results on two challenging data sets, CIFAR-100 and Tiny-ImageNet, were included. Tiny-ImageNet images were resized to 32 \u00d7 32. DenseNet (L=25, k=12) BID8 model was used with momentum optimizer. Training parameters included a learning rate of 0.1, batch size of 128, training epoch of 90, and warm-up threshold \u03b3 of 10. Tests were repeated five times for robustness. The selection pressure in Ada-Boundary determines the sampling probability of boundary samples, affecting convergence speed. Increasing the selection pressure accelerated training loss convergence, but test error convergence was faster only up to a certain value. Overexposure to boundary samples with large selection pressure impacted performance on CIFAR-100 and Tiny-ImageNet datasets. The selection pressure in Ada-Boundary affects convergence speed, with large selection pressure causing overfitting on hard datasets. Ada-Boundary with s e = 2 outperforms random batch selection, reducing training time by up to 20%. Ada-Boundary with momentum was slower than random batch selection but reduced running time by 18.0%-21.4% to achieve the same test error. The wall-clock training time for Ada-Boundary with SGD and momentum was slower than random batch selection, but it achieved a significant reduction in running time to obtain the same test error. In experiments using momentum optimizer, different batch selection strategies showed varying convergence speeds in training loss and test error on three data sets. Online batch selection demonstrated the fastest convergence in training loss but similar results in test error due to overfitting. Ada-Boundary exhibited the quickest convergence in test error for the Fashion-MNIST dataset. In Fashion-MNIST, Ada-Boundary showed the fastest convergence in test error, while online batch selection was fastest in training loss. Ada-Hard and Ada-Uniform had slower convergence speeds in test error compared to random batch selection. In CIFAR-10, Ada-Boundary and Ada-Hard converged slightly faster than random batch selection in both training loss and test error. Online batch selection was slightly slower than random batch selection in both cases. Most adaptive batch selections accelerated convergence in the easiest MNIST data set, but Fashion-MNIST showed different results. In Fashion-MNIST, Ada-Boundary converged faster than random batch selection, while in CIFAR-10, Ada-Boundary and Ada-Hard showed comparable convergence speeds and outperformed random batch selection. Ada-Boundary consistently outperformed both random and online batch selections."
}