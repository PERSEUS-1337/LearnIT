{
    "title": "H1lGHsA9KX",
    "content": "Determining the appropriate batch size for mini-batch gradient descent is time-consuming and often relies on grid search. This paper introduces a resizable mini-batch gradient descent (RMGD) algorithm based on a multi-armed bandit. RMGD samples batch sizes according to a probability distribution, exploring and exploiting successful batch sizes. Experimental results show RMGD outperforms fixed batch sizes and even grid search. The RMGD algorithm outperforms grid search in performance and time efficiency. Gradient descent is an optimization algorithm based on the negative gradient of the loss function, updating model parameters iteratively to converge towards a minimum. Gradient descent (GD) is a method for optimizing convex loss functions to converge to a global minimum with an appropriate learning rate. However, GD can be slow and impractical for large datasets due to the need to evaluate gradients for all data points in each iteration. For non-convex functions, GD can get stuck in local minima. Stochastic gradient descent (SGD) has large gradient variance and slow convergence, while mini-batch gradient descent (MGD) balances robustness and efficiency by using gradients over small batches of data. Mini-batch gradient descent (MGD) offers advantages over both gradient descent (GD) and stochastic gradient descent (SGD) by balancing efficiency and robustness. MGD allows for efficient memory usage, higher model update frequency, and more stable updates. However, selecting the appropriate batch size can be challenging. Various studies have shown a link between performance and batch size in MGD. While there are guidelines for selecting batch size, practical methods like grid search can be time-consuming. Adaptive MGD algorithms are being developed to address this issue. This paper introduces a resizable mini-batch gradient descent (RMGD) algorithm based on a multi-armed bandit for optimal batch size selection in deep learning. RMGD samples batch sizes at each epoch based on previous success/failure probabilities, improving grid search efficiency. The RMGD algorithm updates the probability distribution of batch sizes at each epoch, avoiding the need for grid search. Empirical results show that smaller batch sizes yield better test errors, while larger batch sizes are more likely to converge to sharp local minima. The use of varying batch sizes in Mini-Batch Gradient Descent (MGD) algorithms has been explored. It was found that larger batch sizes can lead to convergence to sharp local minima and poor generalization. Adaptive algorithms like BID3 adjust batch sizes during optimization to balance progress and accuracy. However, research has shown that performance may degrade with larger batch sizes. The use of varying batch sizes in Mini-Batch Gradient Descent (MGD) algorithms has been explored. Adaptive algorithms like BID0 propose dynamic batch size adaptation to decrease variance in stochastic gradients. Increasing batch size is more effective than decaying learning rate for reducing iterations. However, these algorithms lack flexibility as they are unidirectional. Batch size can be considered a hyperparameter, with proposals for bandit-based optimization. Successive halving and HYPERBAND are algorithms for hyperparameter optimization. Adaptive MGD performs better than fixed MGD in small hyperparameter spaces. Resizable mini-batch gradient descent algorithm (RMGD) samples batch sizes from a probability distribution and updates parameters using mini-batch gradients. The algorithm considers multi-armed bandit optimization over batch sizes. Model parameters, training loss function, gradients, and learning rate are key components in the algorithm. The resizable mini-batch gradient descent (RMGD) algorithm aims to minimize misupdating by using 0-1 loss as the cost function for selecting batch sizes. It treats batch sizes as multi arms, sampling one at each epoch from a probability distribution and incurring a cost based on the chosen size. The RMGD algorithm selects batch sizes from a probability distribution and incurs a cost based on the chosen size. It consists of a batch size selector and parameter optimizer. The selector samples a batch size and updates the distribution, while the optimizer uses mini-batch gradient descent. The RMGD algorithm updates model parameters using a batch size selector and parameter optimizer. It iterates through epochs, selecting batch sizes from a probability distribution and updating the model accordingly. The optimizer then calculates validation loss and outputs the cost. The RMGD algorithm uses a probability distribution to explore and exploit different batch sizes during training. The selected batch size evolves based on performance, with better performing sizes having higher probabilities. The regret bound of RMGD is derived from previous work, aiming for low validation loss. The RMGD algorithm aims for low regret in batch size selection, with a regret bound derived from previous work. Experimental results on MNIST, CIFAR10, and CIFAR100 datasets using CNN models are presented. The RMGD algorithm aims for low regret in batch size selection, with experimental results on MNIST dataset using AdamOptimizer and AdagradOptimizer. The probability distribution and batch size selection during training are analyzed, showing that small batch size (32) performs better before epoch 50, while large batch size (512) performs better after epoch 60. After epoch 60, batch size 512 is selected more frequently. The best performing batch size varies with epoch, with 256 performing best from 40 to 55, 128 from 60 to 70, and 256 again after epoch 80. RMGD is more flexible than MGD in adapting to different batch sizes. Test accuracy results are shown in FIG3 with different RMGD settings. The test accuracy results for different RMGD settings show that relatively small batch sizes (16-64) perform better than large batch sizes (128-512), with batch size 64 achieving the best performance in grid search. RMGD is designed to ensure optimal performance without the need for grid search on batch size, and although its performance improvement is not significant compared to the best MGD, it is considered valid. The RMGD is considered valid with little performance gap among settings. The 'sub' setting outperforms 'basic' in left figure, but the opposite is shown in right figure. Performance change does not depend on batch size. RMGD outperforms best MGD and is faster, 8 times faster than grid search. Effective regardless of optimizer, tested on CIFAR10 and CIFAR100 datasets. The experiments of RMGD were repeated 25 and 10 times, respectively, with images preprocessed before input. Test accuracy on CIFAR10 shows larger batch sizes (128-256) perform better than smaller sizes, with 256 achieving the best performance. On CIFAR100, batch size 128 performs the best. Results suggest it's challenging to determine the optimal batch size without grid search. RMGD settings consistently outperform fixed MGD with no significant performance gaps, eliminating the need to worry about batch size selection or cost function. The RMGD algorithm outperforms fixed MGD in terms of speed, showing effectiveness on CIFAR10 and CIFAR100 datasets. It is simpler to implement compared to other adaptive batch size algorithms like CABS, with the goal of reducing validation loss for better generalization performance. The RMGD algorithm achieves higher performance than CABS on CIFAR10 and CIFAR100 datasets without the need for grid search. It focuses on selecting batch size efficiently to improve model quality and training efficiency. The algorithm is designed to be simple, robust, and applicable in various situations, based on a resizable mini-batch gradient descent approach. The RMGD algorithm aims to select the most effective batch size for reducing loss during training. It samples batch sizes based on a probability distribution, updating it after each epoch based on validation loss. The goal is to improve training efficiency by exploring and exploiting successful batch sizes. This approach eliminates the need for grid search and focuses on selecting the best batch size for optimal performance. The RMGD algorithm selects batch sizes based on a probability distribution to reduce loss during training. It achieves the best grid search performance in various machine learning fields, including deep learning, in a shorter time. The algorithm is effective, flexible, and aims to minimize regret for not selecting the optimal batch size. The algorithm aims to minimize regret by selecting batch sizes with low cost. It uses a probability distribution to choose batch sizes and follows the Follow-the-Regularized-Leader (FTRL) learning rule. The gradient of the selecting loss function is estimated using a random vector z \u03c4. The Online Mirror Descent (OMD) algorithm is used to solve the optimization problem in the Follow-the-Regularized-Leader (FTRL) approach in online learning. OMD computes probability distributions iteratively based on gradient updates and Bregman divergence, updating the distribution in a 'dual' space defined by a regularizer. If the regularizer is strongly convex, the probability distribution can be recovered by the inverse mapping of the gradient. The Online Mirror Descent (OMD) algorithm, when used with a strongly convex regularizer, allows for the recovery of probability distributions through the inverse gradient mapping. The Regret Minimization Gradient Descent (RMGD) has bounded regret, particularly when \u03b2 = log(K)/(KT), with a regret bounded by 2K log(K)T. MNIST and CIFAR10 are datasets commonly used for image classification, with MNIST consisting of handwritten digits and CIFAR10 containing color images in 10 classes. The CIFAR10 dataset has 60,000 color images in 10 classes, split into training, validation, and test sets. A simple CNN model with specific layers and optimizer settings is used for MNIST and CIFAR10. The model is trained for 100 epochs with different batch sizes. MomentumOptimizer is used for CIFAR10 and CIFAR100 datasets. The model is trained for 350 epochs with MomentumOptimizer using a fixed momentum of 0.9. The learning rate is scaled up proportionately to the batch size and decayed by a schedule after 200, 250, and 300 epochs. Dropout is applied to the input image and after each convolution layer with specific probabilities. Weight decay and activation function are also specified. RMGD uses different beta values for MNIST and CIFAR10/CIFAR100 datasets. The basic batch size selecting cost for CIFAR10 and CIFAR100 is 0-1 loss, hinge loss, and ratio loss. It involves 1x1 convolutions, ReLU activation, pooling, and softmax for classification."
}