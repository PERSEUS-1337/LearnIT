{
    "title": "HJGv1Z-AW",
    "content": "The study explores the evolution of communication protocols using deep learning methods and reinforcement-learning neural network agents in referential communication games. Agents trained on raw pixel data show that structured compositional language is more likely to emerge when the input data is structured. This research is crucial for understanding language development in both human and artificial contexts. This paper focuses on how environmental conditions impact the communication protocol learned by an agent in language development. The study is motivated by the idea that language gains meaning through its use, especially in cooperative scenarios where agents work towards shared goals. The research analyzes the effects of environmental realism on grounded language learning. In the study of multi-agent and self-play methods in machine learning, the focus is on simple referential games where one agent communicates a target object to another. Compositionality, using smaller building blocks to generate complex forms, is crucial in communication protocols for infinite expression with a finite dictionary. Previous research shows that agents can develop compositional protocols in language games. In the context of referential communication games, two studies contrast results in environments with varying levels of structure. One study focuses on symbolic representations, while the other explores environments more similar to those humans experience, with entangled and noisy elements. The second study examines raw perceptual input, a more challenging and realistic scenario for computational agents. Despite the entangled nature of the data, reinforcement learning agents can successfully communicate using the same setup and learning methods as the first study. This result suggests the possibility of more realistic simulations of language emergence. The referential game is used to train agents end-to-end, but they struggle to produce structured messages with entangled input data. This game is implemented as a form of multi-agent cooperative reinforcement learning, where agents take actions to maximize a shared reward. Evolution of communication in a referential game involves a speaker describing an object using symbols, creating a lexicon. The listener then uses the message to identify the target object among distractors, aiming for communicative success. The game trains agents through cooperative reinforcement learning. In a referential game, the speaker encodes a target object into a dense representation and generates a message using a recurrent policy. The listener receives a candidate set with the target and distractors, aiming for communicative success through cooperative reinforcement learning. The decoder in the referential game is implemented as a single-layer LSTM BID19. The symbols in the agents' alphabet have no predefined meaning and are grounded during the game. The listening agent uses a similar encoder to the speaker with independent network weights. The message is encoded using a single-layer LSTM to produce an encoding, and the listener predicts a target object using a non-parametric pointing module. Stochastic sampling is replaced with deterministic methods during inference. Refer to Appendix B for more details on the agents' architecture. The speaker's message and the listener's pointing module are optimized jointly in the referential game. The agents' weights are not shared, and only communicative success is used as supervision. The objective function maximizes rewards based on correct target identification. Exploration is maintained through entropy in the agents' policies. In the referential game, the speaker's message and the listener's pointing module are optimized jointly. The parameters are estimated using the REINFORCE update rule with an entropy regularization term. Experiments show agents learning to communicate with structured input using the Visual Attributes for Concepts Dataset. The curr_chunk discusses attribute annotations for 500 concrete concepts across different categories, with 636 general attributes. After filtering out homonym concepts, the working set consists of 463 concepts and 573 attributes. Each concept has an average of 11 attributes represented as binary vectors. These vectors are converted to dense representations using a single-layer MLP with a sigmoid activation function. The number of candidate objects K is set to five in all experiments. The experiments set the number of candidate objects to five, with a 20% random baseline. The alphabet size was set to 100 symbols, smaller than the set of 463 objects. Model performance was evaluated on training data with different message length settings. For the shortest message setting, agents developed a protocol of 31 unique messages for 363 training concepts, indicating high ambiguity. Recent findings suggest that ambiguity in language prevents redundant codes, as some message content can be inferred from context. Ambiguity arises from the exploration problem agents face, leading to suboptimal policies that can still achieve high accuracy. This phenomenon makes it challenging to escape from ambiguous solutions during training. In signaling games, polysemy can lead to partial pooling equilibrium. Skyrms suggests allowing the invention of new signals to overcome local communication minima. Allowing agents to produce longer messages improves communicative success. The number of messages in the protocol increases, reducing the average number of concepts a message can denote. In the real world, objects appear in specific contexts, not randomly. An experiment was designed with distractors sampled from a context distribution reflecting object co-occurrence statistics. This leads to more plausible distractor sets, improving the game's realism. The distractor selection process affects language learning dynamics, with non-uniform sampling leading to a degenerate strategy initially boosting success but making the game harder overall. In non-uniform sampling, object confusability is less influenced by similarity and more by visual context co-occurrences, making the game harder overall. The choice of distractors in language emergence literature affects the organization and naturalness of the emerged language. Quantifying compositionality and structure in the language is challenging without a formal mathematical definition. The study focuses on measuring the generalization ability of an emerged language to novel objects and reports quantitative results using a measure of message structure. Trained agents are tested with unseen objects varying in resemblance to training data, such as objects from the same distribution but not presented during training. In the chimeras scenario, novel objects are created by sampling properties from a distribution inferred from training data, breaking feature correlation. In the uniform chimeras scenario, objects are constructed by uniformly sampling properties. Agents perform above random chance but with a drop in performance for unseen objects. The emerged language can generalize to unseen objects, with productivity observed as speakers can create novel messages on-the-fly. In the study, agents can generate novel messages to describe unseen objects, even without specific training. The topographic similarity measure is used to correlate meanings and signals in compositional languages. The topographic similarity measure correlates meanings and signals in compositional languages by comparing Levenshtein distances and cosine similarities between objects' messages and VisA vectors. The positive values in the topographic \u03c1 column indicate that similar objects share common message structures. The topographic similarity measure shows that similar objects receive similar messages, with specific prefix bigrams encoding category-specific information. A clear relation between message similarity and meaning similarity is observed, as shown in Figure 3. In Figure 3, a correlation curve is plotted for an emerged language generated by untrained architectures. The language shows signs of structure, indicating that pre-linguistic representations may be sufficient for structured language emergence in neural network-based agents. Experiments involve agents learning visual processing from entangled pixel input guided by communication-based rewards using a synthetic dataset of scenes with geometric objects. The study uses a synthetic dataset of scenes with geometric objects to generate RGB images depicting single object scenes with various colors and shapes. Different variants of the game are introduced with varying numbers of distractors and different viewpoints for the speaker and listener. The study uses a synthetic dataset of scenes with geometric objects to generate RGB images depicting single object scenes with various colors and shapes. Different variants of the game are introduced with varying numbers of distractors and different viewpoints for the speaker and listener. For each game, train and test splits are created with proportions of 75/25. Pre-linguistic objects are presented as pixel input, converted to dense representations using an 8-layer ConvNet. The ConvNets are not pre-trained on an object classification task, with the only learning signal being the communication-based reward. The speaker/listener architecture is seen as an encoder-decoder with a discrete bottleneck (the message). Despite not being pre-trained, the lower layers of the ConvNets encode similar information to a ConvNet pre-trained on ImageNet. The reward-based learning signal induced from the communication game setup could be used for large-scale ConvNet training. Agents' conceptual spaces align at different levels without weight sharing, resembling theories of interactive conceptual alignment during dialogue. Starting from raw perceptual input presents a challenge as agents establish naming conventions for scenes while processing input with their own visual system. The dense representations used for the message contain no bias towards specific scene information. The extraction of visual properties in the communication game is driven solely by the game itself, contrasting with other methods using pre-trained visual vectors. Results show high performance in communicative success, indicating that reinforcement learning agents can establish effective communication protocols without bias towards specific scene information. In a communication game, reinforcement learning agents establish a lexicon with 1068 messages describing 3000 objects, achieving 93.7% accuracy. Messages encode object locations in a consistent way, similar scenes receive similar messages, and the communication strategy mirrors human players in referential games. In a communication game, reinforcement learning agents establish a lexicon with 1068 messages describing 3000 objects, achieving 93.7% accuracy. Messages encode object locations in a consistent way, similar scenes receive similar messages, and the communication strategy mirrors human players in referential games. Specifically, activations from the speaker's ConvNet and a pre-trained ResNet model are computed for 4000 images. Pairwise cosines are calculated in both spaces, showing a Spearman \u03c1 of 0.6-0.7 between the first 3 layers of the ConvNet and the ResNet. The communicative success of agents playing different games is analyzed, revealing unstable protocols highly influenced by game setup modifications. In game B, the communication protocol changes significantly, with 13 unique messages that are difficult to interpret. In game C, agents focus on describing color, resulting in a compact protocol with 8 unique messages. Game D shows a bias towards describing color, with messages clustering objects by color. The speaker's ConvNet visual representations are probed with 4 classifiers for color, shape, object position, and floor color. Object position is consistently encoded, while object shape provides less salient information. Different games result in varying predictive power of visual representations. The structure and semantics of emergent protocols in communication games are influenced by visual representations. Disentanglement of object factors impacts agents' ability to communicate, with shape being less salient. Training agents with disentangled input data helps retain compositional structure in communication games. In communication games, agents can successfully retain compositional structure in their output, even when presented with raw pixel data. However, their ability to form compositional protocols is hindered by difficulties in pulling apart objects' factors of variations. This research bridges traditional language evolution studies with contemporary deep learning, paving the way for more realistic computational simulations of language emergence with complex image stimuli. Agents achieve alignment through common ground and structural similarity in their conceptual systems. The closer the representations are to raw pixel input, the more aligned the conceptual spaces become. This is similar to language processing, where low-level processing aids in recognition. The first ConvNet layers perform low-level processing like phoneme recognition or word segmentation, while higher layers handle more abstract processing. Successful communication results in similar conceptual spaces between speakers and listeners, but this similarity decreases significantly in communication failures. LSTM hidden states in the modules have a dimension of 50, and the pre-linguistic ConvNet encoders have 8 layers with 32 filters each. ReLU is used as the activation function with batch normalization for every layer. For learning, Rmsprop optimizer with learning rate 0.0001 is used along with ReLU activation function and batch normalization for each layer. Separate entropy regularization values are used for each policy: 0.01 for \u03c0 S and 0.001 for \u03c0 L. A model with perfect attribute classifiers for color, shape, and object position is assumed. Performance is computed using gold attribute classifiers by removing distractors not matching the target's attributes. Experiments are repeated for single attribute classifiers and pairwise combinations. In Section 4.1, accuracies for train and test models with gold classifiers are presented in percentage format."
}