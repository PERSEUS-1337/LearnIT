{
    "title": "r1laEnA5Ym",
    "content": "In this work, the optimization of Generative Adversarial Networks (GANs) is approached by casting the problem in the variational inequality framework. The study explores methods from mathematical programming to address the challenges of training GANs, including applying averaging, extrapolation, and a computationally cheaper variant to stochastic gradient methods like SGD and Adam. Generative Adversarial Networks (GANs) are known for producing realistic images, super-resolution, and style transfer. However, they are difficult to train due to unstable behavior. Recent work has focused on proposing new formulations of the GAN objective as a two-player game in game theory. Solving GAN games involves running variants of stochastic gradient descent (SGD) developed for optimizing neural network objectives. The text discusses the challenges of training Generative Adversarial Networks (GANs) due to oscillatory behavior of stochastic gradient descent (SGD) in two-player games. It suggests using more principled methods like variational inequality problems (VIPs) and introduces techniques like averaging and extrapolation for GAN optimization. In the context of GAN training, new techniques are introduced to address the oscillations in basic SGD. These methods incorporate principles from variational inequality optimization and include extrapolation from the past, which shows promise in improving GAN training by 4-6% compared to previous methods. In GAN training, new techniques improve performance on CIFAR-10 dataset using WGAN-GP BID14 and ResNet generator. The purpose is to generate samples that match the true data distribution. The network training strategy in GAN involves a game between a generator and discriminator to produce realistic samples. The non-saturating GAN formulation aims to jointly minimize the cost function for better gradient learning. The minimax formulation in GAN literature considers different objectives for each player to find a Nash equilibrium. The optimization problems are coupled and need to be considered jointly. Standard GAN objectives are non-convex, making the existence of equilibria uncertain. In non-convex optimization, the existence of equilibria for games is uncertain. The focus is on optimizing these games assuming an equilibrium exists. Variational inequality problems are studied to find points satisfying necessary conditions for two-player game problems. The optimization involves minimizing two different cost functions jointly in \u03b8 and \u03d5. The solution of the smooth two-player game involves defining stationary points and variational inequalities. Stationary points have zero gradient in the unconstrained setting, while in the presence of constraints, they have non-negative directional derivatives. The variational inequality problem generalizes these conditions for any continuous vector field. The optimal set consists of points that satisfy the variational inequality problem. In the context of variational inequalities, the text discusses how saddle point optimization and non-zero sum game optimization can be formulated as VIPs. The focus is on optimization techniques for solving VIPs, including averaging and extrapolation methods. The discussion also includes a novel extrapolation technique called extrapolation from the past. Extensions of these techniques to the stochastic setting are presented later on. The gradient method BID3 and the extragradient method are standard techniques studied in the VIP literature. The gradient method iterates converge linearly under certain conditions, while the extragradient method can converge faster without requiring averaging. The idea of the method is to compute a lookahead step to find a more stable direction to follow. Weighted averaging schemes with weights \u03c1 t \u2265 0 have been proposed for VIP. Averaging can tackle oscillatory behavior in game optimization, as shown in a toy example with linear discriminator and generator functions. The bilinear objective in WGAN is derived from linear discriminator and generator functions. Previous work has shown the challenges of gradient descent in saddle point optimization. Methods to address this issue have been proposed in mathematical programming. The operator associated with the minimax game in multidimensional bilinear examples is F(\u03b8, \u03c6) = (\u03c6, \u2212\u03b8). Two common ways to compute updates are simultaneous and alternating gradient rules. Simultaneous updates diverge geometrically, while alternating iterates are bounded but do not converge to equilibrium. The uniform average behavior differs between the two update methods. The uniform average of simultaneous updates diverges as the sequence of iterates is not bounded, while alternating updates converge to 0, showing better convergence properties. Extrapolation is another technique to prevent oscillations in variational inequality literature. Extrapolation is a technique used to stabilize dynamics by computing the gradient at a different point from the current one. This method differs from Nesterov's momentum and is more stable, benefiting from better convergence properties. Extrapolation has better convergence properties than explicit methods, but they are not practical due to solving a potentially non-linear system at each step. The update rules for the simplified WGAN toy example show that for \u03b7 < 1, the convergence rates of the implicit method and extrapolation are similar. Extrapolation benefits from being more practical, with the squared norm of the iterates decreasing geometrically for any \u03b7 < 1. However, one issue with extrapolation is that the algorithm \"wastes\" a lot of computational resources. Extrapolation is more practical than explicit methods for \u03b7 < 1. A technique called extrapolation from the past only requires a single gradient computation per update, inspired by the extragradient method. This approach was rediscovered from a different perspective and has connections to online convex optimization. Extrapolation from the past, inspired by the extragradient method, proves linear convergence for optimization algorithms. Results for a stochastic version are provided, showing faster convergence compared to other techniques. The algorithms advocated in the paper converge quickly to the solution, as demonstrated in non-convex objective comparisons. Additionally, a faster convergence rate is achieved for a general operator F and any projection on a convex \u2126. In this section, extensions of techniques from previous sections are considered for a stochastic operator, where only an unbiased stochastic estimate of the gradient is available. This scenario is common in GAN training, where a finite sample estimate of the expected gradient is used. The text discusses the use of stochastic operators in GAN training, with a focus on assumptions and algorithms for solving the problem. Three algorithms, including stochastic gradient descent variants, are presented and analyzed for solving the task. The text discusses algorithms for GAN training, including ReExtraSGD and projected versions for handling constraints. Monotonicity of F and compact convex sets are assumed for convergence proofs. The text discusses algorithms for GAN training, including ReExtraSGD and projected versions for handling constraints. The merit functions g and h are defined, with g being related to solutions of the optimization problem and Nash equilibria in zero-sum games. Averaging is introduced as a method in stochastic gradient descent, with a theorem stating the convergence under certain assumptions. The variance term in the convergence bound is discussed, showing a rate of O(1/\u221at) with an extra log factor in the step size. The text discusses algorithms for GAN training, including ReExtraSGD and projected versions for handling constraints. A theoretical consequence of Alg. 2 (AvgExtraSGD) is a smaller variance term compared to SGD with prediction method. SGD with extrapolation provides better convergence guarantees but requires two gradient computations and samples per iteration. This motivates the new method, Alg. 3 (AvgPastExtraSGD), which uses extrapolation from the past and achieves the best of both. The text discusses combining techniques for stochastic monotone operators with standard algorithms for training deep neural networks, such as the Adam optimizer and the SGD optimizer. Different choices in updating moments can lead to different algorithms in practice. For example, the proposed Adam with extrapolation from the past (Alg. 4) differs from Optimistic Adam BID7. The text discusses combining techniques for stochastic monotone operators with standard algorithms for training deep neural networks, such as the Adam optimizer and the SGD optimizer. The proposed Adam with extrapolation step (Extra-Adam) in Algorithm 4 differs from Optimistic Adam BID7. Convergence results can be generalized to gradient updates with different step-sizes for each player in a two-player game. The detailed pseudo-code for Extra-Adam is provided, focusing on practical applications without providing a convergence proof. The extragradient method is a standard algorithm to optimize variational inequalities, introduced by Korpelevich in 1976 and extended by Nesterov and Nemirovski in 2004. Stochastic versions have been analyzed for variational inequalities with bounded constraints. A variance reduced version of the stochastic gradient method has been proposed for strongly monotone variational inequalities. Extrapolation is also related to optimistic methods. Several methods have been proposed to stabilize GANs, including transforming zero-sum formulations into more general games, such as the non-saturating formulation of GANs, DCGANs, and the gradient penalty for WGANs. Optimization methods for GANs have been suggested, such as AltSGD with momentum-based steps and dualizing the GAN objective. The text discusses different methods proposed to stabilize GANs, including transforming zero-sum formulations into more general games and optimization methods like AltSGD with momentum-based steps. Mescheder et al. (2017) suggest adding the norm of the gradient in the objective for better signal. BID10 analyzes the effect of momentum on a generalization of the bilinear example. Unrolling steps by Metz et al. (2017) aim to approximate the \"true generator objective function\" by updating the discriminator and generator. Recent work has used geometric averaging for GANs, but the present work formally justifies its use by relating it to the VIP perspective. The text discusses methods to stabilize GANs, including transforming formulations and optimization methods like AltSGD. Recent work explores geometric averaging for GANs, with this study justifying its use by relating it to the VIP perspective. Yaz\u0131c\u0131 et al. (2019) and Mertikopoulos et al. FORMULA0 delve deeper into averaging and extrapolation, providing convergence results in the context of coherent saddle point. The goal is to optimize standard GANs using techniques introduced earlier, independent of architectural improvements or new formulations. The text discusses optimization algorithms for training GANs, including SimAdam, SimSGD, AltSGD{k}, AltAdam{k}, ExtraSGD, ExtraAdam, PastExtraSGD, and PastExtraAdam. Results show that AvgAltSGD1 and AvgPastExtraSGD perform the best on a finite sum bilinear objective task. The proposed techniques are evaluated in the context of GAN training, focusing on Adam variants of optimization algorithms. Training is done on CIFAR10 dataset with WGAN objective and weight clipping. ExtraAdam shows faster convergence compared to Adam baselines on a state-of-the-art ResNet architecture with WGAN-GP objective. The ResNet model was trained using the WGAN-GP objective similar to BID14 and evaluated using the inception score (IS) on 50,000 samples. Hyperparameters of Adam were extensively searched, with fixed values for \u03b21 and \u03b22. Different learning rates for the generator and discriminator were set as suggested by BID17. Experiments were conducted with 5 random seeds for 500,000 updates. Techniques like extrapolation and averaging consistently improved over baselines, as shown in Tab. 1 and FIG3. Using an extrapolation step and averaging with ExtraAdam outperformed all other methods for training a ResNet generator on a WGAN-GP objective. This approach significantly improved results over the previous state-of-the-art IS and FID on CIFAR10. Methods based on extrapolation are less sensitive to learning rate tuning and can handle higher learning rates with less degradation. The optimization techniques developed for GAN objectives offer more principled methods for training a wide range of GAN models. The presented techniques offer a principled solution to a fundamental problem in GAN training, improving model performance significantly. They are applicable to a wide range of GAN training objectives and are likely to benefit future GAN development. The section also includes definitions and lemmas from convex analysis, focusing on projection mapping onto convex sets. The curr_chunk discusses Lemmas 1 and 2 in convex analysis, focusing on projection mapping onto convex sets and the Lipschitzness of an operator. It also introduces the notion of strong monotonicity as a generalization of strong convexity. The curr_chunk discusses the proof of Proposition 1, showing that simultaneous iterates diverge geometrically while alternating iterates are bounded but do not converge to 0. It presents the update rules and concludes with a summation formula. The update rule (39) leads to telescoping sums, and by simple linear algebra, the matrix M has complex conjugate eigenvalues. Diagonalizing M gives us bounded sequences that do not converge to 0. Additionally, a more precise proposition than Proposition 2 is proved in this section. In this section, a more precise proposition than Proposition 2 is proven. The squared norm of the iterates decreases geometrically for any 0 < \u03b7 < 1. The update rules for the implicit and extrapolation methods are discussed, along with an extension of the algorithm on the general unconstrained bilinear example. The feasibility of the problem is assumed, leading to a re-written equation. The study of simultaneous, alternating, extrapolation, and implicit updates rules for a certain case can be reduced to the study of unidimensional updates. This reduction has been proposed before, and a lemma states that a wide class of first-order methods can be studied on a more simplified version. Lemma 3 states that a general class of first-order methods on a specific problem can be simplified to unidimensional updates. The methods involve update rules with algorithm coefficients that depend on the iteration number. The alternating rule simplifies the update process to unidimensional problems, with coefficients rescaled by singular values of A. This reduction allows for independent updates of each coordinate, leading to a more efficient solution approach. The lemma provides a step-size for the general unconstrained bilinear objective. Corollary results extend previous propositions and introduce new findings. Tseng (1995) also derived similar bounds but with less tight results. Our proof technique simplifies the setting for easier understanding. The reduction above provides a simple proof for a simple setting, showing that simultaneous iterates diverge geometrically while alternating iterates are bounded but do not converge to 0. Extrapolation and Implicit methods generate iterates that converge linearly on a bilinear unconstrained problem. Projected extrapolation from the past involves a specific update step with projection onto the constraint set. Perform update step: DISPLAYFORM1 where P \u2126 [\u00b7] is the projection onto the constraint set \u2126. An operator F : \u2126 \u2192 R d is said to be \u00b5-strongly monotone if DISPLAYFORM2. Theorem' 1 states that if F is \u00b5-strongly monotone and L-Lipschitz, then updates FORMULA1 and FORMULA0 with \u03b7 = 1 4L provide linearly converging iterates. To prove this, a more general result is established with the convention \u03c9 0 = \u03c9 \u22121 = \u03c9 \u22122. Three technical lemmas are proven, showing that if F is \u00b5-strongly monotone, certain inequalities hold. In this section, a refined merit function is presented for handling an unbounded constraint set \u2126. The operator F is discussed, along with the concept of strong monotonicity and Lipschitz continuity. Theorems and lemmas are used to establish linear convergence of iterates with specific update steps and parameters. A refined merit function is derived from a stronger formulation of the variational inequality problem, which is useful for convergence analysis. The merit function is a non-negative function that equals zero if and only if the variational inequality holds. It is used to handle unbounded constraint sets and ensure convergence in the optimization process. The restricted merit function is used to handle unbounded constraint sets in optimization. It ensures convergence by measuring how much the constraint is violated on the restriction set. The reference point is arbitrary, usually the initialization point. The merit function is standard in variational inequality literature and becomes more interpretable when derived from gradients of a zero-sum game. Care must be taken when extending properties from minimization to saddle point settings. In the appendix, a more general set of assumptions is adopted for a bilinear game. Assumption 4 relaxes the bounded constraint set requirement from Assumption 3. The restricted merit function Err (VI) R is defined for saddle point problems, providing a more interpretable measure related to the cost function. If the equilibrium (\u03b8 * , \u03d5 * ) \u2208 \u2126 * \u2229 \u2126 R and L(\u00b7, \u03d5 * ) and \u2212L(\u03b8 * , \u00b7) are \u00b5-strongly convex, the merit function for saddle points upper bounds the distance to the equilibrium. Care must be taken when extending results from minimization to minimax settings. Another potential merit function for saddle point optimization is to extend the suboptimality f (\u03c9) \u2212 f (\u03c9 * ) used in standard minimization. In the context of saddle point optimization, the convergence rate on the \"primal-dual\" gap was discussed by Yadav et al. (2018) using the SGD method for GANs. However, the merit function P may not be reliable if the function L is not strongly convex-concave. Non-convex cost functions can lead to the operator F not being monotone, affecting the existence of solutions to (MVI). Despite this, (VIP) and (MVI) can still be defined, with (VIP) being a local condition and (MVI) a global one. The property of (MVI) is that it is a global condition for the minimization of a function. A less restrictive way to consider variational inequalities is to use a local version of (MVI). An extension similar to AvgExtraSGD Alg. 2 combines extrapolation and SGD, re-using the mini-batch sample for the update of the current point. The AvgExtraSGD algorithm combines extrapolation and SGD, re-using mini-batch samples for updating the current point. The method aims to correct oscillations caused by stochasticity, but faces issues due to biased estimators. The convergence properties of the algorithm are discussed, with a focus on the boundedness of iterates. Comparisons with other methods are also made. The proof of the theorems in this paper focuses on adapting the convergence certificate from a previous method to show an extra variance term. The resulting variance is larger than that of the AvgSGD method due to the Lipschitz constant of the operator F. A lemma is also proven to support the theorems, working with a defined merit function. The proof of the theorems in this paper focuses on adapting the convergence certificate from a previous method to show an extra variance term. The resulting variance is larger than that of the AvgSGD method due to the Lipschitz constant of the operator F. A lemma is also proven to support the theorems, working with a defined merit function. For all t \u2265 0, the operator F is involved in the calculations, leading to an upper bound on the sum. Theorem 2 states that Alg. 1 with constant step-size \u03b7 converges for all T \u2265 1. By maximizing over \u03c9, an upper bound is obtained for the sum, with unbiased estimates of F. Theorem 3 proves convergence of Alg. 1 with step-size restrictions. Lemma 7 is used to show convergence rate for any T \u2265 1 under specific assumptions. Lemma 7 is used to show convergence rate for any T \u2265 1 under specific assumptions. The update rule and proof involve applying various inequalities and Lemmas to establish convergence. Proof of Theorem 4 is obtained by combining Lemma 9 and Lemma 8. The minimax formulation from BID12 is used, with the objective being concave-concave, making it challenging to optimize. Comparing methods on this objective, with \u03c9* = -2, shows that extrapolation performs better than other methods in achieving convergence. Extrapolation outperforms other methods in achieving convergence on a challenging concave-concave objective with \u03c9* = -2. Additionally, using ExtraAdam can train WGAN-GP significantly faster by updating the discriminator and generator only twice, with a small drop in the final score. Inception scores and FID scores show consistent improvements with extrapolation and Exponential Moving Average (EMA). Using Exponential Moving Average (EMA) consistently improves FID scores, but uniform averaging does not enhance performance. Samples from uniform averaging are blurrier, affecting FID sensitivity. ExtraAdam converges faster than Adam baselines in DCGAN architecture with WGAN-GP on CIFAR10. Inception scores for WGAN-GP (DCGAN) on CIFAR10 show improvements with different learning rates. In this section, the comparison of different training methods in DCGAN architecture with WGAN-GP experiment is discussed. The study shows that AvgExtraAdam is less affected by the choice of learning rate compared to other methods. The experiment focuses on the inception score and sample quality, highlighting the advantages of ExtraAdam with higher learning rates. In this section, the comparison of sample quality on the DCGAN architecture with the WGAN-GP objective of AltAdam5 and AvgExtraAdam for different step-sizes is discussed. AltAdam5 is sensitive to step-size tuning, while AvgExtraAdam remains stable. The importance of robustness to step-size tuning for optimization algorithms is emphasized to save time for tuning other hyperparameters. The impact of uniform averaging on method performance is also analyzed, showing differences in the inception score based on the number of generator updates. Uniform averaging improves the inception score but results in slightly more blurry samples. The Fr\u00e9chet Inception Distance (FID) from BID17 is computed using 50,000 samples in WGAN experiments. Averaging performs worse compared to the Inception Score, with the FID being more sensitive to blurriness."
}