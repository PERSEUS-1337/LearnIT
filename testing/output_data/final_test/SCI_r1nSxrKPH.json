{
    "title": "r1nSxrKPH",
    "content": "Hierarchical Reinforcement Learning (HRL) aims to improve RL agents in sparse reward environments by operating at different levels of temporal abstraction. A new architecture, Hierarchical Decompositional Reinforcement Learning (HiDe), decomposes hierarchical layers into independent subtasks while enabling joint training of all layers. By combining a control policy on a lower level with an image-based planning policy on a higher level, HiDe shows promise in complex continuous control tasks. Reinforcement learning (RL) has been successfully applied to sequential-decision making tasks, such as video games and robotics. However, RL agents struggle with planning over long horizons in navigation tasks with sparse rewards. This paper explores generalization and transfer of higher level policies in complex continuous control tasks. Hierarchical Reinforcement Learning (HRL) addresses challenges of sparse rewards by breaking tasks into simpler subtasks. Methods include learning from demonstrations and enhanced exploration strategies. Curriculum learning is often used to train hierarchies, with recent success in training policies jointly through off-policy learning. In this paper, a novel multi-level HRL architecture is proposed to address issues with generalization in unseen environments. The architecture enables functional decomposition and temporal abstraction by decoupling planning and low-level control in a 3-level hierarchy. This modular design improves generalization ability by providing task-relevant information for predefined tasks. The architecture proposed in the paper enables policies learned on a single task to solve randomly configured environments by decoupling planning and low-level control in a 3-level hierarchy. The planning layer can be successfully transferred to new agents, with global environment information provided only to the planning layer during training. The agent's internal state is only accessible by the control layer, with actions and goals relative to current position to prevent overfitting. The planner learns to find a trajectory to the goal using a value map and attention network. The highest level policy issues positions maximizing the masked value map as goal input. The middle layer serves as an interface between the upper planner and lower control layer, refining subgoals for the agent. It decouples planning from continuous control, with policies learning to cooperate. This work focuses on solving long-horizon tasks with sparse rewards in complex navigation domains, showing challenges in generalization and the ability to train in fixed and random environments. The text discusses a novel multi-layer HRL architecture for navigation tasks, enabling functional decomposition and transfer of individual layers between agents. Results show effectiveness in generalizing to unseen environments. Key contributions include hierarchical policy learning and generalization beyond training conditions. Recent works have demonstrated hierarchical reinforcement learning architectures in continuous domains, such as FeUdal Networks (FUN) inspired by feudal reinforcement learning. FUN achieves hierarchic decomposition through a learned state. Hierarchical reinforcement learning architectures in continuous domains involve hierarchic decomposition through learned state representations in latent space. Various methods, such as KL-divergence regularized RL and HIRO, introduce hierarchical structures to improve training efficiency and task performance. HIRO-LR extends HIRO by learning representation spaces from environment images. HIRO-LR, a hierarchical reinforcement learning approach, learns a representation space from environment images to replace state and subgoal spaces with neural representations. While it can generalize to a flipped environment, retraining is necessary as only the learned space representation generalizes. In contrast, HiDe can generalize without retraining. Levy et al. (2019) introduce Hierarchical Actor-Critic (HAC) for learning multiple policies in parallel in sparse reward environments. HAC, HIRO, and HIRO-LR consist of nested policies with a modular design to decouple the functionality of individual layers. The design of HAC and HIRO involves decoupling layers to define different state, action, and goal spaces. Global information is only available to the planning layer, while lower levels receive specific information. Both methods use global information in the form of a top-down view image. Model-based reinforcement learning focuses on learning a dynamics model and planning. Eysenbach et al. (2019) propose a planning method involving a graph search over the replay buffer. Unlike other methods, they require the agent to spawn at different locations and learn a distance function for building the search graph. In model-free reinforcement learning, a spatial value map is learned from rewards to build a search graph. Differentiable planning modules like VIN and MVProp have been proposed to improve parameter efficiency. Contrary to prior work, the planning layer in the approach is based on MVProp but does not rely on a fixed neighborhood mask for action selection. In contrast to prior work, our approach proposes learning an attention mask for generating intermediate goals instead of relying on a fixed neighborhood mask for action selection. We model a Markov Decision Process augmented with a set of goals G, defining it as a tuple M = {S, A, G, R, T, \u03c10, \u03b3}. Our approach involves learning an attention mask for generating intermediate goals in a Markov Decision Process with states, actions, goals, rewards, transition dynamics, initial state distribution, and a discount factor. The goal is to find a policy that maximizes expected return using an actor-critic framework. The Q-function and policy are approximated using neural networks, with training objectives to minimize loss and maximize Q-value. Hindsight Experience Replay is used to address sparse rewards and improve sample-efficiency in training goal-conditioned tasks. In Hindsight Action Conditioned (HAC), techniques are applied to improve sample-efficiency in training goal-conditioned environments by relabeling desired goals as achieved states. This data augmentation allows learning from failed episodes and addresses challenges introduced by non-stationary hierarchical policies and sparse rewards. HAC simulates optimal behavior of lower-level policies by replacing actions with achieved states, similar to HER. The techniques of reward assignment and subgoal testing are effective in training models. Differentiable value iteration networks (VIN) and value propagation networks (MVProp) are proposed for path planning and navigation tasks, with MVProp offering better sample efficiency and generalization. MVProp creates reward and propagation maps to guide the agent through the environment. The reward map is an imager i,j of the same size as the environment image I, where i,j = 0 if the pixel overlaps with the goal position and \u22121 otherwise. The value map V is calculated by unrolling max-pooling operations in a neighborhood N for k steps. The maximum value propagation network (MVProp) calculates a value map V based on the top-view environment image I and goal G. The policy \u03c0 2 selects coordinates with maximum value to assign subgoals relative to the agent's current position. The action is selected to be the pixels maximizing the value in a predefined 3x3 neighborhood N of the agent's current position. The HiDe architecture allows for functional decomposition across layers, with an abstract planning layer guiding a control component. The top layer plans a trajectory towards a goal, the middle layer converts this into subgoals, and the lowest layer learns a control policy for agent locomotion. Functional decoupling across layers depends on reducing the state to task-relevant information. The HiDe architecture focuses on functional decomposition across layers, with a planning layer guiding control. The planning layer learns high-level actions for navigation tasks, improving generalization. A planning-specific layer is introduced to learn the map and find a feasible path to the goal, utilizing a value propagation network to project rewards onto the environment image. The architecture focuses on functional decomposition with a planning layer guiding control. A convolutional network determines flow probability in the environment image. An attention model dynamically defines the neighborhood for goal position. A MVProp network is augmented with the ability to estimate how far the agent can go using a 2D Gaussian model. A 2D mask is built based on the likelihood function. The dynamic attention window shapes the agent's subgoals based on environment image likelihood. The planner policy selects actions maximizing the masked value map, improving generalization performance. The attention model considers agent dynamics for assigning fine or coarse-grained subgoals. The Gaussian window defines a dynamic set of actions for the planner policy, limited to reachable pixels for better obstacle avoidance behavior. Using soft-argmax allows for real-valued actions and invariance to image resolution, with no difference in final performance observed in experiments. The planner policy uses a Gaussian window for dynamic actions, while the hierarchy interfaces high-level planning with low-level control through temporal abstraction. The interface layer policy receives subgoals from the upper layer and selects actions for the lower-level control layer in 2D space. The policy \u03c0 1 uses subgoals relative to the agent's position to generalize and learn better. The lowest layer learns a goal-conditioned control policy with access to the agent's internal state, while higher layers only have access to the agent's position. Hindsight goal transition techniques are used for rewards in failure cases. The hierarchy is jointly-trained using DDPG for control and interface layers, and DQN for the planning layer. Our method utilizes a hierarchy of control layers, with DDPG for control and interface layers, and DQN for the planning layer. The evaluation is done on simulated continuous control tasks in navigation-based environments using the MuJoCo physics engine. Various baseline methods are compared, showcasing the model's generalization capabilities in a more complex maze design. Functional decomposition is demonstrated by combining planning and locomotion layers of different agents. An ablation study is provided for design choices, with task configurations like Maze Forward for reaching a goal from a fixed start point. Our method utilizes a hierarchy of control layers, with DDPG for control and interface layers, and DQN for the planning layer. The evaluation is done on simulated continuous control tasks in navigation-based environments using the MuJoCo physics engine. Various baseline methods are compared, showcasing the model's generalization capabilities in a more complex maze design. We always train in the Maze Forward environment, with a reward signal of -1 unless the agent reaches the goal. The agents are tested on tasks with fixed starting and goal positions, aiming to answer questions about generalization and transferability of planning layer policies. Our method utilizes a hierarchy of control layers, with DDPG for control and interface layers, and DQN for the planning layer. Various baseline methods are compared in a simple Maze Forward environment. Improvements were made to HAC and HIRO implementations, with observations of oscillations around the goal for HIRO agents. A modified HAC model, RelHAC, was trained to assess the planning layer. RelHAC, a modified HAC model, has the same lowest and middle layers as HiDe but lacks an effective planner in the top layer. Baseline models are trained with fixed start and random goal positions, achieving successful task learning. HIRO shows slightly better performance due to dense rewards, while RelHAC performs worse than HAC. Our method outperforms HIRO, HIRO-LR, HAC, and RelHAC in generalization abilities to unseen environments. While these models suffer from overfitting to the training environment, our method achieves higher success rates in generalization tasks. Training our model with random goal positions yields a more robust model, surpassing vanilla HiDe. Subsequent experiments only report results for our method, as it has shown superior performance. In experiments, the model outperforms baseline methods in generalization to unseen environments. Training an ant and a ball agent in a Maze Forward task with a complex layout, the model successfully transfers navigation skills. Performance in Maze Backward and Maze Flipped tasks remains consistent despite increased difficulty. The model shows similar or better performance in randomly generated mazes. Individual layers in the architecture learn separate sub-tasks, demonstrated by transferring them across different agents. The study demonstrates the transfer of planning layers between agents, showing improved success rates in maze tasks. The ant model's planning layer is transferred to a simpler ball agent, resulting in increased success rates for random and forward maze tasks. The performance remains consistent in backward and flipped configurations. In maze tasks, HiDe-A and HiDe-AR methods with absolute subgoals show success rates. HiDe-NI method without the interface layer is also evaluated. The study transfers planning layers between agents, showing improved success rates in maze tasks. The compositional agent performs marginally better or worse in different tasks. Transfer of layers between agents is possible, validating the hypothesis. Training is estimated to be 3-4 times faster with the complex agent. The training process is estimated to be 3-4 times faster with the complex agent, as it does not need to learn the planning layer. The method's transfer capabilities are demonstrated by training a humanoid agent in an empty environment and connecting it with the planning and interface layer from a ball agent. Empirical results comparing different variants of the method with an ant agent show that architectural design choices lead to better generalization and functional decomposition. The results of experiments comparing different variants of the HiDe method show that relative positions improve performance and are crucial for generalization. Random goal position sampling can also benefit agents, but may not always be applicable. The interface layer in HiDe is essential for better performance compared to a variant without it. The interface layer in our architecture, HiDe, is crucial for improved performance in complex navigation tasks. An ablation study showed that HiDe with a learned attention window outperformed fixed window sizes. The architecture includes a planning layer, subgoal refinement layer, and low-level control layer, allowing for end-to-end training. Our method can generalize to unseen settings and transfer planners between different agents, such as from a ball to an ant or humanoid. Future work aims to integrate a more general planner beyond navigation-based environments. Mujoco environments are used with episode termination after 500 steps in one experiment and 800 steps in others. Rewards are sparse, with a goal reached if within a certain distance. HIRO sets distant goals to encourage lower layer learning. The Ant and Ball agents in the study have specific modifications for their gear power and frame skip. The maze environment consists of immovable blocks of a certain size, with the Ball agent designed to roll instead of slide. The maze shapes are illustrated in Figure 1, with randomly generated mazes having a probability of block emptiness. The maze environment consists of randomly generated mazes with a probability of block emptiness. Start and goal positions are randomly sampled, and mazes where the goal is not reachable are discarded. 500 environments were generated for evaluation, and the PyTorch implementation will be available on the project website. The original authors' implementations were used for both HIRO and HAC, with modifications made to improve HAC's performance in experiment one. Target networks were added to both the actor and critic for HAC. The maze environment includes randomly generated mazes with start and goal positions. Target networks were added to both the actor and critic for HAC to improve performance. Input images for the planning layer were binnarized, and the tables in the paper contain means and standard deviation across 5 seeds. In the planning layer, input images are processed through convolutional layers and max pooling to generate a value map. The final result is passed through fully connected layers to produce three outputs. The covariance matrix is calculated to ensure symmetry and positive definiteness. In the planning layer, input images are processed through convolutional layers and max pooling to generate a value map. The final result is passed through fully connected layers to produce three outputs. The covariance matrix is adjusted to be symmetric and positive definite by using a binarized kernel mask instead of actual Gaussian densities. The network architecture includes fully connected layers with ReLU activation and tanh activation for the locomation layer scaled to the action range. Discount \u03b3 = 0.98 for all agents, Adam optimizer with learning rate 0.001, soft updates using moving average with \u03c4 = 0.05, and a replay buffer size of 500 episodes. 40 actor and critic learning updates are performed after each epoch on each layer. In the planning layer, input images are processed through convolutional layers and max pooling to generate a value map. The final result is passed through fully connected layers to produce three outputs. The network architecture includes fully connected layers with ReLU activation and tanh activation for the locomotion layer scaled to the action range. Discount \u03b3 = 0.98 for all agents, Adam optimizer with learning rate 0.001, soft updates using moving average with \u03c4 = 0.05, and a replay buffer size of 500 episodes. 40 actor and critic learning updates after each epoch on each layer, with batch size 1024, no gradient clipping, rewards 0 and -1 without normalization, subgoal testing only for the middle layer, maximum subgoal horizon H = 10 for all 3 layers algorithms, 2 HER transitions per transition using the FUTURE strategy, exploration noise values specified for each layer. Observations were not normalized. Results collected for the paper including individual runs are presented."
}