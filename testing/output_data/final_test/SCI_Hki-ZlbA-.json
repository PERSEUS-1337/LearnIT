{
    "title": "Hki-ZlbA-",
    "content": "The presence of adversarial examples limits the deployment of neural networks in safety-critical systems. Various techniques have been proposed to train networks robust to such examples, but stronger attacks continue to expose weaknesses in existing defenses. To address the challenge of assessing a network's robustness against future attacks, formal verification techniques are proposed. Ground truths are constructed as adversarial examples with a provably-minimal distance from a given input point, which can be used to evaluate the effectiveness of attack and defense techniques. The effectiveness of attack and defense techniques is assessed by computing the distance to ground truths before and after applying defense methods. This technique is used to evaluate recently suggested techniques. Recent work has shown that slight modifications to input data can cause unintended behavior in neural networks, leading to misclassification of normal instances. Adversarial examples pose a significant challenge for deploying neural networks in safety-critical settings. Various techniques have been proposed for generating and defending against adversarial examples. In recent years, new techniques have been proposed for the formal verification of neural networks to assess the robustness of defense mechanisms against adversarial attacks. These techniques aim to prove that a network satisfies a desired property or provide an input that violates the property. This cycle of evaluating defenses against attacks has cast doubt on the effectiveness of newly-proposed defensive techniques. Formal verification can rigorously assess the adversarial robustness of neural networks by finding ground-truth adversarial examples. This method involves examining networks and applying verification to identify these examples based on different labels assigned by the network. The distance to the ground truth indicates the network's robustness to adversarial attacks at a specific point. Ground truths can be used to estimate overall network robustness, assess attack techniques, and evaluate defense techniques. The paper suggests using ground-truth adversarial examples to study attacks and defenses in neural networks. First-order attack algorithms yield near-optimal results close to ground-truth adversarial examples. Adversarial training increases robustness without overfitting to specific attacks. The paper is organized into sections covering background, experiments, results analysis, and conclusion. In this paper, feed-forward neural networks for classification using softmax activation function are studied. The focus is on generating adversarial examples for greyscale MNIST using L\u221e and L1 distance metrics. Three popular methods for constructing adversarial examples are considered, including the Fast Gradient Method (FGM). The Fast Gradient Method (FGM) is a one-step algorithm that takes a single step in the direction of the gradient, with controls for step size and clipping to ensure the adversarial example stays within valid image space. The Basic Iterative Method (BIM) is an iterative application of FGM, while the Carlini and Wagner (CW) method constructs adversarial examples by solving a minimization problem with a chosen target. Neural networks are used for generalization but there is interest in proving their behavior in safety-critical systems. The Fast Gradient Method (FGM) and Basic Iterative Method (BIM) are algorithms for creating adversarial examples, with the Carlini and Wagner (CW) method solving a minimization problem for targeted attacks. The optimization involves minimizing the distance between the input and a target, using logits instead of softmax for better results. The paper introduces the ability to use L1 distortion in addition to L2 and L\u221e. The Reluplex algorithm is a technique for verifying deep neural networks with piecewise-linear activation functions. It checks if a network satisfies a given property by using a convex set of linear constraints on inputs and outputs. Reluplex is a simplex-based SMT solver that finds inputs satisfying the network's constraints. Reluplex is a sound and complete algorithm used to verify deep neural networks with piecewise-linear activation functions. It encodes the network and constraints as linear equations and ReLU constraints to determine the existence of adversarial examples within a certain distance of an input point. It either confirms the absence of adversarial examples or provides a counterexample. Adversarial training is a defense mechanism against adversarial examples where a classifier is trained, adversarial examples are generated, the classifier is retrained using these examples, and the process is repeated. This approach has been shown to be effective for networks with sufficient capacity. Adversarial training using strong iterative attacks increases adversarial robustness of small networks. The problem of neural network verification is NP-complete, and only networks with a few hundred nodes can be soundly verified. Our approach involves training a small fully-connected, 3-layer network on the MNIST dataset, achieving 97% accuracy with only 20k weights and fewer than 100 hidden neurons. As neural network verification becomes more scalable, our method could be applied to larger networks and different datasets. Adversarially trained MNIST model is constructed using iterative methods to generate adversarial examples during training. Our approach involves training a small fully-connected, 3-layer network on the MNIST dataset, achieving 97% accuracy with only 20k weights and fewer than 100 hidden neurons. Adversarial examples were generated using the basic iterative method. The network's capacity is smaller than the original, so adjustments were made. Verification was done using Reluplex implementation, supporting ReLU and max operators. The tool from BID12 was used to measure distances with L1 norm in addition to L\u221e norm. The study involved training a small network on the MNIST dataset, achieving 97% accuracy with minimal weights and neurons. Adversarial examples were generated using the iterative method. Verification was done using Reluplex, supporting ReLU and max operators. The tool from BID12 was used to measure distances with L1 and L\u221e norms, affecting performance. The goal was to find a ground-truth example with minimal distance from the input point. The study involved training a small network on the MNIST dataset to achieve 97% accuracy with minimal weights and neurons. Adversarial examples were generated using the iterative method and verified using Reluplex. The search procedure iteratively shrinks the range to find the closest adversarial input. The algorithm returns the distance to the ground truth example. The study trained a small network on the MNIST dataset to achieve 97% accuracy with minimal weights and neurons. Adversarial examples were generated using the iterative method and verified using Reluplex, which returns the distance to the ground truth example. The experiments used an adversarial input found using the CW attack as the initial x init to improve performance. Starting with a close x init reduces the number of required iterations until the distance is sufficiently small. Experiments using the L 1 distance metric were slower than those using L \u221e due to larger initial distances requiring more iterations. The study evaluated two neural networks, N and N', trained with adversarial training on 10 source images from the MNIST test set using L1 and L\u221e distance metrics. The CW attack was used to generate initial targeted adversarial examples, and Algorithm 1 was employed to search for ground-truth examples. Results are presented in TAB1 and Fig. 1, with a more detailed analysis in the appendix. Each row in TAB1 corresponds to a specific neural network and distance metric, describing the outcomes of the experiments. The study evaluated two neural networks trained with adversarial training on source images from the MNIST test set using different distance metrics. Results are presented in a table with details on 90 individual experiments, showing variability in performance. The study evaluated two neural networks trained with adversarial training on MNIST test set images using different distance metrics. Results showed variability in performance, with iterations ranging from 1 to 19 and varying time per iteration. Various test settings were analyzed to draw conclusions on the CW attack and defense of BID16. The study evaluated neural networks trained with adversarial training on MNIST test set images using different distance metrics. Iterative attacks produce near-optimal adversarial examples, with the CW attack performing better than the FGM method. Additional experiments indicated the CW attack is within 11.4% of the ground truth using L \u221e norm and 5.7% using L 1. The FGM method was less effective on the N network with L \u221e distance metric compared to iterative attacks. Gradient descent often converges to a local minimum instead of a global minimum, resulting in higher distortion rates for adversarial examples. Taking small steps towards the ground truth can improve convergence. When iterative attacks perform suboptimally compared to ground truths for a specific input and target label, it often indicates poor performance for that input and other target labels. Even small relative gaps on one instance can lead to large relative gaps for other target labels. For example, on an adversarially trained network attacked under L \u221e distance, ground-truth adversarial examples were significantly better than iterative attack results. This phenomenon was observed in extreme cases, showing the importance of taking random steps before searching for adversarial examples. The large gap in performance was caused by gradient descent leading away from the ground truth, resulting in the discovery of an inferior local minimum. The defensive technique of BID16 was evaluated by comparing N, L \u221e and N, L \u221e experiments. The results showed that the Madry defense is effective, improving the distance to the ground truth by an average of 427%. The Madry defense improves overall situation but can make things worse in some cases, with an average degradation of 12.8% in 7 out of 37 experiments. Evaluating defensive techniques and network robustness over a large dataset is crucial. Training on iterative attacks does not lead to overfitting issues commonly seen in adversarial training. The Madry defense improves network accuracy against specific attacks but may decrease accuracy on other attacks. Adversarial training of BID16 shows no evidence of overfitting, with ground truths improving on the CW attack by approximately 11.4%. The defense of BID16 makes networks easier to analyze for robustness using L \u221e and L 1 distance metrics. The Madry defense improves network accuracy against specific attacks but may decrease accuracy on other attacks. For L1, the termination rate was 71 for the hardened network compared to just 12 on the standard network. The adversarially trained network may make less use of ReLU units, leading to faster convergence in experiments. Neural networks are promising for safety-critical systems, but their vulnerability to adversarial examples is a challenge. Defensive techniques need to be evaluated against various attacks. Ground-truth adversarial examples are introduced in this paper using formal verification approaches. Evaluation of attack BID1 shows it produces adversarial examples close to optimal, while defense BID16 increases distortion. The study introduces ground-truth adversarial examples using formal verification approaches. Defense BID16 increases distortion to adversarial examples by 427% on the MNIST dataset. Verification tools have limited scalability but improvements are expected. Designing networks for verification techniques can provide strong guarantees of correctness and robustness in safety-critical settings."
}