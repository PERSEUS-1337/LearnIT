{
    "title": "H1ewdiR5tQ",
    "content": "The proposed Graph Wavelet Neural Network (GWNN) improves upon previous spectral graph CNN methods by utilizing graph wavelet transform instead of graph Fourier transform. GWNN outperforms spectral graph CNNs in graph-based semi-supervised classification tasks on benchmark datasets. CNNs have been successful in various machine learning problems with an underlying Euclidean structure. The success of CNNs lies in leveraging the statistical properties of Euclidean data, such as translation invariance. However, when dealing with non-Euclidean data like graphs, convolution becomes a challenge due to varying neighborhood sizes for each node. Methods to generalize CNNs to graph data fall into spatial and spectral categories based on how convolution is defined. Spatial methods directly define convolution on the vertex domain, treating each vertex's neighborhood as a weighted average function. The challenge in generalizing CNNs to graph data lies in defining a convolution operator that can handle varying neighborhood sizes while maintaining weight sharing. Spatial methods use a weighted average function over vertex neighborhoods, while spectral methods leverage graph Fourier transform to define convolution in the spectral domain, maintaining the weight sharing property of CNNs. Graph wavelet neural network introduces a new approach for efficient convolution on graph data by using graph wavelets as bases instead of eigenvectors of the Laplacian matrix. This method avoids the computational cost of eigendecomposition and offers sparsity, making it more efficient than spectral CNNs. The graph wavelet transform is more efficient than the graph Fourier transform as it is localized in the vertex domain. The proposed graph wavelet neural network reduces the number of parameters by learning a sole convolution kernel among all features. Experimental results validate the effectiveness of the network in graph-based semi-supervised classification. Our method outperforms previous spectral CNNs on benchmark datasets like Cora, Citeseer, and Pubmed. The graph Laplacian matrix and normalized Laplacian matrix are defined, with eigenvectors associated with eigenvalues representing frequencies of the graph. The graph Fourier transform of a signal on a graph is defined using eigenvectors of the normalized Laplacian matrix as bases. The graph convolution operator is defined using the convolution theorem, with limitations when using Fourier transform. ChebyNet BID4 addresses limitations of using Fourier transform for graph convolution by restricting the convolution kernel to a polynomial expansion. This approach allows for determining the range of node neighborhoods and coefficients for the convolution operation. Graph wavelet transform addresses limitations of using graph Fourier transform for graph convolution by employing a set of wavelets as bases. These wavelets are defined as diffused signals on the graph, allowing for a more flexible and localized transformation of graph signals. The graph wavelet transform is defined as a signal x on a graph, with benefits including high sparsity and localized convolution compared to the graph Fourier transform. The graph wavelet transform is localized in the vertex domain and offers flexible neighborhood adjustments by varying the scaling parameter. This method outperforms Fourier transform in tasks like graph-based semisupervised learning. The graph wavelet neural network (GWNN) replaces Fourier transform with wavelet transform and is a multi-layer convolutional neural network. It consists of layers that transform input tensors into output tensors for semi-supervised node classification on a graph. The model includes a two-layer GWNN with a loss function based on cross-entropy error over labeled examples. The loss function in GWNN is cross-entropy error over labeled examples. The weights are trained using gradient descent. Each layer's parameter complexity is O(n \u00d7 p \u00d7 q). Conventional CNN methods learn convolution kernel for each pair of input and output features, resulting in a large number of parameters. GWNN separates feature transformation and graph convolution in each layer to address this issue. Graph convolution involves feature transformation using a parameter matrix W, feature matrix X, graph convolution kernel matrix F, and a non-linear activation function h. By separating feature transformation from graph convolution, the parameter complexity is reduced from O(n \u00d7 p \u00d7 q) to O(n + p \u00d7 q). Graph convolutional neural networks generalize CNNs to graphs by defining convolution operators on graphs, with methods classified into spectral and spatial categories. Spectral methods define convolution via convolution theorem, while spatial methods define convolution differently. Various approaches like Spectral CNN BID3, BID1, BID4, BID14, BID12, and BID16 have been developed for implementing CNNs on graphs, each offering unique methods for graph-based tasks. GraphSAGE defines convolution as a weighted average function over neighborhoods of target vertices, while GAT proposes learning the weighting function through self-attention mechanism. MoNet offers a general framework for designing spatial methods, with some works focusing on enhancing graph convolutional networks' power. GraphsGAN generalizes GANs to graphs and generates fake samples in low-density areas to improve performance in graph-based semi-supervised learning. Graph wavelets, introduced by Sweldens in 1998, offer a simple way to adapt wavelets to graphs without the need for a learning process. Various methods have been proposed to construct wavelet transforms on graphs, including bypassing eigendecomposition of the Laplacian and approximating wavelets with Chebyshev polynomials. These graph wavelets have been utilized for tasks such as multi-scale community mining and learning structural node embeddings. Experimental evaluation on datasets like Cora, Citeseer, and Pubmed demonstrates the effectiveness of Graph Wavelet Neural Networks (GWNN) for semi-supervised node classification. In three citation network datasets, nodes represent documents and edges are citation links. The label rate denotes the proportion of labeled nodes used for training. Comparisons are made with traditional semi-supervised learning methods like label propagation, semi-supervised embedding, and graph embeddings. Graph convolutional networks are effective in semi-supervised learning, and comparisons are made with Spectral CNN and ChebyNet. We compare our two-layer graph wavelet neural network with Spectral CNN, ChebyNet, and GCN as baselines. We use Adam optimizer with lr = 0.01 and initialize weights following a specific method. Hyperparameters s and t are optimized through grid search, with s = 1.0 and t = 1e \u2212 4 for Cora dataset. The hyperparameters for different datasets are specified, dropout BID25 is used to prevent overfitting, training is stopped if validation loss doesn't decrease for 100 epochs. The number of parameters for GWNN is high for large networks, so detaching feature transformation is validated on ChebyNet, reducing parameter complexity. Performance and parameter comparison on three datasets are shown in TAB1. The reported performance of GWNN with detaching technique shows improved accuracy on Pubmed due to reduced parameter complexity, alleviating overfitting. However, there is a slight drop in accuracy on Citeseer. Experimental results in TAB2 demonstrate GWNN's effectiveness in node classification, outperforming Spectral CNN with a 10% improvement on Cora. The proposed GWNN outperforms Spectral CNN, ChebyNet, GCN, and MoNet in node classification tasks, showing significant improvements on Cora, Citeseer, and Pubmed datasets. GWNN's success is attributed to its flexible diffusion range and ability to learn convolution kernels effectively. Additionally, the use of wavelet transform ensures sparsity in both spatial and spectral domains. The sparsity of graph wavelet transform is illustrated using Cora as an example, with the transform matrix being much sparser than the Fourier transform matrix. The input features in Cora are represented by a binary matrix, with each column representing the feature vector of a word. The projected vectors via wavelet and Fourier transforms in Cora show sparsity and interpretability. Each word's feature has a spectral wavelet basis associated with it, representing the relation between the word and the document. The relation between words and documents is represented by values in p. Analyzing the projected values of different features, such as Word 984 and Word 1177, top-10 active bases are selected. The nodes in the subgraphs represent documents in the Cora dataset, clustered by OpenOrd BID18. Word 984 appears 8 times in Cora, all belonging to the \"Case-Based\" class. The top-10 active bases of Word 984 and Word 1177 in the Cora dataset are analyzed. Word 984 is exclusively found in the \"Case-Based\" class, with its active bases also belonging to the same class. On the other hand, Word 1177 appears in various classes, indicating its universality. The properties of graph wavelets in Cora dataset allow for an interpretable domain transformation and ease understanding of graph convolution. GWNN, which replaces graph Fourier transform with graph wavelet transform, has advantages such as locality, sparsity, computational efficiency, and localized convolution in the vertex domain. Detaching feature transformation from convolution helps reduce parameters and dependence on large training data. Detaching feature transformation from convolution in GWNN allows for localized graph convolution in the vertex domain, improving performance on large graphs for semi-supervised learning. Using wavelets and matrix multiplication, convolution is localized, as shown by the correlation between nodes during convolution. The locality of the convolution matrix suggests that graph convolution is localized in the vertex domain. Graph convolution in GWNN is localized in the vertex domain, with the scaling parameter s modulating the range of neighborhoods. Increasing s improves accuracy on Cora, but too large values include irrelevant nodes. The hyperparameter t has minimal impact on performance and is chosen via grid search. A suitable s falls within [0.5, 1] to capture graph structure and ensure convolution locality, while t is less sensitive to the dataset. The parameter complexity of Spectral CNN is high, making it difficult to generalize to real world networks. ChebyNet reduces parameter complexity by approximating the convolution kernel via polynomial function of Laplacian eigenvalues. GCN simplifies ChebyNet by setting K=1. Implementing GWNN and Spectral CNN separately reduces parameters to O(n + p * q). GWNN outperforms ChebyNet in Cora and Citeseer due to smaller parameter complexity, showing promise for convolution via graph wavelet transform. However, GWNN's parameter complexity is larger than ChebyNet in Pubmed. Future work could involve selecting wavelets associated with a subset. ChebyNet reduces parameter complexity by using Chebyshev polynomials for convolution. The sparsity of graph wavelets depends on Laplacian matrix sparsity and hyperparameter s. The Laplacian matrix is sparser than graph wavelets, limiting the method's effectiveness. The method discussed in the current chunk utilizes graph wavelets for localized spectral graph convolution, distinguishing it from GCN and ChebyNet. GWNN offers a localized graph convolution by replacing graph Fourier transform with graph wavelet transform, providing good interpretability and spectral basis with localization property."
}