{
    "title": "rJxGGlSKwH",
    "content": "In this work, a self-supervised method is proposed to learn sentence representations by incorporating linguistic knowledge. The goal is to leverage diverse linguistic frameworks to represent sentences by contrasting different views. This approach aims to build embeddings that capture semantic meaning more effectively and are less influenced by the outward form of the sentence. We propose learning sentence embeddings by contrasting multiple linguistic representations to encode high-level representations and align shared information from different views. The model is trained with a contrastive framework to map similar input sentences to close representations while separating unrelated ones. This approach extends the distributional hypothesis to sentences, assuming that meaning can be inferred from context sentences. In extending the framework for learning sentence embeddings, dependency trees are utilized to describe the compositional structure of sentences. The sentence is represented as an oriented acyclic graph where nodes are words and edges show relations between words. Tree representations are then mapped in a shared embedding space using Tree LSTM networks. Model parameters are learned with a discriminating objective. Representation learning in NLP has gained attention, with parameters learned using a discriminating objective. Proxy tasks are used to learn representations, falling into categories of predicting contextual information or discriminating multiple representations. The contrastive multi-views framework involves training a model to distinguish between different views of context sentences and negative examples using LSTM and Tree LSTM networks. Proxy tasks in representation learning aim to contrast between samples by either altering input or discriminating multiple representations of the same data among negative examples. Various methods have been proposed in the literature, such as word embeddings for NLP tasks, learning representations through solving jigsaw puzzles for images, and predicting the future to represent video or audio. The second proxy objective is motivated by the observation that end-to-end frameworks tend to learn semantic similarities among classes, as seen in the hidden layers of supervised neural networks assigning close representations to similar images like leopard and jaguar. Contrastive learning methods aim to distinguish individual samples to ensure global consistency in the dataset. Various contrastive learning techniques have been effective in different domains like image processing, audio input, sentence representation, and word embedding. The selection of samples that lead to close representation can be achieved by considering different views of the same sample. Contrastive learning methods aim to ensure global consistency in datasets by distinguishing individual samples. Different techniques, such as combining multi-views of images and using contrastive frameworks for sentences, have been proposed. Various metrics, like scalar product and Mutual Information, are used to measure proximity in the representation space. Contrastive learning is a self-supervised method for learning semantic mappings with compact representations. Contrastive learning methods focus on learning semantic mappings with compact representations. One approach is to treat each data sample as a distinct class and train a classifier to distinguish between individual instances. Negative sampling and Noise Contrastive estimation methods are used to approximate full instance discrimination. Contrastive learning methods aim to learn semantic mappings with compact representations by training a scoring function to assign high scores to positive pairs and low scores to negative pairs. Different scoring functions have been used, such as the inner product or a combination of distance and angle measures. The classifier is trained using negative log likelihood loss, with the positive pair constructed from the data samples. In contrastive learning, a scoring function is trained to assign high scores to positive pairs and low scores to negative pairs using negative log likelihood loss. Positive pairs are built from different views of the input data, with representations obtained from bidirectional LSTM and ChildSum Tree LSTM. The model is trained to identify semantic mappings with compact representations. The model is trained to identify the sentence appearing in the context from the target sentence by building different views of the data. Models are trained on the BookCorpus dataset and a similar dataset is generated using smashword open book data. The obtained dataset contains 78M sentences from 17,000 books with a 20k words vocabulary. Sentences are tokenized using Microsoft BlingFire tool. Models are trained on a single epoch on the entire corpus without any train-test split. The Tree LSTM models are trained on the entire corpus without a train-test split. They parse sentences in dependency using the Spacy parser. The contrastive learning method draws negative examples for each sentence pair. Training follows the procedure proposed in Logeswaran & Lee (2018) with mini-batches respecting book sentences order. Models are optimized using Adam with a learning rate of 5e-3 and a batch size of 100. The hidden layer size is 1000, embedding dimension is 300, and the model is trained on a 1080Ti Nvidia GPU. All model weights are initialized with a xavier distribution and biases to 0, with biases initialization having a significant impact on training. The Contrastive Tree LSTM training process was significantly slower compared to vanilla LSTM, with training stopped after 33 hours on only 4.6M sentences out of 78M available. Performance on downstream tasks was monitored during training. The training process for Contrastive Tree LSTM involves specific evaluation processes at each learning step, focusing on the intermediate representation's ability to solve the final task. The linear evaluation protocol is used to compare representations, utilizing a logistic regression model for downstream task solving. The algorithm can either learn to map inputs directly to targets in an end-to-end framework or proceed in two phases to learn a representation first. The algorithm learns a representation with a self-supervised proxy task, which is then used for various downstream tasks. Evaluation is done on 7 classification tasks from the SentEval benchmark, including movie review sentiment, product reviews, subjectivity classification, opinion polarity, question type classification, semantic relatedness, entailment, and paraphrase identification. Different tasks have different train/dev/test splits for evaluation. The algorithm learns a representation with a self-supervised proxy task for downstream tasks. Evaluation is done on 7 classification tasks from the SentEval benchmark. The model's performance is evaluated using the proposed train/dev/test splits, with the dev set used for regularization parameter selection. Results are reported on the test set using a logistic regression classifier trained on top of sentence representations. The model's scores are not as good as state-of-the-art results, but show promising trends that suggest improvement with more training data exposure. The table compares sentence representations on downstream tasks, showing performance metrics for different models. Probing tasks are used to evaluate individual linguistic aspects. The probing benchmark evaluates linguistic properties of representations through tasks such as SentLen, WC, TreeDepth, TopConst, BShift, Tense, SubjNum, ObjNum, SOMO, and CoordInv. Results for Contrastive LSTM (CL) and Contrastive Tree LSTM (CTL) are compared in Table 2, with bold-face numbers indicating the best performance values. The interaction between standard Tree LSTM and Tree LSTM is sensitive to sentence length, with results below those of the sole LSTM. Different models are used to embed sentences in training setups, with nearest neighbors determined using cosine similarity. Examples from the SICK test set are presented in Table 3. The Contrastive Tree LSTM (CTL) and Contrastive LSTM (CL) models are compared using cosine similarity in Table 3. Both models can be misled by surface information, but the Tree LSTM excels at identifying passive form and number properties. It also captures concepts like counting faculties. Tree LSTM captures number properties and identifies concepts like grouping 4 people. The method shows promising results without requiring hand annotated data. While results are not state of the art, training on a small proportion of bookcorpus sentences suggests potential for improvement with more data and extended training time. Other linguistic structures like N-ary Tree LSTM or Tree LSTM with attention mechanism could be explored. A batching procedure was implemented to speed up Tree LSTM computations. The Tree LSTM cell implementation involves sequential computation of nodes based on their distance to the root node. A batching procedure optimizes graph computation by decomposing it into steps where nodes with the same depth are computed together."
}