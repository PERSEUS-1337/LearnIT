{
    "title": "SkgJOAEtvr",
    "content": "This work proposes methods to encourage internal consistency in dialog agents in emergent communication settings. Two hypotheses are considered: 1) improving agents' ability to refer to unseen referents, and 2) enhancing agents' ability to generalize across communicative roles. Results show significant support for the latter hypothesis in the study of how linguistic protocols evolve in cooperative tasks. Dialog agents are often trained to only speak or listen, leading to a lack of symmetry in communication. This study aims to test if agents that incorporate symmetry with their partners learn more generalizable representations. Agents are modified to follow the \"golden rule\" of speaking/listening as they would want to be spoken/listened to. Modifications include self-play training objectives, shared embedding spaces, and symmetric decoding and encoding mechanisms. Two hypotheses are tested regarding the effect of these modifications on emergent communication. The study tested two model architectures, an RNN and a Transformer, with two game datasets. It found no evidence supporting internal-consistency improving generalization but significant evidence that constraints enable models to generalize representations across roles. Code and data are available at bit.ly/internal-consistency-emergent-communication. The study tested two model architectures, an RNN and a Transformer, with two game datasets. It found no evidence supporting internal-consistency improving generalization but significant evidence that constraints enable models to generalize representations across roles. The message is produced using symbols from a vocabulary V with length L, represented as 1-hot vectors. In each round of the reference game, a context C is constructed with a vector representing the referent and its index in C. Distractors are sampled randomly each round. Three modifications are introduced to encourage internally consistent representations: a self-play training objective, a shared embedding space, and a symmetric decoding and encoding mechanism with shared parameters. Agents contain four modules including an embedding module. Agents in the study have four modules: embedding, decoder, encoder, and pointing. The decoder module produces discrete messages, while the encoder module generates representations of referents. A non-parametric pointing module creates a distribution over the context. Messages are emitted one symbol at a time using Gumbel Softmax, allowing for optimization without reinforcement methods. The study involves agents with modules for embedding, decoder, encoder, and pointing. The Recurrent model uses LSTM for speaking and listening, while the Transformer model uses Transformer Decoder and Encoder. Parameters are optimized to select referents by minimizing negative log likelihood. Speaker and listener modules instantiate functions for speaking and listening. The study introduces three internal-consistency constraints for baseline agents, focusing on encouraging internally-consistent representations. These constraints include a self-play loss, shared embedding agents, and symmetric encoding and decoding agents. The self-play loss encourages agents to speak/listen to others the same way they do to themselves, while shared embedding agents use the same item and message embeddings for listening and speaking. Symmetric encoding and decoding agents use the same parameters for decoding a message when speaking as they do for encoding it. The evaluation is based on a reference game played across two datasets targeting different aspects of lexical reference. The first dataset, CONCEPTS, is derived from annotated images of items like animals and tools, containing realistic co-occurance patterns. The data suggests that a good lexicon will develop words referring to frequently co-occurring attributes. The second dataset, SHAPES, consists of abstract shapes described by independent attributes. Experimental results test hypotheses using two agents, \"Alice\" and \"Bob,\" to generalize to unseen combinations of features. In the SHAPES dataset, two agents, \"Alice\" and \"Bob,\" are trained and tested within the same communicative role. They are evaluated based on their ability to describe new toys and successfully fetch them. Validation and test accuracies are computed, showing results before any internal-consistency constraints. Adding the self-play objective in the fixed-role setting has mixed effects on different architectures and datasets. While it benefits shared embeddings models, it only adds noise to baseline and symmetric models. The advantage of internal-consistency constraints in the vanilla emergent communication setting is not conclusively established. The internal-consistency constraints in the fixed-role setting do not clearly help agents generalize and may even hinder performance. Performance on referring to/fetching unseen items is compared between models with and without these constraints. Results show no clear advantage to enforcing internal-consistency via self-play. The study examines if internal-consistency improves agents' ability to generalize linguistic knowledge across roles. In a scenario where Alice asks for a \"truck\" but receives a doll instead, the study explores if agents can generalize linguistic knowledge across roles without clear advantage to enforcing internal-consistency via self-play. In a self-play experiment, Alice and Bob switch roles from speaker to listener for testing. They receive direct supervision in both roles but may not have equal training on every vocabulary item. For example, Alice speaks about food while Bob speaks about toys in certain contexts. Alice and Bob switch roles from speaker to listener in a self-play experiment. They are trained on disjoint sets of values using the SHAPES dataset. Results show that adding a self-play objective improves performance significantly. Adding the self-play objective improves performance significantly, especially in settings with limited interaction. Shared-embedding spaces further enhance performance, while symmetric constraints may hinder it. When agents can be trained directly in the role they are tested in, there is no clear evidence that adding internal-consistency constraints improves their ability to generalize to new items. Internal-consistency constraints, such as self-play training objectives and shared embedding spaces, enhance agents' ability to generalize learned representations across roles. This is particularly beneficial when agents have limited training in a specific role. The study investigates the impact of self-play on training efficiency and protocol emergence using a smaller SHAPES dataset with reduced vocabulary and message length. The performance of agents, such as Alice, as a listener is tested under varying levels of data access, showing how self-play can replace direct supervision between agents. The study explores the impact of self-play on training efficiency and protocol emergence using a smaller SHAPES dataset. Results show that self-play models perform well without direct supervision, transferring protocols across roles effectively. Additional training in the primary role is deemed unnecessary, and training on disjoint features is helpful. Positive signaling is used to measure agents' speaking and listening capacities separately. Self-play improves agents' communication accuracy and metrics like positive signaling and listening. Model architectures and self-play impact agents' lexicons, with recurrent models using fewer unique messages than transformer models. Self-play helps recurrent models use more vocabulary and leads to sparser mappings from symbols to features. Work in emergent communication analyzes agents developing shared protocols through reference games. Several approaches have been proposed to encourage models to learn more generalizable representations of language, including compression, efficiency, memory constraints, pragmatic constraints, and positive biases. Some work assumes access to symbolic representations of referents and their attributes, while others are set in pixel-based multi-agent games or multi-agent grid-worlds. This work also relates to a broader body of work on speaker-listener interactions. Our work on speaker-listener models is related to previous research on pragmatically-informed models where speakers reason recursively about listeners' beliefs. These models have been applied in various fields such as image captioning, robotics, linguistics, and psychology to explain complex linguistic inferences. Our proposed internal-consistency constraints align with neural speaker-listener models developed in emergent communication, although past work often assumes a speaker's mental model of their listener to be inconsistent. The proposed model architecture lacks recursion typical in other pragmatics models and may not handle higher-level inferences. Three methods are suggested to encourage dialog agents to follow \"the golden rule.\" Internal-consistency constraints do not consistently improve generalization, but self-play objective and shared embeddings enhance performance in untrained roles. The agents are tested on roles they were not trained for, but internal-consistency constraints allow them to perform well. The deep learning framework Pytorch is used to implement models. Random seeds are set for reproducibility. The general architecture of the agents is shown in Figure 4. The recurrent model decodes and encodes messages by projecting hidden states and sampling words from the resulting distribution. The recurrent model decodes and encodes messages by projecting hidden states and sampling words from the resulting distribution. The LSTM cell used for encoding and decoding is the same in the symmetric variant of the model. Different models vary in the implementation of the Decoder and Encoder modules, with parameters shared in shared embedding models and encoder and decoder shared in symmetric models. The transformer model is also utilized in the architecture. The transformer model decodes and encodes messages by generating a contextualized representation of the input matrix and sampling words to form a message. The process involves concatenating word embeddings and producing words until the maximum message length is reached. The transformer model uses embeddings of words and an ITEM symbol to create a contextualized message encoding. In the Symmetric Transformer Model, parameters are shared between Transformer Encoders and Decoders by using the Encoder for decoding messages. The Symmetric Transformer uses a Transformer Encoder for encoding messages when listening and a Transformer Encoder for speaking. It concatenates embeddings of words, referent, and the next symbol to produce the next symbol. Hyperparameters were uniformly sampled for each model architecture, experiment, and dataset split. The study explored different learning rate schedulers and hyperparameter selections for the TRE method, which requires structured feature representations. The hyperparameters for TRE include an error function and a composition function, with choices aligned with the original paper. The method assumes right-branching features in each item. The composition function is learned with 1000 update steps. The SHAPES dataset is simplified to compute positive listening and signaling scores. Recurrent and transformer models are set with fixed parameters. Results in Sec. 6 are averaged over 5 random seeds. In this study, 5 random seeds were used for both trained and tested data. Positive listening and positive speaking metrics were defined based on previous studies. The metrics were computed without sampling messages due to the tractable number of messages. The game setting involved a single step, with the average policy converging to a uniform distribution over actions. The positive speaking metric is computed over a sampling of the dataset, with empirical averages reported over all items and random seeds. Lexicons across random seeds are shown in figures, with additional results in the vanilla setting for SHAPES-small in Table 8."
}