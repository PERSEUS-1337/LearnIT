{
    "title": "B1Yy1BxCZ",
    "content": "Increasing batch size during training can achieve equivalent learning curves on both training and test sets for various optimization algorithms like stochastic gradient descent (SGD), SGD with momentum, Nesterov momentum, and Adam. This approach leads to equivalent test accuracies with fewer parameter updates, allowing for greater parallelism and shorter training times. By adjusting the learning rate and batch size proportionally, as well as increasing the momentum coefficient, large batch training can be achieved without the need for hyper-parameter tuning. For example, training ResNet-50 on ImageNet can reach 76.1% validation accuracy in under 30 minutes using these techniques. Large batch training has gained interest in deep learning as it aims to increase the step size and reduce the number of parameter updates needed to train a model. However, increasing batch size can lead to a decrease in test set accuracy. Researchers have interpreted stochastic gradient descent (SGD) as integrating a stochastic differential equation, showing that the scale of random fluctuations in the SGD dynamics is influenced by the learning rate, training set size, and batch size. The study explores the impact of learning rate, training set size, and batch size on stochastic gradient descent dynamics. It suggests increasing batch size during training as an alternative to decaying the learning rate, achieving similar model performance with fewer parameter updates. Increasing batch size during training can be an alternative to decaying the learning rate, leading to similar model performance with fewer parameter updates. In experiments, InceptionResNet-V2 on ImageNet achieved 77% validation accuracy in under 2500 parameter updates with batches of 65536 images. Additionally, training ResNet-50 on ImageNet reached 76.1% accuracy in under 30 minutes on a TPU setup. The study demonstrates that decaying learning rate schedules can be converted into increasing batch size schedules, leading to efficient large batch training. Sections discuss convergence criteria for SGD, interpreting decaying learning rates as simulated annealing, and challenges of training with large momentum coefficients. Experimental evidence shows benefits of increasing batch size instead of decaying learning rates in deep learning. This approach, along with other techniques, enables efficient large batch training on CIFAR-10 and ImageNet datasets. The text discusses the impact of decaying learning rates on optimization during training. It highlights the importance of adjusting the learning rate to reach the minimum of a function and the role of batch size in this process. The study also explores how stochastic differential equations can be used to interpret stochastic gradient descent. The text discusses the impact of decaying learning rates on optimization during training. It explains how Gaussian random noise, controlled by the noise scale, affects the training dynamics. The noise scale is influenced by the learning rate, training set size, and batch size. Decaying the learning rate reduces the noise scale, aiding convergence to the minimum of the cost function. The main contribution of this work is to efficiently use vast training batches by increasing batch size during training until B \u223c N/10, then reverting to decaying learning rates. Small batch training often generalizes better than large batch training, with an optimal batch size B opt maximizing test set accuracy at constant learning rate. This optimal batch size arises when noise scale g \u2248 N/B is optimal, supported by empirical evidence. The study by BID8 demonstrated the benefits of exploiting a linear scaling rule between batch size and learning rate to train ResNet-50 on ImageNet in one hour with batches of 8192 images. The results suggest that gradient noise can be advantageous in non-convex optimization, aiding in escaping \"sharp minima\" that generalize poorly. The relevance of equations 1 and 2 in deep learning is questioned, as most researchers use early stopping to prevent the network from reaching a minimum. Despite this, decaying learning rates have shown empirical success, with the introduction of random fluctuations during training resembling simulated annealing in non-convex optimization. This initial noisy optimization phase allows for exploration of a larger parameter space without getting trapped. The noisy optimization phase allows for exploration of a larger parameter space without getting trapped in local minima. Researchers now prefer sharper decay schedules like cosine decay or step-function drops in deep learning, similar to annealing temperature in physical sciences. This shift may explain the decreased popularity of conventional learning rate decay schedules. In deep learning, researchers are moving away from vanilla SGD to using SGD with momentum. Smith & Le (2017) studied the effects of momentum on SGD, finding that the noise scale reduces to that of vanilla SGD when momentum approaches 0. Increasing the learning rate and momentum coefficient while scaling B accordingly can reduce parameter updates needed for training. However, increasing momentum coefficient while scaling B differently slightly decreases test accuracy. The momentum update equations involve accumulation and mean gradient per training example. The accumulation of gradient estimates grows exponentially towards its steady state value over a timescale of approximately B/(N (1 \u2212 m)) training epochs. Increasing momentum coefficient prolongs the timescale needed for the accumulation to forget old gradients, hindering training as it cannot adapt to changes in the loss landscape. In response to the hindrance in training caused by the accumulation of gradient estimates not adapting to changes in the loss landscape, various strategies have been proposed. BID13 suggested initialization bias correction, but it can lead to instabilities with large batch sizes. BID8 recommended a reduced learning rate initially. Sections 5.1 to 5.4 demonstrate the equivalence of decreasing learning rate and increasing batch size, reducing parameter updates, and applying these insights to train models like Inception-ResNet-V2 and ResNet-50 on ImageNet. In section 5.4, ResNet-50 is trained to 76.1% ImageNet validation accuracy within 30 minutes. Experiments on CIFAR-10 use a \"16-4\" wide ResNet architecture with ghost batch norm. Three different training schedules are considered to show the equivalence between decreasing learning rate and increasing batch size. The batch size is constant and the learning rate decays by a factor of 5 at each subsequent step. This schedule mimics hardware limitations on batch size. In \"Increasing batch size\", the learning rate is held constant while the batch size increases by a factor of 5 at every step. Different learning curves and final test set accuracies are expected if the learning rate decays during training. Training curves using SGD with momentum and a momentum parameter of 0.9 are almost identical, suggesting that it is the noise scale that should decay. Increasing the batch size can reduce the number of parameter updates required to train the model, showing similar results to decaying the learning rate. Test set accuracy remains consistent across different learning schedules. In experiments testing different optimizers, increasing batch size showed similar results to decaying the learning rate. Test set accuracy remained consistent across various learning schedules and optimizer configurations. The study focused on increasing batch sizes and adjusting learning rates to minimize parameter updates during model training. Experiments were conducted using SGD with momentum on the CIFAR-10 dataset with a maximum batch size of 5120. Different training schedules were considered, all reducing noise scale by a factor of five. The study experimented with different training schedules to reduce noise scale by a factor of five. Four schedules were considered, including the original training schedule with a learning rate of 0.1 and batch size of 128, increasing batch size, decaying learning rate, increased initial learning rate, and increased momentum coefficient. The study experimented with different training schedules to reduce noise scale by a factor of five. The initial learning rate was 0.5, with an increased momentum coefficient of 0.98, and an initial batch size of 3200. The evolution of test set accuracy was plotted, showing that increasing batch size, increased initial learning rate, and increased momentum coefficient all led to improved accuracy with fewer parameter updates. The study experimented with different training schedules to reduce noise scale by a factor of five. The momentum coefficient requires less than 2500 parameter updates but reaches a lower test accuracy of 93.3%. Varying the initial learning rate between 0.1 and 3.2 while holding the batch size constant, the test accuracy falls for initial learning rates larger than \u223c0.4. Increasing the batch size during training achieves similar results to decaying the learning rate, reducing the number of parameter updates from just over 14000 to below 6000. The study experimented with different training schedules to reduce noise scale by a factor of five. BID8 trained a ResNet-50 on ImageNet in one hour, reaching 76.3% validation accuracy using batches of 8192. They completed 90 training epochs, decaying the learning rate at specific intervals. A warm-up phase was introduced at the start of training. In a bid to set a stronger baseline, ResNet-50 was replaced by Inception-ResNet-V2 BID24 with a ghost batch size of 32. In figure 6, training was done with a batch size of 32, a learning rate of 3.0, and a momentum coefficient of 0.9. The study compared \"Decaying learning rate\" and \"Increasing batch size\" schedules, finding similar test set accuracy evolution. Final accuracies ranged from 76.8% to 78.7%. Increasing batch size reduced parameter updates during training. The training curves appear noisy due to reduced test set evaluations for faster model training. B max = 65536 and increased momentum coefficient improved stability. Three schedules with different initial batch sizes and decay rates were compared, all starting with an initial learning rate of 3. The study compared different momentum coefficients (0.9, 0.95, 0.975) in training Inception-ResNet-V2 on ImageNet. Increasing momentum reduces parameter updates but slightly lowers final accuracy. Training ResNet-50 on a half TPU pod with increased batch size reduced training time to achieve 76.1% validation accuracy in under 45 minutes. Training ResNet-50 on a full TPU pod with increased batch size achieves 76.1% validation accuracy in under 30 minutes, demonstrating efficient scaling across the pod. Increasing batch size during training is crucial for performance gains. The paper discusses the importance of training and performance gains with large-batch training. It extends the analysis of SGD in BID23 to include decaying learning rates and interprets SGD as a stochastic differential equation for approximate Bayesian posterior sampling. BID12 showed that noise has beneficial effects at the start of training, while BID15 proposed using control theory to set the learning rate and momentum coefficient. BID8 observed a linear scaling rule between batch size and learning rate, reducing training time for ResNet-50 on ImageNet to one hour. To our knowledge, the scaling rule was first adopted by BID14. BID2 demonstrated that SGD converges to strongly convex minima in similar numbers of training epochs if batch size is proportional to learning rate. BID10 proposed an alternative scaling rule, where batch size is proportional to the square root of the learning rate. BID27 introduced Layer-wise Adaptive Rate Scaling (LARS) for training ImageNet in 14 minutes, achieving a final accuracy of 74.9%. K-FAC is gaining popularity as an efficient alternative to SGD. BID26 argued that adaptive optimization methods generalize less well than SGD, but our work reduces the gap in convergence speed. Asynchronous-SGD enables the use of multiple GPUs even with small batch sizes, but it is not considered in this work due to the effectiveness of the scaling rules. In this work, the authors explore the benefits of increasing batch sizes during training instead of decaying the learning rate. They conduct experiments on CIFAR-10 and ImageNet with various optimizers like SGD, Momentum, and Adam. By scaling batch sizes and adjusting learning rate and momentum parameters, they achieve high validation accuracies with fewer parameter updates. For example, they train Inception-ResNet-V2 on ImageNet to 77% accuracy in under 2500 updates using batches of 65536 images. This approach has the potential to significantly reduce model training times. The authors achieve 76.1% ImageNet validation set accuracy on TPU in under 30 minutes without hyper-parameter tuning by scaling batch sizes and converting existing hyper-parameter choices. The accumulation variable grows exponentially during training, suppressing the size of parameter updates. The accumulation variable grows exponentially during training, estimating the effective number of \"lost\" training epochs. Additional training epochs may be needed to compensate for this loss, especially when increasing the momentum coefficient. Test accuracy of the \"16-4\" wide ResNet on CIFAR10 is shown in FIG7, varying with the initial learning rate. The batch size remains constant at 128 for a learning rate of 0.1, following the original training schedule. Increasing the learning rate scales the batch size accordingly."
}