{
    "title": "SyxKrySYPr",
    "content": "Self-attention architectures have shown success in NLP, but have not been effectively applied to reinforcement learning. This work proposes modifications to improve the stability and learning speed of the original Transformer and XL variant. The proposed architecture, Gated Transformer-XL (GTrXL), outperforms LSTMs on memory environments and achieves state-of-the-art results on the multi-task DMLab-30 benchmark suite. GTrXL offers a more expressive alternative to standard LSTMs for RL agents in partially-observable environments, showing stability and performance matching or exceeding competitive LSTM baselines. It is argued that self-attention architectures handle longer temporal horizons better than RNNs. Recent work has shown that self-attention architectures like the Transformer outperform traditional recurrent architectures like LSTM in various domains such as language modeling, machine translation, summarization, question answering, and algorithmic tasks. Transformer architecture has been successful in handling longer temporal horizons compared to RNNs. The transformer architecture has been successful in domains requiring sequential information processing, making it suitable for partially observable RL problems with long episodes. Despite the dominance of LSTMs in RL literature, other memory architectures have shown better performance but have not been widely adopted due to complexity. The transformer, proven in challenging domains, offers a promising alternative for memory-based tasks and partially observable environments. In this work, the transformer architecture is investigated in the RL setting. The canonical transformer is found to be difficult to optimize, requiring complex learning rate schedules and specialized weight initialization schemes. These measures, proven effective in supervised learning, do not seem to be sufficient for RL tasks. In this work, a novel Gated Transformer-XL (GTrXL) architecture is introduced to improve training stability and performance in processing sequential information. The GTrXL outperforms the canonical transformer and achieves state-of-the-art results on multitask DMLab-30 suite, surpassing LSTMs on memory-based tasks. The GTrXL architecture surpasses LSTMs on memory-based DMLab-30 levels and outperforms them on memory-based continuous control and navigation environments. Extensive ablations demonstrate superior performance and stability compared to LSTMs, suggesting GTrXL as a potential replacement for LSTM networks in RL. The transformer network applies self-attention to input sequences in stacked blocks, showing consistent performance improvements. The transformer network, introduced in 2017, consists of stacked blocks with each layer containing an attention operation and a position-wise multi-layer network. The input to the transformer block is an embedding from the previous layer. The Multi-Head Attention submodule computes soft attention operations in parallel for every time step, followed by a residual connection and layer normalization. The Multi-Layer Perceptron submodule applies a temporal convolutional network over every step in the sequence, producing a new embedding tensor. Positional encodings are used to account for sequence order in the MHA operation. TrXL-I moves layer normalization to the input stream of the submodules. The GTrXL block adds a gating layer in place of the residual connection of the TrXL-I, enabling a larger contextual horizon with relative position encodings and a memory scheme. The MHA submodule includes a T-step memory tensor treated as constant during weight updates, with a StopGrad function preventing backward gradients. The transformer architecture has shown success in supervised learning tasks, but its application as a useful RL memory has been lacking. Previous work has highlighted training difficulties and poor performance. By introducing powerful gating mechanisms in place of residual connections within the transformer block, coupled with changes to the architecture, improvements can be made in stabilizing learning across various architectures. The transformer architecture has been improved by introducing gating mechanisms instead of residual connections, along with changes to layer normalization in submodules. This \"Identity Map Reordering\" enhances learning stability and performance, enabling an identity map from input to output in the transformer. The TrXL-I introduces normalization operations that non-linearly transform the state encoding, improving stability and performance. The Identity Map Reordering allows the agent to learn a Markovian policy at the start of training, enabling reactive behaviors to be learned before memory-based ones. The GTrXL architecture improves performance by replacing residual connections with gating layers. The final GTrXL layer block includes a gating layer function. Various gating layers with increasing expressivity are tested in experiments. The GTrXL architecture enhances performance by incorporating gating layers instead of residual connections. These gating layers, such as sigmoid modulation, Highway connection, Sigmoid-Tanh gate, and Gated Recurrent Unit-type gating, play a crucial role in modulating the output stream. The Identity Map Reordering is utilized to aid policy optimization by initializing the gating mechanisms close to the identity map, improving stability in the process. The GTrXL architecture incorporates bias b g in gating layers to enhance performance in various RL domains. Experiments show significant improvements over LSTMs on memory-based environments without performance degradation on reactive tasks. GTrXL outperforms MERLIN on both memory and reactive tasks, using deep 12-layer networks with specific parameters. The study demonstrates the effectiveness of deep networks in training transformers for RL tasks, with a focus on maintaining complexity without sacrificing stability. The networks have large receptive fields, with potential for further scaling towards a 52-layer network. Experimental details are provided in Appendix B, with V-MPO used for all experiments as an on-policy adaptation of MPO. V-MPO utilizes a learned state-value function V(s) instead of the state-action value function in MPO. It constructs a target distribution for policy updates based on estimated advantages, achieving state-of-the-art results for LSTM-based agents on the multi-task DMLab-30 benchmark suite. The study focuses on deep networks training transformers for RL tasks, maintaining complexity while ensuring stability. The GTrXL (GRU) outperforms LSTM and MERLIN in a metric proportional to superhuman performance. GTrXL (Output) and reordered TrXL-I also surpass LSTM but are less robust. GTrXL shows better memory scaling properties than LSTM in Numpad environment. The GTrXL outperforms LSTM and MERLIN in performance, showing better memory scaling properties in the Numpad environment. Learning curves for GTrXL on 2x2 and 4x4 Numpad sizes demonstrate substantial improvement even when trained for twice as long. DMLab-30 is a benchmark for architectural and algorithmic improvements in various agent competencies. The GTrXL shows superior performance compared to LSTM and MERLIN in long horizon reasoning tasks, with significant improvement over a 3-layer LSTM baseline. The final results of MERLIN, trained with a different algorithm, are also included for comparison, demonstrating the effectiveness of GTrXL in memory scaling properties. The GTrXL model outperforms LSTM and MERLIN in long horizon reasoning tasks, with significant improvements in memory-based environments. The GRU gating mechanism shows the highest final performance and learning speed in memory environments compared to other gating mechanisms. The GTrXL model demonstrates superior performance compared to LSTM in tasks with longer temporal horizons, showing robustness in hyperparameter sensitivity analysis. The Numpad continuous control task highlights the scalability of GTrXL over LSTM in environments with increased temporal horizons. The agent interacts with number pads by colliding with them to activate a specific sequence without prior knowledge. Activating the correct number in the sequence rewards the agent, while incorrect activations reset the pads. The agent must develop a search strategy to determine the correct sequence, with higher rewards indicating better memorization. The study evaluates the performance of LSTM and GTrXL models in memorizing pad sequences. Results show that LSTM performs poorly on all pad sizes, while GTrXL performs much better due to its more expressive memory. Additionally, a comparison is made between a thinner GTrXL (GRU) model with fewer parameters and other gated variants and TrXL baselines. The study compares the performance of LSTM and GTrXL models in memorizing pad sequences. GTrXL outperforms LSTM due to its more expressive memory. A thinner GTrXL (GRU) model with fewer parameters matches the performance of the best performing counterpart, GTrXL (Output), which has over 10 million more parameters. The GRU-type-gated GTrXL achieves state-of-the-art results on DMLab-30, surpassing deep LSTM and external memory architecture. Extensive ablations on gating variants show improvements in GTrXL (GRU). The study compares the performance of LSTM and GTrXL models in memorizing pad sequences. GTrXL outperforms LSTM due to its more expressive memory. GTrXL (GRU) shows improvements in learning speed, final performance, and optimization stability over other models, even when controlling for the number of parameters. Gating variants in DMLab-30 are evaluated, with GTrXL (Output) performing well, while GTrXL (Input) performs poorly. GTrXL (Highway) and GTrXL (SigTanh) are sensitive to hyperparameter settings. The study demonstrates improved performance and reduced sensitivity in hyperparameters and seeds for GTrXL (GRU) compared to baselines and other variants. The \"Memory Maze\" environment is used for testing, requiring long-range memory for effective navigation and apple collection. Effective mapping results in higher rewards through frequent returns to the apple. The study shows that GTrXL (GRU) outperforms other variants in the Memory Maze environment, requiring long-range memory for navigation and apple collection. Results indicate that GTrXL (GRU) learns faster with fewer environment steps than LSTM, offering improved stability and reduced hyperparameter tuning needs. The GTrXL (GRU) model outperforms others in the Memory Maze environment, reaching human performance in 2 billion steps with 10 runs scoring 8 and above. Comparisons with transformer variants show advantages not solely due to increased parameter count, as halving attention heads reduces parameters even below the TrXL model. The Thin GTrXL (GRU) model surpasses other models in performance, including the GTrXL (Output), with fewer parameters. Gated identity initialization improves optimization stability and learning speed. Figure 7 shows the impact of gated identity initialization on training a 4-layer GTrXL (GRU). Preconditioning the transformer to be close to Markovian results in large learning speed gains. Gating, such as in LSTM and GRU networks, addresses the vanishing gradient problem and improves learnability. Research continues to explore optimal gating mechanisms, including through neural architecture search. Gating and multiplicative interactions have a long history in neural networks. Gating and multiplicative interactions have a long history in neural networks, with research focusing on improving memory in reinforcement learning agents through external memory approaches. This involves a feedforward or recurrent policy interacting with a memory database through read and write operations, inducing priors through specific design choices. In reinforcement learning, various approaches have been explored to improve memory in agents, such as using environment maps, replay buffers, stacked hierarchical connections, and self-attention mechanisms. However, standard transformer models have been found to be unstable and often fail to learn effectively in this setting. The GTrXL is a new variant of the transformer model that outperforms the standard architecture in terms of stability, optimization, and robustness. It features reordered layer normalization modules and a gating layer, with GRU-type gating showing the best performance. The GTrXL (GRU) learns faster, more stably, and achieves higher performance compared to the traditional transformer model. The GTrXL (GRU) outperforms other gating variants on the DMLab-30 benchmark suite, showing faster learning and higher final performance. It demonstrates significant improvement over LSTM architectures in tasks like Numpad and Memory Maze, making a case for wider adoption of transformers in RL. The transformer architecture's scalability to large models and datasets is a core benefit, with future work aiming to test the GTrXL's scalability in diverse training environments. Numpad involves three actions for moving a sphere and jumping over pads for faster traversal. The observation in the DMLab environment includes proprioceptive information, activated pads, previous actions, and rewards. Actions in the native DMLab space consist of 5 integers, with a reduced action set used for training. Observations are 72x96 RGB images, and some levels require language input for all models. The DMLab environment includes proprioceptive information, activated pads, previous actions, and rewards. Observations are 72 \u00d7 96 RGB images. Models use a 64-dimension LSTM for language input. The GTrXL architecture can achieve human-level performance on the Memory Maze task. The action space for Memory Maze includes 8 continuous actions and a single discrete action. A hybrid continuous-discrete distribution is used to output policies. The DMLab environment includes proprioceptive information, activated pads, previous actions, and rewards. Observations are 72 \u00d7 96 RGB images. Models use a 64-dimension LSTM for language input. The GTrXL architecture can achieve human-level performance on the Memory Maze task. The action space for Memory Maze includes 8 continuous actions and a single discrete action. A hybrid continuous-discrete distribution is used to output policies. For DMLab Arbitrary Visuomotor Mapping (AVM), the action set is the same as in Wayne et al. (2018) with an additional no-op, which may be replaced with the Fire action. The image encoder used for DMLab-30 and Memory Maze is the same as in (Anonymous Authors, 2019) for multitask DMLab-30. The ResNet was adapted from Hessel et al. (2018) with specific layer blocks and hyperparameters sampled from a distribution. The models in the DMLab environment use a 64-dimension LSTM for language input and have residual skip connections in depth. Hyperparameter settings were dropped randomly during training, with at least 6 seeds finishing for every model tested. The experiments were carried out in an actor-learner framework using TF-Replicator for distributed training on TPUs. The text chunk discusses the use of MultiHeadAttention in a model, utilizing positional encodings to account for sequence order. It also mentions the use of relative position encodings and a memory scheme to expand the contextual horizon. Additionally, it refers to a partition of DMLab-30 levels into memory-based and reactive splits. The text chunk presents the final human-normalized median return for LSTM and GTrXL models across all 57 Atari levels. Both models are 256 dimensions wide, with results showing no performance regression on the multitask Atari-57 benchmark. The GTrXL can be used as an architectural replacement for the LSTM model. The GTrXL, a 12-layer architectural replacement for LSTM, matches in width at 256 dimensions. Training involved 11.4 billion environment steps, with 8 hyperparameter settings per model."
}