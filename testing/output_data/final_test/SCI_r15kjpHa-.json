{
    "title": "r15kjpHa-",
    "content": "In cooperative multi-agent reinforcement learning (MARL), designing a suitable reward signal is crucial for accelerating learning and stabilizing convergence. The global reward signal assigns the same reward to all agents, while the local reward signal provides different rewards based on individual behavior. Both approaches have drawbacks: the global signal may lead to lazy agents, and the local signal may result in selfish behavior. In this paper, the reward design problem in cooperative MARL based on packet routing environments is explored. The study shows that certain reward signals can lead to suboptimal policies. Mixed reward signals are proposed to improve policy learning, with adaptive counterparts yielding the best results in experiments. The importance of rethinking reward signals in MARL systems is emphasized, as the goal of RL agents is to maximize rewards from the environment. The Reward Hypothesis BID31 states that the goal of a RL agent is to maximize the expected value of cumulative rewards. It is crucial for rewards to accurately reflect the desired goal, rather than how to achieve it. Faulty reward functions can lead to suboptimal policies, as seen in examples like AlphaGo and cleaning up dirt. Human experience influences reward design, which can easily lead to deviations. The Reward Hypothesis in reinforcement learning emphasizes maximizing cumulative rewards. However, using the exact performance objective as a training objective can lead to slow and unstable learning. Intrinsically Motivated Reinforcement Learning combines domain-specific intrinsic rewards with environmental rewards to enhance learning in sparse-reward domains. Multi-agent reinforcement learning complicates reward design, as global and local rewards can have drawbacks. In this paper, the focus is on exploring the use of intrinsic rewards in Multi-Agent Reinforcement Learning (MARL) to accelerate learning and stabilize convergence. The authors propose new MARL environments based on the Packet Routing Domain, where agents aim to minimize the maximum link utilization ratio in the network. The meta reward signals set as 1 - max(U l) serve as a form of intrinsic reward to guide agents towards this goal. The authors propose new MARL environments with intrinsic rewards to minimize the maximum link utilization ratio. They show that global and local reward signals can lead to suboptimal policies, and introduce mixed reward signals for better learning. Adaptive mixed rewards outperform others in experiments, with discussions on various reward signals. The authors propose new MARL environments with intrinsic rewards to minimize link utilization ratio. They evaluate different reward signals and emphasize the importance of careful reward design to guide agent behavior. The paper is organized into sections introducing background, proposed environments and rewards, experiments, and discussions. The study highlights the significance of rewards in guiding agent behavior in MARL environments. The text discusses reward design in reinforcement learning, emphasizing the importance of carefully designing rewards to guide agent behavior. For single objective RL, the focus is on whether the reward is a what reward rather than how reward. In multiple objectives RL, sub-rewards are designed for different objectives, with the final reward being a weighted sum. Manual adjustment of weights is required even in recent studies. IMRL is a relevant field, with VIME using a surprise reward to balance exploitation and exploration for better performance. The text discusses reward design in reinforcement learning, emphasizing the importance of carefully designing rewards to guide agent behavior. Intrinsic reward is used to balance exploitation and exploration, with successful models like Categorical DQN considering long-term rewards. BID18 uses temporal logic quantitative semantics for reward translation and proposes a temporal logic policy search method. Limited reward design studies exist for MARL, with Independent DQN being a notable example demonstrating competitive and collaborative behaviors through different rewarding schemes. In the context of reinforcement learning, the text explores credit assignment in multi-agent systems like Pong. Credit assignment involves determining how rewards should be distributed among individual agents within a team. Approaches in multi-agent reinforcement learning can be categorized into traditional global/local reward methods and other reward strategies. Global reward assignment treats all agents equally, potentially leading to lazy agents receiving more rewards than they contribute. The text discusses how agents in multi-agent systems often receive rewards disproportionate to their contributions, leading to lazy agents not optimizing their policies. Diligent agents may receive lower rewards due to lazy agents making the system worse. The local reward approach assigns rewards based on individual behavior to discourage laziness but may lead to selfish behaviors. After considering various options, the Packet Routing Domain was chosen as the experimental environment for its classical MARL nature. In MARL environments, researchers study packet routing problems with goals of high throughput and low link utilization ratio. Real-world applications like internet packet routing can benefit from similar methods. The meta reward signals are set as 1 -max(U l ) to incentivize good packet routing policies. The environments in MARL focus on minimizing the maximum utility function U l. The network consists of ISP networks with edge routers forming IE-pairs, each with flow demand and available paths. Links have transmission capacities for flow delivery. In MARL, the focus is on minimizing the maximum link utilization ratio in ISP networks. Links have flow transmission capacities and utilization ratios. Flow splitting policies are sought to minimize link utilization, with defined sets for router link ratios. The curr_chunk discusses the definition of reward signals in MARL for minimizing link utilization ratios in ISP networks. It introduces global reward, direct local reward, basin local reward, direct mixed reward, basin mixed reward, and direct adaptive reward. These rewards are calculated based on different sets of link utilization ratios of routers. The reward in MARL for minimizing link utilization ratios in ISP networks is a combination of global reward (gR) and different types of local rewards (dlR and blR). These rewards are calculated based on the link utilization ratios of routers and are used to accelerate learning and stabilize convergence of MARL systems. The rewards in MARL aim to accelerate learning and stabilize convergence of systems by minimizing link utilization ratios in ISP networks. The rewards can be calculated at a low cost using subsets of the ALL set. Experimental environments are designed to be consistent with real-world systems, with routers being partially observable and time delays in router, link, and reward signals being considered. The ACCNet BID20 method is used to address these challenges. More details can be found in the Appendix Section A.1 and A.2. The rewards in MARL aim to accelerate learning and stabilize convergence of systems by minimizing link utilization ratios in ISP networks. Experimental environments are designed to be consistent with real-world systems, with routers being partially observable and time delays in router, link, and reward signals being considered. The proposed rewards are tested using synthetic and real flow trajectories from the American Abilene Network 3. Different reward signals are compared, showing that dlR and blR outperform gR in packet routing environments. The proposed blR shows similar effectiveness to dlR, indicating potential for mixed reward signals. The proposed blR is comparable to dlR in capacity but differs in performance with mixed and adaptive reward signals. blR balances factorization and learnability, making it suitable for asymmetrical topologies, while dlR is better for symmetrical topologies. Adaptive rewards are preferred over mixed rewards, with both being superior to meta rewards. The proposed blR is essential for packet routing environments. The proposed reward signals (gR, dlR, blR) show varying effectiveness based on network topology complexity. Global and local rewards may lead to suboptimal policies, while mixed rewards are effective off-the-shelf. Adaptive rewards require adjustments but can achieve the best results. The proposed rewards excel in more complex topologies, with reductions in suboptimality exceeding 10%. The proposed reward signals (gR, dlR, blR) show varying effectiveness based on network topology complexity. gR and dlR were tested, with dlR slightly outperforming gR. However, dlR struggles to capture certain information, leading to suboptimal results in divergence experiments. The dlgMixedR outperforms gR and dlR alone in capturing both nearest and far away link information. This leads to the proposal of blR and subsequently blgMixedR to simplify dlgMixedR while maintaining its ability. Additionally, adaptive rewards are explored to gradually decay the weight of dlR towards gR for better performance in environments. The blR and blgMixedR are proposed to incorporate both nearest and far away link information adaptively, achieving similar convergence rates as dlgMixedR. blgMixedR can boost convergence rates up to 80%. However, implementing adaptive rewards like dlgAdaptR and blgAdaptR reveals challenges related to replay buffer size and weight decay rate inconsistency. The study suggests using off-the-shelf mixed reward signals and tests min-max and average reward forms to improve learning efficiency. Link utilization ratios are analyzed using real flow trajectory data, showing that all links share responsibility for the flow effectively. The study focuses on designing special reward signals to improve learning efficiency in packet routing. The proposed rewards can be combined with other methods to achieve almost 100% convergence rate for all topologies. However, in environments where local or global rewards cannot be directly calculated, the proposed rewards may only be useful at a high computation cost. The paper discusses the use of special reward signals to enhance learning efficiency in packet routing. These rewards, although with limitations, can be applied to real-world applications like internet packet routing and traffic flow allocation. They are considered as auxiliary reward signals that directly guide learned policies, different from auxiliary tasks in other methods. The mixed rewards are compared to VIME, while adaptive rewards are similar to curriculum learning, progressively training agents from easy to difficult environments. In this paper, the authors address the reward design problem in cooperative MARL for packet routing environments. They find that global and local reward signals can lead to suboptimal policies, and propose mixed reward signals that improve learning. By adapting these mixed rewards, they achieve the best results in experiments. The study suggests a new approach to reward design in MARL and encourages researchers to reconsider their reward systems. Future work may involve using Evolutionary Algorithm to further optimize rewards. In future work, the authors plan to use Evolutionary Algorithm BID11 to optimize local reward weights and test the reward signals in different domains. Real flow trajectories from the American Abilene Network are analyzed. State representation includes flow demands, link utilization history, and agent actions. State dimensions vary based on network topology. The state dimensions of agents vary based on network topology. The ingress-router generates a splitting ratio with a sum-to-one constraint for current traffic demand. The actor network uses softmax activation for continuous action. Settings for ACCNet include buffer size of 6280, batch size of 64, and learning rates of 0.001 for actor and 0.01 for critic. A-CCNet achieves smaller fluctuation range of link utilization ratio in testing on Complex Topology using Abilene Network flow and dlgMixedR. A-CCNet outperforms AC-CNet in managing link utilization ratio, as shown in FIG6 and 9. All links exhibit similar utilization ratios and consistent trends, indicating effective sharing of flow demand based on capabilities."
}