{
    "title": "B1gX8kBtPr",
    "content": "Training neural networks to be certifiably robust is crucial for their safety against adversarial attacks. Despite the current difficulty in achieving both accuracy and robustness, this work aims to address this challenge by proving the existence of a network for every continuous function. Our result proves the existence of accurate, interval-certified networks, serving as a Universal Approximation Theorem for interval-certified ReLU networks. This is the first work to achieve this, contrasting with prior efforts focused on training networks to be robust against adversarial attacks. Recent research has focused on verifying network robustness using methods like mixed integer linear programming, SMT solvers, and linear relaxations. Specific training methods have been developed to produce certifiably robust networks. Recent research has focused on verifying network robustness using specific training methods that aim to discover weights facilitating verification. A statistical approach to certification has been proposed, creating a probabilistic classifier with guarantees. The Interval relaxation technique has achieved some of the best non-probabilistic results on popular datasets like MNIST and CIFAR10. In recent research, there has been progress in verifying network robustness using specific training methods. Despite advancements, there are still gaps in accuracy and robustness levels. The best reported certified robustness for CIFAR10 is 32.04% with an accuracy of 49.49%. The question arises whether certified training can ever succeed or if there is a fundamental limit. A result parallel to the Universal Approximation Theorem is proven in this paper, showing that a ReLU neural network can certifiably approximate any continuous function on a compact domain for any desired level of accuracy. Theorem 1.1 states that for any desired level of accuracy \u03b4, a ReLU neural network can approximate a continuous function on a compact domain using interval bound propagation. This result applies to more precise convex relaxations and recovers the classical universal approximation theorem. The theorem states that a ReLU neural network can approximate a continuous function on a compact domain using interval bound propagation. This allows for handling perturbations to the input and can be extended to functions component wise. The practical meaning is that if a network is trained on a dataset and meets certain criteria, there exists a network as accurate and certifiable with interval analysis. The focus is on the existence of such a network, without providing a method for training it. The study focuses on proving the existence of a certified ReLU network without providing a training method. Classical universal approximation theorems are deemed insufficient as they do not certify the robustness of the approximation, unlike the proposed method using interval bound propagation. The study demonstrates the existence of accurate, interval-certified networks using ReLU networks. Previous work focused on adversarial attacks and defenses, while this research introduces interval certification for network robustness. Recent research has focused on training certifiably robust neural networks to scale to larger networks. Randomized smoothing is a different approach that aims to perform probabilistic classification and certification with high confidence. However, there is ongoing investigation into the fundamental barriers in datasets and methods that hinder the learning of a robust network. In our work, we investigate whether neural networks can approximate functions with robustness using efficient interval relaxation. Early versions of the Universal Approximation Theorem by Cybenko (1989) and Hornik et al. (1989) show that networks can approximate continuous and Borel measurable functions. Our focus is on continuous functions, which is a broader class than Lipschitz continuous functions. In this section, we introduce the concepts needed to explain our main result on adversarial examples and robustness verification in neural networks. Adversarial examples are inputs that are classified differently from the original input by the network, even though they are imperceptible to humans. We focus on l\u221e adversarial balls and networks that are -robust around a given input x. In this paper, the focus is on l \u221e adversarial balls and robustness verification in neural networks. The goal is to show that for a neural network n, input point x, and label t, every possible input in an l \u221e ball around x is classified to t. Interval analysis is used to illustrate the loss in precision, providing an over-approximation for all functions. The proof of the main result, Theorem 4.6, involves deconstructing the function f. The construction of the proof involves deconstructing the function f into slices approximated by ReLU networks. The key insights include confining the output within intervals and using local bump functions to minimize loss of precision. The slicing of the function f and the networks n are depicted in figures. The N-slicing of a continuous function f on a closed box \u0393 involves constructing a ReLU network nmin to capture the behavior of min as a building block. By recursively using nmin, a ReLU network nmin N can be created to map N arguments to the smallest one. Precision bounds can be established for the interval-transformation, and local bumps can be constructed using a clipping function to confine values within a grid G. The grid G specifies local bumps for constructing networks \u03c6 c, increasing finesse improves approximation precision. A local bump \u03c6 c evaluates to 1 within the convex hull of c and decreases linearly to 0. \u03c6 c has ReLUs and layers. Lemma 4.4 shows how a ReLU network n k can approximate f k with limited loss of precision. For a closed box \u0393 in R^m and a continuous function f : \u0393 \u2192 R, there exists a set of ReLU networks approximating f within a given error \u03b4. The proof involves constructing hyperrectangles on a grid to ensure the approximation holds. The proof involves constructing hyperrectangles on a grid to ensure the approximation holds for a continuous function f within a given error \u03b4. The network fulfilling Equation (2) is where \u03c6 c is as in Definition 4.2, with a finite set \u2206 k of possible hyperrectangles in grid G. The proof involves constructing hyperrectangles on a grid to ensure the approximation holds for a continuous function f within a given error \u03b4. Recall that \u03be 0 = min f (\u0393). We define the ReLU network Let B \u2208 B(\u0393). By applying standard rules for interval analysis, we find that for all k except the 3 highest and lowest terms in the sum, there exists x \u2208 B such that f(x) \u2265 \u03be q = \u03be q-1 + \u03b4. If p + 3 \u2264 q, then there is an x \u2208 B such that f(x) \u2265 \u03be p+3 = \u03be p+2 + \u03b4. The proof involves constructing hyperrectangles on a grid to ensure the approximation holds for a continuous function f within a given error \u03b4. By applying standard rules for interval analysis, it is shown that for all real valued continuous functions f on compact sets, there exists a ReLU network approximating f arbitrarily well with the interval abstraction. This answers the open question of whether the Universal Approximation Theorem generalizes to Interval. The Universal Approximation Theorem generalizes to Interval analysis, addressing the question of interval abstraction's expressiveness in analyzing networks approximating functions. Monotonicity of operations and N-slicing of functions are discussed, along with the clipping function's definition and properties. The ReLU network R [* ,b] sends values less than or equal to b to themselves, and values greater than b to b. The network nmin is defined as the minimum of two inputs. A ReLU network nmin N is defined for all N greater than or equal to 1. The expression R(a+c)\u2212R(\u2212a\u2212c) simplifies to a+c under certain conditions. Induction is used to prove properties of the network. The proof by induction shows that for N \u2265 2, the statement holds for all values within a certain range. The Heine-Cantor theorem is used to establish the uniform continuity of the function f on the interval [0, 1]m."
}