{
    "title": "BkxfshNYwB",
    "content": "In this paper, a new pooling operation for Graph Neural Networks (GNNs) is proposed, utilizing a differentiable unsupervised loss based on the minCut optimization objective. The method learns soft cluster assignment vectors for each node, incorporating node features, target inference tasks, and the connectivity structure of the graph. Graph pooling is achieved by applying the assignment vectors to the adjacency matrix and node features. The proposed pooling method in deep convolutional neural networks allows for learning abstract representations by discarding unnecessary information, controlling model complexity, and extending the convolution operation to data with arbitrary topologies through Graph Neural Networks (GNNs). The development of pooling strategies for Graph Neural Networks (GNNs) has lagged behind newer message-passing operations due to the difficulty of defining an aggregated version of the original graph. Na\u00efve pooling strategies like averaging all node features lack flexibility and do not extract local summaries of the graph structure. An alternative approach involves pre-computing coarsened versions of the original graph, which accounts for connectivity but ignores task-specific objectives. In this paper, a differentiable pooling operation is proposed for Graph Neural Networks (GNNs) to address the limitations of existing pooling strategies. The minCUT pooling operator learns to distil global properties from the graph by identifying dense graph components and clustering nodes with similar features. The parameters in the pooling layer are optimized using a combination of task-specific loss and unsupervised regularization, enhancing the performance of downstream tasks. The minCUT pooling operator in Graph Neural Networks aims to partition nodes in a graph by removing the minimum volume of edges. This is achieved by maximizing edge volume within clusters while minimizing edges between clusters. The problem is NP-hard and is usually solved using a relaxed formulation for near-optimal solutions. The optimization problem involves finding a near-optimal solution using spectral clustering, where node representations are clustered with algorithms like k-means. However, the computation complexity limits its applicability to large datasets. To address scalability issues in spectral clustering, gradient descent algorithms can solve the constrained optimization problem by refining the solution through iterative operations. These algorithms search for solutions on the manifold induced by orthogonality constraints on the columns of Q. Alternative approaches use QR factorization to constrain feasible solutions and reduce computational costs. Other methods involve neural networks, such as autoencoders, to avoid spectral decomposition. Many approaches have been proposed to process graphs with neural networks, including recurrent architectures or convolutional operations inspired by filters used in graph signal processing. The focus here is on graph pooling, with a GNN implementation based on a simple MP operation that combines node features with 1st-order neighbors. The minCUT pooling strategy computes a cluster assignment matrix by mapping node features into rows of S using a multi-layer perceptron. The trainable weights for the MP layer are used for mixing and skip components, preserving the original adjacency matrix structure. The minCUT pooling strategy involves trainable parameters for cluster assignment optimization. The softmax function enforces constraints on the cluster assignment matrix. Parameters are jointly optimized by minimizing task-specific and unsupervised loss terms. The cut loss term encourages clustering of strongly connected nodes. It reaches a maximum when cluster assignments are orthogonal and a minimum when cluster assignments are equal in disconnected components. The orthogonality loss term penalizes degenerate solutions in cluster assignment optimization by encouraging orthogonal cluster assignments and similar cluster sizes. It helps prevent trivial optimal solutions where all nodes are assigned to the same cluster or all clusters have uniform assignments. The orthogonality loss term in L o ensures unitary norm and non-dominance over L c, allowing direct summation. I K is a rescaled clustering matrix with Frobenius norm optimizing intra-cluster variance. L o introduces a soft constraint, compromising convergence due to non-convexity of L c. Cluster assignments are well initialized after the MP operation. After the MP operation, connected vertices have similar features due to the smooth MLP function, resulting in similar cluster assignments. The minCUT objective in GNN serves as a regularization term, providing a sub-optimal solution that may be suitable for specific downstream tasks. Optimizing task-specific loss helps GNN avoid degenerate minima. The coarsened adjacency matrix and graph signal are computed to find clusters with many internal connections. The trace maximization in GNN yields clusters with strong internal connections but weak connections to other clusters. A pool is a diagonal-dominant matrix with self-loops, hindering propagation in MP operations. The new adjacency matrix is computed by zeroing the diagonal and applying degree normalization. The method is easy to implement using standard linear algebra operations. minCUTpool differs from classic SC methods by considering node features for cluster assignments. minCUTpool assigns nodes based on their features, suitable for GNNs with similar connected nodes. It can handle multiple graphs independently and generalize to out-of-sample data. This is crucial for graph classification tasks with varying graph structures. Additionally, minCUTpool directly uses soft cluster assignments without the need for k-means. Diffpool is a trainable pooling method that generates coarsened versions of graphs through differentiable functions. It includes two parallel MP layers to compute new node features and cluster assignments. The unsupervised loss encourages nearby nodes to be clustered together and minimizes the entropy of the cluster assignments. Like minCUTpool, Diffpool clusters vertices of annotated graphs. Diffpool is a trainable pooling method that clusters vertices of annotated graphs differently from minCUTpool. It uses a Top-K pooling approach to retain nodes with the highest scores, dropping the rest for memory efficiency. The unsupervised loss in Diffpool exhibits pathological behaviors, discussed in later experiments. Top-K pooling drops rows and columns from the adjacency matrix A 2 to create a new matrix, but computing A 2 is inefficient. Topological pooling methods pre-compute coarsened graphs based on topology only, not node features. These methods provide a stronger bias to prevent degenerate solutions. The approach by Bruna et al. (2013) utilizes GRACLUS, a hierarchical algorithm for graph clustering. Node decimation is a method used in graph signal processing and adapted for GNNs, where nodes are partitioned based on Laplacian eigenvectors and one set is dropped to reduce the number of nodes by half each time. Kron reduction is then applied to compute coarsened Laplacians. The text discusses the use of pooling operations in graph neural networks for tasks like node clustering. It compares minCUTpool with other strategies and includes experiments on graph reconstruction and regression using pooling layers. The effectiveness of the proposed loss is studied in various node clustering tasks. The text evaluates the effectiveness of a proposed loss in node clustering tasks using a simple GNN with a single MP layer and pooling layer. Experiments on synthetic networks show that minCUTpool generates accurate and balanced partitions compared to other methods like Diffpool. Diffpool assigns some nodes to the wrong community and produces an imbalanced partition in image segmentation tasks. The recursive normalized cut technique is used for clustering nodes, with minCUTpool yielding a more precise segmentation compared to SC and Diffpool. In clustering citation networks like Cora, Citeseer, and Pubmed, minCUTpool generates accurate and balanced partitions. The nodes in popular citation networks like Cora, Citeseer, and Pubmed are represented by sparse bag-of-words feature vectors. The GNN architecture with minCUTpool achieves a higher NMI score compared to SC, and outperforms Diffpool due to its ability to avoid degenerate solutions. The quality of partitions is tested by checking the agreement between cluster assignments and true class labels. The NMI scores evolve as unsupervised losses in Diffpool and minCUTpool are minimized during training. The models are tested on various graph classification datasets using fixed network architecture and different pooling methods. Each pooling method drops half of the nodes in a graph to improve performance. Each pooling method drops half of the nodes in a graph to improve performance. Results show that minCUTpool consistently outperforms other GNN architectures, while some pooling methods may not always enhance performance compared to a baseline. The WL kernel generally performs worse than GNNs, except for the Mutagenicity dataset due to smaller graphs. Dense architecture can achieve good accuracy in Proteins and COLLAB datasets, indicating limited information from graph structure. Graclus and Decimation are the fastest pooling methods, with minCUTpool outperforming other GNN architectures consistently. The proposed pooling layer for GNNs optimizes a regularization term based on the minCUT objective to produce optimal node partitions for tasks. Results show that minCUTpool outperforms existing pooling strategies for GNNs on unsupervised clustering and supervised graph classification tasks. An autoencoder is trained to compare information retention in coarsened graphs. The pooling operations in Diffpool and minCUTpool retain 25% of the original nodes by minimizing mean squared error. Top-K generates a binary mask to indicate nodes to drop or retain, creating an upsampling matrix by dropping columns of the identity matrix. The unpooling operation upscales the graph with zeroes for dropped nodes. Original and reconstructed graph signals are compared using different pooling methods on a ring graph and a regular graph. Diffpool and minCUTpool retain 25% of original nodes by minimizing mean squared error, while Top-K pooling fails to maintain enough information from the original graph. Top-K discards nodes associated with the highest K values of a score vector, leading to loss of connectivity and similarity among nodes. The QM9 chemical database consists of small organic molecules represented as graphs. Atoms are nodes, edges represent chemical bonds, and node features include atomic number. Edge attributes are ignored in this experiment to allow for the use of all pooling algorithms without modifications. The experiment compares trainable pooling methods on a graph regression task using small graphs from the QM9 dataset. A GNN architecture with different pooling methods is trained to predict chemical properties. Performance is evaluated with 10-fold cross-validation, using MSE as the supervised loss. The experiment compares trainable pooling methods on a graph regression task using small graphs from the QM9 dataset. The MSE obtained for different pooling methods is reported, with the flat baseline showing lower error in most cases. Top-K outperforms Diffpool on average, while minCUTpool significantly outperforms all other methods. The best results with statistical significance are highlighted in Table 3. Neurons with linear activations in the MLP and MP layer are used to compute the cluster assignment matrix S. The models are trained for 10000 iterations with an ELU activation in both architectures. GNN architectures are trained with Adam, L2 penalty loss, and 16 hidden units. Real-world graph datasets like Mutagenicity, Proteins, DD, COLLAB, and Reddit-2k are used, along with Bench-easy and Bench-hard datasets where node features and adjacency matrix alone are uninformative for classification. The text discusses the importance of considering both node features and graph structure for graph classification tasks. Various GNN architectures are used for different tasks such as clustering, segmentation, classification, and regression. The datasets used include real-world graphs and Bench-easy/Bench-hard datasets. Visual representations of different GNN architectures are provided in figures."
}