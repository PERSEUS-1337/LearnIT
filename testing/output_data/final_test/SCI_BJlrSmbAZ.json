{
    "title": "BJlrSmbAZ",
    "content": "Deep neural networks have significantly advanced various domains, but handling model uncertainty remains a challenge. This paper introduces a practical Bayesian learning approach using batch normalization in deep networks, allowing for useful estimates of model uncertainty without altering the architecture or training process. Our approach using batch normalization in deep networks provides useful estimates of model uncertainty without changing the network or training process. It outperforms baselines on various tasks with strong statistical significance, despite the potential for deep networks to make mistakes with serious consequences in settings like autonomous vehicles and high frequency trading. Automated systems are being developed to screen for skin cancer, breast cancer, and diagnose biopsies. Deep learning techniques lack methods to account for uncertainty in the model, which can be problematic when the network encounters new conditions. This work focuses on estimating predictive uncertainties in deep networks using a Bayesian approach, which provides a theoretical framework for modeling uncertainty. Bayesian neural networks (BNNs) have been studied since the 1990s but require more computational resources and have challenging inference. Bayesian models adapt neural networks to Bayesian reasoning by placing a prior distribution over each weight. Inference in Bayesian neural networks is challenging, leading to the development of approximate methods like variational inference and probabilistic backpropagation. Sampling methods are also used to estimate a factorized posterior distribution. Deep Gaussian Processes (DGPs) are Bayesian models that can handle large datasets efficiently. They outperform other approximate Bayesian Neural Networks (BNNs) in terms of RMSE and uncertainty quality. Bayesian hypernetworks use a neural network to learn a distribution of parameters over another neural network. These techniques address challenges with BNNs but require modifications to architecture and specialized knowledge from practitioners. The methodology of this work is to treat a deep network trained with batch normalization as a Bayesian model to obtain an estimate of the posterior using multiple predictions and dropout masks. This technique, known as MCDO, has been shown to be competitive with other Bayesian Neural Network methods and Deep Gaussian Processes in terms of RMSE and uncertainty quality. The methodology involves treating a deep network trained with batch normalization as a Bayesian model to obtain uncertainty estimates for its predictions. Bayesian models and a variational approximation using KL divergence are introduced, showing how a batch normalized deep network can be seen as an approximate Bayesian model. The induced prior on parameters when using batch normalization is studied, and a procedure for estimating uncertainty in the network's output is described. The methodology involves obtaining label predictions by sampling from a posterior distribution of model parameters in Bayesian modeling. Variational approximation is used to minimize the KL divergence between the approximating distribution and the true posterior. Monte Carlo integration is employed to approximate the integral for parameter estimation. The methodology involves using Monte Carlo integration to approximate the integral for parameter estimation in Bayesian modeling. The optimization procedure of a deep network with batch normalization is described, drawing resemblance to approximate Bayesian modeling. The inference function of a feed-forward deep network with L layers is detailed, with element-wise nonlinearity functions and weight vectors at each layer. Batch normalization is a unit-wise operation in deep networks that standardizes the distribution of each unit's input using estimated mean and variance from mini-batches during training. This process makes inference at training time stochastic. Batch normalization standardizes the input distribution of each unit in deep networks using estimated mean and variance from mini-batches during training. This process introduces stochasticity during inference. Mini-batch optimization involves constructing the Evidence Lower Bound (ELBO) and using multiple samples for \u03b8 optimization in deep networks. The loss function for training includes empirical loss on training data and a regularization penalty on model parameters. Batch normalization in deep networks involves standardizing the input distribution of each unit using estimated mean and variance from mini-batches during training. This introduces stochasticity during inference. The optimization objective for batch normalized networks is equivalent to minimizing the negative log-likelihood with a prior on model parameters. The distribution of sampled batch members converges to the i.i.d. case over a large number of epochs. The optimization objectives of batch normalized networks involve minimizing negative log-likelihood with a prior on model parameters. The induced prior from L2-regularization is discussed to reduce variance in deep networks. The pre-trained batch normalized network can estimate uncertainty in predictions using inherent stochasticity. The induced prior from L2-regularization in deep learning is studied in Appendix 6.5, showing that Batch Normalization (BN) in deep networks with Fully Connected (FC) layers and ReLU activations induces Gaussian distributions over BN unit's means and standard deviations. This distribution converges towards an exact prior for large training datasets and deep networks, assuming Gaussian priors and factorizing the distribution across all stochastic parameters. The mean and variance for the BN unit's standard deviation have no relevance for optimization reconciliation. The standard deviation and variance of BN unit's means have no relevance for optimization reconciliation. Approximate predictive distribution is expressed using the approximate posterior. The first and second moments of the predictive distribution are estimated empirically. Sampling the net's stochastic parameters involves updating parameters in BN units. Sampling from the training set ensures consistency with the mini-batch size used during training. Training the network involved minimizing KL divergence wrt \u03b8. During inference, the network's stochastic parameters are updated after each forward pass with input x. The mean and sample variance of outputs are computed after T passes to estimate the predictive distribution. The network is trained like a regular BN network, but parameters are updated stochastically for prediction instead of using population values. The approximate predictive distribution can be approximated by a Gaussian for regression, assuming bounded input domains. The Liapounov CLT condition ensures Gaussian inputs in the first layer of the network, with bounded outputs for each BN unit. Multimodality is observed in the form of p* and MCDO implies a bimodal variational distribution over weight matrix columns. The uncertainty quality of MCBN is assessed quantitatively and qualitatively. The uncertainty quality of MCBN is evaluated quantitatively and qualitatively using eight standard regression datasets. Results are reported using standard metrics, with proposed upper and lower bounds for normalization. The dataset properties, dataset size (N), and number of input features (Q) are detailed in Table 1. MCBN's uncertainty quality is evaluated using standard metrics like Predictive Log Likelihood (PLL) and Continuous Ranked Probability Score (CRPS). The results are demonstrated through visualizations and applied to SegNet, showing the benefits of MCBN in a batch normalized network. Predictive Log Likelihood (PLL) is a metric that evaluates uncertainty quality in regression models without assumptions about the distribution form. It measures the model's predicted PDF against observed data, with criticisms for its sensitivity to outliers. Continuous Ranked Probability Score (CRPS) is a less sensitive measure that considers the full predicted PDF. CRPS is a metric that evaluates uncertainty quality in regression models by considering the full predicted PDF. It is defined as the sum of the squared area between the predictive CDF and 0 where y < y i and between the CDF and 1 where y \u2265 y i. A perfect prediction yields a CRPS of 0, with no upper bound. To establish a lower bound on useful performance, a baseline model predicts constant variance regardless of input. The Constant Uncertainty BN (CUBN) model optimizes CRPS on validation data by setting a fixed variance, representing a best guess of constant variance on test data. A comparative model, Constant Uncertainty Dropout (CUDO), is implemented using MCDO. Variance modeling differences between MCBN, CUBN, MCDO, and CUDO are visualized in uncertainty bounds plots. Uncertainty estimates can be normalized between lower and upper bounds defined by CUBN and optimized CRPS scores. The normalized measure CRPS = DISPLAYFORM0 provides an intuitive understanding of how close a Bayesian model is to estimating perfect uncertainty for each prediction. Evaluation of Multiplicative Normalizing Flows (MNF) for variational Bayesian networks shows a highly flexible approximate posterior achieved by applying normalizing flows to auxillary variables. The evaluation of MCBN and MCDO is similar to previous work by Hern\u00e1ndez-Lobato & Adams (2015), using comparable datasets and metrics. Different hyperparameter selection, dropout rates, and larger networks with two hidden layers were implemented in comparison to BID6. Models had two hidden layers with 50 units each, using ReLU activations, and normalized input and output data during training. Results were averaged over five random splits of test and training data, with 5-fold cross-validation used to find optimal hyperparameters and epochs. Hyperparameter grid for BN-based models included weight decay factor and RMSE minimization objective. The hyperparameter grid for DO-based models included weight decay range and dropout probabilities. The optimal model was used to optimize \u03c4 numerically based on average CV CRPS. Predictive distribution estimates were obtained by taking 500 stochastic forward passes through the network. TensorFlow was used for implementation, with the Adam optimizer and a learning rate of 0.001. Training and cross-validation were conducted on Amazon web services using 3000 machine-hours. The experiment involved training and cross-validation on Amazon web services using 3000 machine-hours. Results and code are available on a github repository. Uncertainty quality of MCBN, MCDO, and MNF was measured on eight datasets. CRPS and PLL were reported and compared to lower bound models using a t-test. Additional details can be found in the appendix. A novel visualization of uncertainty quality was provided. In Appendix 6.6, a novel visualization of uncertainty quality in regression datasets is presented. Model predictions errors are sorted by estimated uncertainty, with shaded areas representing model uncertainty and gray dots showing prediction errors. A correlation between estimated uncertainty and mean error is observed, indicating the model's ability to recognize samples with potential for predictive errors. Qualitative results for Bayesian SegNet using MCBN on the CamVid model are shown in FIG1. The curr_chunk discusses the estimation of mean and variance of MCBN, with qualitative results shown in FIG1 for Bayesian estimated segmentation on the CamVid driving scenes dataset. Additional experimental results are provided in Appendix 6.6, including mean CRPS and PLL values for MCBN and MCDO in TAB5, indicating comparable performance. RMSE results in Table 6 show slight improvements in predictive accuracy for MCBN and MCDO compared to non-stochastic BN and DO networks. The results in TAB2 and Appendix 6.6 demonstrate that MCBN provides meaningful uncertainty estimates that align with prediction errors. Significant improvements over CUBN are observed in various datasets for CRPS and PLL. Visualizations in FIG0 and Appendix 6.6 show a clear relationship between model uncertainty and prediction errors. Comparisons with MCDO show that MCBN performs similarly, with better CRPS results in most cases. However, caution is advised due to differences in network parameters affecting direct comparisons. Contradictory results are noted in the Yacht Hydrodynamics dataset. The results on the Yacht Hydrodynamics dataset show contradictory trends in uncertainty estimation. The CRPS score for MCBN is extremely negative, while the PLL score is extremely positive. The opposite trend is observed for MCDO. The small size of the dataset, Gaussian assumption of CRPS, and variability in model accuracy confound the measurements. Criticism of uncertainty estimates based on CRPS and PLL scores in TAB2 is noted, with scores rarely exceeding 10% improvement over the lower bound. However, context is crucial in interpreting these measures. In context, caution is advised when interpreting uncertainty measures. Comparisons with previous work show similar results. Training deep networks with batch normalization is equivalent to approximate inference, impacting the prior distribution of BN units' means. Training deep networks with batch normalization is equivalent to approximate inference in Bayesian models, allowing for meaningful uncertainty estimates without modifying the network or training procedure. The uncertainty estimates from MCBN correlate with prediction errors and are beneficial for tasks like regression and image segmentation. MCBN shows improvement comparable to MCDO and MNF. New evaluation metrics and a visualization tool enhance the assessment of uncertainty quality. Batch normalization has become crucial in deep networks for estimating model uncertainty. To approximate the posterior, a family of distributions parametrized by \u03b8 is used. The goal is to make q \u03b8 (\u03c9) similar to p(\u03c9|D) by minimizing KL divergence. This is equivalent to maximizing the ELBO. Subsampling can be used for iterative optimization, and a reparametrization is made by setting \u03c9 = g(\u03b8, ) where is a RV. Batch normalization is essential in deep networks for estimating model uncertainty. To approximate the posterior, a family of distributions parametrized by \u03b8 is used. The goal is to minimize KL divergence between q \u03b8 (\u03c9) and p(\u03c9|D) by maximizing the ELBO. Reparametrization is done by setting \u03c9 = g(\u03b8, ) where is a RV. The distribution of mean and standard deviation of a mini-batch can be approximated separately to two Gaussians. To estimate model uncertainty in deep networks, batch normalization is crucial. The posterior is approximated using a family of distributions parametrized by \u03b8 to minimize KL divergence. Reparametrization is achieved by setting \u03c9 = g(\u03b8, ) where is a RV. The mean and standard deviation of a mini-batch can be approximated separately to two Gaussians. For large M, the Taylor expansion of f (x) = \u221a x around a = \u03c3 2 is used, which is approximately Gaussian by CLT. The Cramer-Slutzky Theorem is applied to show that Term B and Term C are approximately 0 for large M. Predictive distribution properties are derived in section 3.4, with estimations for predictive mean, variance, and log likelihood. The quality of uncertainty in deep networks is evaluated using predictive mean, variance, and log likelihood. The predictive covariance matrix combines observation noise variance and sample covariance from stochastic forward passes. The Predictive Log Likelihood (PLL) measures the model's uncertainty quality by sampling stochastic parameters from the posterior distribution. The Predictive Log Likelihood (PLL) evaluates uncertainty in deep networks by sampling stochastic parameters from the posterior distribution q \u03b8 (\u03c9) using MC integration with T samples. Training involves SGD with mini-batch size M, L2-regularization, and equivalence between objectives of Eq. FORMULA3 and FORMULA11. The approximate posterior induced by batch normalization is found to yield Gaussian distributions of stochastic variables \u00b5 u B, \u03c3 u B. The Conditional Logit Transformation (CLT) results in Gaussian distributions of stochastic variables \u00b5 u B, \u03c3 u B for large M. The population-level moments \u00b5 u and \u03c3 u are considered, with h u as the BN unit's input. By setting \u03c3 q = 0, it is found that for \u03c9 i = \u03c3 u B, \u00b5 q = 0 and \u03c3 q = 0, nullifying Eq. (7). The KL divergence terms' partial derivatives are crucial. Analyzing the training of the Yacht dataset for hidden layers reveals data on epochs and batch sizes, showing the distribution of standard deviation of mini-batches closely following an analytically approximated Gaussian distribution. Data is collected for one unit of each layer for different epochs and batch sizes."
}