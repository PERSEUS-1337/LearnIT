{
    "title": "H1lDbaVYvH",
    "content": "The text proposes the idea that living organisms and artificial agents search for order amidst chaos to develop useful behaviors. This concept is formalized into an unsupervised reinforcement learning method called surprise minimizing RL (SMiRL). SMiRL trains agents to maximize the probability of observed states, resulting in proactive behaviors like balancing and damage avoidance. These agents can successfully play games like Tetris and Doom, as well as control a humanoid to avoid falls, without specific rewards. SMiRL is an unsupervised reinforcement learning method that trains agents to maximize the probability of observed states, enabling proactive behaviors like balancing and damage avoidance. It can be used to control a humanoid to avoid falls without specific rewards. The struggle for existence is not for raw materials or energy, but for negative entropy. Organisms create niches to maintain predictability amidst increasing entropy. Humans build cities to shield themselves from surprise and maintain order amidst chaos. This drive for preserving order guides automatic behaviors. The text discusses the idea of preserving order amidst chaos guiding the acquisition of useful behaviors in artificial agents through unsupervised reinforcement learning. It highlights the importance of natural forces and other agents in providing novelty, contrasting simulated environments like video games. The second law of thermodynamics emphasizes perpetual novelty in natural environments, where agents must maintain homeostasis to preserve their bodies and avoid threats. Novelty-seeking behaviors can naturally emerge to satisfy hunger, while a SMiRL agent can strategically increase surprise temporarily to reduce it in the long term. Building a house initially introduces novelty but ultimately provides stability for the agent. SMiRL is an objective for reinforcement learning based on surprise minimization. It helps agents seek stability in dynamic environments by minimizing novelty and maintaining a stable equilibrium. SMiRL works by maintaining a density of visited states and training a policy to act for high likelihood future states. It induces useful equilibrium-seeking behaviors in various environments and can accelerate reinforcement learning in dynamic environments. Surprise minimization is proposed as a method to learn useful behaviors in entropic environments by seeking to maintain order amidst chaos. Actions taken by an agent to minimize surprise over its lifetime involve reaching stable states and maintaining homeostasis to avoid unexpected outcomes. The long-term effects of actions on surprise can be complex and counterintuitive, as actions not only change the agent's state but also its beliefs about likely states. The SMiRL formulation involves an agent seeking states with high probability and visiting new states to alter probabilities for larger rewards. This meta-level reasoning can lead to behaviors where the agent builds shelter to decrease surprise from changing weather. Natural environments provide disruptive forces to prevent collapse to degenerate solutions. The reinforcement learning agent's reward is based on the familiarity of its current state. The SMiRL agent receives a reward based on the familiarity of its current state, using a generative model to evaluate negative surprise and maximize future state probabilities. The optimal policy in reinforcement learning considers future state changes to maximize rewards. A practical algorithm for surprise minimization involves reasoning about the effect of actions on future states and density estimates. This is achieved through episodic training and resetting the data at the beginning of each episode. SMiRL learns the agent's policy parameters \u03c6 through episodic training and resetting data at the beginning of each episode. The policy is conditioned on sufficient statistics of the dataset D t, such as \u03b8 t and |D t |, to reason about future evolution. This approach allows for utilizing deep density models without high-dimensional parameter conditioning. SMiRL allows the use of deep density models without high-dimensional parameter conditioning. The policy parameters are learned through episodic training, with data reset at the start of each episode. The policy is conditioned on dataset statistics like \u03b8 t and |D t | for future reasoning. The algorithm pseudocode is provided for reference. In practice, the choice of model class for the generative model p \u03b8 (s) is crucial. Simple distribution classes work well in simple environments, but more complex environments may require sophisticated density estimators like variational autoencoders (VAEs) to learn a compressed state representation for SMiRL. The VAE is used to compute s from the encoder output z, with a KL divergence loss between prior p(z) and q \u03c9 (z|s) to maintain distribution. VAE is trained online, with updates interleaved with RL updates. To address data requirements, VAE parameters are not reset between episodes, and a separate episode-specific distribution p \u03b8t (z) is tracked. This distribution is fit to the episode's state history and replaces p \u03b8t (s) in the SMiRL algorithm. Training the VAE online involves fitting the state density model to encoder outputs, providing richer state representations for SMiRL. This approach deviates from updating the density model within episodes only but leads to improved surprise-seeking behavior in practice. SMiRL's performance is evaluated across various dynamic environments, showcasing its effectiveness in video game and robotic control scenarios. The classic game of Tetris provides a naturally entropic environment where the world evolves independently, requiring active engagement from the agent. This dynamic setting is crucial for evaluating unsupervised RL approaches, including SMiRL, in more realistic and challenging environments. The agent's behavior in Tetris requires active intervention to maintain homeostasis on a 4x10 board with tromino shapes. Evaluation measures rows cleared and instances of the agent reaching the top of the board within 100 episodes. VizDoom environments TakeCover and DefendTheLine are considered for evaluation. TakeCover and DefendTheLine are VizDoom environments where enemies throw fireballs at the player. In TakeCover, the agent's performance is based on the \"damage\" taken, while in DefendTheLine, the agent's survival is measured by how long it stays alive. The observation space includes grayscale first-person images, and the action space involves moving left or right. In miniGrid, the agent navigates with partial environmental observations. In miniGrid, the agent navigates with partial environmental observations, trying to escape enemy agents and reach a safe room. The task involves simulated humanoid robots facing danger of falling, with actions based on PD targets for joints. The Cliff task challenges the agent to stay on a cliff edge to avoid irregular configurations. In the Treadmill, Pedestal, and Walk environments, SMiRL is used to assist the agent in learning locomotion, balancing, and walking by providing stability rewards and reducing falls. The Walk domain evaluates the SMiRL reward as a stability reward to help the agent learn how to walk effectively. In various environments, SMiRL is utilized to aid the agent in learning locomotion, balancing, and walking by providing stability rewards and reducing falls. The agent's performance is measured by the proportion of episodes with a fall, with a state classified as a fall under specific conditions. The experiments aim to assess if SMiRL can learn complex behaviors in different tasks, showing meaningful emergent behaviors in Tetris, VizDoom, Cliff, and Treadmill environments. In the Tetris and VizDoom environments, the SMiRL agent learns proactive and emergent game playing behaviors to avoid unusual states. In the Cliff environment, the agent reduces the probability of falling by stabilizing itself at the edge. In the Treadmill environment, SMiRL learns complex locomotion behavior to increase time on the treadmill and reduce falls. In various environments, including TakeCover and Cliff, a VAE model is trained to estimate surprise in the latent space. The VAE representation leads to faster acquisition of emergent behaviors compared to intrinsic motivation methods. SMiRL is compared to ICM and RND for intrinsic motivation, achieving near-perfect play in Tetris after 2000 epochs. In various environments, including TakeCover and Cliff, a VAE model is trained to estimate surprise in the latent space, leading to faster acquisition of emergent behaviors. SMiRL is compared to ICM and RND for intrinsic motivation, achieving near-perfect play in Tetris after 2000 epochs. ICM seeks novelty by creating distinct patterns of blocks, deteriorating game scores over time. SMiRL effectively learns to dodge fireballs on TakeCover. The combination of SMiRL and curiosity leads to increased initial learning speed on the Treadmill environment. SMiRL demonstrates increased learning speed and produces a walking-type gait in a task environment. It successfully navigates hallways, avoids enemies, and reaches a safe room through a randomly placed door. The paper explores pragmatic applications such as joint training for accelerated reward-driven learning and rudimentary imitation learning. SMiRL can be adapted for imitation tasks by initializing the buffer with expert demonstrations or desired outcome states. In a Tetris imitation task, user-specified board states are used. The capability for imitation emerges automatically in SMiRL without any additional modifications. In the next experiment, SMiRL is used to accelerate reward-driven behavior in environments with many possible actions leading to diverse but undesirable states. This can improve safety during training by helping the agent avoid unfamiliar situations. SMiRL is applied in the DefendTheLine and Walk tasks by augmenting the task reward with SMiRL reward. In the Walk task, the SMiRL reward term significantly reduces falls during training and helps the agent learn to walk better, especially when expert data is used. In the DefendTheLine task, SMiRL is compared as a joint training objective to traditional methods. In the comparison between SMiRL and traditional novelty-driven bonuses like ICM and RND, SMiRL shows faster learning without demonstration data. This suggests SMiRL can accelerate learning and reduce unsafe behavior in dynamic environments. Prior works in reinforcement learning have focused on learning intelligent behaviors with provided rewards, which can be scarce or difficult to provide in practical scenarios. Intrinsic motivation in reinforcement learning focuses on encouraging novelty-seeking behaviors through various methods such as maximizing model uncertainty, prediction error, state visitation counts, surprise maximization, and novelty-based reward bonuses. This approach contrasts with traditional reward-driven learning methods. In contrast to traditional reward-driven learning methods, the approach discussed incentivizes minimizing surprise to study behaviors in dynamic environments. This method can lead to complex behaviors in resource-constrained or non-competitive environments, promoting the emergence of behaviors that maintain \"homeostasis.\" Our method promotes the emergence of complex behaviors by resisting entropy growth in dynamic environments, leading to intelligent and sustainable actions in tasks like Tetris, VizDoom, and bipedal robot control. This contrasts with traditional novelty-based exploration methods, showing promise in unsupervised reinforcement learning. Our method promotes intelligent and complex policies in various applications, including risk aversion and imitation. The choice of state representation influences the behavior of a surprise minimizing agent. Future work directions are suggested based on the investigation of surprise minimization. The SMiRL algorithm utilizes independent Gaussian distributions to model observations in VizDoom and Humanoid environments. The reward calculation involves sample mean and standard deviation from accumulated states. The RL algorithm in SMiRL operates on a standard stationary MDP with augmented state parameters. The MDP used in SMiRL is Markovian, with stationary state transition and reward functions. The version using a VAE representation is not Markovian, but still yields good results. Many intrinsic reward methods lack stationary rewards as well. SMiRL uses a Markovian MDP with stationary functions. Intrinsic reward methods like ICM and RND lack stationary rewards. SMiRL seeks state distributions that are easy to preserve. VAE is used for surprise modeling and trained at the end of each episode. The VAE encoder changes during SMiRL, aiding policy learning. The RL algorithms used for training parameters in different environments are deep Q-learning for Tetris and VizDoom, and TRPO for Humanoid domains. Policies are parameterized by neural networks, with VizDoom using a convolutional network. VAEs with different KL-divergence coefficients are used for surprise modeling in VizDoom and Humanoid experiments."
}