{
    "title": "rJlEojAqFm",
    "content": "Relational Forward Models (RFM) are introduced for multi-agent learning, enabling accurate predictions of agents' future behavior in complex environments. These models operate on discrete entities and relations, providing insights into agents' behavior and social interactions. Embedding RFM modules in agents leads to faster learning systems compared to non-augmented baselines. Developing richer analysis tools for understanding how and why agents make decisions is crucial as autonomous systems become more multi-agent. Multi-agent systems, like assembly lines and warehouse management systems, have gained attention, with research in multi-agent reinforcement learning showing impressive results. Coordinating behavior among learning agents remains a challenge in this domain. In learning-based multi-agent systems, coordination is achieved through centralized controllers, but scaling to a large number of agents or mixed human-robot ensembles is a challenge. There is a growing focus on multi-agent systems that can learn to coordinate on their own. Analyzing coordinated behaviors in diverse agent systems is also a challenge, as current tools focus on individual agent functioning and are not equipped to characterize the system as a whole. The text discusses the challenges in characterizing systems of diverse agents and the lack of tools for measuring contextual interdependence of agents' behaviors. The development of Relational Forward Models (RFM) for multiagent systems is proposed to address these challenges, using neural networks for relational reasoning. These models outperform previous methods and provide intermediate representations for social analysis of multi-agent systems. Relational Forward Models (RFM) are proposed to characterize agents' behavior, track influences, and identify environmental factors mediating social interactions. These models, embedded in agents, enhance coordination speed. Deep learning models operating on graphs have been successful in learning dynamics of systems with rich relational structures. Recently, deep-learning methods, including deep reinforcement learning, have been used to predict the dynamics of multi-agent systems like basketball or soccer games. These methods provide insights into relational and social structures present in the data. Researchers are developing analysis tools to understand agents' behavior, common failure modes, and the inner workings of learning systems by embedding behavioral trajectories of single RL agents in a high-dimensional space. Our model, based on graph networks, uses supervised learning to predict multi-agent system dynamics by embedding RFM modules in RL agents. This allows for faster coordination among teammates, similar to imagination-augmented agents in single-agent RL settings. The RFM module in our model, based on graph networks, predicts multi-agent system dynamics and outputs action predictions or cumulative reward predictions. The model performs well and produces interpretable representations for analysis and decision-making by artificial agents. The RFM module in the model utilizes a Graph Neural Network (GN) to stack a GN Encoder, Graph GRU, and GN Decoder for relational reasoning across time steps. RFM-augmented agents append the RFM module output to the original observation input for the policy network. The GN operates on directed graphs with graph-level attributes, vertices with attributes, and directed edges connecting entities. The Graph Neural Network (GN) in the model processes directed graphs with graph-level attributes, vertices with attributes, and directed edges connecting entities. The GN updates edge attributes, vertex attributes, and global attributes through \"update\" and \"aggregation\" functions. The Graph Neural Network (GN) processes directed graphs with graph-level attributes, vertices with attributes, and directed edges. It updates edge attributes, vertex attributes, and global attributes using \"update\" and \"aggregation\" functions. The updated edge attribute is called the \"message\", e k. Vertices are updated by aggregating messages and computing updated vertex attributes. Global attributes are updated as a function of aggregated attributes. GN blocks can be composed for more complex architectures and optimized using gradient descent for supervised learning with input-output pairs. The RFM architecture focuses on supervised learning with input-output pairs. It includes three GN blocks arranged for relational reasoning on raw input data and output. The architecture details involve a GN encoder block with separate MLPs for vertices, edges, and global attributes, followed by a \"GraphGRU\" utilizing Gated Recurrent Units. The GraphGRU in the RFM architecture has a hidden state size of 32 for vertices, edges, and globals. Its output is used in state and output graphs, with the latter passing through a GN decoder block for predictions. The RFM module's prediction performance was compared to Neural Relational Inference networks and Vertex Attention Interaction Networks, which are similar architectures operating on graph data. The study compared the RFM model against ablated variants to assess the importance of relational reasoning and time recurrence components. Variants included a Feedforward model without GraphGRU block, a No-relation model operating on self-connected graphs, and a vector-based MLP + LSTM model. Capacity was matched across models, except for NRI. The study compared models with matched capacity in three multi-agent environments: Cooperative Navigation BID20, Coin Game BID29, and Stag Hunt BID27. In Cooperative Navigation BID20, two agents navigate a 6x6 arena to cover two tiles. In Coin Game BID29, two agents roam an 8x8 arena with 12 coins of 3 colors. In the Coin Game BID29, two agents collect coins of different colors in an 8x8 arena for 10 steps. Each agent can only see one good color and must infer the other from their teammate's actions. Rewards are given based on the number of good coins collected and penalties for bad coins. The game is randomized each episode. In a game where agents navigate an arena with red stags and green apples, they can collect apples for a reward or capture stags for a higher reward. The entities' locations are randomized at the start of each episode. RL agents were trained using a multi-agent A2C algorithm to converge on these tasks. In the Coin Game, agents selected actions to move in the environment containing the arena. The image was processed by a convolutional layer and fed into a MLP. The MLP output was concatenated with player's reward, last action, and coin color for input to the model. The forward RFM model was trained using 500,000 episodes of behavioral trajectories of trained agents in their environments. Semantic descriptions of the environment state, agent actions, and rewards were compiled into a graph for training. The RFM model was trained using 500,000 episodes of behavioral trajectories of trained agents in their environments. The input graph contained no edge attributes, while the output graph of the RFM module did contain attributes computed by the network itself. These attributes represented the effect the sender entity had on the receiver agent. The RFM model was trained using 500,000 episodes of behavioral trajectories. Training involved gradient descent to minimize cross-entropy loss. Models predicted actions for each agent in three games using a graph representation of the environment state. Performance was assessed using held-out episodes to measure the mean length of perfect roll-out. The RFM module outperforms the NRI baseline by a substantial margin on the Coin Game and Stag Hunt environments, suggesting that the graph structure inference step proposed in NRI might not be appropriate when the importance of some relations is revealed over time. Additionally, the RFM consistently outperforms the VAIN model, indicating that distributed interaction representations are superior to simple attention weights for this particular task. In the Stag Hunt game, relational models outperform non-relational models for action prediction in multi-agent systems. The Euclidean norm of a message vector is proposed as a measure of influence between sender and receiver entities. This suggestion is validated in a figure showing the edge norm between sender entities. In the Stag Hunt game, relational models outperform non-relational models for action prediction in multi-agent systems. The edge norm between sender entities is predictive of agent behavior changes, influenced by factors like prey availability. The norm of edge messages between teammates is higher, especially with a scarcity of apples, indicating how entities and relations influence agents' behavior in a multi-agent system. The RFM model can quantify the valence of social influence relations and predict returns based on the social context. Edge activations reveal what agents care about and how it changes over time. The RFM model quantifies social influence relations and predicts returns based on social context. Edge activations show changes in what agents care about over time. The norm of the edge connecting agents is modulated by scarcity, influencing behavior. Comparing estimators for agent 1's return before and after capturing a stag shows mutual benefit. The Full graph model estimates agent 1's return based on both agents' states and environment variables, while the Pruned graph estimate removes agent 2's state. Removing the edge between agents can obtain the Pruned graph estimate. If the predicted return decreases without agent 2's information, it suggests agent 2 helps agent 1; if it increases, it suggests hindrance. An experiment was conducted with modifications to target variable and loss function. The target variable and loss function were changed to mean squared error, and the training set included an equal proportion of environment graphs with and without edges between teammates. The ground truth and predicted return for a sample episode using both full and pruned graphs are shown. The single graph neural network predicts agent 1's return using both full and pruned graphs, with edges randomly dropped out between teammates during training to ensure in-distribution for M. At test time, the full-graph estimate is computed using all edges, while the pruned-graph estimator drops out edges between teammates. The on-board RFM module, trained alongside the policy network without parameter sharing, helps agents learn to coordinate faster than baseline agents. Embedding an RFM is more beneficial than embedding an MLP+LSTM. The study showed that embedding a relational reasoning module (RFM) in multi-agent reinforcement learning (MARL) agents can improve their speed of learning. The RFM module was integrated into each agent's architecture, but did not provide additional information in fully observable games. The on-board RFM modules were trained alongside the policy network to help agents coordinate faster than baseline agents. The study integrated a relational reasoning module (RFM) into multi-agent reinforcement learning (MARL) agents to improve learning speed. Each agent had its own RFM module and policy networks, with no sharing of weights or communication between agents. The RFM module processed graph representations of the environment state and minimized cross-entropy loss between its prediction and the last action taken by fellow agents. The study integrated a relational reasoning module (RFM) into multi-agent reinforcement learning (MARL) agents to improve learning speed by using the RFM module's prediction output to augment the observation stream of the policy network. The experimental design involved training A2C agents to play games and pairing them with pre-trained experts. The study integrated a relational reasoning module (RFM) into multi-agent reinforcement learning (MARL) agents to improve learning speed. The estimatorR Pruned graph can implicitly form a posterior over sa 2 before marginalizing it out. Results show that RFM-enhanced agents outperformed baseline A2C agents during training. The study integrated a relational reasoning module (RFM) into multi-agent reinforcement learning (MARL) agents to improve learning speed. Results show that RFM-augmented agents learn to coordinate faster than baseline agents in various game environments, achieving higher scores in fewer steps. Additionally, RFM-augmented agents demonstrate superior efficiency in interpreting teammate actions and inferring key information during gameplay. The study found that augmenting agents with on-board RFM modules improved learning speed in multi-agent environments. The RFM predictions helped agents plan their actions, leading to faster coordination and understanding of others' preferences. The Relational Forward Model captured social dynamics effectively, providing interpretable information that accelerated the learning process for agents. The analysis tools introduced in the study allow researchers to explore the driving factors behind agents' behaviors in multi-agent systems. Our methods, which do not require access to agents' internals, focus on analyzing human behavior, sports, and ecological systems. Augmenting agents with RFM modules improves coordination speed in multi-agent environments. Explicit modeling of teammates and opponents can enhance coordination without the need for communication or centralized controllers. Future work will apply our methods to diverse domains where artificial and non-artificial agents interact and learn together. In the Coin Game, RFM-augmented agents learn faster by discerning teammate's preferences efficiently. The gap between collecting good and bad coins is wider for RFM-augmented agents compared to baseline agents, indicating superior learning ability. Our agents outperform previously-published agents on the game, suggesting the original paper had poor baseline agents. The game may not be as complex as it seems, leaving less room for improvement. Augmenting agents with predictions from a non-relational model helps them coordinate faster. RFM-augmented agents show better performance in learning to coordinate with expert teammates compared to non-augmented baselines. This setup is relevant for interactions between artificial learning agents and human experts. When training alongside other learning agents, longer episodes are used to facilitate training. In experiments with longer episodes (128 steps), training was made easier, resulting in higher total returns. Results on two-player games showed that agents modeling each other led to faster learning. Embedding RFM into learning agents resulted in faster coordination learning, although there was a delay compared to expert teammate conditions. In control experiments, agents were trained on modified versions of the StagHunt game to test alternative hypotheses regarding coordination behavior. The experiments aimed to determine if changes in agent behavior were due to environmental factors or incentives for coordination. In control experiments, agents were trained on modified versions of the StagHunt game to test alternative hypotheses regarding coordination behavior. The experiments aimed to determine if changes in agent behavior were due to environmental factors or incentives for coordination. Stags can be collected by a lone hunter to test if new reward-carrying objects affect coordination. The experimental pipeline involved training A2C agents, collecting behavioral trajectories, training a RFM model, and analyzing the norm of edge activations between agents before and after the appearance of rewards. The results showed no significant change in edge norms, supporting the hypothesis that coordination is not required for collecting stags. The RFM model reveals how agents influence each other in the environment. It outperforms other models in predicting agent behavior, as shown in the next-step action classification accuracy in FIG9."
}