{
    "title": "B1l08oAct7",
    "content": "Bayesian neural networks (BNNs) are a promising solution for dealing with uncertainty in learning from finite data. Variational Bayes (VB) is a theoretically grounded and computationally efficient approach for probabilistic inference in deep neural networks. However, the practical use of VB for BNNs is limited due to the fragility of variational inference, requiring careful initialization and tuning of prior variances. Two innovations are proposed to enhance VB as an inference tool for BNNs: a deterministic method to approximate moments in neural networks and a hierarchical prior for parameters. Bayesian neural networks (BNNs) offer efficient parameter estimation and uncertainty estimates through hierarchical priors and Empirical Bayes procedure. This method shows good predictive performance in heteroscedastic regression, reducing overfitting on small datasets. In this paper, a novel approach for inference in feed-forward Bayesian neural networks (BNNs) is described to address limitations in posterior inference and sensitivity to priors. The method aims to be simple to implement and overcome these challenges. The novel approach for inference in Bayesian neural networks (BNNs) aims to address limitations in posterior inference and sensitivity to priors by reducing variance in Monte Carlo variational inference and deriving a robust Empirical Bayes approach to prior choice using hierarchical priors. This results in a performant and robust BNN inference scheme called \"deterministic variational inference\" (DVI). The deterministic variational inference (DVI) is a robust Bayesian neural network (BNN) inference scheme that demonstrates improved predictive performance in non-linear regression models. It outperforms existing methods based on MCVI and automatically provides better test performance than manual tuning of the prior. Heteroscedastic models consistently outperform homoscedastic models in regression datasets experiments. The text describes the development of a deterministic procedure for propagating uncertain activations through neural networks with uncertain weights and ReLU or Heaviside activation functions. It also introduces an EB method for tuning weight priors during BNN training and presents experimental results demonstrating the accuracy and efficiency of the method in heteroscedastic and homoscedastic regression on real datasets. The inference task for training a BNN involves discovering the posterior distribution of weights given a model and dataset, which is approximated using a variational distribution to minimize the Kullback-Leibler divergence. The text discusses optimizing the evidence lower bound (ELBO) by minimizing the KL divergence between variational and true posterior distributions. Analytic results exist for Gaussian families, but for non-linear neural networks, Monte Carlo (MC) approximations with finite sample size are used. The goal is to develop an accurate approximation for the expectation calculation, stabilizing Bayesian neural network (BNN) training by removing stochasticity from MC sampling. The architecture of computing E w\u223cq [log p(D|w)] for a neural network involves activation propagation and evaluation of a log-likelihood function. In a deterministic framework, the final layer activation distribution is approximated to compute the likelihood. The l th layer maps activations using weights and biases as random variables. The weights and biases of each layer in a neural network are assumed to be independent. The linear transform is applied after the nonlinearity to emphasize the construction of activations. By appealing to the central limit theorem, elements of the activations become normally distributed. The weights and biases of each layer in a neural network are assumed to be independent. The linear transform is applied after the nonlinearity to emphasize the construction of activations. By appealing to the central limit theorem, elements of the activations become normally distributed. Complicated distribution for h j induced by f 1 is approximately valid even with weak correlations between elements of h during training. A Gaussian form is adopted for a, and the first and second moments are computed approximately. The derivation is presented with more details in appendix A. Modeling W, b, and h as independent random variables, the moments of h are the difficult terms when choosing a variational family with analytic forms for weight means and covariances. The approximation for Heaviside and ReLU non-linearities involves closed form solutions for integrals and asymptotes, which can be used as a first approximation. The residual decays to zero far from the origin in the plane, allowing for a decaying model. The residual decays to zero far from the origin in the plane, modeled by a decaying function. Polynomial coefficients are calculated to match moments of the resulting Gaussian. The improved approximation propagates moments through the network to compute mean and covariances. Deep learning frameworks supporting special functions will support backpropagation through the presented expressions. The presented approximation is empirically verified in section 3.2 for computing log-likelihood and posterior predictive distribution in regression and classification tasks. Assumptions include a form of CLT for hidden units during training, quadratic truncation of Q, and weak correlation between layers. Evidence is provided by training a small ReLU network for 1D heteroscedastic regression on a toy dataset. The training objective involves regression on a toy dataset using a diagonal Gaussian variational family for weights. Comparisons between deterministic approximation and MC evaluation show qualitative excellence. The study explores the trade-off between compute and memory for accuracy in DVI. A method called \"diagonal-DVI\" (dDVI) is introduced to address limitations with large hidden size networks. Surprisingly, dDVI maintains strong test performance compared to DVI across various datasets. Time comparisons for activation propagation on a Tesla V100 GPU show that DVI runtimes are similar to MCVI with S = 300, while dDVI runtimes are equivalent to S = 1. To train BNNs, a function L maps final layer activations to expected log-likelihood. The expected log-likelihood over q(w) can be rewritten as an expectation over q(a L). Closed forms are derived for specific tasks, focusing on regression. Parameters of a Gaussian noise model are considered, with uncertain parameters computed following section 3.1. Gaussian forms for p(y|a L) and q(a L) are inserted into equations. The closed form expression for the ELBO reconstruction term is derived by inserting Gaussian forms for p(y|a L) and q(a L) into equations. This heteroscedastic model can be made homoscedastic by setting certain parameters. The derivations in equation 8 enable the implementation of the closed form approximation for training a network. Additionally, a closed form approximation for the predictive distribution is computed for test-time predictions. Methods for deterministic approximation of the reconstruction term in the ELBO are discussed, followed by considerations for the KL term with a Gaussian prior. In variational Bayes for neural network parameters, the choice of prior variance parameters is crucial. A hierarchical prior approach is proposed to address sensitivity issues, using a conditional diagonal Gaussian prior and variational distribution on the weights. The hierarchical prior involves an inverse gamma distribution on s as the conjugate prior to the elements of the diagonal Gaussian variance. The text discusses the use of an empirical Bayes approach to optimize the setting of variance parameters in a hierarchical prior for neural network weights. The approach aims to find the optimal value for s to produce the tightest Evidence Lower Bound (ELBO). The text introduces the use of empirical Bayes to optimize variational parameters for neural network weights. It demonstrates that while the approach works well, it approximates the full Bayesian solution and may fail with too many degrees of freedom. This contrasts with the previous discussion on optimizing variance parameters for the hierarchical prior. David MacKay demonstrated the benefits of a Bayesian approach to neural network learning, including model flexibility, accurate predictive uncertainty, and robust learning. Early excitement led to the development of variational Bayes methods by Hinton & van Camp and MacKay. The development of Bayesian neural networks was motivated by different perspectives, including MDL compression and variational free energy minimization. Efficient gradient-based Monte Carlo methods were also introduced, leading to the rebirth of Bayesian neural networks with Monte Carlo variational inference. Since 2015, the VB approach has shown gains in predictive performance on real-world tasks. Since 2015, the VB approach to Bayesian neural networks has gained predictive performance on real-world tasks. Key research focuses include reducing variance in MCVI and exploring more complex variational families like Matrix Gaussian, multiplicative, and hierarchical posteriors. Methods like deterministic moment approximation and empirical Bayes estimation can be extended to these richer families. The choice of priors in Bayesian neural networks remains an open issue, with hierarchical priors for feedforward networks being investigated previously. The choice of priors in Bayesian neural networks, particularly hierarchical priors for feedforward networks, has been explored in prior research. Various approaches have been proposed, including a \"cheap and cheerful\" heuristic by MacKay (1995a) and a hierarchical prior with closed-form approximations by Barber & Bishop (1998). Graves (2011) also utilized hierarchical Gaussian priors. Our approach aims to provide a rigorous method that accurately approximates the Bayesian approach with sufficient data. Alternative to variational Bayes, probabilistic backpropagation (PBP) applies approximate inference using assumed density filtering to refine a Gaussian posterior approximation. PBP has been extended to classification and richer posterior families. Our moment approximation improves the inference accuracy of PBP and is more computationally efficient due to handling minibatches of data. Our method for approximate posterior parameter inference using SG-MCMC methods is computationally efficient, but important theoretical questions regarding approximation guarantees for practical computational budgets remain. Despite progress, SG-MCMC methods are still computationally inefficient, especially in evaluating data. The curr_chunk discusses the computational inefficiency of evaluating the posterior predictive due to the need for an ensemble of models. Various methods have been proposed to provide \"cheap\" approximations to the Bayes posterior, such as interpreting dropout as variational inference and using Bootstrap posteriors for robust inference. Implementing deterministic variational inference (DVI) is suggested as a method to train small ReLU networks on UCI regression datasets. The experiments on UCI regression datasets aim to improve model performance by eliminating gradient variance and tuning the prior. Results show that DVI outperforms MCVI by up to 0.35 nats per data point on some datasets. The computationally efficient diagonal-DVI (dDVI) retains much of the performance of DVI, outperforming MCVI by up to 0.35 nats per data point on some datasets. The heteroscedastic model consistently delivers better results than the homoscedastic model (hoDVI) with no overfitting issues. Empirical Bayes method consistently finds priors that produce competitive or improved test log-likelihood relative to manual tuning. The text discusses the advantages of using a deterministic approach and empirical Bayes hyperparameter update in variational inference for neural networks. These innovations make variational Bayes competitive for Bayesian inference in neural heteroscedastic regression models, improving predictive uncertainty estimates and sequential decision making. In future work, the innovations proposed in this paper can be applied to sequential decision making and continual learning applications. Approximate Bayesian inference must be run as an inner loop of a larger algorithm, requiring a robust and automated version of BNN training. The innovations pave the way for automated and robust deployment of BBNs without the need for an expert in-the-loop. In the context of automated deployment of BBNs, the text discusses the evaluation of equation terms using diagonal matrices and analytical methods for activation functions like Heaviside and ReLU. It introduces dimensionless variables and expressions for covariance calculations. In the context of automated deployment of BBNs, the text discusses the evaluation of equation terms using diagonal matrices and analytical methods for activation functions like Heaviside and ReLU. It introduces dimensionless variables and expressions for covariance calculations. In equation 16, dimensionless variables are introduced for quadratic form P, leading to rewriting the equation in terms of a dimensionless integral I using a scale factor S jl. The normalization constant Z is evaluated by integrating over e \u2212P/2 and is explicitly written as Z = 2\u03c0\u03c1 jl. The integral I is represented as the shaded area under the Gaussian for the Heaviside activation, with A and Q evaluated for asymptotic behavior. In the context of automated deployment of BBNs, the evaluation of equation terms involves diagonal matrices and analytical methods for activation functions like Heaviside and ReLU. The integral in question does not have a closed form, but for \u00b5 j \u2192 \u221e, a vanishing weight appears under the Gaussian in the upper-right quadrant. The asymptote of the integral in this limit is derived by marginalizing out \u03b7 j from the bivariate Gaussian. Symmetry allows for the determination of limits, and a symmetrized form is presented to satisfy all required limits. The correction factor is computed by evaluating derivatives of (I \u2212 A) at the origin up to second order to match the moments of e \u2212Q for quadratic Q. Performing a change of variables in the integral in polar coordinates over the region H with the Heaviside function, we find the angle \u03c8 from the coordinate transform. Evaluating the first and second derivatives, we obtain the expressions for I and Q, respectively. The symmetry of (I - A) leads to no linear term in \u00b5, and further derivatives are calculated using identities for functions. The text discusses obtaining derivatives of the residual (I - A) up to second order and proposing a correction factor. Coefficients are found by matching derivatives at \u00b5 = 0, leading to an expression seen in table 1. The process of computing a full 2-dimensional asymptote and constructing the correction factor e \u2212Q is also detailed. The correction factor e \u2212Q is computed following procedures similar to the Heaviside non-linearity. Mathematica is used for intermediate calculations, and the final result is shown in table 1. Derivations of expressions from section 3.2 are provided, along with justifications for rewriting the ELBO reconstruction term. Expected log-likelihoods and posterior predictive distributions for Gaussian regression and classification are derived in subsequent sections. The reconstruction term for data point (x, y) is rewritten in terms of a L. The reconstruction term for a data point (x, y) is rewritten in terms of a L, allowing for the integral over w to be performed, leading to an equivalence established in equation 7. Equation 8 is derived from the main text by inserting Gaussian forms for p(y|a L ) and q(a L ), completing the square, and performing integrals to obtain the final result. Equation 9 is also derived by calculating moments of the predictive distribution under an approximation. The text discusses deriving an expansion for the expected log-likelihood in multivariate classification using the second-order Delta method. By assuming normality of the predictive distribution and utilizing the Delta method, the expansion is derived, enabling the training of a classifier. The expansion for the posterior predictive distribution is derived using the second-order Delta method, enabling classifier training. Good results are obtained using approximations or a lightweight MC approximation for mapping a L to (log)p. The focus is on demonstrating the benefits of the moment propagation method in regression examples without additional likelihood function approximation. In the regime of deep, narrow networks, the Gaussian approximation breaks down for small hidden dimensions, leading to errors accumulating in deep networks. Empirical exploration with networks of different sizes shows that the CLT-based approximation works well for larger hidden units but poorly for smaller ones. This suggests that optimization of neural networks is more effective in high-dimensional settings with at least a few tens of hidden units. In high-dimensional settings with at least a few tens of hidden units, empirical observations suggest that the Gaussian approximation is applicable in relevant architectures. Adding skip connections to combat gradient issues in deep networks, we derive results for a deterministic BNN with skip connections in a simple layer. The moment propagation expressions for this layer involve computations that can be performed analytically, leading to the implementation of a 5-layer network. Using results from previous work, a 5-layer, 25-unit network with skip connections was implemented and validated against Monte Carlo simulations. Test log-likelihoods showed competitive or superior performance compared to state-of-the-art techniques on 9 UCI datasets. An ablation study highlighted the contributions of the deterministic approximation and empirical Bayes prior in different inference schemes. The study used a fixed zero-mean Gaussian prior during training and conducted separate runs to tune the prior variance, achieving the best performance. The empirical Bayes approach offers computational advantages by eliminating MC sampling and addressing high variance gradient estimates in training BNNs. Alternative methods to reduce variance were considered, with the ablation study showing the impact of different inference schemes. The study conducted an ablation study on combinations of DVI and MCVI with EB or a fixed prior, showing benefits of MCVI with the reparameterization trick over vanilla MCVI. The reparameterization trick reduces variance in gradient estimates during backpropagation and allows for efficient linear transforms. The study compared rMCVI and MCVI in terms of gradient variance and runtime, showing that rMCVI is not fundamentally different from MCVI. The efficiency of rMCVI is highlighted by the use of large matrix multiplies compared to smaller batched matrix multiplies. The comparison between DVI and MCVI inference algorithms in terms of test log-likelihood is shown in table 3. Despite using only 10 samples and no learning rate schedule, MCVI consistently performs worse than DVI in achieving test log-likelihood due to increased variance."
}