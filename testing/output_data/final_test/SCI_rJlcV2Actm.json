{
    "title": "rJlcV2Actm",
    "content": "In this paper, the \"memory-augmented hierarchical-classification network (MahiNet)\" is proposed for the many-class few-shot (MCFS) problem in supervised learning and meta-learning scenarios. MahiNet utilizes a convolutional neural network (CNN) for feature extraction and integrates a memory-augmented attention module with a multi-layer perceptron (MLP) to predict probabilities over coarse and fine classes, addressing the challenge of distinguishing between many classes with limited training samples. The \"MahiNet\" model combines a convolutional neural network (CNN) with a memory-augmented attention module and a multi-layer perceptron (MLP) to address the many-class few-shot (MCFS) problem in supervised learning and meta-learning scenarios. It outperforms state-of-the-art models on MCFS classification tasks and introduces benchmark datasets specifically for this problem. The representation power of deep neural networks has significantly improved in recent years, enabling more complex tasks to be tackled with the availability of larger training datasets. The scarcity of annotated data has become a bottleneck for training powerful DNNs, especially in many-class few-shot scenarios. This problem is common in various applications like image search and robot navigation. Previous works have shown DNN's power with many-class many-shot data, but performance degrades when each class has only a few samples available. In few-shot scenarios, model performance degrades when classes have limited samples for training. Acquiring samples of rare species is challenging and costly. Meta-learning approaches have been proposed to address this issue, categorizing into \"learning to optimize\" and metric learning methods. These methods aim to train a meta-learner that can generalize to different tasks, adapting the optimizer or using model-agnostic meta-learning (MAML) techniques. The MCFS problem involves class hierarchy information with coarse and fine classes, aiming to train a classifier for few-shot data using meta-learning approaches like RNN meta-learner and MAML methods. Some methods focus on learning similarity metrics or support sets for building KNN classifiers. Other approaches address few-shot learning problems through data without meta-learning techniques. Some existing few-shot learning approaches struggle when the number of classes grows to hundreds or thousands, as samples per class become insufficient. However, class hierarchy information can help by revealing relationships among fine classes. Coarse class labels can be used to train a reliable coarse classifier, narrowing down candidates for fine classes. In this paper, the use of class hierarchy in solving the \"many-class few-shot (MCFS)\" problem is explored. A DNN architecture called \"memory-augmented hierarchical-classification networks (MahiNet)\" is developed for both traditional supervised learning and meta-learning. MahiNet utilizes a CNN, specifically ResNet, to extract features from images and includes coarse-class and fine-class classifiers to produce final results. The MahiNet architecture utilizes both coarse-class and fine-class classifiers to improve few-shot learning by leveraging class hierarchy information. The classifiers work together to narrow down candidates and provide multiple attributes per coarse class, mitigating the \"many class\" problem. MahiNet employs MLP for coarse classification and KNN for few-shot situations, achieving improvements where previous methods have failed. In MahiNet architecture, MLP is used for coarse classification and KNN for fine classification in both many-shot and few-shot scenarios. An attention module is employed to make KNN more adaptive to classes with few-shot data, and a re-writable memory is used to store and update KNN support set during training. Maintaining a small memory is crucial in supervised learning to avoid computational expenses. In meta-learning, the attention module acts as a meta-learner for a universal similarity metric across tasks. ImageNet and Omniglot subsets are extracted for benchmarking MahiNet, which outperforms ResNet in supervised learning and shows promise in meta-learning scenarios. The text discusses a memory-augmented hierarchical-classification network for supervised learning and meta-learning tasks. It focuses on fine-class classification using class hierarchy information, with the goal of formulating a supervised learning problem. The model parameters are denoted by \u0398. In supervised learning, model parameters are optimized through empirical risk minimization (ERM). In contrast, meta-learning aims to maximize prediction likelihood across tasks sampled from a task distribution. Fine classes are sampled from a distribution for each task, and coarse class information is leveraged through separate model parameters for fine and coarse classifiers. The MahiNet model combines predictions from fine and coarse classes using MLP and attention-based KNN classifiers. KNN classifier utilizes a learnable similarity metric and support set stored in memory for each class. The model aims to optimize the similarity metric and support set for accurate predictions. The MahiNet model combines predictions from fine and coarse classes using MLP and attention-based KNN classifiers. The memory stores feature vectors per class, updated during training. Clustering is applied to samples in a cache to update memory slots. DNN model logit values are used to compute probabilities for classes. MahiNet is a universal model that uses a CNN to extract features and two modules for coarse-class and fine-class predictions. The model includes MLP and attention-based KNN classifiers for better performance based on data availability. The MahiNet model utilizes MLP for coarse prediction and KNN for fine prediction, with an attention-based KNN classifier incorporating a re-writable memory for highly representative support set. The method combines ideas from matching networks and prototypical networks, relying on augmented memory for NN retrieval. MahiNet utilizes MLP for coarse prediction and KNN for fine prediction, with a re-writable memory for highly representative support set. It combines ideas from matching networks and prototypical networks, allowing multiple prototypes per class to prevent confusion in many-class scenarios. The model can also be extended to \"life-long learning\" with memory updates and utility rate adjustments. MahiNet utilizes MLP for coarse prediction and KNN for fine prediction, with a re-writable memory for a highly representative support set. The model trains an attention module to compute similarity for the KNN classifier, using dot product attention for supervised learning and Euclidean distance-based attention for meta-learning. MahiNet uses an attention module for the KNN classifier in supervised learning, with a focus on Euclidean distance for meta-learning. It maintains a support set in memory for each class and calculates class probabilities using nearest neighbors and attention scores. The predicted class probability is obtained by applying a softmax function to the attention scores over all classes. In meta learning, a memory M stores training samples for KNN classifier tasks with C classes and m samples per class. A budget hyper-parameter m limits feature vectors stored per class, with a memory update mechanism for diversity. In meta learning, a memory M stores training samples for KNN classifier tasks with C classes and m samples per class. A budget hyper-parameter m limits feature vectors stored per class, with a memory update mechanism for diversity. The model can choose to forget or merge feature vectors, select new important ones into memory, and update fine-class logits and coarse-class logits during training. Experiments show that a small memory can lead to significant improvement with negligible time cost. In meta learning, a memory M stores training samples for KNN classifier tasks with C classes and m samples per class. A budget hyper-parameter m limits feature vectors stored per class, with a memory update mechanism for diversity. The model can choose to forget or merge feature vectors, select new important ones into memory, and update fine-class logits and coarse-class logits during training. Experiments show that a small memory can lead to significant improvement with negligible time cost. In the classifier, feature vectors are merged with memory slots using a convex combination, and a cache stores candidates for the next epoch. Utility rates of feature vectors in memory are recorded and updated, with clustering of feature vectors per class in the cache to obtain cluster centroids for memory update in the next epoch. In supervised learning and meta learning, MahiNet uses a combination of MLP and KNN classifiers to make predictions. The classifiers are combined by summing up their logits for each class and using a softmax function to generate class probabilities. MahiNet is trained for supervised learning by solving ERM problems and for meta-learning by solving specific equations. The logits used in the ERM problems are obtained by summing up the logits produced by the corresponding combination of classifiers. MahiNet utilizes a combination of classifiers for supervised learning and meta-learning. In supervised learning, the model is pre-trained using backpropagation to minimize cross-entropy loss on coarse and fine classes, followed by fine-tuning with memory updates. For meta-learning, MahiNet stores features from the support set in memory for the KNN classifier. Training procedures for both stages are detailed in algorithms 1 and 2, involving sampling fine classes, support sets, and query sets. MahiNet utilizes classifiers for supervised learning and meta-learning. The model is pre-trained using backpropagation to minimize cross-entropy loss on coarse and fine classes, followed by fine-tuning with memory updates. MahiNet stores features from the support set in memory for the KNN classifier. Two benchmark datasets, mcfsImageNet & mcfsOmniglot, are proposed for the MCFS Problem. Experiments on mcfsImageNet use ResNet18 for the backbone CNN. The attention module in MahiNet consists of two fully connected layers with group normalization and a residual connection. MahiNet outperforms specialized models like ResNet18 and prototypical networks in the MCFS scenario. An ablation study removing the KNN classifier from MahiNet shows its importance in achieving good performance. MahiNet outperforms ResNet18 and prototypical networks in the MCFS scenario. Ablation study shows the importance of the KNN classifier for performance improvement. Average clustering time per epoch is 30s, memory update time is 0.02s. Test accuracy is reported over 600 test episodes with 95% confidence intervals. In 50-way experiments, MahiNet outperforms Relation Net, ResNet18, and Prototypical Net. Different memory types (Mem-1, Mem-2, Mem-3) are utilized in MahiNet, with \"Attention\" and \"Hierarchy\" components contributing to improved performance. Class hierarchy information and combining Mem-1 and Mem-2 show steady performance gains across tasks. In experiments on mcfsImageNet, MahiNet with different memory types (Mem-1, Mem-2, Mem-3) shows improved performance. Attention learned with class hierarchy in MCFS problem is crucial due to insufficient data for reliable similarity metric training. ResNet18 is not used on mcfsOmniglot due to dataset size, instead, four convolutional layers are used as backbone CNN. Ablation study on MahiNet includes analysis with/without hierarchy and different memory types. Experiments show that MahiNet with different memory types outperforms others, with \"Mem-3\" being the best. The use of the class hierarchy improves performance, especially in making accurate predictions. Class hierarchy brings stable improvement in both small-scale and large-scale datasets. ImageNet is not suitable for testing MCFS learning methods due to its hierarchical structure and few-shot per class criteria. miniImageNet is commonly used for meta-learning benchmarks. miniImageNet BID22 is a benchmark dataset for few-shot learning tasks in meta-learning. To address the limitations of miniImageNet, a new dataset called \"mcfsImageNet\" was created specifically for testing MCFS learning methods. mcfsImageNet has a class hierarchy where each fine class belongs to one coarse class, making it about 5\u00d7 larger than miniImageNet. Comparisons with other benchmark datasets are provided in TAB2, and more details on the class hierarchies are available in Appendix F. mcfsImageNet is a dataset designed for MCFS learning methods, with 754 fine classes and 77 coarse classes. Each fine class has around 185 images for training and testing. The dataset avoids selecting broad coarse classes and includes new classes in the test set. For the supervised learning setup, Omniglot dataset was re-split to meet MCFS requirements. ResNet18 BID5 was used as the backbone CNN. The attention module included transformation functions g and h with group normalization BID25. Memory size was set to m = 12 and clusters to r = 3 for better performance. Batch normalization BID7 was applied after each convolution. Cross entropy loss was used during pre-training, and parameters were fixed during fine-tuning. During fine-tuning, parameters are fixed to ensure stability. SGD with mini-batch size 128 and cosine learning rate scheduler are used. Model is trained for 100 epochs during pre-training and 90 epochs for fine-tuning. Meta learning setup involves using the same backbone CNN, g, and h as in supervised learning. Model is trained with Adam, mini-batch size 128, weight decay 0.0001, and momentum 0.9 for 25k iterations. Class hierarchy objective function is the sum of softmax with cross entropy losses. Few-shot learning has a history before deep learning, with generative models used for one-shot learning. Recent approaches utilize generative models for specific prior knowledge. Meta-learning has been applied to solve few-shot learning problems, with recent significant improvements. Meta-learning involves using a dataset of characters for training. Recent advancements in few-shot learning include the introduction of challenging datasets like BID16 and BID22, as well as the exploration of RNN and attention-based methods. New approaches such as BID20, BID4, and BID14 have been proposed, each leveraging different techniques such as metric learning, second-order optimization, and temporal convolution. In contrast, our model incorporates class hierarchy information, improving accuracy in relation networks by over 1%. This demonstrates the advantage of utilizing class hierarchy in various models for both supervised learning and meta-learning scenarios. The relation network in high way settings can get stuck in suboptimal solutions, with training loss and accuracy remaining constant after 100 iterations. Visualization of feature vectors in memory slots shows representative and diverse selections. The relation network in highway settings can get stuck in suboptimal solutions, with training loss and accuracy remaining constant after 100 iterations. Visualization of feature vectors in memory slots shows representative and diverse selections. MahiNet requires only 7.2% of memory compared to storing the whole training set, with negligible performance improvement when increasing memory size. The hierarchy of coarse and fine classes is shown in a dictionary format."
}