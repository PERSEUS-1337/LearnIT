{
    "title": "Hkfmn5n6W",
    "content": "Statistical mechanics results suggest that high-error local minima are rare in high dimensions. Previous works required heavily modified models or training methods to prove low error guarantees for Multilayer Neural Networks. Results: A study on Multilayer Neural Networks with one hidden layer shows that the volume of sub-optimal local minima decreases exponentially compared to global minima as the number of data points increases. The study also demonstrates 0% binary classification training error on CIFAR with only 16 hidden neurons. The mystery of why over-parameterized Multilayer Neural Networks (MNNs) achieve low training error without explicit regularization or why Stochastic Gradient Descent (SGD) often works well in non-convex loss functions remains unclear. Understanding the behavior of errors in Multilayer Neural Networks (MNNs) and Stochastic Gradient Descent (SGD) is crucial, especially in cases where training error hinders performance improvement. Quantifying the probability of converging to a local minimum based on error levels is challenging, as it requires characterizing the basins of attraction for all local minima. Previous works have suggested that local minima with high error tend to diminish with high probability. With high probability, local minima with high error diminish exponentially with the number of parameters in Multilayer Neural Networks (MNNs). Realistic assumptions are needed to guarantee convergence to global minima, making it a necessary first step. The issue was highlighted at the Conference of Learning Theory (COLT) 2015. However, using realistic MNN architectures is crucial to avoid the problem becoming \"too easy\". For instance, extremely wide MNNs can easily achieve zero training error if the last hidden layer has more neurons than training samples. In this paper, the focus is on understanding the properties of local and global minima in Multilayer Neural Networks (MNNs) with at least two trained weight layers. The study aims to determine when sub-optimal local minima become rare compared to global minima in MNNs with a practical number of parameters. The research considers the training of the last two weight layers in an over-parameterized MNN to address issues of overfitting and take advantage of the non-linear nature of MNNs. The study focuses on Multilayer Neural Networks (MNNs) with a single hidden layer and piecewise linear units, optimized using Mean Square Error (MSE) in a supervised binary classification task. It examines Differentiable Local Minima (DLMs) of the MSE and global minima, showing that the volume of sub-optimal DLMs is exponentially smaller compared to global minima when datapoints are sampled from a standard normal distribution. The study focuses on Multilayer Neural Networks (MNNs) with a single hidden layer and piecewise linear units, optimized using Mean Square Error (MSE) in a supervised binary classification task. It improves upon previous results by showing that a wide hidden layer is not necessary to avoid sub-optimal local minima. Numerical validation in section 5 demonstrates low training error with a number of parameters close to N. Convergence to non-differentiable critical points is rare, and potential extensions of the results are discussed in section 6. The study focuses on Multilayer Neural Networks (MNNs) with a single hidden layer and piecewise linear units, optimized using Mean Square Error (MSE) in a supervised binary classification task. It improves upon previous results by showing that a wide hidden layer is not necessary to avoid sub-optimal local minima. Numerical validation in section 5 demonstrates low training error with a number of parameters close to N. Convergence to non-differentiable critical points is rare, and potential extensions of the results are discussed in section 6. Assumptions are made regarding the normal distribution of inputs and the use of asymptotic limits to simplify proofs and final results. The study focuses on Multilayer Neural Networks (MNNs) with a single hidden layer and piecewise linear units, optimized using Mean Square Error (MSE) in a supervised binary classification task. Assumptions are made regarding the normal distribution of inputs and the use of asymptotic limits to simplify proofs and final results. The results show that a wide hidden layer is not necessary to avoid sub-optimal local minima, with low training error demonstrated with a number of parameters close to N. Previous work on general low error guarantees for MNNs has been limited. The study discusses error guarantees for Multilayer Neural Networks (MNNs) and various modifications to the model and learning methods to achieve low error rates. Different approaches such as using random variables for units, weights, and error residuals, as well as unconventional units and tensor methods during training, have been explored to improve performance. The study explores error guarantees for Multilayer Neural Networks (MNNs) and alternative training methods. Previous works made unrealistic assumptions about target functions, while this study makes no assumptions. The study examines Multilayer Neural Networks (MNNs) with a single hidden layer and a scalar output, trained on a finite dataset using mean square error loss. The assumptions in this paper are applicable in some situations where a MNN trained using SGD might be useful. The study focuses on Multilayer Neural Networks (MNNs) with a single hidden layer and scalar output, trained on a finite dataset using mean square error loss. The variables e, MSE, MCE, and related variables depend on W, z, X, y, and \u03c1. Additional notation includes defining g(x)<h(x) and denoting \"M \u223c N\" for matrices with entries drawn independently from a standard normal distribution. MNNs are typically trained by minimizing the loss over the training set. The study focuses on Multilayer Neural Networks (MNNs) trained using Stochastic Gradient Descent (SGD) to minimize mean square error loss. The focus is on differentiable local minima (DLMs) of the loss, showing that sub-optimal DLMs are rare compared to global minima. Non-differentiable critical points are numerically rare and left for future work. The text formalizes necessary notions for measuring the amount of DLMs in the over-parameterized regime. The text discusses differentiable local minima (DLMs) in Multilayer Neural Networks (MNNs) trained with Stochastic Gradient Descent (SGD) to minimize mean square error loss. It highlights that sub-optimal DLMs are rare compared to global minima, and non-differentiable critical points are left for future research. The study formalizes notions for measuring the amount of DLMs in the over-parameterized regime, focusing on partitioning DLMs into equivalence groups based on the differentiable regions of the mean square error. The text discusses the necessary conditions and stability of differentiable local minima (DLMs) in Multilayer Neural Networks (MNNs) trained with Stochastic Gradient Descent (SGD) to minimize mean square error loss. It introduces constraints on the residual error in the over-parameterized regime and defines the concept of angular volume to measure the probability of falling into differentiable regions. The text discusses the stability of differentiable local minima (DLMs) in Multilayer Neural Networks (MNNs) trained with Stochastic Gradient Descent (SGD) to minimize mean square error loss. It introduces constraints on the residual error in the over-parameterized regime and defines the concept of angular volume to measure the probability of falling into differentiable regions. The angular volume of sub-optimal DLMs and global minima is compared using specific definitions and assumptions. The text discusses the stability of differentiable local minima (DLMs) in Multilayer Neural Networks (MNNs) trained with Stochastic Gradient Descent (SGD) to minimize mean square error loss. It introduces constraints on the residual error in the over-parameterized regime and defines the concept of angular volume to measure the probability of falling into differentiable regions. In section 8, an upper bound in expectation is proven for the angular volume of sub-optimal DLMs, showing that it is exponentially vanishing in N. The proof idea involves showing that the matrix A = a (WX) has a low rank with exponentially low probability. Additionally, in appendix section 9, the volume of sub-optimal DLMs is compared with that of global minima. In appendix section 9, it is shown that global minima exist for deep residual networks with N log N parameters, achieving zero error with just one hidden layer and 2N parameters. This construction overfits to the data realization, leading to zero training error but may not be a \"good\" solution in practice. The text discusses the need for additional assumptions on the data to achieve good generalization in machine learning. It mentions the assumption of a \"realizable\" problem where a small \"solution MNN\" exists with low error. The angular volume of global minima is then discussed, with a theorem proving a lower bound on it under certain assumptions. The main result of the paper is the angular volume ratio, which is lower bounded when certain assumptions are met. Theorem 10 shows that with high probability, the angular volume of sub-optimal DLMs decreases exponentially in comparison to global minima as the number of parameters increases. This implies that regions in weight space containing sub-optimal DLMs with high error are limited with asymptotically mild over-parameterization. In weight space, regions with sub-optimal deep learning models have high error, but with asymptotically mild over-parameterization. Experiments were conducted on various datasets to validate these results numerically. Experiments were conducted on random data, MNIST, CIFAR10, and ImageNet-ILSVRC2012 using ReLU activations, binary classification, MSE loss, and MCE for error determination. Results showed error approaching zero when non-redundant parameters exceeded the number of samples. Additionally, simulations were done on MNIST, CIFAR, and ImageNet datasets with a single hidden layer MNN. In experiments with a single hidden layer MNN, the final error was zero or very low for MNIST and CIFAR datasets, and the inputs to hidden neurons converged to non-zero values on the Gaussian dataset, indicating convergence to differentiable critical points. The study focused on Differentiable Local Minima (DLMs) of Multilayer Neural Networks with LReLU nonlinearities. The study focused on Differentiable Local Minima (DLMs) of Multilayer Neural Networks (MNNs) with one hidden layer, scalar output, and LReLU nonlinearities. It was proven that the angular volume of sub-optimal DLMs is exponentially vanishing compared to global minima. The experiment involved training for 1000 epochs, decreasing the learning rate exponentially for another 1000 epochs, and repeating this process 30 times. The final absolute value of the minimal neural input was in the range of 10^-3 to 10^0, while the final Mean Squared Error (MSE) error was in the range of 10^-31 to 10^-7. These results suggest a mechanism for obtaining low training error in MNNs, but do not guarantee it. The study focused on sub-optimal Differentiable Local Minima (DLMs) in Multilayer Neural Networks (MNNs) and their exponentially vanishing angular volume compared to global minima. Future work may address issues by bounding differentiable regions and basin sizes, and avoiding convergence to saddle points without strictly negative eigenvalues. The study discusses the vanishing angular volume of sub-optimal DLMs in Multilayer Neural Networks. Non-differentiable critical points are also considered, where some neural inputs are exactly zero. By using a projection matrix to remove zero neural inputs, similar results can be obtained. The study explores the challenge of minimizing the number of non-differentiable directions in neural networks. Two related works have derived results using a specific condition, but an infinitesimal perturbation of the matrix may lead to undesirable outcomes. The study discusses minimizing non-differentiable directions in neural networks by deriving bounds using singular values of A\u2022X BID53. The derived bounds do not eliminate sub-optimal regions exponentially and require assumptions on activation kernel spectrum \u03b3 m that may not hold in practice. Extensions include relaxing the Gaussian assumption and exploring other loss functions, while challenging directions involve multi-output MNNs and combining training error results with generalization bounds. The appendix is divided into three parts: proving main theorems, technical results, and numerical details. It defines notation and known results for proofs, such as the indicator function, Kronecker's delta, and matrix identities. The curr_chunk discusses properties of matrices, Gaussian vectors, probability density functions, cumulative distribution functions, and the Markov Inequality. It also presents equations involving diagonal matrices and vector stacking. The curr_chunk discusses minimizing mean squared error (MSE) using matrix operations and perturbations. It explains how infinitesimal perturbations in the input can affect the MSE, and how to find local minimums by analyzing the gradient of the MSE with respect to the weights. The curr_chunk discusses the implications of perturbations on minimizing mean squared error (MSE) and finding local minimums. It highlights the conditions for decreasing MSE through perturbations and the relationship between differentiable local minima (DLM) and matrix operations. The expected angular volume of suboptimal differentiable local minima with high residual error is exponentially vanishing in N. The proof involves upper bounding the angular volume of differentiable regions with high residual error, showing that certain configurations lead to low rank matrices with exponentially low probability. The text discusses notation and definitions related to matrices and norms. It also mentions the case when \u03c1 = 0 and provides a lemma implying correct classification. The inequality N < e 0 holds for v = e, and assumption X \u223c N is applied. The text concludes with a lemma regarding the simultaneous behavior of A and S. The text discusses the probability of rank conditions for matrices, using the union bound over all possible ranks. It also mentions the relaxation of conditions and the application of Lemmas for correct classification. The location of S does not affect the probability, and assumptions are made for further analysis. The text discusses rank conditions for matrices with independent and identically distributed columns, using a standard random Gaussian matrix. Theorem is proven for the case \u03c1 = 0, and transition equations are modified accordingly. Corollary states that given assumptions 1-4, for any \u03b4 > 0, certain conditions hold. The text discusses rank conditions for matrices with independent and identically distributed columns using a standard random Gaussian matrix. Given assumptions 1-4, for any \u03b4 > 0, with probability 1 \u2212 \u03b4, the angular volume of sub-optimal DLMs with MCE > > 0 is exponentially vanishing in N. The existence of a solution (W * ,z * ) is proven by explicitly constructing it using LReLU non-linearity and MSE. The text discusses constructing a MNN with hidden neurons to achieve zero error, using LReLU non-linearity and MSE. By finding a vector \u0175 i, a MNN can be constructed without bias, achieving correct classification on all data points. The text discusses constructing a MNN with hidden neurons to achieve zero error using LReLU non-linearity and MSE. By finding a vector \u0175 i, a MNN can be constructed without bias, achieving correct classification on all data points. Additionally, setting DISPLAYFORM4 and defining DISPLAYFORM5 satisfies certain conditions. Theorem 19 restates assumptions 1-3, setting \u03b4=0 or d * 1 if assumption 5 holds. The angular volume of global minima is lower bounded with probability 1 \u2212 \u03b4. The angular volume of G is lower bounded using the region with a single global minimum. Labels are generated using a (X, y) -dependent MNN. If assumption 5 holds, W * and z * are independent from (X, y). Definition 20 defines X having an angular margin \u03b1 from W *. Definition 20 states that X has an angular margin \u03b1 from W* if all datapoints in X are at an angle of at least \u03b1 from all weight hyperplanes in W*. Lemmas 21, 22, and 23 are proven in appendix sections 13.2, 13.3, and 13.4 respectively. Lemma 23 shows that with a standard random Gaussian matrix X, we can find (X, y)-dependent matrices W* and z* as in Theorem 8. The probability of not having an angular margin is bounded as N approaches infinity. Lemma 21, 22, and 23 from appendix sections 13.2, 13.3, and 13.4 respectively show the relationship between X, y, W*, and angular margin \u03b1. Theorem 9 and Theorem 24 are proven using these lemmas, with calculations based on assumptions 1-3. The expectation of the angular volume ratio is calculated to prove Theorem 10, with Markov inequality used to derive results. The Markov inequality is used to derive results related to X, y, W*, and angular margin \u03b1. The Khatari-Rao product between matrices is defined, and specific conditions are examined to prove a theorem. The text discusses linear independence of matrices and the use of a matrix R to transform equations. It proves the linear independence of equations and concludes that |S| must be greater than dSd0 for the equations to hold true. The induction process involves Gaussian elimination of a system of equations, characterizing matrices X where elimination is impossible, and extending the process to eliminate variables. The result is a square matrix Y with coefficients dependent on specific variables. The dimension of the span of certain variables must be d0, and this holds almost everywhere with respect to the Lebesgue measure. The theorem discussed involves matrices and the dimension of their spans, showing that certain events have zero measure. The corollary is not essential for the main results. The necessity of a condition is proven using a counting argument involving matrix transformations. In subsection 12.3.3, Lemma 15 is proven using basic results from subsections 12.2.1 and 12.2.2. A hyperplane in d0 can separate a set of points into different dichotomies. The number of dichotomies is upper bounded based on the columns of matrix X. The inequality is proven for N \u2265 3 using a geometric series. A lemma involving binary and Gaussian matrices is also discussed. The proof involves direct calculation and the independence of the columns of matrix X. It defines a maximal set and uses the symmetry of the standard normal distribution. The expectation over X is taken, and the location of S does not affect the probability. The lemma states properties of random matrices X and W. In the proof, a matrix A is defined as a (WX) matrix with certain properties. The matrix \u00c3 is obtained by permuting rows and columns so that a full rank sub-matrix D is in the lower right block. This involves dividing X and W into corresponding block matrices. The existence of a matrix Q such that QC = Z and QD = B is also discussed. The matrix A is defined as a (WX) matrix with specific properties, and a full rank sub-matrix D is obtained by permuting rows and columns. A matrix Q exists such that QC = Z and QD = B. The union bound over all permutations from A to \u00c3 is used to compute the sum, leading to an upper bound independent of H. The log of P(rank(A) = r) is bounded by rd0(log(d1 - r) + log(k - r)) + r2 + 2r log 2. In this section, Lemma 16 will be proven, relying on elementary results from subsections 12.3.1 and 12.3.2. Functions \u03c6(x) and \u03a6(x) are defined as the probability density function and cumulative distribution function for a scalar standard normal random variable. Lemma 33 discusses a random Gaussian vector in RK with a covariance matrix \u03a3. In this section, Lemma 16 will be proven, relying on elementary results from subsections 12.3.1 and 12.3.2. Functions \u03c6(x) and \u03a6(x) are defined as the probability density function and cumulative distribution function for a scalar standard normal random variable. Lemma 33 discusses a random Gaussian vector in RK with a covariance matrix \u03a3. K \u03b8 > 0. Then, recalling \u03c8 (\u03b8) in eq. (12.17), we have log P (\u2200i : Proof. Note that we can write z = u+\u03b7, where u \u223c N 0, 1 \u2212 \u03b8K \u22121 I K , and \u03b7 \u223c N 0, \u03b8K \u22121 . Using this notation, we have where in (1) we changed the variable of integration to \u03be = \u03b8/ (K \u2212 \u03b8)\u03b7. We denote, for a fixed \u03b8, and \u03be 0 as its global maximum. Since q is twice differentiable, we can use Laplace's method to simplify eq. To find \u03be 0 , we differentiate q (\u03be) and equate to zero to obtain which implies This is a monotonically increasing function from 0 to \u221e in the range \u03be \u2265 0. Its inverse function can also be defined in that range. This implies that this equation has only one solution, \u03be 0 = g \u22121 (\u03b8). Since lim \u03be\u2192\u221e q (\u03be) = \u2212\u221e, this \u03be 0 is indeed the global maximum of q (\u03be). Substituting this solution into q (\u03be), we get. Next, we wish to select good values for \u03b8 and K, which minimize this bound for large (M, N, L, K). To minimize the function for large (M, N, L, K), good values for \u03b8 and K need to be selected. The first term decreases in K, while the second term increases. The function is minimized when both terms are equal. Numerical optimization results in specific values for \u03b8 and K. In the limit of N \u2192 \u221e and \u03b1 (N) \u2192 \u221e, with \u03b1 (N) < N, the function converges to a specific value. Lemma 39 states that for any vector y and x following a normal distribution, certain equations hold. The proof involves setting y to a specific value and using the beta function. Additionally, Lemma 40 discusses the independence of certain matrices and weight hyperplanes in the limit of N approaching infinity. Proof. To lower bound DISPLAYFORM4, we define the event that weight hyperplanes must have an angle of at least \u03b1 from the target hyperplanes. This implies that w i must be rotated in respect to w * i by an angle greater than \u03b1. Given X \u2208 M \u03b1 (W *), we have the equation \u03b3 1 / (1 + \u03b3 2) \u0175 1 max n<d0 x (n), where we make use of certain equations and properties of the vectors involved. To lower bound DISPLAYFORM4, we consider the event where weight hyperplanes must have an angle of at least \u03b1 from the target hyperplanes. This implies that w i must be rotated in respect to w * i by an angle greater than \u03b1. By using certain equations and properties of the vectors involved, we calculate each remaining probability term in eq. (13.11). The text discusses the distribution of Gaussian variables and the bounding of the minimal nonzero singular value. It also combines several equations to derive a final result for a specific parameter."
}