{
    "title": "H1Gnx2CqKQ",
    "content": "Adversaries in neural networks have gained attention for deceiving image classification models and crafting attacks for object detection tasks. Universal adversaries are not specific to object instances, image-independent, and transferable to different models. Two techniques, \\textit{piling-up} and \\textit{monochromatization}, are proposed to improve transferability. Adversarial examples can deceive machine learning models and pose security threats. These modified versions of original data retain the same semantic content but can lead to incorrect predictions. Existing studies focus on crafting efficient adversaries with constraints to preserve the original input's meaning. Existing studies on adversarial examples focus on designing effective methods to craft attacks, defense methods, and improving attack transferability between models. Recent work has also explored attacking object detectors, but most efforts have only targeted specific object instances. Few methods have attempted to attack multiple objects and images or verify transferability to another model. In this work, the goal is to create universal and transferable adversaries to deceive object detectors and hide objects. The adversaries should work for different objects, be image-independent, and capable of black-box attacks. An adversarial mask is crafted with a norm-value constraint to optimize fooling detectors. To create universal and transferable adversaries for object detectors, an adversarial mask is crafted with a norm-value constraint. Optimizing over the mask is challenging due to the risk of overfitting and sparse gradients. Techniques proposed include optimizing over a set of images, using small patches to form the mask, and crafting monochromatic masks for translation invariance. Our methods craft a universal adversarial mask that can deceive object detectors independently at object-level, image-level, and model-level. The mask can hide up to 80% of objects from YOLO V3 BID25 and over 40% from Faster-RCNN BID27 in a black-box setting. The generated patterns are coarse-grained and have a generic appearance, contributing to good transferability. The proposed techniques can generate masks with generic patterns that deceive object detectors independently at different levels. Previous works focused on perturbing images for image classification models, but few addressed object detectors. The crafted perturbations aim to deceive deep learning models by mixing imperceptible perturbations with images. In real-world applications, attackers often lack knowledge about target models, termed as black-box attacks. Transferability between models is key for black-box methods, with ensemble attacks and gradient similarity analysis proposed to improve generalization capacity among models. Generating image-independent adversaries is also explored in related works. BID1 demonstrates generating image-independent adversaries for image classification by optimizing an adversarial patch. Attack methods on object detectors involve stickers or perturbation masks to interfere with classification. These methods are specific to designated object instances, requiring crafting adversaries one-by-one. BID20 explores transferability but with a low success rate. Recently, BID28 showed that feature inference can lead to non-local adversarial effects in object detectors, where generic objects transplanted randomly can distort detection results even at a distance. This phenomenon proves the fragility and sensitivity of object detectors, as features from areas unrelated to the object of interest impact detector behavior both inside and outside the region-of-interest. The probing methods used in BID28 may not be practical attack strategies, as they rely on random search and results are architecture-dependent. In this research, the focus is on object detection using YOLOv3 and exploring how adversaries crafted on YOLOv3 can transfer to Faster-RCNN. The study aims to hide objects by modifying their surroundings systematically, extending from previous work. Object detection involves localizing objects and recognizing their categories, with region-proposal based methods like RCNN and unified methods like SSD and YOLO being prominent. The experiment with YOLOv3 shows its speed and state-of-the-art performance, making it a suitable choice for the study. YOLOv3 performs object detection by spotting and classifying objects in input images using a backbone network and classifiers. It utilizes Non-Maximal Suppression for final results and has 3 classifiers for objects of different sizes. Faster-RCNN incorporates a Region-Proposal Network for detection proposals. In Faster-RCNN, object detection relies on a Region-Proposal Network (RPN) for detection proposals through bounding boxes. Adversarial masks are generated to conceal objects by minimizing the model's objectiveness score. The mask is modeled as a 416x416x3 parameter and optimized to transfer to other settings effectively. The model's score is optimized by setting the minimization target as the average log-likelihood of top-200 anchors in YOLO V3. Online Hard Positive Mining (OHPM) is used to balance different categories and avoid overwhelming negative anchors. Data augmentation is applied to improve mask robustness during optimization. In practice, the norm-value constraint is applied by using a tanh function on the parametrized mask and multiplying it with a distortion rate. Training continues until performance on a test-set does not improve further. The full-mask setting serves as a baseline for newly proposed techniques. A smaller mask is parametrized to encode translation-invariant adversarial masks. The adversarial mask is obtained by piling up smaller masks in a grid-aligned way. During training, the mask is applied to input images by addition followed by clipping. Gradients are averaged over the grid cells. Monochromatic adversarial masks require fewer parameters and are less conspicuous. Green bounding boxes represent objects detected before and after the attack, while red ones stand out. The monochromatic adversarial masks are implemented by setting the values of the three color channels as the same. Combining technique 1 and technique 2 results in simplified and stylish adversarial patterns with fewer parameters. Experiments are designed to evaluate the effectiveness of the trained adversarial masks on YOLO V3 and Faster-RCNN, as well as the transfer to Faster-RCNN. The experiments show that all three methods can hide objects from detectors effectively. The proposed techniques improve transferability significantly, generating adversaries with repetitive patterns. Detection results are illustrated in FIG1 using YOLO V3 and Faster-RCNN models pretrained on COCO Detection dataset. The adversaries are constructed using mini-batch SGD with specific parameters. The main evaluation metric is the average number of detected objects per image. A derived metric called Detection Rate is used for comparison, measuring the proportion of objects still detected when attacked. The lower the Detection Rate, the better. The adversaries are trained with different values of distortion, and a curve is plotted to show its dynamics. Performance evaluations are shown in FIG2. Experiments are conducted for black box attacks by transferring adversaries from YOLO V3 to Faster-RCNN. Results show that the full-mask attack, despite having more parameters, achieves a slightly lower success rate in concealing objects. This could be due to the difficulty in training the full-mask attack. The colorful pile-up setting outperforms other methods significantly, with more parameters but still easy to train. Even with mild distortion, the best method can conceal 40% of objects. Proposed techniques perform better than white noise baseline, showing potential in transferring. Monochromatic method is better than pile-up, and both are effective in improving transferability. The colorful pile-up setting outperforms other methods significantly, concealing 40% of objects with mild distortion. Monochromatization can further improve transferability by reducing the detection rate. However, there are issues with repetitive circle patterns being mistaken as round objects in the evaluation metric used. Visual evaluation shows how these techniques enhance transferability. The experiments show how pile-up significantly improves transferability in object detection. Comparing full-mask, pile-up, and monochromatization techniques, the adversaries generated with pile-up are smoother and less complex, with repetitive patterns designed for better performance. The experiments demonstrate that pile-up enhances transferability in object detection by creating smoother and less complex adversaries with repetitive patterns. The simplified patterns improve performance, especially in the white-box setting, compared to full-mask adversaries. The quality of gradients is crucial in adversarial attacks. Gradients from the classifier layer only cover a small region of the adversarial mask, leading to issues with adaptive training methods. Effective Receptive Field (ERF) helps analyze how the full-mask attack is updated by computing the gradient of a neuron over the input image. Examples in the appendix show that gradients mainly cover the object region. The gradients mainly cover the object region, leading to inefficiency in updates due to significant variance across samples. Piled-up small patches improve efficiency and accuracy compared to full-mask setting. The proposed techniques aim to enhance transferability, with improvements over the full-mask method considered significant contributions. The curr_chunk discusses experimenting with a method adapted from adversaries for image classification to establish a benchmark for universal attack on object detectors. It aims to design an object that can conceal neighboring objects contactlessly, different from existing methods like stickers that alter objects significantly. The curr_chunk discusses a training method to conceal objects contactlessly, with a success rate of up to 50% depending on distance. The trained patch contains circular patterns similar to other settings. It serves as a baseline for understanding object concealment but is essentially another type of attack. The paper provides effective methods to fool object detectors, highlighting the limitation that object detectors are not yet robust enough. The curr_chunk discusses the challenges in fooling image classification models despite their high accuracy, while object detectors still lag behind. The study analyzes gradients in 100 samples to understand errors in object detectors, showing that only the object area obtains gradients. The study found that only the object area in samples obtains non-negligible gradients, with a magnitude ranging from 1e1 to 1e2. Parameters receive unstable gradients, hindering training and leading to lower performance despite potential capacity. The study found that only the object area in samples obtains non-negligible gradients, with a magnitude ranging from 1e1 to 1e2. Parameters receive unstable gradients, hindering training and leading to lower performance despite potential capacity. For better illustration of image distortion, random samples are selected and shown in FIG5. The artificial object is parametrized as a tensor p \u2208 [0, 1] h\u00d7w\u00d73 in round shape, trained with gradient descent methods on images with standard data augmentations for improved robustness. The artificial object is randomly placed around objects without overlapping, and rescaled to a proper size for each image. The artificial object for data augmentation is scaled using a uniform random variable, rotated randomly, and placed around objects without overlapping. The transformation is denoted as function A, and for training, the expected log probability of objectiveness in YOLOv3 is minimized over transformations and images in the training set. In YOLOv3, optimization is done over the top 12 positive prediction points to avoid obfuscation by negative points. The size and distance to target objects are explored, with distance measured as the logarithm of the absolute distance between centers of trained adversarial object and detected object's bounding box. N = 10647 in YOLOv3. The results show the performance of adversarial patches of different sizes and distances from target objects. Baseline methods like black-hole and white-noise significantly change the image but have little effect on detection results. Trained objects can conceal other objects without physical contact. The size of the trained object plays a role in its effectiveness. Trained objects of reasonable size can conceal over 50% of other objects when placed nearby. Larger patches were not considered practical. Distance to target objects affects success rate, with closer objects being more easily concealed. Successful attack demonstration shown in Figure 11."
}