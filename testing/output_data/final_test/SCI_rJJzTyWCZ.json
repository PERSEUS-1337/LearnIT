{
    "title": "rJJzTyWCZ",
    "content": "The paper introduces CLOTH, a human-designed cloze test dataset for language exams. It challenges students with carefully crafted questions and confusing answer choices, requiring a deep language understanding. Human performance surpasses baseline models, even with extensive training data. The study identifies the limited ability to comprehend long-term context as a key factor in the performance gap between models and humans. Automatically generated cloze datasets, like the CNN/Daily Mail dataset, are used to assess machine reading comprehension. These datasets consist of context paragraphs and question sentences with missing words. Named entities are often used in these datasets. Automatically generated cloze datasets, such as the Children's Books test (CBT) BID9, provide context paragraphs with missing words in question sentences. These datasets include candidate answer sets for each question, allowing for significant research progress despite some issues with the automatic generation process. Automatically generated cloze datasets like CBT provide context with missing words in questions. Ambiguity and irrelevance in generated questions can be problematic. Neural models have shown comparable performance to humans in question generation. Incorporating human design into cloze question generation has been attempted. CLOTH is a large-scale cloze test dataset created from English exams by middle-school and highschool teachers to prepare Chinese students for entrance exams. Human-designed questions are harder and provide a better assessment of language proficiency compared to automatically generated questions. The CLOTH dataset consists of human-designed cloze questions that are challenging and assess various aspects of language proficiency. State-of-the-art language models struggle to perform as well as humans on these questions, indicating a gap in long-term context understanding. The CLOTH dataset, collected from English examinations, consists of challenging human-designed cloze questions that assess language proficiency. Human-designed training data leads to better performance compared to automatically generated data. The dataset was obtained from three websites in China and underwent cleaning processes to ensure data validity. After collecting the CLOTH dataset from English exams, it underwent cleaning processes to ensure data validity. This involved removing questions with inconsistent formats, filtering questions relying on external information, deleting duplicates, and extracting answers from images using OCR software. The dataset consists of 7,131 passages and 99,433 questions, divided into middle school (CLOTH-M) and high school (CLOTH-H) parts. 11% of the data was split for the test and dev sets. Detailed statistics are presented in TAB1 for evaluation of language mastery. In TAB1, subsets are presented for evaluating language mastery through tests designed by teachers. The passage in Table 2 describes Nancy's first day at work as a secretary, where she receives flowers anonymously. Nancy, a new secretary, received flowers anonymously on her desk every Monday. She discovered it was a company policy to boost morale. The questions are divided into types such as grammar, short-term reasoning, matching/paraphrasing, and long-term reasoning. 100 passages each from high school and middle school categories were sampled, with 20 questions per high school passage and 10 questions per middle school passage. The types of 3000 questions were labeled on Amazon Turk, with payment of $1 for high school passages and $0.5 for middle school passages. The majority of questions in the passages are short-term reasoning questions that require grammar, vocabulary, and simple reasoning skills. Middle school questions are easier with more grammar questions. Approximately 22.4% of questions require long-term reasoning. A study was conducted to determine if human-designed cloze tests are challenging for state-of-the-art models. It was found that models trained on large external corpora struggle with cloze tests due to difficulty with long-term dependencies. The study evaluates human performance on cloze tests with only one sentence as context. A bidirectional LSTM model is trained to predict missing words using the attention mechanism to gather information from a longer context. The attention models are trained with human-designed data. The attention models in language modeling and cloze tests are trained with human-designed data. Language models predict the next word based on context, while cloze tests require filling in missing words based on surrounding context. Language models can be adapted to cloze tests by treating each word as a possible blank and learning to predict it, providing more supervision than supervised models trained on human-labeled questions. The language model trained on a large unlabeled corpus achieves a perplexity of 30.0. Amazon Turkers' performance on 3,000 questions with the whole passage given is compared to attention models and LSTM, showing similar accuracy. The attention model's lower performance is attributed to the challenge of comprehending longer context in training data focused on short-term information. The language model trained on a large unlabeled corpus achieves a perplexity of 30.0. When only one sentence is given as context, the accuracy of the 1-billion-language model is 0.695, showing that more data leads to better generalization. However, increasing the context length to three sentences only improves the accuracy to 0.707. Human performance surpasses the language model significantly, indicating the importance of deliberately designed questions. The state-of-the-art model in CLOTH lags behind human performance due to errors in long-term reasoning and dependencies within three sentences. The language model outperforms LSTM with more supervision and training on a large external corpus. The attention model does not improve performance compared to vanilla LSTM. Several errors made by the large language model are highlighted in Table 5. The language model in the company fails due to coreference issues and long-term dependency. It is trained on the sentence level, potentially neglecting long-term information. Computational limitations prevent further investigation. Nancy receives flowers and wonders who sent them. The language model in the company fails due to coreference issues and long-term dependency. Nancy receives flowers and wonders who sent them. Error analysis of the 1-billion-language-model shows varying accuracy on different types of questions, indicating long-term-reasoning is harder to predict. The reliability of question type labels depends on turkers being careful. An error analysis revealed that a careless turker may mislabel examples. To assess the language model's strength in short-term information, turkers were asked to label options with limited context. The performance of turkers and the 1-billion-language-model was compared in TAB8. The performance of the 1-billion-language-model in solving short-term cloze questions almost matches the ceiling performance with one sentence as context. However, the model's performance does not significantly improve with long-term context, indicating a gap in long-term reasoning ability. Comparison of model and human performance on different question categories is shown in Figure 1. Human study on short-term ceiling performance reveals carefully selected options when questions have multiple answers. The text discusses the comparison between human-designed data and automatically generated data for cloze tests. Human-designed data is shown to result in a larger performance gap between models and humans. However, using human-designed data for training on automatically generated questions is not suitable due to distributional mismatch. The model's performance can be improved by finding generated data that resembles human-designed data. Research on cloze test design shows that tests created by deliberately deleting words are more reliable than randomly or periodically deleting words. Teachers select words to examine students' proficiency in grammar, vocabulary, and reasoning. Incorrect options provided are grammatically correct and relevant to the context to make questions non-trivial. Human-generated data distribution differs from automatically generated data. The distribution of human-generated data is compared to automatically generated data using an LSTM model. Different proportions of the two types of data are used to train the model, and performance is tested on both types of data. Human-designed data leads to a larger performance gap in the model. The study compares human-designed data to automatically generated data using an LSTM model. Human-designed data results in a significant performance gap between the model and human performance, while the gap is smaller with automatically generated data. The distributional mismatch between the two types of data makes it challenging to transfer a model trained on human-designed data to automatically generated data. The study proposes combining human-designed data with automatically generated data to improve performance in language models. This approach takes advantage of the different strengths of each type of data, with the goal of bridging the gap between model and human performance. The study explores two methods of combining human-designed data with automatically generated data in language models: Equally averaging the loss for both types of questions and using representativeness-based weighted averaging to select representative questions for training. The study compares two methods of combining human-designed and automatically generated data in language models: equally averaging loss for all questions or using representativeness-based weighted averaging to select representative questions for training. The performance of the representativeness prediction network is shown in the results. The study evaluates the performance of combining human-designed and automatically generated data in language models. Results show that representativeness-based weighted averaging improves accuracy to 0.565, and when combined with human-designed data, accuracy increases to 0.583. Large-scale automatically generated cloze tests have led to research advancements, but questions do not consider the language phenomenon being tested. Reading comprehension datasets are labeled by humans to ensure quality. The curr_chunk discusses various datasets used to evaluate machine reading comprehension abilities, including NTCIR QA Lab BID24, CLEF QA Track BID18 BID21, AI2 Elementary School Science Questions dataset, and BID15. It highlights the focus on language proficiency evaluation and reasoning questions in teacher-designed exams. Additionally, a simple supervised approach is employed to predict word likelihood in cloze questions. The curr_chunk introduces a large-scale cloze test dataset called CLOTH designed by teachers to test language understanding. Human performance surpasses state-of-the-art models due to the model's struggle with long context comprehension. Human-designed questions are more challenging compared to automatically-generated ones. The prediction model achieves an F1 score of 36.5 on the test set, showing a larger margin between human and model performance. Content words related to context have higher scores, while obvious words have low scores. The model struggles with long context comprehension, making human-designed questions more challenging."
}