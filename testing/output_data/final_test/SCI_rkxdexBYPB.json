{
    "title": "rkxdexBYPB",
    "content": "Character-level language modeling is a crucial task in Natural Language Processing. In this paper, a lightweight model called Group-Transformer is proposed to reduce resource requirements for character-level language models. The model partitions linear operations to decrease parameters and computational cost, using only 18.2% of parameters compared to LSTM-based models while outperforming them on benchmark tasks. The Group-Transformer also shows better performance compared to Transformers with similar parameters and time complexity. The proposed Group-Transformer model reduces resource requirements for character-level language models, outperforming LSTM-based models with only 18.2% of parameters. It also shows better performance compared to Transformers with similar parameters and time complexity. The implementation code will be available. Character-level language modeling is essential in NLP tasks like classification, sequence tagging, question answering, and recognition. Transformer architecture has shown promise in language modeling, but its large size remains a challenge. The text discusses the challenge of large Transformer-based models for edge device applications and the need for a lightweight model for real-time responsiveness. The proposed method introduces a lightweight transformer for character-level language modeling by using group-wise linear operation and sparse connectivity. The proposed Group-Transformer model in transformer architecture utilizes group-wise linear operation and sparse connectivity to reduce parameters and calculations. Inspired by group convolution approaches, it aims to compress large image processing models for mobile devices. To address performance compromises due to inter-group correlations, inter-group operations were added for the group attention and feed-forward layers. By modeling inter-group information flows, Group-Transformer achieves performance and lightweight characteristics. Extensive experiments were conducted on enwik8 and benchmark datasets. Group-Transformer, a lightweight Transformer model with group strategy, outperformed LSTM-based models on benchmark datasets. It addresses efficiency by restricting dependencies between input tokens and reducing pair-wise calculations. This approach improves time efficiency during inference without heavy parameterization. In this work, a lightweight transformer with less than 10M parameters is described, which is significantly smaller compared to previous character-level language models. The group strategy for compressing convolutional neural networks has gained attention, but it can lead to performance degradation due to lack of interaction between different groups. ShuffleNet introduced channel shuffle operation to address this issue. Group-Transformer is a novel approach that adopts the group strategy for Transformer architecture, incorporating group-wise operations inspired by ShuffleNet. It consists of group embedding, group attention, and group feedforward layer to handle dependencies in the time domain and re-configure grouped features. Group-Transformer converts input characters into multiple group representations and predicts the next character by processing and merging them. Inter-group information flow ensures groups are aware of each other, preventing redundancy. The sub-modules' architectural details and relations are described, with a focus on group embedding layers and representing tokens with sets of embeddings. This approach is commonly found in NLP models for embedding input tokens. The paper describes a process at a single time step where the attention mechanism identifies dependencies between features in the time domain. It focuses on applying a group strategy to the feature space of Transformer, with three main steps involving queries, keys, and values, retrieving relative features at different times, and transforming the attended feature into the input domain. The paper introduces a group attention mechanism for the feature space of Transformer, focusing on queries, keys, and values. Group attention processes grouped features with intra-group and inter-group operations, calculating queries for each group and head within the multi-head attention module. The paper introduces a group attention mechanism for the feature space of Transformer, focusing on queries, keys, and values. It utilizes linear weights to describe intra-group and inter-group combinations, reducing the number of parameters and calculations compared to fully connected layers. The approach allows grouped features to share a common expression across different time frames. Keys and values are processed using fully connected layers for all group pairs, following the original Transformer formula. The paper introduces a group attention mechanism for the feature space of Transformer, focusing on queries, keys, and values. It utilizes linear weights to describe intra-group and inter-group combinations, reducing the number of parameters and calculations compared to fully connected layers. The approach allows grouped features to share a common expression across different time frames. Position encoding plays a crucial role in making features aware of their position in an input sequence. Relative positional encoding is applied for effective description of long-length character sequences. The attention mechanism determines the attended feature of a head in a group based on relative positional information. Linear weights are used to combine intra-group and inter-group information for the final output. The proposed module utilizes group-wise attention mechanism to combine intra-group and inter-group information, reducing parameters and calculations. Inputs are added to the output for a residual connection. The group feed-forward layer re-configures outputs with groupwise operations. The architecture involves shuffling groups to support each other, processing group-wise features with linear transformations and non-linear activation. Linear layers transpose features as in the original module. The proposed module utilizes group-wise attention mechanism to combine intra-group and inter-group information, reducing parameters and calculations. Linear layers in the module transpose input features into a high-dimensional space with non-linear activation. The group feed-forward layer transforms the output back into the input space using linear weights for mapping intra-group and inter-group information. Low-rank matrix approximation is introduced for the inter-group transformation matrix. The proposed module uses group-wise attention to combine intra-group and inter-group information, reducing parameters and calculations. Matrix factorization is applied to reduce the overburden of heavyweights and expensive calculations in the feed-forward layer. The dimension M is set as D group /G to control the dimension relative to the number of groups, and a group-wise linear transformation is efficiently modeled with a shuffle trick. A group-wise linear transformation is then applied to high-dimensional features using linear weights for a residual connection. The Group-Transformer reduces parameters by splitting the hidden state into groups for processing. It aims to decrease resources by increasing the number of groups, with detailed comparisons to the original Transformer in Appendix B. The Group-Transformer reduces parameters by splitting the hidden state into groups for processing, aiming to decrease resources. The efficiency of the proposed Group-Transformer is demonstrated with benchmark datasets enwik8 and text8, showing differences in unique characters and content preprocessing. Experimental settings follow previous works with variations in hyperparameters affecting model size. The Group-Transformer model has 9 layers, 256 features per character, 8 heads, and explores 2 or 4 groups. Regularization includes layer normalization and dropout layers with a probability of 0.1. The feed sequence length is 512 with cached previous sequences. Adam optimizer is used with specific parameters, and the best model is chosen based on validation set performance. Comparison with other models using under 50M parameters is shown in Table 1. Implementation code will be available for further details. The Group-Transformer model with 9 layers, 256 features per character, and 8 heads outperforms LSTM models with under 30M parameters. The number of groups is a key hyper-parameter affecting model size. Reduction in hyper-parameters leads to lower resource requirements but may impact model performance. The Group-Transformer model outperforms models with comparable resources, proving the efficiency of feature grouping methods in reducing model size and time complexity. Ablation studies show the impact of group attention and feed-forward layer on parameters and performance, demonstrating the benefits of Group-Transformer over traditional models. The Group-Transformer model reduces model size by selectively replacing attention and feedforward layers with group-wise modules. Concurrently using both group-wise modules shows more efficiency in reducing required resources. The study investigates the impact of inter-group operations in the Group-Transformer model. Removing inter-group operations resulted in performance degradation of 0.028 bpc for 2-Group-Transformer and 0.051 bpc for 4-Group-Transformer. This emphasizes the importance of inter-group modeling in GroupTransformer. The study highlights the significance of inter-group operations in the Group-Transformer model. Without these operations, the model struggles to fully utilize multi-head attention, affecting its ability to attend to multiple content positions effectively. Transformer models excel in character-level language modeling by capturing long-term dependencies, but their large parameter size makes them computationally expensive. The Group-Transformer model aims to address these limitations. Group-Transformer was developed to prove Transformer's effectiveness in a lightweight setting by grouping features and proposing group-wise operations to reduce parameters and time complexity. It outperformed LSTM-based models with only 6M parameters compared to 30M. The group strategy effectively reduces computational resources. The shuffle trick is used for inter-group interaction in the group feed-forward layer, involving linear weights in low rank matrix factorization. Group-wise linear operations on input features are split into groups and perturbed using the shuffle operation. Linear transformations with weights are applied to vectorize the output. Comparing Group-Transformer to the original transformer is discussed in this section. In this section, Group-Transformer is compared to the original transformer in terms of parameter numbers. Parameters are calculated based on feature size, filter size, and number of heads for both models. The multi-head attention in the original transformer uses 4D^2 model parameters. When the number of groups is 2, the number of parameters is discussed. The Group-Transformer model reduces the number of parameters compared to the original transformer as the number of groups increases. Despite lower scores on quantitative evaluations, the model can still generate real or plausible English words effectively in word and sentence completion tasks. The Group-Transformer model with reduced parameters can effectively generate real or plausible English words in completion tasks. The Transformer-XL with 41M parameters showed no proof of guilt in an inquiry. The proposed method focuses on developing character-level language models that can be applied to various NLP tasks. The word embedding layer compression is crucial for designing a lightweight language model, with an embedding dimension set at 500. The number of layers and hidden dimension are adjusted to maintain the same number of parameters (4.5M). The bottleneck dimension is set as 4 times larger than the hidden dimension, following experimental settings from Dai et al. (2019). Performance comparison between different numbers of groups under similar parameters is presented in Table 7. The study examined the impact of grouping methods on group attention, finding a small performance drop with increased grouping operations. There was a slight performance difference between grouping targets with the same number of operations."
}