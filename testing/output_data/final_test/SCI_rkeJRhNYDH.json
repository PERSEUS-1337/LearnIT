{
    "title": "rkeJRhNYDH",
    "content": "The paper focuses on fact verification using structured evidence, specifically tables, graphs, and databases. A dataset called TabFact with 16k Wikipedia tables is used for 118k human-annotated statements labeled as ENTAILED or REFUTED. The challenge lies in combining soft linguistic reasoning and hard symbolic reasoning, addressed by two models: Table-BERT and Latent Program Algorithm (LPA). In addressing fact verification challenges, two models are designed: Table-BERT encodes tables and statements for verification, while LPA executes LISP-like programs against tables. Both models show similar accuracy but fall short of human performance. The verification problem is crucial in natural language understanding and has applications in misinformation detection. The first end-to-end fact-checking system was proposed in 2017. The verification problem in natural language tasks has been extensively studied using various techniques, including logic rules, knowledge bases, and neural networks. Large-scale pre-trained language models have recently dominated other algorithms in approaching human performance on textual entailment tasks. RTE and NLI consider premise sentences as evidence, claim verification uses passage collections like Wikipedia, and NLVR/NLVR2 use images as evidence. In this paper, the focus is on fact verification using structured evidence like graphs, tables, or databases, which are common in real-world applications. The authors introduce a dataset called TABFACT, consisting of 118K manually verified semi-structured Wikipedia tables for this purpose. The authors introduce TABFACT, a dataset with 118K manually annotated statements related to 16K Wikipedia tables, classified as ENTAILED and REFUTED. TABFACT combines linguistic reasoning, requiring semantic-level understanding, and symbolic reasoning for verification. The authors introduce TABFACT, a dataset with 118K manually annotated statements related to 16K Wikipedia tables, classified as ENTAILED and REFUTED. This dataset combines linguistic reasoning and symbolic reasoning for verification. The verification task involves symbolic execution on the table structure and requires both condition and arithmetic operations. The proposed approaches to deal with mixed-reasoning challenge include Table-BERT, which views the verification task as an NLI problem by linearizing a table as a premise sentence. The authors introduce TABFACT, a dataset with 118K manually annotated statements related to 16K Wikipedia tables, classified as ENTAILED and REFUTED. This dataset combines linguistic reasoning and symbolic reasoning for verification. The proposed approaches to deal with mixed-reasoning challenge include Table-BERT, which views the verification task as an NLI problem by linearizing a table as a premise sentence. Additionally, a state-of-the-art language understanding pre-trained model and Latent Program Algorithm are utilized for classification and symbolic reasoning, respectively. Extensive experiments show reasonable accuracy but below human performance. The proposed table-based fact verification task aims to create a benchmark for building powerful AI that can reason over soft linguistic and hard symbolic forms. The dataset includes 18K clean tables extracted from WikiTables and utilizes crowd-sourcing from native English-speaking countries for annotation. The annotation process for the table-based fact verification task involves two channels: a Low-Reward Simple Channel and a High-Reward Complex Channel. Annotators are provided with table captions to aid understanding, and a pipeline of annotation, statement rewriting, and verification is used to ensure quality. The High-Reward Complex Channel pays workers 0.75 USD for annotating a HIT with more sophisticated statements involving higher-order semantics. The data obtained from this channel is harder in terms of linguistic and symbolic reasoning, aiming to test models under different difficulty levels. Annotation artifacts and conditional stylistic patterns may lead to artificially high performance by shallow models. To prevent artificially high performance by shallow models, a negative rewriting strategy is implemented during annotation. Annotators are guided to rewrite collected entailed statements, modifying words and phrases while maintaining sentence style/length. Naive negations are disallowed, and quality control measures include reviewing randomly sampled statements for typos, grammatical errors, and vagueness. After data collection, annotated samples are redistributed for further filtering to ensure statements are clear, supported or contradicted by the table. Workers are paid to decide on statement rejection based on criteria like no ambiguity or typos. Roughly 18% entailed and 27% refuted instances are abandoned due to poor quality. Basic statistics of data collected from simple/complex channels are shown in Table 1, with Train/Val/Test Split division. Inter-Annotator Agreement is achieved after merging instances from two different sources. After merging instances from two different channels, a diverse dataset was obtained for table-based fact verification. 1000 annotated pairs were redistributed to workers for re-labeling as ENTAILED or REFUTED, resulting in a Fleiss \u03ba of 0.75 indicating strong inter-annotator agreement. Dataset statistics show slightly more data from the complex channel, with similar sample lengths. The dataset for table-based fact verification includes 8 categories of higher-order operations, with the complex channel dominating over the simple channel. The data is split into train, validation, and test sets, with balanced distributions of positive and negative instances. The dataset was formally defined for the fact verification task. The dataset for table-based fact verification task consists of triple instances (T, S, L) with a table T, a natural language statement S, and a verification label L. The table has R rows and C columns, with content in each cell. The statement describes a fact to be verified against the table content. The model is trained on instances (T, S, L) and tested on (T, S) to predict the label. Performance is measured by prediction accuracy. The performance is measured by prediction accuracy. Before building the model, entity linking is performed to detect entities in the statements. The linked entities are masked in the statements to focus on verification against the table. Two models are proposed for table fact verification, treating it as a program synthesis problem. In a two-stage verification process, the first step involves latent program search to parse statements into programs representing semantics using a set of roughly 50 functions. Each function takes specific argument types and outputs specific variables, stored in different-typed caches for semantic compositionality. Trigger words are used to prune the API set and speed up the search process. The latent program search procedure involves using trigger words to prune the API set and accelerate search speed. The process includes initializing caches for numbers, strings, booleans, and views, as well as a result collector and program trace. A queue is used to store intermediate states, and trigger words help identify plausible function sets. The program iterates over functions and caches arguments to execute functions and build the program trace. The program iterates over functions and caches arguments to execute functions and build the program trace. If Type(A)=Bool, the program is valid and the trace is collected. Intermediate boolean values are added to the bool cache. Return the triple (Table, Statement, Program Set) after collecting potential program candidates. The program iterates over functions, caches arguments, and builds the program trace. The discriminator is trained to identify appropriate traces from a set of erroneous and spurious traces using weakly supervised learning. The discriminator is built with a Transformer-based two-way encoder to encode input statements and programs. Table-BERT is a model that linearizes tables into sequences for table verification, treating the statement as another sequence. The model uses a discriminator to assign confidence to candidate predictions and aggregates them based on confidence weights. Linearized tables can be lengthy, surpassing the limits of sequence models like LSTM or Transformers. To address the issue of lengthy linearized tables exceeding the capacity of models like LSTM or Transformers, a method is proposed to shrink the sequence by retaining only columns with entities linked to the statement. Two linearization methods are suggested: Concatenation, where cells are concatenated with [SEP] tokens and column names are included as type embeddings; and Template, which transforms tables into \"somewhat natural\" sentences using simple language templates. This design aims to preserve table information in a machine-readable format. The isolated cells in a language-like format are connected with punctuations and copula verbs. The linearized sub-tableT is concatenated with the natural language statement S and a [CLS] token is added to obtain the sequence-level representation 768 from pre-trained BERT. The model is finetuned to minimize binary cross entropy on the training set and classify statements as ENTAILED when the matching probability is greater than 0.5. The proposed methods are evaluated on TABFACT, including a simple and complex test set split. The test set is split into simple and complex partitions based on the channel of collection. A small test set with 2K samples is held out for human evaluation. An LSTM encoder and decoder with copy mechanism are used to synthesize the program. Training with reinforcement learning is challenging due to underspecified binary rewards. LPA is used as a teacher to search the top programs. Table- BERT is built based on the open-source implementation of BERT, using a pre-trained model with specific parameters. The model is fine-tuned on a single TITAN X GPU and achieves the best performance after about 3 hours of training. Different variants of the Table- BERT model are implemented and compared. The Table-BERT model includes variants like Concatenation vs. Template and Horizontal vs. Vertical for linearization. Latent program search is done on three 64-core machines, terminating after 50 traces or path length of 7. The discriminator model uses transformer-based encoders. LPA models variants are Voting, Weighted-Voting, and Ranking. The study evaluates the effectiveness of a negative rewriting strategy in eliminating artifacts or shallow cues in a BERT classifier without table information. The preliminary evaluation includes testing the entity linking system, program search, and statement-program discriminator. The results show the effectiveness of the rewriting strategy in the BERT classifier without table information. The study evaluates the effectiveness of a negative rewriting strategy in eliminating artifacts in a BERT classifier without table information. Results show the effectiveness of the strategy. The human study evaluates entity linking, latent program search, and statement-program discriminator components. Preliminary case study results are reported in Table 3. The study evaluates different components such as entity linking accuracy, systematic search recall, and discriminator accuracy. Results show that the template is crucial for the pre-trained BERT model to perform reasoning on structured tables. Horizontal scanning outperforms vertical scanning, and LPA-Ranking is the most effective method in suppressing spurious programs. The study found that LPA-Ranking is the best method for suppressing spurious programs. The current LPA method has a recall upper bound of 77%, but the real accuracy is only 65%. The weakly supervised learning of the discriminator is identified as a bottleneck for LPA. Comparing simple-channel with complex-channel split shows a significant accuracy drop of approximately 20%. Table-BERT shows instability during training and poor consistency. The study identified weaknesses in the current LPA method, such as gradual degradation and poor consistency. In contrast, LPA behaves more consistently and provides a clear rationale for its decisions. However, it requires manual API operations and is sensitive to entity linking accuracy. Combining different methods remains an open question in natural language inference and reasoning research. The surge of deep learning has led to powerful algorithms like Decomposed Model, Enhanced-LSTM, and BERT. NLVR and NLVR2 use images for statement verification. The fact verification task is related to these inference tasks, with a semi-structured table as premises. This problem can be seen as a generalization of NLI in the semi-structured domain. TABFACT is unique in that it treats any fact containing misinformation as false. This compositional verification problem requires breaking down facts into sub-clauses or (Q, A) pairs for evaluation, making it more challenging than standard QA. Some facts require inference techniques beyond semantic forms for verification. The proposed TABFACT benchmark challenges neural reasoning models with spurious programs, decomposition, and linguistic reasoning. It requires decomposing statements into sub-clauses and verifying them individually, posing difficulties for logic inference chains and linguistic reasoning. The research investigates semistructured fact verification using Table-BERT and LPA methods based on natural language inference and program synthesis. Trigger words for different functions are listed for future research direction towards more sophisticated architectures for linguistic and symbolic reasoning. The curr_chunk discusses various operations such as negation, superlative, comparative, ordinal, and unique in natural language processing. These operations are used to analyze sentences and extract key information. The curr_chunk discusses the unique operation in natural language processing, highlighting cases where symbolic execution fails due to entity link or function coverage problems. The entity linking model fails to link to a \"6-4\" cell content, and symbolic reasoning models struggle with cases like parsing \"7-5, 6-5\" as \"won two games\". Jordi Arrese achieved a better score in 1986 and won both final games that year. The Table-BERT model faces challenges with long dependencies and memorizing historical information, especially when table content is unfolded into a long sequence. Arrese played all his games on clay surface. The table shows outcomes and dates. 200 samples were categorized based on semantic. BERT model handles linguistic inference well. LPA has slightly higher accuracy than random guess. The BERT model performs well on trivial cases with a horizontal scan order, while the LPA model excels in higher-order logic cases involving operations like Count and Superlative. Error analysis of LPA and Table-BERT shows that reasoning depth is concentrated between 4 to 7 logic inference steps, indicating the difficulty of fact verification in the TABFACT dataset. Ablation annotation tasks compare annotation quality with and without Wikipedia title as context. The study compares annotation quality with and without Wikipedia titles as context, showing that statements with titles are more human-readable according to experts. This context is deemed necessary for annotators to understand the information fluently. The annotation process includes providing Wikipedia titles as context to improve sentence fluency, while also restricting annotators from including unrelated background information. Detailed instructions are given for rewriting fake statements, and experts perform sanity checks on the annotated dataset to ensure quality. The annotation process involves ensuring the dataset is clean and meets requirements by composing non-trivial statements supported by a table. An example of a trivial statement is provided, and annotators are tasked with writing diverse non-trivial facts based on the table provided. The annotation process requires composing non-trivial statements supported by a table."
}