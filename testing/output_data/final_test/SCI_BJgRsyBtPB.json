{
    "title": "BJgRsyBtPB",
    "content": "Generative Adversarial Networks have enabled data generation in various scenarios, but training them for complex, high-dimensional distributions can be challenging due to convergence issues and mode collapse. Sliced Wasserstein GANs, particularly using the Max-Sliced Wasserstein distance, have helped approximate Wasserstein distance efficiently and stably during training, easing convergence problems. This method simplifies sample assignment and distance calculation by sorting the one-dimensional projection of samples, providing a good approximation of the high-dimensional Wasserstein distance. In this paper, it is shown that the greedy assignment of real and fake samples in Generative Adversarial Networks can lead to faster convergence and better approximation of distributions. GANs have revolutionized data generation by using a discriminator network to approximate distances between distributions, improving neural network modeling capabilities. Various studies have demonstrated the effectiveness of GANs in approximating complex high-dimensional distributions in practice. In practice, GANs face challenges like mode collapse and convergence issues. The introduction of the Wasserstein distance has helped improve convergence problems by calculating a minimal transportation distance between distributions. However, the complex approximation and calculation of the Wasserstein distance in high dimensions remain a challenge. In high dimensions, calculating distance is complex and challenging due to exponential complexity with large sample sizes. Deshpande et al. (2018) showed that high-dimensional distributions can be approximated using one-dimensional projections. The minimal transport between one-dimensional samples can be calculated by sorting and assigning them based on their indices. This method allows for direct approximation of real samples by the generator only, simplifying training into a stable minimization problem. The theory of a novel method was described in Deshpande et al. (2019), showing how a high number of random projections can be replaced by a single continuously optimized plane. This adversarial optimization selects the \"worst\" projection to maximize separation between real and fake samples. Sliced Wasserstein distances provide a way to create a mini-max game where the generator produces samples resembling the original distribution based on the selected plane, while the discriminator aims to separate real and fake samples. The Sliced Wasserstein distances method calculates minimal transportation between projected samples in one dimension, approximating the Wasserstein distance in high dimensions. It satisfies properties of a true metric but a greedy assignment approach may outperform sorting real and fake samples. The paper aims to demonstrate flaws in one-dimensional sorting and discuss the application of the Wasserstein distance. Generative adversarial networks involve a generator (G) creating a fake distribution (P F ) resembling real samples (P R ), with a discriminator distinguishing between the two. The Wasserstein-p distance was proposed to enhance GAN stability by calculating minimal transportation between distributions. The sliced Wasserstein distance was introduced to improve the instability and complexity of Wasserstein GANs by using one-dimensional projections of real and fake samples. This method calculates the Wasserstein distance by sorting and assigning samples based on their indices, rather than considering all possible joint distributions. Max-sliced Wasserstein GANs select the worst projected distribution and can be implemented using a fully connected layer. The sliced Wasserstein distance improves Wasserstein GAN stability by using one-dimensional projections of samples. The distance can be calculated by sorting and assigning samples based on their indices. Max-sliced Wasserstein GANs select the worst projected distribution and can be implemented with a fully connected layer. The minimal transportation of samples can result in different pairwise differences, even if the assignments of i, j pairs are the same. Sorting for minimal transport may not assign identical samples to each other, leading to errors in cases like Gaussian distributions with different means. This can affect samples at the intersection of distributions, generating incorrect errors. The Wasserstein distance calculates minimal transportation between samples, which can result in shifting all samples to cover the distribution. However, this may lead to errors for identical samples at the intersection of distributions. The proposed method suggests using a greedy approach to assign similar sample pairs first before calculating transportation between disjunct regions. This method aims to address the limitation of the Wasserstein distance in optimizing global transport between distributions while ignoring identical sample pairs or intersecting regions. The proposed method addresses the limitation of the Wasserstein distance by assigning similar sample pairs before calculating transportation between disjunct regions. This can cause problems in calculating minimal transport due to the appearance of wrongly generated samples. The proposed method introduces greedy training of max sliced Wasserstein GANs to improve network training by assigning similar sample pairs before calculating transportation, addressing the limitation of the Wasserstein distance. The proposed method introduces greedy training of max sliced Wasserstein GANs to improve network training by selecting similar sample pairs iteratively for loss calculation, using a matrix to determine distances instead of sorting. This approach requires O(n^3) steps compared to the original O(nlog(n)) operations of sorting. The proposed method introduces greedy training of max sliced Wasserstein GANs to improve network training by selecting similar sample pairs iteratively for loss calculation. This approach requires O(n^3) steps in the distance matrix with size n \u00d7 n, compared to the original O(nlog(n)) operations of sorting. During training, using batches of 16 to 512 samples did not result in a significant increase in computation time. The flaws of Wasserstein distance using sorting in one-dimension were highlighted, with greedy sample assignment suggested as a potential solution. However, it is noted that the greedy approach may not always ensure minimal transportation, as transportation by sorting will always be lower or equal to the assignment calculated by the greedy approach. The proposed method introduces a hybrid approach for training the Greedy Max-Sliced Wasserstein Generator. This approach combines sorting samples to calculate Wasserstein distance with greedy assignment to prevent cases where the greedy assignment is arbitrarily larger than sorting. The algorithm iteratively selects similar sample pairs for loss calculation, improving network training efficiency. The proposed method introduces a hybrid approach for training the Greedy Max-Sliced Wasserstein Generator, combining sorting samples to calculate Wasserstein distance with greedy assignment. A parameter (\u03bd) can be set to determine a limit for the difference between distances, with \u03bd = 1 used in experiments. Sorting samples before creating the distance matrix can help find the minimal element. Initial tests involved a one-dimensional toy problem. The text describes training a four-layered fully connected network on a one-dimensional toy problem using a Gaussian Mixture Model with five modes. The network was trained for 5000 iterations with specific hyperparameters and no discriminator. Two different setups were used to calculate loss, one involving sorting samples and the other using a greedy approach. The code for reproducibility can be found on github. After training a four-layered fully connected network on a one-dimensional toy problem using a Gaussian Mixture Model with five modes, the generator was used to generate 16000 samples. A comparison was made with samples generated by the Gaussian Mixtures Model, calculating Kullback-Leibler divergence and Pearson correlation coefficient. Results showed that training with a greedy assignment resulted in lower Kullback-Leibler divergence and higher correlation, indicating a better approximation of the original distribution. The Kullback-Leibler divergence histogram was calculated using 100 bins uniformly distributed between -0.5 and 8.5. The histogram calculation was done using 100 bins between -0.5 and 8.5. Fig. 3 shows the histograms. Table 1 displays KL divergence and Pearson correlation for sorting and greedy approaches. Two-dimensional Gaussian Mixture models were used with nine modes in a 3x3 grid. A five-layered network with varying neuron numbers was applied. In this experiment, a five-layered network with varying neuron numbers was trained for 500,000 iterations. The Kullback-Leibler divergence and Pearson correlation coefficients comparing greedy and sorting approaches are shown in Table 2. The experiment was also conducted on the MNIST dataset using DCGAN architecture for image generation. Images were resized to 64x64 with single-channel images used. Batches of 128 and 16 samples were used with Adam optimizer for training the generator and discriminator. In the experiment, a five-layered network with varying neuron numbers was trained for 500,000 iterations. Batches of 128 and 16 samples were used with Adam optimizer for training the generator and discriminator. Images were binarized and KL divergence was calculated between white and black values. Results are shown in Table 3. Randomly selected samples generated with different approaches are displayed in Figure. In the experiment, samples were generated using the hybrid approach after 20 epochs of training. Experiments were also conducted on the resized CelebA dataset with images downscaled to 64 \u00d7 64. The DCGAN architecture was used, comparing three different approaches for sample assignment. 10,000 samples were randomly selected and used for distance calculation using sliced Wasserstein distance. Results are presented in Table 4, with a qualitative comparison of randomly selected samples. All samples were generated using Max-sliced Wasserstein. In this paper, a greedy sample assignment method for Max-Sliced Wasserstein GANs is introduced. The approach involves using one-dimensional samples and assigning them to their most similar counterparts to optimize transportation. A hybrid approach combining sorting and greedy assignment methods is also proposed for automatic selection. The study introduces a greedy sample assignment method for Max-Sliced Wasserstein GANs, demonstrating its superiority over traditional approaches on various datasets. The hybrid approach combines sorting and greedy assignment methods for automatic selection, improving Kullback-Leibler divergence and correlation. The method only changes the distance calculation and can be used with other GAN training techniques."
}