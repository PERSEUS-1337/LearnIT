{
    "title": "HkgYEyrFDr",
    "content": "Reinforcement learning (RL) has led to increasingly complex behavior in recent years, but this complexity can be misleading and lead to over-fitting. Visual representations are proposed as a useful metric of complexity, correlating well with objective optimization and affecting reward optimization. Curious representation learning (CRL) is introduced to improve visual representation learning algorithms, leading to better performance on Atari without relying on rewards. In recent years, reinforcement learning has shown increasingly complex behavior, but there is a lack of quantitative measure of intelligence in these agents. Visual representations are proposed as a metric of complexity, correlating well with objective optimization. The quality of visual representations in RL policies is investigated to ensure agents are not just memorizing actions. The quality of visual representations in reinforcement learning policies is found to correlate with progress in reward optimization. Improved visual representations help agents perform better in optimization tasks. One approach to enhancing visual representations is through curiosity-driven learning, where agents seek to maximize surprisal while uncertainty models aim to become less surprised about new states. This approach aims to enable policies to learn good visual representations effectively. By coupling policy learning with representation learning, Curious Representation Learning (CRL) aims to improve visual representations in reinforcement learning policies. This approach allows for better policy visual representations by applying advanced visual representation learning algorithms to the model. CRL consistently achieves good representations in policies across different environments, surpassing domain-specific objectives. Curious Representation Learning (CRL) uses representation learning algorithms to improve visual representations in reinforcement learning policies. It achieves better visual representation learning than other methods by actively seeing diverse inputs. CRL forces the agent and model to compete in a minimax game to learn good visual representations. Additionally, CRL can obtain better overall performance on Atari compared to other approaches like Forward Dynamics and RND when used as a sole intrinsic bonus. Visual representation learning is crucial in reinforcement learning policies. Curious Representation Learning (CRL) introduces a method to enhance visual representations using better algorithms, outperforming Forward Dynamics and Random Network Distillation benchmarks on Atari. Various unsupervised learning approaches have been explored in recent years to improve visual representation. Our proposed method for curiosity in active environments involves a minimax game between a representation learning algorithm and a target policy. This approach differs from past methods and aims to explain strong results of curiosity while paving the way for better algorithms. Our work involves an adversarial game between a representation learning algorithm and a target policy to achieve better curiosity algorithms. We explicitly formulate this as a minimax optimization problem and discuss visual representation learning, curious representation learning (CRL), architectures, and environment setup in different sections. In Section 3.3, architectures are detailed, and Section 3.4 describes the environment setup. Visual representation learning involves testing disentanglement of features in neural networks by training a linear map on a frozen feature embedding from a backbone architecture. Three approaches are considered: colorization, autoencoding, and a bottleneck reconstruction. Linear maps are shown to be effective in feature representation learning. Autoencoding and RND are methods for visual representation learning in neural networks. Autoencoding reconstructs images through a bottleneck, while RND trains a network to predict random features from another network. Rotation prediction did not perform well in ViZDoom due to rotation artifacts. Classification tasks in ViZDoom involve objects like monsters, health packs, and guns. In the Habitat environment, the classification task involves room scenes from the Places365 dataset. In reinforcement learning, agents receive rewards and aim to maximize overall reward through policy optimization. Curiosity introduces intrinsic and extrinsic rewards, leading to improved visual representation learning in both policies and models. By aligning surprisal and modeling objectives, a Minimax objective is achieved between an agent's policy and a world model. This approach enhances visual representation learning by incorporating a generic visual representation learning algorithm. The text discusses how a generic visual representation learning algorithm can improve both models and policies in an environment. By using diverse data to confuse the model and forcing the policy to focus on learning good visual representations, a balance is achieved. The architecture setup includes using identical base architecture for both RL policies and representation learning models, with specific layers and filters. The study evaluates linear classification performance on 168x168 images using residual blocks with 64 and 128 filters. Different techniques such as reinforcement learning policies, representation learning models like RND, colorization, and autoencoding are applied to the 128 filter residual block. The ViZDoom environment is used for evaluation. In the ViZDoom environment, agents navigate through diverse Doom environments generated by Oblige. They must interact with objects, avoid monsters and traps, and move through rooms to exit/enter. Performance is evaluated on varying numbers of randomly generated training maps to assess visual representation learning algorithms. Exploration of the entire space is necessary to obtain the best visual representation. In the Habitat environment, agents explore sets of rooms in houses using Matterport and Gibson house scans. They compare performance with the PointNav objective, where agents are given a relative distance to a goal. Additionally, they evaluate intrinsic curiosity on the Atari benchmark using CRL as the only reward. The study evaluates intrinsic curiosity on the Atari benchmark using CRL as the only reward. They assess the correspondence between policy learning and visual representation learning, evaluate visual representations in CRL on synthetic and real images, and measure CRL's ability to enable good performance on Atari with no extrinsic reward. They also analyze the relationship between visual representations and policy reward on ViZDoom by varying training levels to train policies with different complexities. The study explores the correlation between visual representations and policy reward in reinforcement learning. It suggests stopping policy training once reward performance plateaus and investigates if better visual representations lead to improved objective optimization. The study examines the impact of better visual representations obtained from CRL on relative performance after 1 million frames of training. Results show that policy initialization with CRL outperforms starting from scratch by 242% in relative performance. This suggests that optimizing for better visual representations can lead to faster reinforcement learning on various tasks. The effect of CRL in inducing good visual representations is further analyzed through scatter plots and quantitative numbers provided in the study. The study shows that better representation learning algorithms in CRL lead to improved policy representations. Results indicate that generic representation learning models can enhance policies' visual representations. Comparison between fixed objective policies and CRL policies reveals the effectiveness of CRL with an autoencoding objective in obtaining superior visual representations across different environments. The study compares representation learning models under different objectives, highlighting the effectiveness of CRL with an autoencoding objective in obtaining superior visual representations across various environments. CRL does not require specific task sources and performs well generically, unlike other objectives that may need manual specification. Visual representations of models trained under non-CRL objectives are also compared. The study evaluates the effectiveness of CRL in inducing exploration in different environments and transferring visual representations to real images. CRL outperforms other objectives in obtaining superior visual features, as shown in Table 6. The study shows that CRL enables better visual features in Table 6, with colorization being the most effective algorithm. Linear classification accuracy is significantly improved compared to PointNav, with values of 0.193 for policies and 0.253 for representation learning models. These accuracies are close to a colorization model trained directly on Places room scenes. Models trained using colorization or specified CRL objective from data in Point Navigation and different CRL objectives are compared with a colorization model trained on Places room scenes data and a randomly initialized model. Qualitative analysis of visual representations in Habitat shows that CRL trained models can cluster certain images together. CRL outperforms RND and Dynamics on Atari using intrinsic reward across different environments. Comparison with other curiosity models like Random Network Distillation and Forward Dynamics is also conducted. In this study, the Curiosity-driven Representation Learning (CRL) method outperforms Random Network Distillation and Forward Dynamics in visual representation learning on VizDoom. CRL shows superior performance in 6 out of 8 environments, indicating that better visual representations can enhance curiosity. The study proposes CRL as a method to improve visual representations in policies, leading to increased exploration in no-reward scenarios. The study demonstrates that Curiosity-driven Representation Learning (CRL) outperforms other methods in visual representation learning on VizDoom. Through CRL, clustering of various doom objects is achieved, as shown in nearest neighbor images."
}