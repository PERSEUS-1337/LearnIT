{
    "title": "Syeehk2ca4",
    "content": "Generative adversarial networks (GANs) are evaluated on synthetic datasets containing points and images with polygons. GANs struggle with recreating datasets with discontinuous support or sharp bends with noise and fail to accurately count objects in images. There is a tension between generalization and learning in GANs, with varying results on different datasets. This study aims to test GANs' distribution learning capabilities. This paper evaluates GANs on synthetic datasets to test their distribution learning ability. It focuses on synthetically generated datasets to study commonly encountered distributions in low dimensional space and the impact of discontinuity. Additionally, it examines the ability of GANs to count similar objects in a scene, which is crucial for learning latent space representations of images. Our evaluation focuses on synthetic datasets to test GANs' ability to learn distributions and count objects in images. We demonstrate that GANs can learn commonly found distributions but may struggle with distributions that have gaps in support. Additionally, we evaluate GANs' capability to count repeated objects in images, highlighting a possible tension in high dimensional space. In our experiments, we generated two types of datasets with 5000 examples each: point datasets and image datasets. The point datasets included Mixtures of Gaussians, Concentric Circles, S-shape curves, and Swiss rolls in 2D and 3D space. Each setting had three variants with different levels of noise. For high dimensional learning, we created image datasets with mixtures of polygons. For high dimensional learning, image datasets were generated with mixtures of polygons. Three datasets were created: Squares 1-4, Squares 3-4, and CT2, each containing different combinations of squares, triangles, and circles. The number of objects in each dataset was fixed, with only their positions varying. The objective was to test if GANs can learn to count the number of objects in the images. The study aimed to test if GANs can learn to count the number of objects in images with varying object positions. Two architectures were used for training models on point datasets, including a Vanilla GAN with a 3-layer MLP and another model with Wasserstein loss. The learned distribution improved after 150k steps. The study used two architectures for training models on image datasets: one with a DCGAN-inspired architecture and another with Wasserstein loss enforced via gradient penalty. The models were trained using different optimizers and learning rates. The study used two architectures for training models on image datasets: one with a DCGAN-inspired architecture and another with Wasserstein loss enforced via gradient penalty. Models were trained with the ADAM BID8 optimizer and a learning rate of 0.0002 for up to 150k training steps. Discriminator was updated 5 times for each generator update in WGAN-GP inspired architectures. Evaluating distribution learning capabilities of GANs on mixtures of Gaussians and concentric circles showed both architectures performed equally well but struggled with disconnected components in the datasets. The study evaluated the distribution learning capabilities of GANs on various datasets, including mixtures of Gaussians and concentric circles. The inability to model discontinuity in the data was noted due to the continuous nature of the latent space and neural network. Increasing noise levels could cause separate surfaces in the distribution to merge, altering the shapes of the learned distributions. In experiments, Swiss rolls lose shape more than S-curves, possibly due to closer overlapping surfaces. GANs may not model certain distributions well in noisy environments. One model converges at 74k steps, while the other doesn't. Image quality doesn't differ much after 150k steps. Both models struggle to count on Squares 3-4 dataset, generating 0-5 squares. The learning capability of GANs is questioned regarding whether poor learning or generalization is at play. The challenge lies in distinguishing between poor learning outcomes and generalization, especially when it comes to image datasets with variations in location. Currently, there is no way to enforce constraints on the number or shape of objects in GAN-generated images. The learning capability of GANs is questioned regarding the tradeoff between learning and generalization. The evaluation of hypotheses regarding GANs' ability to learn distributions is left for future work. The focus is not on shape variability in natural datasets, as shape may vary and not be rigid. The choice of non-overlapping squares in experiments eliminates occlusions, addressing concerns about object count accuracy in GAN-generated images. The experiment focuses on using non-overlapping squares to train GANs. The models learn to recognize the presence of one square in the images, with variations in size affecting the likelihood of multiple squares appearing. In the experiment, GANs were trained on non-overlapping squares to recognize their presence in images. The models struggled with counting constraints and did not improve when fine-tuned on a different dataset. GANs also had difficulty reproducing circles and triangles when trained on a more complex dataset with overlaps and multiple polygons. In this paper, the phenomenon of GANs being unable to count is discussed. Experiments on synthetic datasets show that GANs struggle to learn semantic constraints even without noise from natural image datasets. GANs also have difficulty learning discontinuous distributions in non-image data, suggesting the need for mixtures of generators. Noise parameters in the datasets do not significantly impact learning outcomes. The Squares and CT2 datasets were created to test if GANs can learn to count and handle overlapping shapes. Squares dataset consists of non-overlapping squares while CT2 dataset includes circles and triangles that can overlap. Different architectures were used for the experiments as shown in Tables 2, 3, 4, and 5. In experiments with GAN architectures BID12 BID5 BID2, minor changes did not affect image quality. Latent space size was 100 for image datasets and 2 for points in R2 and 3 for points in R3. Additional samples are shown in Figures 16-24."
}