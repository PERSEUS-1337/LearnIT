{
    "title": "BkM3ibZRW",
    "content": "While autoencoders are commonly used for continuous structures like images or waveforms, developing general-purpose autoencoders for discrete structures such as text sequences or discretized images is more challenging. The proposed adversarially regularized autoencoder (ARAE) aims to learn robust discrete-space representations by training a rich discrete-space encoder and a simpler continuous space generator using generative adversarial network (GAN) training. This method results in a smoother contracted code space that maps similar inputs to nearby codes and an implicit latent variable GAN model for generation. Experiments on text and discretized images validate the effectiveness of ARAE. The latent variable GAN model produces clean interpolations and captures multimodality in text and images. Autoencoders show improvements in semi-supervised learning and achieve state-of-the-art results in unaligned text style transfer using shared continuous-space representation. Regularized autoencoders have made progress in learning smooth representations of high-dimensional data like images, enabling smoother transformations in latent space for complex modifications. However, learning similar latent representations for discrete structures like text sequences or discretized images remains challenging. Learning smooth representations of high-dimensional data like images has been achieved with regularized autoencoders, enabling smoother transformations in latent space for complex modifications. However, mapping discrete structures, such as text sequences or discretized images, to a continuous code vector remains a challenging problem. To address this issue, an adversarially regularized autoencoder (ARAE) is proposed to jointly train a discrete structure encoder and continuous space. The adversarially regularized autoencoder (ARAE) jointly trains a discrete structure encoder and continuous space generator, constrained by a discriminator to agree in distribution. This approach allows for a complex encoder model, like an RNN, to be used while still being constrained by a flexible generator distribution. The model can be used as a smoother discrete structure autoencoder or as a latent variable GAN model. Experiments demonstrate the model's effectiveness on discretized images and sentences. The ARAE-GAN model can generate varied samples covering input spaces and produce consistent image and sentence manipulations. It can be used in a semi-supervised setting for sentence inference tasks and improves sentiment transfer results. Regularization is necessary to prevent degenerate identity mapping in unregularized autoencoders. Regularization is essential in code space to prevent degenerate identity mapping. Variational autoencoders (VAEs) use regularization through an explicit prior and variational approximation. Training VAEs for discrete text sequences can be challenging without careful tuning techniques. Making the prior/posterior more flexible through explicit parameterization has been explored to address training difficulties. Adversarial autoencoders (AAE) aim to enhance model flexibility through adversarial training. Unlike AAE, our approach does not sample from a fixed prior distribution but instead uses a flexible generator. This connection between VAEs and GANs has sparked interest in applying GANs to text data. Many researchers are exploring the use of GANs for discrete data like text. Policy gradient methods are used to handle the non-differentiable generator objective in discrete space. Training on text data often requires pre-training with a language modeling objective, limiting the latent encoding of sentences. Reparameterizing the categorical distribution with the Gumbel-Softmax trick has shown promise but scaling to natural language remains a challenge. Recent approaches involve working directly with soft outputs from a generator. Our approach works in code space without utilizing RNN hidden states directly. Discrete Structure Autoencoders define a set of discrete structures with a distribution over this space. The function encodes input to code space using parameters \u03c6 and a conditional decoder distribution p \u03c8 (x | c) over structures X with parameters \u03c8. The encoder and decoder are tailored to the structure of interest, such as using RNNs for sequences. Generative Adversarial Networks (GANs) are implicit generative models that approximate drawing samples from a true distribution using a latent variable z and a generator function. Initial GANs work minimizes Jensen-Shannon divergence, while recent work on Wasserstein GAN (WGAN) focuses on minimizing a different metric. Recent work on Wasserstein GAN (WGAN) replaces the Jensen-Shannon divergence with the Earth-Mover (Wasserstein-1) distance. GAN training involves a generator and a critic/discriminator, where the generator aims to fool the critic and the critic distinguishes real data from generated samples. WGAN training involves min-max optimization over generator and critic parameters, with the critic parameters restricted to a 1-Lipschitz function set to minimize the Wasserstein-1 distance between real and generated distributions. Weight-clipping is used as a naive approximation to enforce this property. Adversarially regularized autoencoders enforce smoothness in discrete autoencoders by learning a parallel continuous-space generator to act as a reference encoding. This joint objective constrains the discrete encoder to match its continuous counterpart in distribution. The Wasserstein-1 distance between the distribution of codes from the discrete encoder model and the distribution of codes from the continuous generator model is approximated using an embedded critic function. The model is trained using block coordinate descent to optimize the encoder, decoder, WGAN critic function, and generator. The full training algorithm is shown in Algorithm 1. The ARAE architecture enforces smoothness in discrete autoencoders by matching the distribution of codes with its continuous counterpart. The ARAE architecture can function as an autoencoder or a GAN, compressing input to a single code vector for manipulating discrete objects in continuous code space. It utilizes a critic function during training iterations to optimize reconstruction and adversarial loss. The ARAE architecture can compress input to a code vector for manipulating discrete objects in continuous code space. It extends the decoder to condition on a transfer variable denoting an attribute y, known during training, to learn p \u03c8 (x | c, y). The code space is trained to be invariant to this attribute, further regularizing it to fool a code space attribute classifier. Two additional gradient update steps are added to incorporate this regularization. The ARAE architecture trains the encoder to deceive a classifier using three different models: autoencoders for images, text sequences, and text transfer. The models share the same generator architecture and use a low-dimensional z with a Gaussian prior. The generator and critic are parameterized as feed-forward MLPs, with the image model utilizing fully-connected NN. The ARAE architecture utilizes feed-forward MLPs to parameterize the generator and critic for image and text models. The image model autoencodes binarized images using fully-connected NN, while the text model employs an RNN for both encoder and decoder, mapping input structures to hidden vectors. The encoder maps input to a hidden vector c, which is fed as an additional input to the decoder RNN for decoding. The distribution over the vocabulary is calculated at each time step via softmax. The ARAE architecture utilizes feed-forward MLPs for image and text models. Parameters W and b are part of \u03c8. Greedy search or beam search approximates the most likely sequence x. An LSTM architecture BID12 is used for encoder/decoder, with a code space classifier p(y|c) modeled using an MLP. Baselines include a standard autoencoder (AE) and cross-aligned autoencoder BID33 for transfer. Encoded code is normalized to lie on the unit sphere, and generated code is bounded by the tanh function. Experiments with sequence VAE and adversarial autoencoder (AAE) on the SNLI dataset were conducted. The ARAE model utilizes feed-forward MLPs for image and text models, with experiments conducted on the SNLI dataset using sequence VAE and adversarial autoencoder (AAE). Despite parameter tuning, neither model learned meaningful latent representations. The Appendix includes detailed descriptions of hyperparameters, model architecture, and training regimes. Experiments focus on regularization impact, unaligned style transfer, semi-supervised learning, and employing the generator network as an implicit latent variable model (ARAE-GAN) over discrete sequences. The main goal is to produce a smoother encoder by matching the distribution from the encoder to the continuous generator. The encoder code in ARAE training quickly converges to match the generated codec, restricted to the unit sphere. The covariance matrix variance between the generator and encoder also aligns after several epochs. Smoothness is assessed by calculating the average cosine similarity of sentences with edit-distance of at most 5. The ARAE encoder quickly converges to match the generated codec on the unit sphere. The cosine similarity of nearby sentences is higher for ARAE than for AE. Edit-distance may not be an ideal similarity proxy, but it is often sufficient. A robust representation should handle small input changes around training examples in code space. Testing this property involves feeding a noised input to the encoder and checking reconstructions. ARAE can map a noised sentence to a natural one, as shown in empirical results. The ARAE encoder converges to match the generated codec on the unit sphere, with higher cosine similarity for nearby sentences compared to AE. A robust representation in code space can handle small input changes, as demonstrated by ARAE mapping a noised sentence to a natural one in empirical results. Increasing swaps in ARAE pushes input further from the data manifold, leading to better reconstruction of the original sentence. Unlike denoising autoencoders, ARAE does not require a domain-specific noising function but learns to denoise as a byproduct of adversarial regularization. This allows for more robust manipulation of discrete objects through code space without dropping off the data manifold. To transfer tasks, a code space is learned to represent input agnostic to attributes, with a decoder to incorporate the attribute. Experiments on sentiment transfer on Yelp and topic transfer on Yahoo show varied results. Sentiment transfer involves splitting Yelp reviews into positive and negative sets. The ARAE encoder converges to match the generated codec on the unit sphere, demonstrating robust representation in code space. The study split the Yelp corpus into positive and negative reviews and trained an ARAE with separate decoders for each sentiment. Adversarial training was used to remove sentiment information from the code space. Evaluation was based on four metrics: Transfer, BLEU, Perplexity, and Reverse Perplexity. The study conducted human evaluations on the cross-aligned AE and the best ARAE model by asking Turkers to assess sentiment, naturalness, and similarity of transferred sentences. They compared the ARAE model with different \u03bb weighting against a vanilla AE trained without adversarial regularization. The ARAE model with different \u03bb weighting was compared to a vanilla AE trained without adversarial regularization. Adversarial regularization enhances transfer and perplexity but makes the transferred text less similar to the original. The method can be applied to other style transfer tasks, such as the Yahoo QA data. ARAE was also used in a standard AE setup for semi-supervised training on a natural language inference task. The full SNLI training set contains 543k sentence pairs, with supervised sets of 120k, 59k, and 28k pairs used for different settings. ARAE utilizes unsupervised data of length < 15, consisting of 655k single sentences. Training with adversarial regularization improves upon models trained only on labeled data. ARAE can also function as an ARAE-GAN model, controlled by latent variable z and generator g \u03b8. In this section, the effectiveness of an induced discrete GAN is measured by generating samples from different models and evaluating their performance on a common test. Samples are generated from ARAE-GAN, an AE, a RNN LM, and the real training set. Sampling from an AE involves fitting a multivariate Gaussian to the code space and generating code vectors to decode into sentence space. The evaluation includes semi-supervised accuracy on the SNLI test set and perplexity metrics. The effectiveness of an induced discrete GAN is measured by generating samples from different models and evaluating their performance on a common test. Samples are generated from ARAE-GAN, an AE, a RNN LM, and the real training set. The evaluation includes semi-supervised accuracy on the SNLI test set and perplexity metrics. The labels of the full SNLI training set are used for unlabeled AE training. Perplexity of language models trained on synthetic samples from GAN/AE/LM and evaluated on real data is measured. Various scenarios involving people in different settings are described. The ARAE-GAN model generates samples with word changes highlighted. Training on real data outperforms training on generated data, but surprisingly, the ARAE-GAN data performs slightly better in language model evaluation. The ARAE-GAN model outperforms LM-generated/AE-generated data in language model evaluation. GANs allow for smooth interpolation between outputs by exploiting the latent space structure. Experimentation involves sampling points from p(z) and generating intermediary points for text and MNIST ARAE-GAN. Image GANs have the ability to move in the latent space via offset. The final intriguing property of image GANs is the ability to move in the latent space via offset vectors. For example, by subtracting the mean latent vector for \"men with glasses\" from \"men without glasses\" and applying it to an image of a \"woman without glasses\", the resulting image is that of a \"woman with glasses\". Experimentation with ARAE-GAN involved generating 1 million sentences and computing vector transforms to change main verbs, subjects, and modifiers. Successful transformations are shown in FIG2, with quantitative evaluation in FIG2 as well. Adversarially regularized autoencoders are presented as a method for training a discrete structure autoencoder with a code-space generative adversarial network. The model combines a structure autoencoder with a code-space generative adversarial network, showing improvements in semi-supervised experiments and text transfer tasks. It also demonstrates a robust latent space for text generation through natural interpolations and vector arithmetic. The model's sensitivity to hyperparameters is noted, and it offers a unique approach to text generation compared to existing models. The framework could potentially be extended to a conditional setting or used for a more interpretable model of language. The model combines a structure autoencoder with a code-space generative adversarial network, showing improvements in semi-supervised experiments and text transfer tasks. It also demonstrates a robust latent space for text generation through natural interpolations and vector arithmetic. The model's sensitivity to hyperparameters is noted, and it offers a unique approach to text generation compared to existing models. The framework could potentially be extended to a conditional setting or used for a more interpretable model of language. The pathway network maps two distributions into a similar one, ensuring the encoder and generator output similar code vectors. As the Wasserstein distance converges, the encoder distribution converges to the generator distribution, with their moments also converging. This simplifies the generated distribution compared to the encoded one, without being overly restrictive like in VAEs. The text discusses Proposition 1 regarding the convergence of distributions on a compact set. It mentions the convergence in distribution and the convergence of all moments for bounded and continuous functions. The text also touches on the encoding and generation of code vectors, as well as a comparison to existing models in text generation. In Las Vegas, a _num_ star rating for _num_ sushi is highly recommended. The best piece of meat ever had, with good food and friendly service. However, some experiences were the worst, with bad food and rude management. The atmosphere is wonderful with great service. The restaurant has a small, intimate, and cozy feel with friendly staff. The menu is extensive, including Italian food. However, some experiences were disappointing with limited options and rude staff. The restaurant is clean with decent food options. The atmosphere is eclectic and full of flavor. There are a few night places to eat nearby. Waiting in line is worth it for a great dining experience. Recommended by a bell boy. The restaurant was recommended by a bell boy and the food was amazing. The menu is good, the staff is friendly, but the service can be slow. The steak was juicy with a side of salsa. The steak was juicy with salsa to balance the flavor, but bland with sauce and mashed potatoes. The person teaching horse control was rude. The fish had too much sauce. The owner was friendly. The staff was awesome. In a variety of scenes, people are engaged in different activities such as sitting in an office, stealing a dinner bag, running in court, eating a balloon animal, trying on a microscope, hugging art, starting a ski race, swimming, waiting for a show, and having a barbecue. There are also dogs sleeping in bed, women tearing over a tree in a cart, and a man walking outside on a dirt road. A large group of people is taking a Christmas photo at night, and someone is avoiding a soccer game. In a variety of scenes, people are engaged in different activities such as sitting in an office, stealing a dinner bag, running in court, eating a balloon animal, trying on a microscope, hugging art, starting a ski race, swimming, waiting for a show, and having a barbecue. There are also dogs sleeping in bed, women tearing over a tree in a cart, and a man walking outside on a dirt road. In another scene, a man and woman dressed for a movie, a person in an empty stadium pointing at a mountain, two children and a little boy with a man in a blue shirt, a boy riding a bicycle, and a girl running in the forest. The text discusses transforming sentences by altering the main verb, subject, and modifier using latent vectors. Successful transformations are shown in FIG2, with quantitative evaluation of success provided. The goal is for generated samples to only differ in the specified transformation, with average word precision calculated for any match."
}