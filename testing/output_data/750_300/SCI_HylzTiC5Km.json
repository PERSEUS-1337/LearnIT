{
    "title": "HylzTiC5Km",
    "content": "The Subscale Pixel Network (SPN) is proposed as a conditional decoder architecture for generating high fidelity images. Autoregressive image models have struggled with large images due to encoding vast context and preserving global coherence and detail exactness. The Subscale Pixel Network (SPN) is a conditional decoder architecture that generates images as a sequence of slices, preserving global coherence and detail exactness. It uses multidimensional upscaling to grow images in size and depth efficiently. SPNs achieve state-of-the-art likelihood results in generating high-quality images. The Subscale Pixel Network (SPN) generates high-quality images through intermediate stages corresponding to distinct SPNs. State-of-the-art likelihood results are achieved in multiple settings, including unconditional generation of CelebAHQ and ImageNet. New benchmark results are set in unexplored settings. Autoregressive models have high fidelity and generalize well on held-out data in various domains such as text, audio, images, and videos. They have achieved state-of-the-art fidelity in many domains, except for unconditional large-scale samples. Autoregressive models have high fidelity and generalize well on held-out data in various domains such as text, audio, images, and videos. However, in the domain of unconditional large-scale image generation, AR samples have yet to manifest long-range structure and semantic coherence. The relationship between MLE scores and sample fidelity poses difficulties for high-fidelity image generation. The relationship between MLE scores and sample fidelity in high-fidelity image generation poses difficulties due to the model's need to support the entire empirical distribution and the high dimensionality of large images. The high dimensionality of large images poses challenges in learning dependencies among positions, requiring significant memory and computation. Multidimensional upscaling techniques are used to map subimages to full-size images. Multidimensional upscaling techniques are used to map subimages to full-size images, requiring large amounts of memory and computation. The goal is to learn the distribution over 8-bit RGB images up to 256 \u00d7 256 size with high fidelity. The study aims to learn the full distribution over 8-bit RGB images up to 256 \u00d7 256 size with high fidelity. It focuses on visually salient subsets of the distribution, including sub-images of smaller size and the most significant bits of each RGB channel. The study focuses on visually salient subsets of the distribution, including sub-images of smaller size and the most significant bits of each RGB channel. Multidimensional Upscaling is used to map between these subsets by upscaling images in size or depth. Three networks are trained: a decoder on the small. The study utilizes Multidimensional Upscaling to upscale a 32 \u00d7 32 3-bit RGB image to a 128 \u00d7 128 8-bit RGB image. Three networks are trained for this process: a decoder for small size, low depth image slices, a size-upscaling decoder, and a depth-upscaling decoder. The Subscale Pixel Network (SPN) architecture is developed to address difficulties in training decoders for upscaling images. SPN divides an image into sub-images and generates the image slice by slice, capturing rich spatial structure. The Subscale Pixel Network (SPN) generates an N x N image slice by slice, capturing rich spatial structure. It consists of a conditioning network and a decoder that predicts a target slice based on context embedding. SPN can act as an implicit size upscaling decoder or an explicit size upscaling network by initializing the first slice. The Subscale Pixel Network (SPN) acts on image slices with shared weights, serving as an independent image decoder with implicit size upscaling. It can also function as an explicit size upscaling network by initializing the first slice separately. The performance of SPN and upscaling methods is evaluated quantitatively and qualitatively on CelebAHQ-256 and ImageNet datasets up to 256 in size, achieving state-of-the-art results on CelebAHQ-256 in terms of MLE scores. From a fidelity perspective, methods are compared quantitatively on CelebAHQ-256 and ImageNet benchmarks up to 256 in size. State-of-the-art results are achieved on CelebAHQ-256 and ImageNet-64, with MLE baselines established for ImageNet-128 and ImageNet-256. Samples produced at full 8-bit resolution show strong benefits of multidimensional upscaling and the Subscale Pixel Network (SPN), matching visual fidelity of GANs. The study demonstrates the benefits of multidimensional upscaling and the Subscale Pixel Network (SPN) in producing high-quality samples at full 8-bit resolution on CelebAHQ-256 and unconditional ImageNet-128 datasets. The results show comparable visual fidelity to GANs, setting a fidelity baseline for future methods. The study highlights the impact of SPN and multidimensional upscaling on sample quality, setting a fidelity baseline for future methods. A standard AR image model like PixelCNN generates a color image using a raster scan ordering conventionally. Each conditional distribution is parametrized by a deep neural network. The study introduces an alternative ordering for image processing, dividing large images into slices to encode long-range dependencies efficiently. This new ordering aligns subsampled slices, creating a spatial structure and allowing for size upscaling. It also facilitates neural network architecture adaptation. The study introduces a new ordering for image processing, dividing large images into slices to encode long-range dependencies efficiently and induce a spatial structure. This allows for size upscaling and facilitates neural network architecture adaptation. The study introduces a new ordering for image processing, dividing large images into slices to encode long-range dependencies efficiently and induce a spatial structure. BID16 in the SPN to be used without local contexts. The subscale ordering is defined as follows: where x < corresponds to all previously generated intensity values according to this ordering. A scaling factor S is selected, and each slice of size H/S \u00d7W/S is obtained by selecting a pixel every S pixels in both height and width. The subscale ordering divides large images into slices by selecting a pixel every S pixels in height and width. This creates S^2 interleaved slices in the original image, each specified by its row and column offset. The ordering captures size upscaling implicitly and can also perform size upscaling explicitly by training a single slice decoder on subimages. The subscale ordering divides large images into slices by selecting a pixel every S pixels in height and width. This creates S^2 interleaved slices in the original image. Size upscaling can be performed explicitly by training a single slice decoder on subimages. The single-slice model can capture the subscale ordering and act as a full-blown image model as well as a size upscaling model. The subscale ordering divides large images into slices by selecting a pixel every S pixels in height and width, creating S^2 interleaved slices. Size upscaling can be done by training a single-slice decoder on subimages, which can act as a full-blown image model and a size upscaling model. Another approach is the Parallel Multi-Scale BID12 ordering, where pixels are doubled at every stage by distinct neural networks in parallel without sequentiality. Multidimensional upscaling also applies upscaling in the channel depth dimension. Multidimensional upscaling involves generating image bits in parallel stages using distinct neural networks, including upscaling in channel depth. The process is performed sequentially, with each stage generating additional significant bits conditioned on the previous bits. In multidimensional upscaling, image bits are generated in parallel stages using separate neural networks. Each stage adds significant bits conditioned on previous bits, including depth upscaling. Weight sharing is not done between stages, and lower significance bits are only generated after more significant bits have been generated. The goal is to allow the model to focus on generating high-quality images. Depth upscaling in multidimensional upscaling generates lower significance bits only after more significant bits have been generated in previous stages. This approach allows the model to focus on visually salient parts of an image, unaffected by less salient and less predictable bits. The method is related to Grayscale PixelCNN, which constructs a representation of the generated context for each pixel dimension. Existing AR approaches require a superlinear amount of computation and memory proportional to the number of pixels. PixelCNN models 4-bit greyscale images subsampled from colored images to construct a representation of the generated context for each pixel dimension. Existing AR approaches have superlinear computation and memory requirements, especially for larger images, leading to limitations in encoding dependencies among variables. Mitigating these requirements often sacrifices global context. The Subscale Pixel Network (SPN) addresses challenges in encoding dependencies among variables in large images by embodying subscale ordering and maintaining global context. The Subscale Pixel Network (SPN) addresses challenges in learning global structure by using a scaling factor to obtain slices of the original image, reducing memory and computation requirements effectively. The Subscale Pixel Network (SPN) uses a scaling factor to obtain slices of the original image, reducing memory and computation requirements effectively. The SPN architecture consists of an embedding part for slices at preceding metapositions that conditions the decoder for the current slice being generated. The embedding part of the Subscale Pixel Network uses a convolutional neural network with residual blocks to condition the decoder for the current slice being generated. Slices are ordered along the channel dimension with empty padding slices to preserve meta-positions, achieving equivariance in the embedding architecture. The embedding part of the Subscale Pixel Network ensures equivariance in the architecture by aligning slices and maintaining input tensor depth. It receives input including meta-position and pixel intensity values, passing through self-attention in the context embedding network. The decoder in the Subscale Pixel Network processes the target slice in raster-scan order using a hybrid architecture combining masked convolution and self-attention. It also utilizes an initial 1D self-attention network. The decoder in the Subscale Pixel Network utilizes a hybrid architecture combining masked convolution and self-attention. An initial 1D self-attention network is employed to gather context in the slice before inputting it to masked 1D self-attention layers. The output is reshaped back into a 2D tensor and concatenated with the output of the slice embedding network. The 1D self-attention layers perform masking over previous pixels only, output is reshaped into a 2D tensor, concatenated with slice embedding network output, and used as conditioning input for Gated PixelCNN. Memory requirements are significantly lower due to smaller spatial size of slices and compact concatenation along the channel dimension. The memory requirements are significantly lower with the smaller spatial size of slices and compact concatenation along the channel dimension. The log-likelihood is derived from equation 2 and decomposes as a sum over slices. Maximum likelihood learning is performed through stochastic gradient descent on a Monte Carlo estimate. The log-loss is calculated by sampling a target slice and evaluating its logprobability conditioned on previous slices. Maximum likelihood learning is done through stochastic gradient descent on this Monte Carlo estimate. The SPN can upscale the size and depth of images, serving as a size-upscaling network. The SPN can be used to upscale the depth of image channels by dividing the image into slices and concatenating them along the channel dimension. PixelCNN (van den Oord et al., 2016c) presents a model capable of generating high-fidelity samples at high resolution by using low bit depth data. The model outperforms the Glow model BID7 in generating unconditional CelebA-HQ samples and improving the MLE scores. Additionally, the results extend to high-resolution ImageNet images, showcasing state-of-the-art performance. Our model can generate high-fidelity samples at high resolution, outperforming the Glow model BID7 in producing unconditional CelebA-HQ samples and improving MLE scores. It also achieves state-of-the-art log-likelihoods for ImageNet images at 128x128 resolution and sets a benchmark for 256x256 ImageNet. The network operates on small 32x32 slices, allowing for training of large networks with deep layers. The context-embedding network contains 5 convolutional layers and 6-8 self-attention layers depending on the dataset. The masked decoder consists of a PixelCNN with 15 layers in all experiments. The 1D Transformer in the decoder has between 8 and 10 layers depending on the dataset. See Table 4 for dataset-specific hyperparameter details. The hybrid decoder alone performs well on 32 \u00d7 32 and 64 \u00d7 64 Downsampled ImageNet datasets. In the low-resolution setting, SPN negatively impacts performance. On 64 \u00d7 64 Downsampled ImageNet, a state-of-the-art log-likelihood of 3.52 bits/dim is achieved. On 64 \u00d7 64 Downsampled ImageNet, achieving state-of-the-art log-likelihood of 3.52 bits/dim. PixelSNAIL and SPN also perform well at this resolution. Experiments use standard ILSVRC Imagenet dataset resized with Tensorflow's function. SPN improves log-likelihood on 128 \u00d7 128 ImageNet from 3.55 bits/dim to 3.08 bits/dim. Samples with depth upscaling show significant semantic coherence. At 128 \u00d7 128, SPN improves log-likelihood on ImageNet samples. Depth upscaling enhances semantic coherence, while multidimensional upscaling boosts success rate. High-fidelity celebrity face samples at 256 \u00d7 256 from CelebAHQ dataset outperform other models like Glow and GANs BID6. Achieved MLE scores show significant improvement. Additional ImageNet samples can be found in the Appendix. At 256 \u00d7 256, high-fidelity celebrity face samples from the CelebAHQ dataset outperform other models like Glow and GANs BID6. Achieved MLE scores show significant improvement. Samples for 8-bit CelebAHQ-256 are showcased in FIG6, with additional samples in Figures 7, 8, and 9 in the Appendix. The challenge of learning the distribution of complex natural images for generative models is a long-standing issue. The SPN and Multidimensional Upscaling model achieves state-of-the-art MLE scores on large-scale images like CelebAHQ-256 and ImageNet-128. It can generate high fidelity 8-bit samples without altering the sampling process. The SPN and Multidimensional Upscaling model achieves state-of-the-art MLE scores on large-scale images like CelebAHQ-256 and ImageNet-128. It can generate high fidelity 8-bit samples without altering the sampling process, showing semantic coherence and exactness of details even at large scale sizes of 128 \u00d7 128 and 256 \u00d7 256 images. The entropy of the softmax output distributions can be artificially reduced for analysis purposes. The entropy of softmax output distributions can be reduced for analysis by using a \"temperature\" divisor on predicted logits. Large-scale experiments are conducted with high compute and network sizes, maintaining batch size and data parallelism. Large batch sizes up to 2048 are achieved by increasing data parallelism on Google Cloud TPU pods. Different numbers of tensorcores are used for different ImageNet datasets. Lower batch sizes and fewer tensorcores are used for small datasets like CelebA-HQ to prevent overfitting. The SPN architectures have between \u223c50M and \u223c250M parameters depending on the dataset. Depth-upscaling in CelebA-HQ doubles the number of parameters by using two separate networks with untied weights. Sizeupscaling adds even more parameters for the separate decoder-only network. In the multidimensional upscaling setting for ImageNet 128, the total parameter count reaches \u223c650M, with the decoder-only network used to model the first slice having \u223c150M parameters."
}