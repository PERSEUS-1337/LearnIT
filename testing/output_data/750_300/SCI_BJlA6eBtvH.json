{
    "title": "BJlA6eBtvH",
    "content": "Continual learning involves sequentially acquiring new knowledge while preserving previously learned information. Catastrophic forgetting is a major challenge for neural networks in dynamic data scenarios. A Differentiable Hebbian Consolidation model addresses this issue by incorporating a Differentiable Hebbian Plasticity Softmax layer to enable rapid learning and memory consolidation. Differentiable Hebbian Consolidation model integrates rapid learning plasticity with fixed parameters to retain learned representations for longer timescales. The approach is evaluated on various benchmarks including Permuted MNIST and Split MNIST datasets. Our proposed model integrates consolidation methods to penalize changes in slow weights important for each target task. It outperforms comparable baselines by reducing forgetting on Permuted MNIST, Split MNIST, and Vision Datasets Mixture benchmarks, including an imbalanced variant of Permuted MNIST. This model requires no additional hyperparameters and addresses the challenge of embedding adaptability into artificial intelligence in dynamic environments. Recent advances in machine learning have shown improvements in solving complex tasks through extensive training on large datasets. However, ML models used in real-world deployment face non-stationarity, leading to performance degradation when trained with new data after learning is complete. After extensive training on large datasets, machine learning models deployed in the real world face non-stationarity, causing performance degradation when trained with new data. This phenomenon, known as catastrophic forgetting, presents a significant challenge for deep neural networks tasked with continual learning. Continual learning poses a challenge for deep neural networks as they need to adapt to new tasks without forgetting previous ones. The assumption of independent and identically distributed samples is easily violated in real-world applications due to concept drift in the training data distribution. Continual learning in ML systems faces challenges when the iid assumption is violated due to concept drift, imbalanced class distributions, and incomplete data. This leads to the \"stability-plasticity dilemma\" for neural networks. Continual learning in ML systems involves balancing stability and plasticity to integrate new knowledge while preserving existing knowledge, known as the \"stability-plasticity dilemma.\" Synaptic plasticity is crucial for learning and memory in biological neural networks, with two major theories explaining human continual learning abilities. Synaptic plasticity is crucial for learning and memory in biological neural networks. Two major theories explain human continual learning abilities: one involves synaptic consolidation in the neocortex, preserving important synaptic parameters, and the other involves task-specific updates of synaptic weights in a neural network. Recent work on differentiable plasticity has shown that neural networks with \"fast weights\" can be trained end-to-end through backpropagation and stochastic gradient descent. This approach aims to consolidate and preserve important synaptic parameters for previously learned tasks, leveraging Hebbian learning rules. The complementary learning system theory suggests that humans store high-level structural information in different brain areas while retaining episodic memories. Recent work on differentiable plasticity has shown that neural networks with \"fast weights\" can be trained end-to-end through backpropagation and stochastic gradient descent to optimize standard \"slow weights\" and the amount of plasticity in each synaptic connection. Slow weights are associated with long-term memory, while fast weights change quickly from one time step to another. Recent work on differentiable plasticity has shown that neural networks use slow weights for long-term memory and fast weights for short-term memory. Fast weights change quickly based on input representations, allowing for reactivation of long-term memory traces. Miconi et al. (2018) demonstrated that networks with learned plasticity outperform those with uniform plasticity. Various approaches have been proposed to address the catastrophic forgetting problem in fixed-capacity networks. Differentiable plasticity in neural networks has been shown to outperform uniform plasticity models. Recent approaches address catastrophic forgetting by adjusting synapse plasticity dynamically. A Differentiable Hebbian Consolidation model is developed for task-incremental continual learning. Differentiable Hebbian Consolidation model is developed for task-incremental continual learning, adapting quickly to changing environments and consolidating previous knowledge by adjusting synapse plasticity. The traditional softmax layer is modified by augmenting slow weights in the final fully-connected layer with plastic weights using Differentiable Hebbian Plasticity. The model's flexibility is demonstrated by combining it with task-specific synaptic approaches. The Differentiable Hebbian Consolidation model enhances the final fully-connected layer with plastic weights using Differentiable Hebbian Plasticity. It combines with task-specific synaptic approaches to overcome catastrophic forgetting and enable rapid adaptation to new data. Our model integrates Hebbian plasticity, synaptic consolidation, and CLS theory to adapt quickly to new data while preserving previous knowledge. Tested on benchmarks like Permuted MNIST, Split MNIST, and Vision Datasets Mixture, it mitigates catastrophic forgetting and leverages compressed episodic memories. Additionally, we introduce the Imbalanced Permuted MNIST problem to showcase the effectiveness of task-specific synaptic approaches in plastic networks. Neural Networks with Non-Uniform Plasticity: Hebbian learning theory explains continuous learning in humans through weight plasticity, modifying synapse strength based on Hebb's rule. Imbalanced Permuted MNIST problem shows task-specific synaptic consolidation outperforming uniform plasticity in plastic networks. Hebbian learning theory suggests that learning and memory are due to synaptic plasticity, strengthening connections between neurons through correlated activation. After learning, synaptic strength increases while plasticity decreases to preserve knowledge. Recent approaches in meta-learning have shown that incorporating fast weights into neural networks can enable one-shot and few-shot learning. Munkhdalai & Trischler (2018) proposed a model with fast weights implemented using Hebbian learning-based associative memory to bind labels to representations. Incorporating fast weights into neural networks enables one-shot and few-shot learning. Munkhdalai & Trischler (2018) proposed a model with fast weights implemented using Hebbian learning-based associative memory to bind labels to representations. Rae et al. (2018) introduced a Hebbian Softmax layer to improve learning of rare classes by interpolating between Hebbian learning and SGD updates. Miconi et al. (2018) suggested differentiable plasticity, optimizing the plasticity of synaptic connections with SGD. Each synapse consists of a slow weight and a plastic (fast) weight. Differentiable plasticity by Miconi et al. (2018) utilizes SGD to optimize synaptic plasticity in addition to fixed weights. This approach, demonstrated on recurrent neural networks, is effective for pattern memorization and maze exploration tasks. However, it has only been tested on meta-learning problems, not continual learning challenges. Our work introduces a method to overcome catastrophic forgetting by augmenting slow weights in the FC layer with fast weights using DHP. We update only the parameters of the softmax output layer to achieve fast learning and preserve knowledge over time. This strategy includes task-specific synaptic consolidation to protect against catastrophic forgetting. To overcome catastrophic forgetting, our work leverages two strategies: task-specific synaptic consolidation and the CLS theory, which involves a dual memory system. The neocortex gradually learns structured representations, while the hippocampus rapidly learns and stores new instances. Several works have been inspired by these strategies. The neocortex learns structured representations, while the hippocampus stores new instances. Task-specific synaptic consolidation inspired works to overcome catastrophic forgetting. Regularization strategies estimate the importance of each synapse for memory retention. Regularization strategies in continual learning estimate the importance of each parameter or synapse to prevent changes to important parameters of previously learned tasks. This is achieved by adding a regularizer to the loss function when learning new tasks, dynamically adjusting plasticity and retaining memories for long timescales. Regularization strategies in continual learning involve adding a regularizer to the loss function to adjust plasticity and prevent changes to important parameters of previously learned tasks. The main difference lies in how the importance of each parameter is computed, with Elastic Weight Consolidation using values from an approximated Fisher information matrix. In Elastic Weight Consolidation (EWC), the importance of each parameter is computed using values from an approximated Fisher information matrix. An online variant of EWC was proposed to improve scalability, while Synaptic Intelligence (SI) offers an online method for computing parameter importance based on cumulative changes in individual synapses. Our work draws inspiration from CLS theory, a computational framework for representing memories with a dual memory system via the neocortex. Memory Aware Synapses (MAS) measures parameter importance by the sensitivity of the learned function to perturbations in parameters, different from traditional methods like SI and EWC. Our work is inspired by CLS theory, which uses a dual memory system in the neocortex and hippocampus. Various approaches based on CLS principles include pseudo-rehearsal, exact or episodic replay, and generative replay methods. Our work is primarily focused on neuroplasticity techniques inspired by CLS theory for mitigating catastrophic forgetting. Previous methods such as iCaRL utilize rehearsal and regularization with external memory, while our approach aims to leverage CLS principles for memory retention. In our work, we focus on neuroplasticity techniques inspired by CLS theory to address catastrophic forgetting. Previous research has shown how synaptic connections with fixed and fast-changing weights can help store long-term knowledge and prevent memory loss during continual learning. Recent studies have explored replacing soft attention mechanisms with fast weights in RNNs to improve memory retention. The approach involving slow and fast weights, inspired by CLS theory, aims to overcome catastrophic forgetting during continual learning. Recent research has explored various methods such as replacing soft attention mechanisms with fast weights in RNNs, Hebbian Softmax layer, augmenting slow weights with a fast weights matrix, differentiable plasticity, and neuromodulated differentiable plasticity. The methods discussed include Hebbian Softmax layer, differentiable plasticity, and neuromodulated differentiable plasticity. These methods focus on rapid learning on simple tasks or meta-learning over a distribution of tasks, with a few examples seen by the network during training. The Hebbian Softmax layer adapts its parameters for one-shot and few-shot learning tasks by switching between Hebbian and SGD updates based on a scheduling scheme. However, in continual learning setups with frequent exposure to a large number of examples per class, the fast weights memory storage effect diminishes as the network relies more on SGD updates. In continual learning setups, the Hebbian Softmax layer switches to SGD updates when facing the same class, reducing the impact of fast weights memory storage. The goal is to metalearn a local learning rule for the fast weights using fixed weights and an SGD optimizer. Each synaptic connection in the softmax layer has slow weights and a Hebbian plastic component of the same cardinality. In our model, each synaptic connection in the softmax layer has slow weights \u03b8 and a Hebbian plastic component with a scaling parameter \u03b1 and Hebbian traces accumulating hidden activations. The model includes slow weights \u03b8 and a Hebbian plastic component with scaling parameter \u03b1. Traces accumulate hidden activations to compute post-synaptic activations and predicted probabilities using softmax function. The \u03b7 parameter dynamically adjusts learning rate for plastic connections and acts as a decay term. The \u03b7 parameter in Eq. 3 dynamically learns the learning rate for plastic connections and acts as a decay term for the Hebbian traces to prevent instability. Network parameters are optimized by gradient descent in continual learning setup. In the continual learning setup, network parameters are optimized by gradient descent. The model uses Hebbian weight updates and activation levels to update weights for different tasks. Our model utilizes Hebbian weight updates and hidden activations to update weights for different tasks in a continual learning setup. During training, Hebbian traces are updated, while at test time, the most recent traces are used for predictions. The model accumulates hidden activations directly into the softmax output layer weights when a class is encountered, leading to improved initial representations and retention of learned information. Our model utilizes Hebbian weight updates and hidden activations to update weights for different tasks in a continual learning setup. It explores an optimization scheme where hidden activations are accumulated directly into the softmax output layer weights when a class is seen, resulting in better initial representations and longer retention of learned deep representations. This fast learning, enabled by a highly plastic weight component, improves test accuracy for a given task. The model utilizes Hebbian weight updates and hidden activations for continual learning. Fast learning with a plastic weight component improves test accuracy. The plastic component decays between tasks to prevent interference, but selective consolidation protects old memories. DHP Softmax is advantageous for its simplicity and scalability with increasing tasks. The DHP Softmax method is simple to implement and scales easily with increasing tasks. Hebbian updates are used for continual learning, with hidden activations averaged for each class to store memory traces efficiently. The DHP Softmax method utilizes Hebbian updates for continual learning, accumulating hidden activations for each class to store memory traces efficiently. This approach improves learning of rare classes and speeds up the process. The DHP Softmax method utilizes Hebbian updates for continual learning, improving learning of rare classes and speeding up the process. It forms a compressed episodic memory in the Hebbian traces to reflect individual episodic memory traces without introducing additional hyperparameters. The DHP Softmax method utilizes Hebbian updates for continual learning, improving learning of rare classes and speeding up the process. It forms a compressed episodic memory in the Hebbian traces to reflect individual episodic memory traces without introducing additional hyperparameters. Following existing regularization strategies like EWC, Online EWC, SI, and MAS, the loss L(\u03b8) is regularized and synaptic importance parameters are updated in an online manner. The updated quadratic loss for Hebbian Synaptic Consolidation is derived, showing that network parameters represent weights of connections between pre-and post-synaptic activities of neurons. Task-specific consolidation approaches are adapted without computing synaptic importance parameters. When training the first task, the synaptic importance parameter was set to 0 for most consolidation methods except for SI, which estimates the parameter during training. Other methods like Online EWC and MAS compute the parameter after learning a task. The synaptic importance parameter, \u2126 i,j, was set to 0 for most consolidation methods except for SI, which estimates it during training. Our model's plastic softmax layer helps prevent catastrophic forgetting by optimizing connection plasticity. Experiments compared our approach to vanilla neural networks with Online EWC, SI, and MAS. In experiments, the model's plastic weights increase DNN capacity. An extra set of slow weights is added to the softmax layer for fair evaluation. Tested on Permuted MNIST, Split MNIST, and Vision Datasets Mixture benchmarks. Our model's performance was evaluated on various benchmarks including Permuted MNIST, Split MNIST, and Vision Datasets Mixture. We introduced the Imbalanced Permuted MNIST problem and assessed memory retention and flexibility by measuring test performance on the first and most recent tasks. Forgetting was also measured using backward transfer. The study evaluated the model's performance on different benchmarks like Permuted MNIST, Split MNIST, and Vision Datasets Mixture. Memory retention and flexibility were assessed by measuring test performance on the initial and latest tasks, with forgetting measured using backward transfer metrics. The study evaluated the model's performance on different benchmarks like Permuted MNIST, Split MNIST, and Vision Datasets Mixture. Memory retention and flexibility were assessed by measuring test performance on the initial and latest tasks, with forgetting measured using backward transfer metrics. The test classification accuracy on task i after sequentially finishing learning the T th task is analyzed. BWT < 0 indicates catastrophic forgetting, while BWT > 0 shows improvement in learning new tasks. Neural networks were trained with Online EWC, SI, and MAS consolidation methods in a sequential manner. Hyperparameters of the consolidation methods remain consistent with and without DHP Softmax, and plastic components are not regularized. In the Permuted MNIST benchmark, MNIST pixels are permuted differently for each task with a fixed random permutation, causing concept drift. A multi-layered perceptron (MLP) network with two hidden layers is used, along with a cross-entropy loss function. The plastic components are not regularized, and the input distribution changes between tasks. Hyperparameters and details for all benchmarks can be found in Appendix A. In Permuted MNIST benchmarks, a multi-layered perceptron (MLP) network with two hidden layers is used with a cross-entropy loss. The plastic component's \u03b7 value is set at 0.001 without much tuning effort. Performance is compared between the network with DHP Softmax and a fine-tuned vanilla MLP network (Finetune) in Figure 2a, showing improvement with DHP Softmax alone. The network with DHP Softmax showed improvement in alleviating catastrophic forgetting compared to the baseline network. DHP Softmax with consolidation maintained higher test accuracy throughout the learning of new tasks. The DHP Softmax with consolidation method maintains higher test accuracy during sequential task learning compared to without DHP Softmax. An ablation study examines network structural parameters and Hebb traces for interpretability. The behavior of \u03b7 during training on Permuted MNIST benchmark tasks is shown in Figure 8. The behavior of \u03b7 during training on Permuted MNIST benchmark tasks is shown in Figure 8, providing insights into the model's plasticity and interference prevention mechanisms. The Frobenius Norm of the Hebb trace suggests that Hebb grows without runaway positive feedback when a new task is learned, maintaining a memory of recent synapse activity. The plasticity coefficients grow within each task, indicating the network leverages the structure in the plastic component through gradient descent and backpropagation for meta-learning. The network leverages the structure in the plastic component through gradient descent and backpropagation for meta-learning. The Imbalanced Permuted MNIST problem introduces an imbalanced distribution where training samples in each class are artificially removed, motivated by the challenges of class imbalance and concept drift occurring simultaneously. The benchmark study involved artificially removing samples in each class based on random probability to address class imbalance and concept drift challenges. DHP Softmax achieved 80.85% accuracy after learning 10 tasks with imbalanced class distributions, showing a significant 4.41% improvement over the standard neural network baseline. The compressed episodic memory mechanism in Hebbian traces played a crucial role in this benchmark. In a benchmark study, DHP Softmax with MAS achieved a 0.04 decrease in BWT, resulting in an average test accuracy of 88.80% and a 1.48% improvement over MAS alone. The compressed episodic memory mechanism in Hebbian traces allows rare classes to be remembered for a longer period of time, showing significant improvement over all other methods across tasks. The original MNIST dataset was split into 5 binary classification tasks. In a benchmark study, DHP Softmax with MAS achieved a 0.04 decrease in BWT, with an average test accuracy of 88.80% and a 1.48% improvement over MAS alone. The MNIST dataset was split into 5 binary classification tasks, each with disjoint output spaces. An MLP network with two hidden layers of 256 ReLU nonlinearities was used, with an initial \u03b7 value of 0.001. Different values of \u03b7 yielded similar final test performance. In a study by Zenke et al. (2017b), an MLP network with two hidden layers of 256 ReLU nonlinearities and a cross-entropy loss was used. Different values of \u03b7 had similar final test performance after learning 5 tasks. DHP Softmax alone achieved 98.23% test performance, a 7.80% improvement over a finetuned MLP network. Combining DHP Softmax with task-specific consolidation consistently decreased BWT and increased average test accuracy across all tasks, especially T 5. Combining DHP Softmax with task-specific consolidation consistently decreases BWT and improves average test accuracy across all tasks, especially the most recent one, T 5. Continual learning is performed on a sequence of 5 vision datasets: MNIST, notMNIST, FashionMNIST, SVHN, and CIFAR-10. MNIST, notMNIST, and FashionMNIST datasets are zero-padded to match the resolution of SVHN and CIFAR-10 images. The MNIST, notMNIST, and FashionMNIST datasets are zero-padded to match the resolution of SVHN and CIFAR-10 images. A CNN architecture similar to previous studies is used, with an initial \u03b7 parameter value of 0.0001. The network is trained with mini-batches of size 32 using plain SGD with a fixed learning rate of 0.01 for 50 epochs per task. DHP Softmax combined with MAS decreases BWT by 0.04, resulting in a 2.14% improvement in average test accuracy over MAS alone. After training the network with mini-batches and plain SGD, DHP Softmax combined with MAS decreased BWT by 0.04, resulting in a 2.14% improvement in average test accuracy over MAS alone. SI with DHP Softmax outperformed other methods with an average test performance of 81.75% and a BWT of -0.04 after learning all five tasks. The final average test performance after learning all tasks in continual learning problems is summarized in Table 1. Adding compressed episodic memory in the softmax layer through DHP and task-specific updates on synaptic parameters can alleviate catastrophic forgetting in continual learning environments. This allows new information to be learned without interference, improving average test accuracy and reducing BWT. The addition of compressed episodic memory in the softmax layer through DHP and task-specific updates on synaptic parameters helps prevent interference when learning new information, improving generalization across experiences. The \u03b1 parameter in the plastic component dynamically adjusts the plastic connections, balancing the retention of old knowledge and the acquisition of new information efficiently. The neural network with DHP Softmax demonstrates significant enhancement in performance. The plastic component in the neural network with DHP Softmax adjusts the plastic connections to protect old knowledge or acquire new information quickly. It outperforms traditional softmax layers with slow changing weights across all benchmarks. The DHP Softmax does not require additional hyperparameters and the initial tuning effort for setting the \u03b7 value is minimal. The DHP Softmax in neural networks adjusts plastic connections to protect old knowledge or acquire new information quickly. It outperforms traditional softmax layers with slow changing weights. DHP Softmax does not introduce extra hyperparameters, and setting the initial \u03b7 value requires minimal tuning effort. Additionally, the model's flexibility allows for Hebbian Synaptic Consolidation using EWC, SI, or MAS to alleviate catastrophic forgetting after learning multiple tasks sequentially. DHP Softmax combined with SI shows superior performance. Hebbian Synaptic Consolidation can be improved by regularizing slow weights using EWC, SI, or MAS to alleviate catastrophic forgetting after learning multiple tasks sequentially. DHP Softmax combined with SI outperforms other consolidation methods on Split MNIST and 5-Vision Datasets Mixture. Combining DHP Softmax and MAS leads to superior results on Permuted MNIST and Imbalanced Permuted MNIST benchmarks. MAS computes synaptic importance parameters based on Hebb's rule layer by layer. Our model, utilizing Hebbian plasticity, consistently outperforms other methods on Permuted MNIST and Imbalanced Permuted MNIST benchmarks by computing synaptic importance parameters based on Hebb's rule. This approach leads to lower negative BWT and higher average test accuracy, indicating that Hebbian plasticity enables neural networks to continually learn and remember distant memories. Our model, utilizing Hebbian plasticity, consistently exhibits lower negative BWT across benchmarks, leading to higher average test accuracy. Hebbian plasticity enables neural networks to learn continually, remember distant memories, and reduce catastrophic forgetting in dynamic environments. Continual synaptic plasticity plays a key role in learning from limited labelled data and adapting at long timescales, opening new investigations into gradient descent optimized Hebbian consolidation for learning and memory in DNNs. Our work focuses on utilizing Hebbian consolidation for continual learning in neural networks. Unlike traditional supervised learning, continual learning involves training on sequential tasks in chunks. This approach aims to improve learning from limited labeled data and adapt at long timescales. Continual learning trains a model on sequential tasks in chunks, each with its own training data and task-specific loss. The model learns an approximated mapping to prevent forgetting and maps new inputs to target outputs for all tasks. After training on sequential tasks with task-specific loss, the model learns an approximated mapping to prevent forgetting and maps new inputs to target outputs for all tasks. The experiments were conducted on Nvidia Titan V or Nvidia RTX 2080 Ti, training with mini-batches of size 64 and plain SGD with a learning rate of 0.01 for tasks T n=1:10. The experiments were conducted on Nvidia Titan V or Nvidia RTX 2080 Ti, training with mini-batches of size 64 and plain SGD with a learning rate of 0.01 for tasks T n=1:10. Hyperparameters included a regularization hyperparameter \u03bb for Permuted MNIST experiments. For Permuted MNIST experiments, after 5 epochs, training on task Tn was terminated, network weights and Hebbian traces were reset to values with lowest test error. Hyperparameters included \u03bb=100 for Online EWC, \u03bb=0.1 for SI, and \u03bb=0.1 for MAS. SI method used \u03bb as c parameter, with damping parameter \u03be set to 0.1. For synaptic consolidation methods like Online EWC, SI, and MAS, hyperparameters were optimized through grid search. Different values of \u03bb were tested for each method. In the Imbalanced Permuted MNIST problem, training samples were artificially removed from each class. In Online EWC, values of \u03bb were tested from 10 to 400, SI -\u03bb from 0.01 to 1.0, and MAS -\u03bb from 0.01 to 2.0. Training samples were removed based on random probabilities for each class in the Imbalanced Permuted MNIST problem. The distribution of classes for tasks T n=1:10 is shown in Table 2. The distribution of classes in each imbalanced dataset for tasks T n=1:10 is shown in Table 2, with the number of samples for each class listed. For the Imbalanced Permuted MNIST experiments in Figure 5, the regularization hyperparameter \u03bb for each task-specific consolidation method is set to \u03bb = 400 for Online EWC. For the Imbalanced Permuted MNIST experiments in Figure 5, regularization hyperparameters are set as follows: \u03bb = 400 for Online EWC, \u03bb = 1.0 for SI, and \u03bb = 0.1 for MAS. Additionally, a grid search was conducted to find the best hyperparameter combination for each synaptic consolidation method. In a grid search for synaptic consolidation methods, hyperparameters were tested for Online EWC, SI, and MAS. For Split MNIST experiments, \u03bb was set at 400 for Online EWC and 1.0 for SI. For Split MNIST experiments, the regularization hyperparameters \u03bb were set at 400 for Online EWC, 1.0 for SI, and 1.5 for MAS. Grid search was conducted to find the best hyperparameter combination for each synaptic consolidation method. In a grid search for synaptic consolidation methods, values of \u03bb were tested for Online EWC, SI, and MAS. The network was trained on a sequence of 5 tasks with mini-batches of size 64 using plain SGD with a fixed learning rate. The Vision Datasets Mixture benchmark includes tasks from MNIST, notMNIST, FashionMNIST, SVHN, and CIFAR-10 datasets. The notMNIST dataset consists of font glyphs corresponding to letters 'A'. The Vision Datasets Mixture benchmark includes tasks from MNIST, notMNIST, FashionMNIST, SVHN, and CIFAR-10 datasets. The notMNIST dataset consists of font glyphs corresponding to letters 'A' to 'J'. FashionMNIST consists of 10 categories of clothing articles, while SVHN consists of digits '0' to '9' from Google Street View images. The CNN architecture for the Vision Datasets Mixture benchmark includes 2 convolutional layers with 20 and 50 channels, each followed by LeakyReLU nonlinearities. FashionMNIST has 60,000 training and 10,000 testing grayscale images of size 28\u00d728. SVHN has 73,257 training and 26,032 testing color images of size 32\u00d732. CIFAR-10 has 50,000 training and 10,000 testing color images from 10 categories of size 32\u00d732. The CNN architecture for the Vision Datasets Mixture benchmark includes 2 convolutional layers with 20 and 50 channels, LeakyReLU nonlinearities, max-pooling operations, and a final softmax output layer. A multi-headed approach was used due to different class definitions between datasets. The model has a trainable parameter for this benchmark. For the Vision Datasets Mixture benchmark, a multi-headed approach was used with a trainable parameter for each connection in the final output layer. Separate parameters for each connection improved optimization stability and test performance, allowing for individual modulation of plasticity rates. Using a single parameter for all connections led to instability. For the Vision Datasets Mixture benchmark, separate parameters for each connection improved optimization stability and test performance. Individual modulation of plasticity rates was observed, preventing instability. Hyperparameters included \u03bb = 100 for Online EWC, \u03bb = 0.1 for SI, and \u03bb = 1.0 for MAS. In SI, the damping parameter, \u03be, was set to 0.1. For each task-specific consolidation method, different values of \u03bb were used: \u03bb = 100 for Online EWC, \u03bb = 0.1 for SI, and \u03bb = 1.0 for MAS. A random search was conducted to find the best hyperparameter combination for these methods, with varying values of \u03bb tested. Additionally, a sensitivity analysis was performed on the Hebb decay term \u03b7 to assess its impact on test performance. The sensitivity analysis was conducted on the Hebb decay term \u03b7, testing values of \u03bb and its effect on the final test performance after learning tasks in a continual learning setup. Low values of \u03b7 were found to lead to the best performance. In a sequential manner, performance was evaluated after learning tasks for Permuted MNIST and Imbalanced Permuted MNIST benchmarks. Low values of \u03b7 were found to be optimal for alleviating catastrophic forgetting. Sensitivity analysis on the \u03b7 parameter was also conducted for the Split MNIST problem. Table 4 shows average test accuracy for MNIST-variant benchmarks. Table 4 displays average test accuracy for MNIST-variant benchmarks, corresponding to sensitivity analysis plots in Figure 4. The code snippet includes parameters for learning rate and Hebbian traces. The PyTorch implementation of the DHP Softmax model initializes fixed weights with He initialization and sets the learning rate of plastic connections. The PyTorch implementation of the DHP Softmax model adds compressed episodic memory to the final output layer of a neural network through plastic connections. The model was trained on the full CIFAR-10 dataset and sequentially on 5 additional tasks from the CIFAR-100 dataset. The DHP Softmax model, implemented in PyTorch, incorporates compressed episodic memory into the final output layer of a neural network. It was trained on the full CIFAR-10 dataset and sequentially on 5 additional tasks from the CIFAR-100 dataset. The test accuracies show that DHP Softmax outperforms Finetune in class-incremental learning, sometimes even performing as well as training from scratch. DHP Softmax outperforms Finetune on tasks in class-incremental learning. It sometimes performs as well as training from scratch. Test accuracies were compared with Finetune, training from scratch, and SI."
}