{
    "title": "BJeOioA9Y7",
    "content": "In this paper, a new approach called knowledge flow is developed to transfer knowledge from multiple deep nets (teachers) to a new deep net model (student). The student can be trained independently of the teachers and shows improved performance on various supervised and reinforcement learning tasks. The knowledge flow approach transfers knowledge from multiple deep nets to a new deep net model (student), allowing independent training. It outperforms other methods on supervised and reinforcement learning tasks. Research communities have developed numerous deep net architectures for various tasks, with new ones constantly being introduced. Some architectures are trained from scratch, while others are fine-tuned using pre-trained deep nets. In reinforcement learning, teachers play a role in models like progressive neural net and PathNet, where they help extract useful features for new tasks. Several deep net architectures have been developed for various tasks, including progressive neural net, PathNet, 'Growing a Brain', Actor-mimic, and Knowledge distillation. These architectures utilize techniques such as keeping multiple teachers, genetic algorithms for pathway selection, fine-tuning while growing network capacity, pre-training on source tasks, and distilling knowledge from ensembles. Progressive neural net models, PathNet, Actor-mimic, and Knowledge distillation are deep net architectures with various techniques like pre-training on source tasks and distilling knowledge from ensembles. However, these techniques have limitations such as growing number of parameters in progressive neural nets and computational intensity in PathNet. The limitations of current deep net architectures like progressive neural nets, PathNet, Actor-mimic, and Knowledge distillation include the growing number of parameters, computational intensity, and reliance on a single pretrained model. To address these issues, a new approach called knowledge flow has been developed to transfer knowledge from multiple teachers to a student during training, ensuring independence of the student at the final stage. Our approach, knowledge flow, transfers knowledge from multiple teachers to a student during training, ensuring independence of the student at the final stage. It allows flexibility in choosing teacher models and is applicable to various tasks from reinforcement learning to fully-supervised training. Our approach, knowledge flow, enables flexibility in selecting teacher models and is applicable to tasks ranging from reinforcement learning to fully-supervised training. It aims to transfer knowledge from multiple teachers to a student, ensuring independence of the student at the final stage. The goal of reinforcement learning is to find a policy that maximizes the expected future reward from each state. The policy mapping in reinforcement learning is obtained from a probability distribution over states, modeled by a deep net with parameters \u03b8 \u03c0. The value function is approximated by a deep net with parameters \u03b8 v. The goal is to maximize the expected future reward from each state. The policy mapping in reinforcement learning is modeled by a deep net with parameters \u03b8 \u03c0, and the value function is approximated by a deep net with parameters \u03b8 v. The optimization of policy parameters \u03b8 \u03c0 involves a loss function based on negative log-likelihood and a negative entropy regularizer. The empirical k-step return is calculated based on the trajectory generated by following the policy. The optimization of policy parameters in reinforcement learning involves minimizing the empirical expectation of the policy and value function. This is achieved by addressing alternating loss functions to learn a policy and value function that maximize expected return. The proposed framework, knowledge flow, transfers knowledge from multiple deep nets (teachers) to a deep net under training (student) to improve learning efficiency. This involves average normalized weights for teachers' and student's layers, enhancing the student's performance during training. Knowledge flow is a framework that transfers knowledge from multiple deep nets (teachers) to a deep net under training (student). The student initially relies heavily on teacher one, but as training progresses, the student becomes independent. This process is illustrated on example deep nets in FIG0. The student net's parameters are randomly initialized, while the teachers' parameters are fixed and obtained from pre-trained models on different source tasks. The knowledge flow framework transfers knowledge from multiple deep nets (teachers) to a deep net under training (student). Teachers have fixed parameters obtained from pre-trained models on different tasks. The student net's parameters are randomly initialized. Knowledge is transferred by adding transformed representations from teachers to the student net. Knowledge is transferred from multiple pre-trained deep nets (teachers) to a student net by modifying the student net with transformed and scaled teacher representations. This is achieved by adding teacher representations that are transformed and scaled using trainable parameters, allowing the student net to benefit from the teachers' knowledge at different levels of abstraction. The student net is modified with transformed and scaled teacher representations, encoded with normalized weights to trust different levels of abstraction. The student gradually captures all knowledge without relying on teachers after training. During training, the student initially relies heavily on the teacher's knowledge but becomes more independent as it progresses. By the final stages, the student must master the task on its own. This transition is encouraged through additional loss functions. During training, the student transitions from relying on teachers to mastering tasks independently. Additional loss functions encourage this shift by capturing the student's dependence on teachers and ensuring stable behavior when teacher influence decreases. The second part of the text discusses using loss KL to capture changes in student behavior when teacher influence decreases. It combines student net modifications and additional loss terms for supervised and reinforcement learning tasks. The transformed program includes cross-connections and depends on parameters from current and previous iterations. The text discusses modifying the deep net with cross-connections and parameters from current and previous iterations to decrease teacher influence. Parameters \u03bb 1 and \u03bb 2 control the strength of influence, with \u03bb 1 gradually increasing during training. No assumptions are made about the teacher and student's objectives. The method involves gradually reducing the influence of teachers on students during training by adjusting parameter \u03bb 1. Despite potential differences in objectives, low-level knowledge transfer from teachers to students is observed in experiments. During training, the method gradually reduces the influence of teachers on students by adjusting parameter \u03bb 1. Despite differences, students could benefit from low-level knowledge transfer from teachers, observed in experiments. The process involves modifying deep nets and using loss functions dep and KL to decrease teacher influence on student layers. Candidate sets are defined for each layer in the student model, combining layers from teachers to be considered. During training, the method gradually reduces the influence of teachers on students by adjusting parameter \u03bb 1. Candidate sets are defined for each layer in the student model, combining layers from teachers to be considered. To decide which teachers' or the student's representation to trust at every layer of the student net, normalized weights are introduced. The combined intermediate representation of layer j for the student model is obtained using a specific formula. The maximal number of introduced matrices Q in the framework is limited. The student deep net combines intermediate representations from different layers of the teacher network. The number of introduced matrices in the framework is limited. Not every layer of the teacher network is linked to a student's layer. Results from PathNet and PNN are approximated from plots. Features from state-of-the-art methods like A3C, PPO, and ACKTR are likely irrelevant to a student's top layer. In practice, it is recommended to link one teacher layer to one or two student layers, introducing matrices Q. Additional trainable parameters Q and w are introduced in the framework but are not part of the resulting student network. In practice, it is recommended to link one teacher layer to one or two student layers, introducing matrices Q. Additional trainable parameters Q and w are introduced in the framework but are not part of the resulting student network. During training, the influence of the teachers is gradually decreased to encourage the student to become independent. During the final stage of training, the student becomes independent and no longer relies on teachers or transformed representations. The influence of teachers is decreased by encouraging the student's weights to increase, promoting independence. By minimizing dependence cost, the student's weights increase, promoting independence. However, decreasing the influence of teachers too quickly can degrade performance as it takes time to find good transformations. To prevent performance degradation, it is important to not decrease the influence of the teacher too quickly. Using a Kullback-Leibler regularizer can help maintain the student's output distribution. This approach is evaluated in both supervised and reinforcement learning tasks. In supervised learning, parameters \u03b8 are updated, while in reinforcement learning, policies are updated. Knowledge flow is evaluated on both tasks using only the student model to avoid teacher influence. Reinforcement learning is tested on Atari games with raw image inputs, where the agent learns to predict actions based on rewards. The agent learns to predict actions in Atari games using raw images as input. It chooses actions every four frames based on rewards. The model architecture includes three hidden layers with convolutional and fully connected layers. The model architecture consists of three hidden layers: a convolutional layer with 16 filters of size 8x8 and stride 4, a convolutional layer with 32 filters of size 4x4 and stride 2, and a fully connected layer with 256 hidden units. It outputs a softmax distribution over actions and a scalar value function estimate. The hyper-parameter settings are similar to BID17, except for using Adam with shared statistics instead of RMSProp. The estimated value function is provided using Adam with shared statistics, with a learning rate gradually decreased to zero. \u03bb 1 and \u03bb 2 are selected following progressive neural net BID23, with \u03bb 1 linearly increased during training. Each experiment is repeated 25 times with different random seeds and \u03bb 1 values sampled randomly. The training process involves sampling \u03bb 1 from {0.05, 0.1, 0.5} and \u03bb 2 from {0.001, 0.01, 0.05}. \u03bb 1 starts at zero and increases linearly during training. Experiments are repeated 25 times with different seeds and \u03bb values. Evaluation includes playing each game for 30 episodes and using the 'no-op' procedure. Comparisons are made with PathNet. The evaluation procedure of BID16 involves testing student models by playing each game for 30 episodes, including a 'no-op' procedure. Results show that compared to PathNet and progressive neural net (PNN) BID23, the transfer framework achieves higher scores in 11 out of 14 experiments with one teacher. In comparison to PathNet and PNN, our transfer framework with one teacher achieves higher scores in 11 out of 14 experiments. With a two-teacher framework, our student model has significantly fewer parameters than PNN but still outperforms it in five out of seven experiments. Knowledge flow effectively transfers from teachers to the student, improving performance as the number of teachers increases. The results show that knowledge flow effectively transfers from teachers to students, with performance improving as the number of teachers increases. Training curves are depicted in FIG1, showing consistent performance. Different environment/teacher settings were tested, not used by PathNet or progressive neural network, with results summarized in TAB2. The results of different environment/teacher settings experiments are summarized in TAB2. Our A3C implementation achieves better scores than those reported by BID17 for most games. Our A3C implementation outperforms the scores reported by BID17 for most games. Knowledge flow with expert teachers shows better performance than the baseline, indicating successful transfer of knowledge. Additionally, knowledge flow with non-expert teachers also outperforms fine-tuning on a non-expert teacher. Knowledge flow transfers knowledge from expert and non-expert teachers to students, outperforming fine-tuning methods. Students can learn from multiple teachers in knowledge flow, avoiding negative impacts from insufficiently pretrained teachers. Training curves are shown in FIG5, with more in the Appendix. Students benefit from knowledge flow by avoiding performance degradation. Fine-tuning from an insufficiently pretrained model can slow down training and degrade performance. In knowledge flow, students benefit from intermediate representations of teachers, even with different input, output, and objectives. For example, in FIG5, students learning from teachers like Chopper Command and Space Invaders achieve scores ten times larger than without a teacher. The student model benefits from learning from teachers like Command and Space Invaders, achieving scores ten times larger than without a teacher. Supervised learning is done on various image classification benchmarks, with parameters determined using the validation set. Evaluation is based on top-1 error rate on the test set. The student model is trained using supervised learning on image classification benchmarks like CIFAR-10 and CIFAR-100. Parameters are determined using the validation set, and evaluation is based on the top-1 error rate on the test set. The datasets consist of colored images of size 32 \u00d7 32, with CIFAR-10 having 10 classes and CIFAR-100 having 100 classes. Training and test sets contain 50,000 and 10,000 images respectively. Experiments are conducted with standard data augmentation using Densenet (depth 100, growth rate 24) as a baseline. For experiments on CIFAR-10 and CIFAR-100, training and test sets have 50,000 and 10,000 images. Densenet (depth 100, growth rate 24) is used as a baseline for training teacher and student models. Fine-tuning from CIFAR-100 expert improves 4% over the baseline for the CIFAR-10 target task. Fine-tuning from the CIFAR-100 expert improves 4% over the baseline for the CIFAR-10 target task, while fine-tuning from the SVHN expert performs worse than the baseline model. Knowledge flow can leverage a good teacher's 'knowledge' and avoid misleading influence, improving by 13% over the baseline. Knowledge flow improves by 13% over the baseline when presented with both good and inadequate teachers, demonstrating its ability to leverage a good teacher's knowledge and avoid misleading influence. Results on the CIFAR-100 dataset are similar, and additional findings are detailed in the appendix. Previous work on knowledge transfer is briefly discussed, with further details in Sec. 8. PathNet BID6 allows multiple agents to train the same deep net while reusing parameters and preventing catastrophic interference. In contrast to PathNet BID6, our method utilizes multiple pre-trained teacher nets and ensures independence of the student during training, addressing a limitation in Progressive Net BID23. Our method addresses a limitation in BID23 by ensuring independence of the student during training. Distral, a neologism combining 'distill & transfer learning', considers joint training of multiple tasks with a shared policy to encourage consistency between different tasks. In Distral, multiple tasks are jointly trained with a shared policy to promote consistency. Knowledge flow, on the other hand, focuses on transferring information from multiple teachers to help a student learn a new task. In multi-task learning, information from different tasks is shared to improve performance. Knowledge flow leverages information from multiple teachers to help a student learn a new task. Various related works include actor-mimic, learning without forgetting, and knowledge distillation. A general knowledge flow approach allows training a deep net from multiple teachers. The authors developed a general knowledge flow approach for training a deep net from multiple teachers, showing improvements in reinforcement learning and supervised learning. They plan to explore when to use different teachers and how to swap them during student training. Knowledge distillation is used to transfer knowledge from larger to smaller models, with student models having fewer parameters. Experiments were conducted on MNIST dataset. The study explores using multiple teachers to train a deep net, focusing on when to switch teachers during student training. Knowledge distillation transfers knowledge from larger to smaller models, with student models having fewer parameters. Experiments were conducted on MNIST, MNIST with missing digit '3', CIFAR-100, and ImageNet datasets. The student model, an MLP with two hidden layers of 800 units, follows the structure of the teacher model but with halved output channels. The teacher model for CIFAR-100 is from Chen, while for ImageNet, it is a 50-layer ResNet compared to the student's 18-layer ResNet. The distilled student model consistently outperforms Knowledge Distillation (KD) due to benefiting from both the output layer and intermediate layer representations of the teacher. The 'EMNIST Letters' dataset contains 26 balanced classes of handwritten letters in 28x28 pixel images, with training and test sets of 124,800 and 20,800 images respectively. The 'EMNIST Digits' dataset includes 10 balanced classes of handwritten digits in 28x28 pixel images, with training and test sets of 240,000 and 40,000 images respectively. The 'EMNIST Digits' dataset consists of 10 balanced classes of handwritten digits in 28x28 pixel images, with training and test sets of 240,000 and 40,000 images respectively. The study uses the MNIST model as a baseline, teacher, and student model, training teachers on various EMNIST datasets. The student model is trained with different teachers, and the results are compared to fine-tuning and state-of-the-art results on EMNIST. The study compares student learning with different teachers on the EMNIST Letters dataset to fine-tuning and baseline models. Results show better performance with expert, semi-expert, and non-expert teachers. Training accuracy over epochs is illustrated in FIG6. The STL-10 dataset has 10 balanced classes with colored images of size 96x96 pixels. It contains 5,000 labeled images and 100,000 unlabeled images. In the experiment, the STL-10 dataset with 10 balanced classes and colored images of size 96x96 pixels was used. It contained 5,000 labeled images and 100,000 unlabeled images. The training set only utilized the 5,000 labeled images. Teachers were trained on CIFAR-10 and CIFAR-100, which were found to be good teachers for the task. The results were compared to fine-tuning and baseline models in Table 6. In the experiment, teachers were trained on CIFAR-10 and CIFAR-100, which were effective for the task. Results were compared to fine-tuning and baseline models in Table 6. Our approach on the STL-10 dataset showed a significant reduction in test errors compared to the baseline, with further improvement in student model training within our framework. Fine-tuning from pretrained weights on CIFAR-10 and CIFAR-100 reduced test errors by over 10%. Student model training in the framework further decreased test error by 3%. Results are based on labeled data only, making direct comparisons challenging. Comparison to Distral BID26, a multi-task reinforcement learning framework, was also conducted. In Fig. 5, accuracy over training epochs is shown for a multi-task reinforcement learning framework compared to Distral BID26. The framework uses a central model and task models for each task on Atari games. Experiments include three tasks with teachers provided for task 2 and task 3. Distral is trained for 120M steps, while the model is trained for 40M steps, with results reported for fair comparison. Our framework, compared to Distral, aims to reduce negative transfer by decreasing a teacher's influence when the target task differs significantly from the source tasks. Distral is suboptimal for multi-task learning due to its inability to decrease teacher influence in such cases. Our framework aims to reduce negative transfer by decreasing a teacher's influence when the target task differs significantly from the source tasks. In the C10 experiment, the normalized weight (p w) for teachers and the student is plotted, showing that the C100 teacher has a higher p w value than the SVHN teacher, as C100 is more relevant to C10. This decrease in teacher influence helps in reducing negative transfer. The C100 teacher has a higher p w value than the SVHN teacher, as C100 is more relevant to C10. The plot in FIG8 confirms this, showing the C100 teacher's higher p w over the entire training. Ablation study with untrained teachers validates that learning with knowledgeable teachers benefits the student, as performance is worse with untrained teachers. Learning with untrained teachers results in worse performance compared to learning with knowledgeable teachers, as shown in experiments. In the context of the target task being hero, untrained teachers achieve an average reward of 15934, while knowledgeable teachers achieve 30928. Knowledge flow leads to higher rewards than training with untrained teachers in various environments and teacher-student settings. The results show that knowledge flow achieves higher rewards than training with untrained teachers in different environments and teacher-student settings. The KL term prevents drastic changes in the student's output distribution when teachers' influence decreases. Ablation study with KL coefficient set to zero in MsPacman task with Riverraid and Seaquest experts shows drastic reward drop without the KL term. In Fig. 9, the KL coefficient (\u03bb 2) set to zero shows drastic reward drop without the KL term in MsPacman task with Riverraid and Seaquest experts. Training with the KL term achieves higher rewards compared to training without it, with an average reward of 2907 and 1215 respectively. Additional experiments confirm the benefits of using the KL term in training. In additional experiments, the teacher model is based on BID16 with 3 convolutional layers and a fully connected layer with 512 ReLUs, while the student model is based on BID17 with 2 convolutional layers. The KL term in training results in higher rewards compared to training without it. The student model, based on BID17, has 2 convolutional layers with 16 and 32 filters, followed by a fully connected layer with 256 ReLUs. The teacher model, based on BID17, has 3 convolutional layers with 32, 64, and 64 filters, and a fully connected layer with 512 ReLUs. The models are linked in the experiment for the target task of KungFu. In the experiment, the student's and teachers' convolutional and fully connected layers are linked for the target task of KungFu Master. Results show that learning with teachers of different architectures yields similar performance to those with the same architecture. Learning with teachers of different architectures achieves similar performance as learning with teachers of the same architecture in the target task of KungFu Master. Knowledge flow enables higher rewards, even when the architectures differ. An average network can be used for the parameters \u03b8 old to investigate its usage. The results in FIG0 show that knowledge flow can lead to higher rewards, even with different teacher and student architectures. An average network can be used for the parameters \u03b8 old, with an experiment showing that using an exponential average yields similar performance to using a single model. Using an exponential average to update \u03b8 old results in similar performance as using a single model, as shown in FIG0. For the target task of Boxing with a Riverraid expert teacher, the average reward for \u03b8 old is 96.2 with an average network, compared to 96.0 with a single network. Various techniques for knowledge transfer have been explored, such as fine-tuning and progressive neural nets. Using a single network to obtain \u03b8 old achieves an average reward of 96.0. Other techniques for knowledge transfer include PathNet BID6, 'Growing a Brain' BID30, and actor-mimic BID20. PathNet BID6 allows multiple agents to train the same deep net while reusing parameters. Our method introduces scaling with lateral connections to previously learned features, contrasting with PathNet BID6 and Progressive Net BID23 which also focus on knowledge transfer and avoiding catastrophic forgetting. Our method introduces scaling with lateral connections to previously learned features, ensuring independence of the student during training. Distral, a combination of 'distill & transfer learning', involves joint training of multiple tasks with a shared 'distilled' policy encoding common behavior. Distral, a neologism combining 'distill & transfer learning', involves joint training of multiple tasks with a shared 'distilled' policy. This shared policy encourages consistency between different tasks, while knowledge flow focuses on a single task. Both multi-task learning and knowledge flow involve the transfer of information, but in multi-task learning, information from different tasks is shared to improve performance. In multi-task learning, information from different tasks is shared to boost performance, while knowledge flow involves leveraging information from multiple teachers to help a student learn a new task. Knowledge distillation distills information from a larger deep net into a smaller one, assuming both nets are trained on the same dataset. Actor-mimic enables an agent to learn how to address multiple tasks. Our technique allows knowledge transfer between different source and target domains. Our proposed technique, Actor-mimic BID20, enables an agent to learn multiple tasks simultaneously and generalize knowledge to new domains. It utilizes a single policy net guided by expert teachers to produce similar actions and representations. This approach differs by leveraging teachers' representations at the start of training. Our proposed technique, Learning without forgetting BID13, allows adding a new task to a deep net without losing the original capabilities. It utilizes a combination of feature regression and cross entropy loss to encourage the student to mimic expert teachers' actions and representations. This method explicitly transfers knowledge from teacher networks to retain old capabilities while learning new tasks."
}