{
    "title": "SJxyZ81IYQ",
    "content": "Mainstream captioning models often face issues like irrelevant semantics, lack of diversity, and poor generalization. This paper introduces a new approach for image captioning, splitting the process into semantic representation extraction and recursive caption construction. This paradigm aims to better preserve semantic content through explicit factorization of semantics and syntax, using a bottom-up compositional generation procedure. The new approach for image captioning involves a compositional procedure that preserves semantic content through explicit factorization of semantics and syntax. The caption construction follows a recursive structure, fitting human language properties, and requires less effort. Image captioning has gained attention recently, with models using an encoder-decoder paradigm to generate captions for images. Despite its effectiveness, the sequential model used has a fundamental problem. Sequential models in image captioning lack the ability to reflect the hierarchical structures of natural languages, leading to drawbacks such as overreliance on n-gram statistics. Sequential models in image captioning struggle to capture hierarchical dependencies among words in a caption, leading to captions that may be syntactically correct but semantically irrelevant. This overreliance on n-gram statistics hinders the generalization of sequential models. To address issues in image captioning, a new paradigm is proposed where semantics and syntax are separated into two stages. The first stage involves extracting explicit semantic representations from images, such as noun-phrases like \"a white cat\" or \"two men\". This approach aims to improve the generalization of sequential models by focusing on relevant semantic concepts. The process of generating captions from images involves decomposing captions into two stages. The first stage extracts the semantic content of the image, represented by noun-phrases like \"a white cat\" or \"two men\". These noun-phrases are then used to recursively construct the complete caption by combining phrases at each step. The phrase composition process involves joining sub-phrases with a connecting module and evaluating completeness using parametric modular nets. The compositional procedure described involves two parametric modular nets for phrase composition and completeness evaluation. This paradigm offers advantages over conventional captioning models by factorizing semantics and syntax, making caption generation easier to interpret and control. The proposed paradigm for image captioning not only preserves semantic content but also allows for easy interpretation and control of caption generation. It effectively increases caption diversity while maintaining semantic correctness and generalizes well to new data, even with limited training data. The literature on image captioning has seen increased interest in the neural network era. Recent works in image captioning have shifted towards a new paradigm that focuses on preserving semantic correctness and generalizing well to new data, especially with limited training data. The literature in image captioning has expanded significantly in the neural network era, with early approaches being bottom-up and detection based. Recent works on image captioning have shifted towards using convolutional neural networks for image representation and recurrent neural networks for caption generation. Vinyals et al proposed a neural image captioner that uses LSTM to generate words one by one, while Xu et al extended this by representing the input image with a set of feature vectors. Recent works on image captioning have shifted towards using convolutional neural networks for image representation and recurrent neural networks for caption generation. Vinyals et al proposed a neural image captioner that uses LSTM to generate words one by one, while Xu et al extended this by representing the input image with a set of feature vectors. Al BID2 proposed a neural image captioner using a single feature vector and LSTM, Xu et al BID3 extended this with multiple feature vectors and attention mechanism, Lu et al BID0 adjusted attention to include generated text, Anderson et al BID1 added an LSTM for better attention control, and Dai et al BID18 used 2D maps for semantic information capture. In recent image captioning research, attention mechanisms have been adjusted to consider the generated text. Various approaches have been proposed, such as using 2D maps to capture semantic information, extracting phrases directly from images, and predicting frequent training words. Additionally, noun-phrases have been treated as hyper-words to improve decoding efficiency. In recent image captioning research, attention mechanisms have been adjusted to consider the generated text. Various approaches have been proposed, such as using 2D maps to capture semantic information, extracting phrases directly from images, and predicting frequent training words. Additionally, noun-phrases have been treated as hyper-words to improve decoding efficiency. Tan et al BID20 introduced noun-phrases as hyper-words in the vocabulary for efficient decoding. BID21 proposed a hierarchical LSTM approach for phrase and word generation. However, these sequential caption generation methods often result in issues like lack of diversity and incorrect semantic coverage. In contrast, our proposed paradigm follows a bottom-up approach. Our proposed paradigm for image captioning differs from previous models by representing the input image with noun-phrases and constructing captions through a recursive composition procedure. This approach aims to address issues such as lack of diversity and incorrect semantic coverage in sequential caption generation methods. The proposed compositional paradigm for image captioning involves disentanglement between semantics and syntax, using a recursive composition procedure to preserve semantics effectively and generate diverse captions. This approach differs from previous models by representing the input image with noun-phrases and addressing issues of diversity and semantic coverage in caption generation. The proposed compositional paradigm for image captioning involves a non-recursive composition procedure that can generate captions with multiple objects. A set of noun-phrases is extracted from the input image to serve as the initial pool for composing phrases. Powerful neural networks are used to learn plausible compositions, allowing for the generation of diverse and hierarchical captions. The proposed two-stage framework for image captioning involves deriving noun-phrases as a semantic representation and constructing captions in a hierarchical, bottom-up manner using a recursive compositional procedure. The two-stage framework for image captioning involves deriving noun-phrases as a semantic representation and constructing captions in a bottom-up manner using a recursive compositional procedure called CompCap. Unlike mainstream models, CompCap considers nonsequential dependencies among words and phrases in a sentence. CompCap, a two-stage framework for image captioning, utilizes n-gram statistics to consider nonsequential dependencies among words and phrases. It represents image semantics explicitly with noun-phrases like \"a black cat\" and \"two boys\", capturing object categories and attributes. The framework extracts these noun-phrases from input images to provide a more interpretable content representation. In image captioning, the framework CompCap utilizes noun-phrases like \"a black cat\" and \"two boys\" to represent image semantics. These noun-phrases capture object categories and attributes, extracted from input images for a more interpretable content representation. The process of extracting explicit representations for images is crucial for visual understanding tasks. In image captioning, noun-phrases are used to represent image semantics. The number of distinct noun-phrases in a dataset is smaller than the number of images. Noun-phrase extraction is formalized as a multi-label classification problem, with selected noun-phrases treated as classes. In image captioning, noun-phrases are used to represent image semantics. A list of distinct noun-phrases is derived from training captions, treated as classes. Visual features are extracted from images using a Convolutional Neural Network, and binary classification is performed for each noun-phrase using weight vectors and the sigmoid function. The input image is represented using top scoring noun-phrases. The input image is represented using top scoring noun-phrases, which are further pruned through Semantic Non-Maximum Suppression. A recursive compositional procedure called CompCap is used to construct the caption. CompCap is a recursive compositional procedure used to construct captions by retaining noun-phrases with maximum scores. The procedure involves a Connecting Module to generate longer phrases by connecting pairs of phrases in a plausible way. The Connecting Module (C-Module) generates longer phrases by connecting pairs of phrases in a plausible way, using a sequence of words denoted as P(m). The module computes a score for the new phrase and selects the one with the maximum score as the resulting caption. An Evaluation Module (E-Module) assesses if the new phrase is a complete caption before finalizing it. The parametric module determines P new, which is evaluated by the E-Module to check if it is a complete caption. If not, the pool P is updated, and the process continues until a complete caption is obtained or only one phrase remains in P. The Connecting Module selects a connecting phrase P(m) based on left and right phrases P(l) and P(r). The Connecting Module (C-Module) selects a connecting phrase P(m) based on left and right phrases P(l) and P(r) to evaluate the connecting score S(P(m) | P(l), P(r), I). The conventional way of using an LSTM to decode intermediate words fails in this task as the C-Module mainly deals with incomplete captions. In the Connecting Module (C-Module), a new strategy is adopted to treat the generation of connecting phrases as a classification problem due to the limited number of distinct connecting phrases in the proposed paradigm. This approach is motivated by the observation that semantic words such as nouns and adjectives are not involved in connecting phrases. For example, in MS-COCO BID4, there are over 1 million samples. The Connecting Module (C-Module) treats connecting phrases as a classification problem due to limited distinct phrases. Over 1 million samples in MS-COCO BID4 yield only about 1,000 distinct connecting phrases. Distinct connecting sequences are mined from training captions and used as different classes for the classifier. The Connecting Module (C-Module) treats connecting phrases as a classification problem by defining a classifier that takes left and right phrases as input. A two-level LSTM model is used to encode the phrases, with the low-level LSTM controlling attention and interacting with visual features. The Connecting Module (C-Module) uses word embeddings and global/regional image features from a Convolutional Neural Network. A two-level LSTM model encodes phrases, with the low-level LSTM controlling attention and the high-level LSTM driving the encoded state evolution. Encodings go through fully-connected layers and a softmax layer to generate connecting scores. The Connecting Module (C-Module) uses word embeddings and global/regional image features from a Convolutional Neural Network. Encodings of phrases go through fully-connected layers and a softmax layer to generate connecting scores, which are used to connect pairs of phrases based on the highest score. A virtual neg class is added for pairs that cannot be connected, and scores for phrases are computed using a binary classification score. The Evaluation Module (E-Module) determines if a phrase is a complete caption by encoding it into a vector and evaluating the probability of it being a complete caption. The Evaluation Module (E-Module) encodes phrases into vectors to determine if they are complete captions. It can also check for other properties like caption quality using a caption evaluator. Extensions include generating diverse captions using beam search or probabilistic sampling. To improve caption diversity, extensions like beam search or probabilistic sampling can be used instead of a greedy search strategy. This allows for multiple ordered pairs and connecting sequences to be retained at each step, forming multiple beams for beam search and avoiding local minima. Probabilistic sampling can also be used to select a part of ordered pairs or connecting sequences based on normalized scores. The framework allows for generating diverse captions through probabilistic sampling, incorporating user preferences, and controlling the resultant captions by filtering noun phrases or modulating their scores. This control is easier to implement on an explicit representation than on an encoded feature vector. In the Experimental section, examples are shown of how one can influence captions by filtering noun phrases or adjusting their scores. Experiments are conducted on MS-COCO BID4 and Flickr30k BID5 datasets, each containing a large number of images with ground-truth captions. The vocabulary is standardized by converting words to lowercase and removing infrequent or non-alphabetic words. The vocabulary for MS-COCO and Flickr30k datasets is standardized by converting words to lowercase and removing infrequent or non-alphabetic words. Training captions are truncated to 18 words, and ground-truth captions are parsed into trees using NLP toolkit BID31 for training the connecting and evaluation modules separately. To collect training data, ground-truth captions are parsed into trees using NLP toolkit BID31. The C-Module and E-Module are separately trained for classification tasks. The recursive compositional procedure is modularized for better generalization. Testing involves two forward passes for each module, with 2 or 3 steps needed for a complete caption. CompCap is compared with NIC, a Neural Image Captioner. Testing involves two forward passes for each module, with 2 or 3 steps needed for a complete caption. CompCap is compared with other methods such as AdapAtt, TopDown, and LSTM-A5 for image captioning. In comparison to other methods like AdapAtt, TopDown, and LSTM-A5 for image captioning, CompCap utilizes semantical feature vectors and additional visual features predicted by LSTM-A5 BID19. All methods are re-implemented and trained with the same hyperparameters using ResNet-152 BID16 pretrained on ImageNet BID32 for extracting image features. During training, ResNet-152 is used to extract image features without finetuning, and a fixed learning rate of 0.0001 is set for all methods. Best performing parameters on the validation set are selected for caption generation during testing. CompCap selects 7 noun-phrases with top scores to represent the input image, balancing semantics and syntax. Beam-search of size 3 is used for baselines. CompCap selects 7 noun-phrases with top scores to represent the input image, balancing semantics and syntax. Beam-search of size 3 is used for pair selection, while no beam-search is used for connecting phrase selection. Quality of generated captions compared on MS-COCO and Flickr30k test sets, with CompCap achieving the best results among all methods. Among all methods, CompCap with predicted noun-phrases performs best in terms of SPICE metric, but falls behind in CIDEr, BLEU-4, ROUGE, and METEOR. These results reflect the characteristics of methods that generate captions sequentially and compositionally. The results show that methods generating captions sequentially favor certain n-grams, while compositional methods preserve semantic content better but may include unseen n-grams. An ablation study on the proposed compositional paradigm is conducted, with the best performance seen in CompCap using predicted noun-phrases for the SPICE metric. CompCap effectively preserves semantic content by using groundtruth noun-phrases from associated captions, leading to improved caption generation. Additionally, integrating noun-phrases from a ground-truth caption in a specific order further enhances CompCap's performance. CompCap improves caption generation by utilizing ground-truth noun-phrases and integrating them in a specific order, leading to enhanced performance in metrics except for SPICE. This compositional paradigm separates semantics and syntax, with CompCap focusing on composing semantics. The proposed compositional paradigm in CompCap disentangles semantics and syntax into two stages, focusing on composing semantics into a syntactically correct caption. It is effective at handling out-of-domain semantic content and requires less data to learn, as shown in two conducted studies. In two studies, CompCap was evaluated by training baselines and modules on different data ratios and testing on MS-COCO/Flickr30k datasets. Results showed significant drops for baselines but competitive performance for CompCap, suggesting the benefit of disentangling semantics and syntax. Results in SPICE and CIDEr metrics show significant drops for baselines, but competitive performance for CompCap trained on in-domain and out-of-domain data. This highlights the advantage of separating semantics and syntax, as semantics vary between datasets while syntax remains stable. CompCap also excels in generating diverse captions by varying noun-phrases and composing order, as evidenced by five diversity metrics. CompCap can generate diverse captions by varying noun-phrases and composing order, as shown by five diversity metrics. These metrics include the ratio of novel and unique captions, vocabulary usage, and the overall diversity of the generated captions. The diversity of captions generated by CompCap is measured using various metrics such as the percentage of distinct captions, vocabulary usage, and pair-wise editing distances. The diversity of captions generated by CompCap is measured using various metrics such as the percentage of distinct captions, vocabulary usage, and pair-wise editing distances. The average distance over captions of different images is computed to determine diversity at the dataset level, while the average distance over captions of the same image is used to measure diversity at the image level. CompCap achieved the best results in all metrics, indicating that its captions are diverse and novel. In practice, 5 captions with top scores are used to compute diversity at the image level for each method. CompCap achieved the best results in all metrics, indicating diverse and novel captions. Error analysis reveals that errors in CompCap's captions mainly stem from misunderstanding the input visual content. In this paper, a novel paradigm for image captioning is proposed to address errors in captions generated by CompCap and sequential models. The errors in CompCap mainly stem from misunderstanding the input visual content, while sequential models tend to favor frequent n-grams. The proposed approach aims to improve caption generation by applying more sophisticated techniques in noun-phrase extraction. The proposed method for image captioning involves a compositional approach, where the captioning procedure is divided into two stages. The first stage extracts an explicit representation of the input image in noun-phrases, while the second stage assembles these noun-phrases into a caption using a recursive compositional procedure. This method aims to improve caption generation by avoiding errors related to frequent n-grams. The proposed method for image captioning involves a compositional approach, dividing the captioning procedure into two stages. The first stage extracts noun-phrases from the input image, while the second stage assembles these noun-phrases into a caption using a recursive compositional procedure. This hierarchical structure aims to preserve semantics effectively, require less data to train, generalize better across datasets, and yield diverse captions by finding semantically similar noun-phrases. The key for suppression in image captioning is to find semantically similar noun-phrases by comparing central nouns. This can be done by identifying synonyms or plurals of synonyms in noun-phrases. Additionally, noun-phrases without synonymic central nouns can also be considered semantically similar based on the input image. Encoders in the C-Module are used to suppress noun-phrases in such cases. In image captioning, encoders in the C-Module are used to find semantically similar noun-phrases by comparing central nouns. The normalized euclidean distances of encodings z (l) and z (r) for each noun-phrase are computed to measure similarity, which is more robust than using a single encoding. The C-Module in image captioning uses normalized euclidean distances of encodings for noun-phrases to measure similarity. The sum of distances is compared to a threshold to determine semantic similarity, with independent parameters for each encoder. The C-Module in image captioning uses encoders with independent parameters for noun-phrases to measure similarity, leading to better performance. Additional hyperparameters for CompCap include beam search sizes for pair selection. The results from experiments in the main content show that the C-Module with encoders having independent parameters performs better in image captioning. Additional hyperparameters for CompCap include tuning the size of beam search for pair selection and connecting phrase selection. Adjusting these hyperparameters individually has minor influence on the performance of CompCap, as shown in FIG6."
}