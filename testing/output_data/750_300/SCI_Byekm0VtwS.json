{
    "title": "Byekm0VtwS",
    "content": "Uncertainty is a key feature in intelligence, making the brain flexible and powerful. Neuromorphic computing chips, which mimic the brain, have uncertainty but current deep neural networks do not consider this, leading to lower performance on these chips. A new uncertainty adaptation training scheme (UATS) was proposed to address this issue. The uncertainty adaptation training scheme (UATS) addresses the lower performance of neural networks on neuromorphic computing chips by incorporating uncertainty into the training process. Experimental results show improved inference performance on uncertain chips compared to original platforms, highlighting the importance of uncertainty reasoning in intelligent systems. The uncertainty adaptation training scheme (UATS) improves neural network performance on neuromorphic computing chips by incorporating uncertainty into the training process. Fuzziness and stochasticity are two types of uncertainties in intelligent systems, with fuzziness helping the brain efficiently process real-world information by ignoring redundant details, and stochasticity enabling creativity and preventing constant failure. The brain's ability to ignore irrelevant details and embrace uncertainty enables creativity and prevents constant failure. This characteristic is lacking in most AI systems, like deep neural networks, which rely on precise numerical descriptions. Some researchers suggest that using 8-bit integers instead of 32-bit or 64-bit floating numbers is sufficient for many applications. The use of 8-bit integers instead of 32-bit or 64-bit floating numbers in deep neural networks has been suggested by some researchers. Methods like network quantization and Bayesian networks address issues related to training procedures. Neuromorphic computing chips offer a hardware approach to supplement uncertainty in DNNs. Emerging nanotechnology devices and crossbar structures are also being explored in neuromorphic computing. The emerging nanotechnology device and crossbar structure based neuromorphic computing chips have developed significantly in recent years. The crossbar structure, efficient for vector-matrix multiplication, utilizes Ohm's law and Kirchhoff's law. Nanoscale nonvolatile memory devices at each cross point provide additional storage capability. The crossbar holds conductances as memory and performs computing functions when needed. The emerging nanoscale nonvolatile memory device at each cross point in the crossbar structure provides additional storage capability for efficient vector-matrix multiplication. This computing in memory architecture can alleviate the memory bottleneck in traditional architectures, making neuromorphic computing chips more energy and area efficient for AI applications. Neuromorphic computing chips aim to be more energy and area efficient by utilizing von Neumann architecture. They also incorporate uncertainty as a key feature. The crossbar structure in these chips uses applied voltage (V) for input (x), conductance (G) for weight (W), and output current (I) for output (y) based on Ohm's and Kirchhoff's laws. The uncertainty in neuromorphic computing chips arises from analog to digital converters (ADCs) and NVM devices, causing fuzziness and stochasticity, respectively. According to Kirchhoff's law, the VMM result is a summarization of currents, requiring ADC for converting analog currents to digital voltages for data transfer, similar to activation quantization in networks. The stochasticity of NVM devices is caused by random particle movement, leading to varying conductance and different output currents even with the same voltage. This stochastic behavior is simulated as a non-ideal factor affecting network performance. The stochastic behavior of NVM devices results in varied conductance and output currents, impacting network performance. Different types of NVM devices exhibit varying levels of stochasticity. Different types of NVM devices exhibit varied stochastic behavior, impacting network performance. The Gaussian distribution is used to model device stochasticity, with the mean representing the stable state conductance value. The conductance values of different NVM devices are modeled using a Gaussian distribution, where the mean represents the stable state conductance value. The variance of the distribution is typically correlated to the mean, and the standard deviation is assumed to be linearly and positively related to the mean. The conductance values of NVM devices are modeled using a Gaussian distribution. The standard deviation is linearly correlated to the mean. Conductances below a minimum value are cut off. The stochastic conductance is sampled from a Gaussian distribution with mean \u00b5 and variance \u03c3^2. The model of devices stochasticity involves conductance values sampled from a Gaussian distribution. Device fuzziness is a result of the writing process in neuromorphic computing chips for AI applications. Conductance of each device in the crossbar is determined based on neural network weights before writing. When using neuromorphic computing chips for AI applications, writing the conductance of each device in the crossbar is crucial. The mapping process determines the target conductance based on neural network weights, scaling them into the device's working range. The difference between two device conductances is used to express a weight, which can be positive or negative. Lower conductances are preferred for higher energy efficiency, and a mapping algorithm is used to utilize the entire conductance working range effectively. The conductance of each device in the crossbar is crucial for neuromorphic computing chips used in AI applications. Lower conductances are preferred for higher energy efficiency. However, accuracy in writing conductance is affected by device stochasticity and circuit fuzziness, leading to variations in manipulation and measurement. The conductance of devices in neuromorphic computing chips is crucial for AI applications. Accuracy in writing conductance is affected by device stochasticity and circuit fuzziness, leading to variations in manipulation and measurement. A model using Gaussian distribution is used to describe the fuzziness. In neuromorphic computing chips, the Gaussian distribution model is used to describe fuzziness in conductance. Uncertainty in device stochasticity and circuit fuzziness can impact the performance of a well-trained DNN programmed directly onto the chip. The uncertainty in device stochasticity and circuit fuzziness can affect the performance of a well-trained DNN on a neuromorphic computing chip. The proposed uncertainty adaptation training scheme (UATS) can help alleviate the decrease in classification accuracy by guiding neural networks to learn how to handle uncertainty during the training process. The uncertainty adaptation training scheme (UATS) aims to improve accuracy by introducing stochasticity in the feed forward process, guiding neural networks to handle uncertainty. Stochasticity model is used to calculate conductances of stable states based on weights, with w s approximated as (6) if G is significantly larger than G min. The fuzziness model is introduced during training in the uncertainty adaptation training scheme. Weight w is replaced by a random variable w f after every k epochs, with G pf and G nf obtained by the fuzziness model. Target conductances G ptarget and G ntarget are calculated according to w, with w f approximated as (8) if G is significantly larger than G min. The uncertainty adaptation training scheme introduces the fuzziness model, where weight w is replaced by a random variable w f after every k epochs. The target conductances G ptarget and G ntarget are calculated based on w, with w f approximated as (8) if G is significantly larger than G min. The loss function is calculated by the average output of n FF processes with the same input batch, aiming to evaluate network performance under uncertainty. The ideas of UATS were evaluated on multiple models and datasets to improve network training in an uncertain way. The loss function is calculated by the average output of n FF processes with the same input batch. UATS ideas were evaluated on various models and datasets, including the MNIST dataset with MLP and CNN models. Training set has 60,000 images, with 50,000 for training and 10,000 for validation. Test error was calculated using 10,000 test set images. In the experiment, LeNet-5 models were used with 60,000 images in the MNIST dataset. 50,000 images were randomly selected for training, 10,000 for validation, and 10,000 for testing. Different levels of uncertainty (G min = 1\u00b5S, G low = 5\u00b5S, G high = 50\u00b5S) were tested. The models were trained normally and then tested with uncertainty levels using fuzziness and stochasticity models. 20 trials were conducted for each model with each uncertainty level to calculate average test error and standard deviation. Without using the UATS, uncertainty increases test errors for MLP and CNN models. CNN model (LeNet-5) performs best without uncertainty but is most affected by it. Average width of LeNet-5 is smaller than MLP models, leading to higher impact of uncertainty (t-test, p < 0.01). The CNN model (LeNet-5) performs best without uncertainty but is most affected by it due to its smaller average width compared to MLP models. UATS significantly improves accuracies with the same level of uncertainty through weight tuning and retraining experiments. The UATS method was tested on different models with varying parameters in retraining and fine-tuning experiments. Results showed that UATS improved accuracies with the same uncertainty level. Retraining results were generally better than fine-tuning results using UATS. The method was also validated on the CIFAR-10 dataset with a more complex DNN model. The UATS method was validated on the CIFAR-10 dataset using a ResNet-44 DNN model. Results showed that UATS can achieve lower error rates than ideal cases with proper hyper-parameters. It acts as a regularization method, making DNN training easier, especially with more layers. The importance of uncertainty in intelligent systems was highlighted. The UATS method acts as a regularization technique for DNN training, particularly beneficial with more layers. Uncertainty is crucial in intelligent systems, and the Bayesian network is useful for building uncertain neural networks. Neuromorphic computing chips face challenges in controlling weight distribution, unlike UATS, which requires no additional circuitry. The conductance distribution of neuromorphic computing chips is challenging to control, unlike UATS which requires no additional circuitry. Various distributions have been explored to model device stochasticity, including Laplacian, uniform, lognormal, asymmetric Laplacian, and Bernoulli distributions. The conductance distribution of neuromorphic computing chips is challenging to control. Various distributions like lognormal, asymmetric Laplacian, and Bernoulli are used to model device stochasticity. The VMM transforms individual distributions to random parameters, reducing the need for random numbers in UATS computation. The VMM transforms individual distributions of neuromorphic computing chips to random parameters, reducing the need for a large number of random numbers in UATS computation. Methods to reduce random number requirements include sampling weights for inputs or batches instead of VMM, and using uncertainty models of VMM results instead of weights, leading to accelerated simulation speed with similar results."
}