{
    "title": "rJxlc0EtDr",
    "content": "Recent research on neural network architectures with external memory has focused on the bAbI question and answering dataset, which presents challenging tasks requiring reasoning. To further investigate the reasoning capacity of these architectures, a classic associative inference task from human neuroscience literature was employed. This task aims to test the ability to understand distant relationships among elements distributed across multiple facts or memories. However, current architectures struggle to reason over long distance associations, as shown in the study. The study focused on neural network architectures with external memory and their struggle to reason over long distance associations. A novel architecture, MEMO, was developed to address this issue by introducing a separation between memories/facts and utilizing an adaptive retrieval mechanism. MEMO, a novel architecture, introduces a separation between memories/facts in external memory and uses an adaptive retrieval mechanism for reasoning over long distance associations. It can solve novel reasoning tasks and all 20 tasks in bAbI, helping connect facts acquired at different points in time for making judgments in everyday life. Inferential reasoning involves connecting facts acquired at different times to make judgments in everyday life, such as inferring relationships between individuals based on separate encounters. This process is supported by the hippocampus. Inferential reasoning connects separate experiences to infer relationships, supported by the hippocampus. Memories are stored independently to minimize interference, allowing for specific recall of events. The hippocampus stores memories separately to prevent interference, enabling specific recall of events. Recent research suggests that the integration of these separated experiences occurs during retrieval through a recurrent mechanism. Recent research shows that the integration of separated experiences emerges during retrieval through a recurrent mechanism, allowing multiple pattern separated codes to interact and support inference. This paper explores how neuroscience models can enhance inferential reasoning in neural networks, such as the Differential Neural Computer (DNC) and end-to-end memory networks (EMN). Neural networks, inspired by neuroscience models, are being enhanced for inferential reasoning. Models like the Differential Neural Computer (DNC) and end-to-end memory networks (EMN) have shown impressive abilities in tackling complex tasks. Recent advancements in attention mechanisms and context utilization have also enabled traditional neural networks to handle similar tasks. However, challenges like repetitive tasks in datasets such as bAbI still persist for neural networks. Neural networks are being improved for inferential reasoning, with models like DNC and EMN showing impressive abilities. Recent advancements in attention mechanisms and context utilization have enabled traditional neural networks to handle similar tasks. A new task called Paired Associative Inference (PAI) has been introduced to capture inferential reasoning by appreciating distant relationships among elements distributed across multiple facts or memories. The Paired Associative Inference (PAI) task, derived from neuroscientific literature, aims to capture inferential reasoning by appreciating distant relationships among elements. It is procedurally generated to force neural networks to learn abstractions for solving unseen associations. The PAI task is followed by finding the shortest path and bAbi tasks to investigate memory representations supporting memory-based reasoning. Models like EMN have utilized fixed memory for similar tasks. Our approach, called MEMO, retains the full set of facts into memory and learns a linear projection with a recurrent attention mechanism for greater flexibility in memory-based reasoning. MEMO is a language model that retains all facts in memory and uses a linear projection with a recurrent attention mechanism for flexible memory-based reasoning. It addresses the issue of prohibitive computation time in neural networks. MEMO is a language model that uses a linear projection with a recurrent attention mechanism for flexible memory-based reasoning. It addresses the problem of prohibitive computation time in neural networks by allowing for flexible weighting of individual elements in memory. The computation in standard neural networks grows with the input size, but MEMO aims to reduce computation by either padding the input with extra values or dropping input values systematically. These adjustments are typically hand-tuned by experimenters. In some cases, input values are dropped to reduce computation, hand-tuned by experimenters. Inspired by REMERGE model of human memory, content is recirculated to determine network settling. In the REMERGE model of human memory, content is recirculated to determine network settling using techniques like adaptive computation time. The network outputs an action to decide whether to continue computing or answer the task, known as the halting policy. The network in our architecture learns a halting policy to determine when to continue computing or answer a task. Unlike ACT, it uses reinforcement learning to adjust weights based on the optimal number of computation steps. The approach differs from previous work by adding an extra term to the REINFORCE loss, minimizing the expected number of computation steps and encouraging the network to prefer representations that minimize required computation. The approach introduces a new task emphasizing reasoning and investigates memory representations for inferential reasoning. It includes a REINFORCE loss component to learn the optimal number of computation steps. The curr_chunk discusses an in-depth investigation of memory representation for inferential reasoning, extensions to existing memory architectures, a REINFORCE loss component for learning optimal task-solving iterations, and empirical results on three tasks. The prev_chunk introduces a new task emphasizing reasoning and the use of a REINFORCE loss component for learning computation steps. The curr_chunk discusses End-to-End Memory Networks for inference and shortest path finding. It focuses on the multilayer, tied weight variant of EMN and the setup for predicting answers given knowledge inputs and a query. The curr_chunk explains how End-to-End Memory Networks predict answers based on knowledge inputs and a query. It involves embedding words and summing vectors using key, value, and query matrices, along with positional encoding. The curr_chunk describes how End-to-End Memory Networks calculate weights over memory elements to produce output based on key, value, and query matrices. It involves embedding words, summing vectors, and using positional encoding. End-to-End Memory Networks calculate weights over memory elements using key, value, and query matrices. It involves embedding words, summing vectors, and positional encoding. EMN is trained via cross entropy loss, while MEMO embeds input differently with common embeddings for each input matrix. MEMO embeds input differently by deriving a common embedding for each input matrix without using hand-coded positional embeddings. Multiple heads are used to attend to the memory, allowing flexibility in capturing different parts of the input sentence. MEMO uses multiple heads to attend to the memory, allowing flexibility in capturing different parts of the input sentence. Each head has a different view of the common inputs, enabling the model to learn efficiently. The attention mechanism in MEMO utilizes multi-head attention and differs from previous models like EMN. It allows for flexible recombination of stored items and enables efficient learning by weighting key, values, and query matrices separately. The attention mechanism in MEMO, as described by Vaswani et al. (2017), utilizes multi-head attention, DropOut, and LayerNorm for improved generalization and learning dynamics. Matrices K, V, and Q are transformed using matrices W_h and W_q to enhance the attention mechanism. The attention mechanism in MEMO, inspired by Vaswani et al. (2017), uses matrices for transforming logits and queries. It differs by preserving queries separate from keys and values, enhancing learning dynamics. MEMO's attention mechanism, inspired by Vaswani et al. (2017), differs by preserving queries separate from keys and values, reducing computational complexity compared to self-attention methods. It can output potential answers to a query and learn the number of computational steps required effectively. MEMO's attention mechanism, inspired by Vaswani et al. (2017), reduces computational complexity by preserving queries separate from keys and values. Dehghani et al. (2018) have quadratic complexity. To determine the number of computational steps needed to answer a query effectively, information is collected at each step to create an observation s t. This observation is processed by gated recurrent units (GRUs) (Chung et al., 2015) and an MLP to define a binary policy \u03c0(a|s t, \u03b8) and approximate its value function V(s t, \u03b8) using the Bhattacharyya distance between attention weights. The network uses gated recurrent units (GRUs) and an MLP to define a binary policy and approximate its value function. The input to the network is based on the Bhattacharyya distance between attention weights of current and previous time steps, along with the number of steps taken so far. This helps in determining when the attention has settled into a fixed point. The network uses GRUs and an MLP to create a binary policy and approximate its value function based on the Bhattacharyya distance between attention weights. It is trained using REINFORCE with a focus on preventing settling into a fixed point by adjusting parameters with n-step look ahead values. The objective is to minimize L Hop, a term derived from the binary policy nature of \u03c0. The objective function of the network is to minimize L Hop, a term that encourages minimizing the expected number of hops in computation. This term directly promotes representations and computation that require less computation. The objective function of the network aims to minimize the expected number of hops in computation, encouraging representations and computation that require less computation. The variance of using REINFORCE for training discrete random variables can be high, but for a binary halting random variable, the variance is bounded by 1/4, which is manageable for successful learning. In the case of a binary halting random variable, the variance is p(1 \u2212 p) where p is the probability of halting, bounded by 1/4 for successful learning. The reward structure is defined by the target answer a and the prediction \u00e2 from the network. The final layer of MLP R was initialized with bias init to increase the probability of doing one more hop. A maximum number of hops, N, was set for the network, stopping additional hops when reached. No gradient sharing between the hop network and other components was implemented. The network was set with a maximum number of hops, N, to increase the probability of producing a probability of 1. There was no gradient sharing between the hop network and the main MEMO network. The Differential Neural Computer (DNC) is an influential model that operates sequentially on inputs, learning to read and write to a memory store. It has shown capability in solving algorithmic problems but struggles with scalability. An extension incorporating sparsity has improved performance on larger-scale tasks like the bAbI task suite. The DNC model has shown capability in solving algorithmic problems but struggles with scalability. Recent extensions incorporating sparsity have improved performance on larger-scale tasks like the bAbI task suite. Alternative memory-augmented architectures have been developed, such as the Dynamic Memory Network and the Recurrent Entity Network, each with their own unique features. The Dynamic Memory Network, Recurrent Entity Network, Working Memory Network, and RelationNet are new models that excel at various tasks, including the bAbI task suite. They incorporate unique features for relational reasoning over memory contents. The Dynamic Memory Network, Recurrent Entity Network, Working Memory Network, and RelationNet enable relational reasoning over memory contents and excel at various tasks, including the bAbI task suite. Adaptive Computation Time (ACT) is a mechanism for dynamically modulating computational budget based on task complexity. Adaptive Computation Time (ACT) and Adaptive Early Exit Networks are approaches to dynamically adjust computational steps based on task complexity. REINFORCE is used to learn discrete variables for conditional computation. Conditional computation techniques such as using REINFORCE to adjust the number of computation steps have been applied to neural networks, allowing for early exit and reducing the total number of processed inputs. Conditional computation techniques like REINFORCE have been used to adjust the number of computation steps in neural networks, reducing the total processed inputs. This approach has also been applied to recurrent neural networks and neural networks with external memory. Our method introduces the idea of using the distance between attention weights at different time steps as a proxy to determine if more information can be retrieved from memory or if the network has enough information. Our method introduces using the distance between attention weights at different time steps as a proxy to determine if more information can be retrieved from memory or if the network has settled. Graph Neural Networks consist of an iterative message passing process propagating node and edge embeddings throughout a graph, with neural networks aggregating functions over graph components for various learning tasks. Graph Neural Networks use neural networks to aggregate functions over graph components for learning tasks. They implement a message passing process similar to attention mechanisms. GNNs differ from traditional neural networks in their recurrent component implementation. Self-attention can be seen as a fully-connected Graph Neural Network (GNN), but differs in that GNNs use a recurrent component with a fixed number of steps, while our method adapts the number of message passing steps. Our model does not require message passing between memories, as input queries directly attend to memory slots. The paper introduces a task to probe the reasoning capacity of neural networks by testing their ability to appreciate distant relationships among elements in memories. This task does not require message passing between memories as input queries directly attend to memory slots. The paper introduces a task derived from neuroscience to probe the reasoning capacity of neural networks by testing their ability to appreciate distant relationships among elements in memories. This task is formalized in the paired associative inference (PAI) task, where two images are randomly associated together. In the PAI task, two images are randomly associated together to test episodic memory. The agent is presented with pairs of images and later tested with direct and indirect queries. During the PAI task, two images are randomly paired together to test episodic memory. The agent is later tested with direct and indirect queries, where indirect queries require inference across multiple episodes. The network is presented with a cue, image A, and two possible choices: the match, image C, originally paired with B, or a lure, another image C paired with B forming a different triplet. The correct answer can only be determined by understanding the relationship between A and C. The network is presented with a cue, image A, and two choices: image C, the match paired with B, or a lure forming a different triplet. The correct answer requires understanding the relationship between A and C. The comparison was made with other memory-augmented architectures like End to End Memory Networks and DNC. For more details on the batch creation, refer to the appendix. In comparing MEMO with other memory-augmented architectures like End to End Memory Networks and DNC, as well as the Universal Transformer, the results show MEMO achieving the highest accuracy on the hardest inference query for each of the PAI tasks. Full results can be found in the appendix. MEMO outperformed other memory-augmented architectures like End to End Memory Networks and DNC, as well as the Universal Transformer, achieving the highest accuracy on the hardest inference query for each PAI task. In the smaller set A-B-C, MEMO and DNC had the highest accuracy, while EMN and UT struggled. For longer sequences, MEMO was the only architecture successful in answering complex inference queries. Further analysis on the length 3 PAI task revealed DNC required 10 hops to solve it. MEMO outperformed other memory-augmented architectures like End to End Memory Networks and DNC, achieving the highest accuracy on the hardest inference query for each PAI task. For longer sequences, MEMO was the only architecture successful in answering complex inference queries. Further analysis on the length 3 PAI task revealed DNC required 10 hops to solve it. The attention weights of an inference query were analyzed to associate a CUE with the MATCH and avoid interference of the LURE. The original sequence A \u2212 B \u2212 C with class IDs 611 \u2212 191 \u2212 840 was not directly experienced together by the network. MEMO retrieved memory in slot 10 containing the CUE and associated item forming an A \u2212 B association. In the memory retrieval process, MEMO assigned probability masses to different slots to support correct inference decisions. The sequence of memory activation followed the predicted pattern. ID 943 had mass associated with it. MEMO assigned probability masses to slots for correct inference decisions, confirmed in the last hop. The activation sequence resembled hippocampus computational models and neural data. Another MEMO instance used 7 hops with a different memory activation pattern. The algorithm for inference depends on the number of hops. The algorithm used in MEMO for inference depends on the number of hops taken by the network. This pattern of memory activation could be linked to knowledge distillation in neural networks. A set of ablation experiments on MEMO confirmed these findings. The algorithm in MEMO uses multiple hops initially for over-parametrization, which are then automatically reduced for efficiency. Ablation experiments confirmed that specific memory representations combined with recurrent attention support successful inference, especially for inference queries. Direct queries, testing episodic memory, can be solved with a single hop. The algorithm in MEMO uses multiple hops for over-parametrization, automatically reduced for efficiency. Ablation experiments confirmed that specific memory representations combined with recurrent attention support successful inference, especially for inference queries. Our adaptive computation mechanism was compared with ACT, showing more data efficiency for the task. The algorithm in MEMO uses multiple hops for over-parametrization, automatically reduced for efficiency. Ablation experiments confirmed that specific memory representations combined with recurrent attention support successful inference, especially for inference queries. Our adaptive computation mechanism was compared with ACT, showing more data efficiency for the task. In a synthetic reasoning experiment on randomly generated graphs, weights analysis of an inference query in the length 3 PAI task was conducted. Table 2 displays the accuracy of models in predicting the shortest path between nodes on graphs of varying complexity. DNC, Universal Transformer, and MEMO achieved perfect accuracy on a small graph with 10 nodes and path length of 2. However, MEMO outperformed EMN on more complex graphs with 20 nodes and path length of 3. MEMO outperformed EMN and DNC in predicting nodes on more complex graphs with higher connectivity, showing great scalability as the number of hops increases. MEMO showed superior performance in predicting nodes on highly connected graphs, outperforming EMN and DNC. The model demonstrated great scalability by considering more paths as the number of hops increased. Universal Transformer had varying performance in predicting the first and second nodes of the shortest path, with slightly lower results than MEMO in the latter case. Test results for the best 5 hyper-parameters for MEMO are reported, along with mean and standard deviation values. UT had slightly lower performance compared to MEMO, showcasing its strength in direct reasoning tasks. Test results for the top 5 hyper-parameters for MEMO were reported, along with mean and standard deviation values. Results for EMN, UT, and DNC were from their best run. The study also focused on the bAbI question answering dataset, training the model on the joint 10k training set. Table 3 shows the averaged accuracy of MEMO and other baselines on bAbI tasks. In the 10k training regime, MEMO solved all 20 tasks with lower error compared to other models. Ablation experiments were conducted to analyze the contribution of each architectural component to the final performance. The study by Seo et al. (2016) and Dehghani et al. (2018) solved tasks with lower error rates compared to other models. Ablation experiments were conducted to analyze the contribution of each architectural component to the final performance, highlighting the importance of memory representations and recurrent attention. The use of layernorm in the recurrent attention mechanism was crucial for achieving stable training and better performance on the bAbI task. In this paper, an in-depth investigation of memory representations supporting inferential reasoning was conducted. MEMO, an extension to existing memory architectures, showed state-of-the-art results in reasoning tasks. MEMO, an extension to existing memory architectures, demonstrated state-of-the-art results in inferential reasoning tasks, including paired associative inference and graph traversal. It also matched the performance of the current state-of-the-art results on the bAbI dataset. MEMO, an extension to existing memory architectures, achieved state-of-the-art results in inferential reasoning tasks and matched the performance of the current state-of-the-art results on the bAbI dataset by combining separated storage of single facts in memory with a powerful recurrent attention mechanism. The task was made challenging by starting from the ImageNet dataset and creating three sets for training, validation, and testing. MEMO, an extension to existing memory architectures, achieved state-of-the-art results in inferential reasoning tasks by combining separated storage of single facts in memory with a powerful recurrent attention mechanism. The task was made challenging by starting from the ImageNet dataset and creating three sets for training, validation, and testing, with sequences of varying lengths and specific image counts in each dataset. The dataset consists of three, four, and five items with a large number of training, evaluation, and testing images. Each batch entry includes a memory, query, and target, created by selecting N sequences and forming pair-wise associations between items in the sequence. In each batch, N sequences are selected with N = 16. Memory content is created with pair-wise associations between items in the sequence. Queries consist of a cue, match, and lure image. Two types of queries - 'direct' and 'indirect' where the cue and match can be found in the same memory slot. The study involves sequences with pair-wise associations and two types of queries - 'direct' and 'indirect'. 'Direct' queries test episodic memory by retrieving experienced episodes, while 'indirect' queries require inference across multiple episodes. In the study, sequences involve pair-wise associations and two types of queries - 'direct' and 'indirect'. 'Direct' queries test episodic memory by retrieving experienced episodes, while 'indirect' queries require inference across multiple episodes. The queries are presented as a concatenation of three image embedding vectors, with the cue always in the first position. The position of the match and lure are randomized to avoid degenerate solutions. The study involves sequences with pair-wise associations and two types of queries - 'direct' and 'indirect'. The cue is always in the first position in the concatenation of three image embedding vectors. The position of the match and lure is randomized to prevent degenerate solutions. The lure image is always in the same position as the match image but is drawn from a different sequence in memory. This setup requires appreciating the correct connection between images without interference from other memory items. The study involves sequences with pair-wise associations and two types of queries - 'direct' and 'indirect'. For each batch, all possible queries were generated based on the current memory store, with half being direct queries and half indirect. The network predicts the class of the matches, with longer sequences providing more direct and indirect queries that require different levels of inference. The study involves sequences with pair-wise associations and two types of queries - 'direct' and 'indirect'. Longer sequences provide more direct and indirect queries that require different levels of inference. Inputs for different models vary, with memory and query used as corresponding inputs for EMN and MEMO, while DNC embeds stories and query. The study involves sequences with pair-wise associations and two types of queries - 'direct' and 'indirect'. Inputs for different models vary, with memory and query used as corresponding inputs for EMN and MEMO, while DNC embeds stories and query. For DNC, stories and query are embedded in the same way as MEMO, presented in sequence followed by blank inputs for final prediction. UT embeds stories and query like MEMO, using its encoder output as the model's output. The study involves sequences with pair-wise associations and two types of queries - 'direct' and 'indirect'. For UT, stories and query are embedded in the same way as MEMO, using its encoder output as the model's output. Graph generation involves generating graphs by uniformly sampling two-dimensional points from a unit square for shortest path experiments. The study involves generating graphs by uniformly sampling two-dimensional points from a unit square for training neural networks. The graph representation consists of tuples of integers representing connections between nodes, with a query represented similarly. The study involves generating graphs by sampling points from a unit square for training neural networks. Graph descriptions are sequences of tuples representing connections between nodes, with queries represented similarly. During training, mini-batches of 64 graphs, queries, and target paths are sampled. Queries are represented as a 64x2 matrix, targets as a 64x(L-1) matrix, and graph descriptions as a 64xMx2 matrix, where L is the length of the shortest path. During training, mini-batches of 64 graphs, queries, and target paths are sampled. Queries are represented as a 64x2 matrix, targets as a 64x(L-1) matrix, and graph descriptions as a 64xMx2 matrix, where L is the length of the shortest path. The upper bound M is fixed to the maximum number of nodes multiplied by the out-degree of the nodes in the graph. Networks were trained for 2e4 epochs, each consisting of 100 batch updates. For EMN and MEMO, the graph description is set to be the contents of their memory, with the query as input. During training, mini-batches of 64 graphs, queries, and target paths are sampled. The graph description is set to be the contents of memory for EMN and MEMO, with the query as input. The model predicts answers for nodes sequentially, with differences in how the ground truth answer is used between MEMO and EMN. The model predicts answers for nodes sequentially, with differences in how the ground truth answer is used between MEMO and EMN. The weights for each answer are not shared, and the Universal Transformer embeds the query and graph description before concatenating the embeddings. The capabilities of MEMO for reasoning over multiple steps problems involve using weights for each answer that are not shared. The Universal Transformer embeds the query and graph description before using its output as the answer, which is then used as the initial query for the next round of hops. The weights for each answer are not shared in this process. The DNC model embeds the query and graph description sequentially, using pondering steps to output the shortest path nodes. Training is done with Adam using cross-entropy loss against target sequences. The DNC and UT models use a global view to provide answers for the second node in the path. Training involves sampling target sequences and evaluating accuracy over all nodes. The report presents average values and standard deviation for the best 5 hyperparameters used. DNC and UT models have a 'global view' on the problem, allowing them to reason and work backwards from the end node to achieve better performance. In contrast, MEMO has a 'local view', where the answer to the second node depends on the answer about the first node. In contrast to DNC and UT models, MEMO has a 'local view' on the problem, where the answer to the second node depends on the answer about the first node. This means MEMO cannot do better than chance if the answer to the first node is incorrect. An experiment comparing MEMO and EMN models was conducted using different query conditions for the second node, with results summarized in Table 8. In an experiment comparing MEMO and EMN models, different query conditions for the second node were tested. When MEMO was given the ground truth for node 1 as the query for node 2, performance increased significantly compared to using the prediction of the first node. However, when EMN was trained using the same regime as MEMO, its performance dropped to almost chance level. The experiment compared MEMO and EMN models, showing that MEMO performed better when given the ground truth for node 1 as the query for node 2. However, when EMN was trained in the same way as MEMO, its performance dropped significantly. This was confirmed in a simpler scenario with 20 nodes and 3 outbound edges using the English Question Answer dataset. The training and test datasets provided with pre-processing steps such as converting text to lowercase, ignoring periods and interrogation marks, treating blank spaces as word separation tokens, and considering commas only in answers. Each input corresponds to a single answer throughout the dataset, with questions stripped out and given separately as \"queries\" to the system. At training time, a mini-batch of 128 queries is sampled from the test dataset. During training, a mini-batch of 128 queries and corresponding stories are sampled from the dataset. Queries are a matrix of 128 \u00d7 11 tokens, while stories are of size 128 \u00d7 320 \u00d7 11, with padding for incomplete sentences. Stories and queries are used as inputs for EMN and MEMO models. During training, a mini-batch of 128 queries and stories are used. Stories are of size 320 \u00d7 11, with padding for incomplete sentences. Different models handle stories and queries in various ways: EMN and MEMO use them as inputs, DNC embeds them in a similar manner to MEMO, and UT utilizes an encoder with a specific architecture. During training, a mini-batch of 128 queries and stories are used. Stories are of size 320 \u00d7 11, with padding for incomplete sentences. Different models handle stories and queries in various ways: EMN and MEMO use them as inputs, DNC embeds them in a similar manner to MEMO, and UT utilizes an encoder with a specific architecture. The model uses the encoder of UT with architecture described in Section H, followed by optimization steps using Adam for all models in the experiments. Time encoding is added to account for temporal context in MEMO. During training, models like EMN, MEMO, DNC, and UT handle stories and queries differently. MEMO adds time encoding for temporal context. Networks are trained for 2e4 epochs with 100 batch updates. Evaluation involves sampling 10,000 elements and computing mean accuracy over examples for the 20 tasks in bAbI. Average values and standard deviation are reported over the best 5 hyperparameters used. During training, models like EMN, MEMO, DNC, and UT handle stories and queries differently. MEMO adds time encoding for temporal context. The forward pass is computed in the same fashion as in training, with mean accuracy calculated over examples for the 20 tasks in bAbI. Average values and standard deviation are reported over the best 5 hyperparameters used. The network is trained for 2e4 epochs with 100 batch updates, using a cross entropy loss for prediction tasks. MEMO was trained using cross entropy loss for three tasks: predicting class ID, node ID, and word ID. Halting policy network parameters were updated using RMSProp. MEMO's temporal complexity is O(n s \u00b7 A \u00b7 N \u00b7 H \u00b7 I \u00b7 S \u00b7 d), where various parameters are defined. MEMO's temporal complexity is O(n s \u00b7 A \u00b7 N \u00b7 H \u00b7 I \u00b7 S \u00b7 d), where parameters such as the number of samples, answers, hops, heads, stories, and words in each sentence are fixed constants. The hopping procedure is done for every answer, querying memory with all its slots for all experiments. It is noted that MEMO has linear complexity compared to the quadratic complexity of the Universal Transformer. MEMO's spatial complexity is O(I \u00b7 S \u00b7 d), where the size of memory is fixed. ACT is implemented as specified in Graves (2016). The spatial complexity of MEMO is O(I \u00b7 S \u00b7 d), with a fixed memory size. ACT is implemented based on Graves (2016), defining the halting unit h differently from the original ACT. The halting unit in ACT is modified to increase fairness in comparison and enable more powerful representations. The halting probability is defined based on a minimum time step, with a fixed value for the reminder R. The halting mechanism in ACT is adjusted for fairness and stronger representations. The halting probability is determined by a minimum time step, with a fixed value for the reminder R. The architecture used is similar to previous works by Graves et al. (2016) and Dehghani et al. (2018), with hyperparameter search conducted for optimal settings. The architecture used in the study is based on previous works by Graves et al. (2016) and Dehghani et al. (2018), with hyperparameter search conducted for optimal settings. The specific implementation and hyperparameters used are described as 'universal_transformer_small' available at the given link."
}