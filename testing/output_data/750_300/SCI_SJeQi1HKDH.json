{
    "title": "SJeQi1HKDH",
    "content": "Animals develop new skills through environmental interaction and social influence. A model incorporating social influence in reinforcement learning is proposed, where agents learn from both environment and peers. A metric to measure policy distance is defined to quantify uniqueness. The Interior Policy Differentiation (IPD) algorithm encourages agents to learn unique policies while solving the primary task. The Interior Policy Differentiation (IPD) algorithm encourages agents to learn unique policies while solving the primary task, resulting in performance improvement and a collection of policies with distinct behaviors in Reinforcement Learning. The paradigm of RL involves learning through interaction with the environment to maximize rewards, inspired by cognition and animal studies. Behavioral diversity in Reinforcement Learning (RL) is crucial for species evolution and continuation. Previous works have focused on promoting behavioral diversity in RL through designing interactive environments with richness and diversity. Rich environments enable agents to learn different behaviors. Two approaches to encourage behavioral diversity in RL: designing rich environments with diversity and motivating agents to explore beyond maximizing rewards. Rich environments enable learning diverse skills, but are complex to design. Motivating agents to explore beyond rewards can increase diversity through novelty metric optimization. In this work, the concept of policy differentiation in RL is addressed to enhance the diversity of agents while maintaining their task-solving abilities. Drawing inspiration from social influence in animal societies, the goal is to improve agent diversity through social influence in reinforcement learning. The concept of social influence in reinforcement learning is introduced to enhance agent diversity by encouraging them to differentiate their actions from others. This is inspired by social influence in animal societies and aims to maintain task-solving abilities while promoting uniqueness among agents. The concept of social influence in reinforcement learning aims to differentiate agent actions and promote uniqueness. This is achieved through social uniqueness motivation and an optimization constraint using a policy distance metric. The Interior Policy Differentiation (IPD) method is introduced to provide immediate feedback in the learning process. The proposed method, Interior Policy Differentiation (IPD), introduces a novel optimization constraint using a new metric for immediate feedback in the learning process. It encourages agents to perform well in tasks while also taking unique actions compared to other agents. This approach is shown to be effective in locomotion tasks. The proposed method, Interior Policy Differentiation (IPD), introduces a new optimization constraint to motivate agents to perform well in tasks and take unique actions. It is benchmarked on locomotion tasks and shows diverse and well-behaved policies using the Proximal Policy Optimization (PPO) algorithm. Additionally, the Variational Information Maximizing Exploration (VIME) method addresses sparse reward problems by adding an intrinsic reward term based on information gains to RL algorithms. The VIME method by Houthooft et al. (2016) addresses sparse reward problems in RL by adding an intrinsic reward term based on information gains. Curiosity-driven methods by Pathak et al. (2017) and Burda et al. (2018a) define intrinsic rewards based on prediction errors of neural networks. Burda et al. (2018b) proposed Random Network Distillation (RND) to quantify exploration. Competitive Experience Replay (CER) is a method proposed by Liu et al. (2019) to define intrinsic rewards in reinforcement learning. It involves two actors and a centralized critic, with intrinsic rewards based on the state coincidence of the actors. The Task-Novelty Bisector (TNB) learning method, introduced by Zhang et al. (2019), aims to optimize both extrinsic and intrinsic rewards in reinforcement learning, addressing the trade-off between the two types of rewards. The Task-Novelty Bisector (TNB) learning method, introduced by Zhang et al. (2019), aims to optimize extrinsic and intrinsic rewards in reinforcement learning. However, the joint optimization foundation is not solid, requiring additional neural networks for evaluating novelty, leading to extra computation expenses. The Distributed Proximal Policy Optimization (DPPO) method introduced by Heess et al. (2017) enables agents to learn complex locomotion skills in diverse environments. Despite the straightforward learning reward used, the policy learned impressive and effective skills for navigating terrains and obstacles. The research by Such et al. (2018) demonstrates that different RL algorithms can lead to varying policies for the same task. Policy gradient algorithms tend to converge to a similar local optimum in Pitfall, while off-policy and value-based algorithms show differences in convergence. Rich environments can promote diverse locomotion behaviors, but manual effort is needed to design such environments. In contrast to previous research on RL algorithms converging to different policies, this paper focuses on learning diverse policies through a single algorithm and avoiding local optima. Kurutach et al. (2018) maintain model uncertainty using an ensemble of deep neural networks to encourage behavioral diversity in RL. The paper focuses on learning diverse policies through a single algorithm and avoiding local optima. Kurutach et al. (2018) use an ensemble of deep neural networks to maintain model uncertainty and encourage behavioral diversity in RL. The paper discusses learning diverse policies using a single algorithm to avoid local optima. It introduces the concept of a metric space and Total Variance Divergence to measure the distance between policies. The Total Variance Divergence is used to measure the distance between policies in a metric space. It can be extended to continuous state and action spaces. The factor 1/2 in the calculation is omitted for conciseness. The goal is to maximize the uniqueness of a new policy in reinforcement learning. In reinforcement learning, the Total Variance Divergence is utilized to measure policy distance in continuous state and action spaces. The factor 1/2 is omitted for conciseness. The aim is to maximize the uniqueness of a new policy by calculating D \u03c1 T V (\u03b8 i , \u03b8 j ) through Monte Carlo estimation, facing challenges in obtaining sufficient samples in continuous state cases. In reinforcement learning, Total Variance Divergence measures policy distance in continuous state and action spaces. To improve sample efficiency, an approximation is proposed by dividing the domain of possible states and approximating \u03c1(s) using a fixed behavior policy \u03b8. This approach faces challenges in obtaining sufficient samples in continuous state cases. To improve sample efficiency in reinforcement learning, an approximation is proposed by dividing the domain of possible states and using a fixed behavior policy \u03b8. This approach requires the domain of possible states to be similar between different policies, allowing for the use of \u03c1(s|s \u223c \u03b8) as the choice of \u03c1(s). In practice, this condition is typically met by adding noise to \u03b8, ensuring that the properties hold. In more general cases, sampling from S \u03b8 \u222a S \u03b8j is necessary to satisfy the properties. In practice, to satisfy the properties in Definition 1, sampling from S \u03b8 \u222a S \u03b8j is necessary. By plugging Eq.(4) into Eq. (2), the objective function of policy differentiation is obtained. Enabling sufficient exploration in training and initialization of \u03b8 can make the last term disappear. The last term in the objective function of policy differentiation is related to the domain S \u03b8. With sufficient exploration in training and initialization of \u03b8, this term can disappear. Proposition 1 states that estimating \u03c1 \u03b8 (s) using a single trajectory \u03c4 is unbiased. The next step is to develop an efficient learning algorithm for maximizing cumulative rewards in traditional RL. After defining uniqueness and a sampling method, the focus shifts to developing an efficient learning algorithm in traditional RL. The objective is to maximize cumulative rewards by considering both reward from the primal task and policy uniqueness. Previous approaches have combined these factors to enhance behavioral diversity among agents. The objective in traditional RL is to maximize cumulative rewards by considering both reward from the primal task and policy uniqueness. Previous approaches have combined these factors to enhance behavioral diversity among agents. The selection of weight parameter \u03b1 and formulation of intrinsic reward r int can significantly impact the results. In traditional RL, the objective is to maximize cumulative rewards by considering both the primal task reward and policy uniqueness. The selection of weight parameter \u03b1 and formulation of intrinsic reward r int can significantly impact results. To address these issues, inspiration is drawn from social uniqueness, treating it as a constraint rather than an additional target in multi-objective optimization. To address issues in traditional RL, inspiration is drawn from social uniqueness, treating it as a constraint in multi-objective optimization. The optimization problem is transformed into a constrained optimization problem with a penalty method. In this work, the constrained optimization problem is tackled using Interior Point Methods (IPMs) instead of the Task Novel Bisector (TNB) approach. The difficulty lies in selecting the penalty coefficient \u03b1, which is addressed by Zhang et al. (2019). In this work, the constrained optimization problem is solved using Interior Point Methods (IPMs) instead of Task Novel Bisector (TNB) approach. The approach involves reforming the problem to an unconstrained form with a barrier term in the objective. The solution is obtained by setting the limit of the barrier term to zero. Refer to Appendix G for more discussion on the connection between these novel policy seeking methods and constrained optimization methods. In our proposed RL paradigm, the learning process is influenced by peers, allowing for a more natural way to bound collected transitions in the feasible region. This approach is computationally efficient and stable, especially when \u03b1 is small. During the training process of new agents, the feasible region is bounded by terminating any agent that steps outside it. This ensures that all collected samples are within the feasible region, reducing the likelihood of appearing in previously trained policies. As a result, a new policy is naturally obtained at the end of training. During training, valid samples are kept within the feasible region to ensure uniqueness in the new policy. This eliminates the need to balance intrinsic and extrinsic rewards, making the learning process more robust without objective inconsistency. The approach is named Interior Policy Differentiation (IPD) method, inspired by IPMs. The Interior Policy Differentiation (IPD) method is more robust and eliminates objective inconsistency by ensuring uniqueness in the new policy during training. It is inspired by IPMs and tested on MuJoCo environments like Hopper-v3, Walker2d-v3, and HalfCheetah-v3. All environment parameters are set to default values in the experiments. In experiments on MuJoCo environments like Hopper-v3, Walker2d-v3, and HalfCheetah-v3, different policies can be generated by selecting different random seeds. The study aims to benchmark the uniqueness generated from stochasticity in vanilla RL algorithms and random weight initialization before applying a method to enhance behavior diversity. The proposed method is based on PPO. Our method aims to enhance behavior diversity by benchmarking the uniqueness generated from stochasticity in vanilla RL algorithms and random weight initialization. The proposed method is based on PPO and is compared with TNB and WSR approaches. The implementation details are provided in Appendix D, and Theorem 2 shows how the uniqueness metric can be approximated. Our method, based on PPO, aims to enhance behavior diversity by utilizing a uniqueness metric in learning new policies. The implementation details are provided in Appendix D, and Theorem 2 shows how the uniqueness metric can be approximated. The method is compared with TNB and WSR approaches in the same experimental settings. The method enhances behavior diversity by training 10 unique policies sequentially using PPO. Qualitative results are visualized in Fig.2, showing agent motion with fixed settings for frame frequency and velocity correlation. The visualization in Fig. 3 displays the pose of agents at different time steps, with frames spaced based on agent velocity. The experimental results show uniqueness and performance of policies, with higher performance in more unique policies. Our experimental results in Fig. 3 show the uniqueness and performance of policies, with higher performance in more unique policies. Our proposed method outperforms others in Hopper and HalfCheetah, while in Walker2d, both WSR and our method improve policy uniqueness but do not surpass PPO in performance. Detailed comparisons on task-related rewards are in Table 1, and performance of trained policies is depicted in Fig. 5, Fig. 6, and Fig. 7 in Appendix C. In Table 1, detailed comparisons on task-related rewards are provided. Figures 5 and 6 in Appendix C show the performance of each trained policy and their reward gaining curve. Additionally, Figure 7 in Appendix C offers more detailed results from the view of uniqueness. Success rate is also used as a metric to compare different approaches, considering a policy successful if its performance is at least as good as the averaged performance of policies trained without social influences. The success rate of different approaches is evaluated based on their performance compared to a baseline. The new policy is considered successful if it surpasses the baseline during training, indicating it does not sacrifice performance for unique behavior. Results show that the method consistently outperforms the baseline during training. Our method consistently outperforms the baseline during training, ensuring performance is maintained. Significant improvements were observed in the Hopper and HalfCheetah environments, with our method preventing policies from falling into undesirable behavior patterns. Our method prevents policies in the Hopper environment from always falling into the same local minimum, encouraging exploration of different action patterns to improve performance. This enhancement of traditional RL schemes emphasizes the importance of exploring new approaches rather than getting stuck in suboptimal behaviors. Our method enhances traditional RL schemes by promoting exploration of different action patterns to improve performance in the HalfCheetah environment. Unlike other environments, HalfCheetah lacks an explicit termination signal, leading to unique challenges in policy optimization. In the HalfCheetah environment, there is no explicit termination signal, making policy optimization challenging. Our learning scheme involves agents interacting with peers to receive termination signals and avoid random actions, leading to improved performance. During the learning process, agents in our method first learn to terminate themselves early to avoid control costs, then adapt to pursue higher rewards. This process acts as an implicit curriculum, with the challenge of finding unique policies increasing as more policies are learned with social influence. Later policies must diverge from previous solutions. Ablation studies show how performance changes with different scales. In this work, an efficient approach is developed to motivate RL to learn diverse policies. The performance decrease under different scales of social influence is more pronounced in the Hopper environment due to its limited 3-dimensional action space. In this work, an efficient approach called Interior Policy Differentiation (IPD) is developed to motivate RL to learn diverse strategies inspired by social influence. IPD defines policy uniqueness and uses Interior Point Methods to address the constrained optimization problem. Experimental results show that IPD can learn various well-behaved policies, helping agents avoid local minimums. Our proposed method, Interior Policy Differentiation (IPD), draws insights from Interior Point Methods to address the constrained optimization problem in RL. Experimental results demonstrate that IPD can help agents learn diverse strategies, avoid local minimums, and exhibit implicit curriculum learning in certain cases. The triangle inequality is discussed in relation to policy uniqueness in Hopper, Walker2d, and HalfCheetah environments. TNB and WSR can optimize uniqueness directly, sometimes exceeding the proposed method, but this can lead to a decrease in task performance. Careful hyper-parameter tuning and reward shaping are necessary to address this trade-off. The deterministic part of policies is used in the calculation of DTV, removing Gaussian noise. In the context of optimizing policy uniqueness in environments like Hopper, Walker2d, and HalfCheetah, hyper-parameter tuning and reward shaping are crucial. The calculation of DTV involves using the deterministic part of policies and removing Gaussian noise. Actor models in PPO utilize MLP with 2 hidden layers, with varying unit numbers for different tasks. In the ablation study, the choice of unit number in the second layer is detailed in Table 2, Table 3, and Fig. 8. Hidden units of 10, 64, and 256 are used for the tasks based on success rate, performance, and computation expense considerations. Training timesteps are fixed at 1M for Hopper-v3, 1.6M for Walker2d-v3, and 3M for HalfCheetah. The policy uniqueness magnitude can be controlled by adjusting the constraint threshold r0 in the proposed method. In our experiments, training timesteps are fixed at 1M for Hopper-v3, 1.6M for Walker2d-v3, and 3M for HalfCheetah. The policy uniqueness can be adjusted by changing the constraint threshold r0. Different thresholds lead to varying policy behaviors, with a larger threshold causing more significant differences in agent performance. Constraints in Eq. (7) are not used to avoid restricting the learning algorithm's ability to find feasible solutions. In experiments, training timesteps are fixed for different environments. The constraint threshold r0 adjusts policy uniqueness, affecting agent performance. Constraints in Eq. (7) are not used to avoid restricting feasible solutions. A larger threshold leads to more significant differences in performance. Cumulative uniqueness is used as constraints instead. Testing with different threshold values shows varying agent performance. In experiments, training timesteps are fixed for different environments. The constraint threshold r0 adjusts policy uniqueness, affecting agent performance. Constraints in Eq. (7) are not used to avoid restricting feasible solutions. Instead, cumulative uniqueness is used as constraints. Testing with different threshold values shows varying agent performance. The performance of agents under different thresholds is shown in Fig. 9 and a detailed analysis of their success rate is presented in Table 2. The constraints in the optimization problem are based on cumulative uniqueness and can be applied after a certain number of timesteps. Different methods like WSR, TNB, and IPD correspond to various approaches in constrained optimization. The optimization of policy is done using stochastic gradient descent on batches of trajectory samples. The Penalty Method incorporates constraints by penalizing violations. The optimization of policy with stochastic gradient descent is simplified using the Penalty Method to handle constraints. The selection of a fixed weight term \u03b1 is crucial for the final solution, as it affects the gradient calculation. The Taylor series approximation of g(\u03b8) is used to iteratively solve the unconstrained problem. The optimization methods for policy involve choosing a fixed weight term \u03b1 to adjust the gradient calculation. The Feasible Direction Method (FDM) and TNB method use different approaches to handle constraints and select directions for optimization. The TNB method is more strict in its optimization process compared to the FDM. The TNB method in policy optimization uses the bisector of gradients to select the learning stride, which heavily relies on the shape of the function g(\u03b8). It suggests using a barrier term of -\u03b1 log g(\u03b8) with a small positive \u03b1 to introduce minimal influence on the objective. The TNB method in policy optimization suggests using a barrier term of -\u03b1 log g(\u03b8) with a small positive \u03b1 to introduce minimal influence on the objective. Directly applying this method can be computationally challenging and numerically unstable, especially when \u03b1 is small. A more natural approach is to choose a sequence of decreasing \u03b1 values to get closer to the primal objective. During policy optimization, a sequence of decreasing \u03b1 values can be chosen to approach the primal objective. To avoid computational challenges and numerical instability, transitions can be bounded within a feasible region by terminating new agents that step outside it. This ensures that all collected samples during training are valid. During policy optimization, transitions can be bounded within a feasible region by terminating new agents that step outside it. This ensures that all collected samples during training are valid and leads to a new policy with sufficient uniqueness, eliminating the need to balance intrinsic and extrinsic rewards. The learning process becomes more robust and objective inconsistency is no longer a concern. The learning process becomes more robust with a new policy that eliminates the need to balance intrinsic and extrinsic rewards. The algorithm for IPD based on PPO includes additions to the primal PPO algorithm."
}