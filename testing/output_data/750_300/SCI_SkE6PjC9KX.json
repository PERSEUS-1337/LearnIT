{
    "title": "SkE6PjC9KX",
    "content": "Neural Processes (NPs) approach regression by learning to map observed input-output pairs to a distribution over regression functions efficiently. However, NPs suffer from underfitting, leading to inaccurate predictions at the inputs of the observed data they condition on. This issue is addressed by incorporating attention. Incorporating attention into Neural Processes (NPs) improves prediction accuracy, speeds up training, and expands the range of functions that can be modeled. Regression tasks involve modeling the distribution of output given input using a deterministic function like a neural network. Incorporating attention into Neural Processes (NPs) improves prediction accuracy, speeds up training, and expands the range of functions that can be modeled. Regression tasks involve modeling the distribution of output given input using a deterministic function like a neural network. An alternative approach to regression involves computing a distribution over functions that map inputs to outputs, allowing for reasoning about multiple functions consistent with the data. Incorporating attention into Neural Processes (NPs) improves prediction accuracy and expands the range of functions that can be modeled. NPs offer an efficient method to model a distribution over regression functions, with prediction complexity linear in the context set size. They can predict the distribution of an arbitrary target output. Neural Processes (NPs) like GPs are efficient for modeling distribution over regression functions. NPs can predict the distribution of a target output based on context input-output pairs of any size, allowing them to model data generated from a stochastic process. NPs are trained on samples from multiple realizations of a stochastic process, while GPs are typically trained on observations from one. Neural Processes (NPs) and Gaussian Processes (GPs) have different training regimes. NPs are trained on samples from multiple realizations of a stochastic process, while GPs are usually trained on observations from one realization. NPs tend to underfit the context set, leading to inaccurate predictive means and overestimated variances. Neural Processes (NPs) have a weakness in underfitting the context set, resulting in inaccurate predictions and overestimated variances. The encoder aggregates context to a fixed-length latent summary, and the decoder maps this to the target output. The encoder in Neural Processes (NPs) aggregates context to a fixed-length latent summary, which may act as a bottleneck causing underfitting. Increasing dimensionality of the representation is not sufficient to address this issue. In Neural Processes, the encoder aggregates context to a fixed-length latent summary, potentially causing underfitting. Increasing representation dimensionality is not enough to solve this issue. Inspired by Gaussian Processes, a kernel measures similarity between input points, ensuring relevant context points for predictions. The Attentive Neural Processes (ANPs) improve upon Neural Processes (NPs) by using differentiable attention to attend to relevant contexts for a given target, preserving permutation invariance. ANPs show significant enhancements in context reconstruction and training speed for 1D function regression and 2D image regression tasks. The Attentive Neural Processes (ANPs) improve upon Neural Processes (NPs) by utilizing differentiable attention to focus on relevant contexts for a target, maintaining permutation invariance. ANPs demonstrate superior context reconstruction and training speed for 1D function regression and 2D image regression tasks, showing enhanced expressiveness compared to NPs. The Neural Process (NP) is a regression model that maps input x to output y. It defines a family of conditional distributions, allowing for conditioning on observed contexts to model targets in a permutation-invariant way. The model uses a deterministic function to aggregate context information into a finite dimensional representation. The Neural Process (NP) models conditional distributions using a deterministic function to aggregate context information into a finite dimensional representation. The likelihood is modelled by a Gaussian factorised across targets with mean and variance obtained through an MLP. The unconditional distribution is defined by a fixed vector when no context is present. The Neural Process (NP) model uses a deterministic function to model conditional distributions, with a Gaussian likelihood factorised across targets. A global latent variable z is included to account for uncertainty in predictions, modelled by a factorised Gaussian parametrised by s. The Neural Process model incorporates a global latent variable z modelled by a factorised Gaussian parametrised by s. This complements the deterministic path in the model and allows for different realisations of the data generating stochastic process. The encoder consists of q, r, s, while the likelihood is the decoder. The model can be defined using the deterministic path, latent path, or both. The Neural Process model incorporates a global latent variable z to model different realisations of the data generating stochastic process. The model can be defined using the deterministic path, latent path, or both. The parameters of the encoder and decoder are learned by maximising the ELBO for a random subset of contexts and targets. The Neural Process model learns encoder and decoder parameters by maximizing ELBO for random subsets of contexts and targets using the reparametrisation trick. The model reconstructs targets with a KL term to ensure context and target summaries are close. Training iterations involve randomly selecting contexts and targets to facilitate learning. Neural Processes (NPs) learn a wide family of conditional distributions by randomly selecting contexts and targets at each training iteration. NPs offer scalability, flexibility, and permutation invariance in predicting targets. Neural Processes (NPs) offer scalability, flexibility, and permutation invariance in predicting targets by learning a wide family of conditional distributions. However, they do not satisfy consistency in the contexts, as the distribution of targets may vary depending on the order in which they are generated. Neural Processes (NPs) approximate the conditionals of the consistent data-generating stochastic process by minimizing the KL divergence. An attention mechanism computes weights for key-value pairs to aggregate values corresponding to a query. An attention mechanism computes weights for key-value pairs to aggregate values corresponding to a query, with the queried values being invariant to the ordering of the pairs. This permutation invariance property is key in the application of attention to Neural Processes (NPs). The idea of using a differentiable addressing mechanism that can be learned from data has been successfully applied in various areas of Deep Learning. Attention mechanisms are key in Neural Processes (NPs) and have been successfully applied in various areas of Deep Learning. Differentiable addressing mechanisms learned from data have been used in handwriting generation, recognition, neural machine translation, natural language processing, and image modeling. Examples of attention mechanisms include key-value pairs arranged in matrices for queries. Attention mechanisms, such as dot-product attention and Laplace kernel, are used in neural processes for tasks like natural language processing and image modeling. Key-value pairs in matrices are utilized for queries, allowing for similarity measurements and weighting of keys. The multihead attention architecture extends dot-product attention by linearly transforming keys, values, and queries for each head, applying dot-product attention, and concatenating the resulting values for final output. The multihead attention architecture extends dot-product attention by linearly transforming keys, values, and queries for each head, applying dot-product attention, and concatenating the resulting values for final output. Self-attention is then applied to compute representations of each (x, y) pair, allowing the query to attend to different keys for each head. This process results in smoother query-values compared to dot-product attention. Self-attention is used to compute representations of each (x, y) pair in the context points, allowing the target input to predict the target output by attending to these context representations. The self-attention mechanism helps model interactions between context points, giving high weight to relevant points and obtaining richer representations. The self-attention mechanism models interactions between context points by giving high weight to relevant points and obtaining richer representations. Higher order interactions are modeled by stacking self-attention, as seen in Vaswani et al. (2017). In the deterministic path, mean-aggregation is replaced by a cross-attention mechanism where each target query attends to the context to produce a query-specific representation. In the deterministic path, mean-aggregation is replaced by a cross-attention mechanism where each target query attends to the context to produce a query-specific representation, allowing closer attention to relevant context points for prediction. The latent path preserves global latent dependencies between target predictions, with z inducing correlations in the marginal distribution of y T. The latent path aims to preserve global latent dependencies in the target predictions, while the deterministic path focuses on local structure. The decoder is modified to use query-specific representation instead of shared context. Permutation invariance is maintained with the attention mechanism, and uniform attention leads to recovering the NP model. The NP with attention modifies the decoder to use query-specific representation instead of shared context, preserving permutation invariance. However, this added expressivity increases computational complexity to O(n(n + m)). The NP with attention increases computational complexity to O(n(n + m)), but most of the computation is done in parallel. Training time for ANPs is comparable to NPs, with ANPs learning significantly faster despite being slower at prediction time. In practice, ANPs learn significantly faster than NPs in terms of training iterations and wall-clock time, despite being slower at prediction time. The (A)NP learns a stochastic process and should be trained on multiple functions that are realisations of the process. At each training iteration, a batch of realisations is drawn from the data generating process, with random points selected as targets and contexts to optimize the loss. The (A)NPs are trained on data generated from a Gaussian Process with a squared-exponential kernel and small likelihood noise. It is emphasized that (A)NPs do not necessarily need to be trained on GP data or data from a known stochastic process. Two settings are explored, one involving the hyperparameters of the kernel. In this illustrative example, Gaussian Process models are trained on data with a squared-exponential kernel and small likelihood noise. Two settings are explored, one with fixed kernel hyperparameters and another with randomly varying hyperparameters at each training iteration. The number of contexts and targets are randomly chosen, and cross-attention is used in the deterministic path for 1D data analysis. For 1D data analysis, random values are chosen for the number of contexts and targets. Cross-attention is used in the deterministic path, showing ANP's faster error reduction and lower values compared to NP, especially with dot product and multihead attention. The ANP attention mechanism shows faster error reduction and lower values compared to NP, especially with dot product and multihead attention. Despite the added computational cost, learning is fast. Laplace and dot-product ANP have similar computation times to NP, while multihead ANP takes around twice the time. The size of the bottleneck in the deterministic and latent paths of the NP affects underfitting behavior. The size of the bottleneck (d) in the deterministic and latent paths of the NP affects underfitting behavior. Increasing d can improve reconstructions, but there is a limit to the improvement. Beyond a certain value, learning becomes slow, and reconstruction error remains higher than that of multihead ANP with significantly less time. Using ANPs offers significant benefits over simply increasing the bottleneck size in NPs. In the context of NP bottleneck size (d), increasing it can improve reconstructions but has a limit. Beyond a certain value, learning slows down, and reconstruction error is higher than with multihead ANP in less time. ANPs offer significant advantages over just raising the bottleneck size in NPs. Visualizing the learned conditional distribution in FIG2 (right) shows differences in attention mechanisms. The NP underfits the context, explaining data with large likelihood noise, while Laplace and dotproduct attention exhibit similar behavior. The predictive mean of the NP underfits the context, while Laplace and dotproduct attention show similar behavior. Laplace attention is parameter-free, using keys and queries as x-coordinates, while dot-product attention uses parameterized representations of x-values for computing similarities. The dot-product attention, using parameterized representations of x-values, outperforms Laplace attention which computes similarities based on L1 distance in the x-coordinate domain. However, dot-product attention displays non-smooth predictions, while multiple heads in multihead attention help smooth out interpolations for better context reconstruction and target prediction. The multiple heads in multihead attention help smooth out interpolations for better context reconstruction and target prediction, displaying increased predictive uncertainty away from the contexts like a GP. The ANP is more expressive than the NP, as evidenced by its performance with different kernel hyperparameter settings. The ANP is more expressive than the NP, as shown by its performance with different kernel hyperparameter settings. It can learn a wider range of functions and tackle a toy Bayesian Optimization problem effectively. See Appendix C for more details and analysis of results. The ANP is capable of sampling entire functions from a GP prior and accurately reconstructing context. Image data can be interpreted as generated from a stochastic process, with pixel intensity prediction as a regression problem. The ANP is trained on MNIST and CelebA datasets. The ANP is trained on MNIST and CelebA datasets to map pixel locations to pixel intensities. Self-attentional layers are used in the encoder, and three different models are compared: NP, Multihead ANP, and ANP with multihead attention in the deterministic path. The encoder uses self-attentional layers and compares three models: NP, Multihead ANP, and Stacked Multihead ANP on MNIST and CelebA datasets. Results show predictions for varying numbers of random context pixels. The NP and Stacked Multihead ANP models are compared on MNIST and CelebA datasets using varying numbers of random context pixels. Stacked Multihead ANP provides accurate reconstructions indistinguishable from the original image, while NP gives reasonable predictions with diversity but less accurate reconstructions. The use of attention in Stacked Multihead ANP results in crisper inpaintings, enhancing its ability to model less smooth 2D functions compared to NP. The use of attention in Stacked Multihead ANP results in crisper inpaintings, enhancing its ability to model less smooth 2D functions compared to NP. The diversity in faces and digits obtained with different values of z is apparent in the samples, providing evidence for z's ability to model global structure of the image. The Stacked Multihead ANP improves context reconstruction error and NLL for target points compared to NP. It shows gains in crispness and generalization to larger context sizes. Improved context reconstruction error and NLL for target points with multihead crossattention and stacked self-attention. Noticeable gains in crispness and global coherence with stacked self-attention. Visualizing each head of Multihead ANP for CelebA shows different roles for each head. Visualizing each head of Multihead ANP for CelebA reveals distinct roles for different heads, such as focusing on target pixels, nearby regions, columns, or symmetry of faces. Consistent behavior is observed across various target pixels. The Multihead ANP for CelebA shows distinct roles for different heads, focusing on target pixels, nearby regions, columns, or symmetry of faces. The model can map images from one resolution to another by predicting pixel intensities in a continuous space. The Multihead ANP for CelebA can map images from one resolution to another by predicting pixel intensities in a continuous space. Using one grid as context and a finer grid as target, the model can map a given resolution to a higher resolution. ANPs may provide accurate reconstructions for reliable mappings between different resolutions. The Multihead ANP for CelebA can accurately map images between different resolutions by predicting pixel intensities in a continuous space. Results show that the ANP trained on 32 \u00d7 32 images can map low resolutions (4 \u00d7 4 or 8 \u00d7 8) to realistic 32 \u00d7 32 outputs with some diversity. This performance is expected since the model has been trained on data with 32 \u00d7 32 resolution. The Multihead ANP for CelebA can accurately map images between different resolutions, showing that the model trained on 32 \u00d7 32 images can generate realistic 32 \u00d7 32 outputs from low resolutions (4 \u00d7 4 or 8 \u00d7 8). Additionally, the model can also map to even higher resolutions, such as 256 \u00d7 256, producing high-resolution images with sharper edges compared to baseline methods. There is evidence that the model learns an internal representation of facial appearance. The model can generate realistic high-resolution images with sharper edges compared to baseline methods, and it learns an internal representation of facial appearance, such as filling in details like eyes even in coarse images. See FIG0 in Appendix E for larger images. The ANP is not meant to replace state-of-the-art algorithms for image inpainting or super-resolution, but rather to showcase its flexibility in modeling various conditional distributions. The ANP is not intended to replace advanced algorithms for image inpainting or super-resolution. Instead, it demonstrates flexibility in modeling different conditional distributions. The use of attention in NPs is motivated by similarities with GP kernels in measuring similarity between points. The use of attention in NPs is motivated by similarities with GP kernels in measuring similarity between points. This approach is related to Deep Kernel Learning, where a GP is applied to learned data representations in an embedding space. Training regimes of GPs and NPs differ, making direct comparison challenging. In NPs, learning is still done in a GP framework by maximizing marginal likelihood. Comparing GPs and NPs is challenging due to different training regimes. One way to compare is updating GP kernel hyperparameters like NPs, but this still has computational costs. GPs' predictive uncertainties depend on kernel choice, while NPs learn uncertainties directly from data. Variational Implicit Processes (VIP) BID19 are related to NPs, where VIP defines a stochastic process using the same decoder setup with a finite mini-batch of data. GPs have the benefit of being consistent stochastic processes, with exact expressions for covariance between predictions and marginal variance, a feature not present in the current formulation of (A)NPs. The current formulation of (A)NPs lacks exact closed-form expressions for marginal variance of predictions. Variational Implicit Processes (VIP) are related to NPs but approximate the process and posterior with GPs using a Wake-Sleep algorithm. Meta-Learning (A)NPs focus on few-shot learning, reasoning about new functions based on predictive distribution. Few-shot learning in Meta-Learning (A)NPs involves reasoning about new functions based on predictive distribution. Works by Vinyals et al. (2016), Snell et al. (2017), and Santoro et al. (2016) use attention for few-shot classification tasks. Attention is also utilized in Meta-RL for tasks like continuous control and visual navigation. Few-shot density estimation with attention has been extensively explored in various works. Attention has been used in Meta-RL for tasks like continuous control and visual navigation. Few-shot density estimation with attention has also been explored extensively in various works. Specifically, the Neural Statistician and the Variational Homoencoder have similar permutation invariant encoders but use local latents on top of a global latent. For Attentive Neural Processes (ANPs), regression in a less-explored setting is considered. The authors explore regression on a toy 1D domain using a setup similar to Neural Processes (NPs) but optimizing an approximation to the entropy of the latent function. Multitask learning in Gaussian Processes (GPs) has also been addressed in the literature. Generative Query Networks are models for spatial prediction that render a frame of a scene given a viewpoint, corresponding to a special case of NPs. The authors propose ANPs, which augment NPs with attention to address underfitting issues and improve prediction accuracy. They apply attention to resolve the problem of 3D localization with an attention mechanism, focusing on patches of context frames instead of parametric representation of viewpoints. In their work, the authors propose Attention Augmented Neural Processes (ANPs) to address underfitting problems and enhance prediction accuracy by incorporating attention. They suggest incorporating cross-attention into the latent path and modeling dependencies across local latents, similar to the setup of the Neural Statistician but adapted to the regression setting. This approach improves prediction accuracy, speeds up training, and expands the range of functions that can be modeled. Future work includes exploring different model architectures to further enhance ANPs. One way to enhance ANPs is by incorporating cross-attention into the latent path and modeling dependencies across local latents. An interesting application could be training ANPs on text data to fill in blanks in a stochastic manner. The Image Transformer (ImT) BID21 has connections with ANPs, as its local self-attention parallels how ANPs attend to context pixels to predict target pixels. The Image Transformer (ImT) BID21 has connections with ANPs, as its local self-attention parallels how ANPs attend to context pixels to predict target pixels. By replacing the MLP in the decoder of the ANP with self-attention across the target pixels, a model closely resembling an ImT defined on arbitrary orderings of pixels can be created. This contrasts with the original ImT, which assumes a fixed ordering and is trained autoregressively. The plan is to equip ANPs with self-attention in the decoder to extend their expressiveness, although in this setup, targets will affect each other's predictions. Incorporating self-attention in the decoder of ANPs extends their expressiveness, impacting target predictions. Architectural details of NP and Multihead ANP models for regression experiments are shown in Figure 8. All MLPs have relu non-linearities except the final layer. The latent path outputs \u00b5 z , \u03c9 z \u2208 R d. The architectural details of the NP and Multihead ANP models for regression experiments are shown in Figure 8. The models use relu non-linearities in MLPs, with the final layer having no non-linearity. The latent path outputs parameterize the distributions, and the decoder outputs parameterize the target predictions. The 1D regression experiments utilize multihead cross-attention, while the 2D regression experiments use a different form of multihead cross-attention. The 1D regression experiments use multihead cross-attention (M ultihead 1) with a softplus function for regression. In contrast, the 2D regression experiments employ a different form of multihead cross-attention from the Image Transformer BID21. The self-attention module can be stacked with 2 layers for |C| input representations to output |C| representations. The self-attention module in the 2D Image regression experiments uses the same architecture as cross-attention but with modified parameters. Stacking more layers did not significantly improve results. Different kernel hyperparameters were used for fixed and random cases, with a noise level of \u03c3 n = 0.02. In the experiments, a length scale of 0.6 and kernel scale of 1 were used for the fixed kernel hyperparameter case. For the random kernel hyperparameter case, parameters were sampled from specific ranges. A batch size of 16 was utilized, along with the Adam Optimiser with a fixed learning rate. In the experiments, 16 random hyperparameter values were sampled to draw curves from GPs. The Adam Optimiser BID14 with a fixed learning rate of 5e-5 was used, along with Tensorflow defaults for other hyperparameters. A MC estimate of the loss was formed using one sample of q(z|s C). The trained (A)NP models were compared against the oracle GP, showing that the Multihead ANP is closer to the oracle GP but still underestimates predictive variance due to variational inference. The Multihead ANP is closer to the oracle GP than the NP but still underestimates predictive variance due to variational inference. Investigating how to address this issue would be interesting. The conditional distributions for fixed kernel hyperparameters show non-smooth behavior for dot-product attention, resembling a nearest neighbor predictor. The conditional distributions for fixed kernel hyperparameters exhibit non-smooth behavior in dot-product attention, resembling a nearest neighbor predictor. This behavior arises when dot-product attention collapses to a local minimum, leading to good reconstructions but poor interpolations between context points. The KL term in the NP loss differs between training on fixed and random kernel hyperparameter GP data. In the fixed hyperparameter case, the model quickly deems the deterministic path sufficient for accurate predictions. However, in the random hyperparameter case, the attention gives a non-zero KL to model uncertainty in the data. The model uses latents to model uncertainty in the realisation of the stochastic process given context points, indicating multiple realisations can explain the contexts well. The (A)NPs trained on 1D GP data are used to tackle the BO problem of finding the minimum of test functions drawn from a GP prior, comparing different attention mechanisms. The (A)NPs use latents to model variation in contexts and tackle the BO problem by considering all previous function evaluations as context points. Thompson sampling is used to act based on the minimal predicted value. Results are shown in FIG0. The (A)NPs use latents to model variation in contexts and tackle the BO problem using Thompson sampling. Results in FIG0 show that NP with multihead attention has the smallest simple regret, approaching the oracle GP. The cumulative regret decreases most rapidly for multihead, utilizing previous function evaluations effectively for predicting the function minimum. The NP with multihead attention has the smallest simple regret, approaching the oracle GP. The cumulative regret decreases rapidly for multihead, utilizing previous function evaluations effectively for predicting the function minimum. The lower cumulative regret initially is due to under-exploration, with uncertainties of ANP smaller than the oracle GP. Random pixels of an image are taken as targets during training, with a subset chosen as contexts. In training, random pixels of an image are selected as targets, with a subset chosen as contexts. The x values are rescaled to [-1, 1] and y values to [-0.5, 0.5]. A batch size of 16 is used for both MNIST and CelebA datasets, with a learning rate of 5e-5 and 4e-5 respectively. The stacked self-attention architecture is similar to the Image Transformer BID21, without Dropout for stochasticity control. The architecture for both MNIST and CelebA datasets uses a learning rate of 5e-5 and 4e-5 respectively, with a stacked self-attention design similar to the Image Transformer BID21 but without Dropout. Little tuning has been done on the architectural hyperparameters, and one sample of q(z|s C ) is used for a MC estimate of the loss. The NP overestimates the predictive variance visually. The NP overestimates predictive variance visually, with Stacked Multihead ANP showing significant improvement over Multihead ANP in image sharpness and global coherence. The uncertainty is reduced significantly with the NP with attention as the number of contexts increases. Stacked Multihead ANP improves results over Multihead ANP, providing sharper images with better global coherence even when the face isn't axis-aligned. Different heads play various roles in predicting targets, even when the target is disjoint from the context. All heads become useful for target prediction in such cases. In the NP with attention, different heads play various roles in predicting targets, even when the target is disjoint from the context. All heads become useful for target prediction in such cases."
}