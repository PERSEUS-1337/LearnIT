{
    "title": "SJxDDpEKvH",
    "content": "Deep generative models can emulate complex image datasets, providing a latent representation of the data. Manipulating this representation for meaningful transformations remains challenging without supervision. A non-statistical framework is proposed, based on identifying a modular organization of the network through counterfactual manipulations. Experiments show modularity between groups of channels is achieved to a certain degree. The non-statistical framework proposes a modular organization of the network through counterfactual manipulations. Experiments show modularity between groups of channels is achieved to a certain degree, allowing for targeted interventions on complex image datasets. This opens up possibilities for computationally efficient style transfer and automated assessment of robustness in pattern recognition systems. Deep generative models like Generative Adversarial Networks (GAN) and Variational Autoencoders (VAE) have been successful in designing realistic images across various domains. Efforts have been made to create disentangled latent representations for controlling interpretable properties of images. Efforts have been made to create disentangled latent representations in generative models like GAN and VAE for controlling interpretable properties of images, but these models may not be mechanistic or causal in attributing interpretable properties to specific parts of the network architecture. Access to a modular organization of generative models would enhance interpretability and enable extrapolations, such as generating objects in new backgrounds. Access to a modular organization of generative models would enhance interpretability and enable extrapolations, such as generating objects in new backgrounds, which aligns with human representational capabilities. In this paper, a causal framework is proposed to explore modularity in deep generative architectures, aiming to enhance interpretability and enable extrapolations for robust decision making in the visual system. The paper proposes a causal framework to explore modularity in deep generative architectures, focusing on the principle of Independent Mechanisms to allow for individual modifications without affecting each other. The study examines the impact of direct interventions in the network, focusing on modifying individual mechanisms without affecting each other. It applies this principle to generative models to evaluate their ability to capture causal mechanisms and uses counterfactuals to assess the role of specific internal variables. The analysis includes a rigorous definition of disentanglement within a causal framework. The study evaluates the role of specific internal variables in deep generative models using counterfactuals. It shows how VAEs and GANs exhibit modularity in their hidden units, allowing for counterfactual editing of generated images. The study explores how VAEs and GANs demonstrate modularity in their hidden units, enabling the editing of generated images. It relates to the interpretability of convolutional neural networks and addresses disentanglement of latent variables in generative models. The concept of intrinsic disentanglement is introduced to uncover the internal organization of networks, arguing that many transformations are statistically dependent and unlikely to be disentangled in the latent space. This relates to previous works addressing supervised or unsupervised disentanglement of latent variables in generative models. The concept of intrinsic disentanglement in networks is discussed, highlighting the challenges of statistically dependent transformations. Different approaches, such as interventions on internal variables and group representation theory, have been proposed for disentanglement. A flexible approach, introduced independently, allows for arbitrary continuous transformations without the strong requirements of representation theory. Our approach, introduced independently, is more flexible as it applies to arbitrary continuous transformations without the strong requirements of representation theory. Additionally, an interventional approach to disentanglement has been taken by Suter et al. (2018), focusing on extrinsic disentanglement in a classical graphical model setting. The text introduces a framework for disentanglement in generative models, connecting it to causal concepts. It presents a theory section informally for high-level understanding, with mathematical details in Appendix A. The framework involves a generative model M mapping a latent space Z to a manifold Y M where data points are embedded. Samples are generated by drawing from a prior latent variable distribution with independent components in Z. The text introduces a framework for disentanglement in generative models, connecting it to causal concepts. It presents a theory section informally for high-level understanding, with mathematical details in Appendix A. The framework involves a generative model M mapping a latent space Z to a manifold Y M where data points are embedded. Samples are generated by drawing from a prior latent variable distribution with independent components in Z. The representation of data points in Y M is called the latent representation. The text introduces a framework for disentanglement in generative models, connecting it to causal concepts. It presents a theory section informally for high-level understanding, with mathematical details in Appendix A. The framework involves a generative model M mapping a latent space Z to a manifold Y M where data points are embedded. Samples are generated by drawing from a prior latent variable distribution with independent components in Z. The representation of data points in Y M is called the latent representation. The generative model is implemented by a non-recurrent neural network, represented by a causal graphical model called Causal Generative Model (CGM). This model involves a succession of operations to compute the mapping g M using endogenous variables in a causal graph. In addition to the latent representation, a collection of endogenous variables represented by nodes in the causal graph is chosen to compute the mapping g M. These endogenous variables can be statistically dependent and influence each other. A paradigmatic choice for these variables is the output activation maps of each channel in one hidden layer of a convolutional neural network. The endogenous variables chosen for the mapping g M can be statistically dependent and influence each other. The internal representation of the network is defined by mild conditions ensuring left-invertibility of the variables. The V k 's are constrained to subsets of smaller dimension than their Euclidean ambient space. The V k 's are constrained to subsets of smaller dimension than their Euclidean ambient space. The CGM framework allows defining counterfactuals in the network following Pearl (2014). Unit level counterfactuals are obtained by replacing assignments of subset variables with a vector of assignments. Counterfactuals in the network following Pearl (2014) involve replacing subset variable assignments with a vector of assignments to induce a transformation of the generative model output. Faithfulness of a counterfactual mapping ensures not all interventions on internal variables will result in a generated output. In the context of generative models, non-faithful counterfactuals can lead to outputs that deviate from the learned data distribution, potentially causing artifactual results or enabling extrapolation to unseen data. The concept of disentangled representation is also relevant in this context. The classical notion of disentangled representation suggests that individual latent variables encode real-world transformations sparsely. This insight has driven supervised approaches to disentangling representations, where relevant transformations are well-identified and manipulated explicitly using appropriate datasets and training procedures. Unsupservised learning approaches to disentanglement need to learn without explicit guidance. Supervised approaches focus on disentangling representations by manipulating transformations explicitly with labeled data, while unsupervised approaches aim to learn transformations from unlabeled data. State-of-the-art methods encode transformations through changes in latent factors, enforcing conditional independence between them. This statistical approach presents challenges due to constraints on the prior distribution of latent variables. The statistical approach to disentangling representations enforces conditional independence between latent factors, but faces challenges due to constraints on the prior distribution of latent variables. This can lead to issues in specifying a disentangled representation, making the problem ill-posed. Finding an appropriate inductive bias to learn a representation that benefits downstream tasks remains an open question. State-of-the-art unsupervised approaches are mainly demonstrated on synthetic datasets, with limited success on complex real-world data like the CelebA dataset. Disentangled generative models struggle to match the visual quality of non-disentangled models on complex real-world datasets. Disentangled generative models struggle to match visual quality on complex real-world datasets like CelebA. A non-statistical definition of disentanglement involves a transformation T acting on data manifold YM, with T corresponding to a transformation of the latent space acting on a single variable z. In disentangled generative models, the transformation T on the data manifold YM should correspond to a transformation of the latent space acting on a single variable z, following the causal principle of independent mechanisms. In disentangled generative models, transformations T1 and T2 are considered disentangled when they modify different components of the latent representation, following the causal principle of independent mechanisms. This notion of extrinsic disentanglement relies on transformations of the latent representation that are exogenous to the CGM. The \"functional\" definition of disentangled transformation is agnostic to subjective choice and statistical independence. In the latent space, it still requires statistical independence between factors. A different representation is needed to uncover related properties that are disentangled according to this definition. Properties encoded by endogenous variables in the graphical model may not be statistically independent. In the context of a graphical model, properties encoded by endogenous variables may not be statistically independent due to common latent causes. The definition of disentanglement is extended to allow transformations of internal variables within the network. In the context of a graphical model, properties encoded by endogenous variables may not be statistically independent due to common latent causes. Disentanglement is defined as allowing transformations of internal variables within the network, where modularity is a structural property of the internal mechanisms. Modularity is a structural property of the internal representation in a graphical model, allowing for arbitrary disentangled transformations. A subset of endogenous variables E is called modular when they do not have common latent ancestors, enabling disentangled transformations within its input domain. A modular subset of endogenous variables E allows for disentangled transformations within its input domain. The proof extends Proposition 1 and can be applied to multiple modules. The framework is based on a functional definition of disentanglement, linking it to an intrinsic property of the trained network. A disentangled representation is defined by partitioning the intermediate representation into modules that factorize V M. The concept of disentangled representation is defined by partitioning the intermediate representation into modules that factorize V M. This partition allows for valid transformations in the data space, requiring an additional partition of latent variables into modules. This approach differs from classical methods by considering the intrinsic property of the trained network. Our framework introduces the concept of grouping neurons into modules to achieve a disentangled representation in artificial and biological systems. This approach goes beyond traditional methods by recognizing that individual neurons may not provide meaningful representations, but rather need to be grouped into modules at a mesoscopic level. In artificial and biological systems, grouping neurons into modules at a mesoscopic level is crucial for achieving a disentangled representation. Once a modular structure is identified in the network, various disentangled transformations become available, with transformations staying within their input domain being good candidates. Counterfactual interventions also play a role in defining relevant transformations. Once a modular structure is found in the network, disentangled transformations are available, with transformations within the input domain being good candidates. Counterfactual interventions implicitly define transformations by assigning a constant value to endogenous variables, aiming for faithful ones by constraining the value. Sampling from the marginal distribution of the variables in E is used to avoid characterizing V E M. The hybridization procedure involves sampling from the marginal distribution of variables in E to generate original examples of output activations in a neural network. The procedure involves taking two independent examples of a latent variable to generate original examples of the output. By memorizing the values of variables, a hybrid example can be created by mixing different features from the generated images. The counterfactual hybridization framework assesses how a module E affects the output of the generator by generating pairs of latent vectors independently and creating hybrid examples by combining features from the generated images. The framework assesses the causal effect of a module E on the generator's output by generating pairs of latent vectors independently and creating hybrid examples. The influence map is estimated as the mean absolute effect, representing unit-level causal effects in the potential outcome framework. The approach estimates the unit-level causal effects of the generator's output by averaging the absolute values over different interventions with varying latent inputs. The resulting influence map is then averaged across color channels to create a grayscale heat-map pixel map. The approach involves estimating unit-level causal effects of the generator's output by averaging absolute values over different interventions with varying latent inputs. The resulting influence map is then averaged across color channels to create a grayscale heat-map pixel map. A challenge is selecting subsets to intervene on, especially with networks containing many units or channels per layer. A fine to coarse approach is used to extract such groups, particularly in convolutional layers. The hybridization approach involves selecting subsets to intervene on, particularly in networks with many units or channels per layer. A fine to coarse approach is used to extract groups, estimating elementary influence maps for each output channel of convolutional layers. These influence maps are then grouped by similarity to define modules at a coarser scale. In the hybridization approach, subsets are selected for intervention in networks with many units or channels per layer. Influence maps are estimated for each output channel of convolutional layers and grouped by similarity to define modules at a coarser scale. This grouping is done in an unsupervised way by clustering channels using their Elementary Influence Maps (EIMs) as feature vectors. To group individual channels into modules dedicated to specific aspects of the output, clustering is performed using Elementary Influence Maps (EIMs) as feature vectors. Influence maps are pre-processed by averaging and thresholding before being fed into a Non-negative Matrix Factorization (NMF) algorithm for analysis. Thresholding the influence maps at the 75th percentile and applying Non-negative Matrix Factorization (NMF) with manually selected rank K results in cluster template patterns and weights for each map. NMF is chosen for its success in isolating patterns. The approach involves using Non-negative Matrix Factorization (NMF) to cluster template patterns and weights for influence maps. NMF is chosen for its ability to isolate meaningful parts of images. The method will be compared to k-means clustering, and a toy generative model will be introduced to further justify the NMF approach. The toy generative model introduced involves a neural network with hidden layers and random choice for model parameters. The toy generative model involves a neural network with hidden layers and random choice for model parameters, including matrices W k and coefficients of H k sampled from arbitrary distributions. The model assumes sets of indices I k over [1, m] with specific conditions, encoding the influence of certain areas in the image. The toy generative model involves a neural network with hidden layers and random choice for model parameters. The specific condition on the I k 's encodes the assumption of areas in the image influenced by one module. Proposition 3 states that the partition of the hidden layer corresponds to a disentangled representation in Model 1. Proposition 3 in Model 1 states that the hidden layer's partition results in a disentangled representation. This allows for the use of NMF on a thresholded version of the influence map matrix to generate a binary matrix summarizing significant influences on each output pixel. The sliding window is applied to ensure similarity between influence maps within the same module. The study focused on the modularity of generative models trained on the CelebFaces Attributes Dataset. A basic architecture, including DCGAN, \u03b2-VAE, and BEGAN, was used. The sliding window was applied to enforce similarity between influence maps within the same module. The study investigated the modularity of generative models trained on the CelebFaces Attributes Dataset using a basic architecture. The results showed that setting the number of clusters to 3 consistently led to highly interpretable cluster templates. The study found that setting the number of clusters to 3 in generative models trained on the CelebFaces Attributes Dataset consistently led to interpretable cluster templates for background, face, and hair. This was confirmed through cluster stability analysis. The cluster stability analysis involved partitioning influence maps into 3 subsets and running clustering twice on two-thirds of the data. The results showed that 3 clusters were a reasonable choice with high consistency (>90%), dropping considerably for 4 clusters. This analysis confirmed the interpretability of cluster templates for background, face, and hair in generative models trained on the CelebFaces Attributes Dataset. The clustering analysis showed that 3 clusters were a suitable choice with high consistency (>90%), dropping significantly for 4 clusters. NMF-based clustering outperformed k-means, and the cosine similarity between matching cluster templates was .9 on average. The clustering analysis revealed that 3 clusters were optimal with a high consistency (>90%) compared to 4 clusters. NMF-based clustering outperformed k-means, with an average cosine similarity of .9 between matching cluster templates. The influence maps showed that some maps spread over different image locations, and applying the hybridization procedure to the resulting 3 modules led to a replacement of features associated with the intervened module. The hybridization procedure applied to the 3 modules obtained by clustering replaces features associated with the intervened module while maintaining the overall image structure. For example, facial features from one sample are inserted into another image while preserving certain characteristics. The \u03b2-VAE, designed for extrinsic disentanglement, may not be optimal compared to other approaches. The Original 2 samples' facial features are inserted into the Original 1 image while preserving the hair. Further research is needed to explore better extrinsic disentanglement for intrinsic disentanglement in models like GAN-like architectures. After exploring extrinsic disentanglement in GAN-like architectures, it was found that our approach can be applied to models not optimized for disentanglement. This was demonstrated in experiments with basic models and a pretrained Boundary Equilibrium GAN. Our approach was applied to models not optimized for disentanglement, including a pretrained Boundary Equilibrium GAN (BEGAN) known for high-quality face images. The simple generator architecture of BEGAN allowed us to test our hypothesis with minimal modifications. The resolution of the generated images with the simple generator architecture of BEGAN allowed for testing hypotheses with minimal modifications. Interventions on channels from the same cluster in two successive layers were needed to obtain counterfactuals with noticeable effects. Selective transfer of features from Original 2 to Original 1 was observed by intervening on layers 5 and 6. Only one module associated with hair and background features was observed due to the model being trained on tightly cropped face images. Selective transfer of features from Original 2 to Original 1 is evident on layers 5 and 6, with one module dedicated to hair transfer. The remaining modules encode different aspects of face features, such as eye contour, mouth, nose, eyelids, and face shape. The quality of counterfactual images compared to the originals was evaluated using Frechet Inception Distance (FID). The study evaluated the quality of counterfactual images compared to the originals using Frechet Inception Distance (FID). The hybridization procedure minimally affected image quality, and the approach was tested on high-resolution generative models like BigGAN-deep. To generate high-resolution images with a variety of objects, the study utilized the BigGAN-deep architecture pretrained on the ImageNet dataset. The architecture consists of 12 Gblocks with convolutional layers and skip connections, allowing for the generation of hybrids by mixing features of different classes. The approach was tested on complex image datasets and showed minimal impact on image quality. The study utilized the BigGAN-deep architecture pretrained on the ImageNet dataset to generate high-resolution images with a variety of objects. The architecture consists of 12 Gblocks with convolutional layers and skip connections, allowing for the generation of hybrids by mixing features of different classes. Intervening on two successive layers within a Gblock was found to be more effective in generating counterfactuals. Examples in Fig. 4 demonstrate the ability to generate high-quality counterfactuals with modified backgrounds while keeping a similar object in the foreground. Even in challenging situations with objects of different nature, meaningful combinations of original samples are still generated. The study used the BigGAN-deep architecture to generate high-quality counterfactual images with modified backgrounds while maintaining a similar object in the foreground. Examples included a teddy bear in a tree and a \"teddy-koala\" hybrid. The generated images were used to test and enhance the robustness of classifiers to contextual changes. The study used BigGAN-deep to generate counterfactual images with modified backgrounds while maintaining the object in the foreground. Several pretrained classifiers were compared to recognize the original classes, showing higher recognition rates closer to the output layers. The study compared pretrained classifiers' recognition rates on counterfactual images generated by BigGAN-deep. Higher recognition rates were observed closer to the output layers, with Inception resnet performing better at intermediate blocks 5-6. Different classifiers rely on different aspects of image content for decision-making. The study introduced a mathematical definition of disentanglement and used it to characterize the representation encoded by different groups of channels in deep generative architectures. Evidence for interpretable modules of internal variables was found in four generative models trained on real-world datasets. This framework enhances understanding of complex generative architectures and their applications. Our framework reveals interpretable modules of internal variables in various generative models trained on complex datasets, aiding in understanding deep generative architectures and applications like style transfer and object recognition system robustness assessment. This research direction aims to optimize costly and energy-consuming training procedures for deep neural networks. This research direction aims to enhance the interpretability of deep neural networks and enable them to be used for tasks they were not originally trained for, contributing to more sustainable research in Artificial Intelligence. Trained generator architectures can be manipulated independently, offering a perspective on how sustainable AI research could be fostered in the future. Trained generator architectures can be manipulated independently, fostering sustainable AI research in the future through structural causal models (SCMs) using structural equations (SEs) to represent mathematical models. Structural equations (SEs) represent a value to variable Y, computed from other variables Xk and exogenous influences. SEs remain valid even if variables change due to interventions, and can model operations in computational graphs of neural networks. SCMs consist of interdependent modules with directed acyclic dependencies. The Causal Generative Model (CGM) captures computational relations between input latent variables, generator output, and endogenous variables in a directed acyclic graph. The Causal Generative Model (CGM) consists of input latent variables, generator output, and endogenous variables in a directed acyclic graph, with the generator's output decomposed into two steps. The CGM comprises a graph and a set of structural equations, exemplified in Fig. 2b. The CGM M = G(Z, S, G) includes a directed acyclic graph and a set of structural equations. It aligns with Pearl's definition of a deterministic structural causal model, with specificities reflecting practical model structures. Variable assignments may or may not involve latent/exogenous variables in their right-hand side. The CGM includes a directed acyclic graph and structural equations. Variable assignments may involve latent/exogenous variables. This allows modeling feed-forward networks with deterministic operations. The output Y is assigned once z is chosen. The computational graph of generative networks assigns variables based on chosen inputs, ensuring unambiguous outputs. Internal variables and outputs exist on lower-dimensional manifolds within the latent space. Internal variables and outputs in generative networks exist on manifolds of smaller dimension within the latent space Z. These manifolds are defined by operations of the graph, such as latent and endogenous mappings. The variables are constrained to subsets of their euclidean ambient space. The latent and endogenous mappings in generative networks are constrained to subsets of their euclidean ambient space. A CGM satisfying these assumptions is called an embedded CGM. The example in Fig. 2b contains exactly two layers. The mappings g M and g M are well defined and surjective due to appropriate choices for domains and codomains. The image sets in generative networks are constrained by the parameters of the model, making them difficult to characterize. For instance, the image set Y M should ideally match the data distribution's support. Learning the generator parameters aims to align Y M with the data distribution. The image set Y M of a trained model should approximate the support of the data distribution. Learning generator parameters to match Y M with the target data distribution is a key goal for generative models. Topology-preserving transformations are used on Y M, with embeddings as the basic structure. Topology-preserving transformations using embeddings are essential for generative models to match the image set Y M with the target data distribution. Injectivity of g M is a key requirement for embedded CGMs, where the compactness of CGM M implies injectivity of g M. Generative models with uniformly distributed latent variables can satisfy these conditions. Generative models with uniformly distributed latent variables can be embedded if the compactness of the latent space is ensured, leading to injectivity. This allows for matching the image set with the target data distribution. The compactness of the latent space ensures injectivity, allowing generative models with uniformly distributed latent variables to be embedded. This results in an embedded CGM that approximates the original one for most samples, defining counterfactuals in the network following Pearl (2014). Counterfactuals induce a transformation of the output of the generative model, relating to a form of disentanglement allowing transformations of internal variables in the network. The continuous map in our approach relates counterfactuals to intrinsic disentanglement, allowing transformations of internal variables in the network. The concept of intrinsic disentanglement in generative models involves a transformation of endogenous variables that only affects specific indexed variables, leading to a causal interpretation of the model's structure. This is illustrated in Fig. 2d, where a split node indicates the computation of V2 before applying a transformation T2. Intrinsic disentanglement in generative models involves applying transformation T2 to the outcome, expressing robustness to perturbations of subsystems. Counterfactuals are examples of perturbations that can be disentangled based on faithfulness. Armstrong (2013) shows that a continuous and injective gM is an embedding when Z is compact and the codomain of gM is Hausdorff. Additionally, gM injective implies injectivity on their domains VM, which are compact as they are the image of a compact Z by a continuous mapping. The equivalence between faithful and disentangled transformations is proven in the context of intrinsic disentanglement in generative models. If a transformation is disentangled, it is an endomorphism of Y M, making the counterfactual mapping faithful. Conversely, assuming a faithful Y E h, the mapping from V to the output shows that T is disentangled with respect to E in M. The absence of a common latent ancestor between E and E ensures that values in both subsets are unambiguously assigned by non-overlapping subsets of latent variables, A and B respectively. This guarantees that T is an endomorphism of V M for any choice of endomorphism T E. The image set of the layer fully covers the Cartesian product of subsets of latent variables A and B, ensuring T is an endomorphism. The i.i.d. assumption for Z components leads to modular subsets of endogenous variables, creating a disentangled representation in the hidden layer. The subsets of endogenous variables associated with each V k are modular, creating a disentangled representation in the hidden layer. The choice of increasing dimensions and i.i.d. sampling ensure an injective mapping, following embedded CGM assumptions. Counterfactual hybridization of V k components results in an influence map covering I k. Conditions on I k and thresholding guarantee a rank K binary factorization of matrix B. The \u03b2-VAE architecture, similar to DCGAN, guarantees a rank K binary factorization of matrix B with unique factorization. Hyperparameters for both structures are specified in Table 1. The method proposed in Berthelot et al. (2017) was used for the CelebA dataset. The \u03b2-VAE architecture, similar to DCGAN, utilizes a model with three blocks of convolutional layers and skip connections for image sharpness. The pretrained model is obtained from Tensorflow-hub. The architecture of the pretrained model used in this study is BigGan-deep, which consists of ResBlocks with BatchNorm-ReLU-Conv Layers and skip connections for image sharpness. The model was not retrained and was obtained from Tensorflow-hub. The architecture of the model used is BigGan-deep with ResBlocks containing BatchNorm-ReLU-Conv Layers and skip connections. Skip connections bring fresh signal from the input to every ResBlock. The model was not retrained and was obtained from Tensorflow-hub. The notion of influence maps generated by a VAE on the CelebA dataset is discussed, along with FID analysis of BEGAN hybrids. Influence maps generated by a VAE on the CelebA dataset show stronger perturbation influence on lighter pixels. FID analysis of BEGAN hybrids reveals small distances between hybrids and generated data, indicating closeness in distribution. Hybrids show a small distance to the generated data, suggesting visually plausible images. Entropy values indicate that poorer quality modules lead to higher entropy in Gblock 6. The study used probabilistic output to rank classes in hybrids, showing that poorer quality modules lead to higher entropy in Gblock 6. Results suggest that object texture is crucial for the classifier's decision, especially in hybrids from Gblock 4. The study found that object texture is crucial for the classifier's decision, especially in hybrids generated from Gblock 4. The entropy is computed using the probabilistic output for the top ranking classes across all hybrids, normalized to provide a total probability of 1. The entropy is computed for interventions on a module, showing large entropy for the first module of Gblock 5. Table 3 displays the classification outcome of discriminative models for koala+teddy hybrids, investigating the robustness of classifiers. Table 3 shows the classification outcome of discriminative models for koala+teddy hybrids, aiming to assess classifier robustness. The resultant hybrids resemble a teddy bear in a koala context, highlighting the importance of object sensitivity over contextual information. Nasnet large is identified as more robust to changes. Nasnet large is more robust to changes in context compared to other classifiers when classifying koala+teddy hybrids."
}