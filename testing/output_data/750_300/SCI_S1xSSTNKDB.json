{
    "title": "S1xSSTNKDB",
    "content": "Existing public face image datasets are biased towards Caucasian faces, leading to inconsistent classification accuracy for non-White race groups. To address this bias, a new dataset with 108,501 images balanced on race was created, including White, Black, Indian, East Asian, Southeast Asian, Middle Eastern, and Latino groups. Images were sourced from the YFCC-100M Flickr dataset and labeled with race, gender, and age information. Evaluation was done on various face attribute datasets. A new dataset with 108,501 images balanced on race was created, including 7 race groups. Images were sourced from the YFCC-100M Flickr dataset and labeled with race, gender, and age information. The model trained from this dataset showed higher accuracy on novel datasets and consistent accuracy across race and gender groups. Commercial computer vision APIs were also compared for accuracy across gender, race, and age groups. Several large-scale face image datasets have been proposed, fostering research in automated face detection, alignment, and recognition. Commercial computer vision APIs were compared for accuracy across gender, race, and age groups. Several large-scale face image datasets have been proposed, fostering research in automated face detection, alignment, recognition, generation, modification, and attribute classification. These systems have been successfully applied in various fields such as security, medicine, education, and social sciences. Despite the abundance of data, existing public face datasets are limited. Existing public face datasets are biased towards Caucasian faces, with other races like Latino being underrepresented. This bias can lead to models not being applicable to all subpopulations without calibration. Face databases are biased towards lighter skin faces, such as White, compared to darker faces, like Black. This bias can lead to models that are not applicable to all subpopulations without calibration, raising ethical concerns about fairness in automated systems. Commercial computer vision systems have been criticized for their asymmetric accuracy across different demographics. Various commercial computer vision systems have faced criticism for their asymmetric accuracy across sub-demographics, with studies showing better performance on male and light faces. Biases in training data can lead to these disparities, with image datasets prone to unwanted biases due to biased selection and capture methods. To address biases in training data leading to disparities in computer vision systems' accuracy, a novel face dataset with balanced race composition is proposed. The dataset contains 108,501 facial images primarily from the YFCC-100M Flickr dataset and aims to mitigate race bias in existing datasets. The dataset proposed contains 108,501 facial images from various sources, emphasizing balanced race composition with 7 defined race groups. The study shows that existing face attribute datasets and models struggle to generalize well to unseen data with more nonWhite faces. Our dataset is well-balanced on 7 race groups and shows that existing face attribute datasets struggle with nonWhite faces. It outperforms on novel data across racial groups and is the first to include Latino and Middle Eastern faces. Our dataset is the first large-scale face attribute dataset to include Latino and Middle Eastern faces, differentiating East Asian and Southeast Asian. This inclusion significantly expands the applicability of computer vision methods to various fields, allowing researchers to analyze different demographics using image data. The goal of face attribute recognition is to classify human attributes such as gender, race, age, emotions, expressions, and other facial traits from facial appearance. Face attribute recognition aims to classify human attributes like gender, race, age, emotions, and expressions from facial appearance, expanding the applicability of computer vision methods. Existing datasets are predominantly White, but efforts are being made to include Latino and Middle Eastern faces, as well as differentiate between East Asian and Southeast Asian demographics. Face attribute recognition is crucial for various computer vision tasks like face verification and person re-identification. It is essential for these systems to perform equally well across different gender and race groups to maintain trust in the machine learning and computer vision research community. Notable incidents of racial bias, such as Google Photos recognizing African Americans, highlight the importance of addressing these issues. Not addressing racial bias in face attribute recognition can harm the reputations of service providers and public trust in the machine learning and computer vision research community. Incidents like Google Photos mistaking African American faces as Gorilla and Nikon's cameras asking Asian users if they blinked have led to termination of services or features. Commercial providers have stopped offering race classifiers as a result. Face attribute recognition and race classifiers have faced scrutiny due to potential biases and negative impacts on users. Commercial providers have ceased offering race classifiers, and social scientists are using image analysis tools to infer demographic attributes and behaviors. Social scientists are using images of people to infer demographic attributes and analyze behaviors, with a focus on algorithmic fairness and avoiding biases in datasets and models. Unfair classification can lead to significant errors in analyzing specific sub-populations, with potential policy implications. In the context of algorithmic fairness and dataset biases, researchers focus on the cost of unfair classification, particularly in analyzing specific sub-populations with potential policy implications. Fairness in AI and machine learning involves ensuring balanced accuracy independent of race and gender. In this paper, the focus is on balanced accuracy in attribute classification, ensuring fairness regardless of race and gender. Research in algorithmic fairness involves auditing bias in datasets, improving datasets, and designing better algorithms. The paper focuses on balanced accuracy in attribute classification, ensuring fairness regardless of race and gender. It falls into the categories of improving datasets and designing better algorithms for gender classification from facial images. Buolamwini & Gebru (2018) highlighted biases in commercial gender classification systems, especially towards dark-skinned females, possibly due to skewed datasets. The paper aims to address biases in gender classification systems from facial images, particularly towards dark-skinned females. Biased datasets and associations between scene and race contribute to inaccurate results. The paper proposes collecting more diverse face images from non-White races to mitigate existing limitations and biases in databases. The paper aims to collect more diverse face images from non-White race groups to mitigate biases in existing databases. This improves generalization performance to novel image datasets not dominated by the White race, including Southeast Asian and Middle Eastern races. The dataset includes Southeast Asian and Middle Eastern races, aiming to mitigate biases in existing databases dominated by the White race. Race taxonomy defines 7 groups: White, Black, Indian, East Asian, Southeast Asian, Middle Eastern, and Latino, based on physical traits and cultural similarities. In the dataset, race is categorized into 7 groups: White, Black, Indian, East Asian, Southeast Asian, Middle Eastern, and Latino. Race is based on physical traits, while ethnicity is based on cultural similarities. Latino is considered a race, not just an ethnicity, and subgroups like Middle Eastern and East Asian are further divided. In the dataset, race is categorized into 7 groups: White, Black, Indian, East Asian, Southeast Asian, Middle Eastern, and Latino. Latino is considered a race, judged by facial appearance. Subgroups like Middle Eastern, East Asian, and Southeast Asian are distinct. Few examples for Hawaiian, Pacific Islanders, and Native Americans were discarded. Experiments were based on 7 race classification, with a focus on measuring dataset bias based on skin color or race. The experiments in this paper focused on 7 race classification, with a debate on measuring dataset bias based on skin color or race. Skin color is used as a proxy for racial or ethnic grouping in some studies, but it has limitations due to lighting conditions affecting skin color accuracy. The Pilot Parliaments Benchmark dataset used profile photographs of government officials in controlled lighting conditions, making it non-in-the-wild. Skin color variations are significant within groups, and race is multidimensional while skin color is one-dimensional. Skin color alone cannot differentiate between many race groups. The skin color, measured by Individual Typology Angle (ITA), does not provide enough information to differentiate between race groups like East Asian and White. Therefore, race is explicitly used and annotated by human annotators. In addition to race categorization, skin color is also considered using ITA measurements. Face datasets often come from public figures like politicians or celebrities. Many face datasets are sourced from public figures like politicians or celebrities, leading to potential biases in age and attractiveness. Images are often taken by professional photographers in limited situations, resulting in a quality bias. Some datasets are collected through web searches using specific keywords. The curr_chunk discusses the bias in datasets where images of public figures like politicians and actors are often used, leading to age and attractiveness biases. It mentions the use of specific keywords in web searches that may prioritize stereotypical faces or celebrities. The goal is to minimize selection bias and maximize diversity in the dataset by starting with a large public image dataset and detecting faces without preselection. The dataset aims to minimize selection bias by starting with a large public image dataset and detecting faces without preselection. It is smaller but more balanced on race, with 7,125 faces incrementally collected from the Yahoo YFCC100M dataset. The dataset was incrementally increased by detecting and annotating 7,125 faces randomly sampled from the YFCC100M dataset. Demographic compositions of each country were estimated to adjust the number of images, ensuring a balanced representation of races. The U.S. and European countries were excluded in later stages to prevent dominance by White faces. The minimum face size detected was set at 50 by 50 pixels. The dataset was expanded by detecting and annotating 7,125 faces from the YFCC100M dataset. To avoid dominance by White faces, U.S. and European countries were excluded later on. Faces with a minimum size of 50 by 50 pixels were used, allowing for recognizable attributes and robust classifiers. Images with \"Attribution\" and \"Share Alike\" licenses were utilized, and race, gender, and age group were annotated using Amazon Mechanical Turk. We utilized images with \"Attribution\" and \"Share Alike\" licenses for annotation of race, gender, and age group using Amazon Mechanical Turk. Annotations were refined by training a model from initial ground truth annotations and applying it back to the dataset. After refining annotations using Amazon Mechanical Turk, a model was trained from initial ground truth annotations and applied back to the dataset. Annotations were manually re-verified for images with differing model predictions. The race composition of each dataset was measured, with race labels annotated for 3,000 random samples from datasets without race annotations. Most face attribute datasets show bias towards the White race, while gender balance ranges from 40%-60% male ratio. Model performance was compared using ResNet-34 architecture trained on each dataset with ADAM optimization. Face detection was done on images. For model comparison, ResNet-34 architecture was used to train on different datasets with ADAM optimization. Face detection was performed using dlib's CNN-based face detector. The dataset was compared with UTKFace, LFWA+, and CelebA datasets. UTKFace and LFWA+ have race annotations for comparison, while CelebA was used only for gender analysis. The dataset was compared with UTKFace, LFWA+, and CelebA datasets. UTKFace and LFWA+ have race annotations for comparison, while CelebA was used only for gender analysis. CelebA does not provide race annotations, so it was only used for gender classification. FairFace defines 7 race categories but only 4 races (White, Black, Asian, and Indian) were used for comparison. FairFace dataset includes 7 race categories but only 4 races (White, Black, Asian, and Indian) were used for comparison with UTKFace. Models trained on these datasets were tested for race, gender, and age classification across subpopulations. FairFace is the only dataset with 7 races, and fine racial groups were merged for compatibility with other datasets. CelebA, without race annotations, was included for gender analysis. The classification results for race, gender, and age on various datasets were shown. The model performed best on the LFWA+ dataset due to its diversity and generalizability. Three novel datasets were used to test the models' generalization performance. The model's generalization performance was tested on three novel datasets collected from different sources than the training data from Flickr. The test datasets contain people from diverse locations such as France, Iraq, Philippines, and Venezuela, identified through geo-tagged Tweets. The study collected data from diverse locations including France, Iraq, Philippines, and Venezuela. Geo-tagged Tweets and media photographs were used to sample faces for analysis. Media photographs were sourced from professional media outlets through tweet IDs. The dataset includes tweet IDs from 4,000 known media accounts, sampled 8,000 faces, and used a public image dataset from a protest activity study. The authors collected a public image dataset from a protest activity study, sampling 8,000 faces with diverse race and gender groups. Faces were annotated for gender, race, and age. Gender classification accuracy was measured on external validation datasets, and different models' classification accuracy was shown in Table 7. The dataset is larger than LFWA+ and UTKFace datasets. The FairFace model outperforms other models in gender, race, and age classification accuracy on novel datasets, even with smaller training sets of 9k and 18k images. The FairFace model surpasses other models in gender, race, and age classification accuracy on new datasets, even with smaller training sets. The dataset size is not the sole factor for the improved performance, as shown by the balanced accuracy and consistency of results across different race groups. Our model demonstrates improved accuracy and consistency in race, gender, and age classification compared to other datasets. The model's performance is measured by standard deviations of classification accuracy across different race groups. Fair classification can be assessed through conditional use accuracy equality or equalized odds. The FairFace model achieves the lowest maximum accuracy disparity in gender classification across different demographic groups. The LFWA+ model shows a strong bias towards the male category, while the CelebA model tends to exhibit a bias towards the female category. The FairFace model achieves less than 1% accuracy discrepancy between male \u2194 female and White \u2194 non-White for gender classification, while other models show a strong bias towards the male class, performing more inaccurately on the female and non-White groups. The gender performance gap was the biggest in LFWA+ at 32%. The study found that the FairFace model had less than 1% accuracy difference between male and female, as well as White and non-White for gender classification. Other models showed a strong bias towards males, performing less accurately on females and non-White groups. The gender performance gap was largest in the LFWA+ dataset at 32%. The results suggest that unbalanced representation in training data may be the cause of asymmetric gender biases in computer vision services. Further investigation into dataset characteristics revealed data diversity issues. The study found that the FairFace model had balanced accuracy for gender classification across different races. Data diversity was investigated by visualizing faces in 2D space, showing a spread of faces from various datasets dominated by White faces. The FairFace dataset contains diverse faces spread out in space, with race groups loosely separated. It includes non-typical examples due to biased training data. Other datasets like LFWA+ and UTKFace focus on clusters of images. The study also examined the diversity of faces in these datasets. The UTKFace dataset, developed for face recognition, contains multiple images of the same individuals in local clusters. To measure diversity, pairwise distances between faces were examined, showing that UTKFace had tightly clustered faces, while LFWA+ had diverse faces spread far apart. The CDF functions for 3 datasets show that UTKFace has tightly clustered faces, LFWA+ has diverse faces spread far apart. The diversity in LFWA+ is attributed to the face embedding trained on a white-oriented dataset, not actual diversity in appearance. The FairFace dataset was used to test gender classification APIs, including Microsoft Face API, Amazon Rekognition, IBM Watson Visual Recognition, and Face++. Compared to previous work with politicians' faces, this dataset is more diverse in terms of race, age, expressions, and head features. In 2019, the FairFace dataset was used to test gender classification APIs from Microsoft, Amazon, IBM, and Face++. The dataset is diverse in race, age, expressions, and head features, serving as a better benchmark for bias measurement. 7,476 random samples were used, excluding children under 20 due to ambiguity in gender determination. The experiments were conducted in August 2019. The experiments conducted in August 2019 used a diverse dataset for testing gender classification APIs from Microsoft, Amazon, IBM, and Face++. Children under 20 were excluded due to gender ambiguity. The dataset included an equal number of faces from each race, gender, and age group. Table 6 displays gender classification accuracies of tested APIs (IBM, FairFace, Microsoft, Face++). These APIs detect and classify gender from input images, with not all faces being detected by all APIs except Amazon. Table 6 presents gender classification accuracies of tested APIs for detecting and classifying gender from input images. Not all faces were detected by the APIs except for Amazon Rekognition. The detection rate is reported in Table 8. Two sets of accuracies are reported: one treating mis-detections as mis-classifications and the other excluding mis-detections. A model trained with their dataset is included for comparison. Classification accuracy based on skin color is also shown in Figure 6 following prior work. The gender classifiers tested favor the male category, with dark-skinned females showing higher error rates. Some APIs classified Indians more accurately than Whites, suggesting skin color influences classification accuracy. Dark-skinned females tend to have higher classification error rates, but there are exceptions. Skin color alone is not enough to study model bias. Face detection can introduce gender bias, with Microsoft's model failing to detect many male faces. This paper suggests a balanced face image dataset. Microsoft's model exhibited gender bias by failing to detect many male faces, contrasting with previous studies. A novel face image dataset balanced on race, gender, and age was proposed, showing improved classification performance for gender, race, and age on diverse image datasets from various sources. The model trained on this dataset achieved balanced accuracy across race, unlike other datasets. A novel image dataset collected from diverse sources, including Twitter and online newspapers, focuses on race balance. The dataset, derived from Yahoo YFCC100m, allows for academic and commercial usage. It aims to address algorithmic fairness in AI systems by training models with balanced accuracy across different race groups. The dataset allows for academic and commercial usage, focusing on algorithmic fairness in AI systems. It aims to mitigate race and gender bias in computer vision systems. The novel dataset proposed in this paper aims to mitigate race and gender bias in computer vision systems, improving model accuracy and transparency for societal acceptance. The dataset includes measurements of skin color distribution among different races."
}