{
    "title": "BygfghAcYX",
    "content": "In this work, a novel complexity measure based on unit-wise capacities is proposed for two layer ReLU networks, offering a tighter generalization bound. This capacity bound correlates with test error behavior as network sizes increase, potentially explaining the improved generalization with over-parametrization. Additionally, a matching lower bound for Rademacher complexity is presented, showing improvement over existing measures. The study introduces a new complexity measure for two-layer ReLU networks, which is closely related to test error behavior as network sizes increase. It also presents a lower bound for Rademacher complexity, showing improvement over previous capacity lower bounds for neural networks. Deep neural networks have been successful in various tasks, with over-parametrization leading to better generalization. The study discusses the over-parametrization of deep neural networks, which has led to better generalization. It mentions the arms race of training larger networks for improved test performance and the ability of over-parametrized networks to fit random labels. The traditional wisdom of avoiding overfitting by limiting model capacity is challenged. Increasing the capacity of neural network models has traditionally been thought to lead to overfitting, but recent studies show that larger models can actually improve generalization error, even without explicit regularization techniques. This challenges the idea of limiting model size to prevent overfitting. Recent studies have shown that increasing the capacity of neural network models can improve generalization error without explicit regularization techniques. This challenges the traditional belief that larger models lead to overfitting. Recent studies have shown that increasing neural network capacity can improve generalization error without explicit regularization techniques. Complexity measures based on total parameters do not capture this behavior, leading to the exploration of norm, margin, and sharpness-based measures to explain generalization behavior. Existing works have proposed norm, margin, and sharpness-based measures to explain the generalization behavior of neural networks as they increase in size. Even when a network is large enough to fit the training data perfectly, the test error continues to decrease for larger networks. This phenomenon is observed in various network architectures, indicating that increasing network capacity can improve generalization error without explicit regularization techniques. The test error decreases for larger networks, similar to observations in ResNet18 architecture. Unit capacity and unit impact are key factors in the network's output, with both shrinking faster than 1/ \u221a h where h is the number of hidden units. Experiments settings can be found in Supplementary Section A. (Theorem 1). Empirical observations show that average unit capacity and impact decrease faster than 1/ \u221a h with the number of hidden units. Bartlett et al. (2017) presented a generalization bound based on spectral and 1,2 norm of network layers. However, Neyshabur et al. (2017) and FIG6 demonstrate that these measures do not explain why over-parametrization helps, and actually increase with network size. Dziugaite & Roy (2017) evaluated a generalization bound based on PAC-Bayes, showing numerical results. The complexity measures fail to explain why over-parametrization helps and increase with network size. Existing measures depend on the number of hidden units, even for two layer networks. To study this phenomenon, simplifying the architecture is necessary. In order to study the phenomenon of over-parametrization in neural networks, simplifying the architecture is necessary. The authors chose two layer ReLU networks to analyze this, as it exhibits similar behavior to more complex architectures. They prove a tighter generalization bound for two layer ReLU networks in this paper. The authors prove a tighter generalization bound for two layer ReLU networks, showing that complexity at a unit level decreases as network size increases. The generalization bound for two layer ReLU networks shows that unit level measures decrease as network size increases, with the closeness of learned weights to initialization in the over-parametrized setting. In the over-parametrized setting, the closeness of learned weights to initialization can be understood by considering the limiting case as the number of hidden units goes to infinity. Training just the top layer of the network in this extreme setting results in minimizing the training error, as the randomly initialized hidden layer has all possible features. This suggests that as networks are over-parametrized, the optimization problem involves selecting the right features to minimize the training loss. In the over-parametrized setting, the optimization problem involves selecting the right features to minimize the training loss. Dziugaite & Roy (2017) and Nagarajan & Kolter (2017) have empirically observed the significance of initialization in this process. In the over-parametrized setting, the optimization problem involves selecting the right features to minimize the training loss. Nagarajan & Kolter (2017) emphasize the importance of initialization, showing that the Euclidean distance to the initialization is smaller than the Frobenius norm of the parameters. Liang et al. (2017) propose a Fisher-Rao metric based complexity measure for larger networks but only prove the capacity bound for linear networks. In this paper, the authors empirically investigate the role of over-parametrization in generalization of neural networks on different datasets. They show that existing complexity measures increase with the number of hidden units but do not explain generalization behavior with over-parametrization. The authors also prove tighter generalization bounds for two layer ReLU networks, improving upon previous results. The authors propose a new complexity measure for neural networks that decreases with the number of hidden units, potentially explaining the impact of over-parametrization on generalization. They also provide a lower bound for the Rademacher complexity of two layer ReLU networks with a scalar output, significantly improving upon previous results. The authors introduce a new complexity measure for neural networks that decreases with the number of hidden units, shedding light on the effect of over-parametrization on generalization. They also present a lower bound for the Rademacher complexity of two layer ReLU networks with a scalar output, surpassing previous bounds. ReLU networks have input dimension d, output dimension c, and h hidden units. The margin operator \u00b5 is defined for c-class classification tasks, selecting the label with the highest output score as the prediction. The margin operator \u00b5 is defined for c-class classification tasks, selecting the label with the highest output score as the prediction. The ramp loss is defined for any distribution D and margin \u03b3 > 0, with the expected margin loss bounded between 0 and 1. The loss function L \u03b3 (.) is bounded between 0 and 1, with L \u03b3 (f ) representing the empirical estimate of the expected margin loss. Setting \u03b3 = 0 gives the classification loss, denoted by L 0 (f ) and L 0 (f ). The generalization bound for any function f \u2208 F with probability 1 \u2212 \u03b4 over the training set size m is given by a specific formula involving the Rademacher complexity of a class of functions. The Rademacher complexity is a capacity measure that captures the ability of functions in a function class to fit random labels. It increases with the complexity of the class. By bounding the Rademacher complexity of neural networks, we can obtain a bound on the generalization error. The choice of the function class considered is crucial in determining the Rademacher complexity. The Rademacher complexity of neural networks is bounded to estimate generalization error. Choosing the right function class is essential to capture trained networks accurately. Larger function classes may result in weaker capacity bounds. The Rademacher complexity of neural networks is bounded to estimate generalization error. Choosing the right function class is essential to capture trained networks accurately. Larger function classes may result in weaker capacity bounds. In experiments on the CIFAR-10 dataset, the behavior of different measures of network layers with increasing hidden units is investigated. The spectral and Frobenius norms of the learned layer initially decrease but eventually increase with h, with the Frobenius norm increasing at a faster rate. The spectral and Frobenius norms of the learned layer decrease initially but eventually increase with h, with the Frobenius norm increasing at a faster rate. The distance Frobenius norm w.r.t. initialization decreases, suggesting that the increase in the Frobenius norm of weights in larger networks is due to the increase in random initialization. The distance to initialization per unit also decreases. In the last two panels of FIG1, the distance to initialization per unit decreases with increasing h. The distribution of angles between learned weights and initial weights shifts from almost orthogonal in small networks to almost aligned in large networks. This per unit distance to initialization, referred to as unit capacity, is a key quantity in capacity bounds. Unit capacity is defined as \u03b2 i = u i \u2212 u 0 i 2 for hidden unit i. In the second layer of trained networks, the Frobenius norm and distance to initialization decrease with increasing size, suggesting a limited role of initialization for this layer. In the second layer, the Frobenius norm and distance to initialization decrease with increasing size, suggesting a limited role of initialization. The norm of outgoing weights from a hidden unit decreases faster than 1/ \u221a h as the size grows, impacting the final decision. This unit impact measure is important for the remainder of the paper. In two-layer neural networks, the impact of each classifier on the final decision decreases faster than 1/ \u221a h. The unit impact, defined as \u03b1 i = v i 2, plays a crucial role in the hypothesis class of neural networks. The set of parameters W is considered for representing neural networks. The hypothesis class of neural networks is based on the capacity and impact of hidden units. Empirical observations show bounded unit capacity and impact, leading to a study of generalization behavior. A generalization bound for two-layer ReLU networks is proven in this section. The generalization behavior of the function class of neural networks is studied to understand their properties. A generalization bound for two-layer ReLU networks is proven by bounding the Rademacher complexity of the class based on unit capacity and impact. The bound is given by Theorem 1 for a training set S and \u03b3 > 0. The Rademacher complexity of the composition of loss function \u03b3 over the class F W is bounded by Theorem 1 for a training set S and \u03b3 > 0. The proof involves decomposing the complexity of the network into complexity of the hidden units, which differs from previous works that decompose the complexity to that of layers. The complexity of neural networks is typically decomposed into layers, using the Lipschitz property to bound generalization error. However, this approach overlooks the linear structure of individual layers. By decomposing complexity across hidden units, a tighter bound on Rademacher complexity for two-layer neural networks is achieved. The generalization bound in Theorem 1 applies to any function in a specific class defined by fixed \u03b1 and \u03b2 values. To obtain a generalization bound applicable to all networks, a union bound over possible \u03b1 and \u03b2 values is necessary. The generalization bound in Theorem 1 applies to functions in a specific class defined by fixed \u03b1 and \u03b2 values. To extend this bound to all networks, a union bound over possible \u03b1 and \u03b2 values is required. Theorem 2 provides a generalization bound for two-layer ReLU networks, improving existing bounds and decreasing with increasing network width. The generalization error for networks learned in practice is bounded by a new bound that improves existing ones. The Rademacher complexity has a matching lower bound, showing tightness. The additive factor in the bound is small in relevant regimes, resulting in a decrease in capacity. The generalization error for networks learned in practice is bounded by a new bound that improves existing ones, showing tightness. The additive factor in the bound is small in relevant regimes, resulting in a decrease in capacity. The extension of the generalization bound to p norms in Appendix Section B presents a finer tradeoff between terms. Comparing with Golowich et al., 2018, the first term in their bound behaves similarly to the first term in the presented bound. The key complexity term in the bound presented is U - U0F VF, while in Golowich et al., 2018, it is U - U01,2 V2. The difference between V2 and VF is minimal, but U - U01,2 can be significant when hidden units have similar capacity. Their bound increases with h mainly due to the term U - U01,2. The bound presented in the previous paragraph focuses on the term U - U0F VF, while in Golowich et al., 2018, it is U - U01,2 V2. The difference between V2 and VF is minimal, but U - U01,2 can be significant when hidden units have similar capacity. Experimental comparison shows that networks with sizes well beyond 128 can still improve generalization, even though a network of size 128 is enough to achieve zero training error on CIFAR-10 and SVHN datasets. In experiments, networks larger than size 128 improve generalization even without regularization. Unit capacity and impact decrease with increasing network size. Larger networks require fewer epochs to reach 0.01 cross-entropy loss. The effective capacity of function class decreases with increasing network size, as shown in FIG0 and FIG4. The number of epochs needed to reach 0.01 cross-entropy loss decreases for larger networks. Generalization bounds scale as C/m, where C is the effective capacity. Our bound is the only one that decreases with h and is consistently lower than other norm-based bounds. The effective capacity of function class decreases with increasing network size. Generalization bounds scale as C/m, where C is the effective capacity. Our bound decreases with h and is consistently lower than other norm-based bounds, improving over VC-dimension for networks larger than 1024. Our capacity bound decreases with network size, unlike other norm-based bounds which increase significantly for larger networks. This suggests that our bound is a useful tool for understanding generalization behavior in complex networks. Our capacity bound decreases with network size, unlike other norm-based bounds which increase significantly for larger networks. This suggests that our bound is a useful tool for understanding generalization behavior in complex networks. The behavior of our complexity measure is also compared between networks trained on real and random labels in a different setting. In this section, a lower bound for the Rademacher complexity of neural networks is proven, matching the dominant term in the upper bound. The lower bound is shown on a smaller function class than F W, with an additional constraint on the spectral norm of the hidden layer, allowing for comparison with existing results and extending the lower bound. The lower bound for the Rademacher complexity of neural networks matches the dominant term in the upper bound. It is shown on a smaller function class than F W, with an additional constraint on the spectral norm of the hidden layer. This allows for comparison with existing results and extends the lower bound. The lower bound for the Rademacher complexity of neural networks matches the dominant term in the upper bound, extending the lower bound to a smaller function class with an additional constraint on the spectral norm of the hidden layer. The complete proof is provided in the supplementary Section C.3. The lower bound for neural networks' capacity with bounded spectral norm and scalar output improves over previous bounds based on Lipschitz constant, showing a gap between them. The lower bound for spectral norm bounded neural networks with scalar output and element-wise activation functions improves over previous capacity bounds based on Lipschitz constant, revealing a gap between network capacity and Lipschitz constant. This non-trivial lower bound excludes neural networks with all rank-1 weight matrices, demonstrating a capacity gap between ReLU activated networks and linear networks. The smaller function class excludes neural networks with rank-1 weight matrices, showing a capacity gap between ReLU activated networks and linear networks. The construction can be extended to more layers by setting weight matrices in intermediate layers to the Identity matrix. The function class is defined by the parameter set, with Lipschitz bounds s1 and s2. Choosing \u03b1 and \u03b2 such that \u03b1^2 = s1 and max i\u2208[h] \u03b2i = s2 results in a stronger result from Theorem 3. The parameter set defines the function class with Lipschitz bounds s1 and s2. Choosing \u03b1 and \u03b2 such that \u03b1^2 = s1 and max i\u2208[h] \u03b2i = s2 results in a stronger lower bound from Theorem 3. This result improves on previous lower bounds in the literature. In this paper, a new capacity bound for neural networks is presented, which decreases with the increasing number of hidden units. The focus is on understanding the role of width in the generalization behavior of two-layer networks, as well as the interplay between depth and width in controlling network capacity. The study aims to explain the better generalization performance of larger networks. The study focuses on the role of width in the generalization behavior of two-layer networks and the interplay between depth and width in controlling network capacity. It also provides a matching lower bound for capacity improvement in neural networks. The study aims to understand the better generalization performance of larger networks. In this paper, the focus is on the role of width in generalization behavior of neural networks. The study aims to understand the capacity improvement in larger networks and the interplay between depth and width. The authors do not address optimization algorithms converging to low complexity networks or the effects of different hyperparameter choices on complexity. Future work will explore the implicit regularization effects of optimization algorithms for neural networks. Parameter choices impact solution complexity. Implicit regularization effects of optimization algorithms for neural networks are of interest for future research. An experiment involved training a pre-activation ResNet18 on CIFAR-10 dataset with specific architecture details. The architecture consists of a convolution layer, 8 residual blocks, and a linear layer. The number of output channels and strides in the residual blocks follow a specific pattern. Different architectures are trained with varying values of k. Training is done using SGD with specific parameters and stopping criteria. In experiments, fully connected feedforward networks were trained on CIFAR-10, SVHN, and MNIST datasets using specific training parameters and data augmentation techniques. Thirteen architectures were trained with sizes ranging from 2^3 to 2^15, with a focus on achieving a cross-entropy loss of 0.001. In experiments, fully connected feedforward networks were trained on CIFAR-10, SVHN, and MNIST datasets using specific training parameters and data augmentation techniques. Thirteen architectures were trained with sizes ranging from 2^3 to 2^15, with a focus on achieving a cross-entropy loss of 0.001. The training involved random horizontal flip, random crop of size 28 \u00d7 28, and zero padding. The networks were trained using SGD with mini-batch size 64, momentum 0.9, and fixed step size. Weight decay, dropout, and batch normalization were not used in the experiments. Training was stopped when the cross-entropy reached 0.01 or after 1000 epochs. For experiments, fully connected feedforward networks were trained on CIFAR-10, SVHN, and MNIST datasets without weight decay, dropout, or batch normalization. Training stopped when cross-entropy reached 0.01 or after 1000 epochs. Evaluation included calculating exact bounds for generalization, setting margin to 5th percentile of data points, and adjusting bounds for binary classification to account for number of classes. The margin of data points was set at the 5th percentile. Bounds in BID2 and Neyshabur et al. (2015c) were adjusted for binary classification by multiplying with factors. Random initialization was used as the reference matrix for bounds. Distributions were estimated using standard Gaussian kernel density estimation. Figures 6 and 7 display measures on networks of different sizes trained on SVHN and MNIST datasets. In this section, the behavior of measures on networks trained on SVHN and MNIST datasets is shown. The over-parametrization phenomenon in the MNIST dataset is illustrated in FIG10, along with a comparison of generalization bounds. Theorem 2 is generalized to p norm, with Lemma 11 introducing a cover for the p ball with entry-wise dominance. Theorem 5 provides guarantees for any h, p \u2265 2, \u03b3 > 0, \u03b4 \u2208 (0, 1), and U 0 \u2208 R h\u00d7d. Theorem 5 extends the generalization guarantees to p norm, introducing Lemma 11 for constructing a cover with entry-wise dominance. The generalization error is bounded with probability 1 - \u03b4 over the choice of the training set, showing improvements for p of order ln h. The bound tightens for larger values of h, particularly for p = ln h. Corollary 6 provides a tighter bound for generalization error with probability 1 - \u03b4 over the training set, improving for p of order ln h. The bound decreases with larger values of h, particularly for p = ln h. The generalization error for a function f(x) = V[Ux] + is bounded by a vector-contraction inequality for Rademacher complexities. A technical result from Maurer (2016) is used in the proof, showing that the Rademacher complexity of certain networks can be decomposed. The Rademacher random variables i are defined for vectors in R^d. The Rademacher complexity of networks can be decomposed to that of hidden units, as shown in Lemma 9. The complexity of the class F_W is bounded by a vector-contraction inequality for Rademacher complexities. The Rademacher complexity of networks can be decomposed to that of hidden units, as shown in Lemma 9. The complexity of the class F_W is bounded by a vector-contraction inequality for Rademacher complexities. In equations (5) and (4), the inequality is proved using induction on t, where the ramp loss is shown to be Lipschitz with respect to each dimension. The ramp loss is \u221a2\u03b3-Lipschitz due to its dependence on correct label scores. Lemma 10 states a contraction inequality for convex, increasing functions and Rademacher random variables, used in the proof of Theorem 1. The lemma in the current text chunk is used in the proof of Theorem 1, showing a contraction inequality for convex, increasing functions and Rademacher random variables. This lemma is then applied with specific parameters to complete the proof. The lemma in the current text chunk introduces a covering lemma that allows for proving a generalization bound without knowing the norms of network parameters. It shows how to cover a ball with a dominating set and bounds the size of such a cover. The lemma proves a generalization bound by constructing a set of vectors based on certain parameters. The generalization error is bounded with probability over the training set, and the lemma uses a union bound to cover different cases. The lemma provides a generalization bound by constructing vectors based on specific parameters. It uses a union bound to cover different cases and bounds the generalization error with probability over the training set. Lemma 14 provides specific results for the case p = 2, bounding the generalization error for a given function f(x) = V[Ux] +. The generalization error is bounded with probability over the training set using specific parameters and vectors constructed in the lemma. The generalization error for any function f(x) = V[Ux] + is bounded with probability over the training set using specific parameters and vectors. The proof of the lemma directly follows from Lemma 14, providing a generalization bound for any p \u2265 2. The proof of Theorem 2 follows from Lemma 14 and provides a generalization bound for any p \u2265 2. Lemma 15 gives a looser bound for p = 2 with additional constants and logarithmic factors. It states that the generalization error for any function f(x) = V[Ux] + is bounded with probability over the training set. The generalization error for a function f(x) = V[Ux] + is bounded with probability over the training set, as shown in Lemma 15. The proof of Theorem 5 directly follows from Lemma 15, utilizing notation to hide constants. The proof of Theorem 3 involves dividing the dataset into groups and defining specific matrices. The dataset is divided into groups and specific matrices are defined, such as Diag(\u03b2) and F(\u03be). The matrix U(\u03be) is chosen as the product of Diag(\u03b2) and F(\u03be), where F is orthonormal. The 2-norm of each row of F is bounded by 1, leading to U(\u03be) 2 \u2264 max i \u03b2 i."
}