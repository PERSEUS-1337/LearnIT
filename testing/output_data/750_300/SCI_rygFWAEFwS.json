{
    "title": "rygFWAEFwS",
    "content": "SWAP is a method to speed up DNN training by using large mini-batches and averaging weights of models computed in parallel. It produces models with good generalization performance in a shorter time compared to traditional methods like SGD. The SWAP method accelerates DNN training by using large mini-batches and averaging weights of models computed in parallel. This approach results in good generalization performance on CIFAR10, CIFAR100, and ImageNet datasets. Stochastic gradient descent (SGD) and its variants are commonly used for training deep neural networks, with each iteration computing an estimate of the gradient by sampling a mini-batch of training data. Increasing the mini-batch size with available computational resources accelerates DNN training by providing more precise gradient estimates, allowing for higher learning rates and larger reductions in training loss. Training with larger mini-batches accelerates DNN training by providing more precise gradient estimates, allowing for higher learning rates and larger reductions in training loss. Multiple nodes in a distributed setting can compute gradient estimates simultaneously on disjoint subsets of the mini-batch and produce a consensus estimate by averaging all estimates with one synchronization event per iteration. This approach requires fewer updates and synchronization events, resulting in good overall scaling behavior. Training with larger mini-batches accelerates DNN training by providing more precise gradient estimates. Consensus estimate is achieved by averaging estimates with one synchronization event per iteration. However, there is a maximum batch size that can lead to worse generalization performance, limiting the usefulness of large-batch training strategies. Stochastic Weight Averaging (SWA) is a method that produces models efficiently. Stochastic Weight Averaging (SWA) is a method that produces models with good generalization performance by averaging the weights of models sampled from the final stages of training. Generating multiple independent SGD sequences and averaging models from each can lead to improved performance. By generating multiple independent SGD sequences and averaging models from each, the resulting model achieves similar generalization performance. Stochastic Weight Averaging in Parallel (SWAP) is derived from these observations to accelerate DNN training by better utilizing available resources. Using observations from training models with large-batches, Stochastic Weight Averaging in Parallel (SWAP) is a simple strategy to accelerate DNN training by better utilizing compute resources. SWAP achieves comparable generalization performance to models trained with small-batches, but in a similar time frame to large-batch training runs. SWAP accelerates DNN training by utilizing compute resources efficiently, achieving comparable generalization performance to small-batch models in a similar time frame to large-batch training runs. It reduces training times for efficient models and outperforms the state of the art for CIFAR10, training in 68% of the time of the winning entry in the DAWNBench competition. The impact of training batch size on generalization performance remains unknown. The impact of training batch size on generalization performance is still unknown. Larger mini-batches may lead to models getting stuck in sharper global minima, which are sensitive to variations in data. However, the concept of flatness as measured by the curvature of the loss function can provide counterexamples. In a study by Dinh et al. (2017), they transform a flat... Slight shifts in minimizer location can greatly increase average loss value. Flatness, defined as curvature measured by second order approximation of loss, can have counterexamples. Studies show transformation of flat minimizer to sharp without changing model behavior, and vice versa without weight-decay. Batch size can be increased without accuracy drop up to a critical point, as validated empirically. In (McCandlish et al., 2018), authors predict that increasing batch size up to a critical point does not affect accuracy. They suggest that a larger batch size leads to mini-batch gradient being close to the full gradient. In (Hoffer et al., 2017), authors argue that using a larger batch size implies fewer model updates for a fixed number of epochs. In (Hoffer et al., 2017), authors argue that training with larger batch sizes for longer times improves generalization performance by increasing the number of model updates. This strategy, although time-consuming, generates models that generalize well. The batch size affects the optimization process, with larger batch sizes not necessarily improving convergence rates. Adaptive batch size methods have been proposed to address this issue. Local SGD is a distributed optimization algorithm that balances gradient precision and communication costs by allowing workers to adjust batch sizes. Adaptive batch size methods have been developed to improve convergence rates, but they often require specific datasets or extensive hyper-parameter tuning. Post-local SGD is a variant of Local SGD that refines large-batch training outputs. It has been observed to improve model generalization and achieve significant speedups. The authors have observed that SWAP achieves better generalization and speedups compared to Post-local SGD by averaging models after tens of thousands of updates, as opposed to Post-local SGD which averages after at most 32 iterations. This difference suggests that the mechanisms powering the success of SWAP and Post-local SGD are likely different. Stochastic weight averaging (SWA) is a method where models are sampled from later stages of an SGD training run and their weights are averaged to improve generalization. This strategy has been effective in various domains such as deep reinforcement learning and semisupervised learning. Stochastic Weight Averaging (SWA) is a method where models are averaged to improve generalization. SWA has been effective in deep reinforcement learning, semisupervised learning, Bayesian inference, and low-precision training. In this work, SWA is adapted to accelerate DNN training through a three-phase algorithm. Stochastic Weight Averaging (SWA) is adapted to accelerate DNN training through a three-phase algorithm. Workers train a single model in the first phase with synchronization and a higher learning rate. In the second phase, workers refine their models independently with different weights. The final phase averages the weights and computes new batch-normalization statistics for the final output. During the final phase of the Stochastic Weight Averaging (SWA) algorithm, models are averaged and new batch-normalization statistics are computed for the final output. Phase 1 ends before reaching zero training loss or 100% accuracy, allowing for better generalization performance. Phase 2 involves reducing batch size and independent small-batch training. During phase 2 of the Stochastic Weight Averaging (SWA) algorithm, batch size is reduced for independent small-batch training by each worker, resulting in different models produced at the end of training. Figure 1 shows accuracies and learning-rate schedules during a run of SWAP. At the end of the training process, each worker produces a different model. Figure 1 displays accuracies and learning-rate schedules for SWAP. In the small-batch phase, models diverge due to stochasticity, but the averaged model outperforms individual models consistently. In the small-batch phase of SWAP, models diverge from each other due to stochasticity. The averaged model consistently outperforms individual models, as shown by test accuracies. The test accuracy of the averaged weight model is computed by averaging independent models. To visualize the mechanism behind SWAP, orthogonal vectors u, v are chosen to plot the error achieved by the test network on a plane containing the outputs of different algorithm phases. Loss values are plotted by generating a weight vector \u03b8 and evaluating test and train accuracies. The loss values are plotted by generating a weight vector \u03b8 and evaluating test and train accuracies for the model. In Figure 2, the training and testing error for the CIFAR10 dataset is shown, with 'LB' representing the output of phase one, 'SGD' the output of a single worker after phase two, and 'SWAP' the final output. The level-sets of the training error form an almost convex basin, with 'LB' and 'SGD' lying in the outer edges of the basin. In Figure 2a, the training error level-sets form a convex basin, with 'LB' and 'SGD' on the outer edges. The model in phase 2 moved to a different side of the basin, while the final model 'SWAP' is closer to the center. On the test loss landscape (Figure 2b), 'LB' and 'SGD' points have higher error due to basin topology variations, but 'SWAP' is less affected being closer to the center. In Figure 2b, variations in basin topology affect 'LB' and 'SGD' points with higher error, while 'SWAP' is less affected due to its central position. In Figure 3, worker points 'SGD1', 'SGD2', 'SGD3' are at different sides of the basin, with 'SWAP' closer to the center, resulting in lower testing errors. In Figure 3b, worker points lie in regions of higher testing errors than 'SWAP', which is close to the center. The weight iterates behave similar to an Ornstein Uhlenbeck process in later stages of SGD, reaching a stationary distribution similar to a high-dimensional Gaussian. The weight iterates in later stages of SGD behave like an Ornstein Uhlenbeck process, reaching a stationary distribution similar to a high-dimensional Gaussian. This distribution is centered at the local minimum, with a covariance that grows with the learning rate and is inversely proportional to the batch size. The shape of the distribution depends on the Hessian of the mean loss and the covariance of the gradient. The authors argue that the distribution is concentrated near the 'shell' of the ellipsoid, making it unlikely for SGD to access the interior. The authors argue that the weight iterates in later stages of SGD behave like a high-dimensional Gaussian distribution concentrated near the 'shell' of an ellipsoid, making it unlikely for SGD to access the interior. Sampling weights from different SGD runs can lead to weights spread out on the surface of the ellipsoid, converging to the same point if starting in the same basin of attraction. The weight iterates in later stages of SGD behave like a high-dimensional Gaussian distribution concentrated near the 'shell' of an ellipsoid. Sampling from different SGD runs can lead to weights spread out on the surface of the ellipsoid, converging to the same point if starting in the same basin of attraction. The advantage of SWA and SWAP over SGD is shown by measuring the cosine similarity between the gradient descent direction and the direction towards the output of SWAP, which decreases as training progresses. In later stages of training, the cosine similarity between the gradient descent direction and the direction towards the output of SWAP decreases, indicating slower progress. SWAP shows improved performance for image classification tasks on CIFAR10, CIFAR100, and ImageNet datasets. In this section, the performance of SWAP for image classification tasks on CIFAR10, CIFAR100, and ImageNet datasets is evaluated. The best hyper-parameters were found using grid searches, training was done using mini-batch SGD with Nesterov momentum and weight decay. Experiments were conducted on a machine with 8 NVIDIA Tesla V100 GPUs. For image classification tasks on CIFAR10, CIFAR100, and ImageNet datasets, experiments were conducted using SWAP. The data was augmented using cutout and a custom ResNet 9 model was used. Experiments were run on a machine with 8 NVIDIA Tesla V100 GPUs using Horovod for computation distribution. Statistics were collected over 10 runs with specific settings for two phases of training. In image classification experiments using SWAP on CIFAR10, CIFAR100, and ImageNet datasets, different batch sizes and GPU configurations were tested. Phase one used 4096 samples per batch on 8 GPUs, phase two used 8 workers with one GPU each and 512 samples per batch. Results were compared for models trained with small-batch only, large-batch only, and SWAP. The study compared models trained with small-batch only, large-batch only, and SWAP using 2 GPUs for 100 epochs. Significant improvement in test accuracies was observed after averaging the models. Training with small-batches achieves higher testing accuracy than training with large-batches but takes longer. SWAP terminates in comparable time to large-batch runs and achieves accuracies on par with small batch training. State of the art training speeds for CIFAR10 are achieved with SWAP. Using SWAP with 8 Tesla V100 GPUs, a phase one batch size of 2048 samples and 28 epochs, and a phase two batch size of 256 samples for one epoch can achieve state-of-the-art training speeds for CIFAR10. This method reaches 94% test accuracy in 27 seconds, comparable to the current front-runner in the DAWNBench competition using 4 Tesla V100 GPUs in 37 seconds. Using SWAP with 16 Tesla V100 GPUs, the learning rates and batch sizes are modified for large-batch experiments on ImageNet. Phase 1 runs for 22 epochs with doubled batch size and learning rates, while Phase 2 has two independent workers each with 8 GPUs. Using SWAP with 16 Tesla V100 GPUs, large-batch experiments were conducted on ImageNet by doubling batch size and learning rates. Phase 1 ran for 22 epochs with these modifications, while Phase 2 had two independent workers each with 8 GPUs. Doubling the batch size reduced test accuracies, but SWAP recovered generalization performance with reduced training times. Results are shown in Table 3, compiled over 3 runs. No tuning was required for these accelerations. SWAP was used with 16 Tesla V100 GPUs for large-batch experiments on ImageNet, doubling batch size and learning rates. The generalization performance was recovered with reduced training times, and no tuning was needed for these accelerations. Top1 accuracies were maintained by switching back to the original schedule when transitioning between phases. For large-batch experiments on ImageNet, SWAP was used with 16 Tesla V100 GPUs, doubling batch size and learning rates. Top1 accuracies were maintained by switching back to the original schedule when transitioning between phases. SWAP was compared with SWA using the CIFAR100 dataset, sampling the same number of models and maintaining the same number of epochs per sample. In this section, experiments are conducted using the CIFAR100 dataset to compare SWA and SWAP. SWA samples each model with 10 epochs in-between and averages them, while SWAP runs 8 independent workers for 10 epochs each and uses their average as the final model. The goal is to see if SWA can recover the test accuracy of small-batch training on a large-batch training run. Initial training cycles with cyclic learning rates are used to sample 8 models. The large-batch training run achieves lower accuracy, but SWA does not improve it. Evaluating SWA with small batches after a large-batch phase, interrupting at the same accuracy as phase 1 of the CIFAR100 experiment. Executing SWA using small batches after a large-batch training run. Small-batch SWA reaches test accuracy but takes three times longer than SWAP. Starting SWA cyclic learning rate schedule from the best model found by solely small-batch training. Starting SWA cyclic learning rate schedule from the best model found by solely small-batch training, we select the peak learning rate using grid-search. Once the SWA schedule is specified, we re-use the peak learning rate settings in SWAP. Small-batch SWA achieves better accuracy than SWAP by around 0.9% at 6.8x more. In phase two, SWAP achieves a speed-up over SWA by relaxing constraints and increasing the schedule. The resulting model achieved a test accuracy of 79.11% in 241 seconds, 3.5x faster than SWA. By increasing the phase two schedule and sampling two models from each worker, SWAP achieved a test accuracy of 79.11% in 241 seconds, 3.5x faster than SWA. The algorithm uses large mini-batches for quick approximate solutions and refines them by averaging weights from models trained with small-batches to improve generalization performance. The algorithm utilizes large mini-batches for quick approximate solutions and refines them by averaging weights from models trained with small-batches to improve generalization performance. This novel approach allows for faster training and achieves good generalization performance. Refining models with large mini-batch runs using SWA or SWAP can achieve good generalization performance in image classification datasets like CIFAR10, CIFAR100, and ImageNet. Visualizations show that averaged weights are closer to the center of the training loss basin compared to models trained with stochastic gradient descent. The basin where the large mini-batch run converges seems to be the same as where the refined models are. The existing evidence suggests that averaged weights are closer to the center of a training loss basin than models trained with stochastic gradient descent. The basin where large mini-batch runs converge appears to be the same as where refined models are found. Choosing the transition point between large-batch and small-batch training requires an additional hyperparameter, which can be determined through grid search. Our method requires choosing a transition point between large-batch and small-batch training, determined through grid search. Future work will focus on a principled method for selecting this point and exploring SWAP's behavior with other optimization schemes like LARS, mixed-precision training, post-local SGD, or NovoGrad. SWAP's design allows for substituting these schemes in the large-batch stage. The design of SWAP allows for substituting optimization schemes like LARS, mixed-precision training, post-local SGD, or NovoGrad in the large-batch stage. Parameters used in experiments were obtained through independent grid searches. Momentum and weight decay constants were kept at 0.9 and 5 \u00d7 10 \u22124 for all CIFAR experiments. Tables 5 and 6 list the remaining hyperparameters. For all CIFAR experiments, momentum and weight decay constants were set at 0.9 and 5 \u00d7 10 \u22124. Tables 5 and 6 show the other hyperparameters used, with a stopping accuracy of 100% indicating maximum epochs were reached."
}