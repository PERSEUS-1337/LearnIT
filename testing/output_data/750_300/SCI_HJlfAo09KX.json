{
    "title": "HJlfAo09KX",
    "content": "We study model recovery for data classification using a one-hidden-layer fully-connected neural network with sigmoid activations. Under Gaussian inputs, the empirical risk function exhibits strong convexity and smoothness, allowing gradient descent to converge linearly to a critical point close to the ground truth. This can be achieved via the tensor method without needing fresh samples at each iteration. Neural networks have gained research interest for their success in practical applications. Gradient descent converges linearly to a critical point close to the ground truth using the tensor method, without requiring new samples at each iteration. This is the first global convergence guarantee for empirical risk minimization using cross entropy via gradient descent in learning one-hidden-layer neural networks. Efforts have been made to understand the theoretical underpinnings of the success of deep neural networks in practical domains like computer vision and artificial intelligence. Research focuses on which functions can be represented by neural networks and when gradient descent is effective for optimizing non-convex loss functions. Efforts have been made to understand the theoretical underpinnings of deep neural networks, focusing on function representation, gradient descent optimization, and generalization. Research in model-recovery aims to recover the underlying model parameter W for better generalization. The goal is to recover the underlying model parameter W from data generated by a neural network model. Previous studies focused on regression problems where samples are generated based on weight vectors and input data. The curr_chunk discusses regression and classification problems studied in various settings with neural networks. The regression problem involves weight vectors and Gaussian input, while the classification problem involves conditional distributions and Gaussian input. These problems have been studied in different models with ReLU activations and identity mapping. The curr_chunk discusses label distribution under conditional distribution with weight vectors and Gaussian input in neural networks. Previous studies focused on model recovery using gradient descent and statistical guarantees with squared loss. The curr_chunk discusses statistical guarantees for model recovery using squared loss in gradient descent. Previous studies showed positive definite Hessian in the local neighborhood of the ground truth, requiring fresh samples at each iteration for convergence. In this paper, the aim is to develop a strong statistical guarantee for the loss function in eq. (2), which is more challenging but practical for classification compared to the squared loss function. This study provides the first performance guarantee for the recovery of one-hidden-layer neural networks using the cross entropy loss function for the classification problem with sigmoid activations. The study presents performance guarantees for recovering one-hidden-layer neural networks using cross entropy loss for multi-neuron classification with sigmoid activations. It shows that the empirical risk function is uniformly strongly convex in a local neighborhood of the ground truth, leading to linear convergence of gradient descent with near-optimal sample complexity. The study provides performance guarantees for recovering one-hidden-layer neural networks with cross entropy loss for multi-neuron classification. It demonstrates linear convergence of gradient descent with near-optimal sample complexity, showing that W converges to W n at a rate of O(dK 9/2 log n/n) in the Frobenius norm. The convergence guarantee does not require a fresh set of samples at each iteration due to uniform strong convexity in the local neighborhood. The study demonstrates linear convergence of gradient descent for recovering one-hidden-layer neural networks with cross entropy loss. It shows that W converges to W n at a rate of O(dK 9/2 log n/n) in the Frobenius norm, with a computational complexity of O(ndK 2 log(1/ )). The tensor method proposed in BID38 provides an initialization near the ground truth, replacing the homogeneous assumption on activation functions with a condition on the curvature of activation functions around W. The study demonstrates linear convergence of gradient descent for recovering one-hidden-layer neural networks with cross entropy loss. It replaces the homogeneous assumption on activation functions with a condition on the curvature of activation functions around W. The proof develops new techniques to exploit statistical information of geometric curvatures, ensuring uniform concentrations for classification problems using the squared loss. Our technique provides performance guarantees for classification problems using the squared loss, which is easier to analyze than cross entropy. The parameter recovery viewpoint is relevant for non-convex learning in signal processing problems such as matrix completion, phase retrieval, blind deconvolution, and tensor decomposition. The parameter recovery viewpoint is crucial for non-convex learning in signal processing problems like matrix completion, phase retrieval, blind deconvolution, and tensor decomposition. It focuses on average-case performance and enables global convergence of simple local search algorithms. The studies of one-hidden-layer network model can be categorized into landscape analysis and model recovery. In the landscape analysis of one-hidden-layer network models, it is known that with a large network size compared to data input, there are no spurious local minima, and all local minima are global. Spurious bad local minima can exist in the optimization landscape even with multiple neurons in the under-parameterized setting. In the under-parameterized setting with multiple neurons, Tian studied the landscape of the population squared loss surface with ReLU activations, revealing spurious bad local minima. Zhong et. al. provided characterizations for the local Hessian in regression with various activation functions. In the model recovery problem, when the number of neurons is smaller than the input dimension, BID28 showed linear convergence of gradient descent with ReLU activation under Gaussian input. In the model recovery problem, BID28 demonstrated linear convergence of gradient descent with ReLU activation under Gaussian input when the number of neurons is smaller than the input dimension. When \u03c6(\u00b7) has bounded derivatives, there are no critical points other than the global minimum, and gradient descent converges linearly with any initialization. The sample complexity is O(d log 2 d) for classification with sub-Gaussian inputs using squared loss. The study analyzes a different form of cross entropy loss and addresses the model recovery classification problem under the multi-neuron case, a novel approach not previously studied. Our study focuses on analyzing a different form of cross entropy loss and addressing the model recovery classification problem under the multi-neuron case, which is a novel approach not previously studied. This differs from previous work that studied one-hidden-layer or two-layer neural networks with different structures under Gaussian input. The paper discusses different neural network architectures and loss functions, highlighting the unique approach taken in the study. It is organized into sections covering problem formulation, main results, initialization method, numerical examples, and conclusions. Boldface letters are used to denote vectors and matrices throughout the paper. The paper discusses neural network architectures, loss functions, and the convergence of gradient descent. It covers problem formulation, main results, initialization methods, numerical examples, and conclusions. Boldface letters denote vectors and matrices, with specific notations for transpose, norms, and singular values. Constants are denoted by c, C, C1, etc. The paper discusses neural network architectures, loss functions, and the convergence of gradient descent. It covers problem formulation, main results, initialization methods, numerical examples, and conclusions. The gradient and Hessian of a function are denoted by \u2207f(W) and \u22072f(W), respectively. Singular values of W are denoted by \u03c3i(W). Constants are represented by c, C, C1, etc. The generative model for training data is described, followed by the gradient descent algorithm for learning network weights. The generative model for training data is described, followed by the gradient descent algorithm for learning network weights. The goal is to estimate W = [w1, ..., wK] for a one-hidden layer neural network model in a classification setting. Conditioned on x \u2208 R^d, the classification setting involves mapping y to a discrete label using a one-hidden layer neural network model with K neurons. The goal is to estimate W = [w1, ..., wK] by minimizing the empirical risk function, which is the cross entropy loss. Due to the nonconvex nature of the function, vanilla gradient descent with arbitrary initialization may get stuck at local minima when estimating W. The gradient descent algorithm for estimating W in a nonconvex function involves a well-designed initialization scheme to avoid local minima. The update rule is defined with a step size \u03b7, and the algorithm uses the same set of training samples throughout execution. This approach contrasts with existing methods. The algorithm for estimating W in a nonconvex function uses a step size \u03b7 and the same set of training samples throughout. This differs from methods like BID38, which resample at each iteration. An important quantity regarding \u03c6(z) is introduced to capture geometric properties of the loss function. Before stating the main results, an important quantity regarding \u03c6(z) is introduced to capture the geometric properties of the loss function, as distilled in BID38. The local strong convexity of f n (W) in a neighborhood is characterized, with \u03c1(\u03c3) depicted as a function of \u03c3 for the sigmoid activation in Fig. 1. The activation function considered in this paper is depicted as a function of \u03c3 for the sigmoid activation. The local strong convexity of f n (W) in a neighborhood is characterized, with the Hessian of the empirical risk function being positive definite with high probability. Theorem 1 guarantees the positive definiteness of the Hessian of the empirical risk function in the local neighborhood of W for the classification model with sigmoid activation function, under certain conditions. Theorem 1 ensures the positive definiteness of the Hessian of the empirical cross-entropy loss function in a neighborhood of the ground truth W, as long as W is full-column rank and the sample size is sufficiently large for sigmoid activation. The bounds depend on network dimension parameters and activation function. The sample complexity for the sigmoid activation in Theorem 1 is order-wise near-optimal in d up to polynomial factors of K and log d, guaranteeing positive definiteness of the Hessian in a neighborhood of the ground truth W. The sample complexity is near-optimal in d for the classification problem with quantized labels. Due to strong convexity, there exists a unique local minimizer W n close to the ground truth W, and gradient descent converges linearly to W n. Theorem 2 states that for a classification model with sigmoid activation function, there exists a unique critical point W n close to the ground truth W, and gradient descent converges linearly to W n under certain conditions. Theorem 2 guarantees the existence of a critical point W n in the vicinity of W, with gradient descent converging linearly to W n at a rate of O(K 9/4 d log n/n). This ensures consistent recovery of W as n approaches infinity, with linear convergence as long as initialized within the basin of attraction. Achieving -accuracy requires specific conditions. The tensor method proposed in BID38 guarantees consistent recovery of W as n approaches infinity. Gradient descent converges linearly to W n at a rate of O(K 9/4 d log n/n) when initialized within the basin of attraction. Achieving -accuracy requires a computational complexity of O(ndK 2 log(1/ )). The tensor method in BID38 ensures consistent recovery of W as n approaches infinity. The initialization algorithm based on this method is summarized in Algorithm 2, including two steps. The initialization algorithm in Algorithm 2 for consistent recovery of W involves estimating the direction of each column of W and reducing a third-order tensor to a lower-dimension tensor for non-orthogonal decomposition. The algorithm in Algorithm 2 estimates the direction of each column of W and reduces a third-order tensor to a lower-dimension tensor for non-orthogonal decomposition. It then applies non-orthogonal tensor decomposition to output the estimate s i V w i, where s i is a random sign. The magnitude of w i and the sign s i are approximated by solving a linear system of equations, under certain technical assumptions. Assuming the activation function meets specific conditions for regression, including non-zero parameters and curvature around the ground truth. The algorithm estimates the direction of each column of W and reduces a third-order tensor for non-orthogonal decomposition, leading to an estimate of s i V w i under certain technical assumptions. The algorithm estimates the direction of each column of W and reduces a third-order tensor for non-orthogonal decomposition, leading to an estimate of s i V w i under certain technical assumptions. For a larger class of activation functions such as sigmoid and tanh, there exists a positive constant \u03b4 such that m l1,i (\u00b7) is strictly monotone over the interval ( w i \u2212 \u03b4, w i + \u03b4), and the derivative of m l1,i (\u00b7) is lower bounded by some constant for all i. The performance guarantee for the initialization algorithm is presented in Theorem 3, stating that under specific assumptions, the output W 0 \u2208 R d\u00d7K of Algorithm 2 satisfies DISPLAYFORM1 with high probability. Theorem 3 states that for the classification model under certain assumptions, if the sample size is sufficiently large, the output W 0 of Algorithm 2 satisfies DISPLAYFORM1 with high probability. The proof involves accurate estimation of the direction of W and the norm of W. The approach in part (b) differs from previous work by relaxing the homogeneous assumption on activation functions. Further details can be found in the supplementary materials. In part (b), the approach relaxes the homogeneous assumption on activation functions. Gradient descent is implemented to show strong convexity of the empirical risk function around W. Multiple initializations in the local region lead to convergence to the same critical point Wn. Variance of output is calculated after random initialization. The variance of the output of gradient descent is calculated after multiple random initializations in the local region, showing convergence to the same critical point Wn. The success rate of the experiment is determined by the standard deviation SDn \u2264 10 \u22122, with Figure 2 (a) illustrating the successful rate of gradient descent over 50 sets of training samples for each pair of n and d, where K = 3 and d = 15, 20, 25 respectively. The experiment shows that gradient descent converges to the same local minima with high probability when the sample complexity is large enough. Statistical accuracy of the local minimizer is demonstrated when initialized close to the ground truth. The statistical accuracy of the local minimizer for gradient descent is demonstrated when initialized close to the ground truth. Average estimation error decreases as sample size increases, matching theoretical predictions. When K = 3 and d = 20, 35, 50, the estimation error decreases with increasing sample size, aligning with theoretical predictions. Gradient descent with cross entropy loss outperforms squared loss in a classification problem, favoring the former. The study focuses on model recovery of a one-hidden-layer neural network using cross entropy loss in multi-neuron classification. The study focuses on model recovery of a one-hidden-layer neural network using cross entropy loss in a multi-neuron classification problem. It characterizes the sample complexity for local strong convexity near the ground truth, ensuring gradient descent convergence with high probability. Future work aims to extend the analysis to more scenarios. The study focuses on model recovery of a one-hidden-layer neural network using cross entropy loss in a multi-neuron classification problem. It characterizes the sample complexity for local strong convexity near the ground truth, ensuring gradient descent convergence with high probability. Future work aims to extend the analysis to more scenarios, including different activation functions and network structures like convolutional neural networks. The population loss function is denoted as DISPLAYFORM0, with the expectation taken with respect to the distribution of the training sample (x; y). The proof of Theorem 1 involves showing the Hessian \u2207 2 f (W ) of the population loss function. The study focuses on model recovery of a one-hidden-layer neural network using cross entropy loss in a multi-neuron classification problem. The proof of Theorem 1 involves showing the smoothness and convexity properties of the Hessian of the population loss function. Lemmas 1, 2, and 3 demonstrate the smoothness and closeness of the Hessian of the empirical loss function to its population counterpart in a neighborhood of W. The study focuses on model recovery of a one-hidden-layer neural network using cross entropy loss in a multi-neuron classification problem. Lemmas 1, 2, and 3 demonstrate the smoothness and closeness of the Hessian of the empirical loss function to its population counterpart in a neighborhood of W, establishing local strong convexity and smoothness. Lemma 1 shows that for sigmoid activations with W F \u2264 1, a constant C exists where a certain condition holds when W \u2212 W F \u2264 0.7. Lemma 2 establishes local strong convexity and smoothness of the population loss function for sigmoid activations within a neighborhood of W. The proof of Theorem 1 for sigmoid activations involves showing the Hessian of the empirical loss function is close to the Hessian of the population loss function in a uniform sense. Lemma 3 establishes a condition for this closeness, and when combined with Lemma 1, Theorem 1 is obtained. The proof of Theorem 1 involves combining Lemmas 1 and 3 to show the closeness of the Hessian of the empirical loss function to the population loss function. This leads to the establishment of strong convexity in a certain region, ensuring the existence of at most one critical point. The proof of Theorem 2 focuses on the concentration of the gradient around a specific point in the same region. The proof of Theorem 2 shows that the gradient of the function concentrates around a specific point in the region, guaranteeing the existence of a critical point. The gradient descent converges linearly to this point with a properly chosen step size. The lemma establishes that the gradient of the function uniformly concentrates around a critical point. For the sigmoid activation function, there exists a unique critical point in the region due to local strong convexity. Theorem 2 guarantees the existence of one critical point that satisfies certain conditions. Lemma 3 and Lemma 4 establish the existence of a critical point in B(W, r) due to local strong convexity. Theorem 2 guarantees the existence of one critical point that satisfies certain conditions. Corollary 1 states that there exists one critical point Wn in B(W*, r) that is close to W. By utilizing various inequalities and theorems, the existence of a critical point in B(W, r) is established. By Theorem 1, the existence of a critical point in B(W, r) is established. Local linear convergence of gradient descent is proven by showing the update rule for Wt and the gradient of f_n(Wt). Setting \u03b7 < DISPLAYFORM13 ensures convergence. The proof shows that gradient descent converges linearly to the local minimizer Wn by accurately estimating the direction of W and satisfying a mild condition in Assumption 2. The proof in part (b) defines a tensor operation for matrices A, B, and C, without requiring a homogeneous condition for the activation function. The tensor T (A, B, C) entry (i, j, k) is calculated as shown. For regression, if n \u2265 dpoly (K, \u03ba, t, log d), DISPLAYFORM1 holds with high probability. Bernstein inequality is used to bound estimation errors for P 2 and R 3 in the proof. In regression, it's applied to individual neuron terms and combined using triangle inequality in BID38. In classification, a similar approach is taken with slight proof variations. In regression, Bernstein inequality is used to bound estimation errors for P 2 and R 3 by applying it to individual neuron terms and combining them using triangle inequality in BID38. In classification, Bernstein inequality is applied to terms associated with all neurons together. The label y i in classification is naturally bounded, while the output y i in regression needs to be upper bounded through homogeneously bounded conditions of the activation function. Refer to BID38 for detailed proof. To estimate w i for i = 1, . . . , K, a different approach is provided. To estimate w i for i = 1, . . . , K, a different proof is provided in this section, which does not require homogeneous conditions on the activation function but assumes a more relaxed condition. A quantity Q 1 is defined to have information of w i, which can be estimated by solving. A quantity Q1 is defined to have information of wi, which can be estimated by solving an optimization problem. The estimation process involves substituting estimated values and solving equations to obtain an estimate of wi. The estimation process involves substituting estimated values to obtain an estimate of wi using equations. The sign of \u03b2i can correctly estimate si, and there exists a constant \u03b4 > 0 for the inverse function of m3,1 to have an upper-bounded derivative. The estimation process involves substituting estimated values to obtain an estimate of wi using equations. By Assumption 2 and (21), there exists a constant \u03b4 > 0 such that the inverse function g(\u00b7) of m3,1(\u00b7) has an upper-bounded derivative. If the sample size n \u2265 dpoly(K, \u03ba, t, log d), then Q1 and Q1, Vu i and si wi can be arbitrarily close. By employing the mean value theorem, we obtain the desired result. The sub-gaussian and sub-exponential norms of a random variable X are defined as X \u03c82 and X \u03c81 respectively. If X \u03c82 is upper bounded, then X is a sub-gaussian random variable. These definitions are summarized from Vershynin (2012). The gradient and Hessian of E[(W;)] are calculated, with a concise representation of the hessian block provided. The calculation of \u2206 j,l is evaluated using the mean value theorem. The hessian block is represented concisely as g j,l (W ) = \u03be j,l (W ) (p(W )(1\u2212p(W ))) 2, and \u2206 j,l is calculated using the mean value theorem. The goal is to upper bound E T 2 j,l,k. The hessian block is represented as g j,l (W) = \u03be j,l (W)(p(W)(1\u2212p(W)))^2. Using Cauchy-Schwarz inequality, the goal is to upper bound E T 2 j,l,k. Lemma 5 states that for sigmoid activation function \u03c6(x), a certain inequality holds for a large constant C. For the sigmoid activation function \u03c6(x), a certain inequality holds for a large constant C, which depends on the constant z. By plugging in the given formulas and making certain assumptions, it can be concluded that the inequality holds for some constant C. The proof involves presenting upper and lower bounds of the Hessian of the population risk at ground truth, and applying Lemma 1 to obtain a uniform bound in the neighborhood of W. The proof involves presenting upper and lower bounds of the Hessian of the population risk at ground truth, and applying Lemma 1 to obtain a uniform bound in the neighborhood of W. Further, by bounding \u2207 2 f (W ), it can be concluded that when W \u2212 W F \u2264 0.7, a certain inequality holds. The proof involves bounding \u2207 2 f (W) and applying Lemma 1 to obtain a uniform bound in the neighborhood of W. By bounding \u2207 2 f (W), it can be concluded that when W - W F \u2264 0.7, a certain inequality holds. The proof involves bounding \u2207 2 f (W) and applying Lemma 1 to obtain a uniform bound in the neighborhood of W. By bounding \u2207 2 f (W), it can be concluded that when W - W F \u2264 0.7, a certain inequality holds. The triangle inequality is used to adapt the analysis in BID21 to the setting, with the -covering number N of the Euclidean ball B (W , r) and a -cover set W = {W 1 , \u00b7 \u00b7 \u00b7 , W N }. Various events A t , B t , and C t are defined and their probabilities are bounded separately. In the proof, events A t, B t, and C t are defined with their probabilities bounded separately. A technical lemma is stated to bound the term P(B t), with G i being upper bounded."
}