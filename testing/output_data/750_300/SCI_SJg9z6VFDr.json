{
    "title": "SJg9z6VFDr",
    "content": "Recently, various neural networks have been proposed for irregularly structured data such as graphs and manifolds. All existing graph networks have discrete depth, but a new model called graph ordinary differential equation (GODE) extends the idea of continuous-depth models to graph data. The derivative of hidden node states is parameterized with a graph neural network, and the output states are the solution to this ordinary differential equation. Two end-to-end methods for efficient training of GODE are demonstrated: indirect back-propagation with the adjoint method and direct back-propagation through the ODE solver. The GODE model utilizes a graph neural network to solve an ordinary differential equation. Two efficient training methods are demonstrated: indirect back-propagation with the adjoint method and direct back-propagation through the ODE solver. Direct backprop outperforms the adjoint method in experiments. Bijective blocks enable low memory consumption. GODE can be easily adapted to different graph neural networks, improving accuracy in tasks like node classification and graph classification. The GODE model, utilizing a graph neural network, achieves $\\mathcal{O}(1)$ memory consumption and can be adapted to various graph networks for improved accuracy in tasks like node and graph classification. It offers a continuous model in time, memory efficiency, accurate gradient estimation, and generalizability. Neural networks like CNNs excel in tasks like image classification and machine translation but are limited to grid data. Graph data structures are used for irregularly structured data like social networks and protein interaction networks. Graph data structure represents objects as nodes and relations as edges, widely used for modeling irregularly structured data like social networks, protein interaction networks, and citation graphs. Traditional methods like random walk and graph embedding have limitations in expressive capacity, leading to the development of graph neural networks (GNN) inspired by the success of CNNs. Graph neural networks (GNN) were proposed as a new class of models to model graphs, inspired by the success of CNNs. Two main methods for performing convolution on a graph are spectral methods and non-spectral methods, with the former computing the graph Laplacian and filtering in the spectral domain, while the latter aims to approximate filters without computing the graph Laplacian for faster speed. Spectral methods compute the graph Laplacian and filter in the spectral domain, while non-spectral methods perform convolution directly in the graph domain, aggregating information from node neighbors. GraphSAGE learns a convolution kernel in an inductive manner. Existing GNN models have discrete layers, making it challenging for the GNN. The recently proposed GraphSAGE learns a convolution kernel in an inductive manner. Existing GNN models have discrete layers, making it hard to model continuous diffusion processes in graphs. The neural ordinary differential equation (NODE) views a neural network as an ordinary differential equation, and we extend this concept to graphs with graph ordinary differential equations (GODE). GraphSAGE learns a convolution kernel in an inductive manner. The neural ordinary differential equation (NODE) views a neural network as an ordinary differential equation, and we extend this concept to graphs with graph ordinary differential equations (GODE). GODE models message propagation on a graph as an ODE, trained with the adjoint method. NODEs offer adaptive evaluation, accuracy-speed control, and continuous invertible models. However, in benchmark image classification tasks, NODEs are significantly inferior to state-of-the-art discrete-layer models. In this work, the inferior performance of NODEs in image classification tasks is attributed to error in gradient estimation during training. A memory-efficient framework for accurate gradient estimation is proposed, leading to high accuracy for both NODE and GODE in benchmark tasks. The framework proposed improves gradient estimation for NODEs, enhancing performance in image classification tasks. It is memory-efficient and achieves high accuracy for both NODE and GODE in benchmark tasks, including reducing test error on CIFAR10 from 19% to 5%. Additionally, the framework generalizes ODE to graph data with GODE. The framework proposed enhances performance in image classification tasks by improving gradient estimation for NODEs. It is memory-efficient and achieves high accuracy for both NODE and GODE in benchmark tasks, including reducing test error on CIFAR10 from 19% to 5%. Additionally, the framework generalizes ODE to graph data with GODE models, demonstrating improved performance on various datasets. Several new architectures based on numerical methods in ODE solver have been proposed, including a stable architecture based on analysis of the ODE. The neural ordinary differential equation (NODE) treats the neural network as a continuous ODE and has been used in a continuous normalizing flow for generative models. The adjoint method, widely used in optimal control and geophysical problems, has recently been applied to ODE. Additionally, augmented neural ODEs have been proposed. To improve the expressive capacity of neural ordinary differential equations (NODEs), augmented neural ODEs were proposed by Dupont et al. (2019). However, none of the existing methods address the issue of inaccurate gradient estimation, leading to inferior empirical performances of NODE in benchmark tasks compared to discrete-layer models. Graph neural networks (GNNs) can be categorized into spectral and non-spectral methods, with spectral GNNs performing filtering in the Fourier domain of a graph. Empirical performances of NODE in benchmark tasks are inferior to discrete-layer models. GNNs are divided into spectral and non-spectral methods. Spectral GNNs filter in the Fourier domain, requiring information of the whole graph. Non-spectral GNNs only consider message aggregation around neighbor nodes, leading to localized and less computationally intensive models. Several spectral methods, such as graph convolution based on the graph Laplacian, have been introduced. Several spectral methods have been introduced for graph convolution, with some focusing on localized filters to reduce computation burden. Defferrard et al. (2016) used Chebyshev expansion to accelerate running speed without computing the graph Laplacian and its eigenvectors. Kipf & Welling (2016) proposed a localized approach as well. Defferrard et al. (2016) and Kipf & Welling (2016) introduced localized spectral filtering methods for graph convolution, accelerating running speed without the need for computing the graph Laplacian and its eigenvectors. Non-spectral methods focus on convolution operations considering only neighboring nodes, while MoNet (Monti, 2017) and GraphSAGE (Hamilton et al.) generalize convolution to graphs using CNNs. Fast localized spectral filtering on graphs was proposed by et al. (2016), while MoNet (Monti, 2017) and GraphSAGE (Hamilton et al., 2017) use CNNs to generalize convolution on graphs. Graph attention networks (Veli\u010dkovi\u0107 et al., 2017) learn different weights for node neighbors, and the graph isomorphism network (GIN) (Xu et al., 2018) has a structure as expressive as the Weisfeiler-Lehman graph isomorphism test. Invertible blocks are neural network blocks with a bijective mapping forward function. Invertible blocks are neural network blocks with a bijective mapping forward function, used in normalizing flow models to calculate log-density of data distribution. Bijective blocks have been utilized in building invertible networks and were proposed by Gomez et al. (2017). Invertible blocks are used in normalizing flow models to calculate log-density of data distribution. They were first introduced by Kingma & Dhariwal in 2018 and later utilized by Jacobsen et al. in 2018. Gomez et al. proposed using invertible blocks for memory-efficient network structures by discarding activation of middle layers. This approach allows for back propagation without storing activation, as each layer's activation can be reconstructed from the next layer with invertible blocks. Invertible blocks are utilized in normalizing flow models for calculating log-density of data distribution. They enable back propagation without storing activation of middle layers, as each layer's activation can be reconstructed from the next layer with invertible blocks. This leads to the consideration of discrete-layer models with residual connections and the transition to neural ordinary differential equations (NODE) when adding more layers with shared weights. In the transition to neural ordinary differential equations (NODE), the difference equation becomes a NODE where hidden states are represented by z(t) in the continuous case and x k in the discrete case. The forward pass of a model with discrete layers involves different layers having their own function f k, while in the continuous case, f is shared across all time t. The forward pass of a NODE is determined by z(0) = input and integration time T. The forward pass of a model with discrete layers involves applying an output layer on x K. In NODE, the forward pass is determined by z(0) = input and integration time T. Integration can be done with various ODE solvers like Euler Method, Runge-Kutta Method, VODE solver, and Dopris Solver. The adjoint method is commonly used in optimal process control and functional analysis. The forward pass in NODE can be solved using ODE solvers like Euler Method, Runge-Kutta Method, VODE solver, and Dopris Solver. The adjoint method is widely used in optimal process control and functional analysis. Model parameters are denoted as \u03b8, which is independent of time. The adjoint is defined and compared with back-propagation methods on NODE. During back-propagation on NODE, two methods are compared: the adjoint method and direct back-propagation through the ODE solver. The adjoint method involves solving the hidden state in reverse-time, while direct back-propagation saves evaluation time points during the forward pass and rebuilds the computation graph during the backward pass. During direct back-propagation through the ODE solver, evaluation time points are saved during the forward pass and the computation graph is rebuilt during the backward pass to accurately reconstruct the hidden state and evaluate the gradient. This allows for optimization of \u03b8 to minimize the loss function. During back-propagation through the ODE solver, evaluation time points are saved during the forward pass and the computation graph is rebuilt during the backward pass to accurately reconstruct the hidden state and evaluate the gradient. This allows for optimization of \u03b8 to minimize the loss function. The reverse-time integration in Eq. 6 is solved with any ODE solver, requiring determination of z(t) by solving Eq. 2 in reverse-time. The forward pass solves Eq. 2 forward in time, while the backward pass solves Eq. 2 and 6 in reverse time, with initial condition determined from Eq. 5 at time T. The reverse-time ODE solver in the backward pass may lead to inaccurate gradients in adjoint methods due to the instability of the reverse-time integration, causing a mismatch between the hidden states solved forward-time and reverse-time. The reverse-time ODE solver in the backward pass may result in inaccurate gradients in adjoint methods due to instability, causing a mismatch between hidden states solved forward-time and reverse-time. Error in gradient dL d\u03b8 can be caused by the mismatch between z(t) and h(t). If the ODE is stable in both forward-time and reverse-time, then Re(\u03bb i (J f )) = 0 \u2200i, where \u03bb i (J f ) is the ith eigenvalue of J f. Proposition 1 states that if the Jacobian of the original system has eigenvalues with non-zero real parts, either the forward-time or reverse-time ODE is unstable. Large real parts of eigenvalues make the ODE sensitive to numerical errors, affecting the accuracy of solutions and computed gradients. The adjoint method may also be affected by numerical errors in solving the ODE in reverse-time. The instability caused by numerical errors affects the accuracy of solutions and gradients in solving ODEs. To address this, a proposal to directly back-propagate through the ODE solver is made, ensuring accurate hidden states. This can be achieved by saving activation values for back-propagation or accurately reconstructing them. The proposal suggests direct back-propagation through the ODE solver to ensure accurate hidden states at evaluated time points. This can be done by saving activation values or reconstructing them, guaranteeing accuracy regardless of stability. Algorithm 1 outlines a method for accurate gradient estimation in ODE solvers for free-form functions, ensuring precise hidden states at evaluated time points. This approach involves direct back-propagation and defining the adjoint with discrete time. Detailed derivations of the algorithm are provided in appendices E and F. Algorithm 1 outlines a method for accurate gradient estimation in ODE solvers for free-form functions, ensuring precise hidden states at evaluated time points. The algorithm involves selecting an initial step size, iterating through time points, and adjusting the step size based on error estimates. The detailed derivations of the algorithm can be found in appendices E and F. The algorithm outlined in Algorithm 1 ensures accurate gradient estimation in ODE solvers for free-form functions by adaptively varying step sizes based on error estimates during the forward pass. The solver outputs integrated values and evaluation time points, deleting middle activations to save memory. The algorithm in Algorithm 1 ensures accurate gradient estimation in ODE solvers by adaptively varying step sizes based on error estimates during the forward pass. It outputs integrated values and evaluation time points, saving memory by deleting middle activations. During the backward pass, the solver rebuilds the computation graph and performs reverse-time integration. The algorithm supports free-form continuous dynamics, making it a generic method with no constraints on the form of f. Memory consumption analysis is based on the number of layers in f. During the backward pass, the solver performs reverse-time integration. Our algorithm supports free-form continuous dynamics, making it a generic method. Memory consumption analysis shows our method is more efficient than a naive solver due to deletion of middle activations and not needing to search for optimal step sizes. Step-wise checkpoint method is also discussed for gradient computation. Our method for reverse-time integration is more memory-efficient than a naive solver, as it deletes middle activations and avoids the need to search for optimal step sizes. By using a step-wise checkpoint method, memory consumption can be reduced further. Additionally, restricting the form of the function to invertible blocks can lead to even more memory savings. By restricting the form of the function to invertible blocks, memory consumption can be reduced to O(Nf). Input x is split into two parts (x1, x2) of the same size, and the forward and inverse of a bijective block are denoted as F and G. The input x is split into x1 and x2 with shape N \u00d7 C. The bijective block is denoted by F and G, with output (y1, y2) of the same size as (x1, x2). Differentiable neural networks F and G are used, along with a bijective function \u03c8(\u03b1, \u03b2) and its inverse \u03c8\u22121(\u03b1, \u03b2). Theorem 1 states that the block defined is a bijective mapping. Different \u03c8 functions can be applied for various tasks, ensuring accurate reconstruction of x from y. Theorem 1 establishes that a bijective function \u03c8(\u03b1, \u03b2) leads to a bijective mapping as per Eq. 8. This allows for the use of different \u03c8 functions for various tasks, ensuring accurate x reconstruction from y without the need to store activations, making it memory-efficient. The approach transitions from discrete to continuous graph neural networks, introducing graph ordinary differential equations (GODE) for representation. A graph is depicted with nodes and edges in Fig. 2, each node uniquely colored for visualization ease. Neural networks transition from discrete to continuous layers, introducing graph ordinary differential equations (GODE) for graph representation. Nodes and edges are depicted in Fig. 2, with each node uniquely colored for visualization ease. GNNs are typically represented in a message passing scheme, with differentiable functions parameterized by neural networks. A GNN operates at the kth layer with edge representation e u,v between nodes u and v. N(u) is the set of neighbor nodes for u. \u03b6 is a permutation invariant operation like mean, max, or sum. \u03b3(k) and \u03c6(k) are differentiable functions. GNN is a 3-stage model for a node u: (1) Message passing from neighbors v \u2208 N(u) to u using function \u03c6(\u00b7). (2) Message aggregation where u aggregates messages from N(u) using \u03b6. The GNN operates at the kth layer with edge representation between nodes. Message passing from neighbors to a node is done using a function \u03c6(\u00b7), followed by message aggregation using a permutation invariant operation like mean or sum. The node's states are then updated based on the original states and aggregated messages. The discrete-time GNN can be converted to a continuous-time GNN using a graph ordinary differential equation (GODE). The discrete-time GNN can be converted to a continuous-time GNN using a graph ordinary differential equation (GODE), which is an ODE that captures highly non-linear functions and has the potential to outperform its discrete-layer counterparts. The asymptotic stability of GODE is related to the over-smoothing phenomena, and graph convolution is a special case of Laplacian smoothing. The asymptotic stability of GODE is related to over-smoothing phenomena, with graph convolution being a special case of Laplacian smoothing. The continuous smoothing process involves eigenvalues of the symmetrically normalized Laplacian being real and non-negative, leading to real and non-positive eigenvalues in the ODE. When transitioning from a discrete to a continuous model, the ODE becomes asymptotically stable if all eigenvalues of the normalized Laplacian are non-zero. As time progresses, all trajectories converge, leading to similar features among nodes and potentially decreasing classification accuracy. As time progresses, trajectories converge, leading to similar features among nodes and potentially decreasing classification accuracy. Experiments with a CNN-NODE on image and graph classification tasks show the method's effectiveness on various benchmark datasets. Our method was tested on various benchmark graph datasets, including bioinformatic, social network, and citation networks. Different from previous experiments, we used raw datasets without pre-processing for graph classification tasks and followed specific train-validation-test splits for node classification tasks. Details of the datasets can be found in the appendix. For image classification tasks, a ResNet18 model was modified into its corresponding NODE model using a sequence of conv-bn-relu layers. For graph datasets, GODE can be applied to any graph neural network by replacing functions in the model. GODE can be easily applied to various graph neural network architectures like GCN, GAT, ChebNet, and GIN for tasks on graph datasets. Different graph neural network architectures like GCN, GAT, ChebNet, and GIN were trained with different depths of layers for a fair comparison. The models used the same hyper-parameters, such as channel number, and achieved the best results among all depths for each model structure on graph classification tasks. The models used the same hyper-parameters for different graph neural network architectures like GCN, GAT, ChebNet, and GIN. They achieved the best results among all depths for each model structure on graph classification tasks. The channel number and number of hops were set accordingly for each model. The mean and variance of accuracy were calculated for 10 runs with different number of hidden layers. The adjoint method and direct back-propagation were compared on the same task. For GNN structures, different hidden layer numbers were tested (1, 2, 3) with mean and variance accuracy calculated over 10 runs. Direct back-propagation outperformed the adjoint method in both tasks. CNN-NODE modified ResNet18 to NODE18 for classification tasks, while a GODE model with GCN was trained for graph networks. Direct back-propagation consistently outperformed the adjoint method for both tasks, validating the analysis on the instability of the adjoint method. Our training method reduced error rates on image classification tasks, with NODE18 outperforming ResNet101 on CIFAR10 and CIFAR100 datasets. Our training method reduces error rates on image classification tasks, with NODE18 outperforming ResNet101 on CIFAR10 and CIFAR100 datasets. Additionally, our method consistently outperforms the adjoint method on benchmark graph datasets. Our method supports NODE and GODE models with free-form functions, demonstrating robustness to different orders of ODE solvers. Our method supports NODE and GODE models with free-form functions, demonstrating robustness to different orders of ODE solvers. Bijective blocks defined as Eq. 8 can be easily generalized with general neural networks F and G. Differentiable bijective mapping \u03c8(\u03b1, \u03b2) can be adapted to different tasks. Results for different \u03c8 are reported in Table 3, with GODE models outperforming their discrete-layer counterparts. Results for different \u03c8 functions are reported in Table 3, showing that GODE models outperformed their discrete-layer counterparts significantly. The continuous-time model was found to be more important than the coupling function \u03c8. Lower memory cost was also validated, with details in appendix B. Additionally, results for different models on graph classification tasks are summarized in Table 4, including experiments with different structures such as GCN and ChebNet. The continuous-time model is emphasized over the coupling function \u03c8, with lower memory cost validated in appendix B. Results for various models on graph classification tasks are summarized in Table 4, including experiments with GCN, ChebNet, and GIN structures. GODE models, both free-form and invertible block, were compared with discrete-layer counterparts using paired t-tests, showing GODE models performed significantly better. This suggests the importance of the continuous process model for graph models. Inference testing was conducted for NODE and GODE models to assess the influence of integration. The continuous process model is crucial for graph models, as shown by the paired t-test comparing GODE and its discrete-layer counterparts. GODE models outperformed significantly in most experiments. Integration time in NODE and GODE models was tested during inference, with results indicating that short integration times lead to insufficient information gathering, while long integration times result in oversmoothing issues. GODE enables the modeling of continuous diffusion processes on graphs, with a memory-efficient direct back-propagation method proposed for accurate determination. The paper proposes GODE to model continuous diffusion processes on graphs and introduces a memory-efficient back-propagation method for gradient estimation. It addresses the over-smoothing issue in GNNs and improves accuracy on benchmark tasks, making it comparable to state-of-the-art discrete layer models. Our paper addresses the over-smoothing problem in GNNs and improves gradient estimation for NODE, achieving accuracy comparable to state-of-the-art discrete layer models. Experiments are conducted on various datasets including citation networks, social networks, and bioinformatics datasets. The structure and experiments for the invertible block are explained. The structure of invertible blocks is explained, following the work of Gomez et al. (2017) with two important modifications. Generalizing to a family of bijective blocks with different \u03c8 and proposing a parameter state checkpoint method for accurate inversion. The algorithm is summarized in Algo. 2. The parameter state checkpoint method allows bijective blocks to be called multiple times for accurate inversion. Pseudo code for forward and backward functions is provided in PyTorch, with memory optimization in the forward function by keeping only necessary outputs. In the backward function, the block is inverted to calculate inputs from outputs before gradient calculation. In the forward function, only outputs y1, y2 are kept to reduce memory usage. The backward function inverses the block to calculate x1, x2 from y1, y2 and then computes the gradient. Memory efficiency of the bijective block is demonstrated by comparing memory consumption with a memory-inefficient method in training a GODE model on the MUTAG dataset. Memory consumption of bijective blocks was compared using a memory-efficient function and a memory-inefficient method in training a GODE model on the MUTAG dataset. Results showed that the memory consumption significantly differed between the two methods, with the memory-efficient approach demonstrating lower memory usage as the depth of the ODE blocks increased. Our memory-efficient bijective blocks in Algo. 2 significantly reduce memory consumption compared to conventional methods. Increasing depth from 10 to 20 only slightly increases memory usage from 2.2G to 2.6G, as we only need to store outputs in cache. The bijective block takes O(1) memory by deleting activations of middle layers. The increased memory consumption in the memory-efficient network is minimal compared to input data. The memory-efficient bijective blocks in Algorithm 2 reduce memory consumption by caching states of F and G. The increased memory usage is minimal compared to input data. The stability of an ODE in forward-time and reverse-time is determined by the eigenvalues of the Jacobian of f. The stability of an ODE in forward-time and reverse-time is determined by the eigenvalues of the Jacobian of f. If the ODE is stable in both directions, then the real part of the eigenvalues must be non-positive. A bijective block with defined forward and reverse mappings is proven to be a bijective mapping. The eigenvalues of the Jacobian matrix J must have non-positive real parts for stability. Theorem 1 states that a bijective block with defined forward and reverse mappings is a bijective mapping. To prove bijectivity, it needs to be shown that the mapping is both injective and surjective. The assumption that Forward(x1, x2) = Forward(x3, x4) implies x1 = x3 and x2 = x4. This shows the mapping is injective. By constructing x1, x2 such that Forward(x1, x2) = [y1, y2], the mapping is proven to be surjective and therefore bijective. A figure is used to illustrate the computation graph. In the proposition statement, it is shown that the mapping is bijective by constructing x1, x2 such that Forward(x1, x2) = [y1, y2]. A figure is used to demonstrate the computation graph and the gradient derivation. The gradient of parameters in a neural-ODE model is derived from an optimization perspective, extending from continuous to discrete cases. In this section, the gradient of parameters in a neural-ODE model is derived from an optimization perspective, extending from continuous to discrete cases. Notations include z(t) for hidden states, \u03b8 for parameters, x for input, y for target, \u0177 for predicted output, and J(\u0177, y) for loss. The continuous model follows an ODE dz(t)/dt = f(z(t), t, \u03b8), with forward pass defined as \u0177. The continuous model is defined by an ODE dz(t)/dt = f(z(t), t, \u03b8), with the forward pass as \u0177. The loss function is denoted as J(\u0177, y). The training process is formulated as an optimization problem, considering one ODE block. The Lagrangian Multiplier Method is used for optimization. The text discusses the use of the Lagrangian Multiplier Method for optimization in a continuous model defined by an ODE block. It mentions the Karush-Kuhn-Tucker (KKT) conditions as necessary for optimality and derives results from the KKT condition. The derivative with respect to \u03bb is calculated at the optimal point. The KKT conditions are necessary for optimality. Derivative w.r.t. \u03bb at the optimal point is calculated. Continuous and differentiable perturbations on \u03bb(t) are considered. The conditions for Leibniz integral rule are checked. The optimal \u03bb(t) satisfies dz(t)/dt - f(z(t), t, \u03b8) = 0 for all continuous differentiable \u03bb(t). The Leibniz integral rule allows for switching integral and differentiation. In the discrete case, all integrations are replaced with finite sums. The ODE condition in discrete cases corresponds to the analysis in Eq. 10 and 11."
}