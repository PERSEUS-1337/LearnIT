{
    "title": "rJxVxiiDoX",
    "content": "We investigate low-bit quantization to reduce computational cost of deep neural network (DNN) based keyword spotting (KWS) by integrating quantization into model training, known as quantization-aware training. Experimental results show that this approach can recover performance models quantized to lower bits representations. Additionally, combining quantization-aware training with weight matrix factorization reduces model size and computation for small-footprint keyword spotting while maintaining performance. By combining quantization-aware training and weight matrix factorization, model size and computation for small-footprint keyword spotting can be significantly reduced while maintaining performance. Context information is incorporated by stacking frames in the input. Quantization degrades performance on device, but quantization-aware training can mitigate this degradation. Deployed keyword spotting models are always quantized, with 16 bit and 8 bit quantizations being common in the industry. Quantization-aware training is used to optimize weights against quantization errors in small-footprint low-power keyword spotting models. 8 bit and 4 bit quantized models can be successfully trained using this method. In this work, quantization-aware training is used to build a small-footprint low-power keyword spotting system. 8 bit and 4 bit quantized models can be successfully trained using this method, optimizing weights against quantization errors. The approach involves dynamic quantization with shifts and scales calculated independently column-wise, similar to \"bucketing\" or \"per-channel\" quantization with technical differences. The approach involves dynamic quantization of DNN weight matrices, with shifts and scales calculated independently column-wise. Quantization-aware training is used to mitigate accuracy loss, with experiments conducted using the keyword 'Alexa' on a 500 hrs far-field speech dataset. Evaluation is done using end-to-end Detection Error Tradeoff (DET) curves. The study involved training 70 models on a corpus of 69 diverse far-field speech data and a 100 hrs dataset for evaluation. The models were evaluated using end-to-end Detection Error Tradeoff (DET) curves to measure miss rate vs. false accept rate (FAR) and DET area under curve (AUC). Training was done using a GPU-based distributed DNN training method in 3 stages, with a small ASR DNN pre-trained in the 1st stage. The study involved training 70 models on diverse far-field speech data and evaluating them using DET curves. The models were pre-trained with a small ASR DNN and then quantized for performance improvement. The DET curves showed no significant difference between 16 bit and 8 bit quantized models compared to full-precision models."
}