{
    "title": "HJxvcJhVYS",
    "content": "Inverse problems are common in natural sciences, involving inferring complex posterior distributions over hidden parameters from observations. A model based on differential equations is used, but the inference over parameters is challenging. A proposed generalization of Bayesian optimization helps approximate the posterior distribution for better inference. In this work, a generalization of the Bayesian optimization framework is proposed to approximate inference for the inverse problem of finding parameters in a system. The method uses Stein variational gradient descent on estimates from a Gaussian process model to learn approximations to the posterior distribution. Preliminary results show the method's effectiveness in likelihood-free inference for reinforcement learning environments. The problem addressed is estimating parameters of a physical system using a computational model when the likelihood function is not available. Methods like approximate Bayesian computation and conditional density estimation are used, but limited simulations in robotics and reinforcement learning pose challenges to current approaches. Recent methods aim to improve efficiency in simulations for robotics and reinforcement learning by constructing conditional density estimators or learning approximations to the likelihood function followed by Markov chain Monte Carlo. Recent methods aim to improve efficiency in simulations for robotics and reinforcement learning by constructing conditional density estimators or learning approximations to the likelihood function followed by Markov chain Monte Carlo. This paper investigates combining variational inference methods with Bayesian optimization to enhance data efficiency. This paper explores combining variational inference methods with Bayesian optimization to enhance data efficiency in simulations for robotics and reinforcement learning. The approach uses a Thompson sampling strategy to refine variational approximations to a black-box posterior, proposing parameters for new simulations through Stein variational gradient descent over samples from a Gaussian process. The method also includes optimal subsampling of variational approximations for batch evaluations of the simulator. The approach combines variational inference with Bayesian optimization to improve data efficiency in simulations for robotics and reinforcement learning. It uses Stein variational gradient descent over Gaussian process samples to propose parameters for new simulations. The method includes optimal subsampling of variational approximations for batch evaluations of the simulator models. The goal is to estimate a distribution that approximates a posterior distribution over simulator parameters given observations from a target system. The text discusses a Bayesian optimization approach to estimate a posterior distribution over simulator parameters without access to a likelihood function. The method involves minimizing a discrepancy between the estimated distribution and the target distribution using a kernelized Stein discrepancy. The approach does not require gradients of the target distribution and utilizes a Gaussian process model to approximate the likelihood. The text introduces a Bayesian optimization method using the kernelised Stein discrepancy (KSD) to estimate a posterior distribution over simulator parameters. This approach does not require gradients of the target distribution and utilizes a black-box method. The algorithm includes a Gaussian process model for likelihood approximation, Thompson sampling for candidate selection, and kernel herding for optimal parameter sampling. By learning the distribution directly via Stein variational gradient, it bypasses the need to model the parameter space. The text introduces a Bayesian optimization method using the kernelised Stein discrepancy (KSD) to estimate a posterior distribution over simulator parameters. By learning the distribution directly via Stein variational gradient descent (SVGD), it bypasses the need to model the parameter space. The approach utilizes a Gaussian process model for likelihood approximation and a synthetic likelihood function defined by a GP to model g : \u03b8 \u2192 \u2212\u2206 \u03b8. The text discusses using a Gaussian process (GP) to model a synthetic likelihood function for Bayesian optimization. The GP approximates the simulations-observations discrepancy, which is costly to evaluate and non-differentiable, making it suitable for applying SVGD in the optimization loop. The GP approximates the simulations-observations discrepancy, making it suitable for applying SVGD in the optimization loop. Candidate distributions q n \u2208 Q are selected using Thompson sampling, which accounts for uncertainty by sampling functions from the GP posterior. Thompson sampling is used for selecting point candidates in Bayesian optimization problems, accounting for uncertainty by sampling functions from the GP posterior. For models with finite feature maps like SSGPs, the approach involves sampling weights from a multivariate Gaussian to constitute a sample from the posterior of a SSGP. The acquisition function is defined based on this sampling approach. Sampling weights are extracted from a multivariate Gaussian to form a sample from the posterior of a SSGP. The acquisition function is defined based on this sampling method, with SVGD representing the variational distribution as a set of particles optimized through perturbations. The particles in the SSGP model are initialized as i.i.d. samples from the prior distribution and optimized through perturbations using a specific kernel. The optimization process guides the particles towards local maxima of the log posterior while encouraging diversification. Gradients of the log posterior are available for SSGP models with differentiable mean functions. In contrast to the true posterior, gradients of logp n are available for SSGP models with differentiable mean functions. For a uniform prior, note that \u2207 \u03b8 log p(\u03b8) = 0 almost everywhere. Selecting a distribution q n, evaluations of \u2206 \u03b8 from samples \u03b8 \u223c q n update the GP model. Using a large number of particles M improves Algorithm 1: DBO. Using a large number of particles M improves Algorithm 1: DBO by maximizing the acquisition function via SVGD and exploring the approximate posterior surface. To avoid the high cost of simulations, a subset of query parameters {\u03b8 n,j } S j=1 is selected through optimal subsampling of the candidate q n using kernel herding. Kernel herding is used to select a subset of query parameters {\u03b8 n,j } S j=1 from the candidate q n to minimize error in empirical estimates for expectations under a given distribution q. The procedure involves constructing a set of samples that minimizes the error bounded by the maximum mean discrepancy (MMD) between the kernel embedding of q and its subsampled version. In the case of SSGPs, the algorithm involves iterating for j \u2208 {0, . . . , S \u2212 1} and \u03b1 0 = \u03c8 q = E \u03b8\u223cq [\u03c6(\u03b8)], using a modified feature map instead of the original one. The kernel herding procedure in SSGPs involves selecting informative samples based on the GP posterior kernel, which provides an embedding for the query distribution q. This sampling scheme improves model performance by considering previously observed locations in the GP data. The posterior kernel in the distributional Bayesian optimization (DBO) algorithm provides an embedding for the query distribution q, accounting for previously observed locations in the GP data. The method is evaluated against mixture density networks (MDNs) in synthetic data scenarios. The proposed method is compared to mixture density networks (MDNs) in synthetic data scenarios using OpenAI Gym's 3 cart-pole environment. The experiment involves fixing physics parameters and generating a dataset of trajectories with randomly sampled actions. Details on the experimental setup can be found in Appendix B. The study compared a Bayesian optimization approach to inverse problems with MDNs in a synthetic data scenario using OpenAI Gym's cart-pole environment. The method was able to recover the target system's posterior and provide better approximations than MDNs, particularly in terms of MMD. An open-source implementation is available online. The paper presented a Bayesian optimization approach for inverse problems on simulator parameters, showing better approximations than MDNs in terms of MMD. Results suggest that distributional Bayesian optimization is more sample-efficient for inferring parameters in reinforcement learning. Future work includes scalability and theoretical analysis. Bayesian optimization is more sample-efficient than other methods for inferring parameters in reinforcement learning environments. Future work includes scalability and theoretical analysis. The code is available at: https://github.com/rafaol/dbo-aabi2019. The posterior over g is determined by \u00b5 N (\u03b8) := \u00b5 0 (\u03b8) + \u03c6(\u03b8), with fast incremental updates proposed by Gijsberts and Metta (2013) to reduce time complexity to update the GP posterior."
}