{
    "title": "r1e-nj05FQ",
    "content": "Multi-agent cooperation is crucial in various organisms, including humans, despite individual incentives conflicting with the common good. Understanding cooperative behavior in self-interested individuals is significant in multi-agent reinforcement learning and evolutionary theory. This study focuses on intertemporal social dilemmas, where the conflict between individual and group interests is pronounced. Through a combination of MARL and natural selection, the research demonstrates the emergence of cooperative behavior. In intertemporal social dilemmas (ISDs), individual inductive biases for cooperation can be learned through MARL and natural selection. A modular architecture for deep reinforcement learning agents supports multi-level selection, showing cooperation in challenging environments. Results are interpreted in the context of cultural and ecological evolution. Nature exhibits cooperation across all scales, from genomes to bacteria. Learning agents support multi-level selection and cooperation in challenging environments. Nature demonstrates cooperation across all scales, from genomes to bacteria, despite natural selection favoring individual selfish interests. Altruism can be favored by selection when cooperating individuals interact with other cooperators, realizing the benefits of cooperation without being exploited by defectors. Altruism can be favored by selection when cooperating individuals interact with other cooperators, realizing the benefits of cooperation without being exploited by defectors. Cooperation among self-interested agents is an important topic in multi-agent deep reinforcement learning, formalized as an intertemporal social dilemma. In multi-agent deep reinforcement learning, cooperation among self-interested agents is a key topic. The problem is formalized as an intertemporal social dilemma, where agents face a trade-off between collective welfare and individual utility. Evolutionary theory predicts that agents tend to converge to defecting strategies instead of achieving the collectively optimal outcome. The goal is to find training regimes where cooperation emerges among individuals to resolve social dilemmas. Previous work has identified various solutions to this challenge. Evolution can be applied to find multi-agent training regimes for cooperation to emerge in social dilemmas, contrary to hand-crafted approaches. Evolution can be applied to remove hand-crafted intrinsic motivation in deep learning, similar to its use in optimizing hyperparameters, implementing black-box optimization, and evolving neuroarchitectures, regularization, loss functions, and reward functions. Evolution can be applied to optimize hyperparameters, implement black-box optimization, and evolve neuroarchitectures, regularization, loss functions, and reward functions. These principles are driven by single-agent search, competitive multi-agent tasks, and evolutionary simulations of predator-prey dynamics. The proposed system distinguishes between optimization processes to address specific challenges in ISDs. The system proposed distinguishes between optimization processes that unfold over two distinct time-scales: fast learning and slow evolution. Individual agents participate in an intertemporal social dilemma with fixed intrinsic motivation, while that motivation is subject to natural selection in a population. Intrinsic motivation is modeled as an additional term in the reward of each agent. Evolutionary theory suggests that evolving individual intrinsic reward weights in a population can help mitigate intertemporal social dilemmas. This is achieved by bridging fast learning and slow evolution timescales through an intrinsic reward function implemented as a neural network. Evolutionary theory suggests that evolving individual intrinsic reward weights in a population can help mitigate intertemporal social dilemmas by bridging timescales through an intrinsic reward function. To achieve this, evolutionary dynamics must be structured, such as implementing a \"Greenbeard\" strategy where agents choose partners based on cooperativeness signals, known as assortative matchmaking. Assortative matchmaking, where agents choose partners based on cooperativeness signals, is a process that cannot explain cooperation in all taxa and is not a general method for multi-agent reinforcement learning. To address these limitations, a modular training scheme called shared reward network evolution is introduced, inspired by ideas from the theory of multi-level selection. In response to the limitations of assortative matchmaking in multi-agent reinforcement learning, a modular training scheme called shared reward network evolution is introduced. Agents consist of policy and reward network modules that evolve separately, with the policy network trained using modified rewards from the reward network. Each agent has a distinct policy network but shares the same reward network. The network is trained using modified rewards specified by the reward network. Policy and reward network modules evolve separately on a slow timescale. Each agent has a distinct policy network but shares the same reward network. Fitness for the policy network is individual reward, while fitness for the reward network is collective return for the group. Policy networks are lower level units of evolution, while reward networks are higher level units. Evolving modules separately prevents overfitting to specific policies. In multi-level selection theory, policy networks and reward networks evolve separately to prevent overfitting. Different parameters were varied, including environments, reward network features, matchmaking, and reward network evolution. This approach resolves difficult issues without manual intervention and suggests a potential evolutionary origin of social inductive biases. In this paper, we explore different combinations of parameters in Markov games within a MARL setting, focusing on intertemporal social dilemmas characterized by conflicts between short-term individual benefit and long-term group impact. In intertemporal social dilemmas like BID25 games, individual selfish actions benefit in the short term but harm the group in the long term. Two dilemmas are studied on a 2D grid, one involving collecting apples while the cleanliness of an aquifer decreases over time. In a partially observable Markov game on a 2D grid, agents collect apples that spawn in a field with a respawn rate linked to aquifer cleanliness. As waste fills the aquifer, apple respawn rate decreases until no apples can spawn. Agents must leave the apple field to clean, which offers no reward, creating a dilemma where all agents must clean to receive rewards. In the Harvest game, agents collect rewarding apples with a spawn rate depending on nearby apples. There is a dilemma between harvesting quickly for short-term gain and depleting apples for lower total yield in the long-term. In the Harvest game, agents face a dilemma between harvesting all apples quickly for short-term gain and depleting the apples, leading to lower total yield in the long-term. The reward components for players include total reward, extrinsic reward, and intrinsic reward, each contributing to different loss functions. The total reward for player i in the Harvest game is the sum of extrinsic and intrinsic rewards. Extrinsic reward is obtained from the environment when taking action a from state s, while intrinsic reward is calculated using a neural network with evolved parameters. The social preference in the Harvest game is calculated using a 2-layer neural network with evolved parameters based on fitness. The feature vector can be transformed into intrinsic reward via a reward network, with each agent having access to the same set of features. The feature vector in BID25 is player-specific and can be transformed into intrinsic reward via a reward network. Social preferences in Markov games should not be influenced by the precise temporal alignment of rewards received by different players. In Markov games, social preferences should not be influenced by the precise temporal alignment of rewards received by different players. Two ways of aggregating rewards are considered, using off-policy importance weighted actor-critic (V-Trace) BID10 and intrinsic and extrinsic value architecture. The architecture for adjusting policy in Markov games includes intrinsic and extrinsic value heads, a policy head, and evolution of the reward network. Intrinsic reward can be derived retrospectively from past rewards or prospectively from future expectations of rewards. The architecture for adjusting policy in Markov games includes intrinsic and extrinsic value heads, a policy head, and evolution of the reward network. The prospective variant derives intrinsic reward from agents' expectations of future rewards, using value estimates and stop-gradient to prevent gradient flow. Training framework includes distributed asynchronous training in multi-agent environments. The training framework for adjusting policy in Markov games involves a population of 50 agents with policies {\u03c0 i}, sampled in groups of 5 to populate 500 arenas running in parallel. Episode trajectories last 1000 steps and are used for learning. The training framework involves a population of 50 agents with policies sampled in groups of 5 to populate 500 arenas running in parallel. Episode trajectories last 1000 steps and are used for learning, with weights updated using V-Trace. Agents observe their last actions and rewards. The training framework involves a population of 50 agents with policies sampled in groups of 5 to populate 500 arenas running in parallel. Episode trajectories last 1000 steps and are used for learning, with weights updated using V-Trace. Agents observe their last actions, intrinsic and extrinsic rewards, and input them to the LSTM in the agent's neural network. The objective function includes the value function gradient, policy gradient, and entropy regularization, weighted by hyperparameters baseline cost and entropy cost. Evolution is based on a fitness measure calculated as a moving average of total episode return. Evolution is based on a fitness measure calculated as a moving average of total episode return, with matches determined by random or assortative matchmaking. In the case of shared reward network evolution, matches were determined by random or assortative matchmaking based on recent cooperativeness. Cooperative agents played with others of similar rank to ensure cooperation, while defectors played with defectors. Cooperativeness for Cleanup was calculated based on the number of steps in the last game. In the context of shared reward network evolution, matches were determined by random or assortative matchmaking based on recent cooperativeness. For Cleanup, cooperativeness was calculated based on the number of steps in the last game. In contrast, cooperative metric-based matchmaking for Harvest was based on the difference between the agent's return and the mean return of all players. This ensured that highly cooperative agents played with each other, while defecting agents played with other defectors. Cooperative metric-based matchmaking was not used for the multi-level selection model. In contrast to shared reward network evolution, cooperative metric-based matchmaking was not utilized for the multi-level selection model. The reward network was separately evolved within its own population to allow for independent exploration of hyperparameters. In contrast to cooperative metric-based matchmaking, BID24 evolved the reward network separately within its own population, allowing for independent exploration of hyperparameters and a wider exploration of the hyperparameter landscape. This approach also enabled random assignment of reward networks to policy networks, promoting generalization to a variety of policies. In contrast to previous work, this study utilized a shared reward network paired with multiple policy networks in each episode. The evolution of policy network weights and optimization-related hyperparameters was based on individual agent return, while the reward network parameters evolved based on total episode return across all players. This approach differs from evolving intrinsic rewards and focuses on evolving social features rather than remapping environmental events. The study evolved a shared reward network with multiple policy networks in each episode based on total episode return across all players. This approach focused on evolving social features for cooperation rather than remapping environmental events. In a social setting, shared reward networks play a critical role by combining group fitness and individual rewards. This approach contrasts with previous methods of aggregation and improves performance in games. In a social setting, shared reward networks combine group fitness and individual rewards, improving performance in games. PBT without intrinsic reward network performs poorly on games, showing little benefit to adding reward networks over social features if players have separate networks. When using random matchmaking, individual reward network agents perform moderately better at Harvest but no better than PBT on Cleanup. Assortative matchmaking experiments showed that adding individual reward networks significantly improved performance, indicating the importance of conditioning internal rewards on social features and a preference for cooperative agents to play together. Individual reward networks led to high performance, showing the importance of conditioning internal rewards on social features. Shared reward network agents performed as well as assortative matchmaking, indicating that agents did not need immediate access to honest signals of cooperativeness to resolve the dilemma. The study showed that agents with the same intrinsic reward function, evolved based on collective episode return, could resolve the dilemma without needing immediate access to honest signals of cooperativeness. The retrospective variant of reward network evolution outperformed the prospective variant, which relied on agents learning good value estimates before reward networks became useful. The retrospective variant of reward network evolution outperformed the prospective variant, as it does not require agents to learn value estimates before reward networks become useful. Sustainability metrics show that agents without reward networks collect apples quickly but exhibit less sustainable behavior. The absence of reward networks leads to rapid apple collection but unsustainable behavior. The retrospective reward network variant shows higher equality compared to the prospective variant. Tagging frequency increases when players fine each other during the episode. The prospective reward network leads to lower equality, while the retrospective variant has high equality. Tagging frequency is higher with prospective or individual reward networks compared to the retrospective shared reward network. The final weights of the retrospective shared reward networks suggest different social preferences are needed to resolve each game. The final weights of the retrospective shared reward networks suggest that different social preferences are needed to resolve each game. In Cleanup, a less complex reward network sufficed, while Harvest required a more complex reward function. In Cleanup, a less complex reward network sufficed, while Harvest required a more complex reward function to prevent over-exploitation of resources by other agents. The first layer weights tended to take on arbitrary positive values due to random matchmaking, leading to little specialization pressure. Organisms in real environments do not receive scalar reward signals but develop internal drives based on primary needs. In real environments, organisms develop internal drives based on primary needs rather than scalar reward signals. Na\u00efvely implementing natural selection did not lead to cooperation, but assortative matchmaking generated cooperative behavior when honest signals were present. In theory BID0 BID40, natural selection via genetic algorithms did not lead to cooperation. Assortative matchmaking generated cooperative behavior with honest signals. A new multi-level evolutionary paradigm based on shared reward networks promotes cooperation in various situations by evolving intrinsic social preferences. Evolution improves credit assignment between selfish acts and collective fitness timescales. Evolution promotes cooperation by improving credit assignment between selfish acts and collective fitness timescales, exposing social signals correlated with selfishness, and enabling mechanisms like competitive altruism and inequity aversion. Human laboratory experiments support the effectiveness of these social preferences in achieving mutual cooperation. The shared reward network evolution model is inspired by multi-level selection and promotes cooperation by enabling mechanisms like competitive altruism and inequity aversion. This model involves lower level units constantly swapping with higher level units, leading to modularity in evolution. The curr_chunk discusses how modularity in evolution can be seen in various forms in nature, such as microorganisms forming multi-cellular structures and prokaryotes incorporating modules for cooperation. The curr_chunk discusses the emergence of cooperation in organisms through mechanisms like kin selection and reciprocity, suggesting alternative evolutionary pathways. It also touches on the spread of cultural norms and the role of reward networks in human societies. The curr_chunk suggests investigating alternative evolutionary mechanisms for cooperation, such as kin selection and reciprocity. It also proposes studying an emergent version of the assortative matchmaking model to add generality to the setup. Additionally, it raises the question of combining an evolutionary approach with multi-agent communication. To enhance the setup, one could explore an emergent version of the assortative matchmaking model as suggested by BID22. Additionally, combining an evolutionary approach with multi-agent communication could lead to the emergence of cooperative behaviors like cheap talk in gameplay scenarios. The games have episodes lasting 1000 steps, with different playable area sizes for Cleanup and Harvest. Agents can observe through a 15x15 RGB window and have various actions available, including moving and tagging. The games Cleanup and Harvest have episodes lasting 1000 steps with different playable area sizes. Agents can observe through a 15x15 RGB window and have various actions available, including moving, tagging, and cleaning waste. Training involved joint optimization of network parameters and hyperparameters/reward network parameters via evolution in the standard PBT setup. Gradient updates were applied for every trajectory up to a maximum length of 100 steps. The game involves an additional action for cleaning waste. Training was done via joint optimization of network parameters and hyperparameters/reward network parameters using SGD and evolution in the standard PBT setup. Gradient updates were applied for every trajectory up to a maximum length of 100 steps, with a batch size of 32. Optimization was done via RMSProp with specific parameters, and the learning rates were allowed to evolve using PBT. The entropy cost and learning rates were evolved using PBT, with a mutation rate of 0.1 and specific perturbations for each parameter. PBT utilizes genetic algorithms to optimize hyperparameters, resulting in an adaptive schedule for joint optimization with network parameters. The network parameters were optimized through gradient descent with a mutation rate of 0.1. Hyperparameters were evolved using PBT with specific perturbations for entropy cost and learning rate. A burn-in period of 4 \u00d7 10 6 agent steps was implemented before evolution."
}