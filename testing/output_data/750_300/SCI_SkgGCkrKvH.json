{
    "title": "SkgGCkrKvH",
    "content": "Decentralized training of deep learning models using communication compression, specifically Choco-SGD, enables data privacy, on-device learning, and efficient scaling to large compute clusters. The approach achieves linear speedup with high compression ratios on non-convex functions and non-IID training data. Practical performance is demonstrated in training scenarios over decentralized user devices and in datacenters. Distributed machine learning enables training of models using optimization algorithms on non-convex functions and non-IID data. The algorithm's practical performance is demonstrated in training deep learning models on decentralized user devices and in datacenters. Decentralized machine learning methods offer computational scalability and data-locality, with recent theoretical results showing efficiency comparable to centralized approaches. Gradient compression techniques have been proposed for distributed training. Gradient compression techniques have been proposed for decentralized training of deep neural networks to reduce data communication, with algorithms like DCD and ECD introduced by Tang et al. (2018). These algorithms are restrictive in terms of compression operators used. CHOCO-SGD, introduced by Koloskova et al. (2019), is a gradient compression algorithm that overcomes restrictions on compression operators seen in DCD and ECD. It is evaluated for generalization performance on machine learning benchmarks, departing from previous work. CHOCO-SGD is evaluated for generalization performance on machine learning benchmarks, focusing on test-set performance rather than just training performance. It shows speed-ups over decentralized baseline in a challenging peer-to-peer setting. In a peer-to-peer setting, CHOCO-SGD demonstrates speed-ups over decentralized baseline with less communication overhead. In a datacenter setting, decentralized communication patterns improve scalability, especially for large tasks like ImageNet training. However, scaling decentralized algorithms to a larger number of nodes reveals challenges. Communication efficient CHOCO-SGD improves time-to-accuracy on large tasks like ImageNet training in a peer-to-peer setting. However, scaling decentralized algorithms to a larger number of nodes shows difficulties in reaching the same performance as centralized schemes. These findings highlight deficiencies in current decentralized training schemes and call for further research in this area. Reporting results on decentralized training schemes is valuable for spurring further research. CHOCO-SGD converges at rate O 1 / \u221a nT + n /(\u03c1 4 \u03b4 2 T ) on non-convex smooth functions, with n nodes, T iterations, \u03c1 spectral gap, and \u03b4 compression ratio. The main term matches centralized baselines with linear speedup in worker numbers, while \u03c1 and \u03b4 affect a smaller second term. CHOCO-SGD converges at rate O 1 / \u221a nT + n /(\u03c1 4 \u03b4 2 T ) on non-convex smooth functions with n nodes, T iterations, \u03c1 spectral gap, and \u03b4 compression ratio. The main term matches centralized baselines with linear speedup in worker numbers, while \u03c1 and \u03b4 affect a smaller second term. A version of CHOCO-SGD with momentum is presented for practical performance in on-device training over a peer-to-peer social network and in a datacenter setting for computational scalability of training deep learning models. Decentralized training methods, including gradient compression, have been proposed for communication-restricted settings. Performance of decentralized schemes when scaling to larger nodes is systematically investigated, highlighting shared difficulties. Decentralized learning approaches for training in communication-restricted settings include decentralized schemes, gradient compression, asynchronous methods, and multiple local SGD steps before averaging. This is particularly relevant for learning over decentralized data, as seen in federated learning literature. In this paper, the focus is on combining decentralized SGD schemes with gradient compression, specifically emphasizing approaches based on gossip averaging. The convergence rate of these approaches typically depends on the spectral gap of the mixing matrix. The text discusses decentralized SGD schemes combined with gradient compression, focusing on gossip averaging approaches. Convergence rates depend on the spectral gap of the mixing matrix, with recent studies showing consistent results. Communication compression with quantization is also mentioned. Recent studies have shown that mini-batch SGD and spectral gap mainly affect smaller terms. Communication compression with quantization, popular in deep learning, has theoretical guarantees for unbiased and biased compression schemes. Error correction schemes offer the best practical and theoretical results. Communication compression with quantization has been extended to biased compression schemes and error correction schemes, which offer the best practical and theoretical results. Proximal updates and variance reduction have also been studied in combination with quantized updates. Decentralized Optimization with Quantization has shown that gossip averaging can diverge in the presence of quantization noise. In decentralized optimization with quantization, gossip averaging can diverge due to quantization noise. Reisizadeh et al. (2018) proposed an algorithm that can still converge, albeit at a slower rate. Adaptive schemes with increasing compression accuracy have been suggested, leading to convergence at the expense of higher communication cost. Tang et al. (2018) introduced DCD and ECD for deep learning applications. The CHOCO-SGD algorithm can handle high compression rates and has been analyzed for convex functions. For non-convex functions, a rate of convergence is shown with \u03b4 > 0 measuring compression quality. Tang et al. (2019a) also introduced related work. The CHOCO-SGD algorithm introduced in (Koloskova et al., 2019) can handle high compression rates for arbitrary functions, including non-convex ones. A convergence rate is shown with \u03b4 > 0 measuring compression quality. In comparison, DeepSqueeze by Tang et al. (2019a) also converges with arbitrary compression ratio but CHOCO-SGD achieves higher test accuracy in experiments. The decentralized optimization problem and CHOCO-SGD algorithm (Koloskova et al., 2019) are introduced in this section. It discusses the distributed setup, communication constraints, and the gossip-based stochastic optimization algorithm for handling high compression rates and achieving higher test accuracy. The distributed setup involves optimization problems across n nodes with local data distributions and communication limited to local neighbors in a weighted graph. The weights are based on node degrees to facilitate message exchange. The distributed setup involves optimization across n nodes with local data distributions and communication limited to local neighbors in a weighted graph. Positive weights are assigned to edges based on node degrees to facilitate message exchange. Compression operators are used to transmit compressed messages. Compression operators, such as those used in CHOCO-SGD, allow for the transmission of quantized or sparsified messages without the need to be unbiased, supporting a wider range of compression schemes. Examples can be found in literature such as (Koloskova et al., 2019). CHOCO-SGD algorithm supports various compression operators, with specific schemes discussed in Section 5. Each worker stores a private variable x_i \u2208 R^d updated by stochastic gradient and gossip averaging steps. This preserves iterates' averages despite quantization noise, as errors are aggregated in local variables x_i. Nodes communicate with neighbors to update variables x_j \u2208 R^d. The nodes communicate with neighbors to update variables x_j using compressed updates, representing publicly available copies of private x_i. Communication and gradient computation can be executed in parallel, as they are independent. Each node only needs to update its local variables. The communication and gradient computation in the private x_i nodes can be executed in parallel. Each node only needs to store 3 vectors at most, regardless of the number of neighbors. A momentum-version of CHOCO-SGD is proposed in Algorithm 2. The analysis of CHOCO-SGD is extended to non-convex problems as a main contribution. The analysis of CHOCO-SGD is extended to non-convex problems, proposing a momentum-version in Algorithm 2. The convergence rate is determined by technical assumptions and bounded variance of stochastic gradients. The algorithm converges asymptotically with a linear speed-up compared to SGD on a single node. The convergence rate of CHOCO-SGD is denoted by \u03c1 2 \u03b4 82, showing linear speed-up compared to SGD on a single node. Compression and graph topology only affect the higher order second term. Experimental comparisons with relevant baselines for commonly used compression operators are conducted, leveraging momentum in all algorithms. The newly developed momentum version of CHOCO-SGD, Algorithm 2, includes weight decay factor \u03bb, momentum factor \u03b2, and local momentum memory. Experimental comparisons with relevant baselines for commonly used compression operators are conducted, leveraging momentum in all algorithms. In Algorithm 1, local momentum with weight decay is implemented for training ResNet20 on the Cifar10 dataset with a ring topology of 8 nodes. Training data is split between workers and shuffled after every epoch, following standard procedures. Various optimization algorithms with momentum are utilized for the experiments. The training data is split between workers and shuffled after every epoch. Various optimization algorithms with momentum are implemented, fine-tuning the initial learning rate and gradually warming it up. The learning rate is decayed twice and training stops at 300 epochs. Compression schemes are applied to every layer of ResNet20 separately, with two unbiased compression schemes implemented: gsgd b quantization and DeepSqueeze. The initial learning rate is fine-tuned and warmed up gradually, with decay at 150 and 225 epochs, and training stops at 300 epochs. Top-1 test accuracy is evaluated on every node separately, and the average performance over all nodes is reported. Hyper-parameter tuning details can be found in Appendix F. Compression schemes are applied to every layer of ResNet20 separately, with two unbiased compression schemes implemented: gsgd b quantization and random sparsification. Additionally, two biased compression schemes are used: top a and sign compression. The biased compression schemes top a and sign compression are applied to ResNet20 layers, along with unbiased compression schemes gsgd b quantization and random sparsification. The combination of DCD and ECD with biased schemes is not supported by theory. CHOCO-SGD and DeepSqueeze have only been studied with biased schemes. Results show that unbiased compression schemes ECD and DCD perform well at low compression ratios but struggle at higher ratios, aligning with theoretical and experimental findings. The combination of biased schemes with DCD and ECD is not supported by theory, while CHOCO-SGD and DeepSqueeze have only been studied with biased schemes. The results in Table 1 show that ECD and DCD perform well at low compression ratios but struggle at higher ratios, consistent with previous findings. DCD with biased top sparsification outperforms unbiased random counterpart, although not supported by theory. CHOCO-SGD generalizes well with minimal accuracy drop, while sign compression achieves high accuracy with significantly fewer bits per weight. CHOCO-SGD can generalize well with minimal accuracy drop and achieves state-of-the-art accuracy with significantly fewer bits per weight. It focuses on challenging real-world scenarios where training data is decentralized, making centralized methods inefficient. In decentralized scenarios, centralized methods are inefficient due to limited communication bandwidth and privacy concerns. Devices like sensor networks, mobile devices, or hospitals train machine learning models with locally stored data. Global network topology is unknown to individual devices, and the number of connected devices is typically large. This decentralized setting prioritizes privacy by keeping training data private on each device. In a fully decentralized setting, network topology is unknown to individual devices, and a large number of connected devices are involved. Privacy is a key motivation, ensuring training data remains private on each device. The training data is permanently split between nodes, with each node having a distinct part of the dataset. This scenario has not been extensively studied before in decentralized settings. In a fully decentralized setting, network topology is unknown to individual devices, and a large number of connected devices are involved. Privacy is a key motivation, ensuring training data remains private on each device. The training data is permanently split between nodes, with each node having a distinct part of the dataset. This scenario has not been extensively studied before in decentralized settings. No prior works have studied this scenario for decentralized deep learning. Comparisons are made to the centralized baseline where all nodes route their updates to a central coordinator for aggregation. In a fully decentralized setting, network topology is unknown to individual devices, and a large number of connected devices are involved. Privacy is a key motivation, ensuring training data remains private on each device. The training data is permanently split between nodes, with each node having a distinct part of the dataset. Comparisons are made to the centralized baseline where all nodes route their updates to a central coordinator for aggregation. CHOCO-SGD is compared to decentralized SGD without compression and centralized SGD without compression on different network topologies. To study CHOCO-SGD scaling properties, training was done on 4, 16, 36, and 64 nodes using ring and torus topologies. Parameters were compared in Table 2 (Krizhevsky, 2012). Learning rates were kept constant and tuned separately for all methods, with consensus learning rate tuned for CHOCO-SGD. Results showed CentralizedSGD outperforming CHOCO-SGD due to graph topology influence. The testing accuracy comparison after 300 epochs showed CentralizedSGD outperforming CHOCO-SGD due to graph topology influence. CHOCO-SGD slowed down due to graph topology and communication compression, affecting training performance. The slower convergence explained the performance degradation, not a generalization issue. Increasing epochs improved decentralized performance. In the real decentralized scenario, the goal is to reduce communication costs by fixing the number of transmitted bits to 1000 MB. Increasing epochs improves decentralized performance, but even with 10 times more epochs, the gap between centralized and decentralized algorithms in terms of train and test performance could not be fully closed. In the real decentralized scenario, the focus is on reducing communication costs by fixing the number of transmitted bits to 1000 MB. CHOCO-SGD shows the best testing accuracy, with slight degradation as the number of nodes increases. Torus topology is beneficial for large networks due to good mixing properties, while for small networks, there is not much difference between torus and other topologies. Experiments on a Real Social Network Graph involve training models on user devices connected by a social network with 32 nodes. Training a ResNet20 model on the Cifar10 dataset for image classification. Large spectral gap benefits are canceled by increased communication in torus topology for small networks. Decentralized and Centralized SGD require more bits for accuracy. Training models on user devices connected by a real social network. ResNet20 model trained on Cifar10 dataset for image classification and LSTM architecture for language modeling on WikiText-2. Results in Figures 2-3 and Table 3. The study utilized 600 training and 60 validation articles with a total of 2,088,628 and 217,646 tokens respectively. An exponentially decaying learning rate schedule was employed. Results are presented in Figures 2-3 and Table 3. The decentralized algorithm outperformed the centralized and quantized decentralized for training accuracy, but the centralized scheme had the highest test accuracy. CHOCO-SGD showed superior test accuracy compared to the exact decentralized scheme for the same transmitted data. CHOCO-SGD outperforms exact decentralized scheme in test accuracy for the same transmitted data, with a slight drop in accuracy compared to baselines. Decentralized schemes show a drop in training loss, while CHOCO-SGD outperforms centralized SGD in test perplexity for language modeling task. CHOCO-SGD outperforms centralized SGD in test perplexity for language modeling task, showing benefits in large-scale training with Resnet-50 on ImageNet-1k. The quantization scheme \"Sign+Norm\" further enhances performance when scaling to more nodes. Decentralized optimization methods, such as CHOCO-SGD, offer benefits for large-scale training with Resnet-50 on ImageNet-1k. The quantization scheme \"Sign+Norm\" improves performance when scaling to more nodes. Decentralized schemes can outperform centralized ones, as shown by Lian et al. (2017) and Assran et al. (2019) achieved impressive speedups for training on 256 GPUs. Assran et al. (2019) demonstrated significant speedups for training on 256 GPUs with decentralized schemes, utilizing asynchronous gossip updates and exact communication. Their approach differs from CHOCO-SGD with varying communication topology. The study utilized ImageNet-1k dataset with Resnet-50 architecture trained on 8 machines with Tesla P100 GPUs. Communication within machines was fast using all-reduce, while decentralized compressed communication (sign-CHOCO-SGD) in a ring topology was used between machines. Mini-batch size per GPU was 128, following general SGD principles. The study used Tesla P100 GPUs for training Resnet-50 on ImageNet-1k dataset. Communication within machines was fast with all-reduce, while between machines decentralized compressed communication (sign-CHOCO-SGD) in a ring topology was utilized. Mini-batch size per GPU was 128, following general SGD principles. CHOCO-SGD showed faster training time compared to all-reduce. CHOCO-SGD, a decentralized and parallel structure, outperforms all-reduce in terms of time efficiency, with only a slight 1.5% accuracy loss. Despite not heavily tuning the consensus stepsize, it achieves a 20% gain in training time compared to other methods. Our approach, CHOCO-SGD, offers a 20% time gain over common methods on commodity hardware clusters. It enables decentralized deep learning training in bandwidth-constrained environments and shows linear speedup with the number of nodes. Our approach, CHOCO-SGD, provides theoretical convergence guarantees for decentralized training in communication-restricted environments. It shows a linear speedup with the number of nodes and performs well on image classification and language modeling tasks. Our main contribution is enabling training in communication-restricted environments while respecting the constraint of locality of training data. Decentralized schemes show high communication compression and expand the reach of applications for fully decentralized deep learning. The proof of Theorem 4.1 is presented in this section. The proof of Theorem 4.1 is derived from a more general statement in Theorem A.2, analyzing CHOCO-SGD for various stepsizes \u03b7. The structure of the proof follows a previous study by Koloskova et al. (2019), showing Algorithm 1 as a special case of a broader class of algorithms. This algorithm involves stochastic gradient updates and averaging among nodes to demonstrate convergence. Algorithm 1 is a special case of a broader class of algorithms, as shown by Koloskova et al. (2019). It involves stochastic gradient updates and averaging among nodes for convergence. Linear convergence is achieved with the specific averaging used in CHOCO-SGD, as demonstrated in their study. Decentralized SGD with arbitrary averaging scheme achieves linear convergence with specific averaging in CHOCO-SGD, as shown by Koloskova et al. (2019). The algorithm assumes an averaging scheme that preserves iterates' average and converges linearly with a parameter 0 < c \u2264 1. Decentralized SGD with arbitrary averaging scheme achieves linear convergence with specific averaging in CHOCO-SGD. The algorithm assumes an averaging scheme that preserves iterates' average and converges linearly with a parameter 0 < c \u2264 1. Example: Exact Averaging algorithm converges at rate c = \u03c1, where \u03c1 is an eigengap of mixing matrix W. D-PSGD algorithm can be recovered by substituting into Algorithm 3. CHOCO-SGD can be recovered by choosing CHOCO-GOSSIP as consensus averaging scheme. Decentralized SGD achieves linear convergence with specific averaging in CHOCO-SGD. The algorithm assumes an averaging scheme that converges linearly with a parameter 0 < c \u2264 1. Exact Averaging algorithm converges at rate c = \u03c1, where \u03c1 is an eigengap of mixing matrix W. D-PSGD algorithm can be recovered by substituting into Algorithm 3. CHOCO-SGD can be recovered by choosing CHOCO-GOSSIP as consensus averaging scheme. Decentralized SGD achieves linear convergence with specific averaging in CHOCO-SGD. The algorithm assumes an averaging scheme that converges linearly with a parameter 0 < c \u2264 1. Exact Averaging algorithm converges at rate c = \u03c1, where \u03c1 is an eigengap of mixing matrix W. D-PSGD algorithm can be recovered by substituting into Algorithm 3. CHOCO-SGD can be recovered by choosing CHOCO-GOSSIP as consensus averaging scheme. The gradient computation part is exchanged with part 2 to show independence and parallel execution. Changing initial values does not affect convergence rate. The proof of Theorem 4.1 shows that the iterates of Algorithm 3 with constant stepsize \u03b7 satisfy certain conditions. The calculations in the proof are similar to those in a previous study. A recursion verifying a condition completes the proof. Theorem A.2 states conditions for averaged iterates under certain assumptions. The proof in Lemma 21 from Koloskova et al. (2019) is similar to the calculations in the proof of Theorem A.2. Under Assumptions 1-3, Algorithm 3 with constant stepsize \u03b7 achieves linear convergence. The convergence rate affects the second-order term, with a rate of O(1/\u221anT + n/(T\u03c1^2)) when using exact averaging with W. This recovers the rate of D-PSGD (Lian et al., 2017). CHOCO-SGD utilizes CHOCO-GOSSIP averaging for decentralized SGD. The convergence rate of CHOCO-SGD with CHOCO-GOSSIP averaging is affected by the second-order term, with a rate of O(1/\u221anT + n/(T\u03c1^2)). The dependence on \u03c1 (eigengap of W) may be due to our proof technique or high compression support. Theorem A.2 provides guarantees for the averaged vector of parameters x in a decentralized setting, where averaging all parameters across machines is expensive or impossible. The proof involves using L-smoothness to estimate terms and bounding the third term using Lemma A.1. The statement of the theorem is derived by rearranging terms and averaging over time. The theorem provides guarantees for the averaged parameter vector x in a decentralized setting, where averaging all parameters across machines is costly. Similar guarantees can be obtained for individual iterates xi. The convergence of local weights is proven under the same setting as the theorem, utilizing L-smoothness of f. The result holds for T > 64nL^2. Theorem A.4 states that Algorithm 3 converges at a certain speed under specific assumptions and step sizes. The convergence rate is determined by i and c, and is applicable for any T. However, the rate is not as good as in Theorem A.2 due to the difference in values between \u03c3^2 and G^2. Algorithm 3 converges at a speed determined by i and c, applicable for any T. The rate is not as good as in Theorem A.2 due to the difference in values between \u03c3^2 and G^2. Proof of Theorem A.4 involves L-smoothness and rearranging terms. Corollary A.5 shows convergence of local weights x (t) i under specific assumptions. Lemma B.1 provides further insights. Corollary A.5 states that algorithm 1 with \u03b7 = n T +1 converges at speed c 2 (T + 1) under specific assumptions. Lemma B.1 discusses arbitrary sets of n vectors, while Lemma B.3 pertains to two given vectors. Algorithm 4 CHOCO-SGD involves Error Feedback and mixing matrix W in stochastic gradient updates. Algorithm 4 CHOCO-SGD (Koloskova et al., 2019) combines Error Feedback and mixing matrix W in stochastic gradient updates. It can be interpreted as an error feedback algorithm and can be rewritten as Algorithm 4. Nesterov momentum can be adapted for decentralized settings. CHOCO-SGD is an error feedback algorithm that saves quantization errors into internal memory, which is then added to the compressed value at the next iteration. The algorithm transmits the difference x (t) i \u2212 x (t\u22121) i to represent the evolution of the local variable x i at step t. Internal memory is added and updated before compressing the value, correcting for errors in the process. In CHOCO-SGD, internal memory is used to correct errors before compressing the value. The algorithm transmits the difference x (t) i \u2212 x (t\u22121) i to represent the evolution of local variable x i at step t. Model training and hyper-parameter tuning procedures are detailed. Comparison is made with decentralized SGD and centralized SGD without compression, while training ResNet20 for image classification on the Cifar10 dataset. The study compared CHOCO-SGD with decentralized and centralized SGD without compression while training ResNet20 for image classification on the Cifar10 dataset and a three-layer LSTM architecture for language modeling on WikiText-2. For a language modeling task on WikiText-2, a three-layer LSTM with a hidden dimension of size 650 was used. The loss is averaged over all examples and timesteps, with a BPTT length set to 30. Gradient clipping (0.4) and dropout (0.4) are applied, and both ResNet20 and LSTM are trained for 300 epochs. The mini-batch size is 32 per node, and the learning rate of CHOCO-SGD follows a linear scaling rule. During training, the learning rate is gradually warmed up from 0.1 to the fine-tuned initial rate over 5 epochs. The initial learning rate is decayed by a factor of 10 at 50% of training. Momentum is only applied to ResNet20 training. During training, the learning rate is gradually warmed up from 0.1 to the fine-tuned initial rate over 5 epochs. The initial learning rate is decayed by a factor of 10 at 50% and 75% of training epochs, with optimal learning rate per sample determined by a linear scaling rule. The optimal learning rate is searched in a pre-defined grid to ensure best performance. The optimal learning rate is determined by a linear scaling rule and searched in a pre-defined grid to ensure best performance. Fine-tuned hyperparameters of CHOCO-SGD for training ResNet-20 on Cifar10 and LSTM on a social network topology are reported in tables. Table 5 displays the fine-tuned hyperparameters of CHOCO-SGD for training ResNet-20/LSTM on a social network topology with 32 nodes. The training data is split between nodes without shuffling, with a per node mini-batch size of 32 and maximum node degree of 14. Base learning rate and consensus stepsize are adjusted based on node degree. Learning curves are also plotted for different node configurations. The training data is split between nodes without shuffling, with a per node mini-batch size of 32 and maximum node degree of 14. Base learning rate and consensus stepsize are adjusted based on node degree. Learning curves are also plotted for different node configurations, including top-1, top-5 accuracy, and test accuracy. The local mini-batch size is 32, and training and test accuracy plots are provided for the datacenter experiment. The local models reach consensus towards the end of optimization, with their test performances matching the averaged model. Divergence from the averaged model occurs before decreasing the stepsize at epoch 225, as reported in previous studies. Before decreasing the stepsize at epoch 225, local models diverge from the averaged model, but align with it when the stepsize decreases. This behavior was also observed in a previous study by Assran et al. (2019)."
}