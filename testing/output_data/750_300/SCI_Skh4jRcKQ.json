{
    "title": "Skh4jRcKQ",
    "content": "Training activation quantized neural networks involves minimizing a piecewise constant training loss using a straight-through estimator (STE) in the backward pass. This paper provides a theoretical justification for the concept of STE by explaining why searching in its negative direction minimizes the training loss. In this paper, the theoretical justification for using the straight-through estimator (STE) in training activation quantized neural networks is provided. The STE, when properly chosen, yields a coarse gradient that correlates positively with the population gradient, making its negation a descent direction for minimizing the population loss. The choice of straight-through estimator (STE) in training activation quantized neural networks is crucial. Proper selection of STE ensures that the expected coarse gradient aligns with the population gradient, leading to a descent direction for minimizing population loss. Poor STE choice can result in training instability near local minima, as demonstrated in CIFAR-10 experiments. Deep neural networks (DNN) have achieved success in various machine learning applications. However, deploying DNNs requires significant memory and energy resources. The choice of straight-through estimator (STE) is crucial for training activation quantized neural networks to avoid instability near local minima, as shown in CIFAR-10 experiments. Recent efforts have focused on training coarsely quantized DNNs to achieve memory savings and energy efficiency during inference. This involves solving a challenging optimization problem of minimizing a nonconvex empirical risk function. Training fully quantized DNN involves minimizing a nonconvex empirical risk function subject to weight quantization constraints. Weight quantization of DNN has been extensively studied in the literature. The gradient in training activation quantized DNN is almost everywhere zero, making standard back-propagation challenging. The gradient in training activation quantized DNN is almost everywhere zero, making standard back-propagation challenging. To address this issue, a non-trivial search direction is constructed by modifying the chain rule, replacing zero derivatives with a surrogate known as the straight-through estimator (STE). The straight-through estimator (STE) is a surrogate used in the backward pass to replace zero derivatives of quantized activation functions in the chain rule. It was proposed by Bengio et al. (2013) as an alternative approach to stochastic neurons. Additionally, Friesen & Domingos (2017) introduced the feasible target propagation algorithm for learning hard-threshold networks via convex combinatorial optimization, with the idea of STE originating from the perceptron algorithm in the 1950s. The feasible target propagation algorithm for learning hard-threshold networks via convex combinatorial optimization was introduced by Lee et al. (2015), building on the idea of the straight-through estimator (STE) proposed by Bengio et al. (2013) as a surrogate for zero derivatives of quantized activation functions. The perceptron algorithm in the 1950s, which does not calculate the gradient through the standard chain rule, served as the basis for this approach. The derivative of the identity function is used as a proxy for the derivative of binary output functions in training multi-layer networks with binary activations. Different approaches have been proposed, such as using the derivative of the sigmoid function or the signum activation function in Deep Neural Networks (DNN) training. In training Deep Neural Networks (DNN), various approaches have been proposed for using derivatives of different activation functions. Saturated Straight-Through Estimator (STE) was introduced by Hubara et al. (2016) for DNN with weights and activations constrained to \u00b11. Later, STE was applied to DNN with general quantized ReLU activations by different researchers. Despite empirical success, there is limited theoretical understanding of STE in training. Despite empirical success, there is limited theoretical understanding of using derivatives of different activation functions in training Deep Neural Networks (DNN). Various proxies, including derivatives of vanilla ReLU and clipped ReLU, have been explored. Recent studies have considered scenarios where certain layers are not ideal for back-propagation, such as leaky ReLU activation. Wang et al. (2018) proposed an algorithm using the identity Straight-Through Estimator (STE) in the backward pass through the leaky ReLU layer. The Convertron algorithm utilizes the Straight-Through Estimator (STE) in the backward pass through the leaky ReLU layer. Other studies have also explored scenarios where certain layers are not ideal for back-propagation, proposing alternative approaches for improving DNN generalization accuracy and breaking adversarial defenses. The Convertron algorithm utilizes the Straight-Through Estimator (STE) to improve DNN generalization accuracy and break adversarial defenses. The backward pass differentiable approximation introduced by Athalye et al. successfully broke defenses at ICLR 2018. The \"gradient\" of loss function w.r.t. the weight variables through the STE-modified chain rule is referred to as coarse gradient, which is not the standard gradient descent. The coarse gradient obtained through the Straight-Through Estimator (STE) is not the standard gradient of the loss function. Understanding the optimization perspective and the choice of STE is crucial for training quantized ReLU nets. Three representative STEs are considered for learning a two-linear-layer network with binary activation and Gaussian data. The Straight-Through Estimator (STE) is crucial for training quantized ReLU nets. Three representative STEs are considered for learning a two-linear-layer network with binary activation and Gaussian data. Proper choices of STE lead to descent training algorithms for vanilla and clipped ReLUs. The Straight-Through Estimator (STE) is essential for training quantized ReLU nets. Proper choices of STE result in descent training algorithms for vanilla and clipped ReLUs, leading to monotonically decreasing energy in training. The identity STE can cause instability near certain local minima. Empirical performances of three STEs are examined on MNIST and CIFAR-10 classifications. The Straight-Through Estimator (STE) is crucial for training quantized ReLU nets. Empirical performances of three STEs on MNIST and CIFAR-10 classifications show that clipped ReLU STE is the best for deeper networks like VGG-11 and ResNet-20. Training with identity or ReLU STE can lead to instability at good minima, resulting in higher training loss and decreased generalization accuracy. In CIFAR experiments, training with identity or ReLU STE can be unstable, leading to higher training loss and decreased generalization accuracy. Poor STEs generate coarse gradients incompatible with the energy landscape, as shown in theoretical findings about the identity STE. Convergence guarantees for perceptron and Convertron algorithms were proved for the identity STE, with Convertron making weaker assumptions. The Convertron algorithm (Goel et al., 2018) has convergence guarantees for the identity STE, making weaker assumptions. However, these results do not apply to networks with two trainable layers. The identity STE is not ideal in this case, and it is unclear if the analyses can be extended to other STEs. The monotonicity of quantized activation functions, like in Convertron with leaky ReLU, is important for coarse gradient descent. In section 2, the energy landscape of a two-linear-layer network with binary activation and Gaussian data is studied. The main results and mathematical analysis for STE are presented in section 3. Empirical performances of different STEs in 2-bit and 4-bit activation quantization are compared in section 4. The importance of the monotonicity of quantized activation functions, similar to Convertron with leaky ReLU, is highlighted. The energy landscape of a two-linear-layer network with binary activation and Gaussian data is studied in section 2. Main results and mathematical analysis for STE are presented in section 3. Empirical performances of different STEs in 2-bit and 4-bit activation quantization are compared in section 4, highlighting the instability phenomena associated with poor STEs in CIFAR experiments. Technical proofs and figures are deferred to the appendix. The technical proofs and figures are deferred to the appendix. Notations include definitions for vectors, matrices, inner product, and Hadamard product. A model similar to (Du et al., 2018) is considered, with trainable weights w and v, and an activation function \u03c3 acting component-wise on the input Z. The model, inspired by Du et al. (2018), predicts DISPLAYFORM0 for input Z \u2208 R m\u00d7n using trainable weights w and v. The first layer acts as a convolutional layer with shared weight filter w, while the second layer serves as the classifier. Labels are generated by y*(Z) = (v*)\u03c3(Zw*) with squared sample loss DISPLAYFORM1. The model uses shared weight filter w for all patches and a linear classifier. Labels are generated by y*(Z) = (v*)\u03c3(Zw*) with a squared sample loss. The activation function \u03c3 is a binary function. Entries of Z are i.i.d. sampled from a Gaussian distribution. The learning task is cast as a population loss minimization problem. The learning task involves minimizing the population loss by using a shared weight filter w and a linear classifier. The sample loss is calculated using a Gaussian distribution for Z. The gradient of the objective function is not directly available for network training, so the expected sample gradient is used instead. The gradient of the objective function is not directly available for network training, so the expected sample gradient is used instead. The idea of Straight-Through Estimator (STE) is to replace the zero component \u03c3 with a non-trivial function \u00b5, which is the derivative of a (sub)differentiable function \u00b5. This surrogate of the gradient is used in back-propagation for training. The Straight-Through Estimator (STE) replaces the zero component \u03c3 with a non-trivial function \u00b5 for back-propagation in training. Using STE \u00b5 trains a two-linear-layer CNN with binary activation, leading to coarse gradient descent. Preliminaries about the population loss function f (v, w) landscape are discussed. The activation leads to coarse gradient descent for learning a two-linear-layer CNN with STE \u00b5. Preliminaries about the population loss function f (v, w) landscape are discussed, including the angle between w and w * and the analytic expressions of f (v, w) and \u2207f (v, w). The population loss function f (v, w) is analyzed, with expressions for f (v, w) and \u2207f (v, w) provided. The conditions for local minimizers of the model are discussed, including stationary points and non-differentiable points which are potential global minimizers. Saddle points are also mentioned as possible outcomes. The stationary points of the model can only be saddle points, while non-differentiable points are potential global minimizers. Spurious local minimizers are also discussed, with the population gradient shown to be Lipschitz continuous on bounded domains. The model has no saddle points or spurious local minimizers. The population gradient is Lipschitz continuous on bounded domains. The complex case with both saddle points and spurious local minimizers is of interest. The behavior of coarse gradient descent is studied with derivatives of vanilla and clipped ReLUs and the identity function. The study focuses on the behavior of coarse gradient descent in the presence of saddle points and spurious local minimizers. Algorithm 1 using the derivative of vanilla or clipped ReLU converges to a critical point, while the identity function does not. The study examines the convergence properties of Algorithm 1 with ReLU or clipped ReLU functions, showing convergence to a critical point with ReLU but not with the identity function. The convergence properties of Algorithm 1 with ReLU or clipped ReLU functions show convergence to a critical point with ReLU but not with the identity function. The convergence guarantee for coarse gradient descent is established under the assumption of infinite training samples, explaining the effectiveness of proper STE with large datasets in deep learning. The empirical loss decreases along the negative coarse gradient direction, gaining smoothness with increasing sample size. This explains why proper STE works well with large datasets in deep learning. The same results hold even if the Gaussian assumption on input data is relaxed to a rotation-invariant distribution. The mathematical analysis for the main results involves sample sizes of 10, 50, and 1000. Choosing the derivative of ReLU as the STE, the expected coarse gradient with respect to w is a key observation. The expected coarse gradient w.r.t. w has non-negative correlation with the population partial gradient, forming a descent direction for minimizing the population loss. If certain conditions are met, the inner product between the expected coarse and population gradients w.r.t. w is positive. The inner product between the expected coarse and population gradients w.r.t. w is positive under certain conditions, ensuring a descent direction for minimizing population loss. The estimate (12) guarantees the descent property of Algorithm 1 by ensuring monotonically decreasing energy until convergence. When Algorithm 1 using ReLU STE converges, it can only converge to a critical point of the population loss function. When Algorithm 1 using ReLU STE converges, it can only converge to a critical point of the population loss function. The coarse gradient using clipped ReLU STE generally has positive correlation with the true partial gradient of the population loss and vanishes at critical points. The coarse gradient using clipped ReLU STE has a positive correlation with the true partial gradient of the population loss and vanishes at critical points, occurring only at saddle points where specific conditions are satisfied during convergence of Algorithm 1. The coarse gradient using clipped ReLU STE has a positive correlation with the true partial gradient of the population loss and vanishes at critical points, occurring only at saddle points where specific conditions are satisfied during convergence of Algorithm 1. Moreover, Lemma 9 discusses the expected coarse partial gradient with respect to w, highlighting that Algorithm 1 may not converge at local minima due to the coarse gradient derived from the identity STE not vanishing. Lemma 9 states that the coarse gradient derived from the identity STE does not vanish at local minima, potentially preventing Algorithm 1 from converging. If certain conditions are met, the coarse gradient descent may never converge near spurious minimizers. Lemma 9 suggests that the coarse gradient descent may not converge near spurious minimizers. The theory implies that vanilla and clipped ReLUs have different performances on deeper nets. The training loss increases and instability arises due to the limitations of E Z [g id (v, w; Z)]. Empirical performances of vanilla and clipped ReLUs differ on deeper nets. A comparison of identity, ReLU, and clipped ReLU STEs on MNIST and CIFAR-10 benchmarks for 2-bit or 4-bit quantized activations is conducted. The clipped ReLU is expected to perform the best, as it closely approximates the original quantized ReLU. In experiments with 4-bit quantized activations, the clipped ReLU is expected to perform the best, closely approximating the original quantized ReLU. The resolution \u03b1 for quantized ReLU must be carefully chosen for full-precision level accuracy. A modified batch normalization layer is used to maintain unit Gaussian distribution in the output components. In experiments with 4-bit quantized activations, the resolution \u03b1 for quantized ReLU needs to be carefully chosen for full-precision level accuracy. A modified batch normalization layer is used to maintain unit Gaussian distribution in the output components, with \u03b1 pre-computed using a variant of Lloyd's algorithm. The original LeNet-5 is enhanced with batch normalization prior to each activation layer. The resolution \u03b1 for quantized ReLU is determined and fixed for the training process. Batch normalization is added before each activation layer in the modified LeNet-5. The quantization approach used is similar to HWGQ, with uniform quantization. Stochastic gradient descent with momentum = 0.9 is the optimizer used for all experiments. LeNet-5 is trained for 50 epochs on MNIST, while VGG-11 and ResNet-20 are trained for 200 epochs on CIFAR-10. Parameters/weights are initialized accordingly. The training process uses stochastic gradient descent with momentum = 0.9. LeNet-5 is trained for 50 epochs on MNIST, while VGG-11 and ResNet-20 are trained for 200 epochs on CIFAR-10. The learning rate schedule is specified in the appendix. Experimental results show that the derivative of clipped ReLU performs the best, followed by vanilla ReLU and then the identity function. Clipped ReLU is especially effective for deeper networks. In Table 1, training losses and validation accuracies are recorded for different STEs. Clipped ReLU shows the best performance overall, followed by vanilla ReLU and then the identity function. Clipped ReLU is particularly effective for deeper networks, while vanilla ReLU performs comparably on shallow networks like LeNet-5. The instability issue is demonstrated on ResNet-20 with 4-bit activations when using the identity STE. The study found that ReLU is effective for learning shallow CNNs, but the identity function leads to instability in ResNet-20. Gradient descent with clipped ReLU converges to better minima compared to vanilla ReLU, while the identity function performs poorly. The choice of STE does not affect the loss landscape, and training starts with improved minima using the identity function. The study found that ReLU is effective for learning shallow CNNs, but the identity function leads to instability in ResNet-20. Gradient descent with clipped ReLU converges to better minima compared to vanilla ReLU. The choice of STE does not affect the loss landscape, and training starts with improved minima using the identity function. Training with the identity STE results in a much worse minimum after switching to a normal learning rate schedule. Validation error increases significantly within the first 20 epochs, prompting a switch to a normal learning rate schedule at epoch 20. Training with identity STE leads to a worse minimum due to coarse gradients not vanishing at good minima. Similarly, ReLU STE performs poorly on 2-bit activated ResNet-20 due to instability at good minima. The activated ResNet-20 experiences instability during training, leading to divergence at good minima. Coarse gradient descent using identity STE results in being repelled from good minima. The concept of STE is theoretically justified as it gives rise to a descent training algorithm. Three STEs are considered for learning a two-linear-layer CNN with binary activation. The concept of Straight-Through Estimator (STE) was theoretically justified as it leads to a descent training algorithm. Three STEs were considered for learning a two-linear-layer CNN with binary activation. The negative expected coarse gradients based on vanilla and clipped ReLUs are descent directions for minimizing the population loss, while the identity STE generates incompatible coarse gradients. This instability issue was confirmed in CIFAR experiments. The concept of Straight-Through Estimator (STE) was theoretically justified as it leads to a descent training algorithm. The negative expected coarse gradients based on vanilla and clipped ReLUs are descent directions for minimizing the population loss, while the identity STE generates incompatible coarse gradients. The instability issue was confirmed in CIFAR experiments. In future work, further understanding of coarse gradient descent for large-scale optimization problems with intractable gradients is aimed for. When initialized with the weights produced by the clipped ReLU STE on ResNet-20 with 2-bit activations, the coarse gradient descent using the ReLU STE with a learning rate of 10^-5 is not stable, leading to an increase in classification and training errors. When initialized with weights from clipped ReLU STE on ResNet-20 with 2-bit activations, coarse gradient descent using ReLU STE with a learning rate of 10^-5 becomes unstable, causing an increase in classification and training errors. Lemma 11 discusses Gaussian random vectors and angles between nonzero vectors. Lemma 11 discusses Gaussian random vectors and angles between nonzero vectors, proving identities and properties related to them. Lemma 11 discusses Gaussian random vectors and angles between nonzero vectors, proving identities and properties related to them. In Lemma 12, a Gaussian random vector with entries sampled from N(0,1) is considered, along with nonzero vectors w, w in Rn with angle \u03b8. The identities Ezi1{zw>0, zw>0} = 0 for i \u2265 3 and Ezi1{0<zw<1} = 0 for i \u2265 3 are proven. Lemma 13 is then proved, involving inequalities and rearrangement inequalities. Lemma 13 involves inequalities and rearrangement inequalities, showing that p(\u03c0/2, w) \u2264 q(\u03c0/2, w) and sin(\u03c6)\u03be. The proof also includes the fact that sin(x) \u2265 2. Lemma 1 states that if w = 0n, the population loss f(v, w) is determined by certain equations involving vectors and angles. Lemma 2 further discusses partial gradients of f(v, w) with respect to v and w under specific conditions. The text discusses the partial gradients of the population loss function under specific conditions, showing equations involving vectors and angles. The second claim of Lemma 2 is demonstrated, leading to the derivation of certain equalities related to saddle points. The text demonstrates the local optimality of stationary points by analyzing the Hessian matrix and showing that they are saddle points. The objective function is rewritten and its Hessian matrix is shown to be indefinite, indicating saddle points at stationary points. An arbitrary point in the neighborhood of (v, \u03c0) is considered, and the perturbed objective value is analyzed to show local optimality. The unique minimizer to the quadratic function is proven to be optimal for small perturbations in the variables. Lipschitz constants are introduced to establish the validity of the claim. The Lipschitz constant L > 0 is crucial for validating the claim regarding the expected partial gradient of (v, w; Z) w.r.t. v and the expected coarse gradient w.r.t. w. Lemmas 14.1 and 14.2 play a significant role in proving these claims. Lemma 5 states that if w = 0n and \u03b8(w, w*) is between 0 and \u03c0, then the inner product between expected coarse and true gradients w.r.t. w is given. Additionally, if v \u2264 Cv and w \u2265 cw, there exists a constant Arelu > 0 depending on Cv and cw. Lemma 5 states that if v \u2264 Cv and w \u2265 cw, there exists a constant Arelu > 0 depending on Cv and cw. By Lemmas 2 and 4, it is shown that if w = 0n and \u03b8(w, w*) is between 0 and \u03c0, the inner product between expected coarse and true gradients w.r.t. w is given. Lemma 6 shows that under certain conditions, saddle points can be found. If w = 0n and \u03b8(w, w*) is between 0 and \u03c0, then certain expressions hold true. Additionally, if v \u2264 Cv and w \u2265 cw, a constant A crelu > 0 exists. Lemma 7 states that if certain conditions are met, then certain expressions hold true. Additionally, a constant A crelu > 0 exists under further conditions. Lemma 9 discusses the expected coarse partial gradient when \u00b5(x) = x in a given context. Lemma 8 is similar to Lemma 6, with q(\u03b8, w) being non-negative and equaling 0 at \u03b8 = 0, \u03c0, and p(0, w) \u2265 p(\u03b8, w) \u2265 p(\u03c0, w) = 0. Lemma 9 discusses the expected coarse partial gradient when \u00b5(x) = x."
}