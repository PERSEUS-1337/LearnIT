{
    "title": "HkxPtJh4YB",
    "content": "We introduce Sinkhorn variational marginal inference as a scalable alternative for exponential family inference over permutation matrices. This method is justified by the Sinkhorn approximation of the permanent, addressing the intractability issue as the permutation size increases. The effectiveness of this approach is demonstrated in the probabilistic identification of neurons in the worm C.elegans using binary matrices representing permutations of elements. We introduce a method for probabilistic identification of neurons in C.elegans using binary matrices representing permutations. The problem of marginal inference is addressed by computing the matrix of expectations, which is known to be intractable due to the computation of the permanent. To overcome this difficulty, we propose a scalable alternative using Sinkhorn variational marginal inference. The problem of marginal inference, computing the matrix of expectations \u03c1, is intractable due to the permanent of L. To address this, Sinkhorn variational marginal inference is introduced as an efficient and implementable solution, approximating \u03c1 as S(L), the Sinkhorn operator applied to L. The Sinkhorn operator S(L) is used for efficient marginal inference by normalizing the matrix of expectations \u03c1. The Sinkhorn approximation is shown to produce the best results for probabilistic inference of neural identity in C.elegans. The Sinkhorn operator S(L) efficiently normalizes the matrix of expectations \u03c1 for probabilistic inference of neural identity in C.elegans. The argument is based on the relation between marginal inference and the normalizing constant in exponential families, where the dual function coincides with the negative entropy of the exponential family. The dual function A * (\u00b5) is linked to the matrix of marginals \u03c1 L and the permanent Z L through an optimization problem. Approximating \u03c1 depends on the tightness of the approximation to Z L, which is achieved by replacing the variational representation of Z L with a more tractable optimization problem. The approximated \u03c1 is obtained by replacing the intractable dual function A*(\u00b5) with a component-wise entropy, resulting in the Sinkhorn permanent perm S(L). Bounds for this approximation are provided in the proposition. The Sinkhorn permanent perm S(L) is an approximation of the normalizing constant, with bounds provided in a proposition. This approximation has been proposed independently by Powell and Smith in 2019, but without a theoretical framework. The Bethe variational inference method is a general rationale for obtaining variational approximations in graphical models. The Bethe variational inference method is a general rationale for obtaining variational approximations in graphical models, approximating the dual function A*(\u00b5) as if the underlying Markov random field had a tree structure. This method has been successfully applied to permutations, computing the approximate marginal B(L) through belief propagation and offering better theoretical guarantees than the Sinkhorn. The Bethe variational inference method is used for variational approximations in graphical models, particularly for permutations. The approximate marginal B(L) is computed through belief propagation, with better theoretical guarantees than the Sinkhorn approximation. Computational differences exist, with Sinkhorn algorithms requiring row and column normalization, while the Bethe approximation involves more complex message computations. The Sinkhorn and Bethe algorithms have computational differences, with Sinkhorn requiring normalization while Bethe involves complex message computations. Bethe produces better permanent approximations in practice, as shown in Fig 1 (b). Comparisons with ground truth are possible in simple cases, but interestingly, discrepancies exist in many cases. The Sinkhorn approximation outperformed the Bethe approximation in producing better marginals, putting more mass on non-zero entries. Additionally, the Sinkhorn approximation scaled better for moderate n, with faster iteration times compared to Bethe. The Sinkhorn approximation showed better performance in producing marginals with more mass on non-zero entries and scaled better for moderate n compared to the Bethe approximation. For example, with n = 710, each Bethe iteration took 0.035 seconds on average, while each Sinkhorn iteration took only 0.0027 seconds. Examples of a true marginal matrix \u03c1 with Sinkhorn and Bethe approximation, including a histogram of the log permanent. Differences between approximate and true log permanent, as well as mean absolute errors of log marginals for the two approximations. Sampling-based methods can also be used for marginal inference, with sophisticated samplers proposed for polynomial approximability of the permanent. In section 3, an elementary MCMC sampler failed to produce sensible marginal inferences for the worm C.elegans due to its stereotypical nervous system with a fixed number of neurons and connections. Whole brain imaging has enabled advancements in neurotechnology for studying brain activity. Recent advances in neurotechnology have allowed for whole brain imaging of the stereotypical nervous system of the worm C.elegans, with a fixed number of neurons and connections. A technical challenge remains in identifying and assigning canonical labels to the volumetric images of worm neurons for studying brain activity and behavior. Our methodology involves probabilistic neural identification of worm neurons in volumetric images, specifically in the context of NeuroPAL, a multicolor C.elegans transgene designed for neural identification. We aim to estimate probabilities of neuron identities to provide uncertainty estimates for model predictions. In the context of NeuroPAL, a multicolor C.elegans transgene for neural identification, we estimate probabilities of neuron identities to provide uncertainty estimates for model predictions. Using a gaussian model for each canonical neuron, we infer parameters from annotated worms and consider a permutation for observed neurons. The likelihood of observing data is calculated assuming a flat prior over P. In the context of NeuroPAL, probabilities of neuron identities are estimated for uncertainty estimates in model predictions. Parameters are inferred from annotated worms, with a permutation for observed neurons. The likelihood of data is calculated with a flat prior over P, inducing a posterior over P. In NeuroPAL, a deterministic coloring scheme is used for all worms with neuron names labeled. A downstream task involves annotating neurons with uncertain neural identities to improve model accuracy. Human annotations lead to model updates and increased identification accuracy for remaining neurons with few annotations needed for high accuracy. The human annotates neurons in NeuroPAL to improve model accuracy, leading to increased identification accuracy with few annotations needed for high accuracy. Different approximation methods are compared, including Sinkhorn, Bethe, MCMC, random baseline, and naive baseline. Results are shown in Fig 3, with further details in the Appendix. The study compared different approximation methods for neuron annotation accuracy, including Sinkhorn, Bethe, MCMC, random baseline, and naive baseline. Results in Fig 3 show that Sinkhorn and Bethe approximations are similar, with Sinkhorn slightly better due to more accurate estimates of low probability marginals. The Sinkhorn and Bethe approximations show similar results, with Sinkhorn slightly better due to more accurate estimates of low probability marginals. MCMC does not outperform the naive baseline, indicating lack of convergence for chain lengths and comparable computational times to approximated methods. The Sinkhorn approximation is introduced as a sensible alternative to sampling, providing faster and simpler results. The Sinkhorn approximation is a faster and more accurate alternative to sampling, providing better marginal inference results than the Bethe approximation. Future work will analyze the relationship between permanent approximation quality and corresponding marginals. Additionally, S(L) = diag(x)Ldiag(y), where diag(x) and diag(y) are positive vectors turned into diagonal matrices. The relation between quality of permanent approximation and corresponding marginals is analyzed. S(L) = diag(x)Ldiag(y), where diag(x), diag(y) are positive vectors turned into diagonal matrices. The (log) Sinkhorn approximation of the permanent of L, perm S (L), is obtained by evaluating S(L) in the problem it solves. The dataset used consists of ten NeuroPAL worm heads with available human labels and a range of neurons from 180 to. The dataset used in the study consists of ten NeuroPAL worm heads with human labels and a range of neurons from 180 to 195. The log-likelihood matrix L was computed using the methods described in Yemini et al. (2019). Both Sinkhorn and Bethe approximations were used with 200 iterations, leading to the computation times described in Fig 1. The study used ten NeuroPAL worm heads with human labels and a range of neurons from 180 to 195. The log-likelihood matrix was computed using methods from Yemini et al. (2019). Both Sinkhorn and Bethe approximations were used with 200 iterations, leading to computation times described in Fig 1. The MCMC sampler method from Diaconis (2009) was used with 100 chains of length 1000. Results were obtained on a desktop computer with an Intel Xeon W-2125 processor. The Bethe approximation was implemented efficiently in log-space. The study used NeuroPAL worm heads with human labels and log-likelihood matrices computed using methods from Yemini et al. (2019). The message passing algorithm described in Vontobel (2013) was implemented efficiently in log-space. Multiple submatrices of size n were randomly drawn from log likelihood C.elegans matrices for analysis. Error bars were too small to be displayed."
}