{
    "title": "HkebMlrFPS",
    "content": "Most deep learning for NLP uses single-mode word embeddings, but we introduce multi-mode codebook embeddings for phrases and sentences to capture different semantic facets. These codebook embeddings act as cluster centers summarizing co-occurring words in a pre-trained word embedding space. Our neural model predicts these cluster centers directly from input text sequences. Our end-to-end trainable neural model predicts cluster centers from input text sequences, providing interpretable semantic representations and outperforming baselines on various NLP tasks. The curr_chunk discusses the strong performance of NLP models on various tasks using unsupervised learning techniques. These models learn representations from raw text data without supervision, such as word embeddings like word2vec and GloVe, sentence embeddings like skip-thoughts, and contextualized word embeddings like ELMo and BERT. These models use single embeddings to represent sentences or phrases and provide symmetric similarity measurements without annotations. Word sense induction methods and multi-mode word embeddings address the issue of single embeddings representing multiple senses or topics in sentences. Word sense induction methods and recent multi-mode word embeddings aim to represent target words as multiple points or regions in a semantic space by clustering neighboring words. For example, real property can refer to real estate in legal documents or a true characteristic in philosophical discussions. Real property can be observed in legal documents as real estate or as a true characteristic in philosophical discussions. Different from topic modeling like LDA, word sense induction methods cluster neighboring words to discover senses for target words. Extending multi-mode representations to phrases or sentences faces efficiency challenges due to the large number of unique phrases. Extending multi-mode representations to phrases or sentences faces efficiency challenges due to the large number of unique sequences, making clustering-based approaches difficult. The number of parameters required for clustering increases significantly with the number of unique sequences, posing time and space constraints. The previous work faces challenges with efficiency in clustering-based approaches for phrases or sentences due to the large number of unique sequences. Estimating and storing a large number of parameters takes time and space. Our compositional model learns to predict embeddings of cluster centers from the sequence of words in the target phrase to reconstruct the co-occurring distribution. Our compositional model aims to compress the embedding of observed co-occurring words into cluster centers, overcoming the challenge of sparseness in co-occurring statistics. This approach reduces the number of parameters needed to learn the compositional meaning of sequences without overfitting. Clustering approaches aim to learn the compositional meaning of sequences by compressing redundant parameters. A neural encoder and decoder are used to map target sequences to cluster centers, allowing for direct prediction during training. This approach overcomes the challenge of sparseness in co-occurring statistics. Instead of clustering co-occurring words beside a target sequence at test time, a mapping is learned between the target sequence and cluster centers during training. This allows for direct prediction of cluster centers using a single forward pass of the neural network for unseen input sequences. A nonnegative and sparse coefficient matrix is used to match the sequence of predicted cluster centers and observed co-occurring word embeddings during training. During training, a nonnegative and sparse coefficient matrix is used to match predicted cluster centers with observed co-occurring word embeddings. This allows for joint training of the model and better captures compositional meanings of words in unsupervised phrase similarity tasks compared to traditional methods. The proposed model captures compositional meanings of words in unsupervised phrase similarity tasks better than traditional methods. It can also measure asymmetric relations like hypernymy without supervision and outperforms single-mode alternatives in sentence representation. The training setup, objective function, and architecture of the prediction mode are formalized in subsequent sections. The approach for sentence representation is formalized in Section 2.1, 2.2, and 2.3, with an example shown in Figure 2. The model generates codebook embeddings to reconstruct co-occurring words, such as \"Music\". The model generates codebook embeddings to reconstruct co-occurring words like \"Music\" in sentences, using a sequence to embeddings model. The loss function ensures that common topics are not predicted by reconstructing negatively sampled words. The model uses codebook embeddings to reconstruct words in sentences, with a focus on common topics. Training signals differ for sentences and phrases, requiring separate models for each representation. The model uses codebook embeddings to reconstruct words in sentences, with a focus on common topics. For sentence representation, N t is the set of all words in the previous and the next sentence. Training signals differ for sentences and phrases, requiring separate models for each representation. The goal is to cluster words that could possibly occur beside a sequence, rather than the actual occurring words in the training corpus. The model focuses on predicting co-occurring words based on similar sequences, viewing them as a set rather than a sequence. It considers word order in the input sequence but ignores the order of co-occurring words. The distribution of co-occurring words is modeled in the work. The model focuses on predicting co-occurring words as a set rather than a sequence, considering word order in the input sequence but ignoring the order of co-occurring words. It models the distribution of co-occurring words in a pre-trained word embedding space. The model predicts cluster centers of input sequences using a neural network model with a fixed number of clusters. The reconstruction loss in k-means clustering in the word embedding space is discussed. In this work, the reconstruction loss of k-means clustering in the word embedding space is discussed. The model simplifies the design of the prediction model and its application to downstream tasks by using non-negative sparse coding to relax constraints on coefficient values. The use of non-negative sparse coding (NNSC) allows neural networks to generate cluster centers in an arbitrary order, promoting diversity in K cluster centers. This approach is smoother and easier to optimize compared to traditional k-means clustering, resulting in better capturing of conditional co-occurrence distribution. Cluster centers using the kmeans loss collapse to fewer modes, unable to capture co-occurrence distribution well. NNSC loss is smoother and easier for neural networks, promoting diversity in cluster centers. The reconstruction error is defined with a hyper-parameter \u03bb controlling sparsity. Coefficients are constrained to avoid learning small magnitude centers, ensuring stability. The reconstruction error in NNSC is defined with a hyper-parameter \u03bb controlling sparsity. Coefficients are constrained to avoid learning small magnitude centers for stability. The proposed loss efficiently minimizes L2 distance in a pre-trained embedding space, and M Ot can be estimated on the fly using convex optimization. The proposed loss efficiently minimizes L2 distance in a pre-trained embedding space, and M Ot can be estimated on the fly using convex optimization. To prevent the neural network from predicting the same global topics regardless of the input, the loss function for each sequence is defined using co-occurring words of a randomly sampled sequence. Our method extends Word2Vec to encode compositional meaning and decode multiple embeddings. The neural network architecture is similar to a seq2seq model, using the same encoder to transform input sequences into contextualized embeddings. Our neural network architecture, similar to a seq2seq model, utilizes an encoder to transform input sequences into contextualized embeddings. Unlike traditional seq2seq models, our decoder outputs a sequence of embeddings instead of words, allowing for predicting all codebook embeddings in a single pass while maintaining output dependency. Our decoder outputs a sequence of embeddings instead of words, allowing for predicting all codebook embeddings in a single pass while maintaining output dependency. The attention on contextualized word embeddings from the encoder is removed. The decoder outputs embeddings instead of words, predicting all codebook embeddings in one pass while maintaining output dependency. Attention on contextualized word embeddings from the encoder is removed, affecting sentence representation but not phrase representation. The framework is flexible. The attention connection between encoder and decoder is removed in the phrase representation evaluation. The framework allows for flexibility in replacing encoder and decoder architectures, including (bi-)LSTMs. Other input features can also be incorporated. Cluster centers predicted by the model are visualized to summarize the target sequence. The model visualizes cluster centers to summarize the target sequence, incorporating various input features and codebook embeddings to capture semantic facets. Codebook embeddings improve unsupervised semantic tasks, indirectly measuring topic quality. The codebook embeddings enhance unsupervised semantic tasks by utilizing classic metrics for global topic modeling. The model is trained on Wikipedia 2016 data with stop words removed, focusing on noun phrases for phrase experiments. Our models do not require additional resources like PPDB or multi-lingual resources, making them practical for domains with limited resources such as scientific literature. The number of dimensions in our transformers is set to match the GloVe embedding size. Our models, practical for domains with limited resources like scientific literature, use GloVe embedding size for transformer dimensions. Training on a single GPU, our models underfit data after a week due to small size. Comparing with BERT is challenging as it preserves more syntax information. BERT, trained on a masked language modeling loss, outperforms our models due to more parameters, larger corpus, and word piece model for out-of-vocabulary words. Despite this, we provide unsupervised performance comparisons based on cosine similarity. BERT outperforms models due to more parameters, larger corpus, and word piece model for out-of-vocabulary words. Unsupervised performances based on cosine similarity are provided as a reference. Semeval 2013 and Turney 2012 are standard benchmarks for evaluating phrase similarity, while BiRD and WikiSRS contain ground truth phrase similarities. The task of Semeval 2013 is to distinguish similar phrase pairs from dissimilar phrase pairs. Turney's goal is to identify the most similar unigram to a query bigram among candidate unigrams. BiRD and WikiSRS provide ground truth phrase similarities, while BiRD and WikiSRS-Rel measure relatedness and WikiSRS-Sim measures similarity. Our model evaluates two scoring functions for measuring phrase similarity. The first method computes cosine similarity between contextualized word embeddings from a transformer encoder, labeled as Ours Emb. Another method calculates the reconstruction error between normalized codebook embeddings of two phrases. Our model evaluates two scoring functions for measuring phrase similarity. The method labeled as Ours Emb computes cosine similarity between multi-facet embeddings of similar phrases. The reconstruction error between normalized codebook embeddings of two phrases is also calculated to determine similarity. Comparisons are made with 5 baselines including GloVe Avg and Word2Vec Avg, which compute cosine similarity between averaged word embeddings, and BERT CLS and BERT Avg, which calculate cosine similarities between CLS embeddings. Our models significantly outperform all baselines in measuring phrase similarity, including GloVe Avg, Word2Vec Avg, BERT CLS, BERT Avg, and FCT LM Emb. Performance comparisons are presented in Table 2, with SemEval 2013 and Turney having training and testing splits. The study compares the performance of different models in predicting co-occurring word embeddings, showing that their model outperforms all baselines. The results also highlight the effectiveness of non-linearly composing word embeddings and incorporating word order information in producing phrase embeddings. The study demonstrates the effectiveness of non-linearly composing word embeddings to predict co-occurring word embeddings. Results show that Ours (K=1) performs slightly better than Ours (K=10), supporting previous findings that multi-mode embeddings may not always improve performance. Despite this, Ours (K=10) still outperforms baselines, indicating that similarity performance is not highly sensitive to the number of clusters. The study shows that Ours (K=10) performs well compared to baselines in capturing polysemies, indicating that similarity performance is not affected by the number of clusters. STS benchmark is a widely used sentence similarity task where models predict semantic similarity scores between sentence pairs. The study compares different methods for sentence similarity using Pearson correlation coefficient. It includes BERT CLS, BERT Avg, GloVe Avg, word mover's distance (WMD), and cosine similarity between skip-thought embeddings (ST Cos). Additionally, it mentions a method proposed by Arora et al. (2017) to weight words in sentences. In addition to BERT CLS, BERT Avg, and GloVe Avg, the study compares methods like word mover's distance (WMD) and cosine similarity between skip-thought embeddings (ST Cos). Arora et al. (2017) propose weighting words in sentences based on a constant \u03b1 and word probability p(w). They suggest setting \u03b1 to 10^-4 in STS benchmark. Post-processing method from Arora et al. (2017) is used to remove the first principal component in GloVe SIF method. The study compares various methods for sentence representation, including BERT CLS, BERT Avg, GloVe Avg, word mover's distance (WMD), and cosine similarity between skip-thought embeddings (ST Cos). Post-processing in GloVe SIF involves removing the first principal component. The performance of average embedding suggests considering word embeddings in addition to sentence embeddings for measuring sentence similarity. The study compares different methods for sentence representation, such as BERT CLS, BERT Avg, GloVe Avg, WMD, and ST Cos. Average embedding indicates the importance of considering word embeddings along with sentence embeddings for measuring similarity. Multi-facet embeddings allow for estimating word importance by predicting co-occurring words. The study explores various methods for sentence representation, including BERT CLS, BERT Avg, GloVe Avg, WMD, and ST Cos. Multi-facet embeddings are used to estimate word importance by predicting co-occurring words. Importance weighting is calculated by computing cosine similarity between words and predicted codebook embeddings. This weighting is then multiplied with original GloVe weighting vectors to generate results. The study compares different methods for sentence representation, including BERT CLS, BERT Avg, GloVe Avg, WMD, and ST Cos. Importance weighting is calculated by predicting co-occurring words and multiplying it with original GloVe weighting vectors to generate results. Our SC outperforms WMD and BERT Avg, especially in STSB Low, demonstrating the benefits of multi-mode representation. The proposed attention weighting in our method boosts performance, especially in STSB Low, surpassing BERT Avg. A variant using bi-LSTM as encoder and LSTM as decoder performs worse than the transformer alternative. The variant using a bi-LSTM as the encoder and a LSTM as the decoder performs worse than the transformer alternative, with a higher average validation loss and inferior performance on Table 3. This variant outperforms ST Cos and is applied to HypeNet for unsupervised hypernymy detection. Our model applies an approach of ignoring the order of co-occurring words in the NNSC loss for unsupervised hypernymy detection on the HypeNet dataset. The predicted codebook embeddings of a hyponym often reconstruct the embeddings of its hypernym better, based on the assumption that co-occurring words are less related to some hyponyms. Our model utilizes a method that ignores the order of co-occurring words to detect hypernymy on the HypeNet dataset. The embeddings of hyponyms often reconstruct the embeddings of their hypernyms better. Our asymmetric scoring function outperforms baselines, showing improved accuracy in detecting hypernyms. The objective of a good summary is to cover multiple aspects representing all topics in the document. It involves discovering a summary with normalized embeddings that reconstruct the distribution of word embeddings in the document. The summary consists of sentences, and sentences are greedily selected to optimize the process. Our model generates multiple codebook embeddings to represent each sentence in the document, optimizing the selection of sentences for extractive summarization. The approach is compared with alternative methods for modeling aspects of the document. Our model utilizes multiple codebook embeddings to represent each sentence in the document, comparing it with alternative methods for sentence aspect modeling such as average word embeddings and embedding all words in the sentences. The method utilizes codebook embeddings to represent sentences in the document, avoiding issues with sentence length normalization. Results are compared with baselines of selecting random or first n sentences, showing performance on the testing set of CNN/Daily Mail using ROUGE metrics. The testing set of CNN/Daily Mail is compared using ROUGE metrics. Unsupervised methods like Lead-3 are strong baselines, while supervised methods like RL show state-of-the-art performance. The unsupervised method Lead-3 is a strong baseline with performances similar to supervised methods like RL. Larger cluster numbers K yield better results, with K = 100 giving the best performance after selecting 3 sentences. This shows that larger cluster numbers are desired in this application. In Table 5, higher cluster numbers (K) lead to better results, with K = 100 showing the best performance after selecting 3 sentences. Larger cluster numbers are preferred in this application due to improved efficiency. Topic modeling has been extensively studied and applied for its interpretability and flexibility in incorporating various input features. Neural networks have also been shown to discover semantically coherent information. Sparse coding on word embedding space is utilized to model multiple aspects of a word, while parameterizing word embeddings using neural networks is employed to test hypotheses efficiently. Sparse coding on word embedding space is used to model multiple aspects of a word, while parameterizing word embeddings with neural networks is employed for efficient hypothesis testing and storage saving. Representing words as single or multiple regions in Gaussian embeddings is utilized to capture asymmetric relations. However, challenges in extending these methods to longer sequences remain unaddressed. One challenge is designing a neural decoder for sets rather than sequences, requiring a matching step between sets and computing distance loss. Popular loss measures like Chamfer distance are used in auto-encoder models for point clouds, with more advanced matching loss options also proposed. The studies focus on measuring symmetric distances between ground truth and predicted sets, with the goal of reconstructing a set using fewer bases. Various methods for achieving permutation invariants loss for neural networks are discussed, including removing elements from the ground truth set that have been predicted. Our set decoder aims to reconstruct a set using fewer bases, different from predicting the set or sequence of observed instances. Various methods for achieving permutation invariants loss for neural networks are discussed, including predicting the permutation using different models like CNN, transformer, or reinforcement learning. Our goal is to efficiently predict clustering centers that can reconstruct observed instances. We use a neural encoder and decoder to model the meaning of sequences and predict codebook embeddings. During training, a non-negative sparse coefficient matrix is used to match predicted embeddings to observed instances. The proposed model uses a neural decoder to predict codebook embeddings representing sentences or phrases. It outperforms BERT, skip-thoughts, and GloVe in unsupervised benchmarks by learning interpretable clustering centers. The model predicts clustering centers for sequences, outperforming BERT, skip-thoughts, and GloVe in unsupervised benchmarks. Multi-facet embeddings work best for sequences with many aspects, while single-facet and multi-facet embeddings perform similarly for sequences with one aspect. Future work aims to create a single model for generating multi-facet embeddings for both phrases and sentences. In the future, the goal is to train a single model to generate multi-facet embeddings for both phrases and sentences, and evaluate it as a pre-trained embedding approach for supervised or semi-supervised settings. Additionally, the method will be applied to other unsupervised learning tasks relying on co-occurrence statistics. The model is kept simple to reach nearly converged training loss after 1 or 2 epochs due to computational resource constraints, without fine-tuning hyperparameters. The model is kept simple to reach a nearly converged training loss after 1 or 2 epochs due to computational resource constraints. The architecture details in the transformer are similar to BERT, with a smaller model size. Sparsity penalty weights are set at 0.4, sentence size at 50, and the maximum number of co-occurring words at 30. The penalty weights on the coefficient matrix \u03bb are set to 0.4. The maximal sentence size is 50, and sentences longer than that are ignored. The maximal number of co-occurring words is 30 after removing stop words. The number of dimensions in transformers is 300. For sentence representation, the number of transformer layers on the decoder side is 5 with a dropout on attention of 0.1 for K = 10, and 1 for K = 1. For phrase representation, the number of transformer layers on the decoder side is 2. The decoder side has a dropout on attention of 0.1 for K = 10, with 1 transformer layer for K = 1. For phrase representation, there are 2 transformer layers with a dropout on attention of 0.5 and a window size of 5. Hyperparameters are determined by validation loss, and the number of codebook embeddings K is chosen based on training data performance. The number of codebook embeddings in models is determined by validation loss, with K chosen based on training data performance. Performance is not sensitive to K as long as it is large enough. Larger K may lead to longer training time and potential performance drops. Skip-thoughts have a hidden embedding size of 600 and were retrained for 2 weeks in Wikipedia 2016 for fair comparison. The hidden embedding size for skip-thoughts is 600, retrained for 2 weeks in Wikipedia 2016 for fair comparison. BERT Large outperforms BERT Base in unsupervised semantic tasks. In comparison to BERT Base, BERT Large generally performs better in similarity tasks but worse in hypernym detection. Despite the performance gains of BERT in similarity tasks, the method used in the study outperforms BERT in most cases, especially in phrase similarity tasks. Increasing the model size may enhance BERT's performance, but the study's method remains superior. The study's method outperforms BERT in most cases, especially in phrase similarity tasks, possibly due to BERT's training method. Comparisons were made with other baselines using the same number of sentences. In Section 3.4, comparisons were made between different unsupervised summarization methods using the same number of sentences. The study found that their method significantly outperformed W Emb (GloVe) and Sent Emb (GloVe) when summaries were of similar length. The poor performance of W Emb (*) methods was attributed to the tendency of selecting shorter sentences. In Figure 3, Ours (K=100) outperforms W Emb (GloVe) and Sent Emb (GloVe) for summaries of similar length. W Emb (*) tends to perform better than Sent Emb (*) in this scenario, but this comparison may not be fair due to the ability of W Emb (*) to select more sentences. Preventing the selection of many short sentences may be preferable for fluency in extractive summarization. In practice, choosing many short sentences in extractive summarization may affect fluency. Our method (K=100) is best for summaries under 50 words, while W Emb (BERT) is optimal for longer summaries. Combining our method with BERT could yield mixed results. The figure suggests that Ours (K=100) is the best choice for summaries under 50 words, while W Emb (BERT) is more suitable for longer summaries. Combining our method with BERT could lead to improved performance in this task. The predicted embeddings from 10 randomly selected sentences in the validation set are visualized in a format similar to Table 1. The predicted embeddings from 10 randomly selected sentences in the validation set are visualized in a format similar to Table 1, showing the nearest five neighbors in a GloVe embedding space and their cosine similarities to the vector."
}