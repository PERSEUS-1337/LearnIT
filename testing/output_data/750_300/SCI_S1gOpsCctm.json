{
    "title": "S1gOpsCctm",
    "content": "Recurrent neural networks (RNNs) are effective for control policies in reinforcement and imitation learning. A new technique called Quantized Bottleneck Insertion is introduced to learn finite representations of RNN memory vectors and features, making them easier to analyze. Results show small finite representations in synthetic environments and Atari games. The technique of Quantized Bottleneck Insertion is used to create small finite representations of RNN memory in order to improve understanding and analysis. Results on synthetic environments and Atari games show surprisingly small memory states and observations needed for optimal performance in games like Pong. This approach also enhances interpretability of deep reinforcement learning and imitation learning policies. Deep reinforcement learning (RL) and imitation learning (IL) have shown impressive performance, but the learned policies, especially those using recurrent neural networks (RNNs), are hard to interpret. This paper introduces Quantized Bottleneck Insertion to create simpler representations of RNN memory, improving understanding and analysis. In this paper, the focus is on understanding and explaining RNN policies by simplifying memory representations. The challenge lies in interpreting high-dimensional continuous memory vectors updated through complex gating networks like LSTMs and GRUs. The hypothesis is that these continuous memories may actually be capturing discrete concepts, which if identified, could lead to better comprehension of RNN policies. The focus is on simplifying memory representations in RNN policies by quantizing the memory and observation representation to capture discrete concepts. This approach aims to enhance explainability by manipulating and analyzing the quantized system. Our main contribution is introducing a method to transform an RNN policy with continuous memory and observations into a finite-state representation called a Moore Machine using Quantized Bottleneck Network (QBN) insertion. QBNs are auto-encoders with quantized latent representations, encoding memory states and observation vectors encountered during training. The method introduces Quantized Bottleneck Network (QBN) insertion to transform an RNN policy into a Moore Machine Network (MMN) with quantized memory and observations, nearly equivalent to the original RNN. The MMN can be used directly or fine-tuned for improved accuracy. The combination of RNN and QBN results in a Moore Machine Network (MMN) with quantized memory and observations, almost equivalent to the original RNN. The MMN can be used directly or fine-tuned to improve inaccuracies introduced by QBN insertion. Training quantized networks is challenging, but a simple approach using \"straight through\" gradient estimators is effective. Experiments show accurate extraction of ground-truth MMNs in synthetic domains and benchmark grammar learning problems. The experiments demonstrate the effectiveness of \"straight through\" gradient estimators in extracting ground-truth MMNs in synthetic domains and benchmark grammar learning problems. Additionally, experiments on 6 Atari games using RNNs show near-equivalent MMNs can be extracted, providing insights into memory usage not apparent from observing RNN policies. In experiments, near-equivalent MMNs were extracted from RNNs in Atari games, revealing insights into memory usage. Some games showed RNNs using memory reactively, while others used observations in an open-loop manner. Previous efforts have been made to understand Recurrent Networks, but limited work exists on learning finite-memory. Our work is related to extracting Finite State Machines (FSMs) from recurrent networks trained to recognize languages. Previous efforts have focused on understanding Recurrent Networks, but there is limited work on learning finite-memory representations of continuous RNN policies. Our work is related to extracting Finite State Machines (FSMs) from recurrent networks trained to recognize languages. Previous efforts have focused on understanding Recurrent Networks, but there is limited work on learning finite-memory representations of continuous RNN policies. The history of work on extracting FSMs from recurrent networks involves discretizing memory space and using query-based learning algorithms. However, these approaches do not directly apply to learning policies, which require extending to Moore Machines. Our approach involves inserting discrete elements into the RNN to preserve its behavior while allowing for a finite state characterization. This method enables fine-tuning and visualization using standard learning frameworks, unlike other approaches that produce an FSM approximation separate from the RNN. Our approach involves inserting discrete elements into the RNN to allow for fine-tuning and visualization using standard learning frameworks. This extends previous work on learning FSMs and introduces the method of QBN insertion to transform pre-trained recurrent policies into a finite representation. Our work extends previous approaches by introducing QBN insertion to learn MMNs and transform pre-trained recurrent policies into a finite representation. Unlike prior work on fully binary networks, we focus on learning discrete representations of memory and observations while allowing arbitrary activations and weights in the network. Our work focuses on learning discrete representations of memory and observations in neural networks, prioritizing interpretability over efficiency. Recurrent neural networks (RNNs) are commonly used in reinforcement learning for policies requiring internal memory. Recurrent neural networks (RNNs) are utilized in reinforcement learning for policies that require internal memory. An RNN processes observations and outputs actions based on a continuous-valued hidden state, updating it at each time step. The operations include extracting observation features, determining actions through a policy function, and transitioning to a new state based on a transition function. Recurrent neural networks (RNNs) are used in reinforcement learning for policies that need internal memory. They process observations, output actions based on a continuous-valued hidden state, and transition to a new state. The goal is to extract compact quantized representations of the hidden state and observation features to better understand their role in memory. The goal is to extract compact quantized representations of hidden states and observations to understand their role in memory. Moore Machines are introduced as a way to capture key features of memory and observations through a finite system with labeled output values corresponding to actions. A Moore Machine is a finite system with hidden states, observations, actions, transition function, and policy. Moore Machine Networks represent the transition function and policy using deep networks. In this work, we use continuous and discrete states denoted as h t and \u0125 t, respectively. Moore Machine Networks (MMN) represent the transition function and policy with deep networks. MMNs also provide a mapping \u011d from continuous to discrete observations. The state and observation representations are quantized into discrete vectors. In this work, the transition function and policy are represented using Moore Machine Networks (MMN) with deep networks. MMNs map continuous observations to a finite discrete space using a deep network. The state and observation representations are quantized into discrete vectors, with each being a discrete vector describing the raw observation. The memory in MMNs is composed of k-level activation units, and environmental observations are transformed into a k-level representation before being fed to the recurrent module. Learning Moore Machine Networks (MMNs) from scratch can be challenging, especially for complex problems like Atari games. A new approach for learning MMNs is introduced, where the memory is restricted to k-level activation units and environmental observations are transformed before being fed to the recurrent module. Incorporating quantized units into the backpropagation process can help in learning MMNs via standard RNN algorithms. Learning MMNs from scratch can be difficult for complex problems like Atari games. A new approach involves using quantized bottleneck networks (QBNs) to embed continuous features into a k-level quantized representation. These QBNs are inserted into the original recurrent net with minimal behavior changes, allowing for fine-tuning after insertion. This method leverages the ability to learn RNNs and can be viewed as a more effective way to train highperforming MMNs. The text describes the process of embedding continuous features into a k-level quantized representation using quantized bottleneck networks (QBNs). These QBNs are inserted into the original recurrent net with minimal behavior changes, resulting in a network that consumes quantized features and maintains quantized state, effectively creating a Mixed-Memory Network (MMN). A Quantized Bottleneck Network (QBN) is an autoencoder with a constrained k-level activation unit bottleneck, aiming to discretize a continuous space. It involves a multilayer encoder E mapping inputs to a latent encoding E(x) and a decoder D. The QBN output is quantized using 3-level quantization of +1, 0, and -1. The QBN utilizes a multilayer encoder E to map inputs to a latent encoding E(x) and a decoder D. The output is quantized using 3-level quantization of +1, 0, and -1, with a specific activation function to support 3-valued quantization. The QBN uses a specific activation function to support 3-valued quantization, with a flatter region around zero input. The introduction of the quantize function results in non-differentiability, making it seemingly incompatible with backpropagation. The straight-through estimator is suggested as a way to address this issue. The straight-through estimator is effective in dealing with the non-differentiability issue caused by the quantize function in QBN. It treats the quantize function as the identity function during back-propagation, allowing for effective training of the autoencoder with a k-level encoding. The inclusion of the quantize function in the QBN allows for viewing the last layer of E as producing a k-level encoding. Training a QBN as an autoencoder with standard L2 reconstruction error for input x. Running a recurrent policy in the target environment generates training sequences of triples (o t , f t , h t ). Training two QBNs, b f and b h , on observed features and states F and H respectively. If low reconstruction error is achieved, the QBNs can be viewed as effective. The approach involves training two QBNs, b f and b h , on observed features and states F and H respectively. If low reconstruction error is achieved, the QBNs can be seen as high-quality encodings of the original hidden states and features. These QBNs are then inserted as \"wires\" into the original RNN to propagate input to output with some noise. The QBNs b f and b h are inserted as \"wires\" into the original RNN to propagate input to output with some noise, providing a quantized representation of features and states. The QBNs b f and b h are inserted into the RNN to provide a quantized representation of features and states. The resulting MMN may not behave identically to the original RNN, but fine-tuning can help improve performance. After inserting QBNs b f and b h into the RNN to create MMN, fine-tuning can improve performance by matching the softmax distribution of actions produced by the RNN. Visualization and analysis tools can be used to understand the memory and its feature bits. After fine-tuning the MMN created by inserting QBNs into the RNN, one can use visualization and analysis tools to understand the memory and its feature bits. Another approach is to use the MMN to generate a Moore Machine over discrete state and observation spaces for further analysis. The Moore Machine is created by running the learned MMN to produce consecutive pairs of quantized states, quantized features, and actions. The state-space corresponds to distinct quantized states, while the observation-space consists of unique quantized feature vectors. The transition function is constructed from the data to analyze the role of different machine states. The Moore Machine is minimized by applying standard techniques to reduce the number of states and observations, resulting in a minimal equivalent Moore Machine BID19. In this section, standard Moore Machine minimization techniques are applied to reduce the number of states and observations, resulting in the minimal 2 equivalent Moore Machine BID19. The experiments aim to extract MMNs from RNNs without significant performance loss, determine the number of states and observations in minimal machines, especially for complex domains like Atari, and assess if learned MMNs aid in interpreting recurrent policies. In this section, the focus is on exploring the magnitude of states and observations in minimal machines for complex domains like Atari. The study considers two domains with known ground truth Moore Machines: a synthetic environment called Mode Counter and benchmark grammar learning problems. The Mode Counter Environments (MCEs) allow for varying memory requirements in policies, including different types of memory usage and amounts needed for remembering past events. Mode Counter Environments (MCEs) are a type of Partially Observable Markov Decision Process that vary memory requirements for policies. MCEs transition between modes based on a distribution, with actions corresponding to each mode. Rewards are given at the end of episodes for correct actions. The Mode Counter Environments (MCEs) involve transitioning between modes based on a distribution, with actions corresponding to each mode. Rewards are given for correct actions at the end of episodes. Different parameterizations affect memory usage for inferring modes and achieving optimal performance in experiments. In Mode Counter Environments (MCEs), different parameterizations require varying memory usage to infer modes and achieve optimal performance. Three MCE instances test memory use: 1) Amnesia - optimal policy doesn't need memory, 2) Blind - observations don't inform optimal actions, memory is essential. In Mode Counter Environments (MCEs), different parameterizations require varying memory usage to infer modes and achieve optimal performance. Three MCE instances test memory use: 1) Amnesia - optimal policy doesn't need memory, 2) Blind - observations don't inform optimal actions, memory is essential. The optimal policy in MCEs can select actions based on current observations without needing memory. In contrast, the Blind MCE requires memory to implement counters for determining optimal actions based on a deterministic mode sequence. The Tracker MCE necessitates using both observations and memory to select optimal actions by implementing counters to track key time steps where observations provide information about the mode. In Mode Counter Environments (MCEs), memory is crucial for selecting optimal actions based on observations. The memory implements counters to track key time steps where observations provide mode information. A recurrent architecture with feed-forward and GRU layers is used, achieving 100% accuracy on imitation learning for all MCE instances with M = 4 modes. The observation and hidden-state QBN have the same architecture with varying bottleneck units. The encoders consist of tanh nodes feeding into quantized bottleneck nodes. In our experiments, we varied the number of quantized bottleneck units Bf and Bh. The encoders have a feed-forward layer of tanh nodes, with the number of nodes being 4 times the size of the bottleneck. Training QBNs in MCE environments was faster than RNN training, as QBNs do not need to learn temporal dependencies. We trained QBNs with bottleneck sizes of Bf \u2208 {4, 8} and Bh \u2208 {4, 8}, embedded them into the RNN, and measured the performance of the MMN. In experiments, varying bottleneck sizes Bf and Bh, QBNs trained faster than RNNs as they don't learn temporal dependencies. QBNs were embedded into RNNs to create MMNs, with fine-tuning improving performance in most cases. In experiments, varying bottleneck sizes Bf and Bh, QBNs trained faster than RNNs as they don't learn temporal dependencies. QBNs were embedded into RNNs to create MMNs, with fine-tuning improving performance in most cases. The cases where fine-tuning was not required resulted in perfect performance immediately after bottleneck insertion due to low reconstruction error. Fine-tuning in other cases, except for Tracker (B h = 4, B f = 4), yielded perfect MMN performance with 98% accuracy. Inserting one bottleneck at a time resulted in perfect performance, indicating that the combined error accumulation of the two bottlenecks reduced performance. Moore Machine Extraction also provided the number of states and observations of the MMs extracted from the MMNs before and after minimization. The combined error accumulation of the two bottlenecks is responsible for reduced performance. TAB0 provides the number of states and observations of the MMs extracted from the MMNs before and after minimization, showing that there are typically more states and observations before minimization than after. This suggests that MMN learning does not always result in minimal discrete state and observation representations, although they accurately describe the RNN. After minimization, exact minimal machines were obtained for each MCE domain, except for one case where perfect accuracy was not achieved. The MMNs learned via QBN insertions were found to be equivalent to the true minimal machines, demonstrating their optimality in most cases. The ground truth minimal machines are shown in the Appendix, confirming the accuracy of the representations. The MMNs learned via QBN insertions were found to be equivalent to the true minimal machines, demonstrating their optimality in most cases. The machines for Blind and Amnesia illustrate different memory use, with Blind having transitions independent of input observations and Amnesia's actions solely determined by the current observation. The machine for Amnesia shows that each observation symbol leads to the same action choice, determined solely by the current observation. Evaluating the approach over 7 Tomita Grammars, each defining binary string acceptance/rejection sets, treated as environments with 'accept' and 'reject' actions. Rewards are given based on correct action choices on random strings. The RNN for each grammar consists of a one-layer GRU with 10 hidden units and a fully connected softmax layer with 2 nodes. Imitation learning is used to train each RNN with an Adam optimizer and learning rate of 0.001. Training data includes an equal number of accept/reject strings with lengths in the range [1, 50]. The RNNs were trained using imitation learning with an Adam optimizer and learning rate of 0.001. The test results showed high accuracy, except for grammar #6. For MMN training, a bottleneck encoder was not needed due to the finite alphabet observations. Only the hidden memory state bottleneck was learned. The RNNs achieved 100% accuracy, and for MMN training, a bottleneck encoder was not required. The only bottleneck learned was for the hidden memory state. Experiments were conducted with different bottleneck sizes, and MMNs were created by inserting these bottlenecks into the RNNs. Fine-tuning only provided minor improvements in cases already achieving high accuracy. The MMNs maintained RNN performance without fine-tuning, with only minor improvements in some cases. MM extraction and minimization resulted in reduced state-space while preserving performance. MMN learning does not directly lead to minimal machines but is equivalent to them. The MMNs maintained RNN performance without fine-tuning, with only minor improvements in some cases. MM extraction and minimization resulted in reduced state-space while preserving performance. MMN learning does not directly lead to minimal machines but is equivalent to them. In this section, our technique was applied to RNNs learned for six Atari games using the OpenAI gym, where the ground truth minimal machines were unknown. In this section, the technique was applied to RNNs trained for six Atari games using OpenAI gym. Unlike previous experiments with known ground truth minimal machines, for Atari, the complexity of input observations (images) made it unclear if similar results could be expected. While other efforts have been made to understand Atari agents, this work aimed to extract finite state representations for Atari policies with the same recurrent architecture for all agents. The study focused on extracting finite state representations for Atari policies using RNNs with a common recurrent architecture. The Atari agents all had the same network structure with specific preprocessing steps for input observations. The network used in the study has 4 convolutional layers with specific parameters, followed by a GRU layer and a fully connected layer. The A3C RL algorithm was utilized for training with specific settings, and the RNN performance on six games was reported. The study utilized the A3C RL algorithm to predict the value function and computed loss on the policy using Generalized Advantage Estimation. The trained RNN performance on six games was reported, with adjustments made to the encoder input and decoder output sizes for the QBN b f architecture. Training data for b f and b h in Atari domains was generated using noisy rollouts. The encoder and decoder architecture for the continuous observation features in the Atari domains utilized 3 feed-forward layers with varying numbers of nodes. Training data was generated using noisy rollouts to increase diversity, and bottlenecks were trained for specific values to robustly learn the QBNs. The RNN policy was trained with bottlenecks for specific values to increase diversity in the training data and robustly learn the QBNs for Atari games. The performance of the trained MMNs before and after finetuning is shown in TAB2. The number of MMN states can be smaller, trained bottlenecks were inserted into the RNN for each Atari game. MMNs performance was evaluated before and after fine-tuning, showing the ability to learn a discrete representation and memory for complex games without impacting performance. Fine-tuning was required for Boxing and Pong to match RNN performance. The MMNs learned a discrete representation and memory for complex games without affecting performance. Fine-tuning was needed for Boxing and Pong to match RNN performance, while Freeway and Bowling did not require it. Breakout and Space Invaders MMNs achieved lower scores due to poor reconstruction in some parts of the game. After fine-tuning, MMNs achieved lower scores in Breakout and Space Invaders due to poor reconstruction in rare game states, such as failing to press the fire-button in Breakout. This highlights the need for more intelligent approaches to train QBNs to capture critical information in such states. Before minimization, MMs had large numbers of discrete states and observations. After fine-tuning, MMNs achieved lower scores in Breakout and Space Invaders due to poor reconstruction in rare game states. Minimization of MMNs drastically reduces the number of states and observations, making them easier to analyze by hand. However, this analysis is likely to be non-trivial. The number of states and observations in MMNs drastically reduces, making them easier to analyze by hand. Memory use in Atari shows three types of memory use, with Pong having just three states and 10 observation symbols. In Atari, memory use is observed in three types, with Pong having three states and 10 observation symbols. The Pong policy maps individual observations to actions without the need for memory. Bowling and Freeway have only one observation symbol in the minimal MM, ignoring input images for action selection. The Pong policy in Atari does not require memory, similar to the Amnesia MCE. In contrast, Bowling and Freeway policies are open-loop controllers that ignore input images for action selection, similar to the Blind MCE. Freeway's policy always takes the Up action, which was also discovered through MM extraction. The MM extraction approach revealed that Freeway's policy always takes the Up action, similar to the Blind MCE. Bowling has a more complex open-loop policy structure with an initial sequence of actions followed by a repeated loop. This approach provides valuable insights into the policies of Breakout, Space Invaders, and Boxing. The MM extraction approach provides additional insight into the policies of Breakout, Space Invaders, and Boxing by analyzing memory and observations. Future work will involve semantic analysis and visualization tools to better understand memory use in RNN policies. Our approach involves extracting finite state Moore Machines from Atari policies by training Quantized Bottleneck Networks to produce binary encodings of RNN memory and input features. This allows for the transformation of a discrete Moore machine into an equivalent minimal machine for analysis and usage. Our approach involves extracting finite state Moore Machines from Atari policies by training Quantized Bottleneck Networks to produce binary encodings of RNN memory and input features. This yields a BID15, A3C BID16 from the MMN, which can be transformed into an equivalent minimal machine for analysis and usage. Results show accurate extraction of ground truth in known environments and similar performance to original RNN policies in Atari games, providing insight into memory usage. The learned MMNs maintain similar performance to the original RNN policies in six Atari games, providing insight into memory usage. The number of required memory states and observations is small, with some policies not using memory significantly (e.g. Pong) and others relying solely on memory (e.g. Bowling and Freeway). This work offers new insights into policies in complex domains. Future work aims to develop tools for further analysis. The first work to report insights on policies in complex domains, focusing on memory usage and observations. Future work includes developing tools for attaching meaning to discrete observations and analyzing finite-state machine structure for further insight. The MCE is parameterized by mode number M, transition function P, lifespan mapping \u2206(m), and count set C. The hidden state is (m t, c t), where m t is the current mode and c t is the consecutive time-steps in that mode. Mode changes when lifespan is reached, i.e. c t = \u2206(m t) - 1, generating the next mode m t+1 according to transition distribution P. The system operates in different modes (m t) with consecutive time-steps (c t) in each mode. Mode changes occur when the lifespan is reached, generating the next mode based on a transition distribution. Observations (o t) are received at each step, determined by the current state (m t, c t). The agent receives continuous-valued observations o t \u2208 [0, 1] at each step based on the current state (m t, c t). Observations determine the mode when the mode count is in C, otherwise, they are uninformative. The agent must remember the current mode and use memory to keep track of how long the mode has been active for optimal performance. The agent must remember the current mode and use memory to track how long it has been active for optimal performance. Experiments are conducted with three MCE instances: 1) Amnesia, where an optimal policy does not use memory to track past information; 2) Blind, with deterministic initial mode. The current policy tests the ability to determine if it is purely reactive and not using memory. It also examines if the policy is ignoring observations and only using memory for optimal performance. The current policy tests if optimal performance is achieved by using memory to track the mode sequence. The MCE Tracker requires attention to observations and memory usage. The environment becomes challenging as the number of modes and their life-spans increase. The environment becomes challenging as the number of modes and their life-spans increase, with machines being 100% accurate except for Grammar 6."
}