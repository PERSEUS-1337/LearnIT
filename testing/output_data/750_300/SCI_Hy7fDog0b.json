{
    "title": "Hy7fDog0b",
    "content": "Generative models are useful for modeling complex distributions, but current training techniques require fully-observed samples. In scenarios where obtaining full samples is costly or impossible, learning implicit generative models from partial, noisy observations is more practical. A new method is proposed to train generative models using lossy measurements, showing that the underlying distribution can be recovered even with per-sample information loss. The true underlying distribution can be recovered even with per-sample information loss using lossy measurements. A new method called AmbientGAN is proposed for training Generative Adversarial Networks (GANs), showing substantial improvements in qualitative and quantitative results on benchmark datasets. The generator's output is passed through a simulated random measurement function, and the discriminator distinguishes between real and measured samples. Generative models trained with a new method can achieve significantly higher inception scores than baselines. The generator's output is evaluated by a simulated random measurement function, and the discriminator distinguishes between real and generated samples. This work addresses the challenge of training generative models with limited data by using noisy or incomplete samples. This work addresses the challenge of training generative models with limited data by directly training from noisy or incomplete samples. The approach allows for learning a generative model from different types of measurements, with the critical assumption that the measurement process is known and meets specific conditions. The approach presented in the curr_chunk involves training generative models using a new method called AmbientGAN, where the discriminator distinguishes real measurements from simulated ones. This method has been shown to be effective in constructing generative models from limited data and various measurement models. The discriminator in the curr_chunk distinguishes real measurements from simulated ones in the context of training generative models using AmbientGAN. The method is effective in constructing generative models from noisy observations and low-dimensional projections with information loss. The approach is validated through qualitative and quantitative comparisons with baseline methods. The curr_chunk discusses the theoretical results of measurements that are noisy, blurred versions of desired images, showing that the distribution of measured images uniquely determines the distribution of original images in the context of training generative models using AmbientGAN. The distribution of measured images uniquely determines the distribution of original images in the context of training generative models using AmbientGAN. This implies that a pure Nash equilibrium for the GAN game must find a generative model that matches the true distribution. Results are also shown for dropout and random projection measurement models. In FIG1, the celebA dataset of celebrity faces is considered under randomly placed occlusions. In FIG2, learning from noisy, blurred images is discussed. Incorporating the measurement process into GAN training improves sample quality. Incorporating the measurement process into GAN training improves sample quality by learning from noisy, blurred images. The use of Wiener deconvolution for denoising leads to poor sample quality, while models trained with the measurement process produce cleaner samples. Incorporating the measurement process into GAN training improves sample quality by learning from noisy, blurred images. A GAN on images denoised by Wiener deconvolution leads to poor sample quality while models trained with the measurement process produce cleaner samples. Learning a generative model on 2D images in the MNIST dataset from pairs of 1D projections shows that AmbientGAN recovers a lot of the underlying structure. AmbientGAN is able to recover underlying structure in generative models, with two variants considered: one forgetting the choice of line and the other including it. The adversarial framework is powerful for modeling complex data distributions like images, videos, and 3D models. The adversarial framework is powerful for modeling complex data distributions such as images, videos, and 3D models. Generative models have various applications, including solving inverse problems and translating images between domains using GANs. Operating generators and discriminators on different spaces has been proposed before. The use of GANs in generating realistic synthetic data and translating images between domains has been explored in various studies. Training stability can be improved by operating generators against discriminators on different low-dimensional projections of the data. Additionally, there are connections to creating 3D object shapes from 2D projections in related works. Our work involves using an array of discriminators operating on different low-dimensional data projections to enhance stability. It is related to creating 3D object shapes from 2D projections in previous studies. The setup involves real and generated distributions denoted by superscripts 'r' and 'g', respectively, with 'x' for the underlying space and 'y' for measurements. Lossy measurements are observed on samples from the real underlying distribution. The text discusses the use of discriminators on low-dimensional data projections to improve stability. It introduces real and generated distributions denoted by 'r' and 'g', with 'x' for the underlying space and 'y' for measurements. Lossy measurements are observed on samples from the real distribution, with measurement functions parameterized by \u03b8. The text discusses using stochastic parameters for measurement functions to create an implicit generative model of a known distribution. It involves sampling from distributions p\u03b8 and prx to generate measurements y, with the goal of creating a model for the unknown distribution prx using observed realizations from pry. The text introduces the use of a stochastic generative model to create a representation of an unknown distribution prx, by combining measurement functions with adversarial training. The goal is to learn a generator G that can produce samples close to the distribution prx. In a GAN setting, a random latent vector Z is used to learn a generator G that produces samples close to the distribution prx. Instead of having access to desired objects, only a dataset of measurements is available. Random measurements are simulated on generated objects Xg to distinguish real from fake measurements using a discriminator. The AmbientGAN approach involves simulating random measurements on generated objects Xg and using a discriminator to distinguish real measurements from fake ones. The discriminator predicts if a measurement is from the real distribution pry or the generated distribution pgy. The quality function q(x) is used to define the objective, with different functions used for vanilla GAN and Wasserstein GAN. The AmbientGAN objective involves using a discriminator to distinguish real measurements from fake ones by simulating random measurements on generated objects. The quality function q(x) defines the objective, with different functions used for vanilla GAN and Wasserstein GAN. The model is end-to-end differentiable and can be trained using a gradient-based GAN training procedure. Our model is end-to-end differentiable and can be trained using a gradient-based GAN training procedure. We sample Z, \u0398, and Y r to compute stochastic gradients for parameters in G and D. Updates alternate between D and G parameters. The approach is compatible with GAN improvements and can easily incorporate additional information like per sample labels. Our approach is compatible with GAN improvements and can easily incorporate additional information like per sample labels. In our experiments, we use different versions of GAN models. We focus on 2D images for our theoretical and empirical results. In our experiments, we utilize various GAN models including unconditional Wasserstein GAN with gradient penalty and an Auxiliary Classifier Wasserstein GAN with gradient penalty. Our measurement models are tailored for 2D images, but the AmbientGAN framework is versatile for other data formats and measurement models. The measurement models considered include Block-Pixels and Convolve+Noise. In this section, various measurement models for 2D images are discussed. These models include Block-Pixels, Convolve+Noise, Block-Patch, Keep-Patch, Extract-Patch, and Pad-Rotate-Project. Each model has different ways of manipulating the input image for measurement purposes. The measurement models for 2D images discussed include Block-Pixels, Convolve+Noise, Block-Patch, Keep-Patch, Extract-Patch, Pad-Rotate-Project, and PadRotate-Project-\u03b8. These models manipulate the input image in various ways for measurement purposes, such as setting pixels outside a randomly chosen patch to zero, extracting a random patch, rotating the image, and summing pixels along the vertical axis. In image measurement models, various techniques like PadRotate-Project-\u03b8 and Gaussian-Projection are used to manipulate images for measurement purposes. These methods aim to recover the true underlying distribution of the observed measurements. The text discusses the recovery of the true underlying distribution of observed measurements in image measurement models. It highlights the uniqueness of the distribution consistent with the observed measurement distribution, even though the mapping from individual images to measurements is not invertible. The lemma presented in Section 5.1 provides a consistency guarantee with the AmbientGAN training procedure. Lemma 5.1 provides a consistency guarantee with the AmbientGAN training procedure by assuming uniqueness of the true underlying distribution given the measurement distribution. The vanilla GAN model BID11 DISPLAYFORM0 proves that the true underlying distribution can be recovered with the AmbientGAN framework under various measurement models like Gaussian-Projection, Convolve+Noise, and Block-Pixels. This is especially true for Gaussian blurring kernel with additive Gaussian noise. The AmbientGAN framework can recover the true underlying distribution under different measurement models like Gaussian blurring kernel with additive Gaussian noise. The framework also provides a sample complexity result for learning distributions with a finite discrete set of pixel values. The AmbientGAN framework can recover the true underlying distribution under various measurement models for images represented with a finite number of discrete values per channel. The sample complexity result for learning distributions in this framework is provided, along with a unique distribution that can induce the measurement distribution under certain conditions. The study used three datasets for experiments: MNIST for handwritten digits, CelebA for celebrity face images, and CIFAR-10 for images from different classes. The dataset used for experiments includes MNIST for handwritten digits, CelebA for celebrity face images, and CIFAR-10 for images from different classes. The generative models used for the experiments are a conditional DCGAN and an unconditional Wasserstein GAN with gradient penalty for MNIST dataset, and details on architectures and hyperparameters can be found in the appendix. For the MNIST dataset, two GAN models are used: a conditional DCGAN and an unconditional Wasserstein GAN with gradient penalty. For the celebA dataset, an unconditional DCGAN is used. The CIFAR-10 dataset utilizes an Auxiliary Classifier Wasserstein GAN with gradient penalty. The discriminator architectures remain the same for 2D outputs, while 1D projections have their own specifications. The penalty (ACWGANGP) in BID12 4 follows the residual architecture. Different discriminator architectures are used for 2D and 1D outputs. Baseline approaches were implemented to evaluate AmbientGAN framework performance on datasets with IID samples. Baseline approaches were implemented to evaluate the performance of the AmbientGAN framework on datasets with IID samples. One approach is to create a generative model for the measurement distribution without considering any measurements were taken. Another approach involves learning a generative model directly on the measurements to approximate the true distribution. In the AmbientGAN setting, a baseline approach involves learning a generative model directly on the measurements to approximate the true distribution p r x. This approach considers the invertibility of measurement functions and the observation of \u03b8 i for each measurement y i in the dataset. In the AmbientGAN setting, the baseline approach involves learning a generative model directly on the measurements to approximate the true distribution p r x. However, this approach faces challenges as it assumes the observability of \u03b8 i and the invertibility of measurement functions, which are violated in practice. To address this, an approximation of an inverse function is used to \"unmeasure\" the data and train a generative model with the estimated inverse samples. To address challenges in training a generative model directly on measurements, approximate inverse functions are used to \"unmeasure\" the data and train the model with estimated samples. Methods include blurring images for Block-Pixels measurements and using total variation inpainting, as well as approximating Convolve+Noise measurements with Gaussian kernel and additive noise. The text discusses different approaches to fill in zero pixels in images for various measurement models, including using total variation inpainting and Wiener deconvolution for Convolve+Noise measurements. It also mentions challenges in obtaining an approximate inverse function for other measurement models like Keep-Patch and Extract-Patch. Inverting measurement models like Keep-Patch and Extract-Patch poses challenges as information about the position of the patch is lost. Conventional techniques for inverting Pad-Rotate-Project-\u03b8 measurements involve sampling many angles, but with limited projections, these methods are not easily applicable. Inverting Pad-Rotate-Project measurements is even more difficult due to the lack of information about \u03b8. Inverting Pad-Rotate-Project measurements is challenging due to the lack of information about \u03b8. Results with AmbientGAN models are reported on a subset of experiments, showing samples generated by baselines and our models for selected parameter settings. Additional results are available in the appendix. Results on celebA with DCGAN and CIFAR-10 with ACW-GANGP show degraded samples in the measurement process, making it difficult for baselines to produce good samples. Our models, however, are able to generate images with high visual quality using Convolve+Noise with a Gaussian kernel and IID Gaussian. Our models outperform baselines in generating high-quality images by observing only parts of one image at a time using different measurement processes like Convolve+Noise and 1D projections. Our models achieve coherent face generation by observing parts of one image at a time, with Pad-Rotate-Project and Pad-Rotate-Project-\u03b8 measurement models showing signal degradation. Experiments on MNIST with DCGAN demonstrate the ability to learn rotation and reflection without explicit incentives. In experiments on MNIST with DCGAN, two measurement models are used to generate digits. The first model learns rotation and reflection without explicit incentives, resulting in digits with similar orientations and chirality within each class. The second model includes rotation angle, producing upright digits. Despite lower visual quality, the method shows the ability to generate digit images from 1D projections. Our method demonstrates the ability to generate digit images from 1D projections, even though the visual quality is lower. The model trained on celebA dataset with Pad-Rotate-Project-\u03b8 measurements shows a crude outline of a face but lacks details, indicating the challenge of learning complex distributions with just 1D projections. This highlights the need for a better understanding of distribution recovery under projection measurement models and improved GAN training methods. The difficulty in learning complex distributions with 1D projections is highlighted, emphasizing the need for better understanding of distribution recovery and improved GAN training methods. Inception scores are used to quantify the quality of generative models in the AmbientGAN framework, with models trained on CIFAR-10 and MNIST datasets achieving high accuracy. The final test set accuracy of a classification model on MNIST was 99.2%. Different models were trained with varying probabilities of blocking pixels for Block-Pixels measurements, and their inception scores were computed. The plot of inception scores versus the probability of blocking pixels showed that as the probability increased, baseline models performed poorly compared to AmbientGAN models. As the probability of blocking pixels increases, baseline models perform poorly compared to AmbientGAN models. For Convolve+Noise measurements on MNIST with varying additive noise levels, Wiener deconvolution and the \"ignore\" baseline perform well for small noise variances. The inception score for MNIST models trained with varying noise levels shows that Wiener deconvolution and the \"ignore\" baseline perform well for small noise variances, but deteriorate as noise levels increase. AmbientGAN models maintain a high inception score even with increased noise levels. The Pad-Rotate-Project model performs poorly in producing digits at various orientations. The Pad-Rotate-Project model produces poorly aligned digits with an inception score of 4.18, while the model with Pad-Rotate-Project-\u03b8 measurements achieves a score of 8.12. The latter model comes close to the performance of the fully-observed case, with an inception score of 8.99, despite being trained only on 1D projections. The GAN model trained on fully-observed samples achieves an inception score of 8.99, coming close to the performance of the fully-observed case while being trained on 1D projections. The total variation inpainting method is slow, with performance similar to the unmeasured-blur baseline on MNIST. Inpainting baselines are not run on the CIFAR-10 dataset. The trend in the plot shows the superiority of the approach over baselines. Generative models require high-quality datasets, but our approach shows how to learn a distribution from incomplete, noisy measurements. This allows for the construction of new generative models for distributions without a complete dataset. Our approach demonstrates learning a distribution from incomplete, noisy measurements to construct new generative models for distributions lacking high-quality datasets. The text discusses learning a distribution from incomplete, noisy measurements to create new generative models for datasets lacking high-quality data. It mentions the unique probability distribution that induces a given measurement distribution in the context of the vanilla GAN model. The text discusses learning a distribution from incomplete, noisy measurements to create new generative models for datasets lacking high-quality data. It mentions the unique probability distribution that induces a given measurement distribution in the context of the vanilla GAN model. Since \u0398 \u223c N (0, I n ), all possible directions for projections are covered. The measurement model includes the projection vector \u0398 as part of the measurements, matching the measurement distribution requires the underlying distribution p r x to have all 1D marginals matched. By Cramer-Wold theorem, any sequence of random vectors matching the 1D marginals must converge in distribution to the true underlying distribution. The text discusses learning a distribution from incomplete, noisy measurements to create new generative models for datasets lacking high-quality data. It mentions the unique probability distribution that induces a given measurement distribution in the context of the vanilla GAN model. The Convolve+Noise measurement model with convolution kernel k and additive noise distribution p \u03b8 leads to a unique distribution p r x that induces the measurement distribution p r y. The text discusses the unique distribution p r x that induces the measurement distribution p r y in the context of generative models. It explains the bijective map between X and Z through continuous transformations, leading to a relationship between the pdfs of X and Z. The text explains the relationship between the pdfs of random variables X and Z through a bijective map, and discusses the reverse map from the measurement distribution p y to a sample distribution p x. The text discusses the reverse map from the measurement distribution p y to a sample distribution p x, concluding the proof by uniquely determining the true underlying distribution p x. It also presents a slightly different version of Theorem 1 for the discrete setting. The text discusses the empirical version of the vanilla GAN objective, defining the optimal discriminator and generator for a dataset of measurement samples. The Empirical Risk Minimization (ERM) version of the loss is equivalent to taking the expectation of the data dependent term with respect to the empirical distribution. If the discriminator is fixed to be optimal, then any optimal generator must satisfy a specific condition. The proof involves replacing the real data distribution with the empirical version in the proof of a theorem. The theorem assumes image pixels are in a finite set and discusses a unique distribution under certain conditions. The Block-Pixels measurement model involves a discrete distribution over samples, with a transition matrix determining the probability of measurements. The transition matrix A determines the probability of measurements in the Block-Pixels model. If A is invertible, the distribution p x can be recovered from p y. The sample complexity is determined by the minimum eigenvalue magnitude of A. The sample complexity in the Block-Pixels model is determined by the minimum eigenvalue magnitude of the invertible matrix A. The optimal generator must satisfy certain conditions, leading to a specific probability outcome. The dataset is divided into classes based on the number of pixels in the images. In the Block-Pixels model, the dataset is divided into classes based on the number of pixels in the images. The transition matrix A is lower triangular due to the independent blocking of pixels with probability p. In the Block-Pixels model, the transition matrix is lower triangular. Each image has a chance of being unaffected by measurements, with diagonal entries in the transition matrix being strictly positive. The minimum value for diagonal entries is (1 \u2212 p) n. The transition matrix in the Block-Pixels model is lower triangular, with diagonal entries representing unaffected images. The minimum value for these entries is (1 \u2212 p) n. The DCGAN model on MNIST uses a noise input with 100 dimensions sampled from a Uniform distribution on [\u22121, 1]. The DCGAN model on MNIST uses a noise input with 100 dimensions sampled from a Uniform distribution on [\u22121, 1]. The generator and discriminator both have specific architectures involving linear and convolutional layers, with labels concatenated with inputs at each layer. Batch-norm is utilized in both models. The WGANGP model on MNIST and the unconditional DCGAN model on celebA have different architectures. The generator in WGANGP takes a latent vector of 128 dimensions sampled from a Uniform distribution on [\u22121, 1] and uses one linear and three deconvolutional layers. The discriminator in WGANGP uses three convolutional layers followed by one linear layer without batch-norm. In contrast, the generator in DCGAN for celebA takes a latent vector of 100 dimensions sampled from a Uniform distribution on [\u22121, 1] and uses one linear layer followed by four deconvolutional layers. The ACWGANGP model on CIFAR-10 follows a residual architecture with a latent vector of 128 dimensions sampled from a standard Gaussian distribution. The generator includes a linear layer followed by three residual blocks. Each residual block consists of two repetitions. The model on CIFAR-10 utilizes a residual architecture with a latent vector of 128 dimensions sampled from a standard Gaussian distribution. The generator includes a linear layer, three residual blocks, conditional batch normalization, nonlinearity, and upconvolution layers. The discriminator consists of one residual block, two convolutional layers, and a final linear layer. In the analysis and experiments, the generator includes conditional batch normalization, a final convolution, and a final tanh non-linearity. The discriminator consists of one residual block with two convolutional layers followed by three residual blocks and a final linear layer. The results for various measurement models are presented, considering the case where the parameter distribution is only approximately known for robust training. The AmbientGAN approach is robust to systematic mismatches in the parameter distribution of the measurement function, as demonstrated empirically using the Block-Pixels measurement model on the MNIST dataset. Pixels are blocked with a probability of 0.5 to create a dataset of measurements. The AmbientGAN approach is robust to parameter distribution mismatches, demonstrated using the Block-Pixels measurement model on the MNIST dataset. The plot of the inception score peaks at p = 0.5, indicating some level of robustness. The inception score peaks at p = 0.5, showing robustness to parameter distribution mismatch. The AmbientGAN approach captures data distribution well and is used for compressed sensing on MNIST dataset. Using the AmbientGAN approach trained on corrupted samples, we applied compressed sensing on the MNIST dataset. Comparing Lasso with AmbientGAN, we observed a reduction in the number of measurements required for reconstruction."
}