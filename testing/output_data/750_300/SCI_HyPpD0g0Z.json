{
    "title": "HyPpD0g0Z",
    "content": "When training a deep neural network for supervised image classification, latent features can be broadly divided into \"core\" features that remain consistent across domains and \"style\" features that can vary. \"Style\" features may include attributes like position, rotation, image quality, and more complex characteristics like hair color or posture for images of persons. The distribution of orthogonal features X^orth can vary across domains, including position, rotation, image quality, and more complex attributes like hair color or posture for images of persons. To address adversarial domain shifts, we aim to use \"conditionally invariant\" features for classification, assuming the domain as a latent variable that is not directly observed. In data augmentation, images are generated from an original image with an identifier variable. This method only requires a small fraction of images to have an ID variable. In data augmentation, images are generated from an original image with an identifier variable. A causal framework is provided by adding the ID variable to the model. The domain is treated as a latent variable in settings where it cannot be observed directly. Samples with the same class and identifier are treated as counterfactuals under different style interventions. The network is regularized using a grouping-by-ID approach to improve performance in settings where domains change in terms of image quality, brightness, and color changes. By regularizing the network to provide consistent output across samples with the same ID, performance is substantially improved in settings with varying image quality, brightness, and color changes. Deep neural networks have excelled in tasks like visual object and speech recognition, but issues can arise when learned representations rely on dependencies that disappear in test distributions. This approach also raises questions about interpretability, fairness, and transfer learning. Issues can arise in deep neural networks (DNNs) when learned representations rely on dependencies that vanish in test distributions, leading to domain shifts caused by changing conditions. This can result in degraded predictive performance, as seen in the \"Russian tank legend\" example. The \"Russian tank legend\" is an example of sampling biases in training data leading to degraded predictive performance. A machine learning system was trained to distinguish between Russian and American tanks based on photo quality, resulting in high accuracy due to all Russian tank images being of poor quality. This hidden confounding factor highlights the impact of indirect associations in machine learning systems. The system learned to discriminate between images of different qualities but would have failed in practice. Hidden confounding factors, like the example of image quality and tank origin, create indirect associations. Deep learning requires large sample sizes to average out the effects of confounding factors and achieve invariance to known factors through data augmentation. Large sample sizes are necessary in deep learning to average out confounding effects and achieve invariance to known factors through data augmentation. Adversarial examples, which are intentionally perturbed inputs misclassified by ML models, highlight the difference between human and artificial cognition. Humans do not get fooled by adversarial examples and only need to see one rotated example of an object to achieve invariance to rotations in perception. Mimicking this simplicity is a starting point for further exploration. Adversarial examples mislead ML models but not humans. Humans achieve invariance to rotations with few examples. Can we align DNN features with human cognition? Fairness and discrimination are reasons to control input data characteristics in learned representations. Existing biases in datasets used for training ML algorithms tend to be replicated in the estimated models, leading to issues like Google's photo app tagging non-white people as \"gorillas\" due to biased training examples. Addressing fairness and discrimination concerns, it is important to control input data characteristics in learned representations to prevent such biases from impacting decisions. To address biases in ML algorithms due to skewed training data, a proposed solution is counterfactual regularization (CORE) to control latent features extracted from input data. This approach aims to categorize data generating factors into 'conditionally invariant' (core) and 'orthogonal' (style) features, ensuring classifiers focus on core features relevant to the target of interest. CORE aims to categorize latent data generating factors into 'conditionally invariant' (core) and 'orthogonal' (style) features, ensuring classifiers focus on core features relevant to the target of interest. It yields an estimator that is robust to adversarial domain shifts by observing \"counterfactuals\" in certain datasets. CORE is robust to adversarial domain shifts by leveraging counterfactuals in datasets, focusing on grouping instances related to the same object. It reduces the need for data augmentation and improves predictive performance in small sample size settings. In \u00a72, CORE is shown to reduce the need for data augmentation and improve predictive performance in small sample size settings. In \u00a73 and \u00a74, counterfactual regularization is formally introduced, along with the CORE estimator and theoretical insights for logistic regression. In \u00a75, the performance of CORE is evaluated in various experiments using the CelebA dataset for classifying whether a person wears glasses. The CelebA dataset BID26 contains face images of celebrities for classifying whether a person wears glasses. Grouping information is used to ensure the same prediction for all images of the same person. Counterfactual observations are included, with n = 10 identities in the training set and a total sample size of m = 321 images. The training set includes n = 10 identities and a total sample size of m = 321 images. Grouping-by-ID is used to group observations, resulting in counterfactual examples for training the network architecture. The augmented MNIST dataset includes counterfactual examples using the same identity for training. Exploiting grouping information reduces test error by 32% and 50% for rotated digits. Overall, utilizing the group structure decreases average test error from 24.76% to 16.89%. Using the group structure reduces test error by 32% and 50% for rotated digits, decreasing average test error from 24.76% to 16.89%. CORE makes data augmentation more efficient by generating additional samples through interventions on style features. CORE makes data augmentation more efficient by creating additional samples through interventions on style features, such as rotating, translating, or flipping images. This augmented data set enforces invariance with respect to the transformations of interest, like rotation. The grouping information ensures that original and augmented samples belong to the same object, strengthening invariance compared to normal data augmentation. Using CORE for data augmentation creates additional samples by intervening on style features like rotation. This method enforces invariance with respect to style features, resulting in a more efficient process compared to normal data augmentation. The average test error on rotated examples is significantly reduced, showing promising results for improving model performance. The average test error on rotated examples is reduced from 32.86% to 16.33% by using CORE for data augmentation. This approach aims to learn a representation without discriminative information about the input's origin, similar to Domain-Adversarial Neural Networks (DANN) proposed in previous works. BID13 aims to learn a representation without discriminative information about the input's origin through adversarial training. BID14 focuses on different realizations of the same object under different interventions. The data generating process in BID14 is similar to our model, but with a fundamental difference in the data basis used. BID14 identifies conditionally independent features by adjusting transformations to minimize MMD distance between distributions in different domains, while our approach penalizes the classifier using latent features outside the set of observable identifiers. Our approach differs from BID14 as we use a latent domain identifier instead of an explicit one. We penalize the classifier for using latent features outside the set of conditionally independent features. Causal modeling, like in transfer learning, aims to guard against domain shifts and interventions on predictor variables. Causal models ensure valid predictions even under large interventions. Causal models ensure valid predictions under large interventions, but transferring results to adversarial domain changes in image classification faces challenges. Classification tasks are typically anti-causal, and the goal is to guard against specific interventions on variables. The challenge in image classification is to guard against domain shifts in style features, leveraging causal motivations for deep learning. Various approaches have been proposed, but they differ from the goal of anti-causal prediction and non-ancestral interventions. Various approaches have been proposed that leverage causal motivations for deep learning or use deep learning for causal inference. The Neural Causation Coefficient (NCC) is introduced to estimate the probability of X causing Y and is applied to finding causal relations between image features. The Neural Causation Coefficient (NCC) estimates the probability of X causing Y and distinguishes between object features and context features. CGANs are used to find causal relations between image features, with one fitted for X \u2192 Y and another for Y \u2192 X. Generative neural networks are used for cause-effect inference and to orient graph edges. Bahadori et al. (2017) devise a regularizer for causal inference. Based on a two-sample test statistic, the estimated causal direction is returned using generative neural networks. Bahadori et al. (2017) devise a regularizer combining a penalty with weights based on the estimated probability of a feature being causal. Besserve et al. (2017) connect GANs and causal generative models using a group theoretic framework. Kocaoglu et al. (2017) propose causal implicit generative models for sampling from conditional distributions. BID29 propose deep latent variable models and proxy variables for estimating individual treatment effects, while BID21 use causal reasoning to address fairness in machine learning. The causal graph structure must be known for estimating individual treatment effects. BID21 use causal reasoning to address fairness in machine learning by deriving causal nondiscrimination criteria. Algorithms avoiding proxy discrimination require classifiers to be constant as a function of proxy variables in the causal graph, similar to disentangling factors of variation. Estimating disentangled factors of variation in generative modeling has gained interest. Matsuo et al. (2017) propose a \"Transform Invariant Autoencoder\" to reduce dependence of latent representation on specified object transforms in images. They define location as an orthogonal style feature X \u22a5 and aim to learn it. The \"Transform Invariant Autoencoder\" aims to reduce the influence of specified object transforms on the latent representation in images. It does not predefine which features are included in the orthogonal style feature X \u22a5, which could be location, image quality, posture, brightness, background, or contextual information. Matsuo et al. (2017) also consider the data generating process and the effect of the domain on the orthogonal features X \u22a5 mediated via unobserved noise \u2206. In Matsuo et al. (2017), the domain's effect on orthogonal features X \u22a5 is mediated by unobserved noise \u2206. The setting involves a confounding situation where style features differ based on class, with grouped observations exploited in a variational autoencoder framework to separate style and class. In a variational autoencoder framework, Bouchacourt et al. (2017) aim to separate style and content by assuming grouped observations share a common but unknown value for one factor of variation. They solve a classification task directly without estimating latent factors, contrasting with a generative framework. In a classification task, we aim to predict the target variable Y using predictor X. The prediction is made by a function f \u03b8 with parameters \u03b8, where \u03b8 corresponds to the weights in a DNN. The setting of adversarial domain shifts is compared to transfer learning, domain adaptation, and adversarial examples. In a classification task, the prediction y is made by a function f \u03b8 with parameters \u03b8. The goal is to minimize the expected loss by penalized empirical risk minimization using training data samples (x i , y i ). Parameter estimation in machine learning involves minimizing the expected loss by penalized empirical risk minimization using training data samples. The penalty can be a ridge penalty or other penalties that exploit underlying geometries. The full structural model for all variables is shown in the right panel of a figure, with the domain variable D being latent. The structural model includes latent variables like the domain variable D and ID variable, used for grouping observations. The prediction is anti-causal, with predictors X being non-ancestral to Y. The class label is causal for the image. The prediction in the setting of BID14 is anti-causal, with predictors X being non-ancestral to Y. The causal effect from the class label Y on the image X is mediated via core features X ci and style features X \u22a5, where interventions are possible on the style features but not on the core features. If interventions have different distributions in different domains, the distribution P(X ci | Y) is constant across domains. The distinction between core features X ci and style features X \u22a5 lies in the possibility of external interventions on the latter but not the former. Interventions with varying distributions across domains result in a constant distribution P(X ci | Y) while P(X \u22a5 | Y) can change. The style features X \u22a5 and Y are confounded by the latent domain D, while the core features X ci remain conditionally independent of D given Y. The style variable encompasses various elements such as point of view, image quality, resolution, rotations, color changes, body posture, and movement. The core features X ci are conditionally invariant and independent of domain D given Y. The style variable includes elements like point of view, image quality, resolution, rotations, color changes, body posture, and movement. The style intervention variable \u2206 influences both the latent style X and the image X. In this work, the focus is on guarding against adversarial domain shifts using causal graph and potential outcome notation. The causal graph explains domain adaptation, transfer learning, and adversarial examples. The prediction under style intervention \u2206 = \u03b4 is denoted as f \u03b8 (X(\u2206 = \u03b4)). The causal graph is used to explain domain adaptation, transfer learning, and guarding against adversarial examples. The intervention \u2206 is assumed to be within an -ball in q-norm around the origin. Imperceptible changes in the input can lead to misclassification, so the goal is to minimize adversarial loss in classification. The goal is to devise a classification that minimizes adversarial loss in a causal graph, considering imperceptible changes in input that can lead to misclassification. Adversarial domain shifts involve strong interventions on style features not explicitly known, aiming to minimize adversarial loss under large style interventions. In adversarial domain shifts, strong interventions on style features are considered to minimize adversarial loss under large style interventions, even though these interventions are not explicitly known. The term \"adversarial\" is used to describe interventions on style features, distinct from the training procedure in domain adversarial neural networks. The term \"adversarial\" is used to refer to interventions on style features, distinct from the training procedure in domain adversarial neural networks. The motivation is to protect against shifts in test data distribution by distinguishing between core and style features. The classical problem of causal inference is highlighted, where observing a counterfactual is impossible. The classical problem of causal inference involves the challenge of observing counterfactuals, where we can only see the outcome under one treatment condition at a time. This makes it difficult to determine the true treatment effect. Observing counterfactuals is challenging in causal inference as we can only see outcomes under one treatment condition at a time. The term counterfactual is used when keeping class label Y and ID constant while allowing the style intervention \u2206 to change, similar to a treatment effect in medical studies. The style intervention \u2206 in image analysis plays a role similar to treatment T in medical studies, allowing for counterfactuals to be observed by changing conditions. For example, \u2206 could represent variables determining whether a person consistently wears glasses in different images. In image analysis, the style intervention \u2206 acts like a treatment in medical studies, allowing for observation of counterfactuals by changing conditions. It helps determine different images of the same person based on variables like background, posture, and image quality. The focus is not on the treatment effect of \u2206 but on using it to rule out parts of the feature space for classification. The treatment effect of \u2206 occurs in the style features X \u22a5, not in the 'conditionally invariant' space X ci. The style intervention \u2206 is used to rule out parts of the feature space for classification, focusing on penalizing changes in classification under different style interventions while keeping class and identity constant. Notationally, samples are denoted by i with class label and identifier, and observations are represented by x i,j \u2208 R p. The pooled estimator treats all examples identically by summing over the loss with a ridge penalty. The adversarial loss of the pooled estimator will be infinite in general. The pooled estimator uses a ridge penalty and a cross-validated choice of penalty parameter. The adversarial loss of the pooled estimator can be infinite. Conditions (i) and (ii) explain the requirements for predicting Y. The pooled estimator uses a ridge penalty and cross-validated penalty parameter choice. Conditions (i) and (ii) outline the requirements for predicting Y, emphasizing the importance of the relations between Y, X ci, and X \u22a5 being non-deterministic. To minimize the adversarial loss, it is crucial to ensure the function f \u03b8 (x(\u2206)) remains as constant as possible for all values of \u2206. The adversarial loss L adv is minimized when the function f \u03b8 (x(\u2206)) is constant for all x \u2208 R p in the invariant parameter space I. The optimal predictor in the invariant space is f \u03b8, which is solely based on the core features X ci. The optimal predictor in the invariant space is based on core features X ci, which are not directly observable. The unknown invariant parameters space is approximated using empirical risk minimization. The unknown invariant parameters space is approximated by an empirically invariant space, defined with a regularization constant \u03c4. Different values of \u03c4 allow for varying degrees of variations in estimated predictions for class labels across counterfactuals of images. The true invariant space is a subset of the empirically invariant space. The regularization constant \u03c4 controls the degree of variations in predicted class labels across counterfactuals of images. The true invariant space is a subset of the empirically invariant subspace, with the possibility of convergence under certain assumptions. The Lagrangian form of constrained optimization can be used with a penalty parameter \u03bb instead of \u03c4. The regularization constant \u03c4 controls variations in predicted class labels. The matrix L ID is a graph Laplacian where samples with the same ID are connected. Graph Laplacian regularization penalizes variances \u03c3 2 i (\u03b8). The graph is formed in the sample space by the identifier variable ID. The regularization in the sample space is induced by the identifier variable ID, rather than in feature space. The penalty \u03bb value does not strongly affect the outcome, emphasizing the importance of defining the graph based on ID. Other regularizations are not as effective in guarding against domain shifts. Adversarial loss is analyzed for the pooled and CORE estimator in a one-layer network. The adversarial loss is analyzed for the pooled and CORE estimator in a one-layer network for binary classification using logistic regression. The pooled estimator has infinite adversarial loss under suitable assumptions. The CORE estimator in logistic regression predicts class labels with linear style features. It outperforms the pooled estimator in handling confounded data and changing style features. Experiments include assessing confounding levels and classifying elephants and horses based on color. The CORE estimator in logistic regression can handle confounded training data sets and changing style features in test distributions. Experimental results for classifying elephants and horses based on color, gender, wearing glasses, and brightness are included. TensorFlow BID0 implementation of CORE will be available, along with additional code for reproducing experiments. Architecture details can be found in \u00a7C.7. The CORE estimator in logistic regression can handle confounded training data sets and changing style features in test distributions. Experimental results for classifying elephants and horses based on color, gender, wearing glasses, and brightness are included. A TensorFlow BID0 implementation of CORE will be available, along with additional code for reproducing experiments. Information on the employed architectures can be found in \u00a7C.7. An open question is how to set the value of the tuning parameter \u03c4 or the penalty \u03bb in Lagrangian form, with performance typically not very sensitive to the choice of \u03bb. In this example, synthetically generated stickmen images are considered, with the target of interest being Y \u2208 {adult, child} and X ci \u2261 height. The class Y is causal for height, which is considered a core feature and a robust predictor. In this example, synthetically generated stickmen images are analyzed, with the target of interest being distinguishing between adults and children based on height. The data generating process involves a hidden common cause affecting the relationship between age and movement. The images of children typically show them playing, while adults are often depicted in more static postures. The data generating process involves a hidden common cause affecting the relationship between age and movement. The model may fail when presented with images that do not follow the typical associations between large movements and children, and small movements and adults. Test sets 2 and 3 intervene on the relationship between Y and X, causing the dependence to vanish. In test sets 2 and 3, interventions cause the dependence between Y and X to vanish. Large movements are associated with both children and adults in these test sets, with heavier movements in test set 3 than in test set 2. Misclassification rates for CORE and the pooled estimator are shown in FIG0, where CORE outperforms the pooled estimator with as few as 50 counterfactual observations. The pooled estimator for c = 50 with a total sample size of m = 20000 fails to achieve good predictive performance on test sets 2 and 3, with test errors > 40%. CORE, on the other hand, succeeds with as few as 50 counterfactual observations. The results suggest that the pooled estimator uses movement as a predictor for age, while CORE does not due to counterfactual regularization. Including more counterfactual examples would not improve the pooled estimator's performance. The pooled estimator for c = 50 with a total sample size of m = 20000 fails to achieve good predictive performance on test sets 2 and 3, with test errors > 40%. Importantly, including more counterfactual examples would not improve the performance of the pooled estimator as these would be subject to the same bias. The image quality X\u22a5 is affected by the presence of glasses in the image, mimicking confounding seen in the Russian tank legend. The intervention changes image quality based on a Gaussian distribution. Counterfactual observations are only available for images with glasses. Examples from the training set are shown in FIG0. The counterfactual observation uses the same image with a newly sampled image quality value. Counterfactual observations for images without glasses are not available. The counterfactual observation uses the same image with a newly sampled image quality value. Misclassification rates for CORE and the pooled estimator on different test sets are shown in FIG0. Test set 1 follows the same distribution as the training set, while test set 2 reverses the class of the quality intervention. The pooled estimator outperforms CORE on test set 1 by utilizing predictive information from image quality, while CORE is restricted from doing so. The pooled estimator performs better than CORE on test set 1 by utilizing image quality information, while CORE is restricted from doing so. However, the pooled estimator does not perform well on test sets 2-4 as its learned representation relies on image quality as a predictor, unlike CORE whose performance is not affected by changing image quality distributions. In this example, the performance of CORE is not affected by changing image quality distributions. The study aims to assess if CORE can exclude \"color\" from its learned representation by including counterfactuals of different colors, inspired by the concept formation process in recognizing objects like elephants based on color. In assessing CORE's ability to exclude \"color\" from its learned representation, counterfactual examples of different colors are included for elephants. The study uses the AwA2 dataset to classify images of horses and elephants, with a total sample size of 1850. The study assesses CORE's ability to exclude \"color\" from its learned representation using counterfactual examples for elephants. The total sample size is 1850, with misclassification rates shown for CORE and the pooled estimator on different test sets. Test sets include original colored images, grayscale images with modified colorspace, and grayscale images only. Test sets 2 and 3 contain grayscale images and modified colorspace, affecting the predictive accuracy of the pooled estimator but not CORE. The study focuses on the impact of color distribution changes on predictive performance. The study shows that the predictive accuracy of the pooled estimator is affected by changes in color distribution, while the CORE estimator remains unaffected. Adding grayscale images can help improve the predictive accuracy of the pooled estimator, but the CORE estimator requires color invariance for predictions. The CORE estimator emphasizes color invariance for predictions, unlike the pooled estimator which is affected by changes in color distribution. If \"color\" was a protected attribute, CORE would not include it in its learned representation, while the pooled estimator would use \"color\" for its decisions. The proposed counterfactual regularization (CORE) aims to achieve robustness by distinguishing latent features in images into core and style features. By demanding invariance of the classifier amongst instances related to the same object, CORE ensures fairness by not including certain features in its learned representation. The CORE estimator aims to achieve invariance of classification performance by exploiting instances of the same object in the training data. It ensures robustness against adversarial interventions on style features like image quality, fashion type, color, or body posture, and works despite sampling biases. This approach can achieve the same classification performance as standard data augmentation methods with fewer instances. The CORE estimator achieves classification performance invariance by exploiting instances of the same object in the training data, even with sampling biases. It can achieve similar results as standard data augmentation methods with fewer instances, and can automatically penalize features that vary strongly between different instances of the same object. Further research could explore using larger models like Inception or large ResNet architectures. The CORE estimator aims to achieve classification performance invariance by penalizing features that vary strongly between instances of the same object. Research suggests using larger models like Inception or ResNet architectures for better results. The goal is to assess the benefits of CORE for training Inception-style models in terms of sample efficiency and generalization performance. The CORE estimator aims to achieve classification performance invariance by penalizing features that vary strongly between instances of the same object. Research suggests using larger models like Inception or ResNet architectures for better results. The potential future direction includes using video data for grouping and counterfactual regularization, which could also help to debias word embeddings. Regularization can help debias word embeddings. The image X is linear in style features X \u22a5, with logistic regression predicting class label Y. Interventions \u2206 act additively on X \u22a5, which linearly affects X through matrix W. Core features X ci are conditionally invariant. The core features X ci are conditionally invariant in a logistic regression model predicting class label Y from image data X. The model assumes a matrix W and independent noise variables. Training data with m samples is used to estimate \u03b8 with logistic loss for training and testing. The logistic regression model predicts Y from image data X, with latent variables ci, \u2206, X \u22a5, and noise variables. Training data is used to estimate \u03b8 with logistic loss for training and testing. Two expected losses on test data are discussed, one without interventions on style variables and the other with adversarial interventions on X \u22a5. The first loss is a standard logistic loss without interventions on style variables, while the second loss involves adversarial interventions on X \u22a5. The assumptions for Theorem 1 include conditions on \u2206, W, and the number of counterfactual examples in the samples. Assuming \u2206 is sampled from a distribution for training data in R q with positive density on an -ball in 2 -norm around the origin for some > 0, and that the matrix W has full rank q. The number c = m \u2212 n of counterfactual examples in the samples is at least as large as the dimension of the style variables. The sampling process involves collecting n independent samples and selecting c = m \u2212 n samples randomly, redrawing a new value of \u2206 each time. The sampling process involves selecting c = m - n samples randomly, redrawing a new value of \u2206 each time. The pooled estimator has infinite adversarial loss with probability 1 with respect to the training data. The pooled estimator has infinite adversarial loss with probability 1 with respect to the training data, under Assumption 1. The CORE estimator also shows similar results for misclassification loss as n approaches infinity. The proof involves showing that W t\u03b8pool = 0 with probability 1 by using the oracle estimator \u03b8* constrained to be orthogonal to the column space of W. The proof involves showing that W t\u03b8pool = 0 with probability 1 using the oracle estimator \u03b8* constrained to be orthogonal to the column space of W. This is done by assuming W t\u03b8pool = 0 and showing a contradiction, leading to the conclusion that \u03b8pool = \u03b8*. The directional derivative of the training loss with respect to any \u03b4 in the column space of W should vanish at the solution \u03b8*. The counterfactual training data only affects the column space of W, making the oracle estimator \u03b8* identical under both true and counterfactual data. The counterfactual training data alters the column space of W, but the oracle estimator \u03b8* remains the same under both true and counterfactual data. The derivative g(\u03b4) can be expressed as a difference between two formulas, where \u03b4 is in the column-space of W. The model assumptions state that x i,j \u2212 x i,j (0) = W \u2206 i,j, where \u03b4 = W u. The eigenvalues of W t W are positive. The estimator \u03b8* is not affected by interventions \u2206 i,j. The interventions are drawn from a continuous distribution. The interventions \u2206 i,j are drawn from a continuous distribution, leading to the left hand side of the equation having a continuous distribution. The probability of the left hand side not being identically 0 is 1, completing the proof by contradiction. With probability 1, \u03b8 core = \u03b8* as defined in the model assumptions. The invariant space is the linear subspace where W t \u03b8 = 0. The second part discusses the relationship between \u03b8 core and \u03b8* in the model, showing that they are equal with probability 1. The linear subspace I = {\u03b8 : W t \u03b8 = 0} is the invariant space for this model. By the definitions provided, it is shown that I n = {\u03b8 : W t \u03b8 = 0} with probability 1. This implies that \u03b8 core = \u03b8* in the model. The estimator remains unchanged when using data without interventions as training data. The population-optimal vector can be expressed as a formula, showing the relationship between different variables. Comparing two formulas, it is shown that under assumed sampling conditions, certain samples can be redrawn from empirical data. The CelebA dataset is used to classify gender based on images, creating a confounding factor by including mostly men wearing glasses. The proof involves uniform convergence of Ln to the population loss L(0) under assumed sampling conditions. In the CelebA dataset, a confounding factor is created by including mostly men wearing glasses. Counterfactuals are used to address this, with images of the same person without glasses for males and with glasses for females. Test set 2 flips the association between gender and glasses. In the CelebA dataset, a confounding factor is addressed using counterfactuals. Test set 2 flips the association between gender and glasses. The results of training a four-layer CNN versus using Inception V3 features are compared in terms of varying numbers of m and c. The results of training a four-layer CNN versus using Inception V3 features are compared in terms of varying numbers of m and c. As c increases, the performance difference between CORE and the pooled estimator becomes smaller. The pooled estimator performs worse on test set 2 as m becomes larger, exploiting X \u22a5 to a larger extent. Including counterfactual examples for data augmentation, the pooled estimator performs worse on test set 2 as m increases, indicating a greater exploitation of X \u22a5. Analyzing a confounded setting with the CelebA dataset, the scenario involves a hidden common cause where D indicates whether the image was taken indoors or outdoors, affecting the classification of eyeglasses worn by the person in the image. The image dataset includes scenarios where outdoor images are brighter with people wearing glasses, while indoor images are darker without glasses. Test sets vary in brightness interventions, with some reversing the effect on glasses-wearing individuals. The image dataset includes scenarios where outdoor images are brighter with people wearing glasses, while indoor images are darker without glasses. Test sets vary in brightness interventions, with some reversing the effect on glasses-wearing individuals. The pooled estimator performs better than CORE on test set 1 by utilizing brightness information, but struggles on test sets 2 and 4 where brightness is manipulated. The pooled estimator utilizes brightness information for prediction but struggles on test sets with manipulated brightness, unlike CORE which is hardly affected by changing brightness distributions. Results for different parameters can be found in FIG0.5. The predictive performance of CORE is robust to changing brightness distributions. Different counterfactual settings are explored, including using images of the same person or a different person. Results for various parameters can be seen in FIG0.5. Using different images of the same person as counterfactual examples (CF setting 3) yielded the best results, allowing for explicit control over brightness variations. In contrast, using images of different persons in counterfactual setting 2 presented challenges in isolating brightness as the invariant factor. Grouping images of different persons still showed some improvement in predictive performance. In counterfactual setting 2, different images of the same person can vary in factors, making it challenging to isolate brightness as the invariant factor. The tuning parameter \u03c4 or penalty \u03bb in Eq. (4) is an open question. Performance of CORE on the AwA2 dataset is not very sensitive to the choice of \u03bb. Varying the number of identities in the training dataset is explored in the experiment introduced in \u00a72.1. In the experiment, the performance of CORE on the AwA2 dataset is not very sensitive to the penalty \u03bb. Varying the number of identities in the training dataset results in different sample sizes, with CORE showing improved predictive performance compared to other estimators, especially when the number of identities is small. The misclassification rates for the test set of 5000 examples show that CORE improves predictive performance compared to the pooled estimator, especially with small sample sizes. As sample sizes increase, the performance of CORE and the pooled estimator becomes comparable. The experiment introduced in \u00a72.2 shows that increasing the number of augmented training examples improves the efficiency of data augmentation. Misclassification rates are lower with CORE on rotated digits, indicating its effectiveness in improving predictive performance. The experiment in \u00a72.2 demonstrates that CORE improves data augmentation efficiency by reducing misclassification rates on rotated digits. Results show that CORE's performance is not sensitive to the number of counterfactual examples used. The CORE estimator's performance is not sensitive to the number of counterfactual examples, as long as there are enough in the training set. The pooled estimator struggles with predictive performance on certain test sets. Experiment results show that counterfactual setting 1 works best. The experiments for counterfactual settings 1-3 with c = 5000 show that setting 1 works best, with small differences between settings 2 and 3. There is a notable performance gap between \u00b5 = 40 and \u00b5 = 50 for the pooled estimator, suggesting image quality may not be predictive enough at \u00b5 = 50. The latter command rotates image colors cyclically. The pooled estimator shows a performance gap between \u00b5 = 40 and \u00b5 = 50, indicating image quality may not be predictive enough at \u00b5 = 50. The latter command rotates image colors cyclically and all images in test set 3 are changed to grayscale. The models were implemented in TensorFlow with detailed architectures in TAB1.1. CORE and the pooled estimator use the same network architecture and training procedure, differing only in the loss function with a counterfactual regularization term. All experiments were trained five times using the Adam optimizer. The training procedure for the models involves using the same network architecture and Adam optimizer, with the only difference being the loss function. The training data is shuffled in each epoch to ensure mini batches contain counterfactual observations, with a batch size of 120 used in all experiments. In experiments, training data is shuffled to ensure mini batches have counterfactual observations, with a batch size of 120. This makes optimization more challenging for small c values."
}