{
    "title": "Syx_Ss05tm",
    "content": "Deep neural networks are vulnerable to adversarial attacks in computer vision, where perturbations to images can cause the model to make mistakes. Unlike previous attacks that degrade performance or manipulate specific outputs, these new attacks reprogram the model to perform tasks chosen by the attacker without specifying the desired output for each input. This single perturbation can be added to all test-time inputs to manipulate the model's behavior. Adversarial reprogramming involves finding a single perturbation that can be added to all test-time inputs to manipulate a machine learning model to perform tasks chosen by the attacker. This can repurpose models to perform new tasks, such as a counting task or classification tasks on different datasets. Adversarial reprogramming repurposes ImageNet models for counting and classification tasks, motivated by the danger of attackers causing model errors with small input changes. Various methods have been proposed to construct and defend against adversarial attacks on self-driving cars, such as using perturbations to manipulate stop signs or damage photos. The majority of these attacks aim to degrade model performance without a specific output goal. Adversarial attacks can be untargeted or targeted, with the goal of degrading model performance or producing a specific output. It is important to anticipate different adversarial goals to enhance machine security. In this work, a novel adversarial goal is considered: reprogramming a model to perform a task chosen by the attacker without needing to compute the specific desired output. This poses a more challenging threat to machine learning systems' security. An adversary can reprogram a model to perform a different task by learning adversarial functions that map between the original and adversarial tasks, without needing to compute the specific desired output. In our work, h f and h g are functions that map between tasks, with parameters adjusted to achieve specific mappings. The functions convert inputs and outputs between domains, with h f drawing small images in the center of large images, and h g mapping output class labels. The concept of adversarial reprogramming involves repurposing a model to perform a new task by transforming inputs and outputs between domains. The adversarial program, denoted as \u03b8, is a key component in this process. Unlike traditional adversarial attacks, the model does not need to produce imperceptible results to humans. Adversarial reprogramming involves repurposing a model to perform a new task using an adversarial program denoted as \u03b8. The attack does not require imperceptibility to humans to be successful. Potential consequences include theft of computational resources, repurposing AI-driven assistants, and abusing machine learning services for unethical tasks. Consequences of adversarial reprogramming include theft of computational resources, repurposing AI-driven assistants into spies or spam bots, and abusing machine learning services for unethical tasks. The attack can repurpose a neural network for a new task by making changes to its inputs, as shown in BID36. The flexibility of repurposing a neural network for a new task through changes in its inputs is consistent with the expressive power of deep neural networks. Studies have shown that network depth and hyperparameters can exponentially increase the number of unique output patterns achievable. Additionally, networks can achieve high accuracy even with parameter updates restricted to a low-dimensional subspace. An additive offset to a network's input is equivalent to modifying its first layer biases. In this paper, the authors discuss adversarial reprogramming, where networks can be trained to high accuracy with parameter updates in a low dimensional subspace. Adversarial programs correspond to updates in this subspace, allowing for crafting of neural network behavior. In this paper, the authors introduce adversarial reprogramming, which involves crafting adversarial programs to alter neural network behavior. These programs target various convolutional neural networks trained on ImageNet data, changing their function to tasks like counting squares, classifying MNIST digits, and classifying CIFAR-10 images. The susceptibility of trained and untrained networks to adversarial reprogramming is also examined. Adversarial reprogramming alters neural network behavior by crafting adversarial programs to classify ImageNet data. The susceptibility of trained and untrained networks to this reprogramming is examined, showing the possibility of concealing adversarial programs and data. Transfer learning does not fully explain the results of adversarial reprogramming. Adversarial reprogramming involves crafting programs to manipulate neural network behavior, with transfer learning unable to fully explain the results. Adversarial examples are intentionally designed inputs to cause machine learning models to make mistakes, either untargeted or targeted attacks. Adversarial attacks involve using a gradient-based optimizer to find an image that causes a model to make a mistake, either untargeted or targeted. These attacks have been proposed in various domains such as malware detection, generative models, and network interpretation. Reprogramming methods aim to produce specific functionality rather than a hardcoded output in adversarial attacks. Modifications can be applied to different inputs to create adversarial examples, such as an \"adversarial patch\" designed to switch predictions. In adversarial attacks, reprogramming methods aim to create specific functionality rather than a fixed output. A single adversarial program can manipulate multiple input images to make the model process them according to the program, similar to parasitic computing. Adversarial reprogramming involves presenting an adversarial program with input images to manipulate the model's processing, similar to parasitic computing. It is a form of computational exploit where crafted inputs can run arbitrary code on a targeted computer. Crafted inputs can be used for adversarial reprogramming to run arbitrary code on a targeted computer. This form of parasitic computing manipulates neural networks without gaining access to the host computer. Transfer learning and adversarial reprogramming aim to repurpose networks for new tasks by leveraging knowledge from one task to learn another. Transfer learning and adversarial reprogramming repurpose neural networks for new tasks by leveraging knowledge from one task to learn another, without gaining access to the host computer. Neural networks possess properties useful for various tasks, such as developing features resembling Gabor filters in early layers when trained on images with different datasets or training objectives. Transfer learning involves repurposing neural networks for new tasks by leveraging knowledge from one task to learn another. It allows model parameters to be changed for the new task, unlike adversarial reprogramming where the model cannot be altered. Empirical work has shown that convolutional neural networks trained for one task can be adapted for other tasks by simply training a linear SVM classifier. Adversarial reprogramming involves an adversary altering a neural network's parameters to perform a new task, unlike transfer learning where model parameters are changed for the new task. The adversary's goal is to reprogram the model by crafting an adversarial program. Adversarial reprogramming involves an adversary altering a neural network's parameters to perform a new task by crafting an adversarial program to be included in the network input. The adversarial program is an additive contribution that is not specific to a single image but applied to all images. The adversarial program in adversarial reprogramming is an additive contribution to network input, not specific to a single image but applied to all images. It is defined by parameters to be learned and a masking matrix for new task data. The mask is optional and used for visualization purposes. The adversarial program in adversarial reprogramming is an additive contribution to network input, defined by parameters to be learned and a masking matrix for new task data. The mask is optional and used for visualization purposes. The adversarial image is created by applying the perturbation within a specified range to the input image. The adversarial image is created by applying a perturbation within a specified range to the input image. Let x be a sample from the dataset, and X be the equivalent ImageNet size image with x placed in the proper area defined by the mask M. The corresponding adversarial image is then generated based on a probability mapping function for ImageNet labels. The function h g maps adversarial task labels to ImageNet labels to maximize probability P (h g (y adv )|X adv ). Optimization includes weight norm penalty and Adam optimization with hyperparameters in Appendix A. Adversarial program has minimal computation cost post-optimization. Adversarial reprogramming minimizes computation cost for the adversary by exploiting nonlinear behavior of the target model. It only requires computing X adv and mapping the resulting ImageNet label to the correct class. Adversarial reprogramming exploits nonlinear behavior of the target model, requiring the adversary to store the program and add it to the data. This approach contrasts with traditional adversarial examples that use linear approximations. Experiments on six ImageNet architectures demonstrated the feasibility of reprogramming the network to perform specific tasks. The study demonstrated the feasibility of adversarial reprogramming on six ImageNet architectures, reprogramming them to perform tasks like counting squares, MNIST classification, and CIFAR-10 classification. The research also explored if adversarial training could make networks resistant to reprogramming and compared trained networks to random ones. The study explored adversarial reprogramming on ImageNet architectures, testing resistance to reprogramming and comparing trained networks to random ones. The possibility of reprogramming networks with adversarial data dissimilar to the original was investigated, along with concealing the adversarial program and data. An example task involved counting squares in images. The study investigated adversarial reprogramming on ImageNet architectures, testing resistance to reprogramming with adversarial data. An example task involved counting squares in images, with the adversarial program simply framing the counting task images. The study embedded images in an adversarial program, creating X adv images with squares at the center. Adversarial programs were trained per ImageNet model, unrelated to the original labels. Accuracy was evaluated by sampling 100,000 images and comparing the network's performance. The study demonstrated the vulnerability of neural networks to reprogramming on a simple counting task using only additive contributions to the input. Despite the dissimilarity of ImageNet labels and adversarial labels, the adversarial program mastered the task for all networks. The study showed how neural networks can be reprogrammed on simple tasks using only additive contributions to the input. In a more complex task of classifying MNIST digits, the adversarial program was able to achieve high accuracy on both test and train data, indicating it did not simply memorize training examples. The MNIST digits were embedded in a frame representing the adversarial program, with the first 10 ImageNet labels assigned to them. The study demonstrated reprogramming neural networks for simple tasks using additive contributions to the input. By embedding MNIST digits in a frame with ImageNet labels, the adversarial program successfully transformed ImageNet networks into MNIST classifiers. The reprogramming generalized well from training to test sets, indicating it did not rely solely on memorization of training examples. The study reprogrammed neural networks to function as an MNIST classifier using an additive adversarial program. The reprogramming generalized well from training to test sets, suggesting it does not rely solely on memorization. Adversarial programs were crafted to repurpose ImageNet models to classify CIFAR-10 images, increasing accuracy from chance to a moderate level. The study reprogrammed neural networks to classify CIFAR-10 images using adversarial programs, increasing accuracy from chance to a moderate level. Adversarial programs show visual similarities with ResNet architecture and require minimal computation cost at inference time. The programs trained to classify CIFAR-10 images show visual similarities with ResNet architecture and possess low spatial frequency texture. Adversarial training on an Inception V3 model trained on ImageNet data is examined to understand susceptibility to adversarial reprogramming. The Inception V3 model trained on ImageNet data using adversarial training BID42 is vulnerable to reprogramming, despite a slight reduction in attack success. Standard approaches to adversarial defense are ineffective against this type of attack. The Inception V3 model trained on ImageNet data is vulnerable to reprogramming, with little impact on attack success. Standard defense methods are ineffective against this type of attack due to differences in goals and magnitude of adversarial programs. Defense methods may not generalize to data from the adversarial task. Adversarial reprogramming attacks were conducted on models with random weights, using the same setup as in Section 4.2 but with ImageNet models with randomly initialized weights. Training on MNIST classification was challenging for random networks, resulting in lower accuracy compared to pretrained networks. The study used ImageNet models with randomly initialized weights for the MNIST classification task. Training random networks was challenging and generally led to lower accuracy compared to pretrained networks. Only ResNet V2 50 achieved similar accuracy to trained ImageNet models. Adversarial programs generated with random networks were qualitatively different from those with pretrained networks, highlighting the importance of the original task the neural networks perform. The study used ImageNet models with randomly initialized weights for the MNIST classification task. Adversarial programs obtained with pretrained networks were different from those with random networks, showing the importance of the original task performed by the neural networks. Randomly initialized networks may perform poorly due to issues like poor scaling of network weights at initialization. The study explored the potential richness of pretrained ImageNet models in adversarial reprogramming. Randomly initialized networks may perform poorly due to weight scaling issues. Adversarial reprogramming relies on similarities between original and adversarial data, which was tested by randomizing pixels on MNIST digits to remove resemblance to ImageNet images. The study randomized pixels on MNIST digits to remove resemblance to ImageNet images and reprogrammed pretrained ImageNet networks to classify the shuffled MNIST digits with almost equal accuracy to standard MNIST. Reprogramming neural networks to classify shuffled CIFAR-10 images was also successful. Reprogramming neural networks to classify shuffled CIFAR-10 images resulted in decreased accuracy compared to standard CIFAR-10 images, as the convolutional structure of the network was not useful for classifying the shuffled images. However, the accuracy was comparable to that of fully connected networks. This suggests that transferring knowledge between original and adversarial data does not fully explain susceptibility to adversarial reprogramming. The results suggest that transferring knowledge between original and adversarial data does not completely explain susceptibility to adversarial reprogramming. There is a possibility of reprogramming across tasks with unrelated datasets and domains, with the potential to limit the visibility of adversarial perturbations by adjusting program size, scale, or concealing the task entirely. In experiments with an Inception V3 model, adversarial reprogramming was successful even with a small program size, resulting in lower accuracy. The visibility of adversarial perturbations can be limited by adjusting program size, scale, or concealing the task entirely. Adversarial reprogramming remains successful with smaller programs and imperceptible perturbations, even when concealed within normal images. Adversarial reprogramming can be achieved by hiding both the adversarial data and program within a normal ImageNet image, using shuffled pixels and limiting the scale of the data and program. We extended our reprogramming method by adding the resulting image to a random ImageNet image. The adversarial program was optimized for the network to classify MNIST digits, resulting in adversarial images similar to normal ImageNet images. The study optimized an adversarial program to reprogram the network to classify MNIST digits using a simple shuffling technique and an ImageNet image. This resulted in adversarial images similar to normal images, demonstrating the possibility of hiding the adversarial task effectively. Further complexity in hiding the task and optimizing image choice could make the reprogramming more effective and harder to detect. Trained neural networks are more vulnerable to adversarial reprogramming than random networks. Reprogramming remains successful even when the data structure differs significantly from the original task, showcasing the flexibility of repurposing trained weights. This suggests that reusing neural circuits dynamically is feasible in modern artificial neural networks. The flexibility of repurposing trained weights for new tasks in neural networks is demonstrated by the ability to successfully reprogram even when the data structure is different. This suggests that dynamic reuse of neural circuits is feasible, leading to more efficient and flexible machine learning systems. The work in machine learning has focused on building large dynamically connected networks with reusable components. It is unclear whether the reduced performance when targeting random networks or reprogramming for CIFAR-10 classification was due to limitations in expressivity of adversarial perturbation or optimization task difficulty. Future research could explore adversarial reprogramming in other domains like audio, video, or text. Future research could explore adversarial reprogramming in various domains such as audio, video, and text, to investigate the potential for reprogramming across different tasks. Reprogramming trained networks to classify shuffled images suggests that cross-domain reprogramming is feasible. Adversarial reprogramming of recurrent neural networks, especially those with attention or memory, could be particularly intriguing due to their Turing completeness. This would require finding inputs that induce reprogramming. Reprogramming across domains is possible, especially in recurrent neural networks (RNNs) with attention or memory. By finding inputs that induce the RNN to perform simple operations like incrementing or decrementing counters, changing input attention location, adversarial programs can be composed to reprogram the RNN for any computational task. This could lead to achieving nefarious goals through specially crafted inputs. Adversarial programs can reprogram RNNs to perform any task, leading to potential theft of computational resources. For example, a program could make a computer vision classifier solve image captchas for creating spam accounts. This poses a major danger beyond theft as adversaries could repurpose RNNs for arbitrary tasks. Adversarial attacks can reprogram neural networks to perform unauthorized tasks, potentially leading to theft of computational resources and ethical violations by ML service providers. This new class of attacks demonstrates the ability to repurpose RNNs for novel adversarial tasks. The study proposed a new class of adversarial attacks to reprogram neural networks for novel tasks, revealing their surprising flexibility and vulnerability. Future research should focus on understanding and mitigating these attacks. The study introduced adversarial reprogramming as a new attack on neural networks, demonstrating their vulnerability. Adversarial data is used to reprogram neural networks for unrelated tasks, highlighting the need for defense mechanisms. The study introduced adversarial reprogramming as a new attack on neural networks, demonstrating their vulnerability. Adversarial data is used to reprogram neural networks for unrelated tasks, highlighting the need for defense mechanisms. The shuffled image combined with the adversarial program successfully reprograms Inception V3 model to classify shuffled digits, despite the data being unrelated to the original ImageNet data."
}