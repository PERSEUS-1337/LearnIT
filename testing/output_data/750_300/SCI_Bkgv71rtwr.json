{
    "title": "Bkgv71rtwr",
    "content": "Unsupervised domain adaptation has gained attention for handling scenarios where the source and target domains have different categories. This paper proposes augmenting Self-Ensembling with category-agnostic clusters in the target domain to address the open-set situation where unknown classes are present. In this paper, the authors propose Self-Ensembling with Category-agnostic Clusters (SE-CC) to improve domain adaptation by incorporating category-agnostic clusters in the target domain. This approach aims to address the presence of unknown classes in the open-set scenario, providing domain-specific visual cues for better generalization. SE-CC utilizes clustering to reveal target domain structure and enhance representation with mutual information maximization. SE-CC enhances representation by matching assignment distribution to cluster distribution and maximizing mutual information. Experiments on Office and VisDA datasets show superior results compared to state-of-the-art approaches in domain adaptation using Convolutional Neural Networks. Convolutional Neural Networks (CNNs) have advanced vision technologies to new state-of-the-arts, relying on large amounts of annotated data for training. However, manual labeling can be costly and labor-intensive. One solution is to reuse pre-trained models from a source domain for new domains, but this often results in a drop in performance due to \"domain shift.\" Unsupervised domain adaptation, utilizing labeled source samples and unlabeled target samples, can help mitigate this issue. One way to address the drop in performance on new domains is through unsupervised domain adaptation, which leverages labeled source samples and unlabeled target samples to generalize a target model. Existing models often struggle in open-set scenarios due to the unrealistic assumption of shared categories between domains. Existing models in domain adaptation struggle in open-set scenarios due to the unrealistic assumption of shared categories between domains, hindering generalization to distinguish unknown target samples from known ones. The difficulty lies in how to classify known target samples correctly while identifying unknown ones and learning a hybrid network for both closed-set and open-set domain adaptation. Domain adaptation addresses two main challenges: distinguishing unknown target samples from known ones during classification and learning a hybrid network for closed-set and open-set domain adaptation. One approach is to use a binary classifier to label target samples as known or unknown, discarding the unknown samples as outliers during adaptation. However, this method does not fully exploit the data structure as unknown samples are grouped together as a single generic class. In domain adaptation, unknown target samples are often discarded during adaptation from source to target, limiting the performance of binary classification. To address this, clustering is used to explicitly model the diverse semantics of both known and unknown classes in the target domain. Target samples are decomposed into clusters to better utilize the data structure. In domain adaptation, clustering is used to model the diverse semantics of known and unknown classes in the target domain. Target samples are decomposed into clusters to enhance data utilization and improve classification performance. In domain adaptation, clustering is utilized to model the diverse semantics of known and unknown classes in the target domain. Steering domain adaptation with category-agnostic clusters aims to make representations domain-invariant for known classes and discriminative for unknown and known classes in the target domain. Self-Ensembling with an additional clustering branch refines representations by estimating assignment distribution over all clusters for each target sample, preserving the target domain's inherent structure. The new approach, Self-Ensembling with Category-agnostic Clusters (SE-CC), implements clustering to decompose target samples into category-agnostic clusters. The new approach, Self-Ensembling with Category-agnostic Clusters (SE-CC), integrates clustering to decompose target samples into clusters, preserving the target domain's structure. An additional clustering branch in the student model predicts cluster assignment distribution for each target sample, refining representations. The SE-CC approach integrates clustering into the student model of Self-Ensembling to predict cluster assignment distribution for target samples. KL-divergence is used to minimize the mismatch between estimated and inherent cluster distributions, preserving data structure in the target domain. Mutual information is maximized among input features, output classification, and cluster assignment distributions in the student model. The SE-CC framework enforces the learnt feature to preserve the data structure in the target domain. It maximizes mutual information among input features, output classification, and cluster assignment distributions in the student model. Unsupervised domain adaptation involves learning transferrable features in CNNs by minimizing domain discrepancy through Maximum Mean Discrepancy (MMD). Unsupervised domain adaptation in closed-set scenario involves learning transferrable features in CNNs by minimizing domain discrepancy through Maximum Mean Discrepancy (MMD). Early works integrated MMD into CNNs to achieve domain invariant representation. Another approach is to encourage domain confusion across different domains via a domain discriminator to predict the domain of each input sample. In unsupervised domain adaptation, domain confusion is encouraged through a domain discriminator to predict the source/target domain of input samples. This is achieved by enforcing a domain confusion loss in the domain discriminator to make the learned representation domain invariant. Ganin & Lempitsky (2015) treat domain confusion as a binary classification task and use a gradient reversal algorithm to optimize the domain discriminator. Open-set domain adaptation extends traditional domain adaptation to handle scenarios where the target domain includes numerous unknown classes. In open-set domain adaptation, the target domain includes unknown classes not present in the source domain. Panareda Busto & Gall (2017) and Saito et al. (2018b) use different approaches to tackle this scenario, with the latter utilizing adversarial training for feature representation learning. In open-set domain adaptation, various approaches have been used to handle the inclusion of unknown classes in the target domain. Busto et al. exploit target sample assignments as known/unknown classes, Saito et al. utilize adversarial training for feature representation learning, and Baktashmotlagh et al. factorize the data into shared and private subspaces to model known and unknown target samples. In open-set domain adaptation, approaches include modeling target samples from known and unknown classes in shared and private subspaces. Conditional entropy and self-ensembling loss are used to align classification predictions between teacher and student models. Clustering is performed on unlabeled target samples to create category-agnostic clusters for improved adaptation. In open-set domain adaptation, clustering is used to decompose unlabeled target samples into category-agnostic clusters. An additional clustering branch is integrated into the student model to infer cluster assignments for each target sample, aligning the distribution with the original clusters to preserve the data structure in the target domain. SE-CC utilizes unlabeled target samples to align cluster assignment distribution with original clusters, preserving data structure in target domain. It enhances student feature representation by maximizing mutual information among feature map, classification, and cluster assignment distributions. SE-CC leverages category-agnostic clusters for representation learning, preserving the target data structure during domain adaptation. This enables effective alignment of sample distributions within known and unknown classes, and discrimination of samples between known and unknown classes. The preservation of the target data structure during domain adaptation allows for effective alignment of sample distributions within known and unknown classes, as well as discrimination of samples between known and unknown classes. This preservation is represented as a cluster probability distribution and is utilized to enhance representation learning by maximizing mutual information among input features, clusters, and class probability distributions. This approach explores the benefits of category-agnostic clusters for open-set domain adaptation, a novel concept not fully studied before. In this paper, the SelfEnsembling with Category-agnostic Clusters (SE-CC) model is introduced for open-set domain adaptation. It integrates category-agnostic clusters into the domain adaptation procedure, utilizing labeled samples in the source domain and unlabeled samples in the target domain belonging to N classes. The SE-CC model in open-set domain adaptation aims to learn domain-invariant representations and classifiers for recognizing N-1 known classes in the target domain while distinguishing unknown target samples. The method of Self-Ensembling is briefly recalled. The goal of open-set domain adaptation is to learn domain-invariant representations and classifiers for recognizing known classes in the target domain while distinguishing unknown samples. Self-Ensembling method is based on Mean Teacher for semi-supervised learning, encouraging consistent classification predictions between teacher and student models. Self-Ensembling aims to ensure consistent classification predictions between teacher and student models by penalizing differences in classification predictions under small perturbations of input images. The self-ensembling loss penalizes the difference between classification predictions of student and teacher models, with the teacher's weights updated as an exponential moving average of the student's weights. Additionally, the unsupervised conditional entropy loss is adopted to train the student's classification branch, aiming to move decision boundaries away from high-density regions in the target data. The overall training loss of Self-Ensembling includes supervised cross entropy loss on source data, unsupervised self-ensembling loss, and conditional entropy loss on unlabeled target data, balanced with tradeoff parameters. Open-set domain adaptation is more challenging as it requires classifying both inliers and outliers into known categories. The self-ensembling loss (L SE ) and conditional entropy loss (L CDE ) of unlabeled target data are balanced with tradeoff parameters (\u03bb 1 and \u03bb 2 ). Open-set domain adaptation is more challenging as it requires classifying inliers and outliers into known and unknown classes. A typical approach involves learning a binary classifier to recognize each target sample as a known or unknown class, but this oversimplifies the problem by assuming all unknown samples belong to one class. This approach may not be robust when unknown samples span multiple unknown classes. To address the limitations of oversimplified approaches in open-set domain adaptation, clustering is used to model diverse semantics in the target domain. This involves creating category-agnostic clusters that are integrated into Self-Ensembling for guiding domain adaptation. An additional clustering branch is designed in the student of Self-Ensembling to align cluster assignment distribution with the inherent cluster distribution. In open-set domain adaptation, clustering is utilized to create category-agnostic clusters integrated into Self-Ensembling. An additional clustering branch aligns cluster assignment distribution with the inherent cluster distribution, enforcing domain-invariant feature representations for known classes and more discriminative features for unknown and known classes in the target domain. Clustering is used in open-set domain adaptation to group unlabeled data in the target domain. K-means is employed to decompose target samples into clusters, revealing underlying structure tailored to the target domain. The obtained clusters, though category-agnostic, help in identifying samples with similar semantics. In open-set domain adaptation, clustering with K-means decomposes target samples into category-agnostic clusters to reveal underlying structure. Each target sample is represented by CNN output features pre-trained on ImageNet for clustering. The joint relations between target samples and clusters encode the inherent cluster distribution. Periodically refreshing clusters did not significantly impact the results. The joint relations between target samples and category-agnostic clusters are encoded by measuring the inherent cluster distribution through cosine similarities. The centroid of each cluster is defined as the cosine similarity between the sample and the cluster centroid. Periodically refreshing clusters did not have a major impact on the results. The clustering branch in the student model predicts the distribution over all category-agnostic clusters for cluster assignment of each target sample. The centroid of each cluster is the average of all samples belonging to that cluster. The clustering branch in the student model predicts cluster assignment distribution for each target sample x S t using a modified softmax layer. The cluster assignment parameter matrix W is used in the process. The clustering branch in the student model predicts cluster assignment distribution for each target sample using a modified softmax layer with the parameter matrix W. A KL-divergence loss is defined to measure the mismatch between the estimated cluster assignment distribution and the inherent cluster distribution, enforcing the learnt representation to preserve the data structure of the target domain. The KL-divergence loss enforces the learnt representation to preserve the data structure of the target domain and incorporates inter-cluster relationships as a constraint. This constraint ensures that the cluster assignment parameter matrices of semantically similar clusters are similar. The KL-divergence loss with inter-cluster relationships constraint is formulated to ensure similarity between parameter matrices of semantically similar clusters. In a multi-task paradigm, the student in SE-CC produces classification and cluster assignment distributions for a target sample. Mutual Information Maximization is used to strengthen the learned target feature in an unsupervised manner. In a multi-task paradigm, the student in SE-CC produces classification and cluster assignment distributions for a target sample. Mutual Information Maximization (MIM) is utilized to enhance the learned target feature in an unsupervised manner by maximizing mutual information among input features and output distributions. A MIM module is designed in the student to estimate and maximize local and global mutual information among input feature maps, output classification distributions, and cluster assignment distributions. The MIM module in the student model aims to estimate and maximize local and global mutual information among input feature maps, output classification distributions, and cluster assignment distributions. Global mutual information is encoded from the output feature map of the last convolutional layer in the student model, enhancing the learned target feature in an unsupervised manner. The global feature vector G(x is encoded from the feature map using convolutional and pooling layers. It is then concatenated with classification and cluster assignment distributions for input into the Mutual information discriminator to assess alignment. The global Mutual information discriminator uses a fully-connected network with nonlinear activation to assess alignment of the input global feature vector with classification and cluster assignment distributions. The final output score represents the probability of discriminating the real input feature. The global Mutual Information is estimated using the Jensen-Shannon MI estimator. The global Mutual Information is estimated via Jensen-Shannon MI estimator, using softplus function and global feature of a different target image. Local Mutual Information is also utilized among local input features, output classification, and cluster assignment distributions. Feature maps are constructed and concatenated with the input feature map along the channel. The local Mutual Information discriminator utilizes stacked convolutional layers to discriminate input local features based on classification and cluster assignment distributions. The final output score map is generated to determine feature matching. The local Mutual Information discriminator is constructed with three stacked convolutional layers to match local features with given distributions. The final output score map determines the probability of discriminating real input features. The Mutual Information is estimated accordingly, and the objective for the module combines local and global Mutual Information estimations with a tradeoff parameter \u03b1. The SE-CC model integrates local and global Mutual Information estimations with tradeoff parameters in its final objective, along with other loss functions for training. The process of estimating Mutual Information is depicted in Appendix A. The effectiveness of the SE-CC model is empirically verified. The SE-CC model combines various loss functions, including unsupervised self-ensembling and Mutual Information estimation, with tradeoff parameters \u03bb 3 and \u03bb 4. Experiments are conducted on the Office Saenko et al. VisDA dataset, which consists of 280k images from three domains: synthetic, real, and video frames. The study uses images from three domains: synthetic images from 3D CAD models for training, real images from COCO dataset for validation, and video frames from YTBB for testing. The synthetic images are used as the source and COCO images as the target for evaluation in open-set adaptation. In open-set adaptation, the study evaluates the accuracy of known and unknown classes in the target domain using three metrics: Knwn for known classes, Mean for known & unknown classes, and Overall for all target samples. Closed-set adaptation focuses on the accuracy of all 12 classes for adaptation. For closed-set adaptation, the accuracy of all 12 classes is reported. ResNet152 is used as the backbone for clustering and adaptation in both closed-set and open-set scenarios. The performance of different models on Office for open-set adaptation is shown in Table 1. AODA adopts a different open-set setting without unknown source samples for fair comparison. SE-CC \u2666, a variant of SE-CC, is compared with AODA in open-set domain adaptation on Office. SE-CC \u2666 learns classifiers without unknown source samples, recognizing only N-1 known classes. Results consistently show SE-CC's effectiveness across two metrics. Our SE-CC model outperforms other closed-set and open-set adaptation methods on various transfer directions, especially on challenging transfers like D \u2192 A and W \u2192 A. The results highlight the advantage of leveraging target data in improving classification accuracy. The SE-CC model improves classification accuracy on challenging transfers like D \u2192 A and W \u2192 A by exploiting target data structure for domain adaptation. This design ensures domain invariance for known classes while being discriminative for segregating target samples from known and unknown classes. RTN and RevGrad outperform Source-only by aligning data distributions between source and target domains. The SE-CC model improves classification accuracy by exploiting target data structure for domain adaptation, ensuring domain invariance for known classes and segregating target samples from known and unknown classes. Open-set adaptation techniques outperform RTN and RevGrad by rejecting unknown target samples as outliers and aligning data distributions only for inliers. The SE-CC model enhances classification accuracy in domain adaptation by utilizing target data structure, ensuring domain invariance for known classes, and segregating target samples. AODA, ATI-\u03bb, and FRODA outperform RTN and RevGrad in open-set scenario by excluding unknown target samples. However, SE-CC is superior as it injects category-agnostic clusters as a constraint for feature learning and alignment. Further experiments in closed-set scenario validate the effectiveness of SE-CC. In closed-set domain adaptation experiments on Office and VisDA datasets, SE-CC outperforms other techniques by leveraging category-agnostic clusters to exploit target data structure, even without unknown samples. The results show the advantage of using category-agnostic clusters in closed-set domain adaptation, even without unknown samples. Ablation study investigates the impact of each design in SE-CC on overall performance, including Conditional Entropy and KL-divergence Loss. The study explores entropy loss, KL-divergence Loss, and Mutual Information Maximization in SE-CC for open-set domain adaptation, showing performance improvements on VisDA. In SE-CC, different designs like CE, KL, and MIM contribute to performance improvements in open-set domain adaptation on VisDA. CE enhances classifier for target domain, increasing Mean accuracy from 65.2% to 66.3%. KL and MIM designs lead to a total performance boost of 4.2% in Mean metric. The study presents Self-Ensembling with Category-agnostic Clusters (SE-CC) for domain adaptation, showing a performance gain of 4.2% in Mean metric by utilizing CE, KL, and MIM designs. The results validate the effectiveness of exploiting target data structure and mutual information maximization for open-set adaptation. The study introduces Self-Ensembling with Category-agnostic Clusters (SE-CC) for domain adaptation, focusing on separating unknown target samples from known ones and integrating category-agnostic clusters into the network. This involves clustering target samples, aligning cluster assignments, and preserving learned features. The study introduces Self-Ensembling with Category-agnostic Clusters (SE-CC) for domain adaptation, focusing on aligning cluster assignments and preserving learned features in target domain. Experiments on Office and VisDA show performance improvements compared to state-of-the-art techniques. The implementation of SE-CC is developed with PyTorch and optimized with SGD. Learning rate, mini-batch size, and training iterations are set for experiments on Office and VisDA. Global and local mutual information estimation frameworks are illustrated in Figure 3. The weights are optimized with SGD using a learning rate of 0.001 and a mini-batch size of 56. Training iterations are set at 300 and 25 epochs for Office and VisDA, respectively. The global feature dimension is set at 128/1,024 for AlexNet/ResNet. Cluster number, tradeoff parameters, and tuning details are specified for open-set and closed-set adaptation tasks. The number of clusters (K) is determined using Gap statistics method, with fixed \u03bb 1 = 10. Hyper-parameter search ranges for each transfer are specified. Evaluation of Clustering Branch compares KL-divergence in SE-CC with L 1 and L 2 distance, showing results in Table 7 (a). Evaluation of Clustering Branch in SE-CC shows that KL-divergence outperforms L 1 and L 2 distance, as seen in Table 7 (a). Evaluation of Mutual Information Maximization in SE-CC involves comparing different variants of MIM module by estimating mutual information between input feature and various outputs, as shown in Table 7 (b). The evaluation of different variants of the Mutual Information Maximization (MIM) module in SE-CC shows that CLS, CLU, and CLS+CLU estimate mutual information between input features and outputs. CLS and CLU slightly improve performance by exploiting mutual information between input features and individual branch outputs, while CLS+CLU achieves even better results. The CLS and CLU modules in the MIM module of SE-CC improve performance by exploiting mutual information between input features and individual branch outputs. CLS+CLU achieves even better results by combining outputs from both branches for mutual information estimation. This demonstrates the benefit of utilizing mutual information among input features and combined outputs of downstream tasks. In the MIM module of SE-CC, the combined outputs of classification and cluster assignment tasks improve performance by aligning source and target distributions for domain-invariant representation. However, SE-CC separates unknown target samples from known samples by preserving the underlying target data structure, making known samples indistinguishable across domains."
}