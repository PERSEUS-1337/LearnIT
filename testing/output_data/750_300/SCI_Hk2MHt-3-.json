{
    "title": "Hk2MHt-3-",
    "content": "In this paper, the architecture of deep convolutional networks is explored by proposing a reconfiguration of model parameters into parallel branches at the global network level. This arrangement efficiently reduces parameters while improving performance and introducing additional regularization. The branches are tightly coupled by averaging their log-probabilities to enhance learning of better representations. The proposed \"coupled ensembles\" architecture tightens the coupling of parallel branches in neural networks by averaging their log-probabilities, leading to better representations. This approach is applicable to various neural network architectures and achieves low error rates on CIFAR-10, CIFAR-100, and SVHN tasks with a parameter budget of 25M. The \"coupled ensembles\" architecture improves neural network performance by averaging log-probabilities of parallel branches. It achieves low error rates on CIFAR-10, CIFAR-100, and SVHN tasks with a 25M parameter budget. DenseNet-BC networks with 50M parameters also show improved error rates on these tasks. The design of early convolutional architectures involved choices of hyper-parameters such as filter size and number of filters. Since the introduction of VGGNet, the design has moved towards a template with fixed filter size of 3x3 and N feature maps, down-sampling by maxpooling or strided convolutions, and doubling the computed feature maps after each down-sampling operation. This philosophy is used by state-of-the-art models like ResNet and DenseNet. Our work extends the template used by state-of-the-art models like ResNet and DenseNet, by introducing \"coupled ensembling\" where the network is divided into branches, each functioning like a complete CNN. This approach achieves comparable performance to existing models with significantly fewer parameters. The network is decomposed into branches, each functioning like a complete CNN. The proposed template achieves comparable performance to state-of-the-art models with fewer parameters. It is better to split parameters among branches rather than having a single branch. The activations of parallel branches are best combined by taking an arithmetic mean of individual log probabilities. In this paper, the concept of coupled ensembles is introduced to improve the performance of convolutional networks on CIFAR and SVHN datasets by combining activations of parallel branches through an arithmetic mean of individual log probabilities. Further ensembling of coupled ensembles leads to additional improvement, with a heavily reduced parameter count. The paper discusses related work, introduces the concept of coupled ensembles, evaluates the proposed approach, and compares it with the state of the art. The paper introduces the concept of coupled ensembles to enhance convolutional networks' performance on CIFAR and SVHN datasets by combining activations of parallel branches. It compares the proposed approach with existing methods and discusses future work. The network architecture is similar to Cire\u015fan's Neural Networks Committees and Multi-Column Deep Neural Network but differs in training a single model. The paper introduces coupled ensembles to improve convolutional networks' performance on CIFAR and SVHN datasets by combining branch activations. It differs from existing methods by training a single model with fixed parameters and combining log-probabilities of branches over target categories. Multi-branch architectures have been successful in vision applications, with recent modifications using grouped convolutions to factorize spatial and depth-wise features. The approach combines branch activations by combining log-probabilities over target categories. In contrast to recent modifications using grouped convolutions at the building block level, we propose a generic modification of the CNN structure at the global model level. This involves a template for the architecture of an \"element block\". We propose a generic modification of the CNN structure at the global model level by replicating an \"element block\" as parallel branches to form the final composite model. Shake-Shake regularization BID5 improves performance but requires more epochs for convergence compared to the base model. Our method involves a generic rearrangement of a given architecture's parameters, unlike BID5 which requires more epochs for convergence and depends on batch size. BID26 explores parallel paths in ResNet, but our approach does not require local modifications to residual blocks. Our method involves a generic rearrangement of a given architecture's parameters, leading to efficient parameter usage. Ensembling neural networks can improve model performance by combining outputs from multiple trainings with different error distributions on a per-class basis. Ensembling is a reliable technique to improve model performance by combining outputs from multiple trainings with different error distributions on a per-class basis. Our proposed model architecture involves parallel branches trained jointly, similar to the residual block in ResNet and ResNeXt. Our proposed model architecture involves parallel branches trained jointly, similar to the residual block in ResNet and ResNeXt. \"Arranging\" a given budget of parameters into parallel branches leads to an increase in performance. Additionally, classical ensembling can still be applied for the fusion of independently trained coupled ensemble models, resulting in a significant performance improvement. Snapshot ensembles are also mentioned. Our approach involves training parallel branches jointly, leading to improved performance. The ensembling approach on checkpoints during training process is efficient, resulting in higher performance with the same training time budget. However, it increases model size and prediction time significantly. Our goal is to maintain constant model size while enhancing performance. Our approach involves training parallel branches jointly to improve performance while maintaining a constant model size. The model comprises several branches, each producing a score vector for target classes. The goal is to either keep the model size constant and improve performance or achieve the same performance with a smaller model size. The model comprises multiple branches, each producing a score vector for target classes. The branches are combined using a fuse layer operation, such as taking the average of individual log probabilities. Different choices of operations for the fuse layer are explored in Section 4.4. The model combines multiple branches by averaging their individual log probabilities over target classes. Different fuse layer operations are explored for classification tasks like CIFAR, SVHN, and ILSVRC. The neural network models output a score vector of the same dimension as the target classes. Neural network models output a score vector of the same dimension as target classes, usually implemented with a fully connected (FC) layer followed by a SoftMax (SM) layer for probability distribution. Different network architectures for image classification have variations before the last FC layer. The differences among recent network architectures for image classification lie in what is present before the last FC layer. Fusion in ensemble models involves computing individual predictions separately for each model instance and averaging them. Each instance is trained independently. Fusion in ensemble models involves computing individual predictions separately for each model instance and averaging them. This is functionally equivalent to predicting with a \"super-network\" including the instances as parallel branches with an averaging layer on top. Such supernetworks are generally not implemented due to memory constraints, but the averaging layer operation can be implemented separately. Alternatively, the averaging layer can be placed just after the last FC layer. In ensemble models, fusion involves computing individual predictions for each model instance and averaging them. The model consists of parallel branches producing score vectors for target categories, fused through a \"fuse layer\" to generate a single prediction. This approach, known as coupled ensembles, explores different options to combine score vectors during training. The branch produces a score vector for target categories, which are fused through a \"fuse layer\" in the composite model to generate a single prediction. Three options are explored to combine score vectors during training and inference: Activation (FC) average, Probability (LSM) average, and Log Likelihood (LL) average. The branch produces score vectors for target categories, which are fused through a \"fuse layer\" in the composite model to generate a single prediction. Averaging the log probabilities of the target categories from multiple branches leads to improved performance with lower parameter count. The parameter vector of the composite branched model is the concatenation of parameter vectors from element blocks. The \"fuse layer\" does not contain any parameters. The proposed architecture combines score vectors from multiple branches through a \"fuse layer\" to generate predictions. The parameter vector of this composite model is the concatenation of parameter vectors from element blocks. Evaluation is done on CIFAR-10, CIFAR-100, and SVHN datasets, each with specific image sizes and category distributions. The SVHN dataset includes 73,257 training images, 531,131 \"easy\" training images, and 26,032 testing images, all with a size of 32\u00d732 pixels. Hyper parameters are set according to the original descriptions of the \"element block\" used. Input images for CIFAR-10, CIFAR-100, and SVHN are normalized during training. During training on CIFAR datasets, standard data augmentation is used, which includes random horizontal flips and random crops. For SVHN, no data augmentation is used, but a dropout ratio of 0.2 is applied in the case of DenseNet. Testing is done after normalizing the input in the same way as during training, and error rates are given in percentages as an average of the last 10 epochs. During testing on SVHN, error rates are given in percentages as an average of the last 10 epochs for DenseNet. The experiments on CIFAR-100 dataset involve DenseNet-BC with a depth of 100 and growth rate of 12. The execution times were measured using a single NVIDIA 1080Ti GPU with optimal micro-batch 2. Experiments on the CIFAR-100 dataset were conducted using a single NVIDIA 1080Ti GPU with optimal micro-batch 2. The DenseNet-BC model with a depth of 100 and growth rate of 12 was used. Comparisons were made with an ensemble of independent models, showing error rates for averaging predictions from 4 identical models trained separately. The results in Table 1 compare a jointly trained branched configuration with independent training. Averaging predictions from 4 identical models trained separately resulted in a higher test error compared to the branched configuration. Increasing parameters in a single branch model was also explored. The single branch model has a similar number of parameters as the multi-branch configuration. Increasing parameters in a single branch model results in higher test error compared to the branched configuration. The error from the multi-branch model is considerably lower than a single branch model, showing the efficiency of arranging parameters into parallel branches. The branched configuration is more efficient with increasing parameters compared to a single branch or multiple independent models. Section 4.5 analyzes the relationship between the number of branches and model performance, focusing on the best training and prediction fusion combinations for a branched model with e = 4. In this section, the performance of a branched model with different \"fuse layer\" choices is compared. Experiments evaluate training and prediction fusion combinations for a branched model with e = 4. The model is trained with fusion after the FC layer, LSM layer, or LL layer. Table 1 shows the performance of Coupled Ensembles of DenseNet-BCs (e = 4) with various \"fuse layer\" combinations versus a single branch model, measured by the top-1 error rate on the CIFAR-100 test set. The performance of different \"fuse layer\" combinations in a branched model with e = 4 is compared using the top-1 error rate on the CIFAR-100 test set. The architecture, number of branches, and type of \"fuse layer\" during training are considered, along with the error of each branch and performance during inference. Time taken for training epochs and testing each image is also analyzed. The performance of different \"fuse layer\" choices in a branched model with e = 4 is analyzed for inference. Table 1 shows the performance of models under different \"fuse layer\" operations, including models with parameters obtained using different training methods. Key observations include the branched model with e = 4 and Avg. LSM for the \"fuse layer\". The branched model with e = 4 and Avg. LSM for the \"fuse layer\" shows similar performance to a DenseNet-BC model with significantly more parameters. Coupled ensembles with LSM fusion result in lower error rates compared to training individual instances separately, indicating complementary feature learning. The coupling of \"element blocks\" in coupled ensembles with LSM fusion leads to lower error rates compared to training individual instances separately. This forces the blocks to learn complementary features and better representations. Averaging log probabilities ensures consistent updates across all branches, providing a stronger gradient signal. The error gradient back-propagated from the fuse layer is the same for all branches, promoting complementary weight actions. When training with ensemble combinations, all branches act complementarily to each other, resulting in lower error rates. Using 4 branches with a parameter budget of 3.2M reduces the error rate to 17.61 from 20.01 in the best single branch model. However, training with Avg. FC does not perform well as individual branches do not show significant improvement. Using 4 branches with a parameter budget of 3.2M reduces the error rate to 17.61 from 20.01 in the best single branch model. Training with Avg. FC prediction works quite well, outperforming Avg. SM prediction. The SM layer distorts the FC average, but Avg. FC prediction performs well, often better than Avg. SM prediction. The FC layer transmits more information as values remain spread. Experiments use Avg. LSM as the \"fuse layer\" in branched models. Optimal number of branches e is investigated for a given parameter budget on CIFAR-100 with DenseNet-BC as the \"element block\" and a parameter budget of 0.8M. In experiments, the optimal number of branches e is investigated for a given parameter budget on CIFAR-100 using DenseNet-BC as the \"element block\". Results are shown in table 3, with performance for different configurations of branches e, depth L, and growth rate k. The performance of DenseNet-BC models with different configurations of branches, depth, and growth rate was investigated on CIFAR-100. Model parameters are quantified based on specific values, making it critical to select configurations just below the target for fair comparison. In investigating DenseNet-BC models on CIFAR-100, model configurations with parameters just below the target were selected for fair comparison. The optimal configuration for the 800K parameter model was found to be e = 3, L = 70, k = 9, resulting in a decrease in error rates from 22.87 to 21.10. Using 2 to 4 branches showed a significant performance improvement over the classical single branch model. The optimal number of branches for DenseNet-BC models on CIFAR-100 is e = 3, L = 70, k = 9, resulting in a decrease in error rates from 22.87 to 21.10. Using 2 to 4 branches shows a significant performance gain over the single branch model, while 6 or 8 branches perform worse. Model performance is robust to slight variations in parameters. The DenseNet-BC model on CIFAR-100 shows improved performance with 2 to 4 branches, but 6 or 8 branches perform worse. The model is robust to slight variations in parameters, with optimal values for e = 3, L = 70, and k = 9. The gain in performance comes with increased training and prediction times due to smaller values of k. The use of smaller values of k reduces throughput for smaller models. Coupled ensembles were evaluated against existing models using DenseNet-BC and ResNet BID8 architectures. Results are compared with the current state of the art models, showing the performance of coupled ensembles. Coupled ensembles with ResNet pre-act as element block and e = 2, 4 outperform single branch models in terms of performance, despite having comparable or higher parameters. DenseNet-BC architecture was evaluated with 6 different network sizes. In section 4.7, coupled ensembles with ResNet pre-act as element block and e = 2, 4 show better performance than single branch models with similar or more parameters. DenseNet-BC architecture was tested with 6 network sizes ranging from 0.8M to 25.6M parameters. The trade-off between depth L and growth rate k is not critical for a given parameter budget. In experiments with DenseNet-BC architecture, the trade-off between depth L and growth rate k is not critical for a given parameter budget. Different configurations were tested, including single-branch and multi-branch versions with varying numbers of branches. Error rates were higher than reported by BID11 for single branch DenseNet-BC. The Torch7 and PyTorch implementations were found to be equivalent. For multi-branch versions of the DenseNet-BC model, different numbers of branches were tested, including e = 4 and e = 3, 6, 8 for the largest model. Error rates were higher than reported by BID11 for single branch DenseNet-BC. The Torch7 and PyTorch implementations were deemed equivalent, with differences possibly due to error rate measurement methods and statistical variations. Coupled ensemble models showed significantly better performance for all network sizes compared to DenseNet-BC's reported performance. The coupled ensemble models outperform DenseNet-BC for all network sizes, with error rates of 2.92% on CIFAR 10, 15.68% on CIFAR 100, and 1.50% on SVHN. These models are comparable to state-of-the-art implementations, with only the Shake-Shake S-S-I model BID5 performing slightly better on CIFAR 10. Additionally, the performance of coupled ensembles is compared to model architectures learned in a meta-learning scenario. The coupled ensemble models outperform DenseNet-BC for all network sizes on CIFAR 10, CIFAR 100, and SVHN. Only the Shake-Shake S-S-I model BID5 performs slightly better on CIFAR 10. The performance of coupled ensembles is compared to model architectures learned in a meta-learning scenario, limited by network size and training time constraints. The study compared the performance of coupled ensemble models to other model architectures on CIFAR 10, CIFAR 100, and SVHN datasets. The ensembling approach showed significant improvement with a small number of models but plateaued with larger models due to training costs. Instead of repeating the same training, they ensembled four large coupled ensemble models. The study compared the performance of coupled ensemble models on CIFAR 10, CIFAR 100, and SVHN datasets. Significant improvement was seen with a small number of models, but larger models did not show much improvement due to training costs. Ensembling four large coupled ensemble models resulted in a significant gain by fusing two models, with minimal improvement from further fusion. These ensembles outperformed all state-of-the-art implementations at the time of submission. The study compared the performance of coupled ensemble models on CIFAR 10, CIFAR 100, and SVHN datasets, outperforming all state-of-the-art implementations. The proposed approach involves replacing a single deep convolutional network with \"element blocks\" resembling standalone CNN models, showing superior performance in error rates compared to single branch models. The proposed approach involves replacing a single deep convolutional network with \"element blocks\" resembling standalone CNN models. These element blocks produce intermediate score vectors that are coupled via a \"fuse layer\" and averaged at training and test time, leading to significant performance improvement over single branch configurations. The proposed approach involves using \"element blocks\" in place of a single deep convolutional network. These blocks generate score vectors that are combined and averaged during training and testing, resulting in improved performance. This method outperforms single branch configurations, although it does lead to slightly longer training and prediction times. The approach shows the best performance within a given parameter budget, as demonstrated in tables 3 and 4, and figure 2. Additionally, the individual performance of the \"element blocks\" is enhanced compared to when they are trained independently. The proposed approach involves using \"element blocks\" instead of a single deep convolutional network, resulting in improved performance. The increase in training and prediction times is mainly due to sequential processing of branches, making data parallelism less efficient on GPUs for smaller models. To address this, data parallelism can be extended to branches through a parallel implementation of multiple 2D convolutions simultaneously. The proposed approach involves using \"element blocks\" instead of a single deep convolutional network for improved performance. Data parallelism can be extended to branches through parallel implementation of multiple 2D convolutions simultaneously. Preliminary experiments on ImageNet show that coupled ensembles have lower error compared to single branch models with the same parameter budget. The experiments (et al., 2015) demonstrate that coupled ensembles outperform single branch models with the same parameter budget. The test and train versions of networks use element blocks, allowing for the placement of an averaging layer for improved performance. The experiments show that coupled ensembles are more effective than single branch models with the same parameters. The train version allows for an averaging layer placement after the last FC layer, SM layer, or LL layer. Reusing element blocks from other groups for efficiency and meaningful comparisons is preferred. When training coupled networks, a global parameter vector W is used for all versions, with each branch defined by a parameter vector containing the same parameters as the original implementation. A dedicated script is used to split the global parameter vector into individual branch parameter vectors for training and prediction. The global parameter vector W is used for all versions in coupled networks, with a dedicated script used to split it into individual branch parameter vectors for training and prediction. This allows for combining different training and prediction conditions efficiently in the network architecture. The network architecture is determined by global hyper-parameters specifying train versus test mode, number of branches, and placement of AVG layer. Larger models may require data batches to be split into \"micro-batches\" for training. For larger models, data batches may need to be split into \"micro-batches\" with their own hyper-parameters. Gradient accumulation over micro-batches helps approximate the equivalent gradient of processing data as a single batch, except for BatchNorm layer considerations. When processing data in micro-batches, BatchNorm layer uses batch statistics for normalization, which may differ from using whole batch statistics. However, this difference is not significant in practice. Parameter updates are done using batch gradients while forward passes are done with micro-batches for optimal throughput. In the single branch case, memory requirement depends on network depth and batch size. Using micro-batches helps adjust memory usage while maintaining default batch size. Multi-branch version doesn't need more memory if branch width is controlled. The multi-branch version does not require more memory if the branches' width is kept constant. Memory is only needed if the branches' width is reduced. Hyper-parameter search experiments showed that reducing both width and depth was the best option for optimal performance. In practice, for \"full-size\" experiments with 25M parameters, training was done within 11GB memory of GTX 1080 Ti using micro-batch sizes of 16 for single-branch versions and 8 for multi-branch ones. Splitting the network over two GPU boards allows for doubling the micro-batch sizes, but this does not significantly increase speed or improve performance. Splitting the network over two GPU boards allows for doubling the micro-batch sizes, but this does not significantly increase speed or improve performance. Using only two branches provides a significant gain over a single branch architecture of comparable size. Using only two branches in coupled ensembles provides a significant gain over a single branch architecture of comparable size. The performance remains stable against variations in depth and growth rate, with the combination of (L = 82, k = 8, e = 3) predicted to be the best on the test set. The experiment on a validation set with a 40k/10k random split of the CIFAR-100 training set confirmed that the (L = 82, k = 8, e = 3) combination should be the best one on the test set. The (L = 70, k = 9, e = 3) combination showed slightly better results, but the difference is likely not statistically significant. Comparing parameter usage and performance of branched coupled ensembles with model architectures recovered using meta learning techniques revealed challenges in reproducibility and statistical analysis. Parameter usage and performance of branched coupled ensembles with model architectures recovered using meta learning techniques face challenges in reproducibility and statistical significance. Sources of variation include framework implementation (Torch7 and PyTorch), random seed for network initialization, and CuDNN non-determinism during training. Sources of variation in performance measures include framework implementation with Torch7 and PyTorch, random seed for network initialization, and CuDNN non-determinism during training. Fluctuations in batch normalization can also impact results, even with learning rate, SGD momentum, and weight decay set to 0. The influence of fluctuations in batch normalization on training outcomes is consistent regardless of hyper-parameter settings. The choice between the model from the last epoch or the best performing model impacts evaluation results, with dispersion expected despite considerations like numerical determinism and epoch sampling. The dispersion of evaluation measures due to random initialization in neural networks is confirmed to be relatively small but not negligible, with different random seeds leading to different local minima. This dispersion is expected despite considerations like numerical determinism, Batch Norm moving average, and epoch sampling. The dispersion of evaluation measures in neural networks due to random initialization is relatively small but not negligible, complicating comparisons between methods. Statistical significance tests may not be helpful as differences can be observed between models with different seeds, even when everything else is kept equal. Statistical significance tests are not useful due to differences between models with different seeds, even when everything else is the same. Experiments show the dispersion in a moderate scale model, as larger models cannot afford many trials. The relative importance of different effects was quantified for DenseNet-BC with L = 100, k = 12 on CIFAR 100. Results are shown in table 8 for four groups of three rows. In the study of DenseNet-BC with L = 100, k = 12 on CIFAR 100, different effects were quantified. Results in table 8 show four groups of three rows with various experimental configurations. These configurations include Torch7 vs PyTorch and using the same seed vs different seeds. Performance measures such as error rates at the last epoch, average error rates of the last 10 epochs, and error rates of the model with the lowest error rate were considered. The study analyzed the error rates of DenseNet-BC with L = 100, k = 12 on CIFAR 100. Different experimental configurations were compared, including Torch7 vs PyTorch and using the same seed vs different seeds. Performance measures included error rates at the last epoch, average error rates of the last 10 epochs, and error rates of the model with the lowest error rate. The analysis presented minimum, median, maximum, mean\u00b1standard deviation over 10 measures from 10 identical runs. Additionally, the root mean square of the standard deviation of fluctuations on the last 10 epochs was calculated. The study compared error rates of DenseNet-BC with L = 100, k = 12 on CIFAR 100 using Torch7 and PyTorch, and same seed vs different seeds. Observations showed no significant differences between implementations or seed usage in terms of error rates. The study compared error rates of DenseNet-BC with L = 100, k = 12 on CIFAR 100 using Torch7 and PyTorch, and found no significant differences between implementations or seed usage in terms of error rates. The dispersion observed using different seeds implies results cannot be exactly reproduced. There is no significant difference between means computed on the single last epoch and means computed on the last 10 epochs. The standard deviation of measures is slightly smaller when computed on the last 10 epochs, reducing fluctuations. The mean of the measures computed on the last 10 epochs is significantly lower when taken at the best epoch compared to the single last epoch or the last 10 epochs. Averaging over the last 10 epochs reduces fluctuations due to batch normalization and random fluctuations in the final learning steps. The mean of measures computed on 10 runs is lower at the best epoch compared to the single last epoch or last 10 epochs. Choosing the minimum error rate for all models during training is not practical without looking at results for best reproducibility and fair comparisons. Choosing the minimum error rate for all models during training is not practical without looking at results for best reproducibility and fair comparisons. Using the error rate at the last iteration or at the 10 last iteration does not seem to make a difference in the mean but the standard deviation is smaller for the latter, therefore this one should be preferred. In experiments, using the error rate at the last 10 epochs is preferred for more robust estimation. The number of epochs used for evaluation can vary without significant impact on results. In experiments, using the average error rate from the last 10 epochs is recommended for robust estimation. For SVHN experiments, the last 4 iterations were used due to fewer but larger epochs. These observations suggest that using the average error rate from the last epochs leads to more reliable results. In this study, comparisons between single-branch and multi-branch architectures have shown a clear advantage for multi-branch networks under a constant parameter budget. However, the training time for multi-branch networks is currently longer than for single-branch networks. The study compares single-branch and multi-branch networks under a constant parameter budget. Multi-branch networks have longer training times but may still improve over single-branch networks with better parallelization. Ways to reduce training time include fewer iterations, fewer parameters, or increased width with reduced depth. The study compares single-branch and multi-branch networks under a constant parameter budget. Ways to reduce training time include fewer iterations, fewer parameters, or increased width with reduced depth. Results for different options are shown for CIFAR 10 and 100 datasets. The study compares single-branch and multi-branch networks under a constant parameter budget. Options include reducing training epochs, depth, or matching parameter count and training time with different configurations of DenseNet-BC. Results are shown for various DenseNet-BC configurations on CIFAR 10 and 100 datasets. Results are shown for various DenseNet-BC configurations on CIFAR 10 and 100 datasets, including options that match parameter count and training time with different configurations. Despite performing slightly worse than the full multi-branch baseline, these options still outperform the single-branch baseline. DenseNet-BC L = 88, k = 20, e = 4 performs better than the single-branch baseline with reduced parameters and training time. In this section, the performance of single branch models and coupled ensembles is compared in a low training data scenario. A single branch model and multi-branch coupled ensemble, with the same number of parameters, are trained on STL-10 and a 10K balanced random subset of CIFAR-100. Each class in STL-10 has 500 training images. DenseNet-BC L = 88, k = 20, e = 4 outperforms the single-branch baseline with reduced parameters and training time. The study compared the performance of single branch models and multi-branch coupled ensembles with the same parameters on STL-10 and a subset of CIFAR-100. Coupled ensembles outperformed single branch models within a fixed parameter budget. Preliminary experiments on ILSVRC2012 also showed the superiority of coupled ensembles. For preliminary experiments on ILSVRC2012, single-branch and multi-branch coupled ensembles were compared using images of size 256\u00d7256 due to constraints. Data augmentation included random flips and crops of size 224\u00d7224. A DenseNet-169-k32-e1 single-branch model was used as a baseline, compared to a DenseNet-121-k30-e2 coupled ensemble. Despite not being state-of-the-art, this was the strongest possible baseline given the constraints. Further experiments will be conducted with full-sized images and increased data augmentation. The experiments compared DenseNet-169-k32-e1 with a coupled ensemble DenseNet-121-k30-e2, showing significant improvement with two branches despite constraints. Future experiments with full-sized images and increased data augmentation are planned, with current results displayed in table 11."
}