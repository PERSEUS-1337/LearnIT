{
    "title": "ByEtPiAcY7",
    "content": "Knowledge extraction techniques aim to make neural networks more understandable by converting them into symbolic descriptions. The challenge is to find explanations that are simpler than the original model but still accurate. Some believe that the complex nature of deep networks makes it impossible to explain their hidden features in a way that humans can understand. However, this paper proposes a knowledge extraction method using \\textit{M-of-N} rules to address this issue. The paper proposes a knowledge extraction method using \\textit{M-of-N} rules to map the complexity/accuracy landscape of rules describing hidden features in a Convolutional Neural Network (CNN). Experiments show an optimal trade-off between comprehensibility and accuracy, with each latent variable having an optimal \\textit{M-of-N} rule. The landscape of the CNN reveals an optimal trade-off between comprehensibility and accuracy, with each latent variable having an optimal \\textit{M-of-N} rule. Rules in the first and final layer are highly explainable, while those in the second and third layer are less so. This sheds light on rule extraction from deep networks and the value of decompositional knowledge extraction for explainability in AI. Recently, there has been a growing interest in explainable Artificial Intelligence (AI) due to the lack of explainability in neural network models, particularly deep networks. These models rely on distributed representations, making them less explainable compared to symbolic AI or symbolic Machine Learning. The focus is on rule extraction from deep networks and the importance of decompositional knowledge extraction for explainability in AI. Knowledge extraction aims to enhance the explainability of neural networks by revealing the implicit knowledge learned in distributed representations, which may not be easily identifiable by humans. Knowledge extraction aims to increase the explainability of neural networks by uncovering the implicit knowledge learned in their weights. Rule extraction techniques, similar to those in symbolic AI, ML, and logic programming, have been developed over the years to translate trained neural networks into symbolic rules or decision trees. Rule extraction techniques have been developed over the years in AI, ML, and logic programming to generate rules from neural networks, either through decompositional or pedagogical approaches. The complexity of the extracted rules remains a major issue despite the possibility of describing neural networks with symbolic systems. The complexity of rule extraction from neural networks remains a major issue due to the distributed representations found in neural networks, making it challenging to derive comprehensible rules even if symbolic systems can describe them. The distributed representations in neural networks, particularly through decompositional methods, involve patterns of activity over many neurons. This distributed nature is crucial for the capabilities of neural networks and has led to the conclusion that explaining latent features with symbolic knowledge extraction is not effective. Instead, methods like distillation are recommended. In connectionism, explaining latent features with symbolic knowledge extraction is seen as ineffective. Distillation is suggested as a better method, although its efficacy is debated. Some approaches focus on guarantees of network behavior or visualizations for individual classifications rather than understanding the learned model. In this paper, a method is developed to examine the explainability of latent variables in neural networks by using rule extraction to analyze the behavior of the network. The approach involves searching through a space of rules to describe a latent variable, measuring error and complexity, and mapping out a landscape of rule extraction trade-offs. The method developed in this paper examines the explainability of latent variables in neural networks through rule extraction. It involves measuring error and complexity to map out a landscape of rule extraction trade-offs. When applied to a 4-layer CNN trained on fashion MNIST, some layers have accurate rules while others do not, even with complex rules. A 'critical point' on the rule extraction landscape indicates an ideal M-of-N rule for each latent variable, with rule accuracy depending on the variable being described. The discovery of a 'critical point' on the rule extraction landscape reveals an ideal M-of-N rule for each latent variable in neural networks. The accuracy of rules varies depending on the variable, with explainability trends differing between layers and architectures. Convolutional layers extract more complex rules compared to fully connected layers. The rules extracted from convolutional layers were more complex than those from fully connected layers. Rules from the first and final layers had near 0% error, while those from the second and third layers had up to 15% error. The paper also provides an overview of previous algorithms for knowledge extraction and defines accuracy and complexity for M-of-N rules. Experimental results of the rule extraction process are presented in Section 4. One of the first attempts at knowledge extraction used a decompositional approach applied to feedforward networks, specifically the Knowledge-based Artificial Neural Networks (KBANN) algorithm. It extracted symbolic rules based on the activation of a set of neurons, concluding with experimental results on the accuracy/complexity landscape. The KBANN algorithm extracted symbolic rules from feedforward networks based on the activation of a set of neurons. More recent algorithms generate binary trees representing M-of-N rules, selecting rules using input units as concepts. The more recent algorithms generate binary trees representing M-of-N rules based on maximum information gain with respect to the output. These methods treat the model as a black box and can be queried to generate data for rule extraction. Other extraction methods combine pedagogical and decompositional approaches, while some abandon knowledge extraction altogether. The curr_chunk discusses various methods for rule extraction from neural networks, including eclectic approaches combining pedagogical and decompositional methods, as well as visually oriented techniques. Some techniques focus on input/output relationships rather than explaining the latent features of deep networks. The curr_chunk discusses the challenges of applying decompositional rule extraction techniques to deep neural networks with multiple hidden layers, resulting in complex rules that are difficult for humans to understand. The curr_chunk explores the potential for extracting explainable rules from certain layers of deep neural networks, despite the challenges of complex rule hierarchies. This suggests that rule extraction could be used to provide modular explanations for network behavior. The curr_chunk discusses the potential of extracting explainable rules from deep neural networks, which could provide insights into the network's behavior and latent features. It introduces the concept of logical rules in programming for this purpose. In logic programming, a rule is defined as an implication A \u2190 B, where A is the head and B is the body. Disjunctions in the body can be represented by multiple rules with the same head. When explaining a neural network using rules, the literals correspond to neuron states, with True for 1 and False for 0. In neural networks, literals represent neuron states, with True for 1 and False for 0. For neurons with continuous activation values, a literal can be defined by including a threshold. Latent variables in neural networks are not well described by a single conjunctive rule due to various input configurations activating a neuron. In neural networks, a latent variable is often not well described by a single conjunctive rule. M-of-N rules are used to soften the constraint on logical rules by requiring only M variables to be true out of N. M-of-N rules provide a more general and compact representation than conjunctions in neural networks. They require only a subset of variables to be true, reflecting input/output dependencies. XOR cannot be represented as an M-of-N rule. M-of-N rules offer a more general representation in neural networks compared to conjunctions, reflecting input/output dependencies. XOR cannot be represented as an M-of-N rule. These rules share structural similarities with neural networks, viewing them as 'weightless perceptrons'. The output neuron represents the head, while visible neurons represent the body of the rule. M-of-N rules are like 'weightless perceptrons' in neural networks, with the output neuron as the head and visible neurons as the body. They encode rules by setting bias and weights, and have resurfaced in the debate on explainability. This paper highlights the importance of M-of-N rules in the debate on explainability in neural networks. It discusses the process of defining literals for rule extraction using splitting values for continuous neurons based on information gain. The process involves generating literals for target neurons based on information gain to explain neural network outputs. Splits are chosen to decrease entropy, with input literals generated from inputs to the target neuron maximizing information gain. Each target literal in a layer has its own set of inputs. In the process of generating literals for target neurons based on information gain, input literals are created from inputs to the target neuron by selecting splits that maximize information gain. Each target literal in a layer has its own set of input literals, corresponding to the same input neurons but with different splits. In convolution layers, each feature map corresponds to a group of neurons with different input patches, testing only the neuron with the optimal split for maximum information gain with respect to the network output. This results in a single rule for each feature map. In rule extraction, the focus is on the comprehensibility and accuracy of the extracted rules. Accuracy is defined in terms of the expected difference between the predictions made by the rules and the network output. Neurons in a neural network can compute their state from the input neurons. In rule extraction, accuracy is measured by the expected difference between rule predictions and network output. Neurons in a neural network compute their state from input neurons to determine truth values. Rules relating variables can be used to determine values based on input configurations. In rule extraction, accuracy is measured by the expected difference between rule predictions and network output. Neurons in a neural network compute their state from input neurons to determine truth values. Rules relating variables can be used to determine values based on input configurations. By using rule R to relate variables H and X i, the value of H can be determined based on the input x. The discrepancy between rule predictions and network output can be measured by comparing the average error over a test set. Comprehensibility is assessed by looking at the complexity of a rule, similar to Kolmogorov complexity. The complexity of a rule is determined by the length of its body in disjunctive normal form (DNF), analogous to Kolmogorov complexity. For an M-of-N rule, complexity is M N M, normalized relative to a maximum complexity of DISPLAYFORM1 with N possible input variables. The complexity of a rule is determined by the length of its body in disjunctive normal form (DNF). For an M-of-N rule, the complexity is simply M N M, normalized relative to a maximum complexity. The normalized complexity measure is obtained by taking the logarithm of the maximum complexity. An example is given with a simple perceptron and a specific rule h. The complexity of a rule is determined by the length of its body in disjunctive normal form (DNF). An example is provided with a simple perceptron and a specific rule h, showcasing the trade-off between soundness and complexity using a weighted sum defined by a parameter \u03b2. The complexity of a rule is determined by the length of its body in disjunctive normal form (DNF). A weighted sum defined by a parameter \u03b2 determines the trade-off between soundness and complexity. By using a brute force search procedure with various values of \u03b2, the relationship between the allowed complexity of a rule and its maximum accuracy can be explicitly determined. For \u03b2 = 0, the rule with the minimum loss will be the one with minimum error regardless of complexity, while for large \u03b2 values, the rule with the minimum loss will have 0 complexity. The minimum loss is determined by the rule with minimum error, regardless of complexity. For large \u03b2 values, the rule with minimum loss will have 0 complexity, such as a 1 \u2212 of \u2212 1 rule or trivial rules that always predict true or false. Neurons are split using a technique to generate sets of literals, and M-of-N rules are searched to minimize L(R). Neurons are split using a technique to generate sets of literals, and M-of-N rules are searched to minimize loss. This involves negating literals for neurons with negative weights, reordering variables based on weight magnitude, and generating splits for maximum information gain. The search procedure for generating splits for neurons relies on the ordering of variables and maximizing information gain. With O(2^n) possible M-of-N rules for a neuron with n input neurons, an exhaustive search is intractable. The search procedure for generating splits for neurons relies on maximizing information gain with O(2^n) possible M-of-N rules for a neuron with n input neurons, making an exhaustive search intractable. The assumption is that the most accurate M-of-N rules use literals corresponding to neurons with the strongest weights. The conditional independence of hidden units is justified except for the final layer, where a softmax function breaks this independence. The conditional independence of hidden units in all layers except the final one is easily justified. Ordering literals by information gains instead of weights could be an alternative to maintain independence, but experimental results show high accuracy even when ordering by weight. Defining an order on literals reduces the search space from exponential to polynomial, but computational challenges remain with a large number of test examples and input neurons. To expedite rule extraction, an algorithm was implemented in Spark and run on IBM cloud services. Accuracy was measured using examples from the training set, as evaluating rules against network output was deemed more representative than using the test set. The accuracy of the extracted rules was evaluated using examples from the training set to represent the network's behavior. Running the search in parallel allowed mapping the accuracy/complexity graph for 50 hidden neurons in a few hours. The number of examples used in the accuracy calculation was limited to 1000 to reduce time. The extraction process for the first hidden feature in the CNN was demonstrated. To demonstrate the extraction process for the first hidden feature in the CNN trained on the fashion MNIST dataset, 1000 random input examples are selected from the training set. Each neuron in the CNN is computed for activations and predicted labels using these examples. With input images of 28x28 pixels, there are 784 neurons per feature in the first layer, each corresponding to a different 5x5 patch of the input. Neurons in the CNN are analyzed for activations and predicted labels using 1000 random input examples from the fashion MNIST dataset. Each neuron corresponds to a 5x5 patch of the input image, with 784 neurons per feature in the first layer. The neuron with the maximum information gain is identified as neuron 96, with an information gain of 0.015 when split on the value 0.0004, corresponding to the image patch centered at (3, 12). Neuron 96 has the maximum information gain of 0.015 when split on the value 0.0004, corresponding to the image patch centered at (3, 12). The input splits are defined based on this variable H, and M-of-N rules are searched to explain H for various error/complexity. Neuron 96 has the highest information gain when split on the value 0.0004 at (3, 12). M-of-N rules are used to explain H for different error/complexity tradeoffs, resulting in three rules visualized in Figure 1. Neuron 96 has the highest information gain when split on the value 0.0004 at (3, 12). Figure 1 shows various rules extracted for the first feature, with the most complex rule being a 5-of-13 rule with a 0.025 error. Penalizing complexity leads to simpler rules but higher errors. Applying penalties to complexity in rule extraction on a neural network results in simpler rules with higher errors. For example, a heavy penalty produces a trivial 1-of-1 rule with an error of 0.13. This technique was demonstrated on the DNA promoter dataset, showing an exponential relationship between complexity and error in the first layer of the network. In the DNA promoter dataset, a feed forward network with a single hidden layer of 100 nodes shows an exponential relationship between complexity and error. The output layer achieves 100% fidelity using the rule 1-of-{H39, H80}. Each literal in the rule is described by an M-of-N rule from the input layer. The hidden layer splits in the network are defined by information gain with respect to the output. Each literal in the rule is represented by an M-of-N rule from the input layer. For example, a rule of 64-of-119 for H39 results in one incorrect classification, while a rule of 32-of-61 for H80 produces no incorrect classifications. The output of the network can be predicted using these rules. The output of the network can be predicted by the rule DISPLAYFORM0. Errors propagate through layers when stacking rules that don't approximate each layer perfectly. To replace a network with a set of hierarchical rules, a single set of splits for each layer must be decided upon. When replacing a network with hierarchical rules, a single set of splits for each layer must be chosen. This can be achieved by selecting input splits based on information gain against all configurations of the new output layer. Conducting experiments layer by layer independently can provide an idealized complexity/error curve for rule extraction with M-of-N rules. The experiments were conducted layer by layer independently to create a complexity/error curve for rule extraction with M-of-N rules. This approach helps evaluate the usefulness of M-of-N rule extraction and other algorithms. The layerwise rule extraction search was tested on a basic CNN trained on fashion MNIST in tensorflow. In a neural network trained on fashion MNIST in tensorflow, the layerwise rule extraction search was tested on a basic CNN with standard architecture. The CNN had convolutional layers followed by max pooling layers and a hidden fully connected layer. Random inputs from the training data were used to test the extracted rules against the network. The fully connected layer had 1024 units with rectified linear activation function. 1000 random inputs from fashion MNIST training data were used to test extracted rules. Convolutional layers were tested on image patches with maximum information gain. The third layer had 50 randomly chosen features for testing. Rules in the third layer were limited to 1000 literals or less. The final layer output was tested with 10 one-hot neurons. Each layer underwent the rule searching procedure. The search for rules in the final layer was limited to 1000 literals or less. The output was tested with 10 one-hot neurons, each undergoing the rule searching procedure for different values of \u03b2. This resulted in 5 sets of extracted rules with varying error/complexity trade-offs, allowing for the creation of a graph mapping out the error/complexity landscape for rules extracted from each layer. The complexity/error trade-off was analyzed by averaging complexities and errors of rules extracted for each target neuron. A graph was created to show the different trade-offs for rules extracted from each layer, with the first and final layers achieving near-zero error, while the second and third layers showed a similar accuracy/complexity trade-off. The third layer had a minimum error that could not be improved further. The second layer showed a slight improvement in accuracy with more complex rules compared to the third layer. The optimal accuracy/complexity tradeoff is not solely based on the number of input nodes, as the third layer performed similarly to the second layer despite having more input nodes. The final layer provided more accurate rules that were less complex than the first layer. The tradeoff in accuracy and complexity is not solely determined by the number of input nodes in each layer. Despite varying input node values, there is a critical point where error increases rapidly as complexity penalty rises, indicating a natural set of rules for explaining latent features. The error increases rapidly as the penalty on complexity rises, indicating a natural set of rules for explaining latent features. Current rule extraction algorithms do not consider complexity in their optimization, leading to a trade-off between accuracy and complexity. This paper introduces a novel approach to rule extraction algorithms by incorporating rule complexity into the optimization process. Empirical evaluation of popular extraction algorithms is crucial for validation, highlighting both limitations and potential improvements. The study emphasizes the importance of empirically evaluating rule extraction algorithms to validate their effectiveness. It discusses the limitations and potential of these algorithms, noting that while some cases have no simple explanations, others can be accurately captured by relatively simple rules. The final layer in the CNN can accurately capture the behavior of output neurons with simple rules, even achieving near 0% error for rules with complexity under 0.05. This suggests that selective use of decompositional algorithms depending on the layer being explained may be more effective. The black box problem of neural networks remains a challenge for their deployment in society. The black box problem of neural networks hinders their integration into society, with a need for explainability gaining more attention. Knowledge extraction from large neural networks remains challenging, with critics highlighting the distributed nature of neural networks as a key obstacle. Overall, large neural networks are difficult to interpret and explain due to their distributed nature. Critics argue that traditional knowledge extraction methods are not feasible for neural networks. However, a novel search method for M-of-N rules shows that latent features in a CNN can be explained by an optimal rule balancing error and complexity trade-off. The study applies a novel search method for M-of-N rules to explain latent features of a CNN, finding an 'optimal' rule representing an ideal error/complexity trade-off. Rule complexity is included as a measure in the search for extracted rules, revealing a large discrepancy in trade-off between neurons in different layers. This suggests that rule extraction may not adequately describe all latent variables, but in many cases, explanations can be simplified. The study suggests that rule extraction may not provide adequate descriptions for all latent variables in neural networks due to discrepancies in trade-offs between neurons in different layers. However, simplifying explanations without reducing accuracy can still make rule extraction a useful tool for understanding networks. Further research is needed to explore the effects of different transfer functions on the accuracy and interpretability of rule extraction. The results suggest that decompositional rule extraction remains important for understanding network behavior. Further research will investigate the impact of using different transfer functions, data sets, architectures, and regularization methods on accuracy and interpretability."
}