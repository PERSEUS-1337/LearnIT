{
    "title": "HJcjQTJ0W",
    "content": "Massive data on user local platforms often cannot support deep neural network (DNN) training due to resource constraints. Cloud-based training poses privacy risks from excessive data collection. To enable cloud-based DNN training while protecting data privacy, we propose splitting DNNs and deploying them separately on local platforms and the cloud. Local NN generates feature representations using pre-trained NNs to avoid local training and protect data privacy. Cloud NN is utilized for further processing. The idea of splitting DNNs for local and cloud deployment is validated to protect data privacy. The local NN generates feature representations using pre-trained models, while the cloud NN is trained based on these representations for the target learning task. A method called PrivyNet optimizes the local NN topology to improve accuracy in the learning task. The dependency of privacy loss and classification accuracy on the local NN topology for a CNN image classification task is explored. PrivyNet is proposed to optimize the local NN topology for improved accuracy while considering privacy loss, local computation, and storage constraints. The effectiveness of PrivyNet is demonstrated with the CIFAR-10 dataset, addressing the challenge of computationally intensive training processes on local platforms. Massive data generated by sensors, cameras, and mobile devices are stored locally, but training deep models can be too computationally intensive. Cloud-based services offer an alternative for training but raise privacy concerns due to excessive data collection. To address this, data pre-processing schemes are proposed to protect user privacy by uploading transformed representations instead of original data. To protect user data privacy, different data pre-processing schemes are proposed for cloud-based training. Transformed representations are generated locally and uploaded for learning tasks, ensuring utility and privacy requirements are met. The transformation scheme should also be flexible. The target learning task should be accomplished accurately based on released representations, while privacy requirements constrain the leakage of private information. The transformation scheme should be flexible for platforms with different capabilities and types of data. Privacy and utility trade-off is a key question in privacy research, with various measures proposed based on rate-distortion theory and statistical estimation. In recent years, various transformations have been proposed to explore the trade-off between privacy and utility. Syntactic anonymization methods like k-anonymity, l-diversity, and t-closeness aim to protect sensitive attributes in static databases, but are challenging to apply to high-dimensional continuous data. Differential privacy offers a formal privacy guarantee by adding noise to data, preventing additional knowledge gain by adversaries. However, it does not limit total information leakage from released representations. Existing works for achieving differential privacy often involve local platforms in the backward propagation process, making deployment on lightweight platforms challenging. Non-invertible linear and non-linear transformations are proposed for data anonymization, with linear transformations relying on covariance between data and labels or linear discriminant analysis (LDA) for filtering. Non-invertible linear and non-linear transformations are proposed for data anonymization to improve privacy protection. Linear transformations rely on covariance between data and labels or linear discriminant analysis (LDA) for filtering training data, but they have limited privacy protection. Nonlinear transformations like minimax filter or Siamese networks offer better privacy protection but require an iterative training scheme and can only be applied in the inference stage. The proposed PrivyNet framework offers a flexible DNN training approach for fine-grained control of the privacy and utility trade-off. It utilizes nonlinear transformations like minimax filter or Siamese networks for better privacy protection, but requires an iterative training scheme between cloud and local platforms. PrivyNet is a DNN training framework that divides the model into local and cloud parts for privacy and utility trade-off control. The local NN generates intermediate representations while the cloud NN is trained for the learning task based on these representations, achieving privacy protection through nonlinear transformations. The local NN generates intermediate representations for privacy protection through non-linear transformations, derived from pre-trained NNs to achieve good utility by embedding useful features. PrivyNet is a framework that splits a DNN model for cloud-based training with fine-grained privacy control. It uses pre-trained NNs to extract general features and protect privacy by selecting specific features to release in the local NN. The framework characterizes privacy loss and utility of using CNN as the local NN, focusing on three key factors. PrivyNet is a novel framework that splits DNN models for cloud-based training with fine-grained privacy control. It characterizes privacy loss and utility using CNN as the local NN, identifying key factors for the privacy and utility trade-off. A hierarchical strategy is proposed to optimize the local NN's topology considering constraints on computation, storage, and privacy loss. The framework is validated using CNN-based image classification, demonstrating efficiency and effectiveness. PrivyNet is a framework that optimizes utility while considering constraints on local computation, storage, and privacy loss. It validates the idea of using pre-trained NN for intermediate representation generation through CNN-based image classification. Feature representations are generated by a feature extraction network (FEN) from original data, followed by training an image classification network (ICN) based on these representations. The process involves using a pre-trained NN for feature extraction, training an image classification network (ICN) for the target task, and an image reconstruction network (IRN) to reconstruct original images. Utility is measured by task accuracy and privacy by image distance. IRN training assumes knowledge of original images and feature representations. The IRN is trained with knowledge of original images and feature representations. The adversarial model will be detailed in later sections. The collection of training instances represents images with D channels and dimensions W \u00d7 H. The label indicator vector y i \u2208 {0, 1} K denotes correct labels. The transformation t induced by the FEN has output feature representations with depth D and dimension W \u00d7 H. t is parameterized by the number of FEN layers m and the subset of filters selected for each layer f. The output representations for image x i are denoted by t(x i ; m, f). The utility of the transformed representations is evaluated by learning a classifier with minimized empirical risk for the target learning task. The utility is defined as the accuracy achieved by the classifier on testing instances. The privacy of the transformed representations is evaluated by learning a reconstruction model that minimizes the distance between the reconstructed images and the original images, based on pixelwise Euclidean distance. The privacy loss of the transformed representations is measured in this process. The impact of FEN topology on the privacy and utility of transformed representations is characterized by a model that minimizes distance between reconstructed and original images. Privacy loss is measured using peak signal-to-noise ratio (PSNR), where larger PSNR indicates higher privacy loss. Reconstructed images with different PSNR values are shown to understand implications. The FEN topology's impact on privacy and utility of transformed representations is characterized by using a model that minimizes distance between reconstructed and original images. The FEN is derived from VGG16 BID20 pre-trained on Imagenet dataset BID17, with architectures of VGG16, ICN, and IRN shown in detail in Appendix B. The FEN topology is mainly determined by the number of layers, depth of output channels, and subset of channels. The FEN topology's impact on privacy and utility is evaluated by constructing h for image classification and g for reconstruction tasks. The number of layers, output depth, and selected channels influence the FEN topology. Changes in FEN topology generate different representations for evaluating ICN and IRN. Utility and privacy are assessed through training and plotting in FIG2. Based on changes in the number of FEN layers and output depth, different representations are generated to evaluate utility and privacy using ICN and IRN. Privacy loss is observed with smaller PSNR in reconstructed images, indicating less privacy loss with reduced output depth or increased FEN layers. Utility degradation is minimal with fewer FEN layers. The trade-off between accuracy and PSNR in the PrivyNet framework is shown in FIG2 (c). When the number of FEN layers is small, there is minimal accuracy degradation with reduced output depth. Conversely, large accuracy degradation is observed when the number of FEN layers is large and the output depth is small. The trade-off between accuracy and PSNR in the PrivyNet framework is illustrated in FIG2 (c). It shows that FEN with different topologies have similar utility when privacy loss is high, while FEN with more layers provide better utility when privacy loss is low. The selected subset of output channels also impacts the privacy and utility of the output representations. The number of FEN layers and output channel depth impact privacy and utility. Channel selection affects privacy and utility, with comparisons made for each single channel. Utility and privacy loss statistics are detailed for a FEN with 4 VGG16 layers in Figure 4 (a) and Table 4 (c). When the FEN consists of 4 VGG16 layers, the utility and privacy of representations generated by each channel are characterized. Results show a significant difference in utility and privacy between the best and worst channels, with the best channel achieving 4X higher utility and 6 dB less privacy loss. Similar discrepancies are observed with 6 VGG16 layers. The impact of output channel selection on utility and privacy is compared with the number of FEN layers and output depth. Different sets of output channels are evaluated for privacy and utility, showing changes in FIG4. The impact of output channel selection on utility and privacy is compared with the number of FEN layers and output depth. Different sets of output channels are evaluated for privacy and utility, showing changes in FIG4. The privacy and utility of representations generated from the pre-trained CNN-based transformation are analyzed, revealing a larger dependence on the number of FEN layers and output channel depth. Leveraging the pre-trained CNN to build the FEN allows for exploring the trade-off between utility. The pre-trained CNN can be used to build the FEN and analyze the trade-off between privacy and utility by controlling the FEN topology. The number of FEN layers and output channel depth play a significant role in the privacy and accuracy trade-off. Larger dependence is observed for these factors compared to output channel selection. Based on these observations, the framework PrivyNet is proposed to optimize the FEN topology. In the next section, the framework PrivyNet is proposed to optimize the FEN topology for balancing privacy and utility. The FEN topology, derived from a pre-trained NN, impacts privacy, utility, computation, and storage. The FEN topology, derived from a pre-trained NN, impacts privacy, utility, computation, and storage. Especially for lightweight platforms like mobile devices, constraints on local computation and storage are crucial. To optimize utility under privacy, computation, and storage constraints, the PrivyNet framework is proposed. The PrivyNet framework optimizes utility under privacy, local computation, and storage constraints. Privacy characterization is done using cloud-based services and performance profiling of NNs on local platforms. The FEN design considers the number of layers and output depth based on constraints. Channel pruning is conducted on private data to remove ineffective channels for the learning task. The FEN output depth is determined considering privacy, local computation, and storage constraints. A supervised channel pruning step removes ineffective channels for the learning task. Output channels are randomly selected to determine the FEN topology, with a strong assumption on the availability of original images for privacy evaluation. The assumption of having access to original images is crucial for worst-case privacy evaluation in the adversarial model. This assumption also includes the unknown transformation induced by the FEN to prevent sophisticated image reconstruction mechanisms by attackers. In the adversarial model, the transformation induced by the FEN is assumed to be unknown to attackers to prevent sophisticated image reconstruction mechanisms. Anonymity of the FEN, derived from pre-trained NNs, needs to be protected as attackers may have access to the architecture and weights. The pre-characterization stage involves performance/storage profiling on local. To protect the anonymity of the FEN derived from pre-trained NNs, the pre-characterization stage involves performance and storage profiling on local platforms. This profiling is crucial due to different computation capabilities and storage configurations across platforms, which directly impact the FEN's topology. Additionally, cloud-based privacy characterization is conducted for the pre-trained NNs. The privacy characterization for pre-trained NNs involves performance and storage profiling on local platforms to determine the FEN's topology. Cloud-based services are used for this characterization, and the reconstruction network is trained on publicly available data of the same dimension and distribution. Verification is done by comparing the PSNR for FEN with different datasets like CIFAR-10 and CIFAR-100. The privacy characterization for pre-trained NNs involves comparing PSNR for FEN with different datasets like CIFAR-10 and CIFAR-100. Experiments show that less than 1000 samples are needed for an accurate characterization with data augmentation. The privacy characterization is not the target learning task. With data augmentation, less than 1000 samples are needed for an accurate privacy characterization in pre-trained NNs. Detailed PSNR values can be less accurate, reducing the training sample requirement. In PrivyNet, determining the FEN topology involves considering the number of FEN layers and output channel depth, which impact privacy and accuracy of representations. In PrivyNet, the number of FEN layers and output channel depth have significant impacts on privacy and accuracy. The FEN topology is determined based on pre-characterization results, considering local computation, storage, and privacy constraints. The relation between privacy, local computation, and storage on a mobile class CPU is shown in Figure 8. The relation between privacy, local computation, and storage on a mobile class CPU is shown in Figure 8. When the privacy requirement is high, a deep FEN layer is selected for better utility, while for low privacy requirements, a shallow FEN can achieve the necessary PSNR. When privacy requirements are low, a shallow FEN with minimal local computation and storage is selected to achieve the necessary privacy level. This reduces the need for deep FEN layers and output channel depth adjustments. When privacy requirements are low, a shallow FEN with minimal local computation and storage is selected to achieve the necessary privacy level. The number of layers and output depth are determined based on the allowed PSNR, with output channel selection playing a crucial role in utility and privacy trade-offs. After determining the number of layers and output depth based on privacy requirements, output channel selection is crucial for balancing utility and privacy trade-offs. Large discrepancies in utility and privacy are observed when selecting output channels directly, indicating the need for channel pruning to avoid poor utility with high privacy leakage. Channel pruning is necessary to address the large variances in utility and privacy when directly selecting output channels. There is a negligible correlation between utility and privacy loss for single channels, as shown in Figure 10. The correlation remains low even with different output channel depths and FEN layers, as depicted in Figure 9. In channel pruning, the correlation between utility and privacy loss is negligible for single channels, even with varying output channel depths and FEN layers. This observation allows for optimizing utility while suppressing privacy loss. The channel pruning process considers both utility and privacy, using offline pre-characterization to identify channels with the largest privacy loss and leveraging Fisher's linear discriminability to identify channels with the worst utility. The privacy loss for each channel can be determined from offline pre-characterization to prune channels with the highest privacy loss. Fisher's linear discriminability analysis is used to identify channels with poor utility by measuring the distance between representations within the same class and different classes using the covariance matrix. In Fisher's LDA scheme, distance is measured using the covariance matrix to identify ineffective channels based on the output representation for images. The criterion evaluates the distance between representations within the same class and different classes to determine channel utility. In Fisher's LDA scheme, the criterion evaluates the distance between representations within the same class and different classes to determine channel utility based on the output representation for images. The linear discriminability for the j-th output channel can be computed using Fisher's criterion, which considers between-class and within-class variance. The linear discriminability for the j-th output channel in Fisher's LDA scheme is computed using between-class and within-class variance. By evaluating Fisher's discriminability for each channel, the algorithm prunes channels with the worst utility to improve accuracy in the learning task. The effectiveness of the LDA-based supervised channel pruning algorithm is verified in experiments. In experiments, the LDA-based supervised channel pruning algorithm is tested to determine the effectiveness of leveraging Fisher's discriminability to identify ineffective channels. The algorithm prunes the 32 output channels with the worst utility from the first 6 VGG16 layers, resulting in a 69.7% improvement when 64 channels are pruned. The LDA-based supervised pruning algorithm effectively prunes 32 output channels with the worst utility from the first 6 VGG16 layers. When 64 channels are pruned, 69.7% of the worst 32 channels can be pruned on average, resulting in a 33.5% reduction in the probability of selecting a bad channel randomly. The LDA-based pruning algorithm effectively prunes 32 output channels with the worst utility from the first 6 VGG16 layers. The number of samples required for pruning can be adjusted, with similar results observed for different batch sizes. The computation complexity scales with the number of samples used in the pruning process. The experimental results show that the computation complexity of the LDA-based pruning process scales with the number of samples used. The effectiveness of supervised channel pruning is demonstrated by setting the layer of FEN to 6 and the output depth to 8. Three settings for comparing privacy and utility are considered: random selection within all output channels, channel pruning based on privacy and utility, followed by random selection. In the pruning process, 64 channels with the worst utility and 32 channels with the largest privacy loss are pruned. 20 experiments are run for each setting, showing results in FIG2 (a). Average PSNR for random selection without pruning is compared. After pruning 64 channels with the worst utility and 32 channels with the largest privacy loss, 20 experiments were conducted for each setting. Results in FIG2 (a) show that pruning leads to better utility and less privacy leakage. Detailed statistics are listed in Table 13 (b). After conducting experiments with different pruning strategies, it was found that the LDA-based pruning strategy achieved better accuracy and smaller privacy leakage compared to random selection without pruning. Additionally, the method showed similar accuracy but slightly less privacy loss compared to a pruning strategy based on characterization results. This verifies the effectiveness of the supervised pruning strategy. Our method achieves similar accuracy to a pruning strategy based on characterization results, with slightly less privacy loss. The effectiveness of our supervised pruning strategy is verified through utility and privacy comparison. The adversarial model adopted in the paper assumes the transformation induced by the FEN is unknown. In the paper, the adversarial model assumes the FEN transformation is unknown to attackers, but strategies are needed to protect the anonymity of the FEN. Two methods are considered for FEN protection: building a pool of pre-trained NNs and deriving FEN from NNs. In the framework for protecting the anonymity of the FEN, two methods are proposed: building a pool of pre-trained NNs for FEN derivation and applying channel selection to output and intermediate channels. This makes it harder for attackers to guess how the FEN is derived. Applying channel selection to output and intermediate channels in the pre-trained NN pool makes it harder for attackers to guess how the FEN is derived. The number of channels and the subset of selected channels become unknown to attackers, increasing the difficulty of guessing the channels forming the FEN. Privacy and utility are maintained through empirical verification in the first 6 layers of VGG16. The privacy and utility of the intermediate channel selection in VGG16 are empirically verified by gradually reducing the channel depth of the first convolution layer. Despite the reduction in channel depth, privacy and utility are minimally impacted, with a significant decrease in runtime observed. The channel depth of the first convolution layer in VGG16 is gradually reduced from 64 to 16, showing minimal impact on privacy and utility but a significant decrease in runtime. Further reductions in channel depth for each layer also maintain privacy and utility with reduced runtime. Channel selection for intermediate layers makes it difficult for attackers to determine the number of layers in the pre-trained neural network. PrivyNet is a flexible framework designed to protect the anonymity of the FEN derived from a pre-trained NN. It enables cloud-based training with fine-grained privacy protection, addressing resource constraints and policy limitations. This can be beneficial for applications like modern hospitals. PrivyNet enables cloud-based training with fine-grained privacy protection, addressing resource constraints and policy limitations. It allows hospitals to release informative features from patients' data for disease diagnosis, prevention, and treatment without compromising privacy. PrivyNet offers a framework for hospitals to release informative features from patient data while protecting privacy. It can also enable mobile platforms to collect and upload data to the cloud without compromising personal information. PrivyNet allows mobile platforms to upload health data to the cloud while protecting privacy. It is simple, platform-aware, and flexible for different end-users. The CIFAR-10 dataset used in the study contains 60000 color images in 10 classes. The CIFAR-10 dataset consists of 60000 32 \u00d7 32 color images in 10 classes, with 50000 training images and 10000 test images. The CIFAR-100 dataset has images of objects in 100 classes, with 600 images per class. VGG16, pre-trained on ImageNet, is used for privacy and accuracy characterization. The study uses 100 test images of size 32 \u00d7 32 for privacy and accuracy characterization. VGG16, pre-trained on ImageNet, is utilized for this purpose. CNN is used to construct networks for image classification (h) and image reconstruction (g), with specific architectures detailed in the appendices. The generative NN architecture based on ResNet blocks is employed for image recovery tasks. In Appendix C, a state-of-the-art generative NN architecture based on ResNet blocks is used for image recovery tasks such as super resolution and denoising autoencoder. The image IRN is constructed following a specific structure with 8 ResNet blocks per cluster. Gradient descent optimizer is used in training with a learning rate of 0.003 and a mini-batch size of 128 for 100 epochs. For image reconstruction and classification tasks, we use Tensorflow example with gradient descent optimizer. Learning rates are set to 0.003 and 0.05, mini-batch size is 128. Training lasts for 100 epochs for reconstruction and 250 epochs for classification. Data augmentation includes normalization, brightness, and contrast adjustments. Topology of IRN is determined before characterization for accurate privacy. For data augmentation, normalization is applied to each image and brightness/contrast are adjusted randomly. The topology of the IRN is determined before characterization to ensure accurate privacy evaluation. The image recovery capability of IRN depends on the number of ResNet block clusters, with experiments showing that the PSNR of reconstructed images saturates with an increase in clusters. In experiments, the quality of reconstructed images from FENs with different topologies was assessed by monitoring the PSNR. Optimal results were achieved with 2 ResNet block clusters, each containing 8 blocks. Performance profiling of VGG16 on different CPUs was conducted to analyze storage requirements. In experiments, the quality of reconstructed images from FENs with different topologies was assessed by monitoring the PSNR. Performance profiling of VGG16 on different CPUs showed an increase in local computation and storage requirements with the number of layers. Computation mainly comes from convolution layers, while storage is dominated by fully connected layers, especially with larger input images. The computation in neural networks is mainly from convolution layers, while storage is dominated by fully connected layers, especially with larger input images. Different platforms may have varying bottlenecks, and a flexible framework is needed to accommodate these differences. The second part of extra computation is determined by the number of samples in LDA. The necessity for a flexible framework to account for runtime differences on various platforms is highlighted. The complexity of the second part of computation is determined by the number of samples in LDA and the dimensions of the output representations. The complexity is O((K + N LDA)W^2H^2 + W^3H^3). The complexity of computing W^-1 and the largest eigenvalue of W^-1 B is O((K + N LDA)W^2H^2 + W^3H^3), where K and N LDA are determined by the characteristics of the FEN and the learning task. N LDA plays a key role in the extra computation induced by the learning process, but typically a small N LDA is sufficient for good pruning results, resulting in minimal computation overhead."
}