{
    "title": "SJlPZlStwS",
    "content": "Recent studies have shown that convolutional neural networks (CNNs) are vulnerable to various attacks. A new framework called EdgeGANRob aims to improve CNN robustness by focusing on global shape features rather than local texture features in images. This framework extracts shape features and reconstructs images using a generative adversarial network (GAN). The EdgeGANRob framework enhances CNN robustness by extracting shape features from images and reconstructing them using a GAN. A robust edge detection approach called Robust Canny is proposed to reduce sensitivity to adversarial perturbations. Comparing EdgeGANRob with EdgeNetRob shows that EdgeNetRob boosts model robustness but lowers clean model accuracy, while EdgeGANRob maintains model accuracy. EdgeNetRob improves model robustness but reduces clean model accuracy, while EdgeGANRob enhances clean model accuracy without sacrificing robustness benefits. Extensive experiments demonstrate EdgeGANRob's resilience across various learning tasks and settings. Recent studies have shown that Convolutional Neural Networks (CNNs) have achieved state-of-the-art performance in various learning tasks but are vulnerable to adversarial examples. These examples involve adding imperceptible perturbations to test data to manipulate predictions. Another threat is data poisoning or backdoor attacks, where training data is manipulated to reduce model generalization accuracy and achieve targeted poisoning attacks. Recent studies have shown that Convolutional Neural Networks (CNNs) are vulnerable to adversarial examples, where test data is manipulated to manipulate predictions. Data poisoning or backdoor attacks involve manipulating training data to reduce model generalization accuracy. CNNs tend to learn surface statistical regularities instead of high-level abstractions, leading to a lack of generalization under distribution shifting. Improving the robustness of CNNs under these settings remains a challenge. Recent studies explore the vulnerability of Convolutional Neural Networks (CNNs) to adversarial examples, attributing it to non-robust but highly-predictive features. They suggest training classifiers only on \"robust features\" that are insensitive to perturbations, emphasizing the importance of global object shapes in human recognition. Recent studies highlight the importance of training classifiers on \"robust features\" that are insensitive to perturbations, emphasizing the significance of global object shapes in human recognition. It is shown that human recognition relies on global object shapes rather than local patterns, while CNNs are more biased towards the latter. This bias towards local features can lead to misclassifications, as demonstrated by Geirhos et al. (2019) in a texture-shape cue conflict experiment. The bias towards local features in CNNs can lead to misclassifications, as shown in a texture-shape cue conflict experiment where a cat shape with elephant texture was wrongly predicted as an elephant. This vulnerability contributes to adversarial examples and backdoor attacks, highlighting the importance of global object shapes in human recognition. The proposed pipeline, EdgeNetRob and EdgeGANRob, aims to improve CNN robustness by focusing on global shape structure, specifically edges. This approach addresses the bias towards local features in CNNs, which can lead to misclassifications and vulnerabilities to adversarial attacks. The paper proposes EdgeGANRob, a new approach to enhance CNN robustness by leveraging structural information in images, specifically edges. This method aims to address the bias towards local features in CNNs, improving resilience to adversarial attacks and distribution shifting. The paper introduces EdgeGANRob, a new approach to improve CNN robustness by leveraging structural information in images. A simplified version, EdgeNetRob, detects edges and trains the classifier on them, forcing CNNs to make predictions based on shape information rather than texture/color, eliminating texture bias. EdgeNetRob improves CNNs' robustness by training on extracted edges, eliminating texture bias. However, challenges remain with vulnerable edge detection algorithms. To address this, a robust edge detection algorithm, Robust Canny, is proposed to enhance EdgeNetRob's robustness against attacks. This combined method outperforms existing approaches. The proposed Robust Canny algorithm enhances the robustness of EdgeNetRob against adaptive attackers, outperforming existing defense methods. EdgeNetRob improves CNNs' robustness but reduces clean accuracy due to missing texture/color information, leading to the development of EdgeGANRob to refill this information before classification. The development of EdgeGANRob aims to improve CNNs' robustness by refilling texture/colors based on edge images before classification. The proposed unified framework explicitly extracts edge/structure information and reconstructs original images using GAN to combat multiple tasks simultaneously and remain resilient against adaptive evasion attacks. More visualization results can be found on the anonymous website: https://sites.google.com/view/edgenetrob. The EdgeGANRob framework enhances CNNs' robustness by extracting edge information from input images and reconstructing them using GAN to handle multiple tasks and resist evasion attacks. A robust edge detection approach, Robust Canny, is proposed to reduce sensitivity to adversarial perturbations. Additionally, the effectiveness of inpainting GAN in EdgeGANRob is demonstrated through the simplified EdgeNetRob backbone procedure. The inpainting GAN in EdgeGANRob is evaluated for effectiveness in learning tasks directly on robust edge features. Thorough evaluation on EdgeNetRob and EdgeGANRob shows significant improvements in adversarial attacks, distribution shifting, and backdoor attacks. Defense methods against adversarial examples have been proposed, but many are not robust against adaptive attacks. Defense methods against adversarial examples have been proposed, but many are not robust against adaptive attacks. State-of-the-art methods are based on adversarial training, with a focus on evaluating against customized white-box attacks and strong adaptive attacks. Distribution shifting is also a concern compared to adversarial examples. Carlini et al. (2019) and Wang et al. (2019a) propose evaluating defense methods against strong adaptive attacks and robustifying CNNs by penalizing superficial statistical cues. Distribution shifting, as highlighted by Recht et al. (2018), is a common concern in real-world applications. Wang et al. (2019a) proposes a method to robustify CNNs by penalizing predictive power of local representations and evaluating on different patterns. Hendrycks and Dietterich (2019) introduce benchmark datasets for evaluating model robustness. Backdoor attack (Chen et al., 2017a; Gu et al., 2017) is a type of poisoning attack that injects a pattern into training data to manipulate model predictions. Tran et al. (2018) has also proposed a method in this area. Backdoor attack injects a pattern into training data to manipulate model predictions. Tran et al. (2018) proposes a method to detect poisoned training data using robust statistics. Neuron pruning is suggested as an approach to protect models from backdoor attacks. Recent work highlights the connection between recognition robustness and visual features, showing that CNNs rely more on textures than global shape structure for image recognition. Recent work has shown a connection between recognition robustness and visual features in image recognition. CNNs rely more on textures than global shape structure, while humans focus more on shape structure. Adversarially robust models tend to capture the global structure of objects. Non-robust features in natural images are highly predictive but not interpretable by humans. CNNs can achieve robustness by learning from images with only robust features. In response to the connection between recognition robustness and visual features in image recognition, Ilyas et al. (2019) propose using edge as a robust feature. They introduce EdgeGANRob, a classification pipeline that extracts edge/structure features from images and reconstructs them using a generative adversarial network (GAN). This method aims to enhance robustness by focusing on edge features in image processing. EdgeGANRob is a pipeline that extracts edge/structure features from images and reconstructs them using a GAN. The process involves using EdgeNetRob as a simplified backbone, which includes edge detection and image classification stages. EdgeNetRob is a simplified backbone of EdgeGANRob, consisting of two stages: edge detection to extract edge maps and training an image classifier on these maps. The pipeline aims to make CNN decisions based solely on edges, reducing sensitivity to local textures. EdgeNetRob forces CNN decisions based on edges, reducing sensitivity to textures. However, it degrades CNN performance on clean test data. This led to the development of EdgeGANRob, which refills edge images with texture/colors using a generative model. The development of EdgeGANRob aims to improve CNN performance by filling edge images with texture/colors, enhancing clean accuracy. This leads to the proposal of a robust edge detection algorithm called Robust Canny to further enhance the classification system built upon edges. The development of EdgeGANRob aims to enhance CNN performance by filling edge images with texture/colors, improving clean accuracy. This leads to proposing a robust edge detection algorithm named Robust Canny to further enhance the classification system built upon edges. The Robust Canny algorithm is designed to address the vulnerability of existing edge detection algorithms to attacks, ensuring higher accuracy in the recognition task. The Robust Canny algorithm improves the traditional Canny edge detector by truncating noisy pixels in its intermediate stages. It includes 6 stages: noise reduction with a Gaussian filter and gradient computation using the Sobel operator. The Robust Canny algorithm enhances the traditional Canny edge detector by addressing noisy pixels in its intermediate stages. It consists of 6 stages: noise reduction with a Gaussian filter, gradient computation using the Sobel operator, noise masking with thresholding, and non-maximum suppression for edge thinning. The Robust Canny algorithm improves the traditional Canny edge detector by incorporating noise reduction, gradient computation, thresholding, non-maximum suppression, double thresholding, and edge tracking by hysteresis. The Robust Canny algorithm enhances the traditional Canny edge detector by including noise reduction, gradient computation, thresholding, non-maximum suppression, double thresholding, and edge tracking by hysteresis. This modification involves adding a noise masking stage after computing image gradients to reduce perturbation noise. The gradient magnitudes lower than a threshold \u03b1 are set to zero, and a truncation operation helps reduce adversarial noise on the gradient map without affecting the final edge maps. The Robust Canny algorithm improves the traditional Canny edge detector by incorporating noise reduction and gradient computation. Parameters like \u03c3 and thresholds \u03b8 l , \u03b8 h impact robustness, with larger values enhancing smoothing and pruning effects but potentially reducing clean accuracy. The larger \u03c3 and higher thresholds \u03b8 l , \u03b8 h improve robustness in the Robust Canny algorithm by enhancing smoothing and pruning effects. However, this can lead to blurrier images and loss of useful information. Careful parameter selection is crucial for a robust edge detector. The training of a Generative Adversarial Network (GAN) in EdgeGANRob is described in this section, focusing on generating color images from edge maps within the image-to-image translation framework. In EdgeGANRob, a Generative Adversarial Network (GAN) is trained to generate color images from edge maps using the image-to-image translation framework. The GAN is trained with two steps, involving an adversarial loss and feature matching loss with controlled importance levels. In the second stage, the trained GAN is fine-tuned along with the classifier to achieve high accuracy over generated RGB images by minimizing the classification loss. The objective function includes adversarial and feature matching losses with controlled importance levels. The EdgeGANRob method improves robustness under adversarial attack, distribution shifting, and backdoor attack by focusing on generating realistic images without including classification loss initially. The use of edges in the model enhances robustness against small adversarial perturbations. EdgeGANRob is designed to enhance robustness by focusing on shape structure, making it less sensitive to distribution changes during testing. It leverages edge features to improve generalization ability and is effective against backdoor attacks by poisoning training data with specific patterns. In backdoor attacks, an attacker poisons training data with specific patterns to trick models into predicting a certain class at testing time. Extracting edges can sanitize data and prevent such attacks. EdgeNetRob, a component of EdgeGANRob, shows robustness even without inpainting GAN. The proposed method, EdgeNetRob, is evaluated for its robustness against adversarial attacks, distribution shifting, and backdoor attacks. It is viewed as a data sanitization step to prevent malicious patterns and is considered independently valuable as a robust recognition method. The experiments are conducted on two datasets: Fashion. The proposed method, EdgeNetRob, is evaluated for robustness against adversarial attacks, distribution shifting, and backdoor attacks on Fashion MNIST and CelebA datasets. MNIST and CIFAR-10 datasets were not chosen due to their limitations in providing informative benchmarks. The study focuses on evaluating the EdgeNetRob method for robustness against adversarial attacks using the MNIST and CIFAR-10 datasets. The network architecture for classification is the same for both the method and the vanilla classifier. The evaluation is done using \u221e adversarial perturbation constraints with a standard perturbation budget. The study evaluates the EdgeNetRob method's robustness against adversarial attacks on MNIST and CIFAR-10 datasets using \u221e perturbation constraints. Different perturbation budgets are used for Fashion MNIST and CelebA datasets. The evaluation includes measuring robustness to white-box attacks using the BPDA attack. The study evaluates the robustness of EdgeNetRob against adaptive white-box attacks using the BPDA attack. Three edge detection methods are compared: RCF, Canny, and Robust Canny, with a focus on the need for a robust edge detector in defending against adversarial attacks. The study compares the robustness of three edge detection methods: RCF, Canny, and Robust Canny, by training a classifier on the extracted edge maps. Results show that RCF's edges are not robust under strong adaptive attacks, with accuracy dropping near 0. The comparison is done on Fashion MNIST and CelebA datasets against the state-of-the-art baseline. The study compares the robustness of edge detection methods RCF, Canny, and Robust Canny. Results show RCF's edges are not robust under strong adaptive attacks, with accuracy dropping near 0. Results are presented for Fashion MNIST and CelebA datasets, comparing with the state-of-the-art baseline Adversarial training. EdgeNetRob and EdgeGANRob show a small drop in clean accuracy but achieve higher accuracy compared to adversarial training with = 8. Results in Table 2 show that EdgeNetRob and EdgeGANRob have a slight drop in clean accuracy compared to the baseline model but outperform adversarial training with = 8. EdgeGANRob exhibits higher clean accuracy than EdgeNetRob on the CelebA dataset, highlighting the importance of using GANs on complex datasets. Both EdgeNetRob and EdgeGANRob demonstrate robustness against strong adaptive attacks, maintaining a level of robustness comparable to adversarial training baselines. EdgeNetRob and EdgeGANRob show robustness against strong adaptive attacks, with EdgeNetRob being time-efficient due to not using adversarial training. Generalization ability is tested under distribution shifting using perturbed Fashion MNIST and CelebA datasets. The study tests the generalization ability of EdgeNetRob and EdgeGANRob under distribution shifting by using perturbed Fashion MNIST and CelebA datasets with various transformations. Comparisons are made with the state-of-the-art method PAR, which includes a local patchwise adversarial regularization loss. The study compares EdgeNetRob and EdgeGANRob with the state-of-the-art method PAR, showing improved accuracy on various patterns. Edge features are found to enhance CNN generalization to test data under distribution shifting. Our method, EdgeNetRob, utilizes edge features to enhance CNN generalization to test data under distribution shifting. It also serves as a defense against backdoor attacks by embedding invisible watermark patterns into images. Qualitative results are provided for Fashion MNIST and CelebA datasets. Our method, EdgeNetRob, enhances CNN generalization using edge features and defends against backdoor attacks by embedding invisible watermark patterns into images. Results are shown for Fashion MNIST and CelebA datasets, comparing our method with the Spectral Signature baseline. Test accuracy over standard test data is presented in Table 4 and Table 5. Our method, EdgeNetRob, successfully attacks the vanilla Net with high poisoning accuracy on CelebA and Fashion MNIST datasets. Spectral Signature struggles to achieve good performance with invisible watermark patterns, while EdgeNetRob and EdgeGANRob consistently maintain low poisoning accuracy. Figure 4 displays qualitative results of the backdoor images. The Spectral Signature struggles with invisible watermark patterns, while EdgeNetRob and EdgeGANRob consistently maintain low poisoning accuracy. The edge detection algorithm can remove the effect of invisible watermark patterns, and EdgeGANRob achieves better clean accuracy compared to EdgeNetRob. This new method is based on robust edge features for improving general model robustness. Our new method, EdgeGANRob, combines a robust edge feature extractor with a generative adversarial network to improve model robustness against adversarial attacks and distribution shifting. It also enhances robustness against backdoor attacks by utilizing shape information. This approach shows promising results for future research in improving model robustness. Our new method, EdgeGANRob, utilizes shape information to improve model robustness against adversarial attacks and distribution shifting. Data pre-processing involves resizing images in CelebA to 128 \u00d7 128 and normalizing data to [-1, 1]. Different CNN architectures are used for Fashion-MNIST and CelebA datasets. Models are trained using stochastic gradient descent with momentum. For Fashion-MNIST, a LeNet-style CNN is used, while for CelebA dataset, a standard ResNet with depth 20 is employed. Models are trained using stochastic gradient descent with momentum, PGD, and CW attacks. Different step sizes are used based on the distance for PGD attacks, and 1,000 images are randomly sampled for evaluation with CW attack due to its high computational complexity. Robust Canny is used for evaluation. For Fashion MNIST, hyper-parameters for Robust Canny include \u03c3 = 1, \u03b8 l = 0.1, \u03b8 h = 0.2, \u03b1 = 0.3. For CelebA, parameters are set as \u03c3 = 2.5, \u03b8 l = 0.2, \u03b8 h = 0.3, \u03b1 = 0.2. The last three steps in Robust Canny are non-differentiable transformations. In white-box attacks, backpropagation is necessary. In a white-box attack scenario, backpropagation through non-differentiable transformations is required to construct adversarial samples. Athalye et al. (2018) introduced the Backward Pass Differentiable Approximation (BPDA) technique to replace non-differentiable transformations with differentiable approximations for stronger attacks. In a white-box attack scenario, backpropagation through non-differentiable transformations is necessary to create adversarial samples. Athalye et al. (2018) introduced the Backward Pass Differentiable Approximation (BPDA) technique for stronger attacks by replacing non-differentiable transformations with differentiable approximations. To enhance the attack, a differentiable approximation of the Robust Canny algorithm is found, breaking the transformation into two stages: C1 (steps 1-3) and C2 (steps 4-6). Note that C2 is non-differentiable. The transformation of the Robust Canny algorithm is broken into two stages: C1 (steps 1-3) and C2 (steps 4-6). C2 is a non-differentiable operation where the output is a masked version of the input. To make R-Canny differentiable for BPDA, a constant mask assumption is made. The Robust Canny algorithm transformation involves two stages: C1 (steps 1-3) and C2 (steps 4-6). To make R-Canny differentiable for BPDA, a constant mask assumption is made. Test accuracy changes are shown under radial and random mask transformations with varying parameters. Additional visualization results for CelebA under distribution shifting are also presented. The radius of the mask is varied for radial mask transformation, while random mask transformation involves sampling masks with different probabilities. Additional visualization results for CelebA under distribution shifting are shown, along with qualitative results of EdgeGANRob and EdgeNetRob for backdoor attacks on Fashion MNIST. EdgeNetRob can slightly remove poisoning patterns, and the generated images do not share similar patterns."
}