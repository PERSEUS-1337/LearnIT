{
    "title": "BkecJjCEuN",
    "content": "The present work aims to enhance label efficiency of large neural networks for audio data by combining multitask learning and self-supervised learning on unlabeled data. An end-to-end audio feature extractor based on WaveNet is trained, along with task-specific neural networks. Various self-supervised learning tasks are described for large, unlabeled audio corpora to address limited labeled training data scenarios. The study focuses on improving the efficiency of large neural networks for audio data by combining multitask learning and self-supervised learning. They use an end-to-end audio feature extractor based on WaveNet and demonstrate improved performance in supervised classification tasks by up to 6% through simultaneous training with self-supervised tasks. Incorporating data augmentation further enhances performance. Deep neural networks are crucial for modeling auditory data, but they require large amounts of training data. Incorporating data augmentation in multitask settings improves performance. While labeled datasets are limited, there is abundant unlabeled data for effective unsupervised learning. Incorporating self-supervised audio tasks during model training can significantly improve generalization by utilizing abundant unlabeled data. Incorporating self-supervised audio tasks during model training can improve generalization by utilizing unlabeled data. This paper identifies appropriate self-supervised audio tasks and demonstrates their joint training with supervised tasks to enhance performance. WaveNet is used as a feature extractor for rich audio representations from raw waveform data. The framework is applied to three supervised classification tasks - audio tagging, speaker identification. By learning multi-scale hierarchical representations from raw audio, WaveNet-based models can adapt to subtle variations within tasks efficiently. This framework is applied to supervised classification tasks like audio tagging, speaker identification, and speech command recognition, showing improved performance with unlabeled data and data augmentation techniques. Self-supervised tasks can also serve as a pre-training stage for transfer learning. The authors show that their proposed self-supervised tasks can enhance performance through transfer learning and complement traditional data augmentation techniques. They suggest that models trained on multiple tasks may uncover underlying common structures in sensory environments. The literature on sensory environments describes the structure using Gabor filters and gammatone filters for visual and auditory processing. Models trained for multitask learning aim to uncover underlying common structures, improving single-task performance with less data. Other approaches to multitask learning are also explored. Self-supervised learning is a promising solution to label scarcity in deep learning, leveraging unlabeled data for tasks like image completion, colorization, and motion segmentation. In the audio domain, self-supervised learning is underexplored compared to the visual domain. An end-to-end audio processing network was implemented, utilizing a common embedding of the acoustic waveform within a \"trunk\" network inspired by WaveNet architecture. The trunk and head networks are trained jointly for each experiment. The acoustic waveform is embedded in a \"trunk\" network based on WaveNet architecture. The trunk consists of 3 blocks of 6 dilation stacks, each with 64 convolutional units. The outputs are multiplied and summed with the input. Our WaveNet trunk consists of 3 blocks of 6 dilation stacks, each with 64 convolutional units. The effective receptive field length is 190 samples or approximately 12 ms. Tested on audio tagging, speaker identification, and speech command recognition tasks with labeled and unlabeled data. The WaveNet trunk is set up for audio tagging, speaker identification, and speech command recognition tasks using labeled and unlabeled data. The audio tagging task is trained on the FSDKaggle2018 dataset with 11,073 files. Each audio segment is cropped to 2 seconds before being input to the network. The dataset consists of 11,073 uncompressed PCM audio files divided into training and test sets. Audio segments are cropped to 2 seconds and padded with zeros if needed. The WaveNet trunk produces embeddings that are averaged across time to create a single output vector, which is then passed through a fully-connected layer with 512 units and ReLU nonlinearity, followed by a softmax output layer for training with cross entropy loss. The speaker identification task is trained on the VoxCeleb-1 dataset with 336 hours of data from 1251 speakers. Individual clips are sourced from interviews with celebrities in various settings, with one interview held-out for the test set. Each audio segment is cropped to 2 seconds before being fed into the network for training. The speaker identification task is trained on the VoxCeleb-1 dataset with data from celebrities in different settings. Each audio segment is cropped to 2 seconds and normalized before being fed into the network. The task's architecture includes a global average pooling layer, 2-layer perceptron, batch normalization, ReLU nonlinearity, and a softmax layer with cross-entropy loss evaluation for speech command recognition. The speech command recognition task utilizes a global average pooling layer, 2-layer perceptron with 1024 units per layer, batch normalization, ReLU nonlinearity, and softmax layer with cross-entropy loss evaluation. It is trained on the Speech Commands dataset with 65,000 utterances of 30 short words in 12 categories. The recognition head consists of three 1D convolutions with batch normalization between each layer. The speech command recognition head consists of three 1D convolutions with batch normalization and dropout, followed by ReLU nonlinearity. The convolution layers have widths of 100, 50, and 25, and strides of 16, 8, and 4. Next-step prediction, noise reduction, and upsampling are used as self-supervised auxiliary tasks. The final softmax layer evaluates the output using cross-entropy loss. Self-supervised tasks like next-step prediction, noise reduction, and upsampling were chosen as auxiliary tasks. These tasks were trained on both main task data and unlabeled data from the Librispeech dataset. The auxiliary tasks share a common head architecture with convolutional layers and a final linear convolutional layer. The auxiliary tasks in the multitask framework share a common head architecture with convolutional layers and a final linear convolutional layer. The primary goal was to develop a generic framework for audio using waveform inputs instead of high-level feature representations like spectrograms. When working with audio, it is advisable to use waveform inputs rather than high-level feature representations like spectrograms. While models trained on spectral/cepstral representations may offer better classification performance, they limit the range of audio processing tasks they can perform. Different tasks may require varying network architectures, impacting the potential insights gained from self-supervised tasks. When working with audio, using waveform inputs is recommended over high-level feature representations like spectrograms. Tasks may require different network architectures, affecting insights gained from self-supervised tasks. Multitask learning can improve performance compared to single task training on raw audio. Joint training with three self-supervised tasks has shown benefits. Closing the performance gap between models trained on spectral representations and waveforms is a future research direction. Joint training with three self-supervised tasks improved performance for supervised tasks, including audio tagging. Multitask training enhanced MAP@3 score and top-1 classification rate without increasing training data. Incorporating larger versions of Librispeech for self-supervision showed positive effects. Incorporating larger versions of Librispeech for self-supervision led to improved performance metrics, with a MAP@3 increase of up to .056 with an additional 500 hours of unlabeled data. Swapping the audio tagging task with speech command classification or speaker identification also showed a similar trend with increasing amounts of unlabeled data. Multitask learning can enhance supervised task performance without extra labeled data. Speech command classification improved from 93.05% to 93.78% with 500 hours of unlabeled data. Speaker identification on VoxCeleb saw a top-5 classification boost from 73.81% to 75.22%. Multitask learning improved top-5 classification performance in speaker identification on VoxCeleb from 73.81% to 75.22%. Data augmentation with pitch shifting also showed a comparable increase in performance. Training a single task model on audio tagging with pitch shifting and additive noise data augmentation resulted in an increase in MAP@3 of .066 and .024 respectively. Combining pitch-shift augmentation with self-supervised tasks yielded the highest performance increase of .089. In computer vision, transfer learning now includes knowledge transfer from self-supervised tasks on unlabeled data to supervised tasks. Pre-training three self-supervised tasks on purely unlabeled data is a variant of transfer learning. Transfer learning involves pre-training self-supervised tasks on unlabeled data before fine-tuning with a smaller amount of labeled data for a supervised task. Experiments show that transfer learning outperforms training all tasks simultaneously. Transfer learning experiments favor transfer learning over simultaneously training all tasks together. Jointly training supervised task with self-supervised tasks using a WaveNet-based model on raw audio waveforms improves performance and scales with the quantity of unlabeled data. The WaveNet-based model operating on raw audio waveforms shows improved performance on supervised tasks with more unlabeled data. The approach is expected to generalize to various audio tasks, raising questions about the limit of auxiliary tasks for performance improvement. The WaveNet-based model operating on raw audio waveforms has shown improved performance on supervised tasks with more unlabeled data. Questions arise about the limit of auxiliary tasks for performance improvement in multitasking models. Further exploration is needed to understand the representation formed by the model when performing multiple audio-related tasks simultaneously. This exploration could lead to handling a broader range of auditory tasks. Our model, based on the WaveNet architecture, is designed to handle a variety of auditory tasks requiring high temporal resolutions. This approach allows us to process raw audio signals with fine temporal detail, making it suitable for tasks beyond audio tag classification. Our model, based on the WaveNet architecture, utilizes causal dilated convolutions for parallel processing of sequential inputs, making it faster to train compared to RNNs. The WaveNet trunk architecture consists of stacked dilated causal convolution layers with increasing dilation factors. Outputs from the trunk are then fed into task-specific heads for further processing. The WaveNet trunk consists of S dilated causal convolution layers with increasing dilation factors, residual connections, and saturating nonlinearities. Each layer involves a \"residual atom\" computation producing hidden state vector h and layer output x. The WaveNet trunk consists of S dilated causal convolution layers with increasing dilation factors, residual connections, and saturating nonlinearities. Each layer involves a \"residual atom\" computation producing hidden state vector h and layer output x. The first layer applies causal convolutions to raw audio waveforms to produce an output. The effective receptive field of any given block b is 1 + b(2S-1), resulting in a total effective receptive field of \u03c4 = 1+N(2S-1) after an extensive hyperparameter search. The WaveNet trunk consists of S dilated causal convolution layers with increasing dilation factors, residual connections, and saturating nonlinearities. Each block has an effective receptive field of 1 + b(2S-1), resulting in a total receptive field of \u03c4 = 1+N(2S-1). After a hyperparameter search, N = 3 blocks with S = 6 layers each were chosen, giving a total receptive field of \u03c4 = 190, equivalent to 12 milliseconds of audio at 16kHz. Each task-specific head is a neural network that shares the trunk for input processing. The WaveNet trunk consists of dilated causal convolution layers with increasing dilation factors and residual connections. Each task-specific head is a simple neural network that processes input independently. Tasks have their own objective functions, optimizers, and learning rates. Supervised tasks are primary, while self-supervised tasks are auxiliary. In experiments, \"audio tagging\" is the primary task. In experiments, \"audio tagging\" is the primary supervised task, while \"next-step prediction\", \"noise reduction\", and \"upsampling\" are auxiliary tasks trained on varying amounts of unlabeled data. Task-specific heads have parameters detailed in TAB3 of the supplement. Head architectures are kept simple to encourage the shared trunk to learn a versatile representation. The head architectures in the study were designed to be simple, using few layers to solve tasks and encourage the shared trunk to learn a versatile representation. The next-step prediction task involves predicting the next value in a sequence of audio waveform frames, allowing for large training datasets from unlabeled data. The next-step prediction head consists of a 2-layer stack of 1x1 convolutional layers with ReLU nonlinearities. The next-step prediction head in the study consists of a 2-layer stack of 1x1 convolutional layers with ReLU nonlinearities. It uses \u03c4 frames of data from the trunk to predict the next value in the audio sequence, treating it as a regression problem. The next-step prediction head uses \u03c4 frames of data from the trunk to predict the next value in the audio sequence, treating it as a regression problem with mean squared error as the loss function. The original WaveNet implementation treated next-step prediction as a classification problem. The model is trained for noise reduction by treating noise as an additive random process on top of the clean audio waveform. The denoising task is defined by adding noise to the clean signal and training the model to reduce this noise. The model is trained for noise reduction by treating noise as an additive random process on top of the clean audio waveform. It predicts the clean sample given a window of noisy samples, with a structure similar to the next-step prediction task. Our noise reduction head, similar in structure to the next-step head, is trained to minimize a smoothed L1 loss between clean and noisy waveform inputs. The smooth L1 loss ensures stable convergence for denoising tasks. Additionally, an unsupervised upsampling task can be created by downsampling the audio. For noise reduction, a smooth L1 loss is used for stable convergence. An unsupervised upsampling task involves downsampling the audio source and using the original as the target. This is analogous to the \"super-resolution\" task in computer vision. The original audio is downsampled to 4 kHz for the upsampling task. The network's task in computer vision involves upsampling the audio by repeating time-points to mimic the original signal's sample rate. The network infers high frequency information lost during the transform using an upsampling head with a structure similar to other tasks. Smooth L1 loss function is used for stability in convergence. The model used a smooth L1 loss function to compare estimated upsampled audio with the original, trained on raw audio waveform inputs from specific datasets. Audio samples were cropped to two seconds and downsampled to 16 kHz for normalization. The upsampling task was similar to next-step prediction and noise reduction tasks. The experiments involved using the PyTorch framework to process audio samples, cropping them to two seconds, downsampling to 16 kHz, and normalizing inputs. Noise was added from ChiME3 datasets at varying SNRs for a noise-reduction task. For the noise-reduction task, noisy inputs were generated from ChiME3 datasets at SNRs ranging from 10dB to 15dB with different noise types. A hyperparameter search was conducted for the main task, exploring the number of blocks, layers per block, layers and units in the task head, and learning rate. The network's performance was found to be consistent across different trunk block and dilated convolution layer configurations. The network's performance was consistent across different trunk block and dilated convolution layer configurations. Hyperparameter search was conducted for the main task, exploring number of blocks, layers per block, task head configuration, and learning rate. Performance was largely unaffected by architecture specifications, with learning rate being important. Hyperparameters were chosen based on best performance on main and auxiliary tasks. The hyperparameters for the model were chosen based on performance on both the main task and auxiliary tasks. The model was trained on all tasks simultaneously using a weighted sum of the losses. Advanced weighting strategies did not show any benefit in the experiments. The model was trained on all tasks simultaneously using a weighted sum of the losses. A uniform weighting strategy was used in the experiments. The \"Adam\" optimizer with specific parameters was utilized. The learning rate decayed every 5 epochs to improve convergence. A batch size of 48 was used in all experiments. Noise reduction and upsampling tasks required separate forward propagation. The model was trained on all tasks simultaneously using a weighted sum of losses with a uniform weighting strategy. The \"Adam\" optimizer was used with specific parameters, and the learning rate decayed every 5 epochs for improved convergence. A batch size of 48 was utilized in all experiments, with separate forward propagation required for noise reduction and upsampling tasks. Additional details can be found in TAB3."
}