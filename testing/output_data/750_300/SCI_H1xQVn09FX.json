{
    "title": "H1xQVn09FX",
    "content": "Efficient audio synthesis is a challenging machine learning task due to human perception sensitivity to global structure and waveform coherence. GANs can generate high-fidelity and locally-coherent audio by modeling log magnitudes and instantaneous frequencies with sufficient frequency resolution in the spectral domain. In demonstrating the ability of GANs to generate high-fidelity audio, it was found that they can model log magnitudes and instantaneous frequencies with sufficient resolution in the spectral domain. Extensive empirical investigations on the NSynth dataset showed that GANs outperformed WaveNet baselines in generating audio faster and more efficiently. Neural audio synthesis faces challenges in modeling temporal scales over a wide range. Autoregressive models like WaveNet focus on fine-scale details but have slow sampling speeds. Research has been dedicated to improving the quality of audio generation. Autoregressive models like WaveNet rely on external conditioning signals for global structure but have slow sampling speeds. Research has focused on speeding up generation, introducing methods with significant overhead. Generative Adversarial Networks (GANs) offer an alternative approach in neural audio synthesis. Generative Adversarial Networks (GANs) have been successful in generating high-resolution images efficiently. They use transposed convolutions and latent vectors for global latent control. GANs also show potential for audio transformations similar to those seen in image processing. The potential for audio GANs lies in conditioning transposed convolutions on a latent vector. Attempts to adapt image GAN architectures for audio waveform generation have not achieved the same level of fidelity. Sound is composed of locally-coherent waves with periodicity, and frame-based techniques are used for audio waveform estimation. Frame-based techniques for audio waveform estimation involve transposed convolutions or STFTs with a specific frame size and stride. The challenge lies in preserving phase coherence by covering all frequencies and possible phase alignments. Transposed convolutional filters in audio waveform estimation must cover all frequencies and phase alignments to preserve phase coherence. Instantaneous radial frequency can be derived from the phase of an STFT, showing the constant relationship between audio frequency and frame frequency. Spectra for a trumpet note from the NSynth dataset are illustrated.GAN researchers have made significant progress in image modeling by evaluating models on focused datasets with limited degrees of freedom. Researchers have made rapid progress in image modeling using focused datasets with limited degrees of freedom, such as the CelebA dataset. The NSynth dataset was introduced with similar goals. The NSynth dataset focuses on individual notes from musical instruments, aligning and cropping the data to emphasize fine-scale details like timbre and fidelity. Each note is accompanied by attribute labels for conditional exploration. The NSynth dataset focuses on individual notes from musical instruments, emphasizing fine-scale details like timbre and fidelity. It includes attribute labels for conditional exploration and has been further developed with various approaches such as autoregressive WaveNet autoencoders, bottleneck spectrogram autoencoders, frame-based regression models, inverse scattering networks, VAEs with perceptual priors, and adversarial regularization for domain transfer. This work introduces adversarial training and explores effective representations for noncausal convolutional generation in audio waveforms, focusing on logarithmically-scaled frequency selective filter banks spanning the range of human hearing. Convolutional filters trained on audio waveforms learn to form frequency selective filter banks spanning human hearing range. Maintaining periodic signal regularity over short to intermediate timescales is crucial for human perception. Waveform alignment is affected when frame stride does not match periodicity. When the frame stride does not match a waveform's periodicity, phase precession occurs, challenging synthesis networks to learn frequency and phase combinations for coherent waveform production. This phenomenon is similar to short-time Fourier transform (STFT) observations, where filterbanks overlap. Phase precession, similar to STFT observations, occurs when filterbanks overlap in synthesis networks trying to produce coherent waveforms. A pure tone's phase precesses and unwrapping it causes linear growth, with the derivative of the unwrapped phase remaining constant over time. Phase precession occurs when a pure tone's phase precesses and unwrapping it causes linear growth. The derivative of the unwrapped phase remains constant over time, representing the instantaneous frequency of the signal oscillation. In this paper, the interplay of architecture and representation in synthesizing coherent audio with GANs is investigated. Generating log-magnitude spectrograms and phases directly with GANs can produce more coherent waveforms than directly generating waveforms with strided convolutions. In synthesizing coherent audio with GANs, generating log-magnitude spectrograms and phases directly can produce more coherent waveforms than using strided convolutions. Estimating IF spectra leads to even more coherent audio. It is important to prevent harmonics from overlapping by increasing STFT frame size and switching to mel frequency scale. Switching to mel frequency scale and increasing STFT frame size improves performance by creating more separation between harmonic frequencies. GANs on the NSynth dataset outperform WaveNet in automatic and human evaluations, generating examples much faster. Global conditioning on latent and pitch vectors allows GANs to produce smooth timbre interpolation and consistent timbral identity across pitch. The study focuses on the NSynth dataset, containing 300,000 musical notes from 1,000 different instruments. The study focuses on the NSynth dataset, which contains 300,000 musical notes from 1,000 different instruments. Each sample is four seconds long, sampled at 16kHz, with labels for pitch, velocity, instrument, and acoustic qualities. The dataset is composed of diverse timbres and pitches, with a focus on acoustic instruments and fundamental pitches for training. The study focused on the NSynth dataset, containing 300,000 musical notes from 1,000 instruments. Samples are four seconds long, sampled at 16kHz, with labels for pitch, velocity, instrument, and acoustic qualities. Training was restricted to acoustic instruments and fundamental pitches MIDI 24-84. A new 80/20 test/train split was created from shuffled data to avoid instrument type bias. The study adapted progressive training methods from image generation successes. The study adapted progressive training methods from image generation successes to generate audio spectra from musical notes in the NSynth dataset, containing 300,000 notes from 1,000 instruments. A new 80/20 test/train split was created from shuffled data to avoid instrument type bias. The model samples a random vector and uses transposed convolutions to generate output data. The model samples a random vector from a spherical Gaussian and uses transposed convolutions to upsample and generate output data. A discriminator network estimates the divergence measure between real and generated distributions, with a gradient penalty for Lipschitz continuity and pixel normalization at each layer. Both progressive and nonprogressive training variants show comparable quality in convergence time. In BID16, a gradient penalty promotes Lipschitz continuity and pixel normalization is applied at each layer. Progressive training shows slightly better convergence time and sample diversity compared to nonprogressive training. The method involves conditioning on musical pitch to achieve independent control of pitch and timbre in the generated output. Our method involves conditioning on an additional source of information by appending a one-hot representation of musical pitch to the latent vector. An auxiliary classification loss is added to the discriminator to predict the pitch label. STFT magnitudes and phase angles are computed using TensorFlow's implementation with specific parameters. The resulting \"image\" size is (256, 512, 2) after trimming the Nyquist frequency and padding in time. We compute STFT magnitudes and phase angles using TensorFlow's built-in implementation with specific parameters. The resulting \"image\" size is (256, 512, 2) where the two channels correspond to magnitude and phase. Magnitudes are scaled to be between -1 and 1, while phase angles are also scaled accordingly. Optional unwrapping of the phase angle and taking the finite difference results in different models. The phase angle and magnitudes are scaled between -1 and 1 in the \"phase\" models. Optional unwrapping of the phase angle and increasing frequency resolution are done in the \"instantaneous frequency\" (\"IF\") models. The STFT frame size and stride are doubled to achieve high frequency resolution in spectral images. To achieve high frequency resolution in spectral images, the STFT frame size and stride are doubled, resulting in images with size (128, 1024, 2). The log magnitudes and instantaneous frequencies are transformed to a mel frequency scale without compression, referred to as \"IF-Mel\" variants. Converting back to linear STFTs using an approximate inverse linear transformation does not significantly affect audio quality. Comparisons are made against WaveGAN, the current state of the art in waveform generation with GANs. Comparing against WaveGAN, the current state of the art in GAN-based waveform generation, the study adapts it to accept pitch conditioning and retrain it on a subset of the NSynth dataset. Additionally, their own GAN models achieve similar performance to WaveGAN without progressive training. The study primarily showcases results from WaveGAN for comparison. The study adapts WaveNet to accept pitch conditioning and retrain it on a subset of the NSynth dataset. They create strong WaveNet baselines by adapting the architecture to accept the same one-hot pitch conditioning signal as the GANs. Different output distributions are tested, but WaveNet is currently the state of the art in generative modeling of audio. The study adapts WaveNet to accept pitch conditioning and retrain it on a subset of the NSynth dataset. Strong WaveNet baselines are created by adapting the architecture to accept the same one-hot pitch conditioning signal as the GANs. Different output distributions are tested, with the 8-bit model proving more stable and outperforming the 16-bit model. Generative model evaluation is challenging due to the difficulty in formalizing goals, leading to heuristic metrics with blind spots. Models are evaluated against a diverse set of metrics to capture various aspects of performance. The evaluation of models for audio generation involves using diverse metrics, including human evaluation as a gold standard due to the complexity of measuring audio quality. Training networks to produce coherent waveforms is crucial, considering human sensitivity to phase irregularities that can disrupt the listening experience. The study involved using Amazon Mechanical Turk to compare audio quality of different models for waveform synthesis. Participants evaluated pairs of 4s examples on a Likert scale, with 3600 ratings collected in total. Participants evaluated pairs of 4s examples on a Likert scale, with 3600 ratings collected in total. The diversity of generated examples was measured using the Number of Statistically-Different Bins (NDB) metric, where training examples were clustered into Voronoi cells and compared to generated examples. The training examples are clustered into k=50 Voronoi cells by k-means in log-spectrogram space. Generated examples are also mapped into the same space and assigned to the nearest cell. NDB is reported as the number of cells where the number of training examples is statistically significantly different from the number of generated examples. Inception Score (IS) is a metric for evaluating GANs, calculated as the mean KL divergence between image-conditional output class probabilities and the marginal distribution. The Inception Score (IS) is a metric used in GAN literature to evaluate generated examples. It measures the mean KL divergence between image-conditional output class probabilities and the marginal distribution. The metric penalizes models that have examples that are not easily classified into a single class or belong to only a few possible classes. The Inception features are replaced with features from a pitch classifier trained on spectrograms of an acoustic NSynth dataset, introducing Pitch Accuracy (PA) and Pitch Entropy (PE) as additional evaluation metrics. The Inception Score (IS) is a metric used in GAN literature to evaluate generated examples by measuring the mean KL divergence between image-conditional output class probabilities and the marginal distribution. To improve evaluation, Inception features are replaced with features from a pitch classifier trained on spectrograms of an acoustic NSynth dataset, introducing Pitch Accuracy (PA) and Pitch Entropy (PE) as additional metrics. Additionally, Fr\u00e9chet Inception Distance (FID) is proposed as a metric for evaluating GANs based on the 2-Wasserstein distance between multivariate Gaussians fit to features extracted from a pretrained Inception model. The Fr\u00e9chet Inception Distance (FID) is a metric for evaluating GANs based on the 2-Wasserstein distance between multivariate Gaussians fit to features extracted from a pretrained Inception model. Quality decreases as output representations move from IF-Mel, according to human evaluation. The study compares different model and representation variants for audio quality, with human evaluation showing a trend of decreasing quality from IF-Mel to Waveform. The highest quality model, IF-Mel, is slightly inferior to real data. The WaveNet baseline produces high-fidelity sounds but occasionally breaks down, resulting in comparable scores to IF GANs. Sample diversity correlates with audio quality, as seen in both human evaluation and NDB. The WaveNet baseline produces high-fidelity sounds but occasionally breaks down into feedback and self oscillation, resulting in comparable scores to IF GANs. NDB follows the trend of human evaluation, with high frequency resolution improving scores across model types. The TAB0 autoregressive sampling tends to gravitate towards the same type of oscillation, leading to a lack of diversity in the generated samples. The generative model assigns high likelihood to training data, but TAB0 autoregressive sampling leads to lack of diversity in oscillation types. FID scores show IF models with high frequency resolution perform better. Mel scaling has less impact on FID compared to listener study. Phase models have high FID, indicating poor sample quality. High-resolution IF models perform well on classifier metrics like IS, Pitch Accuracy, and Pitch Entropy due to explicit conditioning. Despite mode collapse issues, these models generate examples classified similarly to real data. Phase models, however, have high FID even at high frequency resolution, reflecting poor sample quality. High-resolution models generate examples classified similarly to real data, but differences in metrics may not indicate sample quality due to distribution issues. Low frequency models and baselines seem to have less reliable pitch generation. Visualizing qualitative audio concepts is recommended, along with listening to accompanying audio examples. The low frequency models and baselines are less reliable in generating classifiable pitches. It is recommended to listen to accompanying audio examples for a better understanding. The WaveGAN and PhaseGAN models show phase irregularities in the waveform. The WaveGAN and PhaseGAN models exhibit phase irregularities in the waveform, while the IFGAN shows more coherence with only small variations from cycle-to-cycle. The Rainbowgrams illustrate the differences, with the real data and IF models displaying strong consistent colors for each harmonic, while the PhaseGAN has speckles due to phase discontinuities and the WaveGAN model is irregular. FIG1 visualizes the phase coherence of examples from different GAN variants. The PhaseGAN model shows speckles due to phase discontinuities, while the WaveGAN model is irregular. Real data and IF models have strong consistent colors for each harmonic. Rainbowgrams depict the log magnitude of frequencies and IF as colors, showing clear phase coherence of harmonics. The PhaseGAN exhibits phase discontinuities, while the WaveGAN is irregular. Rainbowgrams show clear phase coherence in real data and IF models, with consistent colors for harmonics. GANs allow conditioning on the same latent vector for the entire sequence, unlike WaveNet which only uses short subsequences. WaveNet autoencoders in BID9 learn local latent codes for generation on a millisecond scale but have limited scope. Pretrained WaveNet autoencoder is compared with IF-Mel GAN in Figure 4 for waveform interpolation. In Figure 4, a pretrained WaveNet autoencoder is compared with an IF-Mel GAN for waveform interpolation. WaveNet improves mixing in the space of timbre, but the linear interpolation does not align with the complex prior on latents, leading to intermediate sounds that tend to fall apart, oscillate, and whistle. The GAN model, on the other hand, utilizes a spherical gaussian prior for global interpolation. The WaveNet model fails in waveform interpolation due to a lack of compact prior over time series, resulting in less realistic sounds. In contrast, the GAN model with a spherical gaussian prior produces better global interpolation. The WaveNet autoencoder lacks a compact prior over time series, leading to unrealistic sounds in waveform interpolation. In contrast, the IF-Mel GAN with global conditioning produces high-fidelity audio examples through spherical interpolation aligned with the prior. The IF-Mel GAN with global conditioning produces high-fidelity audio examples through spherical interpolation aligned with the prior, resulting in smooth perceptual changes and sounds without additional artifacts. Timbre morphs smoothly between instruments while pitches follow a composed piece like Bach's Suite No. 1 in G major. In the IF-Mel GAN with global conditioning, 15 random points in latent space are used with pitches from Bach's Suite No. 1 in G major. The timbre of the sounds smoothly morphs between instruments while maintaining consistency. The timbral identity of the GAN remains largely intact, creating a unique instrument identity in latent space. The GAN maintains timbral identity across different pitches, allowing for parallel processing of training and generation on modern GPU hardware. The IF-Mel GAN allows for parallel processing of training and generation on modern GPU hardware, significantly reducing latency compared to WaveNet baseline. This opens up new possibilities for music performance applications. The IF-Mel GAN allows for much faster neural network audio synthesis on device, enabling users to explore a wider range of expressive sounds in real-time. Previous applications of WaveNet autoencoders for music performance required prerendering all possible sounds due to long synthesis latency. This work focuses on deep generative models for audio, particularly speech synthesis datasets that handle variable length conditioning and rely on recurrent or autoregressive models. Comparisons between different models would be interesting. Generative models for audio, specifically speech synthesis, focus on handling variable length conditioning and rely on recurrent or autoregressive models. Adapting GANs for variable-length conditioning or recurrent generators is a challenging task left for future work. In comparison, music audio generation is relatively under-explored, with autoregressive models showing potential for synthesizing musical instrument sounds. Our work extends previous research on audio generation, focusing on music rather than speech. We propose a modification to GANs' loss function for improved training stability and architectural robustness, building on recent advances in GAN literature. Our work builds on recent advances in GAN literature by proposing a modification to the loss function, introducing progressive training for improved generation quality, and employing architectural tricks to enhance model quality. The NSynth dataset, known as the \"CelebA of audio,\" was used to interpolate between timbres of musical instruments with WaveNet autoencoders. The NSynth dataset, referred to as the \"CelebA of audio,\" was utilized to interpolate between musical instrument timbres using WaveNet autoencoders. BID23 improved upon this by incorporating an adversarial domain confusion loss for timbre transformations across various audio sources. BID5 achieved faster sampling speeds by training a frame-based regression model mapping pitch and instrument labels to raw waveforms, resulting in significant speedups over WaveNet autoencoders. BID5 achieved significant speedups (\u223c2,500x) over WaveNet autoencoders by training a frame-based regression model to map pitch and instrument labels to raw waveforms. They used a unimodal likelihood regression loss in log spectrograms and controlled the audio representation for generative modeling with GANs on the NSynth dataset, surpassing WaveNet fidelity. By controlling the audio representation for generative modeling, high-quality audio generation with GANs on the NSynth dataset was achieved, surpassing WaveNet fidelity. Further validation and expansion to different types of natural sounds are needed. This work also presents opportunities for domain transfer and other applications of adversarial losses in audio generation. This work aims to validate and expand the use of adversarial losses in audio generation, including speech and other natural sounds. It addresses issues like mode collapse and diversity in GANs for audio and suggests combining adversarial losses with encoders or regression losses for better data distribution capture. The study includes a pitch classifier in the discriminator and was trained with the ADAM optimizer. The study includes a pitch classifier in the discriminator and was trained with the ADAM optimizer. Models were trained with different learning rates and weights for the auxiliary classifier loss, finding that a learning rate of 8e-4 and classifier loss of 10 performed the best. Both networks use box upscaling/downscaling and pixel normalization in the generators. In the original progressive GAN paper, a learning rate of 8e-4 and classifier loss of 10 were found to perform the best. Both networks use box upscaling/downscaling and pixel normalization in the generators. The discriminator appends the standard deviation of minibatch activations as a scalar channel. Tanh output nonlinearity is used for the generator, and real data is normalized before passing to the discriminator. The generator uses a Tanh output nonlinearity and real data is normalized before passing to the discriminator. Training each GAN variant takes 4.5 days on a single V100 GPU with a batch size of 8. For nonprogressive models, training is done on approximately 5M examples, while progressive models train on 1.6M examples per stage (7 stages). The training process for the models involves training on a single V100 GPU for 4.5 days with a batch size of 8. Progressive models train on 1.6M examples per stage (7 stages), totaling approximately 11M examples. The WaveNet baseline includes a decoder with 30 layers of dilated convolution, each with 512 channels and a receptive field of 3. The decoder in the model is made up of 30 layers of dilated convolution with 512 channels and a receptive field of 3. It has 3 stacks of 10 layers each, with increasing dilation from 20 to 29. The audio encoder stack is replaced with a conditioning stack operating on a one-hot pitch signal. The conditioning stack consists of 5 layers of dilated convolution, increasing to 25, and 3 layers of regular convolution, all with 512 channels. The conditioning signal is passed through a 1x1 convolution for each layer of the decoder. The conditioning stack in the model consists of 5 layers of dilated convolution, increasing to 25, and 3 layers of regular convolution, all with 512 channels. The conditioning signal is passed through a 1x1 convolution for each layer of the decoder. WaveNets converged to 150k iterations in 2 days with 32 V100 GPUs trained with synchronous SGD with batch size 1 per GPU, for a total batch size of 32."
}