{
    "title": "rkhlb8lCZ",
    "content": "Convolutional Neural Networks advance 2D and 3D image classification. Wavelet Pooling is introduced as an alternative to traditional pooling, reducing feature dimensions and addressing overfitting. Neighborhood pooling decomposes features into a second level decomposition, reducing feature dimensions and addressing overfitting. It outperforms or performs comparably with other pooling methods like max, mean, mixed, and stochastic pooling in classification tasks on benchmark datasets. Convolutional Neural Networks (CNNs) have become the standard in image and object classification, consistently outperforming vector-based deep learning techniques. Researchers constantly upgrade CNN components like the convolutional and pooling layers to improve accuracy and efficiency beyond previous benchmarks. The convolutional and pooling layers of CNNs are constantly upgraded to improve accuracy and efficiency. Pooling, which originated from predecessors like Neocognitron and Cresceptron, subsamples the results of convolutional layers to reduce spatial dimensions, parameters, increase computational efficiency, and regulate overfitting. Pooling in deep learning is an operation that subsamples the results of convolutional layers to reduce spatial dimensions, parameters, increase computational efficiency, and regulate overfitting. The most popular forms of pooling are max pooling and average pooling, but other methods like mixed pooling and stochastic pooling use probabilistic approaches to address weaknesses in the deterministic methods. Pooling operations in deep learning, such as max pooling and average pooling, are efficient but have weaknesses that hinder optimal network learning. Other methods like mixed pooling and stochastic pooling use probabilistic approaches to address these issues. However, all pooling operations employ a neighborhood approach to subsampling, similar to nearest neighbor interpolation in image processing, which can introduce artifacts like edge halos and blurring. Minimizing data discontinuities is crucial for network regularization and improving classification accuracy. We propose a wavelet pooling algorithm that uses second-level wavelet decomposition to subsample features, aiming to reduce artifacts like edge halos and blurring. Our method is compared to other pooling techniques like max, mean, mixed, and stochastic pooling to demonstrate its effectiveness in producing comparable or superior results. The paper proposes an organic, subband method for feature representation with less artifacts. The proposed pooling method is compared to other techniques like max, mean, mixed, and stochastic pooling on various image classification datasets. The simulations are conducted in MATLAB R2016b. In this paper, the proposed pooling method is compared to other techniques on image classification datasets like CIFAR-10, Street House View Numbers (SHVN), and Karolinska Directed Emotional Faces (KDEF). The simulations are done in MATLAB R2016b. Pooling, also known as subsampling, reduces the dimensions of the output of the convolutional layer by summarizing regions into one neuron value. Max pooling and average pooling are the two most popular forms of pooling. In pooling, the output of the convolutional layer is condensed by summarizing regions into one neuron value using max pooling or average pooling. Max pooling selects the maximum value of a region, while average pooling calculates the average value. Max pooling and average pooling are methods used in pooling to condense the output of a convolutional layer. Max pooling selects the maximum value of a region, while average pooling calculates the average value. However, max pooling can erase details from an image if the main details have less intensity than insignificant details. Researchers have developed probabilistic pooling methods to address the shortcomings of max and average pooling. Mixed pooling combines both methods by randomly selecting one for better results. Researchers have created probabilistic pooling methods to address issues with max and average pooling. Mixed pooling combines both methods by randomly selecting one during training, applied in various ways within a layer. Probabilistic pooling methods, such as mixed pooling and stochastic pooling, offer improvements over traditional max and average pooling. Mixed pooling randomly selects between max and average pooling within a layer, while stochastic pooling samples from neighborhood regions based on activation probabilities. The stochastic pooling method selects activations within neighborhood regions based on probabilities, avoiding the limitations of max and average pooling. The process involves normalizing activations, sampling from a multinomial distribution, and selecting activations with the highest probabilities. Our proposed pooling method utilizes wavelets to reduce feature map dimensions, selecting activations based on probabilities to avoid limitations of max and average pooling. The proposed pooling method uses wavelets for dimension reduction and minimizing artifacts in image interpolation. It discards first-order subbands to capture data compression organically, reducing jagged edges and other artifacts that could affect image classification. The wavelet pooling scheme performs a 2nd order decomposition in the wavelet domain. The proposed wavelet pooling scheme reduces artifacts in image classification by performing a 2nd order decomposition in the wavelet domain using the fast wavelet transform (FWT). It involves approximation and detail coefficients, scaling and wavelet vectors, and sample representation in the vector. The fast wavelet transform (FWT) is used for a 2nd order decomposition in the wavelet domain to reduce artifacts in image classification. This involves approximation and detail coefficients, scaling and wavelet vectors, and sample representation in the vector. The FWT is applied twice on images, first on rows and then on columns, to obtain detail subbands (LH, HL, HH) at each decomposition level and an approximation subband (LL) for the highest decomposition level. Reconstruction of image features is done using only the 2nd order wavelet subbands. The proposed wavelet pooling algorithm performs backpropagation by reversing the process of its forward propagation, using the 2nd order wavelet subbands for image feature reconstruction. This method pools image features by a factor of 2 using the inverse FWT (IFWT) based on the IDWT. The wavelet pooling algorithm performs backpropagation by reversing the forward propagation process. It involves 1st and 2nd order wavelet decomposition of image features, followed by reconstruction using the IDWT for further backpropagation. CNN experiments utilize MatConvNet. The backpropagation algorithm of wavelet pooling involves 2nd order wavelet decomposition using Haar wavelet basis for image feature reconstruction. CNN experiments utilize MatConvNet and are run on a 64-bit operating system with GeForce Titan X Pascal GPUs. The experiments are conducted on a 64-bit operating system with an Intel Core i7-6800k CPU @ 3.40 GHz processor and 64.0 GB of RAM. Two GeForce Titan X Pascal GPUs with 12 GB of video memory are used for training. Different CNN structures are employed, with variations in regularization techniques like Dropout and Batch Normalization for CIFAR-10 and SHVN datasets. Each pooling method is tested individually with a 2x2 window for fair comparison. The pooling methods are tested on various datasets using a 2x2 window for fair comparison. The network architecture is based on the MNIST structure with batch normalization. The proposed method outperforms all other methods. The proposed method outperforms all other pooling methods tested on the MNIST database of handwritten digits. Different pooling methods show varying performance in terms of overfitting and error reduction during training. The proposed method outperforms other pooling methods on the MNIST database. Average and wavelet pooling show smoother learning and error reduction. Two sets of experiments were run with pooling methods, one without dropout layers and the other with dropout and batch normalization. The network structure for CIFAR-10 experiments is shown in FIG7. The network structure for CIFAR-10 experiments is illustrated in FIG7, using the full training and test data from the CIFAR-10 dataset. Our proposed method ranks second in accuracy, with wavelet pooling showing resistance to overfitting. Different pooling methods exhibit varying learning progressions, with mixed and stochastic pooling maintaining consistency. The change in learning rate prevents overfitting in our method, showing a slower learning propensity. Mixed and stochastic pooling maintain consistent learning progression. Two sets of experiments are run with pooling methods, one without dropout layers and the other with dropout. The SHVN dataset is used for training and testing, with 55,000 images in the case without dropout and the full training set in the case with dropout. The SHVN experiments involved using the SHVN dataset for training and testing. Two sets of experiments were conducted, one without dropout and the other with dropout. The network structure and results are shown in FIG0, with the proposed method having the second lowest accuracy. Max and wavelet pooling slightly overfit the data, while our method, following the path of max pooling, performed slightly better. Our proposed method in the SHVN experiments had the second lowest accuracy, with max pooling slightly overfitting the data. Mixed, stochastic, and average pooling showed a slow progression of learning. The KDEF experiments used the KDEF dataset with 4,900 images. The KDEF dataset contains 4,900 images of 35 people displaying seven basic emotions using facial expressions at five different poses. The dataset had errors such as missing or corrupted images, which were fixed by the researchers. The KDEF dataset contains 4,900 images of 35 people displaying seven basic emotions at different poses. Errors like missing or corrupted images were fixed by mirroring counterparts in MATLAB and manually cropping to match dimensions. The data was shuffled, with 3,900 images for training and 1,000 for testing, resized to 128x128 due to constraints. Dropout layers regulate the network. The KDEF dataset contains 4,900 images of 35 people displaying seven basic emotions at different poses. The data was shuffled, with 3,900 images for training and 1,000 for testing, resized to 128x128 due to constraints. Dropout layers regulate the network. Our proposed method has the second highest accuracy, with different pooling methods showing varying levels of overfitting resistance. Stochastic pooling and wavelet pooling maintain consistent learning progression. Wavelet pooling, average pooling, mixed pooling, and stochastic pooling show varying levels of overfitting resistance. Wavelet pooling follows a smoother learning progression. Computational complexity is a challenge for wavelet pooling, requiring improvements in efficiency. The proposed method serves as a proof-of-concept with potential for enhancements in computational efficiency. The proposed method aims to improve computational efficiency by reducing the number of mathematical operations required for the algorithm. The code implementation is not optimized, leaving room for enhancements and further research contributions. The proposed method aims to improve computational efficiency by reducing mathematical operations. Efficiency is calculated based on operations for different pooling methods like max, average, mixed, stochastic, and wavelet pooling. The study compares different pooling methods for computational efficiency, including average, max, mixed, stochastic, and wavelet pooling. Average pooling requires the least number of computations, followed by mixed and max pooling. Stochastic pooling is the least efficient method. The table compares computational efficiency of different pooling methods in forward propagation. Average pooling is the most efficient, followed by mixed and max pooling. Stochastic pooling is less efficient, while wavelet pooling is the least efficient due to subband coding implementation. Wavelet pooling is the least computationally efficient method, using significantly more mathematical operations than average pooling. However, with improvements in coding practices, GPUs, and the FTW algorithm, it can become a viable option. Various enhancements to the FTW algorithm, such as multidimensional wavelets and parallelization, aim to improve efficiency in speed and memory. Wavelet pooling, with enhancements like multidimensional wavelets and parallelization, has the potential to outperform traditional methods in CNNs. It outperforms others in the MNIST dataset, most in CIFAR-10 and KDEF datasets, and performs well in the SHVN dataset. Dropout and batch normalization improve network regularization in our proposed method. Our proposed method, utilizing wavelet pooling with enhancements like multidimensional wavelets and parallelization, outperforms all but one in the CIFAR-10 and KDEF datasets, and performs within respectable ranges of the pooling methods that outdo it in the SHVN dataset. The addition of dropout and batch normalization demonstrates our method's response to network regularization, confirming previous studies that no one pooling method is superior, but some perform better depending on the dataset and network structure. Many networks alternate between different pooling methods to maximize effectiveness. Our results confirm that no one pooling method is superior, with some performing better depending on the dataset and network structure. Future work could involve varying wavelet basis and adjusting upsampling and downsampling factors for better image feature reduction. Improvements in retaining discarded subbands for backpropagation and enhancing the method of FTW could lead to higher accuracies and fewer errors. Improving downsampling factors in decomposition and reconstruction can enhance image feature reduction beyond 2x2 scale. Retaining discarded subbands for backpropagation may increase accuracies. Enhancing the method of FTW could boost computational efficiency. Analyzing SSIM of wavelet pooling versus other methods could validate the effectiveness of our approach."
}