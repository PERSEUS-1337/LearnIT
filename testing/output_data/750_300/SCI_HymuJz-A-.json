{
    "title": "HymuJz-A-",
    "content": "The text discusses the limitations of modern machine vision algorithms in learning visual relations, especially when faced with high intra-class variability. It highlights how convolutional neural networks struggle with visual-relation problems and may break down when unable to rely on rote memorization. Additionally, it introduces relational networks as a potential solution for solving complex visual question answering problems. Feedback mechanisms, such as working memory and attention, are proposed as key components for abstract visual reasoning, contrasting the limitations of modern machine vision algorithms in handling high intra-class variability. The left image was correctly classified as a flute by a deep CNN after training on millions of photographs, surpassing human accuracy on the ImageNet challenge. The right image, simpler in comparison, is being considered. The left image, a flute, was accurately classified by a deep CNN, surpassing human accuracy on the ImageNet challenge. In contrast, the simpler right image with two curves failed to be recognized by the CNN, highlighting its struggle to learn intuitive relations. The CNN accurately classified an image of a flute but struggled to recognize the concept of \"sameness\" depicted in another image with two curves. This difficulty in learning intuitive relations is evident in the CNN's performance. Contemporary computer vision algorithms struggle to learn the concept of \"sameness\" as shown in an image with two curves from the SVRT challenge BID6. This task has been difficult for CNNs and has been overshadowed by the success of relational networks on visual question answering benchmarks. The recent success of relational networks (RNs) in visual question answering benchmarks has overshadowed the limitations of modern computer vision algorithms in recognizing visual relations. RNs have only been tested on toy datasets, showing similar limitations as CNNs for tasks like same-different comparisons. This failure is surprising considering the ability of animals to recognize visual relations across species. The limitations of modern computer vision algorithms in recognizing visual relations are highlighted, despite the success of relational networks in visual question answering benchmarks. Animals have the ability to recognize visual relations across species, but existing models fail on relational reasoning problems. Previous work showed that black-box classifiers struggle on tasks from the synthetic visual reasoning test. There has been no systematic exploration of machine learning algorithms on relational reasoning problems. Previous studies have shown that black-box classifiers and CNN architectures struggle with visual relation tasks, even with extensive training data. CNNs failed to learn simple binary tasks without carefully engineered training schedules. CNNs and other visual reasoning networks have struggled with solving visual-relation tasks, with only a few successful attempts using carefully engineered training schedules. The question remains whether this difficulty is due to poor hyperparameters or a fundamental limitation of feedforward models. This study aims to systematically test the limits of these networks on visual-relation tasks. The study aims to test the limits of CNNs and other visual reasoning networks on visual-relation tasks, highlighting the strain these tasks pose on CNNs and the lack of improvement in RNs designed for visual-relation problems. The research suggests that brain mechanisms like working memory and attention play a crucial role in primates' ability to reason about visual relations. The study examines the role of brain mechanisms like working memory and attention in primates' visual reasoning abilities. It suggests that these mechanisms are crucial for solving complex visual-relation tasks and proposes a systematic performance analysis of CNN architectures on SVRT problems to differentiate between hard same-different and easy spatial-relation problems. The study conducts a systematic performance analysis of CNN architectures on SVRT problems, revealing a dichotomy between hard same-different and easy spatial-relation problems. It introduces a visual-relation challenge demonstrating CNNs solve same-different tasks through rote memorization and shows how a modification of the sort-of-CLEVR challenge disrupts state-of-the-art relational network architectures. The aim is to encourage the computer vision community to reconsider visual question answering challenges and seek inspiration from neuroscience and cognitive science for design improvements. The SVRT challenge consists of twenty-three binary classification problems where stimuli obey abstract rules. It aims to disrupt state-of-the-art relational network architectures and inspire the computer vision community to look to neuroscience for visual reasoning architecture design. Classes in the SVRT challenge differ based on abstract rules, with stimuli depicting simple black curves on a white background. CNNs were trained on each of the twenty-three problems, with different hyper-parameter combinations. Nine CNNs were trained on each of the twenty-three SVRT problems, with different hyper-parameter combinations. The CNNs produced lower accuracies on same-different problems compared to spatial-relation problems, with one problem requiring detection of both relations simultaneously. The study tested CNNs with varying depths and receptive field sizes on twenty-three SVRT problems, showing lower accuracies on same-different tasks compared to spatial-relation tasks. The study tested CNNs with varying depths and receptive field sizes on twenty-three SVRT problems, showing lower accuracies on same-different tasks compared to spatial-relation tasks. The networks used pooling kernels of size 3\u00d73, convolutional strides of 1, pooling strides of 2, and three fully connected layers. ReLu activations were used in the pooling layers. 2 million examples were generated for each problem, split evenly into training and test sets. Nine networks were trained on each problem using an ADAM optimizer with a base learning rate of \u03b7 = 10 \u22124. The accuracy of the best networks obtained for each problem individually is shown in FIG0. After training CNNs on twenty-three SVRT problems with varying depths and receptive field sizes, the best networks' accuracies were sorted and colored based on problem descriptions. Same-Different (SD) problems with congruent items were colored red, while Spatial-Relation (SR) problems with phrases like \"left of\" were colored blue. CNNs were trained on twenty-three SVRT problems with varying depths and receptive field sizes. The problems were categorized into Same-Different (SD) problems, colored red, and Spatial-Relation (SR) problems, colored blue. CNNs performed much worse on SD problems compared to SR problems, with some SD problems resulting in accuracy not substantially above chance. This suggests that SD tasks pose a particularly difficult challenge to CNNs. The analysis shows that CNNs struggle more with Same-Different (SD) problems compared to Spatial-Relation (SR) problems. Some SD problems had accuracy barely above chance, indicating a significant challenge for CNNs. Additionally, hyperparameter search revealed that SR problems are consistently learned well across all network configurations, with minimal differences in accuracy. Conversely, larger networks generally achieved higher accuracy on SD problems. Our hyperparameter search showed that SR problems are equally well-learned across all network configurations, with minimal differences in accuracy. Larger networks yielded significantly higher accuracy on SD problems. Experiment 1 confirmed previous studies that feedforward models perform poorly on visual-relation problems. The study confirms previous findings that feedforward models perform poorly on visual-relation problems, even when hyperparameters are optimized. The SVRT challenge, while useful, has limitations in its sample selection of visual relations. The SVRT challenge uses a limited sample of visual relations, which may not fully represent all possible scenarios. The images in the challenge are categorized based on same/different and horizontal/vertical patterns. The SVRT challenge categorizes images based on same/different and horizontal/vertical patterns. Images are generated with specific parameters and different problems have unique image structures, making direct comparisons challenging. For example, Problem 2 requires one large and one small object in the image. Comparing visual-relation problems in the SVRT challenge is difficult due to unique image structures and distributions. For instance, Problem 2 requires a large and small object in an image, conflicting with other problems like Problem 1 where objects must be identically-sized. Different problems also vary in the number of objects needed in an image. To improve comparison, defining problems on the same set of images would be more effective. Using closed curves as items in SVRT images hinders quantification and control of image variability. The ad hoc procedure for generating these objects makes it challenging to determine the impact of image variability on task difficulty within a single problem. The PSVRT challenge aims to address issues with SVRT by creating a new dataset with two problems: Spatial Relations (SR) and Same-Different (SD). This dataset helps quantify image variability and its impact on task difficulty. To address issues with SVRT, a new dataset was created with two problems: Spatial Relations (SR) and Same-Different (SD). The image generator produces gray-scale images based on simple rules for classification. The image generator produces gray-scale images with square binary bit patterns on a blank background. It uses parameters like item size, image size, and number of items to control image variability. The image generator uses parameters like item size, image size, and number of items to control image variability. Item size (m) and image size (n) determine the spatial extent and placement of individual items, while the number of items (k) affects both item and spatial variability. The SD category label is determined by the presence of at least 2 identical items when k \u2265 3. The image generator uses parameters like item size, image size, and number of items to control image variability. When k \u2265 3, the SD category label is determined by whether there are at least 2 identical items in the image, and the SR category label is determined by the average orientation of displacements between pairs of items. The number of possible images in a dataset can be quantified using specific parameters. The image generator uses parameters to control image variability, such as item size, image size, and number of items. The number of possible images in a dataset can be quantified using specific parameters, allowing for the generation of images through a test called Parametric SVRT (PSVRT). Images are created by sampling joint class labels for SD and SR, with the creation of identical copies based on the sampled SD label. The image generator creates images by sampling joint class labels for SD and SR, with the creation of identical copies based on the sampled SD label. Items are randomly placed in an n \u00d7 n image with background spacing. The goal is to examine the difficulty of learning PSVRT problems over a range of image distributions. In this experiment, images are generated by drawing class labels for both same-different and spatial-relation problems to ensure identical image distribution. The goal was to assess the difficulty of learning PSVRT problems with varying image parameters. A baseline architecture was established for learning these problems, and instances were trained from scratch for different combinations of item size, image size, and item number. The number of training examples needed for each session was measured. The study focused on training a baseline architecture for PSVRT problems with varying image parameters. Training sessions were conducted for different combinations of item size, image size, and item number to measure the number of training examples needed to reach 95% accuracy. The goal was to estimate the difficulty of fitting training data in various conditions without a holdout test set. The study trained a baseline CNN architecture from scratch to estimate the difficulty of fitting training data in various conditions without a holdout test set. Three sub-experiments were conducted by varying image parameters separately to examine their effect on learnability. Each condition had 20 million training images with a batch size of 50. The baseline CNN was trained with 20 million images and a batch size of 50. Different configurations were tested with varying parameters such as k, n, and m. The network consisted of four convolution and pool layers with different kernel sizes. The best results were reported for each experimental condition out of 10 random initializations. The CNN model had convolution and pool layers with different kernel sizes, followed by fully-connected layers. Dropout was used in the last fully-connected layer. An ADAM optimizer with a base learning rate was employed. The network size was varied to study its effect on learnability. The study used an ADAM optimizer with a base learning rate and Xavier method for weight initialization. Experiments were repeated with a larger network size, showing a strong dichotomy in learning curves. A \"learning event\" was observed where accuracy suddenly rises from chance-level to 100%. The study observed a \"learning event\" where accuracy rapidly increases from chance-level to 100%. Training runs that exhibited this event almost always reached 95% accuracy within 20 million training images. If 95% accuracy was not reached, a learning event did not occur. The study observed a \"learning event\" where accuracy rapidly increases from chance-level to 100%. Training runs that exhibited this event almost always reached 95% accuracy within 20 million images. The final accuracy showed a strong bi-modality - chance-level or close to 100%. In the experiments, the learning event occurred immediately after training began and accuracy reached 95% soon after. In SR, no straining effect was found across all image parameters. The learning event occurred immediately after training began, with accuracy reaching 95% soon after. In SD, a significant straining effect was observed with image size and number of items. Increasing image size led to higher TTA and decreased likelihood of the learning event. The network learned SD in 7 out of 10 random initializations for baseline parameters, but only in 4 out of 10 on 120 \u00d7 120 images. Image sizes of 150 \u00d7 150 and above never exhibited a learning event. The network showed a strong straining effect in learning the Same-Different task, with only 4 out of 10 initializations successful on 120 \u00d7 120 images. Image sizes of 150 \u00d7 150 and above never led to a learning event, and having 3 or more items in an image also hindered learning. Relaxing the same-different rule to allow for rotation by a multiple of 90\u00b0 quadrupled the number of matching images in the dataset, but the CNN still did not learn the task. The relaxation of the strict same-different rule by allowing rotation by a multiple of 90\u00b0 quadrupled the number of matching images in the dataset. However, this imposed a severe strain on CNNs, hindering their ability to learn the task due to the increased number of \"same\" templates. The straining effects are believed to be influenced by image size and item number, which contribute to image variability exponentially. Increasing image size and item number imposes a severe strain on CNNs, affecting image variability exponentially. The straining effect is equally strong across different network widths, with a constant rightward shift in the TTA curve over image sizes. Increasing item size had no visible straining effect on CNN, unlike the exponential-rate increase in image variability with an increase in the number of items. Learnability remains stable across different item sizes, indicating the possibility of constructing feedforward feature detectors that can generalize to coordinated item variability. When CNNs learn a PSVRT condition, they build a feature set tailored for a specific dataset rather than learning a general rule. The study found that CNNs build a feature set specific to a dataset when learning a PSVRT condition, rather than learning a general rule. The CNN's ability to capture visual relations was shown to be sensitive to irrelevant image variations, leading to decreased performance with increasing image variability. The CNN in the experiment showed increasing sensitivity to irrelevant image variations, leading to a lack of learning. This suggests that the features learned are not invariant rule-detectors but templates covering a specific image distribution. The Relational Network (RN) was proposed as an architecture designed to detect visual relations and tested on various VQA tasks. The Relational Network (RN) is an architecture explicitly designed to detect visual relations and outperforms a baseline CNN on various visual reasoning problems by learning a map from pairs of high-level CNN feature vectors to answers to relational questions. The Relational Network (RN) outperforms a baseline CNN on visual reasoning problems by processing input through LSTM or hardcoded binary strings. The RN excelled on a VQA task called \"sort-of-CLEVR\" with scenes containing simple 2D items, answering both relational and non-relational questions. The sort-of-CLEVR task involves scenes with simple 2D items in a grid (circle, square) and six colors (yellow, green, orange, blue, red, gray). The task requires answering relational and non-relational questions, but has limitations in promoting true understanding of sameness and has low item variability, leading to rote memorization for problem-solving. The study focused on training a model on a two-item sort-of-CLEVR same-different task and PSVRT stimuli to measure the ability of a Relational Network (RN) to transfer concepts and solve problems without relying on rote memorization. The study aimed to measure the ability of a Relational Network (RN) to transfer the concept of same-different from training to novel objects. The model used a convolutional network with four layers and 24 features per layer. Architecture details can be found at https://github.com/gitlimlab/Relation-Network-Tensorflow. The model consisted of a convolutional network with four layers, each with 24 features, using ReLu activations and no pooling. The RN part included a 4-layer MLP with 256 units per layer, followed by a 3-layer MLP with 256 units per layer. The system used ReLu activations, 50% dropout in the penultimate layer, softmax function in the final layer, and was trained with cross-entropy loss using ADAM optimizer with a base learning rate of 2.5 \u00d7 10 \u22124. The model used a convolutional network with four layers, ReLu activations, and no pooling. The penultimate layer had 50% dropout, and the final layer used a softmax function. Training was done with cross-entropy loss and ADAM optimizer. The weights were initialized using Xavier initialization. This architecture was able to reproduce results from BID22 on the sort-of-CLEVR task. The model used a convolutional network with four layers, ReLu activations, and no pooling. The penultimate layer had 50% dropout, and the final layer used a softmax function. Training was done with cross-entropy loss and ADAM optimizer. The weights were initialized using Xavier initialization. This architecture was able to reproduce results from BID22 on the sort-of-CLEVR task. We constructed twelve different versions of the sort-of-CLEVR dataset, each missing one of the twelve possible color+shape combinations. Images depicted two items, half of the time being the same color and shape. The CNN+RN architecture was trained to detect sameness while measuring validation accuracy on left-out images. Learning stopped at 95% training accuracy, and averages were taken across all left-out conditions. The CNN+RN model was trained to detect sameness in color+shape combinations on the sort-of-CLEVR task. It did not generalize well to left-out conditions, learning faster than CNNs on PSVRT stimuli. Training accuracy quickly reached 90%, but validation accuracy remained lower. The CNN+RN model trained on sort-of-CLEVR task learns faster than CNNs on PSVRT stimuli but struggles to generalize to left-out conditions. Training accuracy reaches 90%, but validation accuracy remains low, indicating a lack of transfer of same-different ability. The CNN+RN model trained on sort-of-CLEVR dataset with one item type left out struggles to generalize to left-out conditions. Training accuracy reaches 90%, but validation accuracy remains low, indicating a lack of transfer of same-different ability. The experiment then replaced simple shapes with PSVRT bit patterns and varied image size while measuring TTA. Training was done on 20M images over three different repetitions. The CNN+RN model trained on sort-of-CLEVR struggled to generalize when one item type was left out. Training accuracy reached 90%, but validation accuracy remained low. PSVRT bit patterns were used instead of simple shapes, with image size varied from 30 to 180 pixels. Training was conducted on 20M images over three repetitions. The CNN+RN behaved like a vanilla CNN, achieving over 95% accuracy for image sizes of 120 or below, but did not learn for sizes of 150 and 180. This cutoff point may correspond to the representational capacity of the RN architecture. The CNN+RN model achieved over 95% accuracy for image sizes up to 120 pixels, but did not learn for sizes of 150 and 180 pixels. This cutoff point may correspond to the representational capacity of the RN architecture, indicating that visual-relation problems can exceed the capacity of CNNs. Learning templates for arrangements of objects become rapidly intractable for deep networks. Our results show that visual-relation problems can surpass the capacity of CNNs due to the combinatorial explosion in the number of templates needed for arrangements of objects. This limitation, overlooked by computer vision scientists, has long been recognized by cognitive scientists. In contrast, biological visual systems excel at detecting relations. Biological visual systems excel at detecting relations, which has been acknowledged by cognitive scientists. Current computer vision scientists have overlooked the limitation of feedforward networks in detecting relations. Humans can learn complicated visual rules and generalize them from just a few training examples. For example, participants could learn a complex rule from about 6 examples, surpassing the capacity of CNNs. The visual reasoning ability of humans surpasses that of CNNs, as humans can learn complex rules from just a few examples, while even the best performing network struggles after a million training examples. This ability is not unique to humans, as birds and primates can also be trained to recognize and transfer knowledge of same-different relations to novel objects. High-throughput search struggled to exceed chance levels after a million training examples. Birds and primates, like humans, can learn same-different relations and apply this knowledge to new objects. Ducklings have shown the ability to perform a one-shot version of a visual reasoning experiment from birth, demonstrating a preference for novel objects based on training phase observations. Ducklings in Experiment 3 quickly learned the concepts of same and different from a single example, showing a preference for novel objects based on their training. In contrast, the CNN+RN in the same experiment struggled to transfer this concept even after extensive training. The CNN+RN in Experiment 3 struggled to transfer the concept of same-different to novel objects, despite extensive training. Feedback signals beyond feedforward processes may play a role in visual-relation detection. The detection of natural object categories can be achieved with a single feedforward sweep in the visual cortex, but object localization in clutter may require attention due to the spatial coarseness of the feedforward sweep. Object localization in clutter requires attention as the feedforward sweep in the visual cortex is too spatially coarse. Neuroscience evidence suggests that processing spatial relations between objects in a cluttered scene also requires attention, even when objects can be detected pre-attentively. The processing of spatial relations in a cluttered scene requires attention, even when objects can be detected pre-attentively. Working memory in prefrontal and premotor cortices plays a role in solving tasks that involve spatial reasoning. The computational role of attention and working memory in detecting visual relations is a key question. The role of working memory in prefrontal and premotor cortices is crucial when solving tasks involving spatial reasoning. Attention and working memory play a computational role in detecting visual relations by allowing flexible representations to be dynamically constructed at run-time, avoiding the need to store visual-relation templates in synaptic weights. This dynamic construction helps prevent the capacity issues associated with storing templates for all possible relations. Humans can easily detect visual relations and construct structured descriptions about the visual world around them, surpassing modern computers in this ability. This is achieved by dynamically constructing flexible representations at run-time, avoiding the need to store visual-relation templates in synaptic weights. Humans can effortlessly construct structured descriptions about the visual world around them, surpassing modern computers in detecting visual relations. Understanding attentional and mnemonic mechanisms is crucial for computational visual reasoning."
}