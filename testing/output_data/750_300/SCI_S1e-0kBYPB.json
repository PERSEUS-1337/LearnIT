{
    "title": "S1e-0kBYPB",
    "content": "To ensure widespread public acceptance of AI systems, it is crucial to develop methods that can explain the decisions made by black-box models like neural networks. Current explanatory methods face two main issues. Firstly, different perspectives on explanations lead to varying instance-wise explanations, causing confusion when directly compared. Secondly, post-hoc explainers have only been validated on simple models like linear regression, raising concerns when applied to real-world neural networks. We introduce a verification framework for explanatory methods in AI systems, focusing on neural networks. Our framework is based on a non-trivial neural network architecture trained on a real-world task, providing guarantees on its inner workings. A verification framework for explanatory methods in AI systems is introduced, focusing on feature-selection perspective. The framework is based on a neural network architecture with guarantees on its inner workings, aiming to provide off-the-shelf evaluation for explanations. A large number of post-hoc explanatory methods have been developed to explain black-box machine learning models. Two widely used perspectives on explanations are feature-additivity and feature-selection. Two widely used perspectives on explanations in post-hoc explanatory methods for black-box machine learning models are feature-additivity and feature-selection. These perspectives lead to fundamentally different explanations for instance-wise explanations, with methods adhering to different perspectives being directly compared in practice. For example, Chen et al. (2018) and Yoon et al. (2019) compare L2X, a feature-selection explainer, with LIME. In practice, different explanatory methods are compared, such as L2X, LIME, and SHAP. These comparisons may not be coherent due to fundamentally different explanation targets. Current methods successfully identify biases but have strengths and limitations. The text discusses the strengths and limitations of current explanatory methods for neural networks, highlighting the challenge of explaining models with less dramatic biases. The ground-truth decision-making process of neural networks is unknown, making it difficult to evaluate explainers applied to complex real-world datasets. The text discusses the challenge of explaining neural networks with less dramatic biases, as the ground-truth decision-making process is unknown. Evaluating explainers on complex real-world datasets is difficult due to this uncertainty. In addressing the challenge of explaining neural networks with unknown decision-making processes, a framework is proposed to generate evaluation tests for explanatory methods under feature-selection. This is necessary due to the unreliable nature of penalizing explainers for pointing to insignificant tokens. The framework proposed aims to generate evaluation tests for explanatory methods under the feature-selection perspective. It identifies tokens with zero contribution and tests if explainers rank them higher than relevant tokens. The framework aims to evaluate explanatory methods by identifying tokens with zero contribution and testing if explainers prioritize them over relevant tokens. The study was conducted on three pairs of models for multi-aspect sentiment analysis. The study introduces a framework for evaluating explanatory methods by testing if explainers prioritize tokens with zero contribution over relevant tokens. It was conducted on three pairs of models for multi-aspect sentiment analysis. The framework generates evaluation tests with guarantees on the target model's behavior to test explainers for critical failures. The framework introduced in the study evaluates explainers by testing if they prioritize tokens with zero contribution over relevant tokens. It generates evaluation tests with guarantees on the target model's behavior to test explainers for critical failures. The study also evaluates L2X, a feature-selection explainer, under this test. The study evaluates LIME and SHAP explainers compared to L2X in a feature-selection test. LIME and SHAP generally outperform L2X, with details on this performance discussed in Section 5. Error rates of these explainers are provided to highlight potential failures in the feature-selection perspective. In Section 5, it is explained why LIME and SHAP generally outperform L2X. Error rates of these explainers are provided to show potential failures in feature selection. The study encourages the community to use their test for evaluating future explanatory methods. Our test for explanatory methods under the feature-selection perspective is generic and can be applied to other tasks. Feature-based explainers provide signed weights for input features, explaining predictions. Feature-based explainers provide signed weights for input features, explaining predictions. There are two major types of explanations: feature-additive, which assigns weights to each input feature, and feature-selective, which identifies a subset of features responsible for the prediction. Other types include example-based explanations. In this work, the focus is on verifying feature-based explainers, which provide signed weights for input features to explain predictions. Other types of explanations include example-based and human-level explanations. In this work, the focus is on verifying feature-based explainers, which provide signed weights for input features to explain predictions. Evaluations commonly include testing explainers on linear regression and decision trees, as well as support vector representations. The evaluation of explainers often involves testing them on simple models like linear regression and decision trees, or synthetic tasks with controlled feature sets. However, these may not accurately represent the complex neural networks used in practice. In evaluation setups, explainers are often tested on simple models or synthetic tasks with controlled feature sets. This may not accurately represent the complexity of real-world neural networks. Another popular evaluation setup involves identifying intuitive heuristics that high-performing models are assumed to follow. For example, in sentiment analysis. In evaluation setups, explainers are often tested on simple models or synthetic tasks with controlled feature sets, which may not accurately represent the complexity of real-world neural networks. Crowd-sourcing evaluation is performed to ensure that the features produced by the explainer align with the model's prediction, but neural networks may still rely on surprising artifacts even when achieving high accuracy. This evaluation method may not always be reliable. The evaluation of explainers may not accurately represent real-world neural networks as they may rely on surprising artifacts despite high accuracy. Evaluating if explanations help humans predict the model's behavior involves presenting predictions and explanations for inference by humans to determine the effectiveness of different explainers. The evaluation of explainers involves presenting predictions and explanations for inference by humans to determine the effectiveness of different explainers. An automatic evaluation method is proposed as an alternative to the costly and labor-intensive human-based approach. Our evaluation framework is fully automatic and targets non-trivial neural network models, providing guarantees on their inner-workings. It is more challenging than previous tests as it requires different explanations for models trained on real data compared to randomized data. The sanity check introduced by Adebayo et al. (2018) tests the fidelity of explainers to the target model. Current explanatory methods adhere to two major perspectives of explanations, including Feature-additivity. Perspective 1 (Feature-additivity) explains the prediction of a model f for an instance x using contributions from each feature i of x. Many explanatory methods follow this perspective, such as LIME and methods unified by Lundberg & Lee (2017). The prediction of a model for an instance is explained using feature-additive contributions, with Shapley values from game theory being the only set that satisfies desired constraints. The contribution of each feature is an average over a neighborhood of the instance. The contribution of each feature in an instance is an average over a neighborhood of the instance, with the choice of neighborhood being critical. The explanation of a model for an instance involves feature-additive contributions, with Shapley values being the only set that satisfies desired constraints. The choice of neighborhood is critical in determining the contribution of each feature in an instance. Different perspectives exist on feature selection for model explanation, with some focusing on identifying a small subset of features that lead to similar predictions as the original model. The choice of neighborhood is crucial for feature selection in model explanation. Different perspectives exist on this, with some aiming to identify a small subset of features that lead to similar predictions as the original model. Chen et al. (2018), Carter et al. (2018), and Ribeiro et al. (2018) support this view. L2X (Chen et al., 2018) maximizes mutual information between S(x) and prediction, assuming |S(x)| is known, which is often not the case. This approach may not always hold true, but can be valid for tasks like sentiment analysis. The model relies on a small subset of features for certain tasks like sentiment analysis. A hypothetical sentiment analysis regression model is used to illustrate the differences between perspectives. The model behavior is similar to real-world neural networks, which may heavily rely on biases in datasets. The hypothetical sentiment analysis regression model behavior is similar to real-world neural networks, which may heavily rely on biases in datasets. For example, natural language inference neural networks trained on SNLI may heavily rely on specific tokens in the input, which may not be accurate indicators for the correct target class. The feature-additive perspective in sentiment analysis regression models aims to explain the model's behavior by highlighting the most relevant features for different classes, such as \"nice\" for entailment, \"tall\" for neutral, and \"sleeping\" for contradiction. This perspective provides insights into how the model assigns scores based on the presence or absence of specific features. The feature-additive perspective in sentiment analysis regression models aims to explain the model's behavior by highlighting relevant features for different classes. It shows how the model assigns scores based on specific features like \"nice\" for entailment, \"tall\" for neutral, and \"sleeping\" for contradiction. The feature-selective perspective focuses on pointwise features used by the model on instances in isolation, ranking \"good\" and \"nice\" as important features. The feature-selective perspective in sentiment analysis regression models ranks \"good\" and \"nice\" as important features on instance x 2, showing a difference from the feature-additive perspective. While both perspectives offer insights into the model's behavior, the preference may vary depending on real-world use-cases. The paper proposes a verification framework for the feature-selection perspective of instance-wise explanations. The paper proposes a verification framework for the feature-selection perspective of instance-wise explanations in sentiment analysis regression models. It leverages the RCNN model architecture and introduces metrics to measure explainers' failures. The paper introduces a verification framework for instance-wise explanations in sentiment analysis regression models using the RCNN model architecture. It includes metrics to measure explainers' failures in ranking irrelevant tokens lower than relevant ones. The RCNN consists of a generator and an encoder instantiated with recurrent convolutional neural networks. The RCNN model (Lei et al., 2015) includes a generator and an encoder trained jointly with supervision only on the final prediction. The generator selects a subset of tokens from the input text x, passed to the encoder for the final prediction. Two regularizers were used to encourage the generator's selection process. The final prediction is based solely on S x without direct supervision on subset selection. The generator and encoder were trained jointly with supervision only on the final prediction. Two regularizers were used to encourage the generator to select a short sub-phrase and fewer tokens. Gradients for the generator were estimated using a REINFORCE-style procedure to handle non-differentiability. This intermediate hard selection allows tokens that may not contribute to the final prediction. The gradients for the generator were estimated using a REINFORCE-style procedure, allowing tokens that do not contribute to the final prediction. The model may have learned an internal communication protocol that encodes information from non-selected tokens, known as a handshake. The model may have learned an internal communication protocol (handshake) that encodes information from non-selected tokens. The goal is to eliminate handshakes by gathering a dataset where non-selected tokens have zero contribution to the model's prediction. The model aims to eliminate handshakes by ensuring that non-selected tokens have zero contribution to its prediction. The proof of this concept is shown in Appendix B, where the model's behavior is illustrated using an example. Equation 7 effectively captures the presence of handshakes in the model's predictions. The model eliminates handshakes by ensuring non-selected tokens have zero contribution to its prediction. S Sx = S x does not guarantee a handshake, as some tokens may be non-selected without affecting the prediction. To simplify, instances where S Sx = S x are retained. After pruning the dataset to retain instances where S Sx = S x, it is uncertain if all non-selected tokens are relevant to the prediction. Some tokens may be noise but were selected to ensure a contiguous sequence. The RCNN was penalized for selecting disconnected tokens during training. This ensures explainers are not penalized for not differentiating irrelevant tokens. To prevent penalizing explainers for not distinguishing between noise and relevant features, the dataset is pruned to include at least one clearly relevant token. This token must significantly impact the prediction when removed, ensuring its relevance is confirmed. To confirm the relevance of selected tokens for prediction, a significant threshold is used to determine if a token is clearly relevant. Tokens are divided into clearly relevant (SR) and selected don't know (SDK) categories based on their impact on prediction when removed. This ensures that at least one clearly relevant token is included in the dataset. The text discusses the partitioning of tokens into clearly relevant (SR) and selected don't know (SDK) categories based on their impact on prediction. It emphasizes that even if a selected token alone does not significantly change the prediction, it may still be relevant in combination with other tokens. The procedure ensures that tokens with a high impact on prediction are ranked higher than non-selected tokens with zero contribution. The procedure ensures that tokens with a high impact on prediction are ranked higher than non-selected tokens with zero contribution. The dataset is pruned to retain datapoints with at least one clearly relevant token per instance. The procedure does not provide an explainer but guarantees that all tokens in SR x are important. Our procedure guarantees that all tokens in N x are ranked lower than any token in SR x. The most important token must be in S x. Error metrics are defined based on the ranking provided by explainers on the features of an instance. Error metrics are defined based on the ranking provided by explainers on the features of an instance, including the percentage of instances where the most important token provided by the explainer is among the non-selected tokens, the percentage of instances where at least one non-selected token is ranked higher than a clearly relevant token, and the average number of non-selected tokens ranked higher than any clearly relevant token. Metric (A) shows the most dramatic failure. In this work, error metrics are defined based on the ranking provided by explainers on the features of an instance. Metric (B) shows the percentage of instances with errors in the explanation, while metric (C) quantifies the number of zero-contribution features ranked higher than clearly relevant ones. The framework is instantiated on the RCNN model trained on the BeerAdvocate corpus. The RCNN model trained on the BeerAdvocate corpus evaluates human-generated beer reviews with aspects like appearance, aroma, and palate. The model predicts ratings rescaled between 0 and 1. The RCNN model trained on the BeerAdvocate corpus evaluates human-generated beer reviews based on appearance, aroma, and palate. Reviews are accompanied by fractional ratings between 0 and 5 for each aspect. The model aims to predict ratings rescaled between 0 and 1. Three separate RCNNs are trained for each aspect independently. Three datasets are gathered, one for each aspect, with a threshold of \u03c4 = 0.1 to select relevant tokens for prediction. The dataset includes three separate RCNN models trained on BeerAdvocate reviews for appearance, aroma, and palate. A threshold of \u03c4 = 0.1 is used to select relevant tokens for prediction, with statistics provided in Appendix A such as average review lengths and selected token counts. In Appendix A, statistics are provided for the datasets, including average review lengths and selected token counts. The threshold of 0.1 for selecting relevant tokens shows strictness, with 1 or 2 relevant tokens per datapoint. Three popular explainers, LIME, SHAP, and L2X, are evaluated. In the evaluation test, three popular explainers - LIME, SHAP, and L2X - were tested with default settings for text explanations. For L2X, the dimension of word embeddings was set to 200 and training was allowed for a maximum of 30 epochs. The evaluation did not directly target LIME and SHAP due to their feature-additivity perspective. In the evaluation, L2X had word embeddings set to 200 dimensions and trained for a maximum of 30 epochs. LIME and SHAP outperformed L2X on most metrics, despite L2X being a feature-selection explainer. A limitation of L2X is the need to know the number of important features per instance. In practice, L2X learns a distribution over features by maximizing mutual information between subsets of features and the response variable. The average number of tokens highlighted by human annotators was used as K for testing L2X, resulting in average K values of 23, 18, and 13 for three aspects. All explainers in Table 1 are prone to errors on metric (A). In Table 1, explainers like LIME and L2X often highlight tokens with zero contribution as the most relevant feature, leading to ranking mistakes. SHAP, on average, performs better in placing relevant features. In a comparison of explainers, SHAP generally performs better in ranking relevant features compared to LIME and L2X. However, both LIME and L2X have instances where they rank zero-contribution tokens higher than clearly relevant features, indicating ranking mistakes. In a comparison of explainers, SHAP generally outperforms LIME and L2X in ranking relevant features. However, both LIME and L2X sometimes prioritize zero-contribution tokens over clearly relevant ones, leading to ranking errors. Figure 4 shows explainers' rankings on an instance from the palate aspect, with top 5 features displayed. Figure 6 presents another example from the dataset, with a heatmap illustrating token rankings by each explainer. Only the top 10 ranked tokens are shown for visibility, with bold tokens from Sx and underlined tokens from SRx. The heatmap displays token rankings by explainers LIME and SHAP, with LIME ranking \"mouthfeel\" and \"lacing\" as top tokens, while SHAP ranks \"gorgeous\" lower. L2X prioritizes \"taste\" over relevant words, showing errors in ranking. The text discusses the differences in token rankings between LIME, SHAP, and L2X explainers, highlighting how L2X prioritizes \"taste\", \"great\", \"mouthfeel\", and \"lacing\" as important tokens over \"gorgeous\". The study introduces an evaluation test for post-hoc explanatory methods. In this work, a distinction between two perspectives of explanations is highlighted. An evaluation test for post-hoc explanatory methods is introduced, offering guarantees on the behavior of real-world neural networks. Error rates on different metrics for three popular explanatory methods are presented to raise awareness of potential failures. The error rates on different metrics for three popular explanatory methods are presented to highlight potential failures in explainers. The methodology is generic and can be adapted to various tasks and areas, such as computer vision. The core idea is to check for zero contribution of non-selected super-pixels in neural network predictions. The evaluation procedure involves selecting super-pixels and making predictions based on blurred images. The core algorithm of post-hoc explainers is domain-agnostic, highlighting fundamental limitations. Statistics of the dataset are provided in Table 2. The statistics of the dataset in Table 2 show the number of instances retained, average review length, and average numbers of selected tokens. Percentages of instances eliminated and datapoints further eliminated are also provided. The statistics in Table 2 include the number of instances retained, average review length, and average number of selected tokens. The column Sx provides the percentage of instances eliminated due to a potential handshake, while |SRx| = 0 shows the percentage of datapoints further eliminated due to the absence of a selected token with an absolute effect of at least 0.1 on the prediction. The model should have a different prediction when a non-selected token influences the final prediction. If all non-selected tokens are eliminated and the model gives the same prediction, it indicates no handshake in the instance. This implies that the tokens have zero contribution. The absence of a handshake in an instance is indicated when all non-selected tokens have zero contribution to the final prediction. This is confirmed by the equation RCNN(S x ) = RCNN(x) =\u21d2 no handshake in x. The proof is completed by ensuring S Sx = S x, as shown in Equation 11. The proof is completed by ensuring S Sx = S x, which indicates no handshake in x. LIME: nice brown \"grolsch\" like bottle, pours a dark yellow color with fruit and slight warming notes, better than most American lagers. The beer in a nice brown \"grolsch\" like bottle pours a dark fizzy yellow color with a fruity aroma. It tastes of fruit with a slight warming sensation, better than most American lagers. The beer, poured from a nice brown \"grolsch\" like bottle, has a dark fizzy yellow color with a fruity aroma. It tastes of fruit with a slight warming sensation, better than most American lagers. The taste is smooth and the beer is easy to drink. The beer, poured from a \"grolsch\" like bottle, has a dark fizzy yellow color with a fruity aroma. It tastes of fruit with a slight warming sensation, better than most American lagers. The taste is smooth and easy to drink, with minimal mouthfeel and alcohol presence."
}