{
    "title": "rJeeKTNKDB",
    "content": "Our work in this paper extends prior state-of-the-art on graph-to-graph translation methods for molecular optimization by creating coherent multi-resolution representations. The graph decoder is fully autoregressive, adding new substructures while resolving their attachment to the molecule. Multiple molecular optimization tasks were evaluated, demonstrating the effectiveness of our model. Our graph decoder is fully autoregressive, adding new substructures while resolving their attachment to the molecule. The model significantly outperforms previous state-of-the-art baselines in multiple molecular optimization tasks. The task involves translating molecular graphs into better forms to improve chemical properties. It is challenging due to the vast space of potential candidates and complex dependencies in graph generation. Success relies on the inductive biases in the encoder-decoder architecture, similar to machine translation. Graph generation for molecular structures is complex due to dependencies in the joint distribution over nodes and edges. Prior work utilized valid chemical substructures to build molecular graphs, but the approach has limitations. The previous work used chemical substructures to create molecular graphs, but faced limitations. The current approach represents molecules as junction trees over substructures and atom-level graphs. However, the process is limited as the tree and graph encoding are done separately, leading to non-autoregressive attachment predictions that can be inconsistent. The proposed approach introduces a multi-resolution, hierarchically coupled encoder-decoder for graph generation. It utilizes an auto-regressive decoder to predict substructure components and their attachments in a sequence of triplet predictions. This method allows for modeling strong dependencies between successive substructures in molecule generation. The encoder is designed to represent molecules at different resolutions in order to match the proposed decoding process. Specifically, the encoding of each molecule proceeds across three levels, with each layer capturing essential information for its corresponding decoding step. The graph convolution of atoms at the lowest level supports the prediction of attachments and the convolution over substructures. The decoding process efficiently decomposes generation steps into smaller hierarchical steps to avoid combinatorial explosion. It can handle conditional translation by taking desired criteria as input, allowing for different combinations of criteria to be processed. The method efficiently decomposes generation steps into smaller hierarchical steps to avoid combinatorial explosion. It can handle conditional translation by taking desired criteria as input, enabling different combinations of criteria to be processed. The model addresses issues with invalid junction trees by interleaving tree and graph decoding steps, allowing predicted attachments to guide substructure prediction. Additionally, inconsistencies in local substructure attachments during training are tackled by proposing a non-autoregressive graph decoder. The proposed autoregressive decoder interleaves substructure prediction with attachments to address inconsistencies in local substructure attachments during training. The new model outperforms previous state-of-the-art graph generation methods in multiple molecular optimization tasks. Our model significantly outperforms previous state-of-the-art graph generation methods in discovering molecules with desired properties, showing improvements on QED and DRD2 optimization tasks. It runs 6.3 times faster during decoding and benefits from hierarchical decoding and multi-resolution encoding. Additionally, conditional translation can generalize even when trained on a small percentage of molecular pairs. Previous work in molecular graph generation has utilized various methods to generate molecules based on their SMILES strings. Our hierarchical decoding and multi-resolution encoding approach outperforms existing methods, demonstrating success in conditional translation even with limited training data. Various methods have been developed to generate molecules based on their SMILES strings, including generative models that output adjacency matrices and node labels of graphs at once, as well as models that decode molecules sequentially node by node. Hypergraph grammar-based methods have also been explored for molecular graph generation. Our work is closely related to Jin et al. (2018) who generate molecules based on substructures using a two-stage procedure. The first step creates a junction tree with substructures as nodes, capturing their relative arrangements, while the second step resolves the full graph by specifying how the substructures should be attached. Our method differs from previous work by jointly predicting substructures and their attachments using an autoregressive decoder, eliminating the need for stage-wise decoding. Our method jointly predicts substructures and their attachments using an autoregressive decoder, unlike previous methods that require stage-wise decoding. Graph neural networks have been extensively studied for graph encoding, including graph encoders for molecules. Our method represents molecules as hierarchical graphs spanning from atom-level graphs. Our method represents molecules as hierarchical graphs, different from previous methods. It utilizes graph encoders for molecules and learns to represent graphs hierarchically. Defferrard et al. (2016) and Ying et al. (2018) used graph coarsening algorithms to create multiple layers of graph hierarchy. Gao & Ji (2019) proposed learning the graph hierarchy alongside the encoding process. These methods aim to represent graphs as a single entity, with molecules represented hierarchically. Our focus is on graph generation, where a molecule is encoded into multiple sets of vectors for different resolutions. These vectors are aggregated by decoder attention modules in each graph generation step. The goal is to learn a function that maps a molecule into another molecule with improved chemical properties. The graph translation task involves learning a function that maps a molecule into another molecule with better chemical properties using an encoder-decoder with neural attention. The decoder adds a new substructure in each generation step and predicts how it should be attached to the current graph. The decoder in the graph translation task adds a new substructure and predicts its attachment to the current graph using a hierarchical graph representation. Our model represents a molecule X with a hierarchical graph H X consisting of substructure, attachment, and atom layers. Nodes in H X are encoded into substructure, attachment, and atom vectors for decoding. The decoder in the graph translation task predicts attachment of a new substructure to the current graph using this representation. The encoder generates vectors for substructures and atom vectors for the decoder's prediction steps. The decoder utilizes a sigmoid function and a multi-layer neural network for attention over vectors. Substructures are defined as subgraphs of molecules, extracted to cover the entire molecular graph. The substructures in the molecule are defined as subgraphs induced by atoms and bonds. Two types of substructures considered are rings and bonds, forming a vocabulary constructed from the training set. A substructure tree is created to show how substructures are connected in the molecule. The molecule G has a substructure tree T with nodes representing substructures S1 to Sn connected by common atoms. A graph decoder generates molecule G by expanding its substructure tree in a depth-first order. The model predicts new substructures based on the encoding of input X. The graph decoder generates a molecule G by incrementally expanding its substructure tree in a depth-first order. It predicts new substructures and how they should be attached to the graph, moving to the new substructure and repeating the process. The decoder also makes topological predictions to determine if a new substructure should be attached to the current node, backtracking if not. The graph decoder predicts new substructures and their attachment to the molecule in a depth-first order. It uses topological predictions to decide on creating new substructures and their attachment points. The model creates a new substructure S t from S k and predicts its type using an MLP. It then determines how S t should be attached to S k by predicting atom pairs. The model predicts atom pairs for attaching a new substructure S t to S k in two steps: 1) Predicting attaching atoms {v j } from a fixed graph S t, forming a vocabulary A(S t) for classification, and 2) Finding corresponding atoms {u j } in S k based on predicted attaching points {v j}. The model predicts atom pairs for attaching a new substructure S t to S k in two steps: predicting attaching atoms {v j} from a fixed graph S t and finding corresponding atoms {u j} in S k. The probability of a candidate attachment M is computed based on atom representations, giving an autoregressive factorization of the distribution over the next substructure and its attachment. Each decoding step depends on the outcome of the previous step, with predicted attachments influencing subsequent predictions. Teacher forcing is applied during training. During training, teacher forcing is applied to the generation process, with a depth-first traversal over the ground truth substructure tree determining the generation order. The attachment enumeration is manageable due to small substructure sizes, with an average attachment vocabulary size of less than 5 and fewer than 20 candidate attachments. The encoder represents a molecule X with a hierarchical graph HX to support the decoding process. The encoder represents a molecule X with a hierarchical graph HX consisting of an atom layer and an attachment layer derived from the substructure tree. The average attachment vocabulary size is less than 5, with fewer than 20 candidate attachments. The atom layer in the hierarchical graph of molecule X has nodes labeled with atom type and charge, and edges labeled with bond type. The attachment layer is derived from the substructure tree, with nodes representing attachment configurations of substructures. Each node provides information for attachment prediction. The attachment layer in the molecule's hierarchical graph contains nodes representing attachment configurations of substructures, providing essential information for attachment prediction. The substructure layer in the graph mirrors the substructure tree, offering crucial details for substructure prediction during decoding. Edges connect atoms and substructures across different layers. The hierarchical graph for molecule X includes three layers: atom, attachment, and substructure. Edges connect atoms and substructures between layers to propagate information. The graph is encoded by a hierarchical message passing network (MPN) with three MPNs for each layer. The hierarchical graph for molecule X is encoded by a hierarchical message passing network (MPN) with three MPNs for each layer: atom, attachment, and substructure. The MPN architecture from Jin et al. (2019) is used to encode the graph, with the atom layer being encoded first using embedding vectors for atoms and bonds in X. The atom layer of molecule X is encoded using a hierarchical message passing network (MPN). The network propagates message vectors between atoms for T iterations to output atom representations. The attachment layer of the MPN combines embedding vectors and atom vectors for each node and edge in the layer. The input features for each edge in the layer are embedding vectors based on relative node ordering. Message passing is done over H a X to compute substructure representations. Node features are computed by concatenating embeddings and node vectors from the previous layer. Message passing over H s X is then used to obtain substructure representations. The hierarchical encoder computes node features by concatenating embeddings and node vectors from the previous layer. Message passing is done over substructure layers to obtain representations of molecules at multiple resolutions. The hierarchical MPN architecture is used during decoding to encode the hierarchical graph at each step, generating substructure and atom vectors for prediction. During decoding, the hierarchical MPN architecture is used to encode the hierarchical graph at each step, generating substructure and atom vectors for prediction. The training set contains molecular pairs where each compound can be associated with multiple outputs, allowing for diverse output generation. To generate diverse outputs, a variational translation model is used with an additional input z indicating the mode of translation. The model is trained using variational inference and encodes X and Y into representations cX and cY to compute the posterior Q(z|X, Y). The model is trained using variational inference to sample z from the posterior Q(z|X, Y) and compute vector \u03b4X,Y to summarize structural changes. The latent code z is used to reconstruct output Y in the decoder, following a standard conditional VAE training objective. The model is trained using variational inference to sample z from the posterior Q(z|X, Y) and compute vector \u03b4X,Y to summarize structural changes. The latent code z is passed to the decoder along with the input representation c X to reconstruct output Y. The training objective follows a standard conditional VAE. The model is extended to handle conditional translation where desired criteria are fed as input to the translation process. The method is extended to handle conditional translation by incorporating translation criteria as input. During variational inference, \u00b5 X,Y and \u03c3 X,Y are computed with the additional input g X,Y. The latent code is augmented as [z, g X,Y] and passed to the decoder. Users can specify criteria in g X,Y during testing to control the outcome. The translation model is evaluated on single-property optimization tasks following an experimental design by Jin et al. (2019). The translation model allows users to specify criteria during testing to control the outcome. A novel conditional optimization task is constructed where desired criteria are fed as input to the translation process. Molecular similarity between input X and output Y must meet a certain threshold at test time to prevent arbitrary compound generation. The translation model requires molecular similarity between input X and output Y to be above a threshold to prevent arbitrary compound generation. The model is trained on four different tasks, including LogP Optimization, without conditional input. The model is trained on tasks such as LogP Optimization, where it aims to improve the solubility and synthetic accessibility of compounds by translating input X into output Y with a higher logP score. Two similarity thresholds are experimented with, and the model needs to meet certain criteria for the output Y to be considered drug-like and DRD2-active or drug-like but DRD2-inactive. The model is trained on tasks like LogP Optimization, aiming to improve compound solubility and synthetic accessibility by translating input X into output Y with a higher logP score. Two similarity thresholds are tested, with criteria for output Y to be considered drug-like and DRD2-active or drug-like but DRD2-inactive. Evaluation metrics include translation accuracy and diversity. The evaluation metrics for the model include translation accuracy and diversity. Each test molecule is translated multiple times with different latent codes, and the final translation is selected based on property improvement and similarity constraints. Translation success rate is reported for other tasks. HierG2G is compared against baselines like GCPN, MMPA, Seq2Seq, and JTNN. Translation success is determined by satisfying similarity and property constraints, with diversity measured by Tanimoto distance between translated compounds. The Tanimoto distance is used to measure similarity between translated compounds. HierG2G is compared against baselines like GCPN, MMPA, Seq2Seq, JTNN, and CG-VAE. The decoder of molecules is not fully autoregressive. A comparison is made with CG-VAE, a generative model that decodes molecules atom by atom. An atom-based translation model (AtomG2G) is developed for direct comparison. It predicts completion of decoding, creates new atoms, predicts atom types, and bond types autoregressively. The AtomG2G model predicts completion of decoding, creates new atoms, predicts atom and bond types autoregressively, achieving state-of-the-art results in translation tasks. Our model achieves state-of-the-art results on four translation tasks, outperforming JTNN in accuracy and output diversity. It runs 6.3 times faster during decoding and shows over 10% improvement on the DRD2 task compared to AtomG2G. Our hierarchical model outperforms AtomG2G on three datasets, with over 10% improvement on the DRD2 task. It runs 6.3 times faster during decoding and achieves higher translation accuracy and output diversity compared to Seq2Seq, JTNN, and AtomG2G. Our hierarchical model outperforms other models in translation accuracy and output diversity, especially on c = [1, 1]. Training on 1.6K examples gives a 4.2% success rate compared to 13.0% with other pairs, showing the effectiveness of our conditional translation setup. The hierarchical model outperforms other models in translation accuracy and output diversity, especially on c = [1, 1]. Ablation studies show that replacing the hierarchical decoder with an atom-based decoder decreases model performance by 0.8% and 10.9% on the QED and DRD2 tasks. This suggests that the DRD2 task benefits more from structure-based decoding. The model's performance decreases on the QED and DRD2 tasks when the decoder attention includes both atom and substructure vectors. The DRD2 task benefits more from structure-based decoding due to the importance of specific functional groups in biological target binding. Removing hierarchies in the encoder and decoder MPN results in a drop in translation accuracy, with further removal of the attachment layer significantly degrading performance on both datasets. When the top substructure layer is removed, translation accuracy drops slightly by 0.8% and 2.4%. Further removal of the attachment layer significantly degrades performance on both datasets as substructure information is lost. Replacing LSTM MPN with GRU MPN results in decreased translation performance, but our method still outperforms JTNN. Therefore, we use the LSTM MPN architecture for both HierG2G and AtomG2G baseline. In this paper, a hierarchical graph-to-graph translation model was developed, utilizing chemical substructures as building blocks. The model, which is fully autoregressive, generates molecular graphs and learns coherent multi-resolution representations. Experimental results demonstrate its superior performance compared to previous models. The LSTM MPN architecture is used for both HierG2G and AtomG2G baseline, despite a slight decrease in translation accuracy when replacing it with GRU MPN. Our model, a hierarchical graph-to-graph translation model, is fully autoregressive and generates molecular graphs with coherent multi-resolution representations. Experimental results show its superior performance over previous models. The message passing network MPN \u03c8 over graph H is defined as Algorithm 3 LSTM MPN with T message passing iterations. Our attention layer is a bilinear attention function with parameter \u03b8. Figure 7 illustrates the AtomG2G decoding process. AtomG2G is a bilinear attention function used in a hierarchical graph-to-graph translation model. It predicts new atoms and bond types in molecular graphs, adding them to a queue for processing. This atom-based method is comparable to HierG2G and focuses on molecular graph representations. AtomG2G is an atom-based translation method that predicts new atoms and bond types in molecular graphs. It operates sequentially on nodes in a queue for a set number of steps and adds the new atom to the queue. The training set size and substructure vocabulary for each dataset are listed in Table 3. The datasets are downloaded from the link provided in Jin et al. (2019). Multi-property optimization is done by combining QED and DRD2 training sets. The test set includes 780 compounds that are not drug-like and DRD2-inactive. Hyperparameters for HierG2G and AtomG2G models are specified. For HierG2G and AtomG2G models, the hidden layer dimension is set to 270 and 400 respectively, with an embedding layer dimension of 200 for HierG2G and 400 for AtomG2G. The latent code dimension is |z| = 8 with a KL regularization weight of \u03bb KL = 0.3. Both models undergo 20 iterations of message passing in each layer of the encoder. The models are trained using the Adam optimizer with default parameters. CG-VAE is used for molecule generation and property prediction from the latent space. For CG-VAE models, training is done with Adam optimizer using default parameters. Three CG-VAE models are trained for logP, QED, and DRD2 tasks. At test time, compounds are translated into latent representations and gradient ascent is performed to maximize property scores. Multiple vectors are decoded to select the best molecule. In the study, the authors utilized gradient ascent to maximize property scores of molecules translated into latent representations. Keeping the KL regularization weight low was crucial for meaningful results, as a higher weight led to dissimilar molecules. Ablation studies were conducted, including changing the decoder to AtomG2G's atom-based decoder. In the study, gradient ascent was used to maximize property scores of molecules in latent representations. Ablation studies were conducted, including modifying the decoder to AtomG2G's atom-based decoder and reducing the number of hierarchies in the encoder and decoder MPN. In the experiments, the number of hierarchies in the encoder and decoder MPN is reduced. Two-layer model uses c X = c G X \u222a c A X for molecule representation, while one-layer model uses c X = c G X. Topological and substructure predictions are made based on hidden vectors h A k or atom vectors v\u2208S k h v. Hidden layer dimension is adjusted to match the original model size."
}