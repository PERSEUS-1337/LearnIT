{
    "title": "HyldojC9t7",
    "content": "The D2KE methodology constructs positive definite kernels from dissimilarity measures on structured inputs like time series, strings, histograms, and graphs. It draws from Random Features literature but builds a kernel from a random feature map specified by the distance measure. A finite number of random objects are used to create a random feature embedding for each instance. The D2KE methodology proposes using a finite number of random objects to create a random feature embedding for each instance. It offers better generalizability than universal Nearest-Neighbor estimates and generalizes existing Random Features methods to handle complex structured inputs of variable sizes. Our proposed framework for classification experiments compares favorably to existing distance-based learning methods in terms of testing accuracy and computational time across various domains such as time series, strings, and histograms for texts and images. It is easier to specify dissimilarity functions for structured inputs like real-valued time series, strings, histograms, and graphs than to construct feature representations. For structured inputs like time series, strings, histograms, and graphs, constructing representations can be challenging due to varying sizes. Dissimilarity measures like Dynamic Time Warping for time series and Edit Distance for strings exist, but machine learning methods are designed for vector representations. Distance-based methods, such as Nearest-Neighbor Estimation (NNE), use dissimilarity measures like Dynamic Time Warping for time series and Edit Distance for strings. However, standard machine learning methods are primarily designed for vector representations, leading to less focus on distance-based methods for structured inputs. Nearest-Neighbor Estimation predicts outcomes by averaging nearest neighbors in the input space, but it is unreliable with high variance. Nearest-Neighbor Estimation (NNE) predicts outcomes using an average of nearest neighbors in the input space, but it is unreliable due to high variance when neighbors are far apart. Research has focused on developing global distance-based machine learning methods to address this issue. Research has focused on developing global distance-based machine learning methods, drawing upon connections to kernel methods or directly learning with similarity functions. The most direct approach involves treating the data similarity matrix as a kernel Gram matrix and using standard kernel-based methods like Support Vector Machines or kernel ridge regression. However, a key caveat is that most similarity measures do not provide a positive-definite kernel. Research has focused on developing global distance-based machine learning methods, utilizing kernel-based methods like Support Vector Machines or kernel ridge regression. However, most similarity measures do not yield a positive-definite kernel, leading to challenges in defining the empirical risk minimization problem. Efforts have been made to estimate a positive-definite Gram matrix that approximates the similarity matrix through methods such as clipping, flipping, or shifting eigenvalues, or explicitly learning a positive-definite approximation. Estimating a positive-definite Gram matrix that approximates the similarity matrix can be achieved by clipping, flipping, or shifting eigenvalues, or explicitly learning a PD approximation. However, these modifications often result in a loss of information and the PD property is typically guaranteed only on the training data, leading to inconsistency between testing and training samples. Another approach is to select a subset of training samples as a representative set for feature extraction. In this paper, a novel general framework is proposed to construct a family of positive-definite kernels. This framework addresses issues related to inconsistency between testing and training samples by providing a more general and richer family of kernels that outperform representative-set methods in various application domains. The proposed framework, D2KE, constructs a family of PD kernels from a dissimilarity measure on structured inputs. It draws from Random Features literature to build novel kernels specifically designed for a given distance measure, outperforming representative-set methods in various application domains. The D2KE framework constructs novel kernels from a random feature map designed for a given distance measure. This kernel ensures Lipschitz-continuity in the corresponding RKHS and provides a tractable estimator with better generalization properties than nearest-neighbor estimation. The framework generates a feature embedding for each instance, suitable for classification and regression models. Our framework in distance kernel learning produces a feature embedding for each instance, improving generalization properties compared to nearest-neighbor estimation. It outperforms existing distance-based learning methods in classification experiments across various domains, offering better testing accuracy and computational efficiency, especially with large datasets or structured inputs. Our main contributions include proposing a methodology for constructing PD kernels via Random Features for structured inputs and generalizing Random Features methods to complex structured inputs of variable sizes. From the perspective of Random Features (RF) methods, a generic RF method has been used to accelerate kernel machines on structured inputs such as time-series, strings, and histograms. Existing approaches for Distance-Based Kernel Learning either impose strict conditions on the distance function or construct empirical PD Gram matrices. Distance-Based Kernel Learning methods impose strict conditions on the distance function or construct empirical PD Gram matrices that may not generalize well to test samples. Conditions for obtaining a PD kernel through simple transformations of the distance measure are not satisfied for commonly used dissimilarity measures like Dynamic Time Warping, Hausdorff distance, and Earth Mover's distance. The curr_chunk discusses the limitations of commonly used dissimilarity measures such as Dynamic Time Warping, Hausdorff distance, and Earth Mover's distance in satisfying conditions for distance-based kernel learning methods. It also mentions different approaches like Euclidean embedding and theoretical foundations for SVM solvers in Krein spaces. The curr_chunk discusses specific approaches for building a positive definite kernel on structured inputs like text and time-series, which can result in a diagonal-dominance problem in the kernel Gram matrix. Randomized feature maps have been used to approximate non-linear kernel machines, reducing training and testing times for kernel-based learning algorithms. Various explicit nonlinear random feature maps have been developed for different types of kernels, such as Gaussian and Laplacian Kernels. The use of randomized feature maps has increased due to reduced training and testing times for kernel-based learning algorithms. Various explicit nonlinear random feature maps have been created for different types of kernels, including Gaussian, Laplacian, intersection, additive, dot product, and semigroup kernels. The Random Fourier Features (RFF) method, which approximates a Gaussian Kernel function by multiplying the input with a Gaussian random matrix, has been extensively studied both theoretically and empirically. D2KE is a method that differs from traditional Random Fourier Features (RFF) by considering structured inputs, allowing for faster matrix computation and less memory consumption. D2KE differs from traditional Random Fourier Features (RFF) by considering structured inputs of different sizes and using a structured distance metric for computing the Random Features (RF). It constructs a new PD kernel through a random feature map, making it computationally feasible via RF. D2KE constructs a new PD kernel through a random feature map, making it computationally feasible via RF. It differs from existing RF methods by considering structured inputs of different sizes and using a structured distance metric for computing RF. BID49 has developed a kernel and algorithm for single-variable time-series but cannot be applied to discrete structured inputs like strings, histograms, and graphs. We present a unified framework for various structured inputs beyond the limitations of BID49, offering a general theoretical analysis regarding KNN and other distance-based kernel methods for estimating a target function from a collection of samples. The unified framework presented offers a theoretical analysis of KNN and distance-based kernel methods for estimating a target function from structured input samples, using a dissimilarity measure instead of feature representation. The size of the structured inputs may vary widely, such as strings or graphs with different sizes. The equivalence between PD of similarity matrix and Euclidean of dissimilarity matrix is proven in BID4. The dissimilarity measure needs to be a metric for some analyses. An ideal feature representation for learning should be compact and result in a simple target function. Ideal dissimilarity measures for learning should satisfy certain properties. The target function f(x) is a simple linear function of the resulting representation. An ideal dissimilarity measure for learning should have small expected distances among samples and a small Lipschitz-continuity constant with respect to the dissimilarity measure. The target function should have a small Lipschitz-continuity constant with respect to the dissimilarity measure. The covering number N(\u03b4; X, d) measures the size of the space implied by the dissimilarity measure. The covering number N(\u03b4; X, d) measures the size of the space implied by a given dissimilarity measure. We extend the analysis of the estimation error of k-nearest-neighbor to any structured input space X with a distance measure d and a finite covering number N(\u03b4; X, d) by defining the effective dimension. The effective dimension is defined as the minimum value that satisfies a certain condition in the context of finite-dimensional vector spaces and structured input spaces. An example is provided for measuring the effective dimension in the case of Multisets, which are sets allowing duplicate elements. The Hausdorff Distance and covering number under a ground distance are also discussed in relation to the effective dimension. The (modified) Hausdorff Distance measures the distance between elements in a set. The covering number of V under a ground distance is denoted by N(\u03b4; V, \u2206). By constructing a covering of sets of size bounded by L, we can obtain a bound on the estimation error of the k-Nearest-Neighbor estimate of f(x). Equipped with the concept of effective dimension, a bound on the estimation error of the k-Nearest-Neighbor estimate of f(x) can be obtained. Theorem 1 states that for \u03c3 > 0, minimizing the right-hand side with respect to the parameter k yields a certain constant c > 0. The proof is similar to a standard analysis of k-NN's estimation error, with the covering number replacing the space partition number and the effective dimension replacing the dimension in Assumption 3. The proof is similar to a standard analysis of k-NN's estimation error, with the covering number replacing the space partition number and the effective dimension replacing the dimension in Assumption 3. The estimation error of k-NN decreases slowly with n when p X,d is large, requiring the number of samples to scale exponentially in p X,d for the error to be bounded. An estimator based on a RKHS derived from the distance measure is developed with better sample complexity for higher effective dimension problems. The estimatorf is developed based on a RKHS derived from the distance measure, offering improved sample complexity for higher effective dimension problems. A simple yet effective approach D2KE is introduced to create positive-definite kernels from a given distance measure, constructing a family of kernels for a structured input domain X. The kernel is constructed from a structured input domain X and a distance measure d(., .), using a family of kernels with random structured objects \u03c9 \u2208 \u2126. The kernel is parameterized by p(\u03c9) and \u03b3, and can be interpreted as a soft version of the distance substitution kernel. The kernel in Equation (4) is interpreted as a soft version of the distance substitution kernel, where it substitutes a soft version of the distance. When \u03b3 \u2192 \u221e, the value is determined by the minimum distance between x and y. When \u03b3 \u2192 \u221e, Equation FORMULA15 is determined by min distance between x and y. Kernel in Equation FORMULA13 is always PD. Draw R samples from p(\u03c9) to get {\u03c9 j } R j=1. Solve problem for some \u00b5 > 0: Random Feature Approximation. Kernel in Equation FORMULA12 can't be evaluated analytically but can be used in practice. Random Feature Approximation allows for the use of kernel methods in large-scale settings with a large number of samples, where standard kernel methods are no longer efficient enough. This approximation method is particularly natural when the kernel itself is defined via a random feature map. Our kernel with the RF approximation can be used in large-scale settings with a large number of samples, where standard kernel methods are not efficient enough. The RF approximation allows for learning a target function as a linear function of the RF feature map by minimizing domain-specific empirical risk. Another approach involves selecting a set of random features through optimization in a supervised setting, which is orthogonal to our D2KE method. The RF approximation in our kernel allows for learning a target function by minimizing empirical risk. A recent work that selects random features through optimization in a supervised setting is different from our D2KE approach. The RF feature embeddings are computed using a structured distance measure and an exponent function parameterized by \u03b3, contrasting traditional RF methods. The structured distance measure and exponent function parameterized by \u03b3 are used to compute RF feature embeddings, contrasting traditional RF methods. A detailed analysis of the estimator is provided in Algorithm 1 in Section 5, comparing its statistical performance to K-nearest-neighbor. The approach is related to the representative-set method when a naive choice of p(\u03c9) is made. The structured distance measure and exponent function parameterized by \u03b3 are used to compute RF feature embeddings, contrasting traditional RF methods. In Algorithm 1 in Section 5, the statistical performance is compared to K-nearest-neighbor. A naive choice of p(\u03c9) relates the approach to the representative-set method, giving a kernel Equation (4) dependent on data distribution. A Random-Feature approximation to the kernel can be obtained by creating an R-dimensional feature embedding using a part of the training data as samples from p(\u03c9). This is equivalent to a 1/ \u221a R-scaled version of the embedding function in the representative-set method. By interpreting Equation (8) as a random-feature approximation to the kernel in Equation (4), a much nicer generalization error bound is obtained even in the case R \u2192 \u221e. This contrasts with the representative-set method where the size of the representative set needs to be kept small. Equation (8) as a random-feature approximation to the kernel in Equation (4) yields a better generalization error bound even with large R values. The choice of p(\u03c9) is crucial in the kernel, with \"close to uniform\" selections showing improved performance across various domains. In various domains, different choices of p(\u03c9) outperform the data distribution p(x). Examples include time-series with DTW, string classification with edit distance, and vector classification. In various domains, different choices of p(\u03c9) outperform the data distribution p(x). Examples include time-series with DTW, string classification with edit distance, and vector classification. When classifying sets of vectors with the Hausdorff distance, a distribution p(\u03c9) corresponding to random sets of size uniform in [3, 15] with elements drawn uniformly from a unit sphere yields significantly better performance than the Representative-Set Method (RSM). The chosen distribution p(\u03c9) in some cases outperforms RSM due to the ability to generate unlimited random features, leading to a better approximation of the exact kernel. Additionally, even with a small number of random features, the performance of the selected distribution can still yield significantly better results. In some cases, the selected distribution p(\u03c9) outperforms RSM by generating objects capturing more relevant semantic information for estimating f(x) under the dissimilarity measure d(x, \u03c9). The proposed framework is analyzed from the error decomposition perspectives in this section. The proposed framework is analyzed from the error decomposition perspectives in this section. The dissimilarity measure d(x, \u03c9) is used to analyze the framework. The RKHS corresponding to the kernel in Equation (4) is denoted as H. The population risk minimizer subject to the RKHS norm constraint is DISPLAYFORM0, and the corresponding empirical risk minimizer is DISPLAYFORM1. The estimated function from the random feature approximation is denoted as f R. The population and empirical risks are denoted as L( f ) and L( f ) respectively. The risk decomposition is discussed, focusing on the function approximation error. The RKHS implied by the kernel in Equation FORMULA12 is a smaller function space than the space of Lipschitz-continuous function w.r.t. the distance d(x 1 , x 2). Any function f in the RKHS is Lipschitz-continuous w.r.t. the distance d(., .). Additional smoothness can be imposed via the function f. The RKHS, defined by the kernel in Equation FORMULA12, contains Lipschitz-continuous functions with additional smoothness constraints. The goal is to find the best function within this class that approximates the true function well. The RKHS assumption leads to a qualitatively better estimation error, as discussed with D \u03bb and tuning parameter \u03bb. The RKHS assumption results in a better estimation error, with D \u03bb and tuning parameter \u03bb playing a key role. The estimation error is bounded by c(log 1 \u03b4 ) 2 C 2 \u03bb for \u03bb \u2265 D \u03bb /n, with a preference for smaller \u03bb values. The dependency on n is significantly improved compared to standard estimators. The RKHS estimator aims to minimize \u03bb as a function of n, with a bound on the estimation error that improves significantly with n. Analyzing D \u03bb for the kernel in Equation FORMULA12 is challenging due to the lack of an analytic form. Random Feature Approximation is used for empirical risk estimation. The analysis of D \u03bb for the kernel in Equation FORMULA12 is challenging due to the lack of an analytic form. Random Feature Approximation is used for empirical risk estimation, focusing on the approximation error of the kernel. The analysis focuses on the approximation error of the kernel in empirical risk estimation, with a bound on the empirical risk minimization using the Representer theorem. Proposition 2 provides an approximation error in terms of kernel evaluation for empirical risk minimization. The Representer theorem is used to consider the optimal solution, leading to Corollary 1 which states conditions for guaranteeing a certain bound on empirical risk. The constants M and A play a key role in this analysis. The constants M and A play a key role in the analysis, where M is the Lipschitz-continuous constant of the loss function and A is a bound on \u03b1 1 /n. Corollary 1 states that having a number of Random Features proportional to the effective dimension O(p X,d / 2) can achieve an approximation error. The proposed framework can achieve -suboptimal performance according to the combined error terms. Claim 1 discusses the estimated function from the random feature approximation based ERM estimator. The proposed framework can achieve -suboptimal performance. Claim 1 discusses the estimated function from the random feature approximation based ERM estimator, showing that the target function lies close to the population risk minimizer in the RKHS spanned by the D2KE kernel. The method is evaluated in various domains such as time-series, strings, texts, and images, with a focus on dissimilarity measures and data characteristics for each experiment. The proposed method is evaluated in different domains like time-series, strings, texts, and images. Various dissimilarity measures are discussed for each experiment, including Dynamic Time Warping, Edit Distance, and Earth Mover's distance for measuring semantic distance between Bags of Words. The proposed method evaluates dissimilarity measures in different domains such as time-series, strings, texts, and images. Measures include Dynamic Time Warping, Edit Distance, Earth Mover's distance for Bags of Words, and (Modified) Hausdorff distance for Bags of Visual Words. Computational complexity is addressed through adapted C-MEX programs and Matlab code. For time-series data, multivariate time-series datasets with varying lengths were selected, including samples from the UCI Machine Learning repository and a wireless communication system. String data consisted of strings with different alphabet sizes and lengths. Distance measures were implemented using C-MEX programs and Matlab code to address computational complexity. The curr_chunk discusses datasets from various sources including the UCI Machine Learning repository, GMU, LibSVM Data Collection, and Kaggle. It mentions data types such as string, text, and image with specific characteristics like alphabet size, string length, document length, and SIFT feature vector size. The datasets used in the study vary in length and feature vector size. They were sourced from Kaggle and split into train and test subsets. The study compares D2KE against 5 state-of-the-art baselines, including KNN and DSK_RBF. In comparing D2KE against 5 state-of-the-art baselines, including KNN, DSK_RBF, DSK_ND, KSVM, and RSM, various methods are utilized such as distance substitution kernels and learning from similarity matrices. D2KE is compared against 5 state-of-the-art baselines, including KNN, DSK_RBF, DSK_ND, KSVM, and RSM. The baselines have different computational complexities, with D2KE having linear complexity in both the number of data samples and the length of the sequence. Our new method D2KE has complexity linear in both the number of data samples and the length of the sequence. We perform 10-fold cross validation to find the best parameters and use random samples to achieve performance close to an exact kernel. The range for the best number is typically between 4 and 4096. Linear SVM is employed for embedding-based methods, and LIBSVM BID5 is used for precomputed data. D2KE outperforms baseline methods in classification accuracy with less computation time, showing it as a strong alternative to KNN. D2KE consistently outperforms baseline methods in classification accuracy with less computation time, making it a strong alternative to KNN. Our method achieves better performance than DSK_RBF, DSK_ND, and KSVM, indicating that a representation induced from a truly PD kernel utilizes data more effectively. RSM is the closest method to ours in terms of practical construction of the feature matrix. In this work, a general framework is proposed for deriving a positive-definite kernel and a feature embedding function from a given dissimilarity measure between input objects. The method outperforms RSM in practical construction of the feature matrix, with random objects sampled by D2KE performing significantly better. More detailed experimental results for each domain are discussed in Appendix C. The framework proposed in this work derives a positive-definite kernel and feature embedding function from a dissimilarity measure for structured input domains. It subsumes existing approaches and opens a new direction for creating embeddings based on distance to random objects. An extension could involve developing distance-based embeddings within this framework. The text chunk discusses the development of distance-based embeddings for structured objects within a deep architecture, aiming to support structured inputs in an end-to-end learning system. The proof involves bounding the magnitude of Hoefding's inequality for a given input pair. The text chunk provides a proof for bounding the magnitude of Hoefding's inequality for a given input pair using a Lipschitz constant \u03b3. It involves finding a covering E of X w.r.t. d(., .) and applying a union bound over the covering. By choosing \u03b3 \u2264 1, the result is obtained. The text chunk presents a proof for bounding Hoefding's inequality using a Lipschitz constant \u03b3 \u2264 1. The process involves finding a covering E of X w.r.t. d(., .) and applying a union bound over the covering. By choosing \u03b3 \u2264 1, the result is obtained through a series of equations and theorems. The general setup involves searching for the best parameters on the training set using 10-fold cross validation. The text chunk discusses parameter selection methods for different algorithms, including DSK_RBF, DSK_ND, KSVM, and RSM. The methods involve 10-fold cross validation and random sample selection to achieve optimal performance. The new method D2KE generates random samples to approximate an exact kernel. The text chunk discusses parameter selection methods for algorithms like DSK_RBF, DSK_ND, KSVM, and RSM. It involves 10-fold cross validation and random sample selection to optimize performance. The new method D2KE generates random samples to approximate an exact kernel, with the best number of samples typically in the range of 4 to 4096. Linear SVM is used for embedding-based methods, while LIBSVM is used for precomputed dissimilarity kernels. Datasets are sourced from popular public websites for Machine Learning and Data Science research. The datasets used in the study were sourced from popular public websites for Machine Learning and Data Science research. Computation was done on a DELL system with Intel Xeon processors and SUSE Linux operating system to accelerate the methods used (RSM, D2KE) with LIBSVM BID5 for precomputed dissimilarity kernels. The study utilized datasets from various domains and conducted computations on a DELL system with Intel Xeon processors and SUSE Linux OS. Multithreading with 12 threads was used for distance computations. Gaussian distribution with parameters \u03c3 and random time series length were optimized. The study optimized Gaussian distribution with parameters \u03c3 and random time series length for distance computations. D2KE consistently outperformed other baselines in classification accuracy, with 26.62% higher performance than KNN on IQ_radio. Our method, D2KE, outperforms KNN by 26.62% on IQ_radio due to its insensitivity to data noise. Compared to DSK_RBF, DSK_ND, and KSVM, our method achieves much better performance, indicating that a representation induced from a truly p.d. kernel utilizes data more effectively than indefinite kernels. Our method, D2KE, outperforms KNN by 26.62% on IQ_radio due to its insensitivity to data noise. Compared to DSK_RBF, DSK_ND, and KSVM, our method achieves better performance, suggesting that a representation induced from a truly p.d. kernel utilizes data more effectively than indefinite kernels. RSM is closest to our method in practical construction of the feature matrix, but D2KE performs significantly better by sampling random time series. Our method, D2KE, outperforms KNN by 26.62% on IQ_radio due to its insensitivity to data noise. Compared to DSK_RBF, DSK_ND, and KSVM, our method achieves better performance, suggesting that a representation induced from a truly p.d. kernel utilizes data more effectively than indefinite kernels. RSM may suffer from noise and redundant information in time-series, while our method samples random sequences to denoise and find patterns in the data efficiently. The number of possible random sequences drawn from the distribution is unlimited, making the feature space more abundant and computationally efficient for long time-series. Levenshtein distance is used as the distance measure for string data in the setup. Parameters for \u03b3 and the length of random strings are optimized. The feature space is more abundant with unlimited random sequences, reducing computational cost for long time-series. The study optimized parameters for \u03b3 and the length of random strings to generate random strings using the alphabet computed from the original data. Results show that D2KE outperforms other distance-based baselines, including DSK_RBF, due to the use of Levenshtein distance as a metric. D2KE outperforms other baselines, including DSK_RBF, on large datasets with better performance and less computation. D2KE achieves superior performance with less computation compared to other baselines like DSK_RBF, DSK_ND, and KSVM. It uses the earth mover's distance for text data, showing strong performance when combined with KNN for document classifications. D2KE uses earth mover's distance for text data, showing strong performance when combined with KNN for document classifications. Bag of Words is computed for each document, represented as a histogram of word vectors. Random documents are generated with word vectors sampled from the unit sphere. Parameters for \u03b3 and length of random document are optimized. D2KE outperforms other baselines on all four. Results show that D2KE outperforms other baselines on all datasets, with distance-based kernel methods performing better than KNN. The effectiveness of SVM over KNN on text data is illustrated, and D2KE excels due to its random documents of short length fitting the task of document classification particularly well. D2KE outperforms other baselines on all datasets, achieving a significant speedup with random features. For image data, the modified Hausdorff distance is used as the distance measure between images. The modified Hausdorff distance (MHD) is used as the distance measure between images in D2KE, with SIFT-descriptors of dimension 128. Random images are generated from each SIFT-descriptor, and parameters for \u03b3 and sequence length are optimized. D2KE performance surpasses other baselines on all datasets. The D2KE algorithm outperforms other baselines on all datasets by optimizing parameters for \u03b3 and sequence length of random SIFT-descriptors. Despite the effectiveness of D2KE, the quadratic complexity of other methods makes scaling difficult. SIFT features are not effective for finding patterns quickly in images. The quadratic complexity of DSK_RBF, DSK_ND, and KSVM makes scaling to large data challenging. D2KE outperforms KNN and RSM, showing it can be a strong alternative across applications."
}