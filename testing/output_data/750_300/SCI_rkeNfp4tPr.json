{
    "title": "rkeNfp4tPr",
    "content": "Stochastic gradient descent with stochastic momentum is commonly used in nonconvex stochastic optimization, especially for training deep neural networks. The addition of a momentum term biases the parameter update in the direction of the previous change, aiming to reduce convergence time. However, proving the effectiveness of momentum adjustment in stochastic and non-convex settings has been challenging. In non-convex settings, momentum adjustment is known to reduce convergence time, but its effectiveness has been elusive. Stochastic momentum is widely used in training deep networks and has been observed to improve convergence time. Theoretical justification for its use has been a significant open question. This paper proposes that stochastic momentum helps escape saddle points faster in SGD, leading to quicker convergence. Stochastic momentum improves deep network training by helping SGD escape saddle points faster and find a second order stationary point more quickly. The ideal momentum parameter should be large (close to 1), as supported by theoretical and empirical findings. Experimental results further validate the effectiveness of SGD with stochastic momentum in nonconvex optimization and deep learning. SGD with stochastic momentum, with a large $\\beta \\in [0,1)$ close to 1, is widely used in nonconvex optimization and deep learning. It is a popular algorithm for training machine learning models in various applications such as computer vision, speech recognition, natural language processing, and reinforcement learning. SGD with stochastic momentum is widely used in nonconvex optimization and deep learning for training machine learning models in various applications such as computer vision, speech recognition, natural language processing, and reinforcement learning. The advantage of SGD with stochastic momentum has been widely observed in achieving faster convergence compared to standard SGD. Momentum is considered a necessary tool for designing new optimization algorithms in optimization and deep learning. Training deep neural nets with stochastic momentum in SGD leads to faster convergence compared to standard SGD. Momentum is essential for designing new optimization algorithms in deep learning. Popular adaptive stochastic gradient methods like Adam and AMSGrad incorporate momentum. Despite its widespread use, the empirical improvements and guidelines for setting the momentum parameter remain unclear. In this paper, a theoretical analysis is provided for stochastic gradient descent (SGD) with momentum. The momentum parameter remains a key aspect in optimization algorithms for deep learning, with large values like \u03b2 = 0.9 often working well in practice. The default momentum method in popular software packages like PyTorch and Tensorflow is Algorithm 1, which requires setting the step size parameter \u03b7 and momentum parameter \u03b2. In this paper, a theoretical analysis is provided for stochastic gradient descent (SGD) with momentum. The analysis identifies conditions that guarantee SGD with stochastic momentum escapes saddle points faster than standard SGD. Stochastic heavy ball momentum maintains a weighted average of stochastic gradients at visited points for updates. In this paper, the analysis focuses on finding second-order stationary points for smooth non-convex optimization using stochastic heavy ball momentum in SGD. The method maintains a weighted average of stochastic gradients to amplify escape directions from saddle points. The paper focuses on finding second-order stationary points for smooth non-convex optimization using stochastic heavy ball momentum in SGD. It aims to obtain a second-order guarantee in the nonconvex optimization community, as finding a global or local minimum in general nonconvex optimization can be NP hard. The paper aims to achieve a second-order guarantee in nonconvex optimization by utilizing momentum in reaching a ( , )-second-order stationary point, following related works targeting approximate second-order stationary points with Lipschitzness assumptions in gradients and Hessian. The paper introduces a condition ensuring updates align with negative curvature directions for reaching a ( , )-second-order stationary point in nonconvex optimization. The stochastic momentum in SGD with heavy ball momentum amplifies the escape signal \u03b3, aiding in faster escape from saddle points under the Correlated Negative Curvature (CNC) assumption. If SGD with momentum satisfies Almost Positively Aligned with Gradient (APAG) properties and an upper-bound parameter \u03b2, it helps in reaching ( , )-second-order stationary points in nonconvex optimization. The stochastic momentum in SGD with heavy ball momentum amplifies the escape signal \u03b3, aiding in faster escape from saddle points under the Correlated Negative Curvature (CNC) assumption. If SGD with momentum satisfies Almost Positively Aligned with Gradient (APAG) properties and an upper-bound parameter \u03b2, it helps in reaching second-order stationary points in nonconvex optimization with a larger momentum parameter \u03b2 assisting in escaping saddle points. Theoretical results show that a larger momentum parameter \u03b2 in SGD with momentum helps in escaping saddle points faster, enabling training to be faster in optimization and deep learning. This is particularly useful as saddle points are common in the loss landscape of optimization and deep learning. Theoretical results demonstrate that a larger momentum parameter in SGD with momentum accelerates the escape from saddle points, facilitating faster training in optimization and deep learning. This is crucial as saddle points are prevalent in the loss landscape of optimization and deep learning. In an iterative update scheme, parameters can enter a saddle point region where gradient updates may drift slowly. Moving along the direction of the smallest eigenvector guarantees a fast escape under certain constraints on the step size. Moving along the direction of the smallest eigenvector guarantees a fast escape under certain constraints on the step size, as studied by Daneshmand et al. (2018) for non-momentum SGD. In the study of stochastic momentum, the CNC property requires that the update direction is strongly non-orthogonal to the direction of large negative curvature. This allows updates to escape saddle point regions, similar to the findings in non-momentum SGD by Daneshmand et al. (2018). The effect is further amplified in successive iterations. The study of stochastic momentum shows that updates need to be non-orthogonal to the direction of large negative curvature for effective escape from saddle points. Momentum accelerates this escape process in successive iterations, as updates align with the negative curvature direction. Momentum accelerates the escape process from saddle points by aligning updates with the negative curvature direction. It can speed up escape by a factor of 1 \u2212 \u03b2, but \u03b2 is constrained and cannot be chosen arbitrarily close to 1. The text chunk discusses how momentum can accelerate the escape process from saddle points by a factor of 1 \u2212 \u03b2, but \u03b2 is constrained and cannot be chosen arbitrarily close to 1. Empirical evidence is provided to show the benefits of stochastic momentum in solving optimization challenges with significant saddle points. The text chunk discusses a non-convex optimization challenge with a saddle point, stochastic gaussian perturbations, and convergence in function value. Initialization is always set at w0 = 0 with a step size of \u03b7 = 5 \u00d7 10 \u22125. In the experiment, all algorithms are initialized at the origin and use the same step size \u03b7 = 5 \u00d7 10 \u22124. Convergence in relative distance to the true model is plotted to capture progress as the global sign of the objective is unrecoverable. In the experiment, all algorithms are initialized at the origin with the same step size \u03b7 = 5 \u00d7 10 \u22124. The second objective involves phase retrieval, aiming to find an unknown w * \u2208 R d with limited samples y i = (a i w * ) 2. The empirical findings show stark results for both objectives. In phase retrieval, finding an unknown w* in R^d with limited samples y_i=(a_i w*)^2 is accelerated by larger choices of \u03b2. Optimization trajectories with large momentum escape saddle points quicker than those with smaller momentum. This is the first reported empirical finding of its kind. The heavy ball method, proposed by Polyak in 1964, shows no convergence speedup over standard gradient descent in most cases. However, in convex quadratic objectives, an \"accelerated\" rate is possible. Large-momentum trajectories escape saddle points faster than smaller momentum ones, as observed in empirical findings for phase retrieval optimization. The heavy ball method, proposed by Polyak in 1964, does not provide a convergence speedup over standard gradient descent, except in cases like convex quadratic objectives where an \"accelerated\" rate is possible. Specialized algorithms aim at reaching a second order stationary point by exploiting negative curvature explicitly. Our work belongs to the category of specialized algorithms designed to exploit negative curvature explicitly and escape saddle points faster. Pioneer works in this category include Ge et al. (2015) and Jin et al. (2017), who showed that adding isotropic noise in each iteration helps gradient descent escape saddle points and find a second order stationary point. Jin et al. (2017) demonstrated that adding isotropic noise in each iteration ensures gradient descent avoids saddle points and converges to a second order stationary point. Phase retrieval is known to be nonconvex with strict saddle properties, where local minimizers are global up to phase and saddles have negative curvature. Daneshmand et al. (2018) assumed that stochastic gradient has a component to escape, following the Correlated Negative Curvature (CNC) assumption for stochastic gradient. Our work assumes Correlated Negative Curvature (CNC) for stochastic momentum instead of gradient, comparing results with related works in Appendix A. We assume L-Lipschitz gradient and \u03c1-Lipschitz Hessian. The study assumes CNC for stochastic momentum instead of gradient, comparing results with related works in Appendix A. It assumes L-Lipschitz gradient and \u03c1-Lipschitz Hessian, ensuring properties for stochastic momentum dynamics. The norm of stochastic momentum is bounded by mt \u2264 cm. \u03a0iMi is the matrix product of matrices {Mi} and we use to denote the spectral norm of the matrix M. Our analysis relies on three properties of the stochastic momentum dynamic, which we argue should hold in natural settings and aim to demonstrate empirically. SGD with stochastic momentum satisfies Almost Positively Aligned with Gradient (APAG) 4 and Almost Positively Correlated with Gradient (APCG) with parameter \u03c4 if certain conditions are met. SGD with stochastic momentum satisfies Almost Positively Aligned with Gradient (APAG) 4 and Almost Positively Correlated with Gradient (APCG) with parameter \u03c4. The momentum term must not be significantly misaligned with the gradient for Gradient Alignment or Curvature Exploitation (GrACE) to occur. SGD with momentum exhibits Gradient Alignment or Curvature Exploitation (GrACE) if the momentum term is not significantly misaligned with the gradient. This condition ensures progress in the algorithm when the gradient is large. The property of Almost Positively Correlated Gradient (APCG) is related to Gradient Alignment or Curvature Exploitation (GrACE) in SGD with momentum. It requires the momentum term to be positively correlated with the gradient in the Mahalanobis norm induced by a PSD matrix. This property is crucial for algorithm progress when the gradient is large. The PSD matrix M measures the local curvature of the function with respect to the trajectory of SGD with momentum. Empirical evidence supports this property on natural problems. APCG is necessary in saddle regions with significant updates. Gradient values are reported when large, and mostly nonnegative. Gs,t values are plotted in saddle point regions. The experiments show that SGD with momentum exhibits APAG and APCG properties. The expected values for the phase retrieval problem are mostly nonnegative. The analysis does not require APCG to hold in cases of large gradients or updates. In the experiments, an interesting observation is that for the phase retrieval problem, expected values may be nonnegative. The analysis does not require APCG to hold when the gradient is large or the update is at a second order stationary point. GrACE measures alignment between stochastic momentum and gradient, as well as curvature exploitation. The first term measures alignment between stochastic momentum and gradient, while the second term measures curvature exploitation. A small sum of the two terms allows bounding the function value of the next iterate. The text discusses bounding the function value of the next iterate using the sum of two terms related to APAG and APCG. The analysis follows a similar template to previous works, structured into three cases based on gradient and Hessian conditions. Our proof is structured into three cases based on gradient and Hessian conditions. The algorithm analyzed is Algorithm 2, with a boosted step size. Step size parameters, momentum parameter, and period parameter are initialized. The algorithm analyzed in our proof, Algorithm 2, with boosted step size parameters, shows progress in different cases. The iteration complexity remains constant even with larger numbers on the right-hand side. Ultimately, a second-order stationary point is reached with high probability. The algorithm analyzed in our proof, Algorithm 2, shows progress in different cases, ultimately reaching a second-order stationary point with high probability. The proof borrows tools from previous works but introduces novel momentum analysis. The stochastic momentum in SGD with momentum (Algorithm 2) satisfies CNC. Higher \u03b2 leads to reaching a second order stationary point faster by enabling faster escape from saddle points. The proof of Theorem 1 is detailed in Subsection 3.2.1. Higher \u03b2 enables faster escape from saddle points, reaching a second order stationary point faster in SGD with momentum. Constraints on \u03b2 are needed to prevent it from being too close to 1. The dependency on 1 \u2212 \u03b2 makes T thred smaller compared to previous work. Based on the constraints on \u03b2 to prevent it from being too close to 1, the dependency on 1 \u2212 \u03b2 in our results leads to a smaller T thred compared to previous work. In the high momentum regime, Algorithm 2 is shown to be better than CNC-SGD, indicating that higher momentum can help find a second order stationary point faster. Empirical findings suggest that c is approximately 0. In the high momentum regime, Algorithm 2 outperforms CNC-SGD, showing that higher momentum speeds up finding second order stationary points. Empirical results indicate c \u2248 0, making the condition easily met for a wide range of \u03b2. Analyzing the escape from saddle points by SGD with momentum, focusing on regions with small gradients and large negative eigenvalues of the Hessian. The process of escaping saddle points by SGD with momentum is analyzed, focusing on regions with small gradients and large negative eigenvalues of the Hessian. It is shown that it takes at most T thred iterations to escape the region, with the function value decreasing by at least F thred on expectation. The technique used is proving by contradiction to show that the function value must decrease by at least F thred in T thred iterations on expectation. This is demonstrated by obtaining upper and lower bounds on the expected distance, with the analysis revealing a contradiction where the lower bound is greater than the upper bound. The analysis reveals a contradiction where the lower bound is larger than the upper bound, concluding that the function value must decrease by at least F thred in T thred iterations on expectation. The dependency on \u03b2 suggests that larger \u03b2 leads to smaller T thred, indicating that larger momentum helps in escaping saddle points faster. Lemma 1 provides an upper bound of the expected distance, with the proof in Appendix C. Lemma 1 in Appendix C provides an upper bound on the expected distance, while Lemma 2 in Appendix D gives a lower bound using the recursive dynamics of SGD with momentum. Lemma 3 introduces key quantities for the analysis. Lemma 3 introduces key quantities for the analysis, including q v,t\u22121 , q m,t\u22121 , q q,t\u22121 , q w,t\u22121 , q \u03be,t\u22121. The dominant term in the lower bound is crucial for ensuring it surpasses the upper bound of the expected distance. The proof of Lemma 2 can be found in Appendix D. The lower bound in the analysis is crucial for surpassing the upper bound of the expected distance. It grows exponentially with t and the momentum parameter \u03b2, leading to a contradiction that needs to be proven by Lemma 1 and Lemma 3. The lower bound in the analysis grows exponentially with t and the momentum parameter \u03b2, leading to a contradiction that needs to be proven by Lemma 1 and Lemma 3. Lemma 5 states conditions for SGD with momentum to reach a second-order stationary point faster. SGD with momentum reaches second-order stationary points faster with higher momentum, justified by three properties identified in the paper. The greater momentum helps escape strict saddle points by enlarging the projection to an escape direction. Conditions ensuring these properties are unclear, prompting the need for further research. The heavy ball method, proposed by Polyak in 1964, does not provide a speedup over standard gradient descent in most cases. Understanding why properties hold in phase retrieval may shed light on the success of SGD with momentum in non-convex optimization and deep learning. The heavy ball method, proposed by Polyak in 1964, does not provide a speedup over standard gradient descent in most cases. Recent works have analyzed its performance for non-quadratic functions, showing convergence rates for stochastic heavy ball momentum and Nesterov's momentum in smooth non-convex objective functions. The heavy ball method, proposed by Polyak in 1964, has been analyzed for non-quadratic functions, including stochastic heavy ball momentum and Nesterov's momentum for smooth non-convex objective functions. The expected gradient norm converges at a rate of O(1/ \u221a t), but it does not outperform standard SGD. Other works have proposed variants of stochastic accelerated algorithms with first-order stationary point guarantees, but they do not capture the stochastic heavy ball momentum used in practice. Additionally, there is a negative result about the heavy ball momentum. Specialized algorithms and simple GD/SGD variants aim at reaching a second-order stationary point. Kidambi et al. (2018) found that for a specific problem, SGD with heavy ball momentum fails to achieve the best convergence rate compared to other algorithms. Specialized algorithms and simple GD/SGD variants aim to reach a second-order stationary point. Specialized algorithms exploit negative curvature explicitly to escape saddle points faster. Fang et al. (2019) propose average-SGD with a suffix averaging scheme for updates. The work introduces a new algorithm for optimization, showing its effectiveness in escaping saddle points faster than standard SGD. Fang et al. (2019) propose average-SGD with a suffix averaging scheme for updates. The iteration complexity results of different SGD variants are compared to clarify differences. The text discusses the effectiveness of the popular algorithm SGD with heavy ball momentum compared to other variants. It focuses on the advantage of using stochastic heavy ball momentum and clarifies differences with other methods. The analysis framework is based on previous work, aiming to provide insights for optimization algorithms. The text focuses on the advantage of stochastic heavy ball momentum in optimization algorithms, building on previous work. Lemma 7 shows that SGD with momentum decreases function value by a constant under certain conditions, while Lemma 8 provides an upper bound. Lemma 6, 7, and 8 discuss the benefits of stochastic heavy ball momentum in optimization algorithms. Lemma 7 states that under the APAG property, SGD with momentum decreases function value by a constant. Lemma 8 provides an upper bound on the increase of function value of the next iterate. Lemma 7 states that under the APAG property, SGD with momentum decreases function value by a constant. Lemma 8 provides an upper bound on the increase of function value of the next iterate. Considering the update step w t+1 = w t \u2212 \u03b7m t, we have that 2, where the last inequality is due to the constraint of \u03b7. Lemma 7 states that under the APAG property, SGD with momentum decreases function value by a constant. Lemma 8 provides an upper bound on the increase of function value of the next iterate. If SGD with momentum has the GrACE property, then the update step is w t+1 = w t \u2212 \u03b7m t. The text discusses the update step in SGD with momentum, where the function value decreases by a constant. It also provides an upper bound on the increase of function value for the next iterate. The update step is represented as w t+1 = w t \u2212 \u03b7m t. The text discusses bounding the increase in function value for the next iterate in SGD with momentum. It introduces a quadratic approximation at a specific time point for the update step. The text introduces a quadratic approximation at a specific time point for the update step in SGD with momentum. It defines various terms and equations to calculate the update in function value. The text introduces a quadratic approximation for the update step in SGD with momentum. It defines terms and equations to calculate the update in function value. Lemma 5 is proven using the definitions and update rule from previous Lemmas. Lemma 5 is proven using the definitions and update rule from previous Lemmas, showing that if SGD with momentum has the APCG property, certain inequalities hold. Lemma 5 proves that if SGD with momentum has the APCG property, specific inequalities must hold, such as constraints on the parameter \u03b2 to ensure certain conditions are met. These constraints limit the value of \u03b2 and are used in proofs, although they are mostly artifacts of the analysis. Lemma 5 proves constraints on parameter \u03b2 for SGD with momentum to have the APCG property. The dependence on L, \u03c3, and c are artificial and can be adjusted without loss of generality. Lemmas with parameter choices are needed for proof. To prove Lemma 5 for SGD with momentum, a series of lemmas with parameter choices from Table 3 are needed. Lemma 9 upper bounds E t0 [ q q,t\u22121 ] by applying triangle inequality and properties of matrices and vectors. The upper bound of \u2207f (w t0+k ) \u2212 \u2207f (w t0+s ) is derived using Lipschitz gradient assumptions and triangle inequality. Another upper bound is obtained for E t0 [ \u2207f (w t0+s ) \u2212 \u2207Q(w t0+s ) ] by leveraging properties of Lipschitz Hessians. Lemma 5 for SGD with momentum is proven by upper bounding E t0 [ \u2207f (w t0+s ) \u2212 \u2207Q(w t0+s ) ] using properties of Lipschitz Hessians and parameter choices from Table 3. The analysis involves applying the triangle inequality and deriving upper bounds based on Lipschitz gradient assumptions. The text discusses the upper bounding of terms in equations using parameter choices and properties of Lipschitz Hessians for SGD with momentum. The analysis involves applying the triangle inequality and deriving upper bounds based on Lipschitz gradient assumptions. The text discusses lower bounding terms in equations using parameter choices and properties of Lipschitz Hessians for SGD with momentum. The analysis involves applying the triangle inequality and deriving lower bounds based on Lipschitz gradient assumptions. The text discusses lower bounding terms in equations using parameter choices and properties of Lipschitz Hessians for SGD with momentum. The results of Lemma 9 and Lemma 10 are under review for a conference paper at ICLR 2020. Lemma 12 provides a lower bound for E t0 [2\u03b7 q v,t\u22121 , q \u03be,t\u22121], while Lemma 13 gives a lower bound for E t0 [2\u03b7 q v,t\u22121 , q m,t\u22121]. The analysis involves applying the triangle inequality and deriving lower bounds based on Lipschitz gradient assumptions. Lemma 1 and Lemma 2 provide proofs for the matrix B being symmetric positive semidefinite. The lower bound for 2\u03b7E t0 [ q v,t\u22121 , q w,t\u22121 ] is discussed in Lemma 14, assuming SGD with momentum has the APCG property. Lemma 14 discusses lower bounding 2\u03b7E t0 [ q v,t\u22121 , q w,t\u22121 ] under the assumption that SGD with momentum has the APCG property. The proof involves defining certain properties and leveraging negative curvature to show a contradiction in function value decrease. The text discusses proving by contradiction that the function value must decrease at least F thred in T thred iterations on expectation. By leveraging negative curvature, upper and lower bounds are established to show a contradiction, concluding the desired outcome. The function value must decrease at least F thred in T thred iterations on expectation, proven by leveraging negative curvature and establishing upper and lower bounds. The function value must decrease at least F thred in T thred iterations on expectation, proven by leveraging negative curvature and establishing upper and lower bounds. At ICLR 2020, the constraint of \u03b7 guarantees certain conditions, including using inequalities and conditions to show the desired result. Lemma 15 by Daneshmand et al. (2018) defines an event and provides a proof regarding the selection of specific values with a certain probability. The text discusses the selection of specific values with a certain probability to achieve second order stationary points in optimization algorithms. It leverages negative curvature and establishes upper and lower bounds to ensure the function value decreases as expected. The text discusses achieving second order stationary points in optimization algorithms by leveraging negative curvature and specific values selection with high probability. It provides a sketch of the proof of Theorem 1 and references a lemma for guaranteeing second order stationary points. The proof in Appendix G utilizes Lemma 15 from Daneshmand et al. (2018) to ensure sampling a w from a specific set gives a second order stationary point with high probability. By selecting w from a certain range, a w_k can be chosen where a specific condition does not occur, satisfying the conditions for Lemma 15. The proof in Appendix G uses Lemma 15 to ensure sampling a w from a specific set gives a second order stationary point with high probability. Conditions for Lemma 15 are satisfied by choosing w_k where a specific condition does not occur. The analysis involves bounding the difference in function values based on gradient norms and curvatures. The proof in Appendix G utilizes Lemma 15 to guarantee that sampling a specific w results in a second order stationary point with high probability. The conditions for Lemma 15 are met by selecting w_k where a particular condition does not happen. The analysis focuses on limiting the difference in function values based on gradient norms and curvatures. The proof in Appendix G utilizes Lemma 15 to ensure that selecting a specific w leads to a second order stationary point with high probability. By meeting the conditions of Lemma 15, a w_k is chosen where a specific condition does not occur. The analysis aims to restrict the discrepancy in function values based on gradient norms and curvatures. The parameters are set accordingly to achieve this result. Algorithm 2 is proven to be better than previous methods as it can find a second order stationary point faster with a higher momentum."
}