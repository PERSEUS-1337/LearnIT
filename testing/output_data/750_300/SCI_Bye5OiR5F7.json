{
    "title": "Bye5OiR5F7",
    "content": "A new method for training GANs involves using the Wasserstein-2 metric proximal on the generators. The gradient operator induced by optimal transport connects sample space and parameter space in deep generative models, providing a regularizer for parameter updates. This method improves training speed and stability in GANs, as shown in experiments measuring wall-clock time and FID learning curves. Generative Adversarial Networks (GANs) are a powerful approach to learning generative models where a discriminator distinguishes between real and generated data, while the generator aims to deceive the discriminator by recreating the density distribution from the real source. This adversarial game is formulated as an optimization problem over an implicit generative model for the generator, involving the minimization of a discrepancy measure like the Kullback-Leibler (KL) divergence. The generator in Generative Adversarial Networks aims to recreate the density distribution from the real source by minimizing a discrepancy measure like the Kullback-Leibler (KL) divergence. An alternative approach is the optimal transport, such as Wasserstein distance, used to define loss functions for learning generative models. The Wasserstein distance, also known as Earth Mover's distance, is an alternative approach to defining a discrepancy measure between densities. It has been used to define loss functions for generative models like Wasserstein GAN. Optimal transport can also introduce structures for optimization, such as the Wasserstein steepest descent flow. This paper focuses on deriving the Wasserstein steepest descent flow for deep generative models in GANs. In this paper, the Wasserstein steepest descent flow for deep generative models in GANs is derived using the Wasserstein-2 metric function to obtain a Riemannian structure and natural gradient. Natural gradients, like the Fisher-Rao natural gradient induced by KL divergence, offer advantages over Euclidean gradients in learning problems. In GANs, the Fisher-Rao natural gradient induced by the KL divergence is problematic due to low dimensional support sets. To address this, the gradient operator induced by the Wasserstein-2 metric is proposed for GAN generators, with regularization as the squared constrained Wasserstein-2 distance. The constrained distance can be approximated by a simple neural network in practice for implicit generative models. The Wasserstein-2 metric is used for GAN generators with regularization as the squared constrained distance. A simple neural network can approximate the constrained distance in practice. The relaxed proximal operator simplifies computation by only involving the difference of outputs, making parameter updates easier. This method can be easily implemented as a regularizer for generator updates. The relaxed proximal operator simplifies computation by only involving the difference of outputs, making parameter updates easier. It can be easily implemented as a regularizer for generator updates. The paper introduces a Wasserstein proximal method and demonstrates its effectiveness in experiments with various types of GANs. Related work on optimal transport and its proximal operator on a parameter space is also reviewed. In Section 3, the proposed methods are tested on various GANs to show their effectiveness. Section 4 discusses related work and introduces optimal transport and its proximal operator on a parameter space. Optimal transportation defines distance functions between probability densities, with W p as the Wasserstein-p distance. The focus in this paper is on the case p = 2. The Wasserstein-2 distance, denoted as W, is a distance metric between probability densities with finite second moments. It involves transporting an initial density to a final density with minimal kinetic energy along a trajectory. This paper extends the classic theory to cover parameterized density models. The classic theory of the Wasserstein-2 distance involves transporting an initial density to a final density with minimal kinetic energy along a trajectory. This paper extends the theory to cover parameterized density models, where the density path is constrained within a parametrized model. The constrained Wasserstein-2 metric function is defined for parameterized probability densities. The constrained Wasserstein-2 metric function d W is defined for parameterized probability densities, allowing for optimization schemes based on steepest descent. The constrained Wasserstein-2 metric on parameter space differs from the full density set's Wasserstein-2 distance. It can be used for steepest descent optimization, formulated with a Riemannian structure and Fisher natural gradient. The constrained metric allows for a Wasserstein natural gradient, useful for loss function optimization. The constrained Wasserstein-2 metric structure allows for the Wasserstein natural gradient in optimization of loss functions. The gradient operator is defined with respect to the constrained Wasserstein metric and the steepest descent flow is determined accordingly. The constrained Wasserstein-2 metric enables the use of the Wasserstein natural gradient in loss function optimization. The steepest descent flow is determined by the natural gradient operator with respect to the constrained Wasserstein metric. The gradient descent iteration can be computed using the forward Euler method with a step size of h > 0. The computation of matrix G(\u03b8) \u22121 can be challenging in practice. Another numerical scheme for the equation involves the proximal operator, known as the Jordan-Kinderlehrer-Otto (JKO) scheme. The backward Euler method, also known as the Jordan-Kinderlehrer-Otto (JKO) scheme, provides a numerical scheme for equation 2. The distance of the parameter update acts as a regularization to the loss function, and computing d W distance can be approximated locally by a second order Taylor expansion. This approximation is particularly useful in the parameterized setting, allowing for the derivation of first order schemes like the Semi-Backward Euler method. The Semi-Backward Euler method for the gradient flow of loss function F : \u0398 \u2192 R is given by DISPLAYFORM8 where the supremum is taken over \u03a6 : R n \u2192 R with sufficient regularity for the integral to be well defined. It is easier to approximate than the forward Euler method and simpler than the backward Euler method (JKO) as it does not require computing and inverting G(\u03b8). The Semi-Backward Euler method is implemented in implicit generative models by defining the generator for each parameter \u03b8 in the parameter space R^d. It takes an input noise prior Z to generate an output sample with density X, simplifying the constrained optimization involved in computing the output. The generator in implicit generative models maps noise prior Z to output sample X with density, simplifying optimization. The constrained Wasserstein-2 metric allows for a simpler formulation, defining the relaxed Wasserstein metric and introducing a proximal operator algorithm on generators. The constrained Wasserstein-2 metric in implicit generative models introduces a simpler formulation, defining the relaxed Wasserstein metric and a proximal operator algorithm on generators. It requires the derivative of the generator with respect to a parameter to be a gradient vector field of a potential function. The constrained Wasserstein metric requires the generator's derivative with respect to a parameter to be a gradient vector field of a potential function. The gradient constraint is satisfied when the sample space is one-dimensional. Finding the function \u03a6 involving the parameter space \u0398 is computationally challenging. To simplify computations, a relaxed Wasserstein metric on the parameter space is considered. The constrained Wasserstein metric requires the generator's derivative to be a gradient vector field of a potential function. Finding the function \u03a6 in parameter space \u0398 is computationally challenging. To simplify computations, a relaxed Wasserstein metric on the parameter space is considered, withdrawing the gradient constraint. This approximation leads to a relaxed Wasserstein proximal operator based on a new metric, which regularizes the generator in high-dimensional sample spaces. When the sample space is high dimensional, the update is a regularization of the generator by the expectation of squared differences. The Relaxed Wasserstein Proximal algorithm uses a parameterized function to minimize and a generator, with specified step-size and batch size. It demonstrates the effectiveness of the Wasserstein proximal operator in GANs through a toy example. The Wasserstein proximal operator in GANs is illustrated using a toy example with a family of distributions. The proximal regularization for a loss function is defined, incorporating a commonly used statistical distance. The proximal regularization for a loss function F : \u0398 \u2192 R is defined using statistical distance functions like Wasserstein-2, Euclidean, and Kullback-Leibler divergences. Euclidean distance is independent of the model structure, while Wasserstein-2 distance is effective in measuring differences in probability models. The Wasserstein-2 and Euclidean distances are effective in measuring differences in probability models, unlike KL divergence and L2-distance. The Euclidean distance is model-independent, while the constrained Wasserstein-2 metric depends on the model structure. Wasserstein proximal outperforms Euclidean proximal in decreasing the objective function based on the shrinkage operator. The Wasserstein proximal solution outperforms the Euclidean proximal in decreasing the objective function. Numerical experiments show that the Relaxed Wasserstein Proximal algorithm provides better speed and stability in training GANs. The Relaxed Wasserstein Proximal (RWP) algorithm improves speed and stability in training GANs by applying regularization on the generator, which is novel compared to traditional GAN training methods. The Relaxed Wasserstein Proximal (RWP) algorithm introduces a novel approach to GAN training by focusing on regularizing the generator instead of the discriminator. It modifies the update rule for the generator by introducing hyperparameters and updating the discriminator before updating the generator. This method aims to improve speed and stability in training GANs. The Relaxed Wasserstein Proximal (RWP) algorithm is tested on various GAN types using CIFAR-10 and CelebA datasets with DCGAN architecture. Fr\u00e9chet Inception Distance (FID) is used to measure sample quality and convergence. The DCGAN architecture is used with the CIFAR-10 and CelebA datasets to measure sample quality and convergence using the Fr\u00e9chet Inception Distance (FID). Hyperparameter choices for training are provided in Appendix C. The Relaxed Wasserstein Proximal regularization improves the speed and stability of convergence in training with the DCGAN architecture on CIFAR-10 and CelebA datasets. The regularization shows improved convergence speed measured in wallclock time and achieves a lower Fr\u00e9chet Inception Distance (FID). Using RWP improves convergence speed and FID for all GAN types, with a 20% improvement in sample quality for DRAGAN. Results are consistent on the CelebA dataset as well. Multiple generator iterations may hinder Standard GANs on CelebA initially, but restarting the algorithm leads to successful learning. Multiple generator iterations may hinder Standard GANs on CelebA initially, but restarting the algorithm leads to successful learning. This defect may be rectified with a more stable loss function or different parameters. The effect of multiple generator updates compared to discriminator updates is worth examining in RWP. In RWP, the effect of not using regularization on GANs is examined. Even with the most stable GAN type - WGAN-GP, omitting regularization leads to high FID variance. However, with RWP, FID converges more stably and achieves a lower FID. Samples and latent space walks demonstrate that RWP regularization does not cause GANs to memorize. Using RWP regularization improves speed and achieves a lower FID in GANs. Multiple generator iterations may cause initial learning to fail, but once successful, it remains stable. Successful runs are shown in Figure 4. RWP regularization improves speed and lowers FID in GANs. Multiple generator iterations may initially fail learning but stabilize once successful. Experiment in Figure 4 shows the impact of 10 generator iterations per outer-iteration with and without RWP over 1,000,000 iterations, demonstrating convergence and lower FID with RWP. The experiment involves a loop of discriminator and generator iterations, totaling 1,000,000 outer-iterations to demonstrate long-term behavior. RWP regularization leads to convergence and lower FID, while training without RWP shows high variability and increasing FID. The Semi-Backward Euler (SBE) method is compared to standard WGAN-GP training on the CIFAR-10 dataset, with results averaged over 5 runs. The experiment compared the Semi-Backward Euler (SBE) method to standard WGAN-GP training on the CIFAR-10 dataset. Results were averaged over 5 runs, showing that SBE is comparable to norm WGAN-GP. The algorithm and hyperparameter settings are detailed in the appendix. In Section G, optimization attempts were made on three networks in FIG3. The Semi-Backward Euler method was found to be comparable to norm WGAN-GP. The use of Wasserstein distance as the loss function in optimal transport and GANs has been widely studied in the literature. The Wasserstein distance is commonly used as the loss function in machine learning and GANs due to its statistical properties and ability to compare probability distributions on lower dimensional sets. This has been leveraged in Wasserstein GANs for optimal performance. The Wasserstein distance is utilized in GANs as the loss function, specifically the Wasserstein-1 distance function, requiring the discriminator to satisfy the 1-Lipschitz condition. Regularization techniques are employed to meet this condition. The Wasserstein-2 metric provides a metric tensor structure for the probability space. The regularization of the discriminator in GANs aims to satisfy the 1-Lipschitz condition using Wasserstein-2 metric, creating a density manifold with gradient flows linked to transport-related equations like the Fokker-Planck equation. The Fokker-Planck equation is a key example of gradient flow in probability space, with applications in learning communities. Groups explore stochastic gradient descent using transition equations in probability over parameters. Nonparametric models like the Stein gradient descent method are studied as a generalization of Wasserstein gradient flow. Additionally, approximate inference methods are considered for computation. Many nonparametric models, including the Stein gradient descent method, have been studied as a generalization of Wasserstein gradient flow. Approximate inference methods for computing Wasserstein gradient flow in full probability sets are also considered, introducing an approximation towards Kantorovich dual variables. The Wasserstein structure can be constrained on parameter space, with studies on constrained Wasserstein gradient with fixed mean and variance. Many approaches focus on Gaussian families or elliptical distributions. Our approach applies the constrained Wasserstein gradient to general implicit generative models, focusing on regularizing the generator. This is a departure from previous works that mainly focused on regularizing the discriminator. The constrained Wasserstein gradient is applied to implicit generative models, focusing on regularizing the generator. This method improves convergence speeds and minimizes FID in Wasserstein GAN. The proposed method improves convergence speeds and minimizes FID in Wasserstein GAN by introducing a Riemannian structure in density space and considering a metric function W2 in the full probability set. The proposed method introduces a Riemannian structure in density space and considers a metric function W2 in the full probability set. The tangent space of smooth and strictly positive probability densities is defined, along with the Wasserstein gradient operator for a loss function in (P + , g W ). The inner product g W endows P + with a Riemannian metric tensor, defining a geometric action energy in (P + , g W ). The Wasserstein gradient operator in (P + , g W ) is given for a loss function F : P + \u2192 R. More analytical results on the Wasserstein-2 gradient flow are provided in BID3. The Wasserstein-2 metric and gradient operator are then constrained on statistical models defined by a triplet (\u0398, R n , \u03c1), where \u0398 \u2282 R d and \u03c1 : \u0398 \u2192 P(R n ). The Wasserstein-2 metric and gradient operator are constrained on statistical models defined by a triplet (\u0398, R^n, \u03c1), where \u0398 \u2282 Rd and \u03c1 : \u0398 \u2192 P(R^n). A Riemannian metric is defined on \u03c1(\u0398) by pulling back the Wasserstein-2 metric tensor. The Wasserstein statistical manifold is introduced, with a metric tensor associated with it. The Wasserstein-2 metric tensor is defined on statistical models by pulling back the metric tensor. The Wasserstein statistical manifold is introduced with an associated metric tensor. The constrained Wasserstein gradient operator in parameter space is studied in Theorem 2. The distance function can be expressed as an action function in the Wasserstein statistical manifold. The constrained Wasserstein gradient operator in parameter space is studied, with the distance function expressed as an action function in the Wasserstein statistical manifold. The gradient operator on a Riemannian manifold is defined, along with the derivation of the proposed semi-backward method and proof of a claim regarding geodesic paths. The derivation of the proposed semi-backward method and proof of a claim regarding geodesic paths are presented, proving equations 6 and 7 on a Riemannian manifold. The maximizer \u03a6 * satisfies equation 8, leading to the proof of the claim. The Semi-backward method is derived from the definition of G(\u03b8) and the maximizer \u03a6 * satisfying equation 8, leading to the proof of a claim regarding geodesic paths on a Riemannian manifold. This consistent numerical method is presented as a proof of Proposition 4, with the implicit model defined by a push-forward relation and a gradient constraint. The implicit model is defined by a push-forward relation and a gradient constraint. The probability density transition equation of g(\u03b8(t), z) satisfies the constrained continuity equation. The equations involve gradient and divergence operators with integration by parts. The text discusses the computation of the proximal operator explicitly using the push forward relation and gradient constraint. It involves equations with gradient and divergence operators, integration by parts, and the Wasserstein proximal operator. The text discusses the computation of the proximal operator explicitly using the push forward relation and gradient constraint. It involves equations with gradient and divergence operators, integration by parts, and the Wasserstein proximal operator. The proof of Proposition 5 involves computing the Wasserstein and Euclidean proximal operators explicitly, with specific hyperparameter settings for experiments. The hyperparameter settings for the Relaxed Wasserstein Proximal experiments in Section 3.1 include batch sizes, optimizer details, latent space dimensions, and generator iterations for different datasets like CIFAR-10 and CelebA. The hyperparameter settings for the Relaxed Wasserstein Proximal experiments in Section 3.1 include batch sizes, optimizer details, latent space dimensions, and generator iterations for CIFAR-10 and CelebA datasets. The Adam optimizer with specific parameters was used for both datasets, along with a latent space dimension of 100 and 5 generator iterations. The Relaxed Wasserstein Proximal algorithm is designed as an easy-to-implement regularization method. An example is provided for Standard GANs with specific optimizer choices, step-sizes, and batch size. The algorithm involves updating the discriminator, performing Adam gradient descent multiple times, and repeating until a stopping condition is met. The Relaxed Wasserstein Proximal algorithm involves updating the discriminator, performing Adam gradient descent multiple times, and repeating until a stopping condition is met. The only difference between training GANs and using this algorithm is the inclusion of specific terms and the number of generator iterations. In this paper, a Standard GAN with RWP regularization was trained on the CelebA dataset, producing images with an FID of 17.105. WGAN-GP with RWP was also trained on the CIFAR-10 dataset, resulting in images with an FID of 38.3. The study found that walking in the latent space did not detect memorization in GANs with RWP regularization, as shown in smooth transitions in generated samples. The FID for images generated by a GAN with RWP regularization on CIFAR-10 dataset is 38.3. A study on latent space walking did not detect memorization in GANs with RWP regularization. Specific hyperparameters for training WGAN-GP on CIFAR-10 include a batch size of 64, DCGAN architecture, and Adam optimizer with learning rate 0.0002. The architecture for the discriminator and generator includes a one-hidden-layer fully connected network for the potential \u03a6 p, using layer-normalization for each layer. The Adam optimizer with specific parameters is used for both the generator, discriminator, and potential \u03a6 p. The latent space dimension is 100, and updates are made to the discriminator, generator, and potential in each outer-iteration loop. The architecture for the discriminator and generator includes a one-hidden-layer fully connected network for the potential \u03a6 p, using layer-normalization for each layer. The Adam optimizer with specific parameters is used for both the generator, discriminator, and potential \u03a6 p. In each outer-iteration loop, updates are made to the discriminator, generator, and potential. The generator is updated once, and the potential is updated 5 times."
}