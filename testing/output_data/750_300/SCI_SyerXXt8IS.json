{
    "title": "SyerXXt8IS",
    "content": "Auto-generate enhanced input features for ML models with limited training data. Biological neural nets (BNNs) excel at fast learning, particularly in the insect olfactory network which rapidly learns new odors using competitive inhibition, sparse connectivity, and Hebbian updates. MothNet, a computational model of the moth olfactory network, generates new features for ML classifiers, resulting in improved performance. MothNet, an automatic feature generator, enhances ML classifiers by providing new features derived from original ones. These \"insect cyborgs\" outperform baseline ML methods on MNIST and Omniglot data sets, reducing test set errors by 20% to 55%. MothNet also surpasses other feature generating methods like PCA, PLS, and NNs, showcasing the value of BNN-inspired feature generators in the ML context. The study demonstrates that MothNet, an automatic feature generator, surpasses traditional methods like PCA, PLS, and NNs in generating new features for machine learning. This highlights the potential value of BNN-inspired feature generators in improving ML methods' ability to learn from limited data. The architecture aims to enhance machine learning methods' ability to learn from limited data by automatically generating new class-separating features from existing ones. Biological neural nets, such as the insect olfactory network, demonstrate rapid learning capabilities even with minimal exposure to new stimuli. This suggests that effective feature-generators inspired by BNNs could be beneficial for improving machine learning algorithms. The insect olfactory network, specifically the Antennal Lobe (AL) and Mushroom Body (MB), is a simple but effective feedforward network that can rapidly learn new odors with just a few exposures. It incorporates competitive inhibition, high-dimensional sparse layers, and a Hebbian update mechanism, with largely random synaptic connections. The MothNet model, based on the M. sexta moth AL-MB, showed superior performance in learning vectorized MNIST digits compared to standard machine learning methods with limited training data. The MothNet model, based on the M. sexta moth AL-MB, demonstrated rapid learning of vectorized MNIST digits with superior performance using competitive inhibition in the AL, sparsity in the MB, and random synaptic connections. The MothNet model utilizes competitive inhibition in the AL and sparsity in the MB to achieve rapid learning of vectorized MNIST digits. Weight updates only affect MB\u2192Readout connections, with Hebbian updates based on neural firing. The MothNet model uses competitive inhibition in the AL and sparsity in the MB for rapid learning of vectorized MNIST digits. Weight updates only impact MB\u2192Readout connections, with Hebbian updates based on neural firing rates. The architecture was tested as a front-end feature generator for an ML classifier by combining MothNet with a downstream ML module. The MothNet model was used as a feature generator for an ML classifier by combining it with a downstream ML module. The AL-MB acted as an automatic feature generator for ML, improving accuracies on a non-spatial dataset. The MothNet model, acting as an automatic feature generator, significantly improved ML method accuracies on a non-spatial dataset compared to traditional methods like PCA, PLS, NNs, and transfer learning. The MothNet model significantly outperformed traditional methods like PCA, PLS, NNs, and transfer learning in generating features that improved ML accuracy. The vMNIST dataset, created by downsampling and preprocessing the MNIST dataset, provided samples with 85 pixels-as-features, giving an advantage over baseline ML methods. The vMNIST dataset, with 85 pixels-as-features, outperformed traditional methods like PCA, PLS, NNs, and transfer learning in improving ML accuracy. The MothNet model's full network architecture details are provided in [11], and the Matlab code for the cyborg experiments can be found at [12]. MothNet instances were randomly generated from connectivity parameter templates. The MothNet model was trained on vMNIST dataset using stochastic differential equation simulations and Hebbian updates. Cyborg experiments compared MothNet with baseline ML methods, with training samples drawn randomly per class. Full Matlab code for these experiments can be found at [12]. The MothNet model was trained on vMNIST using stochastic differential equation simulations and Hebbian updates. ML methods were then retrained with Readout Neuron outputs from MothNet as additional features, creating \"insect cyborgs\". Trained accuracies of baselines and cyborgs were compared to assess gains. Experiments compared MothNet features with conventional ML methods on vMNIST. Trained ML accuracies of baselines and cyborgs were compared to assess gains. MothNet features were compared with features generated by PCA, PLS, and NN pre-training on vMNIST. The study compared MothNet features with features generated by PCA, PLS, and NN pre-training on vMNIST data. Transfer learning was applied to the NN baseline using an 85-feature vectorized Omniglot dataset. The study compared MothNet features with features generated by PCA, PLS, and NN pre-training on vMNIST data. Transfer learning was applied to the NN baseline using an 85-feature vectorized Omniglot dataset. MothNet readouts significantly improved ML accuracy by capturing new class-relevant features. MothNet architecture effectively captured new class-relevant features, improving ML accuracy on various tasks. Features generated by MothNet outperformed PCA, PLS, and NN, with accuracy gains ranging from 10% to 88% on vMNIST data. MothNet features improved ML accuracy on vMNIST test set, with accuracies ranging from 10% to 88%. The relative reduction in test set error was 20% to 55%, with NN models benefiting the most with a 40% to 55% reduction in error. Relative reduction in test set error ranged from 20% to 55%, with NN models experiencing the greatest benefits. Even high baseline accuracies saw significant improvements. MothNet's front-end enhanced ML accuracy, surpassing its own accuracy ceiling of \u2248 75%. Gains were observed in almost all cases with N > 3 samples per class. The cyborg framework was utilized on vMNIST using PCA projections onto the top 10 modes. The cyborg framework on vMNIST utilized various feature generators like PCA, PLS, and NN to improve ML accuracy. MothNet features were found to be more effective than Nearest Neighbors and SVM. Gains in accuracy were significant with N > 3 samples per class. The MothNet architecture, with a competitive inhibition layer (AL) and a high-dimensional sparse layer (MB), showed significant improvements in accuracy over baseline ML methods even when using a pass-through AL layer. The MothNet architecture with a competitive inhibition layer (AL) and a high-dimensional sparse layer (MB) demonstrated significant accuracy improvements over baseline ML methods. Even with a pass-through AL layer, cyborgs still showed notable gains in accuracy, with the MB being the most crucial component. The AL layer contributed up to 40% of the total gain by generating strong features, benefiting neural networks the most. The AL layer added value by generating strong features, contributing up to 40% of the total gain. A bio-mimetic feature generator with competitive inhibition, sparse projection, and Hebbian weight updates significantly improved learning abilities of standard ML methods on vMNIST and vOmniglot datasets. The bio-mimetic feature generator, MothNet, improved learning abilities of standard ML methods on vMNIST and vOmniglot datasets by making class-relevant information accessible in raw feature distributions. MothNet features outperformed standard methods like PCA, PLS, NNs, and pre-training, with a competitive inhibition layer enhancing classification by creating attractor basins focused on class-specific features. The competitive inhibition layer enhances classification by creating attractor basins for inputs, pushing similar samples away from each other towards their respective class attractors, increasing the effective distance between samples. The sparse connectivity from AL to MB has computational and anti-noise benefits, resembling sparse autoencoders but with differences such as not seeking to match the identity function. The sparse connectivity from AL to MB has computational and anti-noise benefits. The insect MB is similar to sparse autoencoders but with key differences. MB neurons do not have recurrent connections and the Hebbian update mechanism is distinct from backpropagation. The Mushroom Body (MB) requires very few samples for structure improvement in classification, unlike Reservoir Networks. The Hebbian update mechanism in MB is different from backpropagation, with weight updates occurring on a \"use it or lose it\" basis. The dissimilarity of optimizers (MothNet vs ML) may have increased total encoded information."
}