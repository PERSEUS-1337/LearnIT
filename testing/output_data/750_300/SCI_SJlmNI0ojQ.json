{
    "title": "SJlmNI0ojQ",
    "content": "End-to-end acoustic-to-word speech recognition models are popular for their ease of training, scalability, and lack of a lexicon requirement. Contextual acoustic word embeddings can be constructed directly from these models using attention distribution, showing competitive performance on standard sentence evaluation tasks. Our study explores methods to create contextual acoustic word embeddings from a supervised sequence-to-sequence acoustic-to-word speech recognition model. These embeddings perform well on standard sentence evaluation tasks and match the performance of text-based embeddings in spoken language understanding tasks. The approach of learning fixed-size representations for variable length data, whether text or speech-based, is a key focus of our research. The study focuses on creating contextual acoustic word embeddings from a supervised sequence-to-sequence acoustic-to-word speech recognition model. These embeddings match the performance of text-based embeddings in various natural language processing tasks. Popular methods like word2vec, GLoVE, CoVe, and ELMo are utilized for this purpose. In the speech recognition community, research has also progressed in creating embeddings from short-term audio features. In the speech recognition community, research has advanced in creating word representations from variable length acoustic frames, addressing challenges such as speaker variability and different acoustic conditions. Prior approaches involved aligning speech and text by providing word boundaries or segmenting input speech into fixed-length segments. Our work focuses on constructing individual acoustic word embeddings grounded in utterance-level acoustics, unlike previous techniques that ignore specific audio context. Our work presents methods for obtaining acoustic word embeddings from utterance-level acoustics, using an attention-based sequence-to-sequence model trained for direct Acoustic-to-Word speech recognition. This approach eliminates the need for pre-defined word boundaries by automatically segmenting and classifying input speech into individual words. The direct Acoustic-to-Word (A2W) speech recognition model eliminates the need for pre-defined word boundaries by automatically segmenting and classifying input speech into individual words. It also learns acoustic word embeddings in the context of their containing sentence, showing usability in non-transcription downstream tasks. In this paper, the authors evaluate contextual acoustic word embeddings for spoken language understanding tasks. They demonstrate the usability of attention for aligning words to acoustic frames and constructing Contextual Acoustic Word Embeddings (CAWE). Their methods for constructing word representations directly from a speech recognition model are competitive with text-based word2vec embeddings. The utility of CAWE is shown in a speech-based downstream task of Spoken Language Understanding. The authors evaluate Contextual Acoustic Word Embeddings (CAWE) for spoken language understanding tasks, showing their competitiveness with text-based word2vec embeddings. CAWE is demonstrated to be useful in a speech-based downstream task of Spoken Language Understanding, highlighting the potential for transfer learning from pretrained speech models. A2W modeling has been pursued using CTC and S2S models, with prior work emphasizing the need for large amounts of training data. Prior work on A2W modeling using CTC and S2S models has shown the need for large amounts of training data with extensive vocabularies. Recent progress indicates the possibility of training these models with less data but limited vocabularies. Solutions for out-of-vocabulary words involve using smaller units like characters or sub-words. The solutions for generating out-of-vocabulary words in A2W modeling involve using smaller units like characters or sub-words. Recent work has focused on training models with limited vocabularies, such as a 300-hour Switchboard corpus with a vocabulary of about 30,000 words. Further advancements have been made to improve the training of these models for large vocabulary tasks. In previous work, models were trained on a 300-hour Switchboard corpus with a vocabulary of 30,000 words. BID12 improved training for large vocabulary tasks by automatically learning word boundaries. Various methods, except BID6, used unsupervised learning to obtain acoustic word embeddings. BID4, BID5, BID7, BID25, and BID8 focus on learning acoustic word embeddings. Except for BID6, they use unsupervised methods without transcripts or speech recognition. BID6 uses a supervised CNN model with short speech frames for single-word recognition. BID4 proposes an unsupervised approach for learning speech embeddings with a fixed context of words. BID4 proposes an unsupervised method to learn speech embeddings using a fixed context of words in the past and future, but drawbacks include the need for forced alignment between speech and words for training. Learning text-based word embeddings is also a rich area of research with established techniques. The research on word embeddings has advanced into learning contextualized word embeddings that are beneficial for text-based tasks. Our work connects a speech recognition model with learning contextual word embeddings from speech, similar to the Listen, Attend and Spell model structure. The encoder in our model maps input acoustic feature vectors. The S2S model used in this work consists of an encoder network, a decoder network, and an attention model. The encoder is a pyramidal multi-layer bi-directional LSTM network that maps input acoustic features to higher-level features. The decoder is an LSTM network that models the output distribution over the next target based on previous predictions. The model aims to learn contextual word embeddings from speech data. The decoder network in the Short Term Memory (BLSTM) network learns to model the output distribution over the next target based on previous predictions. It uses an attention mechanism to generate targets from the input. The attention mechanism enforces monotonicity in alignments by applying a convolution across time. This leads to a peaky distribution in the attention for the current time step. The model uses a convolution across time to enforce monotonicity in alignments and generate peaky attention distributions. Acoustic word embeddings are obtained from an end-to-end trained speech recognition system. The model utilizes a convolution across time to ensure alignment monotonicity and generate focused attention distributions. Acoustic word embeddings are derived from the end-to-end trained speech recognition system, using hidden representations from the encoder and attention weights from the decoder. The method for creating \"contextual\" acoustic word embeddings is akin to CoVe BID2 for text embeddings, with a focus on aligning input speech with output words through a location-aware attention mechanism. Our method utilizes a location-aware attention mechanism to automatically segment continuous speech into words and obtain word embeddings. This process constructs contextual acoustic word embeddings by assigning higher probability to certain frames, leading to a peaky attention distribution. The method utilizes an attention mechanism to segment continuous speech into words and obtain word embeddings. Attention weights on acoustic frames reflect their importance in classifying a word, allowing for the construction of word representations based on these weights. The model uses an attention mechanism to assign importance to acoustic frames for word representations. The hidden representations are weighted based on their relevance to the word, as shown in Figure 1. The model maps words to acoustic frames with the highest attention weights, allowing for word segmentation and embeddings. The model uses attention to assign importance to acoustic frames for word representations. It obtains mappings of words to acoustic frames and describes three ways of obtaining acoustic word embeddings. This includes unweighted Average (U-AVG) and Attention weighted Average (CAWE-W). The text discusses three methods for obtaining acoustic word embeddings: unweighted Average (U-AVG), Attention weighted Average (CAWE-W), and maximum attention (CAWE-M). These techniques use attention to assign importance to acoustic frames for word representations, resulting in Contextual Acoustic Word Embeddings (CAWE). The text introduces Contextual Acoustic Word Embeddings (CAWE) techniques, which use attention scores to create word representations from acoustic frames in speech data. Two datasets, the Switchboard corpus and a subset of the How2 dataset, are used for evaluation. The CAWE technique utilizes attention scores to generate word representations from acoustic frames in speech data. The Switchboard corpus consists of 2,430 telephonic conversations with 500 speakers, while the How2 dataset subset includes 300 hours of instructional videos with 13,662 videos. The A2W achieves word error rates of 22.2% on Switchboard and 36.6% on CallHome from the Switchboard Eval2000 test set, and 24.3% on the dev5 test set of How2. The embeddings are evaluated in 16 benchmark sentence tasks. The A2W achieves word error rates of 22.2% on Switchboard and 36.6% on CallHome from the Switchboard Eval2000 test set, and 24.3% on the dev5 test set of How2. The embeddings are evaluated in 16 benchmark sentence tasks covering various evaluation tasks such as Semantic Textual Similarity, classification, sentiment analysis, question type, Subjectivity/Objectivity, opinion polarity, entailment, semantic relatedness, and paraphrase detection using datasets like STS, SICK, and MRPC. The A2W achieves word error rates of 22.2% on Switchboard and 36.6% on CallHome from the Switchboard Eval2000 test set, and 24.3% on the dev5 test set of How2. Evaluation tasks include Semantic Textual Similarity (STS), sentiment analysis (SST), question type (TREC), Subjectivity/Objectivity (SUBJ), opinion polarity (MPQA), entailment, semantic relatedness using SICK dataset for SICK-E (entailment) and SICK-R (relatedness), and paraphrase detection (MRPC). Training involves logistic regression for classification tasks and evaluation using the SentEval toolkit BID26. The remaining tasks are evaluated using the SentEval toolkit BID26 with logistic regression for classification. CAWE-M outperforms U-AVG by 34% and 13% on Switchboard and How2 datasets, and CAWE-W by 33.9% and 12% respectively. The concatenation of CAWE and CBOW features in the logistic regression model outperforms U-AVG on STS tasks by 34% and 13% on Switchboard and How2 datasets. CAWE-M performs better than CAWE-W, attributed to noisy estimation of word embeddings in CAWE-W. The comparison between CAWE-M and U-AVG shows that CAWE-M outperforms U-AVG due to a more confident attention score in constructing the embeddings. Additionally, CAWE-W performs worse than CAWE-M on STS and SICK-R tasks because it uses an even noisier process in weighting encoder hidden representations equally. The text discusses the comparison between CAWE-M and U-AVG, highlighting that CAWE-M outperforms U-AVG due to a more confident attention score in constructing the embeddings. It also mentions that CAWE-W performs worse than CAWE-M on certain tasks due to its noisier process of weighting encoder hidden representations equally. The datasets used for downstream tasks and training details are also briefly mentioned. The text discusses the performance of CAWE compared to word2vec CBOW, highlighting that CAWE has a smaller vocabulary but competitive performance. Despite A2W model's limited vocabulary coverage, CAWE performs well. The performance of CAWE is competitive with word2vec CBOW despite having a smaller vocabulary. Evaluations show that acoustic embeddings combined with text embeddings outperform word2vec on 10 out of 16 tasks. The gains are more prominent in Switchboard dataset compared to How2. The concatenated embedding of CAWE-M improves the CBOW embedding on 10 out of 16 tasks. The gains are more noticeable in the Switchboard dataset compared to the How2 dataset. Additionally, CAWE is evaluated on the ATIS dataset for Spoken Language Understanding, which is similar in domain to Switchboard. CAWE is evaluated on the ATIS dataset for Spoken Language Understanding, similar to Switchboard, using a simple RNN-based model architecture with embedding, RNN variant, dense layer, and softmax. Model trained for 10 epochs with RMSProp. Neural Network (RNN) model with embedding layer, Simple RNN or GRU, dense layer, and softmax trained for 10 epochs with RMSProp. Demonstrated that speech-based word embeddings can match text-based word embeddings performance on ATIS dataset. Word embeddings trained on large text corpora consistently lead to better performance on the ATIS dataset. Direct speech-based word embeddings show comparable performance to text-based word embeddings in speech-based tasks, highlighting the utility of speech-based embeddings. Test scores are compared using CAWE-M, CAWE-W, and CBOW embeddings, fine-tuning based on the task. Contextual acoustic word embeddings are learned from a sequence-to-sequence acoustic-to-word speech recognition model, analyzing the role of attention in constructing contextual embeddings. The method presented learns contextual acoustic word embeddings from a speech recognition model, utilizing attention. These embeddings are competitive with word2vec text embeddings and outperform simple methods by up to 34% in semantic tasks. They match text-based embeddings in spoken language understanding, serving as a pre-trained model. Contextual acoustic word embeddings outperform simple methods by up to 34% in semantic tasks and match text-based embeddings in spoken language understanding. The model shows potential for improving downstream tasks despite the complexity of noisy audio input. Future work will focus on scaling the model to larger corpora and vocabularies, and comparing with non-contextual acoustic word embedding methods. In the future, the model will be scaled to larger corpora and vocabularies, and compared with non-contextual acoustic word embedding methods. This research was supported by the Center for Machine Learning and Health at Carnegie Mellon University and Facebook."
}