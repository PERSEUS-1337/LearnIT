{
    "title": "Byl5NREFDr",
    "content": "In the study of model extraction in natural language processing, an adversary can reconstruct a victim model without real training data by using random word sequences and task-specific heuristics. This method is effective across various NLP tasks like natural language inference and question answering. The study demonstrates that random word sequences and task-specific heuristics can be used to extract a model in natural language processing tasks. This method is effective in tasks such as natural language inference and question answering, highlighting the potential for model extraction with a limited query budget. Additionally, the research explores defense strategies like membership classification and API watermarking, which can be circumvented by more sophisticated attackers. Machine learning models are valuable intellectual property, often only accessible through web APIs. Malicious users may try to extract models to avoid the costly development process. Defense strategies like membership classification and API watermarking can be circumvented by clever attackers. Malicious users may attempt to steal machine learning models served through web APIs to avoid the expensive development process. This model extraction technique involves issuing numerous queries to train a local copy of the model using collected input-output pairs. Extracted models can leak sensitive information about the training data or be used to create adversarial examples. Contextualized pretrained representations like ELMo and BERT are popular for NLP APIs due to their performance boost and reduced sample complexity. They are used to train local copies of models through a large number of queries, which can lead to theft of intellectual property and leakage of sensitive information. In this paper, it is demonstrated that NLP models fine-tuned from a pretrained BERT model can be extracted without access to the training data used by the API provider, even without issuing well-formed queries. This is due to the advantageous properties of contextualized pretrained representations like ELMo and BERT, which boost performance and reduce sample complexity. Experiments show that NLP models can be extracted without access to training data or well-formed queries, using simple heuristics. This contrasts with prior work that required access to semantically-coherent data for large-scale attacks. Experiments show that NLP models can be extracted without access to training data or well-formed queries, using simple heuristics. This contrasts with prior work that required access to semantically-coherent data for large-scale attacks. The extraction performance improves by using randomly-sampled sentences and paragraphs from Wikipedia as queries. These attacks are cost-effective, with the most expensive one estimated at around $500. The attacker extracts NLP models without training data or coherent queries, using random word sequences. The process involves sampling words, querying a victim BERT model, and fine-tuning their own BERT model with the victim's outputs. This method is cost-effective, with the most expensive attack estimated at $500. The attacker extracts NLP models using random word sequences to fine-tune their own BERT model with the victim's outputs. Random queries are effective but nonsensical, with pretraining on the attacker's side making model extraction easier. Simple defenses against this method are also studied. The study explores how pretraining on the attacker's side facilitates model extraction. It also evaluates the effectiveness of defenses like membership classification and API watermarking against extraction attempts. The research aims to inspire further exploration of robust defenses against model extraction and understanding vulnerabilities in models and datasets. The study discusses the vulnerability of models and datasets to model extraction attacks and calls for stronger defenses. It relates the work to prior efforts in computer vision and zero-shot distillation. Model extraction attacks have been studied empirically and theoretically. The study discusses model extraction attacks in NLP systems, highlighting the lack of prior work in this area compared to image classification APIs. Prior research has focused on synthesizing queries close to decision boundaries, but this method does not transfer well to text-based systems due to the discrete nature of the input space. Pal et al. (2019) is the only prior work attempting extraction on NLP systems using pool-based active learning. The study explores model extraction attacks in NLP systems, focusing on the lack of prior work in this area compared to image classification APIs. Previous research has concentrated on synthesizing queries close to decision boundaries, which does not transfer well to text-based systems due to the discrete input space. Pal et al. (2019) is the only prior work attempting extraction on NLP systems using pool-based active learning. Our work focuses on distilling knowledge from larger models to smaller ones for tasks like question answering, unlike prior work that requires white-box access to the teacher model. Rubbish inputs, randomly-generated examples yielding high-confidence predictions, have been explored in model extraction literature. Prior work has successfully extracted knowledge from SVMs and 1-layer networks using i.i.d noise, but scaling this to deeper neural networks is a new challenge. Unnatural text inputs can lead to overly confident model predictions, disrupt translation systems, and trigger disturbing outputs from text generators. BERT, a 24-layer transformer model, is studied for model extraction. Unnatural text inputs have been shown to train models effectively for NLP tasks without real examples. BERT, Bidirectional Encoder Representations from Transformers, is a 24-layer transformer model that converts word sequences into contextualized vector representations. Its parameters are learned through masked language modeling on unlabeled text data, leading to state-of-the-art performance in NLP tasks with minimal supervision. The release of BERT revolutionized NLP by achieving state-of-the-art performance on various tasks with minimal supervision. A modern NLP system typically uses fine-tuning with task-specific networks to leverage BERT's parameters for end-to-end learning on training data. A composite function g T = f T,\u03c6 \u2022 f bert,\u03b8 is constructed using a network with parameters \u03c6 and input v. The parameters \u03c6 T , \u03b8 T are learned end-to-end through fine-tuning with training data for task T. Description of extraction attacks involves reconstructing a local copy of a black-box API model g T using a task-specific query generator to create nonsensical word sequences as queries. The attacker reconstructs a local copy of a black-box API model g T by using a task-specific query generator to create nonsensical word sequences as queries. This dataset is then used to train a new model g T on four diverse NLP tasks. The attacker reconstructs a black-box API model g T using a task-specific query generator to create nonsensical word sequences. Models are extracted on four NLP tasks with different input and output spaces. The curr_chunk discusses different NLP tasks such as entailment, contradiction, extractive question answering, and boolean question answering using various datasets. It also mentions two query generators, RANDOM and WIKI, for generating input queries. The curr_chunk discusses two query generators, RANDOM and WIKI, used for generating input queries. These generators were found insufficient for tasks requiring complex interactions between different parts of the input space. Task-specific heuristics were applied to address this issue, such as in the MNLI task where premise and hypothesis often share similarities. The text discusses task-specific heuristics applied to address complex interactions in tasks like MNLI and SQuAD/BoolQ. For MNLI, three words in the premise are replaced with random words to construct the hypothesis. In SQuAD/BoolQ, words are sampled from the passage to form a question, with a question starter word added at the beginning and a ? symbol at the end. The text discusses query generation by sampling words from a passage to form a question. Examples and evaluation of the extraction procedure are provided, along with cost estimates for different query budgets using Google Cloud Platform's Natural Language API. The text discusses query generation by sampling words from a passage to form a question and evaluates the extracted models' accuracy at different query budgets using the Google Cloud Platform's Natural Language API calculator. The text discusses query generation by sampling words from a passage to form a question and evaluates the extracted models' accuracy at different query budgets. The extracted models show high accuracies even with low query budgets, and there are diminishing accuracy gains at higher budgets. In a controlled setting, the extracted models are surprisingly accurate on the original development sets of all tasks, even when trained with nonsensical inputs. For example, extracted SQuAD models recover 95% of original accuracy despite seeing only nonsensical questions. The extracted models show high accuracies on original development sets of all tasks, even with nonsensical inputs. Despite training with only nonsensical questions, SQuAD models recover 95% accuracy. However, agreement between extracted models is slightly better than accuracy, with lower agreement on held-out sets. On SQuAD, extracted WIKI and RANDOM models have low agreements, indicating poor functional equivalence. An ablation study with alternative query generation heuristics for SQuAD and MNLI is conducted in Appendix A.4. Classification with argmax labels only: For classification datasets, we assumed the API returns a probability distribution over output classes. This information may not be available to the adversary in practice. The API's output probability distribution may not be available to adversaries in practice. When using argmax labels only, there was a minimal drop in accuracy for classification datasets, indicating that access to the full probability distribution is not crucial for model extraction. The study found that access to the output probability distribution is not necessary for model extraction. Even with small query budgets, extraction can be successful, but accuracy gains diminish with more queries. The approximate costs of these attacks can be inferred from the results. In this section, an analysis is performed to understand why nonsensical input queries are effective for extracting NLP models based on BERT. The study found that even without access to the output probability distribution, model extraction can be successful with small query budgets, but accuracy gains diminish with more queries. The approximate costs of these attacks can be inferred from the results. In this section, the study delves into the effectiveness of nonsensical input queries for extracting NLP models based on BERT. The focus is on understanding why models trained on these queries perform well, examining different victim models' responses to nonsensical queries, and evaluating the representativeness of these queries. Specifically, the RANDOM and WIKI extraction configurations for SQuAD are analyzed to address these questions. The study analyzes the effectiveness of nonsensical input queries for NLP models based on BERT, focusing on interpreting these queries for humans. Different victim SQuAD models are trained and their agreement on answers to nonsensical queries is measured. Models agree well on SQuAD training and development set queries but less on other types. The study examines the agreement between NLP models based on BERT when answering nonsensical queries. Models show high agreement on SQuAD training and development set queries but significantly lower agreement on WIKI and RANDOM queries. This suggests that models are less robust on nonsensical inputs, but high-agreement queries may be more useful for model extraction. High-agreement queries are more useful for model extraction as victim models tend to be brittle on nonsensical inputs. Sorting queries from RANDOM and WIKI datasets by agreement levels, high-agreement subsets consistently yield large F1 improvements in model extraction compared to random and low-agreement subsets of the same size. This indicates that agreement between victim models serves as a good indicator of input-output pair quality for extraction. The study found that high-agreement subsets are more effective for model extraction compared to random and low-agreement subsets of the same size. This suggests that agreement between victim models is a reliable indicator of input-output pair quality for extraction. The researchers also raise the question of whether high-agreement nonsensical queries are interpretable to humans, highlighting the potential for future work to integrate this observation into active learning objectives for improved extraction. The study found that high-agreement subsets are more effective for model extraction. Researchers question if high-agreement nonsensical queries are interpretable to humans. An experiment with human annotators showed 23% exact matches with victim models on WIKI subset questions. Three human annotators answered SQuAD questions from WIKI and RANDOM subsets with 23% exact match to victim models. Annotators scored significantly higher on original SQuAD questions (77% exact match). Annotators used a word overlap heuristic to select answer spans. In interviews, annotators used a word overlap heuristic to select answer spans, scoring 85% F1 against original answers. Annotators found nonsensical question-answer pairs mysterious. The victim and attacker fine-tune a pretrained BERT-large model, but in practical scenarios, the attacker may not have information about the victim's architecture. In practical scenarios, the attacker may lack information about the victim's architecture when fine-tuning a different base model. The extraction accuracy depends on the pretraining setup, with higher accuracy observed when using different configurations of BERT-large and BERT-base models. BERT comes in two sizes: BERT-large and BERT-base. Accuracy is higher when the attacker starts from BERT-large, even if the victim used BERT-base. When the victim and attacker use the same model, accuracy is better. Finetuning BERT gives attackers an advantage. The study found that attackers have a significant advantage when finetuning BERT models, as only the final layer is randomly initialized. Training a QANet model without pretraining also showed high accuracy on SQuAD inputs. The study demonstrated that training a QANet model on SQuAD without pretraining resulted in high accuracy with original inputs but a significant F1 drop with nonsensical queries. This highlights the importance of better pretraining for models to start with a good language representation, simplifying extraction. BERT-based models are vulnerable to model extraction, leading to a significant F1 drop in performance. Two defense strategies are explored to preserve API utility and remain undetectable to attackers without requiring re-training the victim model. The defense strategies explored aim to protect BERT-based models from model extraction attacks without the need for re-training. One defense involves using membership inference to detect nonsensical inputs or adversarial examples, issuing random outputs when such inputs are identified. The API uses membership inference to detect nonsensical inputs or adversarial examples, issuing random outputs to protect BERT-based models from model extraction attacks. The API uses membership inference to protect BERT-based models from model extraction attacks by training classifiers on original and WIKI extraction examples. The classifiers transfer well to a balanced development set and remain robust to query generation processes like RANDOM and SHUFFLE. The API uses membership inference to protect BERT-based models from model extraction attacks. The classifiers remain robust to query generation processes like RANDOM and SHUFFLE. Watermarking is another defense strategy against extraction. Watermarking is a defense strategy against model extraction attacks, where a fraction of queries are randomly modified to return incorrect outputs. These \"watermarked queries\" are stored on the API side to detect any memorization by extracted models, making them vulnerable to post-hoc detection. The effectiveness of watermarking is evaluated on MNLI and SQuAD datasets. Table 8 shows the results of watermarking on MNLI and SQuAD datasets. Watermarking is done by permuting predicted probability vectors to ensure a different output. Only 0.1% of queries are watermarked to minimize API performance drop. Watermarked models have high WM Label Acc and low Victim Label Acc. Watermarking on MNLI and SQuAD datasets involves permuting predicted probability vectors to alter outputs. Watermarked models exhibit high WM Label Acc and low Victim Label Acc, while non-watermarked models predict victim model outputs. Training with more epochs accentuates these differences. Watermarking works by altering predicted probability vectors on MNLI and SQuAD datasets. Watermarked models have high WM Label Acc and low Victim Label Acc, while non-watermarked models predict victim model outputs. Training for more epochs amplifies these differences. However, watermarking is only effective after an attack has occurred and assumes the attacker will publicly deploy the extracted model with black-box query access. If the attacker keeps the model private or anticipates watermarking, they may take steps to prevent detection. Model extraction attacks against NLP APIs serving BERT-based models are effective at extracting good models with low query budgets, even with nonsensical input queries. Fine-tuning or re-extracting the model with different queries can help prevent detection by the attacker. Model extraction attacks against NLP APIs serving BERT-based models are effective at extracting good models with low query budgets, even with nonsensical input queries. Fine-tuning large pretrained language models simplifies the extraction process for attackers. Existing defenses against extraction are generally inadequate, requiring further research to develop robust defenses against adaptive adversaries. Further research is necessary to develop robust defenses against adaptive adversaries who anticipate simple defenses. Future directions include leveraging nonsensical inputs for model distillation, diagnosing dataset complexity using query efficiency, and investigating agreement between victim models for input distribution proximity in an active learning setup. In this paper, the agreement between victim SQuAD models on RANDOM and WIKI queries is investigated using query efficiency as a proxy. The cost estimate from Google Cloud Platform's Calculator was used, with Natural Language APIs allowing inputs of up to 1000 characters per query. Input instances with more than 1000 characters were counted multiple times for cost calculation. To estimate costs for different datasets, input instances over 1000 characters were counted multiple times using APIs allowing up to 1000 characters per query. Costs for tasks not covered by Google Cloud APIs were extrapolated based on entity and sentiment analysis APIs for natural language inference and reading comprehension. This estimation was deemed reasonable due to the similarity in model complexity to BERT-large. The cost of issuing queries through APIs can vary, with some providers offering free queries. Attackers could exploit this by setting up multiple accounts to collect data. Most APIs are used on webpages and can be easily emulated to extract data if precautions are not taken. APIs are used on webpages and can be exploited by attackers to extract data at no cost. API costs can vary based on infrastructure or revenue models. It is crucial to focus on the low costs of extracting datasets rather than estimates. In this section, details are provided on the low costs of extracting datasets for tasks like machine translation and speech recognition. For example, it costs -$430.56 to extract a large conversational speech recognition dataset and $2000.00 for 1 million translation queries. The focus is on the input generation algorithms used for each dataset. The input generation algorithms for the datasets involve building a vocabulary from wikitext103, preserving the top 10000 tokens based on unigram frequency, and randomly sampling tokens up to a chosen length. The input generation algorithms for the datasets involve building a vocabulary from wikitext103, preserving the top 10000 tokens based on unigram frequency, and randomly sampling tokens up to a chosen length. Tokens are uniformly randomly sampled from the top-10000 wikitext103 vocabulary up to the chosen length. A sentence is chosen at random from wikitext103, and words not in the top-10000 vocabulary are replaced with random words from this vocabulary. The process is repeated three times to construct the final hypothesis. The final hypothesis is constructed by randomly choosing words from the top-10000 wikitext103 vocabulary and replacing them in the premise. This process is repeated three times. The vocabulary is built using wikitext103, and unigram probabilities are stored for each token. The final paragraph is constructed by sampling tokens from the wikitext103 vocabulary based on chosen lengths. Question tokens are randomly sampled to build the question, which is then appended with a ? symbol and prepended with a question starter word chosen randomly. The final paragraph is constructed by sampling tokens from the wikitext103 vocabulary based on chosen lengths. Question tokens are randomly sampled to build the question, which is then appended with a ? symbol and prepended with a question starter word chosen randomly from a list. In query generation, paragraphs are randomly chosen from wikitext103. Questions are sampled similarly to SQuAD and BoolQ. Question starter words are selected from a predefined list. Additional query generation heuristics are explored, and different extraction datasets are compared for SQuAD 1.1. The study shows that RANDOM performs better when paragraphs are sampled. In query generation, paragraphs are randomly chosen from wikitext103. Comparing extraction datasets for SQuAD 1.1, it is found that starting questions with common question starter words like \"what\" helps, especially with RANDOM schemes. A similar study on MNLI shows that lexical overlap between premise and hypothesis is crucial. In an ablation study on MNLI, it was found that lexical overlap between premise and hypothesis impacts model predictions. Low overlap leads to neutral or contradiction predictions, high overlap leads to entailment predictions, and a few different words result in balanced datasets with strong extraction signals. The study found that lexical overlap between premise and hypothesis affects model predictions. Low overlap leads to neutral or contradiction predictions, high overlap leads to entailment predictions, and a few different words result in balanced datasets with strong extraction signals. Human studies involved fifteen annotators who were unfamiliar with the research goals. The study used human annotators to annotate five sets of questions, including original SQuAD questions, WIKI questions with high and low agreement among victim models, and RANDOM questions with high and low agreement. Inter-annotator agreement was shown in Table 10, with the original SQuAD questions having the highest agreement. In an ablation study on input features for the membership classifier, two input feature candidates were considered. The ordering of inter-annotator agreement followed the pattern: original SQuAD > WIKI, highest agreement > RANDOM, highest agreement \u2248 WIKI, lowest agreement > RANDOM, lowest agreement. This ordering reflects the closeness to the actual input distribution. The ablation study compared two input feature candidates for the membership classifier: the logits of the BERT classifier and the last layer representations. Results showed that the last layer representations were more effective in distinguishing between real and fake inputs. Using both features yielded the best results in most cases. The ablation study compared the effectiveness of using the last layer representations versus the logits in distinguishing between real and fake inputs. Results showed that using both feature sets yielded the best results in most cases."
}