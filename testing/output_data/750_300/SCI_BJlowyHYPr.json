{
    "title": "BJlowyHYPr",
    "content": "CloudLSTM is a new recurrent neural model for forecasting data streams from geospatial point-cloud sources. It uses a Dynamic Point-cloud Convolution (D-Conv) operator to extract local spatial features from neighboring points, maintaining permutation invariance and capturing neighboring correlations for spatiotemporal predictive learning. The D-Conv operator in CloudLSTM maintains permutation invariance and captures neighboring correlations for spatiotemporal predictive learning, resolving grid-structural data requirements and easily integrating into LSTM architectures with sequence-to-sequence learning and attention mechanisms. Our proposed architecture, CloudLSTM, is applied to point-cloud stream forecasting for mobile service traffic and air quality indicator forecasting. Real-world datasets show CloudLSTM delivers accurate long-term predictions, outperforming other neural network models. Point-cloud stream forecasting predicts future values and locations of data streams from geospatial point-clouds. Point-cloud stream forecasting predicts future values and locations of data streams from geospatial point-clouds, such as mobile network antennas and air quality sensors, operating on irregular and unordered sets of points. Point-cloud stream forecasting operates on irregular and unordered sets of points, requiring models that can handle complex spatial correlations. Vanilla LSTMs have limited spatial feature exploitation, while ConvLSTM and PredRNN++ are not suitable for scattered point-cloud data. Convolution-based recurrent neural network models like ConvLSTM and PredRNN++ are not ideal for handling scattered point-cloud data, which requires models capable of complex spatial correlations. Different approaches to geospatial data stream forecasting include predicting over grid-structured data streams and forecasting directly over point-cloud data streams using historical information. The proposed PointCNN leverages spatial-local correlations of point clouds, regardless of input order, for forecasting. Existing neural network structures are used for grid-structured data streams, while historical information is utilized for direct forecasting over point-cloud data streams. The CloudLSTM architecture is designed for forecasting over point cloud-streams, utilizing the DConv operator at its core. It combines Seq2seq learning and attention mechanisms for precise forecasting of spatial features in point clouds. CloudLSTM and its variants are explained for precise forecasting over point-cloud streams. A point-cloud is defined as a set of N points, each containing value features and coordinates. Different channels of the point-cloud can be obtained at each time step through various measurements. An ideal point-cloud stream forecasting model should have five key properties: (i) Order invariance, where the arrangement of points does not affect the forecast; (ii) Information preservation. An ideal point-cloud stream forecasting model should exhibit order invariance, information intactness, interaction among points, and robustness to transformations. The Dynamic Point Cloud Convolution (DConv) operator is introduced as the core module of the CloudLSTM, addressing the need for capturing local dependencies among neighboring points, robustness to transformations, and revising dynamic correlations among points during training. The Dynamic Point Cloud Convolution (DConv) operator is introduced as the core module of the CloudLSTM, allowing for dynamic correlations to be revised and learnable during training. DConv generalizes ordinary convolution on grids by computing weighted summations on point-clouds, inheriting properties of ordinary convolution. The Dynamic Point Cloud Convolution (DConv) operator, part of CloudLSTM, performs weighted summations on point-clouds for dynamic correlations, inheriting properties of ordinary convolution. DConv takes U in channels of a point-cloud S and outputs U out channels with the same number of elements to preserve information intactness. The DConv operator in CloudLSTM performs weighted summations on point-clouds to maintain information intactness. It uses 3D tensors for input and output channels, and defines subsets of points in the input set for nearest point calculations. Each point contains value and coordinate features. The DConv operator in CloudLSTM uses anchor points in point clouds to calculate weighted summations, aggregating value and coordinate features. Learnable weights are shared across different anchor points to maintain dynamic spatial correlations. The DConv operator in CloudLSTM utilizes anchor points in point clouds for weighted summations, aggregating value and coordinate features based on their positions at the previous layer/state. Learnable weights are shared across different anchor points to exploit dynamic spatial correlations. The DConv operator in CloudLSTM uses anchor points in point clouds for weighted summations of value and coordinate features. Learnable weights are shared across anchor points to exploit dynamic spatial correlations. Coordinates of raw point-clouds are normalized to (0, 1) before feeding them to the model for improved transformation robustness. The DConv operator in CloudLSTM uses anchor points in point clouds for weighted summations of value and coordinate features. Coordinates of raw point-clouds are normalized to (0, 1) to improve transformation robustness. The K nearest points can vary for each channel, reflecting different types of measurements in the dataset. The spatial correlations in the mobile traffic dataset involve traffic consumption of different mobile apps, while the air quality dataset focuses on various air quality indicators. These correlations vary between measurements due to human mobility, impacting data consumption of apps and air quality indicators. To ensure learnability, the K nearest neighbors are not fixed across channels, allowing each channel to find the best neighbor set. The CloudLSTM contributes to improving forecasting performance by allowing each channel to find the best neighbor set for spatial correlations in data consumption of mobile apps and air quality indicators. The DConv operator weights its K nearest neighbors across all features to produce values and coordinates in the next layer, maintaining symmetry regardless of input permutation. The DConv operator weights its K nearest neighbors across all features to produce values and coordinates in the next layer, maintaining symmetry regardless of input permutation. This allows for capturing local dependencies and improving robustness in forecasting performance. DConv operator in set S captures local dependencies and improves robustness to global transformations by normalizing coordinate features, learning layout and topology of cloud-point for next layer. DConv enables dynamic positioning tailored to each channel and time step by learning the layout and topology of the cloud-point for the next layer. This allows for \"location-variance\" and is essential in spatiotemporal forecasting neural models. The implementation using simple 2D convolution can be easily parallelized in existing deep learning frameworks. DConv enables dynamic positioning tailored to each channel and time step by learning the layout and topology of the point cloud for the next layer. It can be efficiently implemented using simple 2D convolution and is essential in spatiotemporal forecasting neural models. The DConv operator builds upon PointCNN and Deformable Convolution, introducing variations for point cloud structural data. The DConv operator, based on PointCNN and DefCNN, introduces variations for point cloud structural data. It aligns weight based on distance ranking to maintain permutation, avoiding information loss from aggregation over points. The DConv operator maintains permutation by aligning weights based on distance ranking, ensuring order invariance without information loss. It can be seen as a variation of DefCNN over point-clouds, deforming input maps and selecting neighboring points for operation. DefCNN and DConv are convolutional neural networks that operate over point-clouds. DefCNN deforms weighted filters, while DConv deforms input maps. DefCNN uses bilinear interpolation with continuous offsets, while DConv selects neighboring points for its operations. Both models offer transformation modeling flexibility for adaptive receptive fields. The DConv operator can be integrated into LSTMs to learn spatial and temporal correlations over point-clouds, forming the Convolutional Point-cloud LSTM (CloudLSTM). The Convolutional Point-cloud LSTM (CloudLSTM) integrates the DConv operator into LSTMs to learn spatial and temporal correlations over point-clouds. It includes input, forget, and output gates, memory cell, hidden states, learnable weight and bias tensors, and element-wise product operations. The CloudLSTM integrates the DConv operator into LSTMs for spatial and temporal correlations over point-clouds. It combines with Seq2seq learning and soft attention mechanism for effective spatiotemporal modeling on grid-structural data. The CloudLSTM with Seq2seq learning and soft attention mechanism is used for forecasting, incorporating an encoder and decoder with CloudLSTMs. The encoder encodes historical information into a tensor, while the decoder decodes it into predictions using the soft attention mechanism. The encoder encodes historical information into a tensor, and the decoder decodes it into predictions using the soft attention mechanism. Point Cloud Convolutional layers process the data before generating the final forecasting. The CloudCNN layers perform DConv operations similar to word embedding in NLP tasks, translating the raw point-cloud into tensors. A two-stack encoder-decoder architecture is employed with 36 channels for each CloudLSTM cell. In this study, a two-stack encoder-decoder architecture with 36 channels for each CloudLSTM cell is utilized. Additionally, the integration of DConv into vanilla RNN and Convolutional GRU results in new models called Convolutional Point-cloud RNN (CloudRNN) and Convolutional Point-cloud GRU (CloudGRU). These models share a similar Seq2seq architecture with CloudLSTM. The study integrates DConv into vanilla RNN and Convolutional GRU, creating Convolutional Point-cloud RNN (CloudRNN) and Convolutional Point-cloud GRU (CloudGRU). These models, similar to CloudLSTM, do not use the attention mechanism. Performance comparison is conducted using traffic and air quality datasets. CloudLSTM is used to forecast future demands and air quality in target regions. The study utilizes CloudLSTM to forecast future mobile service demands and air quality indicators in target regions, comparing it with 12 baseline deep learning models. All models are implemented using TensorFlow and TensorLayer, trained on a computing cluster with NVIDIA Tesla K40M GPUs, and optimized by minimizing mean square error. The study uses TensorFlow and TensorLayer libraries to train deep learning models on a computing cluster with NVIDIA Tesla K40M GPUs. The models are optimized using the Adam optimizer and compared with baseline models. Experiments are conducted on spatiotemporal point-cloud stream forecasting tasks in 2D geospatial environments. The study reports experimental results on spatiotemporal point-cloud stream forecasting tasks in 2D geospatial environments. Experiments were conducted on two scenarios for each use case, omitting coordinate features in the final output. Mobile Traffic Forecasting was done using large-scale multi-service datasets from European metropolitan areas over 85 days. The study experimented with large-scale multi-service datasets from European metropolitan areas over 85 days. The data included traffic volume from devices associated with antennas in urban regions, expressed in Megabytes and aggregated over 5-minute intervals. Antennas were non-uniformly distributed, forming 2D point clouds. The study analyzed large-scale multi-service datasets from European metropolitan areas over 85 days. Traffic volume data from antennas in urban regions was collected in Megabytes at 5-minute intervals, resulting in 24,482 traffic snapshots for 38 different mobile services. The dataset included popular apps for video streaming, gaming, messaging, cloud services, and social networking. Additionally, air quality forecasting performance was evaluated using a public dataset with six air quality indicators collected by 437 air quality monitoring stations. Air quality forecasting performance is investigated using a public dataset from China, comprising six air quality indicators collected by 437 monitoring stations over one year. The dataset includes 8,760 snapshots for each of the two city clusters, with data measured hourly and missing values filled using linear interpolation. The dataset includes 8,760 hourly snapshots for each city cluster. Experiments are conducted on both clusters individually, with missing data filled using linear interpolation. Measurements for mobile service and air quality indicators are transformed into input channels of the point-cloud S, with coordinate features normalized to the (0, 1) range. Point-clouds are transformed into grids for baseline models using the Hungarian algorithm. The dataset includes hourly snapshots for each city cluster, with measurements transformed into input channels of the point-cloud S. Point-clouds are normalized to the (0, 1) range and transformed into grids for baseline models using the Hungarian algorithm. CloudLSTM is compared with baseline models like PointCNN and CloudCNN for feature extraction from point clouds. CloudCNN and PointLSTM are original benchmarks introduced for feature extraction from point clouds. CloudLSTM is compared with baseline models like PointCNN and CloudCNN. Other models such as CloudRNN and CloudGRU are also compared. In addition to CloudCNN and PointLSTM, CloudLSTM is compared with baseline models like PointCNN and CloudCNN. Other variations such as CloudRNN and CloudGRU are also evaluated. Various baseline models including MLP, CNN, 3D-CNN, LSTM, ConvLSTM, and PredRNN++ are discussed in detail in the appendix. The accuracy of CloudLSTM is measured using Mean Absolute Error (MAE) and Root Mean Square Error (RMSE). Mobile traffic snapshots are likened to \"urban images,\" and Peak Signal-to-Noise Ratio (PSNR) and Structural metrics are also considered. The proposed CloudLSTM is evaluated for accuracy using Mean Absolute Error (MAE) and Root Mean Square Error (RMSE). Mobile traffic snapshots are compared to \"urban images,\" and Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM) are used to measure forecast fidelity and similarity with the ground truth. Neural networks are employed to predict city-scale mobile traffic consumption over a 30-minute time horizon based on six consecutive past measurements. For the mobile traffic prediction task, neural networks are used to forecast city-scale mobile traffic consumption over 30 minutes based on six past measurements. RNN-based models are then evaluated for long-term performance over 3 hours. In the air quality forecasting use case, models receive 12 measurements as input and forecast indicators for the following 12 hours. In the air quality forecasting use case, models receive 12 measurements as input and forecast indicators for the following 12 hours. RNN-based models extend prediction steps to 3 days and show superior performance compared to CNN-based architectures. The study also explores the impact of neighboring points and the attention mechanism on forecasting accuracy. The study compares different architectures for air quality forecasting, including RNN-based models like CloudLSTM, CloudRNN, and CloudGRU, which outperform CNN-based models and MLP. The CloudLSTM model performs the best among the proposed approaches. The study compares different architectures for air quality forecasting, with CloudLSTM outperforming CloudRNN and CloudGRU. CloudLSTM shows insensitivity to the number of neighbors, suggesting the use of a small K to reduce complexity. The attention mechanism improves forecasting performance by capturing better dependencies between input sequences and vectors in decoders. In practice, using a small K reduces model complexity. The attention mechanism improves forecasting performance by capturing dependencies between input sequences and vectors in decoders. Long-term forecasting performance is evaluated up to 36 time steps for RNN-based architectures, showing MAE evolution in city 1. In city 1, the MAE does not significantly increase with prediction steps for most RNN-based models, indicating reliability in long-term forecasting. For city 2, low K may impact CloudLSTM's long-term performance before step 20. Results of 12-step air quality forecasting on six indicators using all models are shown in Table 2. In city 1, RNN-based models show reliable long-term forecasting with no significant increase in MAE. CloudLSTM's performance may be impacted by low K before step 20. Results of 12-step air quality forecasting on six indicators show CloudLSTMs outperform ConvLSTM by up to 12.2% and 8.8% in MAE and RMSE, respectively. CloudCNN consistently outperforms PointCNN as a better feature extractor. The CloudLSTM models demonstrate superior performance in modeling spatiotemporal point-cloud stream data, outperforming PointCNN. Lower K values result in better prediction performance, with CloudCNN consistently proving to be a better feature extractor. Performance evaluations for long-term forecasting up to 72 future time steps are presented in Appendix I. Performance evaluations of long-term forecasting up to 72 future time steps are conducted on RNN-based models using strict variable-controlling methodology. Comparing different models like LSTM, ConvLSTM, PredRNN++, PointLSTM, and CloudLSTM, it is evident that D-Conv significantly improves performance. CloudRNN and CloudGRU are also compared in the study. CloudLSTM, a dedicated neural model for spatiotemporal forecasting tailored to pointcloud data streams, utilizes D-Conv for performance improvements. Comparing CloudRNN, CloudGRU, and CloudLSTM, it is observed that CloudRNN and CloudGRU are inferior to CloudLSTM. Additionally, the attention mechanism in Attention CloudLSTM does not show significant effects. The core operator, RNN structure, and attention mechanism are ranked in terms of their contribution to the model. CloudLSTM, a neural model for spatiotemporal forecasting using DConv, adapts to changing spatial correlations in point-cloud data streams. DConv can be combined with various RNN models and attention mechanisms for flexibility. DConv predicts values and coordinates of each point, adapting to changing spatial correlations. It can be combined with various RNN models, Seq2seq learning, and attention mechanisms. DConv is efficiently implemented using a standard 2D convolution operator. The input and output are 3D tensors with shape (N, (H + L), U in ) and (N, (H + L), U out ). Top K nearest neighbors are found for each point in the input, transforming it into a 4D tensor. DConv efficiently implements spatial correlations using a 2D convolution operator. The input and output tensors have shapes (N, (H + L), U in ) and (N, (H + L), U out ). Top K nearest neighbors are found for each point in the input, transforming it into a 4D tensor. The DConv operation involves steps like reshaping the output tensor and applying the sigmoid function to the coordinates feature. DConv efficiently implements spatial correlations using a 2D convolution operator. The operation involves finding neighboring sets for each point and performing weighting computations. The complexity of each step is discussed separately. DConv efficiently implements spatial correlations using a 2D convolution operator by finding neighboring sets for each point and performing weighting computations. The complexity of each step is discussed separately, with step (i) involving finding K nearest neighbors and step (ii) computing features of the output points. The complexity of computing features of the output points in DConv is O((H + L) \u00b7 K), similar to a vanilla convolution operator. DConv introduces extra complexity by searching the K nearest neighbors for each point, O(K \u00b7 L log N), which does not increase much even with higher dimensions. The DConv operator introduces extra complexity by searching for the K nearest neighbors for each point, but this complexity does not increase significantly with higher dimensional point clouds. Normalizing the coordinates features enables transformation invariance with shifting and scaling, making the model invariant to these transformations. The proposed model combines CloudLSTM with an attention mechanism. The context tensor for the encoder state i is represented by a score function e i,j. The model was compared against baseline models MLP and CNN. The proposed model combines CloudLSTM with an attention mechanism. The encoder state i tensor is represented by a score function e i,j = v. Baseline models MLP, CNN, 3D-CNN, DefCNN, LSTM, and ConvLSTM are compared in mobile traffic forecasting. The Convolutional filters shape is similar to the DConv operator. LSTM is commonly used for time series forecasting. PredRNN++ is the top model for spatiotemporal forecasting on grid-structural data. CloudRNN and CloudGRU share a Seq2seq architecture with CloudLSTM but without the attention mechanism. The CloudRNN and CloudGRU models achieve high performance without employing the attention mechanism. Different models considered in the study have varying configurations and parameters. Using 3x3 filters in image applications has been effective, providing a receptive field of 9. In the study, ConvLSTM, PredRNN++, and PointLSTM did not benefit from increasing layers. 3x3 filters are commonly used in image applications with proven effectiveness, yielding a receptive field of 9. PredRNN++ has a unique structure compared to other Seq2seq models. Different configurations of 2-stack Seq2seq CloudLSTM with varying K values were tested. In the study, different configurations of 2-stack Seq2seq CloudLSTM with varying K values were tested for mobile traffic volume forecast and air quality indicator prediction. Performance evaluation was done using MAE, RMSE, PSNR, and SSIM metrics. The study evaluated the performance of models for mobile traffic volume forecast and air quality indicator prediction using MAE, RMSE, PSNR, and SSIM metrics. PSNR is defined as 20 log v max (t) \u2212 10 log 1 where \u00b5 v (t) and v max (t) are the average and maximum traffic recorded for all services/quality indicators. Coefficients c 1 and c 2 are used to stabilize the fraction in the presence of weak denominators. The study evaluated models for mobile traffic volume forecast and air quality prediction using metrics like MAE, RMSE, PSNR, and SSIM. Coefficients c1 and c2 stabilize fractions with weak denominators. Antenna locations in cities are anonymized for data collection via deep packet inspection at the packet gateway. Traffic classifiers associate flows with services while maintaining data confidentiality. The study utilized deep packet inspection at the packet gateway to evaluate mobile traffic volume forecast models and air quality prediction. Antenna locations in cities were anonymized for data collection, and traffic classifiers associated flows with services while maintaining data confidentiality. All measurements were conducted under the supervision of the national privacy agency and in compliance with regulations. The dataset used for the study is fully anonymized and does not contain personal information about individual subscribers. Due to a confidentiality agreement, the raw data cannot be made public. The set of services considered in the analysis comprises 38. The dataset used is fully anonymized, and due to a confidentiality agreement, the raw data cannot be shared. The analysis includes 38 different services, with streaming being the dominant type of traffic. The measurement campaign data in Fig. 6 shows streaming as the dominant type of traffic, accounting for almost half of total consumption. Other significant traffic sources include web, cloud, social media, and chat services, while gaming only contributes 0.5% of the demand. The air quality dataset includes information from 43 cities in China, with a total of 2,891,393 air quality records. The air quality dataset contains information from 43 cities in China, with 2,891,393 air quality records from 437 monitoring stations. The stations are divided into two clusters based on geographic locations, with missing data filled through linear interpolation. The dataset can be accessed at https://www.microsoft.com/en-us/research/project/urban-air/. Cluster A has 274 stations, Cluster B has 163. Missing data filled through linear interpolation. Dataset available at https://www.microsoft.com/en-us/research/project/urban-air/. Evaluation of Attention CloudLSTMs forecasting accuracy shown in Fig. 8. Similar performance in both cities at service and category level. Services with higher traffic volume analyzed jointly with Fig. 6. The MAE evaluation on service and category basis in Fig. 8 shows that CloudLSTMs achieve similar performance in both cities. Services with higher traffic volume, like streaming and cloud, result in higher prediction errors due to frequent fluctuations. Figure 9 displays the MAE evolution for RNN-based models in air quality forecasting on both city clusters for long-term prediction. The MAE evolution for RNN-based models in air quality forecasting on city clusters shows that the error grows with time for all models. Larger K in CloudLSTM improves robustness, with slower MAE growth when K = 9. In evaluating the mobile traffic forecasting task, larger K in CloudLSTM enhances robustness, with slower MAE growth when K = 9. Visualization of hidden features provides insights into the model's learned knowledge. Scatter distributions of hidden states in H t of CloudLSTM and Attention CloudLSTM are shown in Fig. 10, along with input snapshots. In Fig. 10, scatter distributions of hidden states in H t of CloudLSTM and Attention CloudLSTM are shown for encoders and decoders, with input snapshots from City 2. Each scatter subplot represents 1 value feature and 2 coordinate features for each point. In Fig. 10, scatter distributions of hidden states in H t of CloudLSTM and Attention CloudLSTM are shown for encoders and decoders, with input snapshots from City 2. Each scatter subplot in Fig. 10 displays the Attention Cloud-LSTM's superior prediction performance in NO2 forecasting examples in City Cluster A compared to other RNN-based models. Figures 11 and 12 illustrate NO2 forecasting examples in both city clusters A and B, showcasing the effectiveness of Attention CloudLSTMs in air quality prediction. The performance of RNN-based models for air quality prediction is compared visually, with Attention CloudLSTMs showing better long-term fidelity in capturing trends in point-cloud streams. NO2 forecasting examples in Cluster B are generated by these models, with DConv using Sigmoid functions for feature regularization. The DConv in Cluster B uses Sigmoid functions to regularize coordinate features, allowing points to move closer together for better computation. CloudLSTM, with stacked DConv via LSTM, refines positions of input points at each time step for improved representability. This model helps each point move to its optimal position. The CloudLSTM model, with stacked DConv via LSTM, enhances representability by refining the positions of input points at each time step. Outlier points are identified using DBSCAN, showing improved forecasting performance. The CloudLSTM model uses DBSCAN to identify outliers in the air quality dataset, achieving the lowest prediction error compared to other models. The CloudCNN with DConv operator shows the best forecasting performance among CNN-based models. The CloudCNN model, utilizing the DConv operator, demonstrates superior forecasting performance compared to other CNN-based models. Further experiments on the robustness of CloudLSTM to outliers involve creating a toy dataset with randomly selected weather stations, including outliers moved away from the center by varying distances on both axes. In a study on the robustness of CloudLSTM to outliers, 10 outliers were randomly selected and moved away from the center by different distances on both axes. The positions of the remaining 40 weather stations were unchanged. The performance of CloudLSTM and PointLSTM was compared after retraining under the same settings, showing that CloudLSTM performed almost equally well. The study tested the robustness of CloudLSTM to outliers by moving 10 outliers away from the center by different distances. CloudLSTM performed well in forecasting over inliers and outliers, outperforming PointLSTM. The model was also compared to simple baselines using MLPs for forecasting based on knearest neighbors. Our CloudLSTM model is robust to outliers, outperforming PointLSTM. Comparisons with simple baselines using MLPs and LSTMs show that CloudLSTM significantly outperforms them in forecasting based on k-nearest neighbors. Our CloudLSTM model significantly outperforms MLPs and LSTMs in forecasting based on k-nearest neighbors. The number of neighbors K affects the model's receptive field, with a small K relying on local spatial dependencies and a large K looking at larger location spaces. The results suggest that K does not significantly affect the baseline performance. The CloudLSTM model outperforms MLPs and LSTMs in forecasting by incorporating local and global spatial dependencies. Seasonal information in mobile traffic data can further enhance forecasting performance, but directly using data spanning multiple days is impractical. The seasonal information in mobile traffic data can improve forecasting performance, but directly inputting data spanning multiple days is impractical due to the large number of data points it would create. To efficiently capture seasonal information in mobile traffic data, the input is concatenated with 30-minute sequences sampled every 5 minutes and a sub-sampled 7-day window sampled every 2 hours. This creates an input length of 90. Experiments conducted on a subset of mobile traffic data show improved forecasting performance when incorporating seasonal information. Experiments on a subset of mobile traffic data demonstrate improved forecasting performance by incorporating seasonal information with a 7-day window. The seasonal information helps reduce prediction errors, although it increases model complexity. Future work will focus on a more efficient way to fuse seasonal information with minimal complexity increase."
}