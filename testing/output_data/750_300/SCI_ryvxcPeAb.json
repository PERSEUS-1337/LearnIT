{
    "title": "ryvxcPeAb",
    "content": "Deep neural networks excel in various applications but are susceptible to adversarial examples, which are small perturbations that can deceive different models. This vulnerability allows attackers to exploit black-box systems. In this work, adversarial perturbations are decomposed into model-specific and data-dependent components, with the latter contributing significantly to transferability. A new approach using noise reduced gradient (NRG) to craft adversarial examples enhances transferability across various ImageNet models. Low-capacity models exhibit stronger attack capability compared to high-capacity models with similar test performance. The new approach enhances transferability significantly, showing that low-capacity models are more effective in attacks compared to high-capacity models with similar test performance. This insight can guide the construction of successful adversarial examples and aid in designing defense strategies against black-box attacks in neural networks applied in real-world applications. Recent works have shown that adversaries can manipulate inputs to neural network models, creating adversarial examples that produce incorrect outputs. Understanding this phenomenon and effectively defending against adversarial examples remain open questions. Additionally, it has been found that adversarial examples can transfer across different models. Adversarial examples can transfer across different models, allowing them to fool black-box systems. The vulnerability to adversarial attacks is still a challenge, with the generation of adversarial examples modeled as an optimization problem. The presence of adversarial vulnerability in black-box systems was first studied and modeled as an optimization problem. Different methods like FGSM and DeepFool have been proposed to address adversarial instability caused by the linear nature and high dimensionality of deep neural networks. Various methods have been proposed to address adversarial vulnerability in deep neural networks, such as the fast gradient sign method (FGSM) and the DeepFool method. While some approaches focus on crafting adversarial examples for attacks, others explore more effective defense strategies. Transferability of adversarial examples and the difference between white-box and black-box attacks have also been analyzed in detail. In BID3, high-confidence adversarial examples were shown to have stronger transferability in black-box attacks. Various defense strategies have been proposed, including defensive distillation by BID12 and adversarial training methods introduced by BID5, examined on ImageNet by BID6 and BID16. BID9 used image transformation to mitigate adversarial perturbations, while some works focused on detecting adversarial examples for manual processing (BID7, BID4). In this work, the transferability of adversarial examples is explained, leading to enhanced black-box attacks. Adversarial perturbation is decomposed into model-specific and data-dependent components, with the former coming from the model architecture. The transferability of adversarial examples is enhanced by understanding the model-specific and data-dependent components of adversarial perturbations. The data-dependent component plays a crucial role in the transferability across different models, leading to the proposal of constructing adversarial examples using this component. The transferability of adversarial perturbations is mainly influenced by the data-dependent component. A new method called noise-reduced gradient (NRG) is proposed to construct adversarial examples by utilizing this component. The NRG method, combined with other techniques, significantly improves the success rate of black-box attacks on the ImageNet validation set. The proposed noise-reduced gradient, when combined with other methods, boosts the success rate of black-box attacks on the ImageNet validation set. Model-specific factors like capacity and accuracy influence the success rate, with higher accuracy and lower capacity models showing better attack capability on unseen models. This can be attributed to transferability and provides insights for attacking unseen models. The high dimensionality of the model function makes it vulnerable to adversarial perturbations, with small imperceptible changes that can be exploited for attacks on unseen models. The high dimensionality of the model function makes it vulnerable to adversarial perturbations, with small imperceptible changes that can be exploited for attacks on unseen models. Adversarial examples exist in deep neural networks and other models like support vector machines and decision trees. These attacks are non-targeted, meaning the adversary has no control over the output other than requiring a specific input. Adversarial examples can affect various models like support vector machines and decision trees, not just deep neural networks. These attacks can be non-targeted, where the adversary aims to misclassify the input without controlling the output. In a black-box attack, the adversary has no knowledge of the target model and cannot query it. In a black-box attack, the adversary has no knowledge of the target model and cannot query it. They can construct adversarial examples on a local model trained on a similar dataset and deploy them to fool the target model. This type of attack is different from a white-box attack, where the target is the source model itself. Crafting adversarial perturbations for black-box attacks involves deploying adversarial examples to fool the target model. The optimization problem includes a loss function J measuring prediction discrepancy and a perturbation magnitude metric. Image data perturbations must adhere to the constraint x adv \u2208 [0, 255] d. BID2 introduced a loss function manipulating output logit directly. The perturbation magnitude for adversarial examples is crucial, with constraints on image data perturbations. BID2 introduced a loss function manipulating output logit directly, commonly used in works. Distortion measurement is challenging, with human eyes being the ideal metric. Ensemble-based approaches, using multiple source models, are suggested to enhance adversarial example strength. Ensemble-based approaches suggest using a large ensemble of source models to improve adversarial example strength. The objective involves averaging predicted probabilities of each model, with various optimizers available for solving the problem. The objective involves using ensemble weights to improve adversarial example strength. Various optimizers are available for solving the problem, with a focus on the normalized-gradient based optimizer. The Fast Gradient Based Method attempts to solve the problem by performing one step iteration using normalized gradient vectors. This method is empirically shown to be fast and has good transferability, although not optimal. The Iterative Gradient Method is a simple yet effective optimizer that performs normalized-gradient ascent for k steps. It uses a projection operator to enforce constraints and a step size alpha. The normalized gradient g(x) is chosen based on the type of attack being used. The Iterative Gradient Method performs normalized-gradient ascent for k steps using a projection operator and step size alpha. Different types of attacks determine the choice of normalized gradient. Transferability of adversarial examples between models is crucial for black-box attacks and defenses. Only a few articles have explored this topic. The transferability of adversarial examples between models is crucial for black-box attacks and defenses. Previous works suggest that transferability comes from the similarity between decision boundaries of source and target models, especially in the direction of transferable adversarial examples. Investigating the similarity between different models A and B that enables transferability of adversarial examples across them is essential. Transferable adversarial examples span a contiguous subspace, and the similarity between models A and B enables transferability across them. Models A and B have learned a similar function on the data manifold, but their behavior off the data manifold can differ due to architectural differences and random initializations. Perturbations can be decomposed into data-dependent and data-independent factors. The data manifold's characteristics are influenced by model architectures and random initializations. Perturbations can be broken down into data-dependent and model-specific components. The data-dependent component contributes more to transferability between models A and B, as it captures shared information on the data manifold. Model-specific components have varying behaviors off the data manifold. The model-specific component contributes little to transferability between models due to its different behaviors off the data manifold. Adversarial perturbation crafted from model A can mislead both model A and B, decomposed into a data-dependent component and a model-specific one. The boundaries of two models are similar in the inter-class area. Adversarial perturbation from model A can mislead both model A and B. The perturbation is decomposed into a data-dependent component and a model-specific one. The data-dependent component can easily attack model B, while the model-specific component contributes little to transferability between models. The NRG method aims to enhance black-box adversarial attacks by reducing model-specific noise, as shown in the decision boundaries of resnet34 and densenet121 for ImageNet. The data-dependent component plays a crucial role in increasing attack success rates. The NRG method aims to reduce model-specific noise in black-box adversarial attacks by applying local averaging to remove noisy information, resulting in a noise-reduced gradient (NRG) that captures more data-dependent information than the ordinary gradient. The NRG method reduces model-specific noise in black-box adversarial attacks by applying local averaging to yield an approximation of the data-dependent component. The noise-reduced gradient (NRG) captures more data-dependent information than the ordinary gradient, helping to avoid overfitting to specific source models. The Noise-reduced Iterative Sign Gradient Method (nr-IGSM) uses \u2207f in Eq.(8) to drive the optimizer towards data-dependent solutions, reducing model-specific noise in black-box attacks. The special case nr-FGSM is a noise-reduced fast gradient sign method. The Noise-reduced Iterative Sign Gradient Method (nr-IGSM) is a method mounted by NRG that uses \u2207f in Eq.(8) to drive the optimizer towards data-dependent solutions, reducing model-specific noise in black-box attacks. The special case nr-FGSM is a noise-reduced fast gradient sign method. The noise-reduced version for q-attack replaces the second equation of (9) with a different equation. The effectiveness of NRG for enhancing transferability is analyzed using classification models trained on the ImageNet dataset. The dataset used is the ImageNet ILSVRC2012 validation set containing 50,000 samples. To analyze the effectiveness of NRG for enhancing transferability, classification models trained on the ImageNet dataset are used. The ImageNet ILSVRC2012 validation set with 50,000 samples is utilized for the experiments. Pre-trained models provided by PyTorch, including resnet and vgg models, are used for the experiments. For targeted attack experiments, various pre-trained models from PyTorch are used, including resnet, vgg, densenet, alexnet, and squeezenet. Different models are chosen for specific experiments to save computational time. Adversarial examples are generated using a model F, and the effectiveness of NRG for enhancing transferability is analyzed using the ImageNet dataset. For targeted attack experiments, various pre-trained models like resnet, vgg, densenet, alexnet, and squeezenet are used. Adversarial examples are generated using model F, and the Top-1 success rate is evaluated for targeted attacks. The cross entropy 2 is chosen as the loss function for measuring distortion. The targeted attack performance is evaluated using Top-1 success rate, with Top-5 rates computed similarly. Distortion is measured using \u221e norm and scaled 2 norm, with FGSM and IGSM optimizers considered. The effectiveness of noise-reduced gradient technique is demonstrated by combining it with common methods like fast gradient based method. In this section, the effectiveness of noise-reduced gradient technique is demonstrated by combining it with commonly-used methods like FGSM and IGSM. The results show that nr-FGSM consistently outperforms original FGSM in both blackbox and white-box attacks, indicating the superiority of noise-reduced gradient direction. The noise-reduced gradient technique, specifically nr-FGSM, outperforms the original FGSM in both blackbox and white-box attacks. Even with the increased computational cost, nr-IGSM generates more effective adversarial examples. The noise-reduced gradient technique, nr-IGSM, generates more effective adversarial examples compared to IGSM. Large models like resnet152 are more robust to adversarial transfer, as shown in Table 2. Large models like resnet152 are more robust to adversarial transfer than small models. The transfer among models with similar architectures is influenced by model-specific components. IGSM generally generates stronger adversarial examples than FGSM, except in attacks against alexnet. This contradicts previous claims but aligns with other research findings. The choice of hyperparameters in generating adversarial examples can impact their transferability between models. Adversarial examples with higher confidence in the source model are more likely to transfer to the target model. The differences in model architectures, such as alexnet, can affect the success of attacks. The inappropriate choice of hyperparameters in BID6, such as \u03b1 = 1 and k = min(\u03b5 + 4, 1.24\u03b5), led to underfitting. The alexnet model differs significantly from the source models, causing IGSM to overfit more than FGSM. This indicates a lack of trust in the objective function (Eq. 2) and poor transferability. Our noise reduced gradient technique helps regularize the optimizer by removing noise. Our noise reduced gradient technique improves cross-model generalization capability by removing model-specific information from gradients, leading to better convergence. NRG is applied to ensemble-based approaches with a reduced evaluation set of 1,000 images for efficiency. Both FGSM and IGSM are used for non-targeted attacks. In this study, the NRG method is applied to ensemble-based approaches with a reduced evaluation set of 1,000 images for efficiency. Both FGSM and IGSM attacks are tested for non-targeted attacks, with the Top-1 success rates of IGSM attacks nearly saturated. For targeted attacks, generating adversarial examples predicted by unseen target models is more challenging than non-targeted examples. Generating targeted adversarial examples is much harder than non-targeted examples, as demonstrated by BID8. Different from non-targeted attacks, targeted examples are sensitive to the step size \u03b1 in optimization procedures. A large step size is necessary for generating strong targeted attacks. Targeted adversarial examples are sensitive to the step size \u03b1, requiring a larger step size for strong attacks. NRG methods outperform normal methods for both targeted and non-targeted attacks. The NRG methods show significantly higher success rates compared to normal methods for both targeted and non-targeted attacks, with a much larger step size needed for strong attacks. Top-5 success rates of ensemble-based approaches are reported in Table 3, showcasing the superiority of NRG methods. In this section, the sensitivity of hyper parameters m and \u03c3 in NRG methods for black-box attacks is explored using the nr-FGSM approach. Four attacks are considered, with larger m leading to higher fooling rates for any distortion level \u03b5. The results are shown in FIG6. The results of the evaluation set show that larger values of m lead to higher fooling rates for any distortion level \u03b5. An optimal value of \u03c3 is crucial for best performance, with overly large or small values affecting the bias and effectiveness of removing noisy model-specific information. The optimal \u03c3 varies for different source models, with values around 15 for resnet18 and 20 for others. In this experiment, the optimal \u03c3 for removing noisy model-specific information varies for different source models, being around 15 for resnet18 and 20 for densenet161. The robustness of adversarial perturbations to image transformations is explored, which is crucial for real-world applications. The influence of transformations is quantified using the notion of destruction rate. The destruction rate is used to quantify the influence of image transformations on adversarial examples. Densenet121 and resnet34 are the source and target models, with four image transformations considered. The above rate R describes the fraction of adversarial images no longer misclassified after transformation T. Adversarial examples generated by NRG methods are more robust than vanilla methods. Decision boundaries of different models are studied to understand NRG-based methods' performance. Resnet34 is the source model with nine target models considered. In this section, decision boundaries of different models are studied to understand why NRG-based methods perform better. Resnet34 is the source model, and nine target models are considered. The \u2207f is estimated with m = 1000, \u03c3 = 15. Each point in the 2-D plane corresponds to an image perturbed along sign \u2207f and sign (\u2207f \u22a5 ). The decision boundaries of different models are studied to understand the performance of NRG-based methods. Resnet34 is the source model, and various target models are examined. Each point in a 2-D plane corresponds to an image perturbed along sign \u2207f and sign (\u2207f \u22a5 ). The sensitivity of different models to these perturbations is analyzed, showing that sign \u2207f is as sensitive as sign (\u2207f \u22a5 ) for resnet34, while other target models are more sensitive along sign \u2207f. The sensitivity of different target models to perturbations along sign \u2207f and sign (\u2207f \u22a5 ) is analyzed. While sign \u2207f is as sensitive as sign (\u2207f \u22a5 ) for the source model resnet34, other target models show greater sensitivity along sign \u2207f. Removing sign (\u2207f \u22a5 ) from gradients penalizes the optimizer along the model-specific direction, preventing convergence to a source model-overfitting solution. The minimal distance u required to produce adversarial transfer varies for different models, with complex models requiring significantly larger distances than smaller models. The minimal distance u needed for adversarial transfer varies among different models, with complex models requiring larger distances than smaller models. Adversarial examples crafted from alexnet generalize poorly across models, while attacks from densenet121 consistently perform well. Different models exhibit varying performances in attacking the same target model. Adversarial examples crafted from alexnet generalize poorly across models, while attacks from densenet121 consistently perform well. For example, 84.3% of nr-IGSM adversarial examples can transfer to vgg19 bn, indicating the importance of choosing a better local model to generate adversarial examples for attacking remote models. Different models can exhibit varying performances in attacking the same target model. To choose a better local model for generating adversarial examples to attack remote black-box systems, different models like vgg19 bn and resnet152 were selected for FGSM and IGSM attacks. Results show that models with powerful attack capability are concentrated in the bottom left corner of the Top-1 test error versus model capacity graph. The results in FIG11 show that models with strong attack capability are concentrated in the bottom left corner, with lower test error and model capacity. Smaller test error and lower capacity indicate stronger attack capability, explained in Section 3.1 in terms of transferability. The model's attack capability is stronger with lower capacity and smaller test error. This is explained by transferability in Section 3.1. A less complex model allows the data-dependent factor to dominate, leading to strong adversarial examples. The number of parameters and top-1 error are shown in the table, excluding vgg-style models. In this study, it was found that models with small gradient norms and large gradient norms can generate strong adversarial examples that transfer easily. The contribution of architecture similarities, specifically vgg-style models, was not considered. Adversarial perturbations were decomposed into model-specific and data-dependent components, with the latter being the main contributor to transferability. The study found that adversarial perturbations can be decomposed into model-specific and data-dependent components, with the latter being the main contributor to transferability. The proposed noise-reduced gradient (NRG) based methods for crafting adversarial examples are more effective than previous methods. Models with lower capacity and higher test accuracy are better equipped for black-box attacks. Future work will explore combining NRG-based methods with adversarial training to defend against black-box attacks. Models with lower capacity and higher test accuracy are more capable of black-box attacks. Future research will focus on combining NRG-based methods with adversarial training for defense. The data-dependent component contributing to transferability is low-dimensional, suggesting defensibility against black-box attacks. White-box attacks, originating from high-dimensional space, pose greater defense challenges. Further research will explore stable feature learning for transfer learning with NRG strategy integration. Future research will focus on learning stable features for transfer learning by incorporating the NRG strategy to reduce model-specific noise. The influence of hyperparameters in IGSM for targeted black-box attacks, such as the number of iterations and step size, is explored for generating adversarial examples. The success rates are evaluated on 1,000 randomly selected images using ResNet152 and VGG16 BN as target models. In exploring the influence of hyperparameters in IGSM for targeted black-box attacks, the success rates are evaluated on 1,000 images using ResNet152 and VGG16 BN as target models. The optimal step size \u03b1 is crucial, with very large or small values harming attack performance. With a small step size \u03b1 = 5, a large number of iterations can result in worse performance than a small number. In an experiment on the influence of hyperparameters in IGSM for targeted black-box attacks, it was found that both too large and too small step sizes can harm attack performance. A small step size with a large number of iterations may lead to worse performance, possibly due to overfitting. Conversely, a large step size can prevent overfitting and encourage exploration of more model-independent areas, resulting in better performance. Additional experiments on the MNIST dataset were conducted to confirm the impact of model redundancy on attack capability. The experiment on the influence of model redundancy on attack capability in targeted black-box attacks showed that smaller models have stronger attack capability compared to larger models. The experiment showed that smaller models have stronger attack capability than larger models. Top-1 success rates of cross-model attacks were reported, demonstrating the low-capacity model's superior attack capability."
}