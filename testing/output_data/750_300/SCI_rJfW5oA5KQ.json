{
    "title": "rJfW5oA5KQ",
    "content": "Generative Adversarial Networks (GANs) have shown impressive results in learning complex real-world distributions, but recent works have highlighted issues such as lack of diversity or mode collapse. Theoretical work by Arora et al. (2017a) suggests a dilemma regarding GANs' statistical properties: powerful discriminators lead to overfitting, while weak discriminators may result in mode collapse. In contrast to previous works on GANs, this paper demonstrates that GANs can learn distributions in Wasserstein distance with polynomial sample complexity by using discriminators with strong distinguishing power against specific generator classes. Discriminators are designed for various generator classes to approximate Wasserstein distance and/or KL-divergence, such as mixture of Gaussians, exponential families, and neural network generators. This implies improved training outcomes. Generators are designed with corresponding discriminators, often neural nets, to approximate Wasserstein distance and/or KL-divergence. Successful training results in a distribution close to the true distribution, indicating that lack of diversity in GANs may be due to optimization sub-optimality rather than statistical inefficiency. Experiments show that the lack of diversity in GANs may be caused by sub-optimality in optimization rather than statistical inefficiency. Various ideas have been proposed to improve the quality and stability of training in Generative Adversarial Networks. Recent work has highlighted concerns that distributions learned by GANs may suffer from mode collapse or lack of diversity, missing significant modes of the target distribution. Various proposals aim to enhance the quality and stability of training in Generative Adversarial Networks. Recent work has shown that distributions learned by GANs can suffer from mode collapse or lack of diversity, missing significant modes of the target distribution. The paper suggests that designing proper discriminators with strong distinguishing power against specific families of generators, such as special subclasses of neural network generators, can alleviate mode collapse. The focus is mainly on the Wasserstein GAN (WGAN) formulation BID0. The paper focuses on the Wasserstein GAN (WGAN) formulation BID0 and introduces the F-Integral Probability Metric (F-IPM) for comparing distributions. WGAN sets up a family of generators and discriminators to learn the data distribution by solving a specific optimization problem. The paper introduces the F-Integral Probability Metric (F-IPM) for comparing distributions in the context of Wasserstein GAN (WGAN). It focuses on learning the data distribution by solving an optimization problem using a family of generators and discriminators. The use of parametric families like neural networks allows for empirical optimization of the objective. The paper discusses the issue of \"mode collapse\" in Generative Adversarial Networks (GANs) due to the weakness of the Integral Probability Metric (IPM) compared to the Wasserstein distance. This leads to the generation of high-quality but low-diversity examples. The weakness of IPM compared to W 1 leads to mode collapse in GANs. Increasing the discriminator strength to larger families like all 1-Lipschitz functions is a suggested solution. The weakness of IPM compared to W 1 leads to mode collapse in GANs. Increasing the discriminator strength to larger families like all 1-Lipschitz functions is a suggested solution. However, Arora et al. points out that Wasserstein-1 distance doesn't have good generalization properties. The lack of diversity in GANs is a dilemma: powerful discriminators lead to overfitting, while weak discriminators result in diversity issues as IPM fails to approximate the Wasserstein distance. This poses challenges in establishing GAN theories. This paper proposes a solution to the lack of diversity in GANs by designing a strong discriminator class F against a specific generator class G. The discriminator class F has restricted approximability with respect to the generator class G and the data distribution p, distinguishing them as well as all 1-Lipschitz functions can do. The paper focuses on designing a strong discriminator class F against a specific generator class G, with restricted approximability w.r.t. G and the data distribution p. It aims to distinguish p and any q \u2208 G approximately as well as all 1-Lipschitz functions can do, using specific monotone nonnegative functions. The paper focuses on designing a strong discriminator class F against a specific generator class G, with restricted approximability w.r.t. G and the data distribution p. It aims to approximate the Wasserstein distance W 1 for p and any q \u2208 G using specific monotone nonnegative functions. A discriminator class F with restricted approximability aims to avoid mode collapse by ensuring that if the IPM between p and q is small, then p and q are also close in Wasserstein distance, preventing significant mode-dropping. This allows for transitioning from population-level guarantees to empirical-level. The text discusses how a discriminator class F helps prevent mode collapse in Wasserstein GANs by ensuring that if the IPM between two distributions is small, they are also close in Wasserstein distance. This transition allows for moving from population-level guarantees to empirical-level guarantees. The text introduces a theoretical framework for analyzing the statistical properties of Wasserstein GANs with polynomial samples. It addresses the diversity and generalization properties of the distance W F, providing guarantees on the proximity of distributions in Wasserstein distance. The paper focuses on designing a discriminator class F with restricted approximability to prevent mode collapse. The paper introduces a theoretical framework for analyzing the statistical properties of Wasserstein GANs with polynomial samples. It focuses on designing a discriminator class F with restricted approximability to prevent mode collapse and provides diversity guarantees for various generator classes. In the context of analyzing Wasserstein GANs with polynomial samples, the paper discusses designing discriminators using neural networks to distinguish distributions like Gaussians and exponential families. It also explores distributions generated by invertible neural networks and the use of special neural network discriminators for this purpose. In Section 4, the paper explores distributions generated by invertible neural networks. Special neural network discriminators with one additional layer than the generator have restricted approximability, guaranteeing certain properties in the learned distribution. The paper discusses distributions generated by invertible neural networks, highlighting the limitations of the invertibility assumption in producing distributions supported on the entire space. It mentions the challenge of matching the support of estimated distributions with the support of natural image distributions residing on low-dimensional manifolds. The paper focuses on distributions generated by invertible neural networks and the challenge of matching supports with natural image distributions on low-dimensional manifolds. It highlights the limitations of using KL-divergence as a measurement of statistical distance when both distributions have low-dimensional supports. The technical part of the paper aims to approximate Wasserstein distance using IPMs for generators with low-dimensional supports. The paper demonstrates the advantage of GANs over MLE in learning distributions with low-dimensional supports by approximating Wasserstein distance using IPMs. It develops tools for approximating the log-density of a neural network generator and shows the correlation between IPM and Wasserstein distance in experiments. The theory suggests that the IPM could be an alternative for measuring diversity and quality of distributions when KL-divergence or Wasserstein distance is not measurable in complex settings. The test IPM could serve as an alternative for measuring diversity and quality of distributions in complex settings where KL-divergence or Wasserstein distance is not measurable.GANs often learn distributions that can be distinguished from the data distribution by a well-trained discriminator, indicating that the IPM may not be well-optimized. The lack of diversity in real experiments with GANs may be due to sub-optimality of the optimization process, rather than statistical inefficiency. Various empirical tests for diversity, memorization, and generalization have been developed, such as interpolation between images and semantic combination of images in latent space. Various empirical tests for diversity, memorization, and generalization have been developed for GANs, indicating that lack of diversity is a common issue. Arora et al. (2017a; b) formalized the potential theoretical sources of mode collapse and proposed a \"birthday paradox\" to demonstrate this phenomenon. Mode collapse in GANs is a common issue, with various proposed solutions but no provable general solution yet. Arora et al. (2017a; b) identified theoretical sources of mode collapse and introduced the \"birthday paradox\" to illustrate the phenomenon. Feizi et al. (2017) provided guarantees for training GANs with quadratic discriminators using Gaussian generators. Zhang et al. (2017) demonstrated that IPM is a proper metric under certain conditions and offers a KL-divergence bound with finite samples. This work extends the findings of Zhang et al. (2017) and focuses on developing statistical solutions. Our work extends the statistical guarantees in Wasserstein distance for distributions like injective neural network generators, which do not have proper density. Liang (2017) discusses GANs in a non-parametric setup, highlighting that the sample complexity improves with the smoothness of the generator family. The sample complexity for learning GANs improves with the smoothness of the generator family, but the rate derived is non-parametric unless the Fourier spectrum decays extremely fast. In Flow-GAN, the invertible generator structure was used to address issues with GAN training on real datasets. Successful GAN training implies learning in KL-divergence when the data distribution can. The Flow-GAN model uses an invertible generator structure to address issues with GAN training on real datasets. Successful GAN training implies learning in KL-divergence when the data distribution can be generated by an invertible neural net. This suggests that real data cannot be generated by an invertible neural network. Additionally, if the data can be generated by an injective neural network, the closeness between the learned distribution and the true distribution can be bounded in Wasserstein distance. The theory suggests that if data can be generated by an injective neural network, the closeness between the learned distribution and the true distribution can be bounded in Wasserstein distance, even though KL divergence is not informative in this case. IPM includes statistical distances like TV and Wasserstein-1 distance. F-IPM refers to the neural net IPM when F is a class of neural networks. There are other distances of interest between distributions that are not IPMs. The text discusses the use of TV and Wasserstein-1 distance in F-IPM, which refers to the neural net IPM when F is a class of neural networks. It also mentions other distances like KL divergence and Wasserstein-2 distance for comparing distributions. The Rademacher complexity of a function class is defined as the largest Rademacher complexity over a distribution set G. The training IPM loss for the Wasserstein GAN is governed by the quantity R n (F, G) for generalization. The Wasserstein GAN's dataset is analyzed using Eqn [W F (p n ,q n )]. Generalization is determined by R n (F, G) as per Theorem 2.1. Notations like N(\u00b5, \u03a3) for Gaussian distributions are used. Discriminators are designed for simple distributions like Gaussian. One-layer neural networks with ReLU activation can distinguish Gaussian distributions with restricted approximability guarantees, considering Gaussian distributions with bounded mean and well-conditioned covariance. The discriminators are designed for simple distributions like Gaussian. The set of one-layer neural networks with ReLU activation has restricted approximability w.r.t. Gaussian distributions with bounded mean and well-conditioned covariance. The discriminators are designed for simple distributions like Gaussian, with lower and upper bounds differing by a factor of 1/ \u221a d. The lower and upper bounds for Gaussian distributions in G differ by a factor of 1/ \u221a d. The 1/ \u221a d factor is not improvable without using more sophisticated functions than Lipschitz functions of one-dimensional projections of x. The maximum Wasserstein distance between one-dimensional projections of p, q is on the order of W 1 (p, q)/ \u221a d when p, q have spherical covariances. Extension to mixture of Gaussians and designing a discriminator family F with restricted approximability is also possible. The linear combinations of sufficient statistics in exponential families can serve as discriminators with restricted approximability. The proof for this is deferred to Appendix C. The exponential family G with linear discriminators F has restricted approximability. If the log partition function log Z(\u03b8) satisfies certain conditions, then for any X with diameter D and T(x) being L-Lipschitz in X, there are complexity bounds. The log partition function is always convex. The log partition function log Z(\u03b8) is always convex. The curvature of the Fisher information matrix must have a positive lower bound and a global upper bound for certain assumptions to hold. Geometric assumptions on the sufficient statistics are necessary for the bound eq. (8) due to the intrinsic dependence of the Wasserstein distance on the underlying geometry of x. In this section, discriminators with restricted approximability for neural net generators are designed for GANs. The proof of eq. FORMULA15 follows from the theory of exponential families, while the proof of eq. (8) is deferred to Section B.2. The focus is on designing generators, such as invertible neural networks, widely used in GANs to model real data. In Section 4.1, invertible neural networks generators with proper densities are considered. Section 4.2 extends the results to injective neural networks generators, allowing latent variables of lower dimension than observable dimensions (Theorem 4.5). Generators parameterized by invertible neural networks are discussed in this section. In this section, generators parameterized by invertible neural networks are discussed. The distributions no longer have densities, and variables can have lower dimensions than observable dimensions. The hidden factors' standard deviation allows for non-spherical variances, enabling different impacts on the output distribution. The model can represent data around a \"k-dimensional manifold\" with noise level \u03b4. By allowing non-spherical variances, hidden dimensions can have varying impacts on the output distribution. The model can represent data around a \"k-dimensional manifold\" with noise level \u03b4. The family of invertible neural networks G \u03b8 consists of standard -layer feedforward nets. Assuming invertible generators with parameters R W , R b , \u03ba \u03c3 , \u03b2 \u03c3 > 0, \u03b4 \u2208 (0, 1], we consider neural networks G \u03b8 parameterized by \u03b8 = (W i , b i ) i\u2208[ ] in set DISPLAYFORM2. The activation function \u03c3 is twice-differentiable and hidden factors' standard deviation \u03b3 i \u2208 [\u03b4, 1]. The neural net is invertible with its inverse being a feedforward net with activation \u03c3 \u22121. The standard deviation of hidden factors in neural networks satisfies \u03b3 i \u2208 [\u03b4, 1]. An invertible neural net with activation \u03c3 \u22121 can be achieved. Assumptions on generator networks are necessary to prevent pseudo-random functions. The function log p \u03b8 can be computed by a neural network with at most + 1 layers and O( d 2 ) parameters. The function log p \u03b8 can be computed by a neural network with at most + 1 layers, O( d 2 ) parameters, and specific activation functions. This family of neural networks contains all functions log p \u2212 log q for p, q \u2208 G. The exact form of the parameterized family F may not be crucial in practice. Other neural net families may also provide good approximations of log p \u2212 log q. The proof for computing log p \u03b8 involves a change-of-variable formula and the observation that G \u22121 \u03b8 is a feedforward neural net with layers. The log-det of the Jacobian requires computing the determinant of the weight matrices, which is non-trivial but ultimately a constant. The log-det of the Jacobian involves computing the determinant of the weight matrices, which is a constant not dependent on the input. This allows for adding a bias on the final output layer, freeing from structural assumptions on the weight matrices. The proof of Lemma 4.1 is deferred to Section D.2. The proof of Lemma 4.1 is deferred to Section D.2. Theorem 4.2 states that the discriminator class F has restricted approximability with respect to the set of invertible-generator distributions G. The proof of Theorem 4.2 uses Lemma 4.3, which relates the KL divergence to the IPM when the log densities exist and belong to the family of discriminators. Lemma 4.3 states that the KL divergence is related to the IPM when log densities exist in the discriminator family. It outlines a proof sketch for Theorem 4.2, which uses the discriminator class from Lemma 4.1. The proof involves lower bounding a quantity by the Wasserstein distance and upper bounding W F (p, q) by the Wasserstein distance. The discriminator class chosen in Lemma 4.1 implements log p \u2212 log q for any p, q \u2208 G. It suffices to lower bound this quantity by the Wasserstein distance and upper bound W F (p, q) by the Wasserstein distance. Transportation inequalities by Bobkov-G\u00f6tze and Gozlan are used to prove this result, stating that if X \u223c p (or q) and f is 1-Lipschitz, then f (X) is subGaussian. In the invertible generator case, X = G \u03b8 (Z) where Z are independent Gaussians, so as long as G \u03b8 is suitably Lipschitz, f (X) = f (G \u03b8 (Z)) is a. In the invertible generator case, X = G \u03b8 (Z) where Z are independent Gaussians. As long as G \u03b8 is suitably Lipschitz, f (X) = f (G \u03b8 (Z)) is a sub-Gaussian random variable. Two workarounds are provided to handle functions in F that are not globally Lipschitz. In the context of invertible generator models, functions in F are Lipschitz globally. Two workarounds are proposed to handle functions that are not globally Lipschitz, involving truncation arguments and W 2 bounds. Theorem D.2 extends previous results and shows that successful training results in estimated distributions close to true distributions in Wasserstein distance. Successful training in invertible generator models leads to estimated distributions close to true distributions in Wasserstein distance, with small expected IPM indicating proximity between estimated and true distributions. Efforts to minimize training error remain an open challenge for designing efficient algorithms. In this section, injective neural network generators are discussed, aiming to generate distributions on a low dimensional manifold for modeling real images. The challenge lies in the KL divergence becoming infinite, making existing methods ineffective. A novel divergence is proposed to address this issue. In this section, a novel divergence is proposed to address the challenge of generating distributions on a low dimensional manifold for modeling real images. The divergence is designed to approximate the Wasserstein distance and is optimized as IPM, making use of injective neural network generators. Our key idea is to design a variant of the IPM that approximates the Wasserstein distance for distributions generated by neural nets on a k-dimensional manifold in R^d. The smoothed F-IPM between p and q can be optimized as W_F with an additional variable \u03b2. Certain discriminator classes can approximate the Wasserstein distance for G distributions. Certain discriminator classes can approximate the Wasserstein distance for distributions generated by neural nets on a k-dimensional manifold in R^d. Theorem 4.5 states that for any pair of distributions p, q \u2208 G, if d(p^n, q^n) is small for n poly(d), then W(p, q) is also small, preventing mode collapse. Our theoretical results show that mode collapse can be prevented in neural network generators as long as the discriminator family has restricted approximability with respect to the generator family. The Wasserstein distance is upper and lower bounded by the IPM given this restricted approximability, ensuring that mode collapse does not occur. Certain specific discriminator classes are designed to guarantee this outcome. The IPM W F (p, q) is bounded by the Wasserstein distance W 1 (p, q) with restricted approximability in the generator family G. Specific discriminator classes are designed to prevent mode collapse, and experiments confirm the theory's consistency in GAN training. In experiments with synthetic datasets, GANs trained with a proposed discriminator class show correlation between IPM and Wasserstein / KL divergence. This suggests GAN training difficulty may stem from optimization challenges rather than statistical inefficiency. In experiments with synthetic datasets, GANs trained with a proposed discriminator class show correlation between IPM and Wasserstein / KL divergence, indicating that optimization challenges, rather than statistical inefficiency, may be the primary source of training difficulty. In synthetic experiments with GANs, invertible neural net generators are trained with discriminators of restricted approximability and vanilla architectures. The IPM is found to be correlated with the KL divergence, especially when considering perturbations of generators. The experiments involve training GANs to learn curves like the unit circle and a \"swiss roll\" curve in two dimensions. In synthetic experiments with GANs, invertible neural net generators are trained with discriminators of restricted approximability and vanilla architectures. The IPM is correlated with the KL divergence, especially when considering perturbations of generators. GANs are trained to learn the unit circle and a \"swiss roll\" curve in two dimensions using the Wasserstein distance to measure the quality of the learned generator. The results show evidence that restricted approximability is likely to hold for these distributions. The ground truth distributions used in the experiments are a unit circle or a Swiss roll curve. Standard two-hidden-layer ReLU nets are used as both the generator and discriminator classes, with specific architectures and optimizer settings. The generator and discriminator architectures are 2-50-50-2 and 2-50-50-1, respectively. RMSProp optimizer is used with a learning rate of 10^-4 for both. Two metrics, neural net IPM WF(p, q) and Wasserstein distance W1(p, q), are compared between the ground truth distribution p and the learned distribution q during training. The Wasserstein distance W1(p, q) is computed on fresh batches using the POT package, providing a good proxy of the true distance. Results show that the learned generator closely matches the ground truth distribution at iteration 10000, with a strong correlation between the neural net IPM and Wasserstein distance. The learned generator closely matches the ground truth distribution at iteration 10000, with a strong correlation between the neural net IPM and Wasserstein distance. At iteration 500, the generators have not fully learned the true distributions yet, as indicated by the large IPM and Wasserstein distance. Sample complexity bounds for learning various distributions using GANs with convergence guarantees in Wasserstein distance are presented. The first polynomial-in-dimension sample complexity bounds for learning various distributions using GANs with convergence guarantees in Wasserstein distance or KL divergence. Discriminators are designed with restricted approximability tailored to the generator class, aiming to avoid mode collapse. Techniques may be extended to other distribution families with tighter sample complexity bounds. The generator class considered has good generalization and mode collapse avoidance properties. Techniques may be extended to other distribution families with tighter sample complexity bounds by designing discriminators with better restricted approximability bounds. The goal is to explore and generalize approximation theory results in the context of GANs. Taking expectation over p_n on the bound yields a sufficient way to bound Ep_n [W F (p,p_n)] by 2R_n (F, G). The discriminator family in the context of GANs is explored for approximation theory results. The upper bound W F (p1, p2) \u2264 W1(p1, p2) is established, while the lower bound is recovered using mean distance. Symmetrization is used to derive the desired result for q as well. The discriminator family in GANs is analyzed for approximation theory results. The upper bound is established, and the lower bound is recovered using mean distance. Neuron distance between two Gaussians is computed for covariance distance. The neuron distance between two Gaussians is computed for covariance distance, showing that at least one term is greater than a certain value. The function R is strictly increasing, and the quantity in the supremum can be further bounded. The text discusses using the W 2 distance to establish a lower bound, with a specific value for c, by manipulating the sign of v and bounding the quantity in the supremum. The perturbation bound is utilized along with the mean difference bound to derive the final result. The text utilizes the W 2 distance to bridge the KL and F-distance for two Gaussian distributions. By bounding the growth of \u2207 log p 1 (x) 2, a lower bound is established with a specific value for c. The text discusses bounding the growth of \u2207 log p 1 (x) 2 to establish a lower bound with a specific value for c. The Rademacher contraction inequality is used to bound the right-hand side, leading to the proof of Theorem 3.2 on KL bounds for exponential families. The Rademacher contraction inequality is used to bound the right-hand side, leading to the proof of Theorem 3.2 on KL bounds for exponential families. Additionally, Wasserstein bounds are shown for a 1-Lipschitz function. The proof involves showing Wasserstein bounds for a 1-Lipschitz function and computing the Rademacher complexity for any \u03b8 \u2208 \u0398. The family F is suitable for learning a mixture of k Gaussians using a one-hidden-layer neural network. The family F is suitable for learning a mixture of k Gaussians using a one-hidden-layer neural network. The Gaussian concentration result will be used in later proofs. The concentration result (Vershynin, 2010, Proposition 5.34) is used for proving the upper and lower bounds in learning a mixture of k Gaussians with a one-hidden-layer neural network. The upper bound is shown by proving each discriminator is D-Lipschitz, while the lower bound is established by considering the regularity properties of the distributions. The text discusses implementing KL divergence for distributions p1 and p2 in the Bobkov-Gotze sense. By applying Gaussian concentration, it is shown that f(X) is at most (D^2 + 1)-sub-Gaussian. Reparameterizing a one-hidden-layer neural net is then discussed to bound the Rademacher complexity of f\u03b8. Reparameterizing a one-hidden-layer neural net to bound the Rademacher complexity of f\u03b8, we show that Y\u03b8 is Lipschitz in \u03b8 and use a one-step discretization bound. The expected supremum over a covering set can be bounded by the product of each \u00b5i, cj separately. The expected supremum over a covering set can be bounded by the product of each separate covering. Each individual process Y\u03b8 is the i.i.d. average of random variables, and the log-sum-exp part is D-Lipschitz in X. This leads to sub-Gaussian maxima bounds. The log-sum-exp part is D-Lipschitz in X, leading to sub-Gaussian maxima bounds. Choosing \u03b5 = c/n for sufficiently small c gives an upper bound on f-contrast by Wasserstein. Theorem D.2 provides upper bounds on f-contrast by Wasserstein for distributions on R d with positive densities. If \u2207f (x) \u2264 c 1 x + c 2 for all x \u2208 R d, then specific bounds are obtained. The proof involves a truncation argument and the use of Cauchy-Schwarz inequality. Proof. (a) Follows from the dual formulation of W 1. (b) Truncation argument used with Cauchy-Schwarz inequality. Term II bounded by Cauchy-Schwarz. Term I dealt with using Wasserstein distance definition. Inequality (i) used Lipschitzness of f in D-ball, and (ii) used Cauchy-Schwarz. Terms I and II combined yield the result. (c) Extension of (Polyanskiy & Wu, 2016, Proposition 1) presented for completeness. For any x, y \u2208 R d, coupling (X, Y) \u223c \u03c0 exists such that X \u223c P, Y \u223c Q. The proof extends Proposition 1 from Polyanskiy & Wu (2016) by presenting a straightforward extension. It involves defining the W 2 distance, finding a coupling (X, Y) \u223c \u03c0, and computing the inverse of x = G \u03b8 (z) using a -layer feedforward net with activation \u03c3 \u22121. The problem of representing log p \u03b8 (x) by a neural network is also considered. The inverse of x = G \u03b8 (z) can be computed using a -layer feedforward net with activation \u03c3 \u22121. Representing log p \u03b8 (x) by a neural network involves considering the density of Z \u223c N(0, diag(\u03b3 2)) and implementing G \u22121 \u03b8 with specific layers and parameters. The log density formula is also discussed, along with adding a layer on top of z with square activation and inner product with \u2212\u03b3. The network has layers with parameters in each layer and \u03c3 as the activation function. By adding branches, the log determinant of the Jacobian can be computed. The inverse network's hidden layers can be used to calculate the log determinant. The log determinant of the Jacobian can be computed by adding branches from each layer of the inverse network. This allows for the calculation of log p \u03b8 (x) using a neural network with no more than + 1 layers and O( d 2 ) parameters, and choice of activations within {\u03c3}. A similar restricted approximability bound is stated in terms of the W 2 distance. The log determinant of the Jacobian can be computed by adding branches from each layer of the inverse network, allowing for the calculation of log p \u03b8 (x) with no more than + 1 layers and O( d 2 ) parameters. The theorem follows by combining lemmas and showing that p \u03b8 satisfies the Gozlan condition for any \u03b8 \u2208 \u0398. The log determinant of the Jacobian can be computed by adding branches from each layer of the inverse network, allowing for the calculation of log p \u03b8 (x) with no more than + 1 layers and O( d 2 ) parameters. The theorem follows by combining lemmas and showing that p \u03b8 satisfies the Gozlan condition for any \u03b8 \u2208 \u0398. Additionally, the network G \u03b8 is L-Lipschitz, and the random variable DISPLAYFORM5 is L 2 -sub-Gaussian, satisfying the Gozlan condition with \u03c3 2 = L 2. Furthermore, for any \u03b8 1 , \u03b8 2 \u2208 \u0398, applying Theorem D.1(b) yields an upper bound on W F. The text discusses applying Theorem D.1(b) to obtain an upper bound on W F for any \u03b8 1 , \u03b8 2 \u2208 \u0398. It involves bounding the Lipschitzness of log p \u03b8 (x) for all \u03b8 \u2208 \u0398 and showing the W 2 bound. The log determinant of the Jacobian is computed by adding branches from each layer of the inverse network. The text discusses obtaining upper bounds on W F for any \u03b8 1 , \u03b8 2 \u2208 \u0398 by bounding the Lipschitzness of log p \u03b8 (x) and showing the W 2 bound. The log determinant of the Jacobian is computed by adding branches from each layer of the inverse network. The W 1 bound is also considered, with eq. (22) implying a bound for X 2 \u2264 D. The text discusses obtaining upper bounds on W F for any \u03b8 1 , \u03b8 2 \u2208 \u0398 by bounding the Lipschitzness of log p \u03b8 (x) and showing the W 2 bound. The W 1 bound is also considered, with eq. (22) implying a bound for X 2 \u2264 D. Applying Theorem D.2(c) yields a tail bound for X 2 \u2265 D. The text discusses reparametrizing a log-density neural network to obtain upper bounds on W F for any \u03b8 1 , \u03b8 2 \u2208 \u0398. This involves substituting variables and performing additional re-parametrizations to bound the Lipschitzness of log p \u03b8 (x) and show the W 2 bound. The Rademacher complexity of the log-density network F is bounded by two times the quantity obtained after re-parametrization. The constant C(\u03b8) includes the normalizing constant for Gaussian density and the sum of log det(W i ), which is bounded by d R W. A parameter K can be introduced to further bound this term. The metric is defined for any reparametrized \u03b8, and the Rademacher complexity is calculated as Y \u03b8 = 1/n \u03a3 \u03b5 i F \u03b8 (X i). The Rademacher complexity of the log-density network F is bounded by two times the quantity obtained after re-parametrization. A parameter K can be introduced to further bound the sum of log det(W i ). The metric is defined for any reparametrized \u03b8, and the Rademacher process is calculated as Y \u03b8 = 1/n \u03a3 \u03b5 i F \u03b8 (X i). Two lemmas deal with discretization error and expected max over a finite set. The discretization error and expected max over a finite set are addressed in Lemmas D.6 and D.7. By substituting these Lemmas into the bound equation, it is shown that the generalization error dominates a certain term. Fixing parameters such that the difference between them is less than \u03b5, the empirical average over n samples is sufficient. The generalization error dominates a certain term by choosing \u03bb = n log n/\u03b4 4. Fixing parameters such that the difference between them is less than \u03b5, the empirical average over n samples is sufficient. The hidden layers are claimed to be Lipschitz for all k, leading to specific bounds when \u03c1(\u03b8, \u03b8 ) \u2264 \u03b5. The hidden layers are claimed to be Lipschitz for all k, leading to specific bounds when \u03c1(\u03b8, \u03b8 ) \u2264 \u03b5. Induction is used to show these bounds for different layers, with verification for each layer k. The Lipschitzness is maintained for the log \u03c3 \u22121 term and the quadratic term. The hidden layers are proven to be Lipschitz for all k, with specific bounds when \u03c1(\u03b8, \u03b8 ) \u2264 \u03b5. Induction is used to verify this for each layer k, maintaining Lipschitzness for the log \u03c3 \u22121 and quadratic terms. The random variable is shown to be sub-exponential and sub-Gaussian. The random variable DISPLAYFORM32 is proven to be suitably sub-exponential and sub-Gaussian. It is shown to be Cd-sub-Gaussian with a mean and sub-Gaussianity parameter O(Cd). The term h, A \u03b3 h, is a quadratic function of a sub-Gaussian random vector and is subexponential with a mean bounded by Cd/\u03b4^2. Its sub-exponential parameter is 1/\u03b4. The term h, A \u03b3 h, is a quadratic function of a sub-Gaussian random vector, proven to be subexponential with a mean bounded by Cd/\u03b4^2. Its sub-exponential parameter is 1/\u03b4^2 times the sub-Gaussian parameter of h. Y \u03b8 is shown to be mean-zero sub-exponential when multiplied by \u03b5 i and summed over n. The parameter K is upper bounded by a constant, making Y \u03b8 mean-zero sub-exponential with a MGF bound. The covering number of \u0398 is bounded by the product of independent covering numbers. Jensen's inequality is used to show that for any \u03bb \u2264 \u03bb 0 \u03b4 2 n, a certain bound holds. The number of \u0398 is bounded by the product of independent covering numbers. Using Jensen's inequality and a certain bound, we find that for any \u03bb \u2264 \u03bb 0 \u03b4 2 n. To state the theorem more quantitatively, we need to specify relevant quantities of the generator class. The distribution p \u03b2 \u03b8 is obtained by adding Gaussian noise to a sample from G \u03b8 and truncating it to a high-probability region in both latent and observable domains. The notation f is used for convenience, with z * representing the maximum of f. The distribution p \u03b2 \u03b8 is defined over R d with bounds on partial derivatives of f. Regularity conditions are introduced for the family of generators G, including bounds on derivatives and activation functions. The distribution p \u03b2 \u03b8 is defined over R d with bounds on partial derivatives of f. Regularity conditions are introduced for the family of generators G, including bounds on derivatives and activation functions. We denote S as the maximum value in Dz, \u03bb min as the maximum value in Dz, t(z) as a function, and T as the maximum value in Dz. An upper bound R is assumed for a certain quantity. The inverse activation function is assumed to be Lipschitz. The main theorem states that for certain F and d, F approximates the Wasserstein distance. Theorem E.1 states that under certain conditions, the generator class G approximates the Wasserstein distance. The main theorem states that for certain F and d, F approximates the Wasserstein distance. Theorem E.1 states that under certain conditions, the generator class G approximates the Wasserstein distance. The proof involves a parameterized family F that can approximate the log density of p \u03b2 for every p \u2208 G. The proof involves a parameterized family F that can approximate the log density of p \u03b2 for every distribution p \u2208 G. Theorem E.2 shows the existence of a family of neural networks F that approximates log p for typical x, acts as a lower bound of p globally, and approximates entropy. The proof involves a family of neural networks F that approximates log density of p \u03b2 for every distribution p \u2208 G. The networks satisfy conditions for typical x, act as a global lower bound of p, and approximate entropy. The approach involves approximating p \u03b2 (x) using Laplace's method of integration, with a focus on calculating the maximum value for typical x. The proof of Theorem E.1 involves neural networks N1, N2 in F approximating log p \u03b2 and log q \u03b2, satisfying conditions for typical x. The proof relies on Theorem E.2, using a greedy \"inversion\" procedure to calculate the maximum value for typical x and providing a lower bound for atypical x. The proof of Theorem E.1 involves neural networks N1, N2 in F approximating log p \u03b2 and log q \u03b2. By Theorem E.2, N1 and N2 approximate log p \u03b2 and log q \u03b2 respectively. The lower bound is proven using Bobkov-G\u00f6tze theorem, while the upper bound is obtained by setting \u03b2 = W 1/6. The lower bound is proven using the Bobkov-G\u00f6tze theorem, while the upper bound is obtained by setting \u03b2 = W 1/6. The optimal coupling C of p, q and the induced coupling C z on the latent variable z in p, q are considered. The generalization claim follows analogously to Lemma D.5. The proof of Theorem E.2 in Section E.3 follows a reverse induction on l, with helper lemmas established. The generalization claim is analogous to Lemma D.5, using Lipschitzness bound of generators in Theorem E.2. In Section E.3, helper lemmas are proven through reverse induction on l. The claim is established for i = 0, and then the induction is carried out. Estimates are iteratively produced by N to recover a\u1e91, s. The proof by induction shows the validity of the claim, with inequalities derived from the Lipschitzness of \u03c3 \u22121. The induction proof establishes the claim for i = 0 and continues with the induction step. Inequalities are derived from the Lipschitzness of \u03c3 \u22121, leading to the completion of the claim. The size/Lipschitz constant of the neural network is also discussed, with a focus on the cdf of a Gaussian with covariance. The algorithm approximates the integral using a small, Lipschitz network. It is a discriminator family with restricted approximability. The algorithm approximates the integral using a small, Lipschitz network and is a discriminator family with restricted approximability. The algorithm involves calculating gradients and finding the nearest matrix in a set with bounded spectral norm. The algorithm approximates the integral using a small, Lipschitz network and involves calculating gradients to find the nearest matrix in a set with bounded spectral norm. The output of the \"invertor\" circuit is used to calculate g = \u2207f (\u1e91), H = \u2207 2 f (\u1e91), and approximate eigenvector/eigenvalue pairs of H + E i. The Algorithm 1 approximates the integral using an invertor circuit to find the nearest matrix in a set with bounded spectral norm. Matrices E i are chosen from a net S with spectral norm bounded by O(1/\u03b2 2), ensuring eigenvalues are \u2126(\u03b2)-separated. The Algorithm 1 approximates the integral using an invertor circuit to find the nearest matrix in a set with bounded spectral norm. Matrices E i are chosen from a net S with spectral norm bounded by O(1/\u03b2 2), ensuring eigenvalues are \u2126(\u03b2)-separated. There exist matrices E 1 , E 2 , . . . , E r , r = \u2126(d log(1/\u03b2)), s.t. if M \u2208 S, at least one of the matrices M + E i has eigenvalues that are \u2126(\u03b2)-separated. The integral in Algorithm 1 is approximated using an invertor circuit to find the nearest matrix in a set with bounded spectral norm. Matrices E i are chosen from a net S with spectral norm bounded by O(1/\u03b2 2), ensuring eigenvalues are \u2126(\u03b2)-separated. In synthetic WGAN experiments, invertible neural net generators and discriminators with restricted approximability are used to compute the KL divergence between p and q on synthetic data. The empirical IPM W F (p, q) is well correlated with the KL-divergence between p and q on synthetic data generated from a ground-truth invertible neural net generator. The data is X = G \u03b8 (Z), where G \u03b8 is a layer-wise invertible feedforward net and Z is a spherical Gaussian. Leaky ReLU with negative slope 0.5 is used as the activation function \u03c3. The data is generated from a ground-truth invertible neural net generator using Leaky ReLU with negative slope 0.5 as the activation function. The weight matrices are well-conditioned with singular values between 0.5 to 2. The discriminator architecture is chosen based on restricted approximability guarantee. The derivative and inverse of the activation function can be efficiently computed. The discriminator architecture is chosen based on restricted approximability guarantee, with constraints on parameters. Training involves generating batches from ground-truth and trained generators, solving min-max problem in Wasserstein GAN formulation. Discriminator is updated 10 times between each generator step with various regularization methods. We generate stochastic batches (batch size 64) from the ground-truth and trained generators in Wasserstein GAN. The discriminator is updated 10 times between each generator step with different regularization methods. Evaluation includes computing KL divergence between the true and learned generator. The KL divergence and training loss metrics are used to evaluate the distributional closeness between the true and learned generator in Wasserstein GAN. The KL divergence is computed analytically, while the training loss is the unregularized GAN loss during training. The balance between discriminator and generator steps can affect the training IPM's proximity to the true Wasserstein distance. The training loss in Wasserstein GAN is the unregularized GAN loss during training. The neural net IPM is a separately optimized WGAN loss where the discriminator is trained from scratch to optimality to maximize contrast. The learned generator is held fixed while the discriminator is trained from scratch to optimality in norm balls without regularization. The goal is to find f \u2208 F that maximizes contrast, with the loss approximating W F. The theory suggests that WGAN can learn the true generator in KL divergence, with F-IPM indicative of the KL divergence. In experiments, G is a two-layer net in d = 10 dimensions. In experiments, a two-layer neural net is used as the generator in 10 dimensions. The discriminator is trained with either Vanilla WGAN or WGAN-GP, with results shown in FIG5. The study compares Vanilla WGAN and WGAN-GP in training a discriminator with a fixed generator. Results show that WGAN with restricted approximability can learn the true distribution with low KL divergence, indicating GANs are effective and mode collapse is avoided. The correlation between W F (eval) and KL divergence is also noted. The study compares Vanilla WGAN and WGAN-GP in training a discriminator with a fixed generator. Results show that WGAN with restricted approximability can learn the true distribution with low KL divergence, indicating GANs are effective and mode collapse is avoided. The W F (eval) and KL divergence are highly correlated, with adding gradient penalty improving optimization significantly. W F can serve as a good metric for monitoring convergence, better than the training loss curve. The quantity W F can serve as a good metric for monitoring convergence, correlating well with the KL-divergence between the true and learned distributions. Testing with vanilla discriminators also shows a strong correlation with the KL-divergence. The left-most figure shows the KL-divergence between the true distribution and learned distribution at different training steps, the middle shows the estimated IPM between the distributions, and the right shows the training loss. The estimated IPM in evaluation correlates well with the KL-divergence. In this section, the inferior performance of the WGAN-Vanilla algorithm in terms of KL divergence is attributed to the training performance rather than the statistical properties of GANs. The correlation between the KL divergence and neural net IPM is directly tested, removing the effect of optimization. In this section, the correlation between the KL divergence and neural net IPM is directly tested by comparing pairs of perturbed generators. Gaussian noise is added to one generator to create a perturbation, and the KL divergence and neural net IPM are computed between the original and perturbed generators. The discriminator is optimized from 5 random initializations to denoise the training process for computing the neural net IPM. The correlation between KL divergence and neural net IPM is tested by perturbing generators with Gaussian noise. The discriminator is optimized from 5 random initializations to denoise the training process for computing the neural net IPM. A positive correlation is observed in the majority of points, with outliers due to large perturbations. The neural net distance scales linearly in the KL divergence, with outliers due to large perturbations causing weight matrices to become poorly conditioned. Experiments with vanilla fully-connected discriminator nets show good convergence in KL divergence. Results from experiments using vanilla fully-connected discriminator nets show good convergence in KL divergence, with slightly weaker correlation compared to restricted approximability. This suggests that vanilla discriminator structures can be effective in generating good results, although specific designs may further improve performance. The experiments with vanilla fully-connected discriminator nets show good convergence in KL divergence, suggesting their effectiveness in generating good results. Specific designs may further improve performance. The correlation between KL and neural net IPM is roughly the same with vanilla fully-connected discriminators. The estimated IPM in evaluation correlates well with the KL-divergence. Moving average is applied to all curves. Correlation between KL and neural net IPM is computed with vanilla fully-connected discriminators and plotted in FIG8, with a correlation of 0.7489, similar to discriminators with restricted approximability at 0.7315."
}