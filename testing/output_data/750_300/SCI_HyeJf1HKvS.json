{
    "title": "HyeJf1HKvS",
    "content": "This work introduces a two-stage neural architecture for learning structural correspondences between graphs. It utilizes localized node embeddings from a graph neural network to rank soft correspondences initially, followed by synchronous message passing networks to iteratively refine the rankings for matching consensus in local neighborhoods. The message passing scheme computes a reliable measure of consensus for corresponding neighborhoods, guiding the re-ranking process effectively. The architecture is scalable to large, real-world inputs while being purely local and sparsity-aware. Our message passing scheme computes a well-founded measure of consensus for neighborhoods, guiding the iterative re-ranking process. The architecture scales well to large inputs and consistently recovers global correspondences. The method improves upon the current state-of-the-art in tasks such as computer vision and entity alignment between knowledge graphs. Graph matching establishes structural correspondences between nodes in multiple graphs using node similarities and pairwise edges. Graph matching involves establishing structural correspondences between nodes in multiple graphs using node similarities and pairwise edge similarities. This problem is crucial in various real-world applications such as comparing molecules in cheminformatics, matching protein networks in bioinformatics, and linking user accounts in social network analysis. Graph matching is essential in various real-world applications such as comparing molecules in cheminformatics, matching protein networks in bioinformatics, linking user accounts in social network analysis, and tracking objects in computer vision. The problem has been extensively studied in theory and practice, often using domain-agnostic distances like graph edit distance and maximum common subgraph problem. Recently, various neural architectures have been introduced to address the NP-hard graph matching problem, which traditional combinatorial approaches struggle to solve optimally for large-scale instances. These neural architectures consider continuous node embeddings to adapt to data distribution and capture crucial information about node semantics. Various neural architectures have been proposed to tackle graph matching and similarity tasks in a data-dependent fashion, incorporating continuous node embeddings for better adaptation to data distribution and capturing crucial node semantics. Graph matching approaches based on neural architectures have limitations in computing similarity scores between whole graphs, relying on inefficient global matching procedures, and not generalizing to unseen graphs. Typically, graph matching is formulated as an edge-preserving, quadratic assignment problem subject to one-to-one mapping constraints. Graph matching is typically formulated as an edge-preserving, quadratic assignment problem based on neighborhood consensus to prevent adjacent nodes from being mapped to different regions in the target graph. The problem of supervised and semi-supervised graph matching involves achieving neighborhood consensus between source and target graphs. In the supervised setting, the model learns from pair-wise ground-truth correspondences to generalize to unseen graph pairs. In the semi-supervised setting, ground-truth correspondences are limited to a small subset of fixed source and target graphs. Our proposed deep graph matching architecture involves two stages: local feature matching and iterative refinement using synchronous message passing networks. The feature matching step aims to compute correspondences between graphs in a supervised or semi-supervised setting. The deep graph matching architecture involves two stages: local feature matching and iterative refinement using synchronous message passing networks. The feature matching step computes initial correspondence scores based on node embeddings, while the refinement strategy aims to reach neighborhood consensus for correspondences using a differentiable validator for graph isomorphism. The method is scalable to large, real-world inputs. In Section 3.4, the method is scaled to large, real-world inputs by reaching neighborhood consensus for correspondences using a differentiable validator for graph isomorphism. The two-stage neighborhood consensus architecture involves local feature matching and iterative refinement. The method involves neighborhood consensus for correspondences using a differentiable validator for graph isomorphism. An injective node coloring of G s is transferred to G t via S, distributed by \u03a8 \u03b82 on both graphs, and updated by a neural network \u03a6 \u03b83 based on pair-wise color differences. Local feature matching is done by computing similarities between nodes in G s and G t based on node embeddings. The method involves neighborhood consensus for correspondences using a differentiable validator for graph isomorphism. Local feature matching is done by computing similarities between nodes in the source graph G s and the target graph G t based on node embeddings. Initial soft correspondences are obtained using sinkhorn normalization to fulfill specific constraints. The neural network \u03a8 \u03b81 is trained in a discriminative, supervised fashion against ground truth. The i-th row vector S i,: \u2208 [0, 1] |Vt| represents a distribution over potential correspondences in G t for each node i \u2208 V s. \u03a8 \u03b81 is trained against ground truth correspondences \u03c0 gt (\u00b7) by minimizing negative log-likelihood. Implemented as a Graph Neural Network (GNN), \u03a8 \u03b81 provides localized, permutation equivariant node representations. A Graph Neural Network (GNN) like \u03a8 \u03b81 is used to obtain localized, permutation equivariant vectorial node representations. GNN follows a neural message passing scheme to update node features in layers by aggregating localized information. Recent work in geometric deep learning and relational representation learning offers various operators for precise control over extracted properties. The use of geometric deep learning and relational representation learning provides a wide range of operators for precise feature control. However, the local nature of node embeddings can lead to false correspondences, violating neighborhood consensus criteria. Detecting and resolving these violations iteratively is necessary due to the NP-hard nature of finding a global optimum. The proposed algorithm utilizes graph neural networks to detect and resolve violations of neighborhood consensus criteria in local neighborhoods iteratively. The soft correspondence matrix S is used to pass node functions along the soft correspondences, aiming to refine correspondences in an iterative fashion. The algorithm utilizes the soft correspondence matrix S to map node functions between graphs, refining correspondences iteratively. By passing node indicator functions and performing synchronous message passing, consensus is achieved in local neighborhoods. The algorithm uses soft correspondence matrix S to map node functions between graphs, refining correspondences iteratively through synchronous message passing and neighborhood consensus measurement. This process combines feature matching error and neighborhood consensus error to improve correspondence scores. The algorithm uses soft correspondence matrix to refine correspondences iteratively through message passing and neighborhood consensus. This process combines feature matching error and neighborhood consensus to improve correspondence scores using trainable updates based on an MLP. The consensus stage distributes global node colorings to resolve ambiguities and false matchings by using purely local operators. The algorithm refines correspondences through message passing and neighborhood consensus using global node colorings to resolve ambiguities and false matchings. The two-stage approach emphasizes the importance of local operators in matching local neighborhoods between isomorphic graphs. The algorithm refines correspondences through message passing and neighborhood consensus using global node colorings to resolve ambiguities and false matchings. The proofs for the isomorphic graphs can be found in Appendices B and C. Theorem 1 states that a permutation equivariant GNN can encode an isomorphism between two isomorphic graphs. Theorem 2 discusses a permutation equivariant and T-layered GNN with injective AGGREGATE and UPDATE functions for graph matching. A GNN satisfying permutation equivariance and injectivity criteria provides equal node embeddings. Common GNN architectures are equivariant due to permutation invariant neighborhood aggregators. Injectivity can be achieved by using a powerful GNN like the Weisfeiler & Lehman heuristic. Injectivity of graph neural networks is a key topic in recent literature, fulfilled by using a powerful GNN like the Weisfeiler & Lehman heuristic. This approach relates to classical graph matching techniques, such as the graduated assignment algorithm. The approach involves a doubly-stochastic relaxation of the problem, inspired by classical graph matching techniques like the graduated assignment algorithm. The softassign operator is used for sinkhorn normalization on rescaled inputs to encourage integer solutions. The softassign operator is implemented for sinkhorn normalization on rescaled inputs to encourage integer solutions. The gradient Q is related to a neighborhood consensus scheme for a non-trainable GNN instantiation. Correspondence scores are updated via trainable neural networks based on the difference between O s and O t. Our model updates correspondence scores using trainable neural networks based on the difference between O s and O t, resembling a deep parameterized version of the graduated assignment algorithm. This approach supports continuous node and edge features through established GNN models, simplifying graph matching computations. Our approach supports continuous node and edge features through established GNN models, simplifying graph matching computations. We propose sparsifying initial correspondences by filtering out low score correspondences before neighborhood consensus. The final optimized algorithm proposes sparsifying initial correspondences by filtering out low score correspondences before neighborhood consensus. This is achieved by computing top k correspondences with the help of the KEOPS library, reducing memory footprint and time complexity. The algorithm proposes sparsifying initial correspondences by filtering out low score correspondences before neighborhood consensus, reducing time complexity from O(|V s ||V t |) to O(k|V s |). The refinement phase time complexity is also reduced, and optimizing the initial feature matching loss is crucial for efficiency. The algorithm suggests accelerating training by sparsifying correspondences and replacing node indicator functions with randomly drawn node functions to reduce parameter complexity. The refinement strategy aims to maintain injectivity while improving efficiency. The algorithm aims to accelerate training by sparsifying correspondences and replacing node indicator functions with randomly drawn node functions. The refinement strategy maintains injectivity while improving efficiency, with a focus on resolving ambiguities through re-sampling in each iteration. Softmax normalization is compared to sinkhorn normalization, highlighting potential issues with the latter pushing correspondences to inconsistent integer solutions early on. The algorithm aims to accelerate training by sparsifying correspondences and replacing node indicator functions with randomly drawn node functions. Softmax normalization is proposed as a solution to avoid pushing correspondences to inconsistent integer solutions early on. The approach involves row-wise softmax normalization on the matrix, allowing the supervised refinement procedure to resolve violations on its own. The algorithm proposes row-wise softmax normalization on the matrix to resolve violations and accelerate training. Varying the number of refinement iterations for training and testing speeds up runtime and encourages convergence with fewer steps. The algorithm proposes row-wise softmax normalization on the matrix to resolve violations and accelerate training by varying the number of refinement iterations for training and testing. This speeds up runtime and encourages convergence with fewer steps, as shown empirically on three different tasks. Our method is validated on various tasks, including synthetic graph analysis, supervised keypoint matching in images, and cross-lingual knowledge graph alignment. Implementation is in PYTORCH using PYTORCH GEOMETRIC and KEOPS libraries for efficient processing with GPU acceleration. Optimization is performed with ADAM for all experiments. Our implementation utilizes PYTORCH GEOMETRIC and KEOPS libraries for efficient processing with GPU acceleration and minimal memory usage. Optimization is done using ADAM with a fixed learning rate. Hits@k is used to evaluate the model's performance on synthetic graphs, measuring the proportion of correctly matched entities in the top k rankings. In the first experiment, the model is evaluated on synthetic graphs to learn a matching for pairs of graphs in a supervised fashion. The graphs consist of Erd\u0151s & R\u00e9nyi graphs with different node and edge probabilities. Training and evaluation are conducted on multiple graph configurations. The model is trained and evaluated on 1,000 graphs with different edge probabilities. Additional experiments test the approach's robustness to node changes. The graph neural network uses GIN operators with three layers for expressiveness. Architecture parameters include 2 layers and a hidden dimension of 32, with ReLU activation. The graph neural network uses GIN operators with three layers for expressiveness. MLPs have 2 layers and a hidden dimension of 32 with ReLU activation. Input features are initialized with one-hot encodings of node degrees. Jumping Knowledge style concatenation is used to compute final node representations. The model is trained and tested with 10 and 20 refinement iterations, respectively, showing matching accuracy Hits@1 for different choices of |V s | and p. The graph neural network utilizes GIN operators with three layers for expressiveness and MLPs with 2 layers and a hidden dimension of 32. Input features are initialized with one-hot encodings of node degrees. Jumping Knowledge style concatenation is employed to compute final node representations. The model is trained and tested with 10 and 20 refinement iterations, respectively, showcasing matching accuracy Hits@1 for different choices of |V s | and p. The proposed two-stage architecture can recover all correspondences, regardless of the structural noise p s applied. The proposed two-stage architecture can recover all correspondences, independent of structural noise, using matching consensus and enhancements for scalability. Increasing the number of iterations during testing ensures convergence even when training does not reach convergence. The proposed two-stage architecture ensures convergence by increasing the number of iterations during testing, even when training does not reach convergence. The refinement strategy shows improved performance on sparsified correspondences, converging to Hits@1 \u2248 Hits@k with increasing k. The proposed two-stage architecture ensures convergence by increasing the number of iterations during testing, even when training does not reach convergence. It consistently converges to Hits@1 \u2248 Hits@k in case the correct match is included in the initial top k ranking of correspondences, making it an excellent option to scale the algorithm to large graphs. Experiments were performed on the PASCALVOC and WILLOW-OBJECTCLASS datasets with labeled keypoint locations. The PASCALVOC dataset, along with the WILLOW-OBJECTCLASS dataset, contains labeled keypoint locations for image categories. The PASCALVOC dataset is pre-filtered to exclude difficult, occluded, and truncated objects, with 6,953 annotated images for training and 1,671 for testing. The dataset includes instances of varying scale, pose, and illumination, with keypoints ranging from 1 to 19. The WILLOW-OBJECTCLASS dataset consists of images with consistent orientations for each category, with 10 keypoints per image. The model is pre-trained on PASCALVOC and fine-tuned on 20 random splits with 20 per-class images for training. Graphs are constructed using Delaunay triangulation of keypoints. Our model is pre-trained on PASCALVOC and fine-tuned on 20 random splits with 20 per-class images for training. Key points include constructing graphs via Delaunay triangulation of keypoints and using input features from a pre-trained VGG16 model. Architecture and parameters involve adopting SPLINECNN as the graph neural network operator with a trainable B-spline based kernel function. The curr_chunk discusses the use of a pre-trained VGG16 model on IMAGENET and the adoption of SPLINECNN as a graph neural network operator with B-spline based kernel functions. Results are evaluated for both isotropic and anisotropic cases with different parameter settings. The curr_chunk presents edge features with normalized relative distances and 2D Cartesian coordinates. SPLINECNN uses a kernel size of 5 in each dimension. The curr_chunk describes the network architecture of SPLINECNN, including kernel size, hidden dimensionality, and activation function. Training involves forming pairs of examples from the same category. The network architecture of SPLINECNN includes dropout with probability 0.5 and a final linear layer. Training involves forming pairs of examples from the same category and evaluating the model with test graph pairs. Results show superior performance using negative log-likelihood compared to displacement loss. Hits@1 results are presented for PASCALVOC and WILLOW-OBJECTCLASS datasets. Sminchisescu (2018) evaluates their architecture using isotropic and anisotropic GNNs for different values of L, with ablation results using MLP for local node matching. Hits@1 results for PASCALVOC and WILLOW-OBJECTCLASS datasets are shown in Tables 1 and 2. Their refinement strategy outperforms competitors and non-refined baselines, reducing errors by half on WILLOW-OBJECTCLASS. The second stage of refinement is crucial for improving initial features. Our refinement stage significantly improves initial model errors by half on the WILLOW-OBJECTCLASS dataset. Starting from a weaker baseline also shows up to 14 percentage points improvement on PASCALVOC. Task-specific isotropic or anisotropic GNNs enhance performance in the consensus stage. Geometric feature matching is successfully tackled using only point coordinates. In the refinement stage, task-specific isotropic or anisotropic GNNs are used to improve performance further. The approach is verified by solving the geometric feature matching problem with only point coordinates. Synthetic graph pairs are generated for training by sampling source points and adding Gaussian noise. Additionally, outliers are included in the dataset. For training, synthetic graph pairs are generated by sampling source points and adding Gaussian noise. Outliers are also included in the dataset. An unmodified anisotropic keypoint architecture is trained with input x i = 1 \u2208 R 1 until 32 000 synthetic examples are seen. The trained model is evaluated on the PASCALPF dataset consisting of 1 351 image pairs within 20 classes. The trained model is evaluated on the PASCALPF dataset, showing improved results compared to the baseline. The consensus architecture outperforms the state-of-the-art in most categories, even without visual information. Our model outperforms the baseline on various categories, demonstrating the effectiveness of the consensus stage. It also performs well without utilizing visual information. The evaluation is done on the DBP15K datasets, linking entities from different knowledge graphs. The dataset contains 15,000 links between equivalent entities, split into training and testing sets. Entity input features are obtained using monolingual FASTTEXT embeddings aligned into the same vector space. The graph neural network operator architecture mostly matches previous works, retaining the direction of edges. The graph neural network operator architecture, based on Xu et al. (2019d), uses ReLU followed by dropout for non-linearity and obtains final node representations. A three-layer GNN with dimensionality 256 and 32 is used for initial similarities and refining alignments. Training is done in a semi-supervised manner with negative log likelihood. The three-layer GNN with dimensionality 256 and 32 is used for obtaining initial similarities and refining alignments. Training is done in a semi-supervised fashion using negative log likelihood. Hits@1 and Hits@10 are reported to evaluate the model's performance. Our model updates the correspondence matrix L 10 times for efficiency, training it initially and sequentially for 100 epochs each. Results show improvements in Hits@1 and Hits@10 compared to previous work, with gains of up to 9.38 percentage points. The refinement strategy consistently enhances Hits@1 significantly, while Hits@10 results are comparable. Our approach outperforms the state-of-the-art in all categories, showing gains of up to 9.38 percentage points. The refinement strategy consistently improves Hits@1 significantly, while Hits@10 results are not as pronounced due to operating on sparsified initial correspondences. The scalability of our approach allows for multiple refinement iterations while maintaining large hidden feature dimensionalities, effectively solving challenging real-world problems. The proposed approach effectively solves challenging real-world problems, but may fail to converge when two nodes are assigned the same color by the WL heuristic for graph isomorphism testing. This limitation is inherited from the expressive power of GNNs. One limitation of the proposed approach is that it may fail to converge when two nodes are assigned the same color by the WL heuristic for graph isomorphism testing. This issue arises when nodes have equal neighborhood sets, leading to non-convergence in the feature matching procedure. Resolving these ambiguities by adding noise to the initial correspondence distributions is theoretically possible but unlikely to occur due to the presence of feature noise in real-world datasets. Adding noise to resolve ambiguities in graph matching is unlikely due to existing feature noise in real-world datasets. Various related problems include maximum common subgraph, network alignment, graph edit distance, and graph matching. Graph neural networks have recently become a focus of research. Graph neural networks have become a focus of research, leading to proposed deep graph matching techniques. A two-stage neural architecture was presented for learning node correspondences between graphs in a supervised or semi-supervised fashion. Our proposed two-stage neural architecture aims to learn node correspondences between graphs by reaching a neighborhood consensus between matchings and resolving violations iteratively. We also introduced enhancements to scale our algorithm to large input domains and evaluated it on real-world datasets, consistently outperforming the state-of-the-art. Our final optimized algorithm is provided in Algorithm 1. Our proposed two-stage neural architecture aims to learn node correspondences between graphs by reaching a neighborhood consensus between matchings and resolving violations iteratively. Enhancements were made to scale the algorithm to large input domains, and it consistently outperformed the state-of-the-art on real-world datasets. The final optimized algorithm is detailed in Algorithm 1, which includes input parameters and proof of its effectiveness. The T-layered GNN \u03a8 \u03b82 maps T-hop neighborhoods around nodes i \u2208 Vs and j \u2208 Vt to the same vectorial representation, distinguishing graph structures with the power of the WL heuristic. This isomorphism relation is represented by a permutation matrix P, derived from injective node colorings. The isomorphism relation between graph structures is represented by a permutation matrix P, derived from injective node colorings. If nodes i, i \u2208 Vs map to the same node j, it contradicts the injectivity requirements of AGGREGATE (t) and UPDATE (t) for all t \u2208 {1, . . . , T }. Therefore, S must be itself a permutation matrix describing an isomorphism. The algorithm described in Section 3.3 is a generalization of the graduated assignment algorithm (Gold & Rangarajan, 1996) with trainable parameters. The impact of a trainable refinement procedure was evaluated by implementing \u03a8 \u03b82 as a non-trainable, one-layer GNN instantiation \u03a8 \u03b82 (X, A, E) = AX. The algorithm described in Section 3.3 extends the graduated assignment algorithm (Gold & Rangarajan, 1996) with trainable parameters. Using trainable neural networks \u03a8 \u03b82 consistently improves results compared to fixed-function message passing schemes. This approach learns to utilize node and edge features for refinement and offers flexibility for task-dependent GNN selection. Our approach enhances the graduated assignment algorithm with trainable neural networks to utilize node and edge features for refinement and task-dependent GNN selection. Theoretical expressivity could be improved with higher-order GNNs in future work. Experimental validation was conducted for robustness towards node addition or removal. To validate the robustness of our approach, we conducted synthetic experiments by adding noisy nodes to Erd\u0151s & R\u00e9nyi graphs and forming graph-pairs for comparison. Our approach involves adding noisy nodes to the source graph and generating edges between these nodes and all other nodes. The consensus stage of our neural architecture is robust to node additions or removals, unlike the first stage which struggles with finding the right matching. Unmatched nodes do not affect the neighborhood consensus error, as they do not receive a color from the functional map. Our neural architecture can detect and reduce false positive influence of unmatched nodes in the refinement stage, improving the matching process between nodes in two graphs. The problem of identifying correspondences between nodes in graphs has been studied in graph theory, specifically the combinatorial maximum common subgraph isomorphism problem, which remains NP-hard even in trees. In graph theory, the combinatorial maximum common subgraph isomorphism problem is NP-hard and remains so even in trees unless the common subgraph is required to be connected. Most variants of the problem are difficult to approximate with theoretical guarantees. Exact polynomial-time algorithms are available for specific problem variants relevant in cheminformatics. Exact polynomial-time algorithms are available for specific problem variants in cheminformatics, while bioinformatics and computer vision use non-exact techniques for network alignment or graph matching. In graph matching, the goal is to minimize a function for two graphs of order n with adjacency matrices A s and A t. Large networks in graph matching often lack specific structural properties, and the techniques used are non-exact. The goal is to minimize a function for two graphs of order n with adjacency matrices A s and A t. The optimization problem can be simplified by minimizing Equation (12), which is equivalent to Equation (1). Previous research in graph matching is briefly summarized, with a more detailed discussion available in a recent survey by Yan et al. Minimizing Equation (12) in graph matching is equivalent to solving Equation (1). Previous research has focused on using a Frank-Wolfe algorithm to minimize Equation (12) and project the fractional solution to P. However, the applicability of relaxation and projection techniques is not well understood, with limited theoretical results available. The applicability of relaxation and projection techniques in graph matching is still poorly understood, with limited theoretical results available. The WL heuristic distinguishes two graphs G s and G t if there is no fractional S such that the objective function takes 0. Kersting et al. (2014) modified the Frank-Wolfe algorithm to obtain the WL partition, while Aflalo et al. (2015) proved the standard relaxation yields correct solutions for specific asymmetric graphs. Equation (12) takes 0. Kersting et al. (2014) modified the Frank-Wolfe algorithm to obtain the WL partition. Aflalo et al. (2015) proved the standard relaxation yields correct solutions for specific asymmetric graphs. Bento & Ioannidis (2018) studied various relaxations, their complexity, and properties. Other graph matching approaches include spectral relaxations (Umeyama, 1988; Leordeanu & Hebert, 2005) and random walks (Gori et al., 2005). The problem of graph matching is closely related to the quadratic assignment problem (QAP) (Zhou & De la Torre, 2016). Graph matching methods, such as spectral relaxations and random walks, are closely related to the quadratic assignment problem (QAP). Recent literature on graph matching considers a weighted version, leading to the formulation of Lawler's QAP. Zhou & De la Torre (2016) proposed a method to factorize the affinity matrix for computational efficiency. The weighted version of graph matching considers node and edge similarities, leading to Lawler's QAP formulation. Zhou & De la Torre (2016) proposed factorizing the affinity matrix for computational efficiency, while Zhang et al. (2019c) studied kernelized graph matching using kernels for node and edge similarities. Swoboda et al. (2017) explored Lagrangean decompositions inspired by MAP inference in conditional random fields. Swoboda et al. (2017) studied Lagrangean decompositions for graph matching, utilizing dual ascent algorithms inspired by MAP inference in conditional random fields. Specific message passing schedules and update mechanisms have been identified for state-of-the-art performance in graph matching tasks. Functional representation has been proposed as a concept to avoid constructing the affinity matrix. Recently, mechanisms for achieving state-of-the-art performance in graph matching tasks have been identified experimentally. Functional representation has been proposed as a generalizing concept to avoid constructing the affinity matrix. The graph edit distance, a related concept in computer vision, measures the minimum cost needed to transform one graph into another by adding, deleting, and substituting vertices and edges. Despite being proposed over 30 years ago, its computation remains NP-hard due to its relation to the maximum common subgraph problem. The graph edit distance measures the minimum cost to transform one graph into another by modifying vertices and edges. It is NP-hard and related to the maximum common subgraph problem. Various algorithms have been proposed for computing it, with heuristics based on the assignment problem being widely used in practice. The graph edit distance measures the minimum cost to transform one graph into another by modifying vertices and edges. Various algorithms have been proposed for computing it, with heuristics based on the assignment problem being widely used in practice. Network alignment is another problem that involves a similarity function between pairs of nodes. Network alignment involves computing a similarity matrix from node-to-node similarities and graph topology, followed by solving the assignment problem to align the graphs. ISORANK by Singh et al. (2008) uses the adjacency matrix of the product graph for alignment. The matrix is obtained using a normalized version of PAGERANK. Singh et al. (2008) proposed ISORANK for network alignment, based on the adjacency matrix of the product graph. Kollias et al. (2012) suggested an efficient approximation of ISORANK using decomposition techniques. Zhang (2016) extended ISORANK to support vertex and edge similarities. Klau (2009) proposed a linear approach for network alignment. In network alignment, various techniques aim to find optimal correspondences between graphs without generating a product graph of quadratic size. Zhang (2016) extended ISORANK to include vertex and edge similarities, while Klau (2009) proposed a linear approach using integer linear programming. Bayati et al. (2013) developed a message passing algorithm for sparse network alignment. These techniques focus on achieving an optimal correspondence based on a defined objective function. Sparse network alignment techniques aim to find optimal correspondences between graphs with a small number of matches allowed. Recent approaches involve learning node and edge similarity functions for specific tasks, such as a cost model for graph edit distance. Caetano et al. (2009) proposed a method to learn correspondences in a principled manner, related to deep graph matching procedures. The method presented in this work is related to deep graph matching procedures that aim to learn correspondences between graphs. It involves refining local feature matchings by enforcing neighborhood consistency, similar to the functional maps framework for manifolds. The curr_chunk discusses the relevance of enforcing neighborhood consistency for matching in images and the use of deep graph matching networks. Various studies have explored deep graph matching, including supervised networks based on displacement and combinatorial objectives. The approach differs from previous methods by incorporating a learnable matching procedure. Deep graph matching networks are explored in the curr_chunk, with different approaches such as using node-wise features and dense node-to-node cross-graph affinities. Wang et al. (2019b) utilize sinkhorn normalization for linear assignment, while Zhang & Lee (2019) propose a compositional message passing algorithm for point coordinate mapping. The matching procedure involves computing pairwise inner products between points. In contrast to previous approaches utilizing sinkhorn normalization and compositional message passing, our work addresses inconsistent neighborhood assignments in graph matching. Xu et al. (2019b) also tackle graph matching by incorporating Gromov-Wasserstein discrepancy and learning node embeddings to account for noise in graphs. In a follow-up to previous work on graph matching, Xu et al. (2019a) extend the concept by learning a Gromov-Wasserstein barycenter for multi-graph partitioning and matching. Derr et al. (2019) investigate network alignment using CYCLEGANs from various perspectives. The task of network alignment has been explored using various approaches. Derr et al. (2019) utilize CYCLEGANs to align NODE2VEC embeddings, while Heimann et al. (2018) use a local matching procedure based on node embedding similarity. Bai et al. (2019) employ shared graph neural networks for approximation. In network alignment, Heimann et al. (2018) use a local matching procedure based on node embedding similarity, while Bai et al. (2019) employ shared graph neural networks for approximation of graph edit distance. Bai et al. (2018) propose a breadth-first-search approach to process correspondence matrices with traditional CNNs, aiming to improve consistency in matching correspondences. In a follow-up work, Bai et al. (2018) proposed ordering the correspondence matrix in a breadth-first-search manner and processing it with traditional CNNs. They focused on enhancing GNN operators by aggregating information from local neighbors and similar embeddings in other graphs through a cross-graph matching procedure. Wang et al. (2019b) and Xu et al. (2019d) improve GNN operators by aggregating information from local neighbors and similar embeddings in other graphs. Wang & Solomon (2019) address the unknown rigid motion problem by relating it to point cloud matching and utilizing a Transformer module for intra-graph node embeddings. Our approach addresses the unknown rigid motion problem in point cloud matching by utilizing a differentiable SVD module and Transformer module for intra-graph node embeddings. While existing methods focus on localized node embeddings, we propose using these methods to enhance initial feature matching, offering a complementary approach to improving consistency in matching procedures. Neighborhood consensus methods have a history in computer vision, improving local feature matching efficiently. A recent approach using 4D convolution was proposed, related to but not the same as our method. Our algorithm improves local feature matching efficiently by inferring errors on the original graphs, avoiding the computational complexity of applying a GNN on a product graph. The functional maps framework defines continuous maps between function spaces on manifolds and is commonly used in computer vision. Our algorithm infers errors for the product graph but computes on the original graphs. Functional maps define continuous maps between function spaces on manifolds and are commonly used in computer vision."
}