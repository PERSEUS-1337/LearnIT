{
    "title": "H1xSNiRcF7",
    "content": "There is a growing interest in geometrically-inspired embeddings for learning hierarchies, partial orders, and lattice structures, with applications to transitive relational data. Recent work has extended these ideas to probabilistically calibrated models, enabling learning from uncertain supervision and inferring soft-inclusions among concepts. Building on the Box Lattice model of Vilnis et al. (2018), which uses high-dimensional hyperrectangles to model soft-inclusions, this approach maintains the geometric inductive bias of hierarchical embedding models. In this work, a novel hierarchical embedding model is presented, inspired by a relaxation of box embeddings into parameterized density functions using Gaussian convolutions over the boxes. This approach offers an alternative surrogate to the original lattice measure, addressing the challenges posed by the hard edges of the boxes in standard gradient-based optimization. The novel hierarchical embedding model, inspired by a relaxation of box embeddings into Gaussian convolutions, improves optimization robustness in the disjoint case and maintains desirable properties of the original lattice. It shows increased performance on various tasks, especially in sparse data scenarios. The novel hierarchical embedding model improves optimization robustness in the disjoint case and shows increased performance on various tasks, especially in sparse data scenarios. Embedding methods have long been a key technique in machine learning, providing a natural way to convert semantic problems into geometric problems. Recent years have seen a resurgence in interest in structured or geometric representations for information retrieval, moving beyond traditional vector space models. These methods associate objects like images, words, and knowledge base concepts with complex geometric structures such as density functions, convex cones, or hyperrectangles. In recent years, there has been a resurgence in interest in structured or geometric representations for information retrieval. Instead of traditional vector space models, methods now associate objects with complex geometric structures like density functions, convex cones, or hyperrectangles. These geometric objects better express asymmetry, entailment, ordering, and transitive relations, providing a strong inductive bias for tasks. The focus is on the probabilistic Box Lattice model for its empirical performance in modeling transitive relations and probabilistic interpretation. The focus is on the probabilistic Box Lattice model of BID22, known for its strong empirical performance in modeling transitive relations and complex joint probability distributions. Box embeddings are a generalization of order embeddings and probabilistic order embeddings, replacing vector lattice ordering with overlapping boxes. The Box Lattice model of BID22, known for its strong empirical performance, uses BID20 and probabilistic order embeddings (POE) BID9 with overlapping boxes instead of vector lattice ordering. However, the \"hard edges\" of boxes can cause issues for gradient-based optimization, especially with sparse data. The Box Lattice model of BID22 uses overlapping boxes with hard edges, causing issues for gradient-based optimization with sparse data. To address this, a new model is proposed based on smoothing the edges into Gaussian density functions. The new model proposed in contrast to BID22's Box Lattice model addresses the issue of gradient-based optimization with sparse data by smoothing the hard edges of boxes into Gaussian density functions. This approach demonstrates superior performance in modeling transitive relations on various datasets and outperforms existing state-of-the-art results. The curr_chunk discusses the order embeddings of BID20 and the box lattice model of BID22, along with a hyperrectangle-based generalization proposed by BID18. These models aim to embed nonprobabilistic DAGs or lattices in vector spaces, showing improvements in the pseudosparse regime. The curr_chunk discusses the probabilistic extension of the box lattice model by BID9 and BID22, along with a hyperrectangle-based generalization proposed by BID18. These models focus on embedding points in hyperbolic space for learning hierarchical embeddings. The curr_chunk discusses nonprobabilistic models for learning hierarchical embeddings in hyperbolic space, which optimize an energy function. The negative curvature of hyperbolic space favors tree structures but may not be suitable for non-treelike DAGs. Our approach involves smoothing the energy landscape of models in hyperbolic space using Gaussian convolution, a technique common in optimization methods. This approach is being integrated into machine learning models like Mollifying Networks, diffusion-trained networks, and noisy activation functions. Our focus on embedding orderings and transitive relations is a subset of knowledge graph embedding, with a key difference being our probabilistic approach. Our focus is on embedding orderings and transitive relations in knowledge graph embedding. We seek to learn an embedding model mapping concepts to subsets of event space, tailored for transitive relations and fuzzy concepts of inclusion and entailment. We introduce methods for representing ontologies as geometric objects, utilizing order theory definitions and vector representations. The text discusses representing ontologies as geometric objects using order theory definitions and vector representations, focusing on embedding orderings and transitive relations in knowledge graph embedding. It introduces methods for this representation, including vector and box lattices, and defines a non-strict partially ordered set (poset) with reflexivity, antisymmetry, and transitivity properties. A poset is a binary relation with reflexivity, antisymmetry, and transitivity properties. It generalizes the concept of a totally ordered set to allow incomparable elements. Posets are useful for representing acyclic directed graph data with transitive relations. A lattice is a poset where any subset has a unique least upper bound and greatest lower bound. In a bounded lattice, there are additional elements denoting the least upper bound and greatest lower bound of the set, equipped with join and meet operations. In a bounded lattice, the set contains two additional elements, (top) and \u22a5 (bottom), representing the least upper bound and greatest lower bound. The lattice is equipped with join (\u2228) and meet (\u2227) operations. Bounded lattices must satisfy specific properties. The extended real numbers and sets partially ordered by inclusion form bounded lattices under certain operations. The extended real numbers and sets partially ordered by inclusion form bounded lattices under min and max operations. The dual lattice can be obtained by swapping \u2227 and \u2228 operations. A semilattice has either a meet or join operation. In the real numbers, this corresponds to a sign change. A vector lattice, also known as a Riesz space or Hilbert lattice, is a vector space with a lattice structure. The standard choice of partial order for the vector lattice R^n is the product order from the underlying real numbers, where meet and join operations are pointwise min and max. The Order Embeddings of BID20 represent partial orders as vectors using the reverse product order, creating a lattice structure. Objects are embedded as vectors that become more specific as they move away from the origin. FIG0 shows a two-dimensional example of this representation. The Order Embedding vector lattice representation of a simple ontology is demonstrated in FIG0. Vilnis et al. introduced a box lattice for knowledge graphs, where concepts are associated with minimum and maximum vectors forming hyperrectangles. This creates a natural partial order and lattice structure based on set inclusion between boxes. In a box lattice for knowledge graphs, each concept is linked to two vectors representing an axis-aligned hyperrectangle. The lattice structure is based on set inclusion between boxes, with a natural partial order. The box lattice structure involves maximum and minimum coordinates, with operations for least upper bounds and greatest lower bounds. The box lattice structure involves operations for least upper bounds and greatest lower bounds, with \u2228 and \u2227 denoting max and min when applied to scalar coordinates. The lattice meet is the largest box contained entirely within both x and y, while the lattice join is the smallest box containing both x and y. Marginal probabilities of events are given by the volume of boxes, their complements, and intersections under a suitable probability measure. The probability measure for events x is determined by the volume of boxes with interval boundaries. The use of the uniform measure requires boxes to be within the unit hypercube. Probability of an empty set is zero. Box embeddings in gradient-based optimization face challenges. When learning box embeddings with gradient-based optimization, a problem arises when two concepts are mistakenly labeled as disjoint, leading to zero gradient flow. The volume of the meet under the uniform measure is crucial for determining probabilities in box embeddings. The volume of the meet under the uniform measure is important for determining probabilities in box embeddings. When embedding a sparse lattice, a surrogate function is proposed to optimize in case of disjoint intervals, avoiding gradient flow issues. The authors propose a surrogate function to optimize in case of disjoint intervals in box embeddings, aiming to avoid gradient sparsity issues. They suggest developing alternate measures to improve optimization and final model quality, demonstrating the concept with a one-dimensional example in Figure 2. The authors propose a surrogate function to optimize in case of disjoint intervals in box embeddings to avoid gradient sparsity issues. They seek a relaxation of the assumption of \"hard edges\" in standard box embeddings for better optimization and geometric intuition. The results extend from representing boxes as products of intervals and volumes under associated measures. In this section, the authors discuss representing boxes as indicator functions of intervals and rewriting the joint probability measure using convolution with kernel smoothing to address gradient sparsity issues. The authors propose using kernel smoothing with Gaussian kernels to replace indicator functions with infinite support, addressing gradient sparsity issues in representing boxes as intervals. This approach is demonstrated in Figure 2 for one dimension. The authors propose using kernel smoothing with Gaussian kernels to replace indicator functions with infinite support, addressing gradient sparsity issues in representing boxes as intervals. This approach is demonstrated in one dimension in Figure 2. Specifically, given x = [a, b], we associate the smoothed indicator function. The integral admits a closed form solution, where the solution is given by a specific formula involving the softplus function and the logistic sigmoid antiderivative. The antiderivative of the standard normal CDF is given by a specific formula involving the softplus function and the logistic sigmoid antiderivative. In the zero-temperature limit, as \u03c1 goes to zero, the formula converges to the original equation, expected from convolution with a zero-bandwidth kernel. The formula DISPLAYFORM7 is recovered with equality in the last line for intervals (a, b) and (c, d). This is expected from convolution with a zero-bandwidth kernel. However, multiplication of Gaussian-smoothed indicators does not give a valid meet operation on a function lattice due to violating the idempotency requirement. For practical considerations, treating the outputs of p \u03c6 as probabilities complicates applications that train on conditional probabilities. By modifying equation 3, a function p can be obtained such that p(x \u2227 x) = p(x), maintaining the smooth optimization properties of the Gaussian model. By modifying equation 3, a function p can be obtained such that p(x \u2227 x) = p(x), while maintaining the smooth optimization properties of the Gaussian model. This identity holds true for the hinge function m h and two intervals (a, b) and (c, d), but not for the softplus function. However, a similar functional form as equation 6 is true for both the hinge function and the softplus. The commutativity of min and max with monotonic functions applies to two intervals x = (a, b) and y = (c, d). In the zero-temperature limit, equations 3 and 7 are equivalent, but equation 7 is idempotent for overlapping intervals (a, b) and (c, d), while equation 3 is not. This leads to defining probabilities using equation 7 instead of equation 3. In the zero-temperature limit, equations 3 and 7 are equivalent, but equation 7 is idempotent for overlapping intervals. This inspires the definition of probabilities using a normalized version of equation 7. Softplus upper-bounds the hinge function, allowing output values greater than 1, requiring normalization in experiments with different approaches. In experiments with different approaches, probabilities are defined using a normalized version of equation 7 to handle output values greater than 1. Two normalization methods are used for data with a small number of entities, while a unit hypercube projection is used for data where computing global minimum and maximum values is infeasible. The final probability is calculated as a product over dimensions. The final probability p(x) is calculated as a product over dimensions, using a function by m soft. This function, while similar to the standard uniform probability measure in the zero temperature limit, is not a valid probability measure on the entire joint space of events. Our approach retains the inductive bias of the original box model and satisfies the necessary condition that p(x, x) = p(x). Our approach retains the inductive bias of the original box model, equivalent in the limit, and satisfies the necessary condition that p(x, x) = p(x). A comparison of 3 different functions in FIG2 shows that the softplus overlap performs better for highly disjoint boxes than the Gaussian model, preserving the meet property. Experiments on the WordNet hypernym prediction task evaluate these improvements in practice. The Gaussian model lowers its temperature, causing vanishing gradients. Experiments on the WordNet hypernym prediction task evaluate the performance of these improvements. The WordNet hypernym hierarchy contains 837,888 edges. Positive examples are randomly chosen, while negative examples are generated by swapping terms. The smoothed box model performs nearly as well as the original box lattice in terms of test accuracy. The smoothed box model performs nearly as well as the original box lattice in terms of test accuracy, with less hyper-parameter tuning required. Further experiments are conducted to explore performance in a higher degree of sparsity. In further experiments, the smoothed box model is compared with the original box lattice and order embeddings using different numbers of positive and negative examples from the WordNet mammal subset. The training data consists of 1,176 positive examples, while the dev/test sets have 209 positive examples, with negative examples generated randomly. The training data for the mammal WordNet subset includes 1,176 positive examples, while the dev/test sets have 209 positive examples. Negative examples are generated randomly. Models like OE baseline, Box, and Smoothed Box perform well with balanced data, but Smoothed Box outperforms the others on imbalanced data. This is crucial for real-world entailment graph learning. The Smoothed Box model outperforms OE and Box on imbalanced data, crucial for real-world entailment graph learning. Experiments conducted on the WordNet mammal subset and Flickr entailment dataset show superior performance. Experiments on the Flickr entailment dataset, a large-scale dataset with 45 million image caption pairs, were conducted using the WordNet mammal subset. The study used the same dataset as BID22 for comparison, with constraints on box dimensions and the application of the softplus function. Results were reported for KL divergence and Pearson correlation on various subsets of the test data. The study conducted experiments on the Flickr entailment dataset using the WordNet mammal subset. Results showed a slight performance gain compared to the original model, with improvements focused on unseen captions. The method was also applied to a market-basket task using the MovieLens dataset to predict users' movie preferences. The task involves predicting users' preference for movie A based on their liking of movie B using the MovieLens dataset. Pairs of user-movie ratings higher than 4 points are collected and pruned to a subset of movies with more than 100 user ratings. This results in 8545 movies in the dataset. The conditional probability P(A|B) is calculated and compared with various baselines including low-rank matrix factorization and hierarchical embedding methods. The conditional probability P(A|B) is calculated and compared with baselines such as low-rank matrix factorization and hierarchical embedding methods. Separate embeddings for target and conditioned movies are used due to the asymmetric training matrix. The complex bilinear model includes an additional vector of parameters to capture the \"imply\" relation. Evaluation metrics include KL divergence, Pearson correlation, and Spearman correlation with ground truth probabilities. The results show that the smoothed box embedding method outperforms other methods. The experimental results in TAB4 show that the smoothed box embedding method outperforms the original box lattice and other baselines, especially in Spearman correlation, which is crucial for recommendation tasks. Additionally, a study on the robustness of the smoothed model to initialization conditions is presented in Appendix C. The approach to smoothing the energy and optimization landscape of probabilistic box embeddings is explained with theoretical justification. In Appendix C, an additional study on the robustness of the smoothed model to initialization conditions is presented. The approach to smoothing the energy and optimization landscape of probabilistic box embeddings is discussed, showing improved performance on various datasets, especially with sparse data and poor initialization. This model requires fewer hyper-parameters, making it easier to train and achieving state-of-the-art results. The study demonstrates the effectiveness of a model for sparse data and its robustness to poor initialization. The research addresses learning challenges posed by geometrically-inspired embedding models, with a focus on complex embedding structures like unions of boxes. The exploration of function lattices and constraint-based learning approaches will continue. The research explores function lattices and constraint-based learning approaches to address learning challenges posed by geometrically-inspired embedding models, focusing on complex embedding structures like unions of boxes. The study also demonstrates the effectiveness of a model for sparse data and its robustness to poor initialization. The Gaussian overlap formula is evaluated for two lattice elements x and y with associated smoothed indicators f and g. The antiderivative of \u03c6 is the normal CDF, represented as the difference \u03a6(x; a, \u03c3 2 ) \u2212 \u03a6(x; b, \u03c3 2 ). To evaluate equation 8, an identity is recalled, and Fubini's theorem is applied. The MovieLens dataset, with a large proportion of small probabilities, is suitable for optimization by the smoothed model. The distribution of probabilities is shown in buckets of width 0.1 in FIG0. The MovieLens dataset is suitable for optimization by the smoothed model due to its large proportion of small probabilities. Additional experiments were conducted to test the model's robustness to initialization, specifically focusing on the overlap of intervals within the boxes. The distribution of probabilities is illustrated in buckets of width 0.1 in FIG0. The study focuses on determining the model's robustness to disjoint boxes by adjusting the width parameter of the initial distribution of boxes. Results show different percentages of disjoint boxes at initialization before learning on the MovieLens dataset. The smoothed model does not appear to suffer from this condition. The study presents results on adjusting the width parameter of box distributions to create disjoint boxes before learning on the MovieLens dataset. The smoothed model shows no degradation in performance compared to the original box model, indicating its ability to optimize smoothly in the disjoint regime. Detailed methodology and hyperparameter selection methods are provided for each experiment. The smoothed box model optimizes smoothly in the disjoint regime. Methodology and hyperparameter selection details for each experiment can be found at https://github.com/Lorraine333/smoothed_box_embedding. WordNet experiments evaluate the model on the development set every epoch for a fixed number of epochs. Baseline models use BID22 parameters, while the smoothed model uses hyperparameters determined on the development set. The experimental setup involves training baseline models with BID22 parameters and smoothed models with hyperparameters determined on the development set. Negative examples are randomly generated based on the ratio for each batch of positive examples. A parameter sweep is done for all models to select the best result for each model. The architecture used is a single-layer LSTM that reads captions and produces a box embedding parameterized by min and delta. The experimental setup involves training models with a single-layer LSTM architecture that reads captions and produces box embeddings. A parameter sweep is done for all models to select the best result. Hyperparameters are determined on the development set, and the model is evaluated every 50 steps on the development set for MovieLens experiments. The best development model is used to report test set score, with hyperparameters determined on the development set. Evaluation is done every 50 steps on the development set for MovieLens experiments, and optimization stops if no improvement is seen after 200 steps."
}