{
    "title": "BJInEZsTb",
    "content": "In this paper, a deep autoencoder network is introduced for studying representation learning and generative modeling of geometric data represented as point clouds. The learned representations outperform the state of the art in 3D recognition tasks and enable shape editing applications. Thorough study of different generative models including GANs and Gaussian mixture models is also conducted. The study explores various generative models like GANs and Gaussian mixture models for geometric data represented as point clouds. Gaussian mixture models trained in the latent space of autoencoders produce samples with the best fidelity and diversity. The study evaluates generative models for geometric data using simple measures of fidelity and diversity based on matching point clouds. Different 3D representations like view-based projections, volumetric grids, and graphs are discussed, highlighting their effectiveness in various domains but limitations in semantics. The study discusses various 3D representations such as view-based projections, volumetric grids, and graphs, noting their effectiveness in different domains but limitations in semantics. High-dimensional encodings like 3D meshes and CAD models are often inadequate for generative models due to their lack of semantic information, requiring complex parametric models for object design and manipulation. Recent advances in deep learning have revolutionized the design of generative models, eliminating the need for complex parametric models. Deep learning tools like autoencoders and Generative Adversarial Networks are successful at learning complex data without the need for hand-crafted features. Recent advances in deep learning have eliminated the need for hand-crafting features and models. Deep learning architectures like autoencoders and Generative Adversarial Networks are successful at learning complex data representations and generating realistic samples. Point clouds, a relatively unexplored 3D modality, are the focus of this paper. Point clouds are a relatively unexplored 3D modality that provide a compact representation of surface geometry. They are commonly generated by devices like the Kinect and iPhone's face identification feature. Only a few deep architectures, like PointNet, exist for 3D point clouds in the literature. Generative models for point clouds have gained attention in the deep learning community. Few deep architectures exist for 3D point clouds, such as PointNet BID17 and BID14. Fan et al. (2016) used point clouds to extract 3D information from 2D images. This study focuses on learning representations and generative models for point clouds. Generative models for point clouds have gained attention in the deep learning community, with a focus on learning representations and generative models for point clouds using deep architectures. Evaluating generative models is challenging due to the difficulty in training GAN-based pipelines and the lack of a standardized evaluation method. Fidelity and coverage are key aspects in evaluating generative models. Generative models for point clouds are challenging to evaluate due to the lack of a standardized method. Fidelity and coverage are crucial in assessing these models. The authors propose new methods to address training and evaluation issues in their domain, including a novel AE architecture inspired by recent classification architectures. The authors propose a new AE architecture for generative models of point clouds, capable of learning compact representations with excellent reconstruction quality. These representations improve classification and allow for meaningful interpolations. They also create generative models that can generate point clouds similar to training and test data, providing good coverage of the dataset. The authors introduce generative models for point clouds that can generate data similar to the training and test datasets. They propose a workflow that involves training an AE with a compact bottleneck layer to learn a representation, followed by training a GAN in that fixed latent representation. This approach is supported by theory and shown to be empirically effective, making latent GANs easier to train than raw GANs. Latent GANs are easier to train than raw GANs and achieve superior reconstruction with better coverage. GMMs trained in the latent space of fixed AEs perform the best. Multi-class GANs work almost as well as dedicated GANs trained per-object category in the latent space. Various metrics are carefully studied to support the qualitative evaluation. Multi-class GANs perform well when trained in the latent space, comparable to dedicated GANs per-object category. Various metrics are evaluated for learning good representations and evaluating generated samples, including proposing fidelity and coverage metrics. Chamfer distance metric may not effectively distinguish between pathological and good examples. The paper proposes fidelity and coverage metrics for generative models, in addition to the Chamfer distance metric. Sections 2 and 3 introduce the necessary background and models for latent representations and point cloud generation. Section 4 evaluates the models quantitatively and qualitatively, with further results in the appendix. In Section 4, the models are evaluated quantitatively and qualitatively, with further results available in the appendix. The code for all models is publicly accessible. Autoencoders aim to reproduce input data and create a low-dimensional representation in a bottleneck layer. Autoencoders are useful with a bottleneck layer for data compression. The Encoder compresses data into a latent representation, while the Decoder reproduces the data. Generative Adversarial Networks (GANs) are state-of-the-art models with a generator and discriminator playing an adversarial game to create realistic data samples. Generative models involve an adversarial game between a generator and discriminator to create realistic data samples. The generator synthesizes samples to look like real data, while the discriminator distinguishes between synthesized and real samples. The most commonly used losses for the networks are based on parameters \u03b8 (D) and \u03b8 (G). The improved Wasserstein GAN is used for enhanced stability during training. The improved Wasserstein GAN is utilized for stability during training in generative models. Point clouds pose unique challenges due to the lack of grid-like structure, making them harder to encode. The convolution operator used in image processing requires a grid-like structure, which is lacking in raw point clouds, making them difficult to encode. Point clouds are unordered, complicating comparisons between sets of points. The issue with point clouds is that they are unordered, making comparisons between sets difficult. Two permutation-invariant metrics have been proposed for comparing unordered point sets. The Earth Mover's distance (EMD) and Chamfer distance (CD) are two metrics used for comparing unordered point sets. EMD transforms one set to another through a transportation problem, while CD measures the squared distance between points and their nearest neighbors. EMD is differentiable almost everywhere, while CD is more computationally efficient. The Chamfer distance (CD) measures the squared distance between points in one set to their nearest neighbor in the other set, making it more computationally efficient. It is used for evaluating representations and generative models by comparing reconstructed or synthesized point clouds to their ground truth counterparts. This comparison helps assess the quality of a model in terms of matching training or test sets, evaluating faithfulness, diversity, and potential mode-collapse in generative models. To evaluate the quality of a representation model, metrics like Coverage are used to measure how well a point-cloud distribution matches a ground truth distribution. Coverage is calculated by finding the closest neighbor in the ground truth for each point-cloud in the distribution, using either Chamfer distance (CD) or Earth Mover's Distance (EMD). A high coverage score indicates a good representation of the ground truth distribution. Coverage in evaluating representation models can be measured using either CD or EMD, resulting in COV-CD and COV-EMD metrics. High coverage indicates a good match between point-cloud distributions. Fidelity is assessed by matching each point cloud in G to the closest one in A using MMD, with distances reported as MMD-CD and MMD-EMD. MMD measures the realism of the matched elements in A. The Jensen-Shannon Divergence (JSD) measures the similarity between point clouds A and B in 3D space by counting points within each voxel. In this section, the architectures of representation and generative models for point clouds are described, starting with an autoencoder design. A GAN architecture tailored to point-cloud data is introduced, followed by a more efficient pipeline that learns an AE and then trains a smaller GAN in the latent space, along with a simpler generative model. The text describes models for point clouds, including an autoencoder design, a GAN architecture for point-cloud data, and a simpler generative model based on Gaussian Mixtures. The input to the autoencoder network is a point cloud with 2048 points representing a 3D shape. The encoder architecture uses 1-D convolutional layers with increasing features and a symmetric function to encode each point independently. The BID17 principle involves using 1-D convolutional layers with increasing features and a symmetric function to encode each point independently in a point cloud model. The implementation includes 5 1-D conv layers with ReLU and batch-norm layers, producing a k-dimensional vector for the latent space. The decoder consists of 3 fully connected layers to generate a 2048 \u00d7 3 output. The decoder transforms the latent vector with 3 fully connected layers to produce a 2048 \u00d7 3 output. For a permutation invariant objective, two distinct AE models, AE-EMD and AE-CD, are explored using efficient EMD-distance approximation and Chamfer-Distance as structural losses. The study explored different bottleneck sizes for autoencoders (AEs) and found that k = 128 had the best generalization error on test data. The raw point cloud GAN (r-GAN) was the first to operate directly on a 2048 \u00d7 3 point set input. The first version of the generative model, r-GAN, works on raw 2048 \u00d7 3 point set input and is the first GAN for point clouds. The discriminator architecture is similar to AE, with leaky ReLUs and a sigmoid neuron in the output layer. The generator uses a 128-dimensional noise vector to produce a 2048 \u00d7 3 output through 5 FC-ReLU layers. In the latent-space GAN (l-GAN), a different approach is taken. The l-GAN uses a pre-trained autoencoder on the input data, operating on a 128-dimensional bottleneck variable for both the generator and discriminator. The generator output is decoded to a point cloud after GAN training. The l-GAN utilizes a pre-trained autoencoder on a 128-dimensional bottleneck variable for both the generator and discriminator. The generator output is decoded to a point cloud post GAN training. The architecture for the l-GAN is simpler compared to the r-GAN, with shallow designs for the generator and discriminator. Additionally, Gaussian Mixture Models are trained on the latent spaces learned by the autoencoders. The study utilized Gaussian Mixture Models trained on latent spaces learned by autoencoders to generate point clouds. The GMMs were experimented with varying numbers of components and covariance matrices. The GMMs can be used as point-cloud generators by sampling the latent space and using the autoencoder's decoder. The study used Gaussian Mixture Models trained on latent spaces learned by autoencoders to generate point clouds. Sampling the latent space from the GMM distribution and using the autoencoder's decoder, similar to l-GANs, was done to reconstruct unseen shapes. Shapes were sourced from the ShapeNet repository and pre-centered into a sphere of diameter 1. Specific per-class models were trained with an 85%-5%-10% split for training/testing/validation sets. Classification was used to evaluate the quality of unsupervised representation learning algorithms. The study evaluated unsupervised representation learning algorithms by training specific per-class models on ShapeNet models from 55 categories of man-made objects. The autoencoder was trained across all shape categories and used to compute latent features for linear model evaluation. The study evaluated unsupervised representation learning algorithms by training per-class models on ShapeNet models from 55 categories of man-made objects. The autoencoder was trained across all shape categories, using a bigger bottleneck of 512 and applying batch-norm to the decoder. Features for input 3D shapes were extracted by feeding forward the point-cloud to the network and processing the 512-dimensional bottleneck layer vector. Linear classification SVM was used for evaluation on the ModelNet BID32 benchmark, showing comparative results in Table 1. The study evaluated unsupervised representation learning algorithms by training per-class models on ShapeNet models from 55 categories of man-made objects. Features for input 3D shapes were extracted by feeding forward the point-cloud to the network and processing the 512-dimensional bottleneck layer vector. Linear classification SVM was used for evaluation on the ModelNet BID32 benchmark, showing comparative results in Table 1. The 512-dimensional feature is more intuitive and parsimonious compared to the previous state of the art BID31, which uses several layers of a GAN to derive a 7168-long feature. The study evaluated unsupervised representation learning algorithms on ShapeNet models, extracting features from 3D shapes. The AE loss affects learned features, with CD performing better on collections with increased variation. The experiment demonstrates domain-robustness of learned features. Qualitative evaluation visually assesses the quality of the learned representation. The experiment demonstrates the domain-robustness of learned features through qualitative evaluation using reconstruction results to assess the quality of the learned representation. The learned representation in the experiment shows the ability to generalize to unseen shapes, enabling shape editing applications like interpolations, part editing, and analogies. Reconstruction quality is highlighted through qualitative evaluation and quantitative measurements. Our AEs can reconstruct unseen shapes, as shown in results and quantitative measurements. Five generative models were trained on chair point-cloud data, including AEs with 128-dimensional bottleneck trained with CD or EMD loss. The study involved training various generative models on chair point-cloud data, including AEs with 128-dimensional bottleneck trained with CD or EMD loss. GANs were trained in the latent spaces of the AEs, with additional models using Wasserstein objective and GMMs. An r-GAN was also trained directly on the point cloud data, with all GANs trained for a maximum of 2000 epochs. The study involved training various generative models on chair point-cloud data, including AEs with 128-dimensional bottleneck trained with CD or EMD loss. GANs were trained in the latent spaces of the AEs, with additional models using Wasserstein objective and GMMs. An r-GAN was also trained directly on the point cloud data, with all GANs trained for a maximum of 2000 epochs. Model selection was based on how well synthetic results matched the ground-truth distribution using JSD or MMD-CD metrics. The study involved training various generative models on chair point-cloud data, including AEs with 128-dimensional bottleneck trained with CD or EMD loss. GANs were trained in the latent spaces of the AEs, with additional models using Wasserstein objective and GMMs. An r-GAN was also trained directly on the point cloud data, with all GANs trained for a maximum of 2000 epochs. Model selection was based on how well synthetic results matched the ground-truth distribution using JSD or MMD-CD metrics. To compare the synthetic dataset to the validation dataset, JSD or MMD-CD metrics were used instead of MMD-EMD due to computational cost. Model selection was done every 100 epochs (50 for r-GAN), with the optimal number of Gaussian components for GMM determined to be 32. GMMs performed better with full covariance. The study involved training various generative models on chair point-cloud data, including AEs with 128-dimensional bottleneck trained with CD or EMD loss. GANs were trained in the latent spaces of the AEs, with additional models using Wasserstein objective and GMMs. Model selection was based on how well synthetic results matched the ground-truth distribution using JSD or MMD-CD metrics. GMMs performed much better with full covariance matrices, suggesting strong correlations between latent dimensions. The optimal number of Gaussians for GMM was determined to be 32. The study involved training various generative models on chair point-cloud data, with GMMs performing better with full covariance matrices. The optimal number of Gaussians for GMM was determined to be 32. Evaluating 5 generators on train-split of chair dataset showed an average classification score of 84.7%. Quantitative evaluation compared models based on their capacity to generate synthetic samples resembling the ground truth distribution. The study evaluated generative models on chair point-cloud data, with GMMs performing better with full covariance matrices. The optimal number of Gaussians for GMM was found to be 32. The models were compared based on their ability to generate synthetic samples resembling the ground truth distribution, with results reported in Table 2. The average classification probability for samples being recognized as a chair using the PointNet classifier BID17 was also shown. The study evaluated generative models on chair point-cloud data, with GMMs performing better with full covariance matrices. Results of the train split are reported in Table 2, along with the average classification probability for samples recognized as chairs using the PointNet classifier BID17. A similar experiment was conducted on synthetic samples to match the test split dataset, with results averaged over three pseudo-random repetitions. The study evaluated generative models on chair point-cloud data, with GMM-32-F representing a GMM with 32 Gaussian components with full covariances. The experiment was repeated with three pseudo-random seeds, and the average measurements were reported in TAB10 for various comparison metrics. Training a Gaussian mixture model in the latent space of the EMD-based AE yielded the best results in terms of fidelity and coverage. The achieved fidelity and coverage were close to the reconstruction baseline, as shown in TAB5 of the appendix. The study evaluated generative models on chair point-cloud data, with GMM-32-F representing a GMM with 32 Gaussian components with full covariances. Training a Gaussian mixture model in the latent space of the EMD-based AE yielded the best results in terms of fidelity and coverage. The achieved fidelity and coverage were very close to the reconstruction baseline, as shown in TAB5 of the appendix. Comparing TAB10 shows the generalization ability of the models, with comparable performance for training vs. testing splits. The study evaluated generative models on chair point-cloud data, with GMM-32-F representing a GMM with 32 Gaussian components with full covariances. Training a Gaussian mixture model in the latent space of the EMD-based AE yielded the best results in terms of fidelity and coverage. The generalization ability of the models was highlighted by comparing TAB10, showing comparable performance for training vs. testing splits. The number of synthetic point clouds generated for the train split experiment was equal to the size of the train dataset, while for the test split experiment and model selection, synthetic datasets three times bigger than the ground truth dataset were generated to reduce sampling. For model selection, synthetic datasets three times bigger than the ground truth dataset are generated to reduce sampling bias when measuring MMD or Coverage statistics. The MMD-CD distance to the test set for r-GANs appears small in TAB10, but qualitative inspection reveals inadequacy of chamfer distance in distinguishing pathological cases. The r-GANs appear small in size, but qualitative inspection shows the inadequacy of chamfer distance in distinguishing pathological cases. Examples are showcased in Fig. 3 with two triplets of images comparing r-GANs and l-GANs generating synthetic point clouds. The study compares r-GANs and l-GANs in generating synthetic point clouds, showing the limitations of chamfer distance in distinguishing between them. The r-GAN results are of lesser quality due to generating clouds with many points concentrated in specific areas. The study highlights the limitations of chamfer distance (CD) in distinguishing between r-GAN and l-GAN results. r-GANs generate point clouds with concentrated points in specific areas, leading to lower quality results. This results in CD metrics being \"blind\" to partial matches, causing CD-based coverage metrics to be consistently larger than those reported by Earth Mover's Distance (EMD). The study discusses the limitations of chamfer distance (CD) in distinguishing between r-GAN and l-GAN results. The EMD distance promotes a one-to-one mapping and correlates more strongly to visual quality, heavily penalizing the r-GAN result in terms of MMD and coverage. Extensive measurements were performed during training to understand the models' behavior. During training, extensive measurements were conducted to understand the behavior of the models. The r-GAN struggled to provide good coverage of the test set, indicating the difficulty in training end-to-end GANs. In contrast, the l-GAN (AE-CD) performed better in terms of MMD and coverage. The r-GAN struggles to cover the test set, a common challenge for end-to-end GANs. The l-GAN (AE-CD) performs better in terms of fidelity with fewer epochs, but its coverage remains low due to the CD promoting unnatural topologies. Switching to an EMD-based AE for the representation in the synthesized point sets results in a dramatic improvement in coverage and fidelity compared to the previous AE-CD approach. Both l-GANs still face mode collapse issues, with coverage dropping significantly during training. The text discusses the issue of mode collapse in GANs on point-cloud data, with coverage dropping during training. Switching to a latent WGAN helps eliminate this collapse. Comparisons are made to voxel-based methods, showing improvements in coverage and fidelity. The models are also compared to a voxel-grid based approach in terms of JSD on the training set of the chair category. The text compares GANs on point-cloud data to a voxel-grid based approach in terms of JSD on the training set of the chair category. The voxel grid output is converted into a point-set with 2048 points for comparison. The authors compared GANs on point-cloud data to a voxel-grid based approach in terms of diversity and classification performance. The r-GAN slightly outperformed BID31 in diversity, while the l-GANs performed even better in both classification and diversity with fewer training epochs. Training time for the l-GAN was significantly shorter than for the r-GAN. The l-GANs outperformed the r-GAN in terms of classification and diversity with fewer training epochs. Training time for the l-GAN was significantly shorter due to its smaller architecture. High-quality synthetic results were produced by both models, showcasing the strength of the learned approach. In Fig. 5, synthetic results from l-GANs and a 32-component GMM trained on AE-EMD latent space are shown. Both models produce high-quality results, with l-GANs generating crisper shapes compared to r-GAN. The use of a good structural loss on the pre-trained AE benefits the l-GAN results. The appendix contains components and results using l-GAN and r-GAN. l-GAN produces clearer results than r-GAN, showing the advantage of using a good structural loss on the pre-trained AE. Synthetic point clouds were generated by l-GAN and a 32-component GMM trained on AE-EMD latent space. Experiments were also conducted with an AE-EMD trained on a mixed set of point clouds from 5 categories. The study conducted experiments with an AE-EMD trained on a mixed set of point clouds from 5 categories. The multi-class AE was trained for 1000 epochs and compared against class-specific AEs trained for 500 epochs. Additionally, six l-WGANs were trained on top of the AEs. The study compared a multi-class AE-EMD trained for 1000 epochs against class-specific AEs trained for 500 epochs. Additionally, l-WGANs were trained on top of all AEs for 2K epochs, showing similar performance in fidelity/coverage. Visual quality comparison also indicated minimal sacrifice with the multi-class AE-EMD. The study found that l-WGANs based on multi-class AE perform similarly to class-specific ones. Visual quality comparison showed minimal sacrifice with the multi-class AE-EMD. Limitations were shown in some failure cases of the models. The study found that l-WGANs based on multi-class AE perform similarly to class-specific ones, with minimal sacrifice in visual quality. Limitations include failure cases where chairs with rare geometries are not faithfully decoded, high-frequency details may be missed, and r-GAN struggles to create realistic shapes for certain classes like cars. Designing more robust raw-GANs for point clouds is an interesting avenue. The r-GAN struggles to create realistic shapes for certain classes like cars, despite being easily recognizable for chairs. Designing more robust raw-GANs for point clouds is an interesting avenue for future work. Training Gaussian mixture models in the latent space of an autoencoder is closely related to VAEs, with documented issues of over-regularization affecting reconstruction quality. In the context of VAEs, over-regularization can impact reconstruction quality. Methods exist to gradually increase the regularizer weight. Fixing the AE before training generative models can yield good results. Novel architectures for 3D point-cloud representation learning and generation were presented, showing good generalization and meaningful semantics in representations. Generative models can produce faithful outputs. The novel architectures for 3D point-cloud representation learning and generation yield good results, with generative models able to produce faithful samples and cover ground truth distribution without memorization. The best-performing generative model is a GMM trained in the fixed latent space of an AE, suggesting that simple classic tools should not be dismissed. In experiments, a GMM trained in the fixed latent space of an AE showed promising results, indicating the effectiveness of simple classic tools. Further investigation into the power of latent GMMs compared to adversarially trained models would be valuable. The AE used had specific encoder and decoder configurations with batch normalization between layers, and online data augmentation was applied to input point-clouds. The AE used specific encoder and decoder configurations with batch normalization between layers and online data augmentation. The encoder had varying filters in each layer, while the decoder consisted of FC-ReLU layers with different numbers of neurons. The AEs were trained for different numbers of epochs based on the experiment conducted. The decoder in the AE model had 3 FC-ReLU layers with different numbers of neurons. Training lasted up to 500 epochs for single class data and 1000 epochs for 5 shape classes. Different AE setups did not show significant advantages over the basic architecture. Dropout layers led to worse reconstructions, while using batch-norm on the encoder only improved generalization error. The discriminator's initial layers were 1D-convolutions with specific stride/kernel sizes. The r-GAN model used batch-norm on the encoder only, resulting in better generalization error when trained with single-class data. The discriminator had 1D-convolutions with specific stride/kernel sizes and leaky-ReLU layers. The generator consisted of FC-ReLU layers with varying numbers of neurons. Training was done with Adam optimizer and a noise vector was used. The generator in the r-GAN model consists of 5 FC-ReLU layers with varying numbers of neurons. The discriminator has 2 FC-ReLU layers and a final FC layer with a single sigmoid neuron. The noise vector used is drawn from a spherical Gaussian with 128 dimensions. When using l-Wasserstein-GAN, a gradient penalty regularizer is applied. The generator in the r-GAN model has 5 FC-ReLU layers with varying numbers of neurons, while the discriminator has 2 FC-ReLU layers and a final FC layer with a single sigmoid neuron. When using l-Wasserstein-GAN, a gradient penalty regularizer is applied with \u03bb = 10. Training parameters and noise distribution are the same as those used for the r-GAN. For classification experiments, a one-versus-rest linear SVM classifier with an l2 norm penalty and balanced class weights is used. In the experiments, a one-versus-rest linear SVM classifier with an l2 norm penalty and balanced class weights was used. Table 5 shows the training parameters for SVMs with different structural loss models. The C-penalty controls the trade-off between margin size and misclassification rate, intercept adds an extra dimension to center input features, and loss refers to the optimization loss function of the SVM. The AE-EMD trained across all 55 object classes shows comparable reconstruction quality between training and test datasets, indicating generalization ability for shape editing applications. The embedding learned with AE-EMD is used, not separately per-category. The AE-EMD trained across all 55 object classes demonstrates comparable reconstruction quality between training and test datasets, showcasing its ability to generalize. The embedding learned with AE-EMD is utilized for shape editing applications, enabling interesting applications involving different shapes. Examples include tuning car appearance towards convertibles, adding armrests to chairs, and removing handles from mugs. Editing parts in point clouds using vector arithmetic on the AE latent space allows for shape modifications guided by shape annotations. Objects can be subdivided into categories with structural differences represented in the latent space. By using latent representations, structural differences between objects A and B can be modeled. Transforming the latent representation of object A can change its property, as shown in Figure 8. Chairs with armrests are on average 13% smaller than those without. By interpolating between latent representations of different shapes, we can create intermediate variants. This allows for morphing between shapes by removing and merging parts. The latent representation enables morphing between shapes by removing and merging parts, creating intermediate variants and shape analogies. The latent space allows for shape morphing and finding shape analogies through linear manipulations and nearest-neighbor searching in geometry processing. Preliminary results of point-cloud generators are included. In this section, preliminary results of point-cloud generators working with voxel-based AEs are presented. The latent space was learned by an AE using occupancy grids of 3D shapes, and a full-GMM model with 32 centers was used for generation on ShapeNet's chair class at two different grid resolutions. The latent space was learned by an AE using occupancy grids of 3D shapes. Generation was done with a full-GMM model with 32 centers on ShapeNet's chair class at two grid resolutions. Voxel-grids were converted into 2048 points for comparison with other point-cloud generators. Comparison was also made with Wu et al.'s voxel-based GANs. For more details and quantitative results, refer to Table 7. The latent AE-based GMM models outperform Wu et al.'s voxel-based GAN architecture by a large margin, showing an advantage of using latent representations for shape analogies. The voxel GMM provides a significant improvement over the raw voxel GAN architecture, indicating the advantage of using latent representations for generation in the voxel modality. The performance of the 64 3 voxel-based GMM is comparable to the one operating at 32 3 resolution, suggesting that high-frequency details in the ground-truth data do not significantly affect fidelity. Point-cloud-based models outperform voxel-based models in terms of fidelity between output and ground truth. The fidelity of results is not affected by high-frequency details in ground-truth data. Point-cloud-based models outperform voxel-based models in fidelity to ground truth. Voxel-based models may have a coverage boost due to how the coverage metric is computed, matching all shapes regardless of quality. The voxel-based models may artificially increase coverage by matching poor quality shapes to ground truth, as shown in the histogram. The voxel-based method shows a heavier \"tail\" in the ground truth, indicating poor quality matchings. Qualitative inspection confirmed that the covering mostly came from very poor quality partial shapes. The voxel-based output was evaluated using MMD and Coverage metrics at different resolutions against the chair test set. The methods used resolutions 32 3, 64 3, and 128 3, with GMMs and mesh conversion using marching cubes algorithm. Voxel-based AEs were fully-convolutional with specific layer parameters listed. The study utilized voxel-based AEs with specific layer parameters for encoding and decoding. The model was trained for 100 epochs using Adam optimizer under the binary setting. The study trained voxel-based autoencoders for 100 epochs using Adam optimizer with specific layer parameters for encoding and decoding. The models were compared in terms of reconstruction quality to a state-of-the-art method on the ShapeNetCars dataset. The study compared the reconstruction quality of a dense voxel-based AE with a state-of-the-art method on the ShapeNetCars dataset. The comparison involved measuring the intersection-over-union between input and synthesized voxel grids. The study also compared a GMM-generator against a model that memorizes training data for the chair class. Different training set sizes were considered for this comparison. The study compared the reconstruction quality of a GMM-generator with a model that memorizes training data for the chair class. Different training set sizes were considered for this comparison, showing slightly lower coverage/fidelity for the generative model compared to the memorized sets. This validates the metrics used in the evaluation. The study compared the reconstruction quality of a GMM-generator with a model that memorizes training data for the chair class. The generative model showed slightly lower coverage/fidelity compared to the memorized sets, validating the evaluation metrics used. The advantage of using a learned representation lies in learning the structure of the underlying space, enabling compact data representation and generating novel shapes. Some mode collapse was present in the generative results, with a 10% drop in coverage, but the achieved MMD of the generative models was almost identical to that of the memorized sets. The study compared the reconstruction quality of a GMM-generator with a model that memorizes training data for the chair class. While some mode collapse was present in the generative results, the achieved MMD of the generative models was almost identical to that of the memorized sets. Additional comparisons with BID32 for ShapeNet classes are provided in Tables 10, 11, and 12. The authors have publicly shared their models, including JSD-based comparisons for two models in Table 10 and MMD/Coverage comparisons in TAB12. The comparison involves BID31 and their generative models, with GMM/32 representing a GM model trained on the latent space of their AE with EMD structural loss. Generalization error of various GAN models is shown in FIG0, estimating closeness to training and test data using JSD and MMD-CD metrics. The generalization error of various GAN models is measured using JSD and MMD-CD metrics in FIG0. GMM model selection shows that models with a full covariance matrix achieve smaller JSD than those with diagonal covariance. Models with full covariance achieve smaller JSD than those with diagonal covariance. 30 or more clusters are sufficient for minimal JSD. The covariance matrix of a Gaussian component is shown in pseudocolor. Evaluation of five generators on chair data using minimal MMD-CD on validation-split. The GMM fitted to latent codes is evaluated using five generators on chair data. Models with 40 Gaussian components with full covariances are compared, showing consistent quality regardless of the selection metric used. Synthetic point-cloud datasets are generated and compared to the ground truth dataset. The GMM-40-F model is evaluated using synthetic point-cloud datasets that are 3x the size of the ground truth test dataset. The evaluation measures how well the synthetic dataset matches the ground truth in terms of MMD-CD, complementing a different evaluation measure used in FIG2."
}