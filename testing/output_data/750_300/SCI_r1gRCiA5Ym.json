{
    "title": "r1gRCiA5Ym",
    "content": "Dropout is a technique to improve generalization performance and prevent overfitting in deep neural networks. This paper discusses three novel observations about dropout in DNNs with ReLU activations: 1) dropout encourages training each local linear model on nearby data points, 2) a constant dropout rate can lead to different neural-deactivation rates for layers with varying fractions of activated neurons, and 3) the rescaling factor of dropout causes inconsistencies between training and testing normalization. The proposed method \"Jumpout\" improves dropout by sampling the dropout rate using a monotone decreasing distribution, leading to better performance for nearby data points in deep neural networks with ReLU activations. Jumpout improves dropout by sampling the dropout rate with a monotone decreasing distribution, training the local linear model at each data point to work better for nearby data points. It adaptively normalizes the dropout rate at each layer and rescales the outputs for a better trade-off between variance and mean of neurons during training and testing phases. Jumpout improves dropout by sampling the dropout rate with a monotone decreasing distribution and normalizing it at each layer. It rescales the outputs for a better trade-off between variance and mean of neurons during training and testing phases, mitigating incompatibility with batch normalization. Jumpout shows significantly improved performance on various datasets with minimal additional memory and computation costs. Deep learning has achieved remarkable success on various machine learning tasks, but overfitting can weaken generalization performance. Dropout is a technique that randomly sets hidden neuron activations to mitigate overfitting without significant computational overhead. However, dropout has drawbacks. Dropout is a technique in deep learning that randomly sets hidden neuron activations to reduce co-adaptation amongst neurons and mitigate overfitting. However, it has drawbacks such as the need to tune dropout rates for optimal performance, which can slow convergence or yield no improvements in generalization performance. In practice, dropout rates should be tuned separately for each layer and training stage to improve generalization performance. However, to reduce computation, a single dropout rate is often used for all layers throughout training. Dropout acts as a perturbation on training samples, helping the DNN generalize to noisy samples with a specific expected amount of perturbation. The fixed dropout rate rules out samples with less perturbation, which may be closer to the original. When a constant dropout rate is applied to layers and samples with different fractions of activated neurons, the effective dropout rate varies, leading to too much perturbation for some layers and samples and too little for others. This can hinder generalization performance by not allowing samples closer to the original to contribute effectively. The dropout technique can lead to varying levels of perturbation for different layers and samples due to the inconsistent deactivation of activated neurons. Additionally, dropout is not compatible with batch normalization, as rescaling undropped neurons to match the original activation gain can disrupt normalization parameters and lead to poor performance during testing. The dropout technique can cause inconsistencies in deactivating neurons and is not compatible with batch normalization, leading to poor performance during testing. To address these issues, three modifications to dropout are proposed to improve its effectiveness. The dropout technique can lead to inconsistencies and is not compatible with batch normalization, resulting in poor performance during testing. To address these issues, three modifications to dropout are proposed to improve its effectiveness, leading to an improved version called \"jumpout.\" This approach is motivated by observations on how dropout enhances generalization performance for DNNs with ReLU activations. Dropout is a technique used with DNNs with ReLU activations to improve generalization performance. It randomly changes ReLU activation patterns during training, leading to better predictions for data points in nearby polyhedra. This approach has limitations when used with batch normalization, prompting the proposal of \"jumpout\" as an enhanced version of dropout. Each linear model in a DNN is trained to work for data points in its associated polyhedron and nearby ones, influenced by the dropout rate used. The typical number of units dropped out on a layer with n units is np, smoothing each linear model to improve generalization performance. In jumpout, the dropout rate p is a random variable sampled from a decreasing distribution, ensuring local smoothness in each linear model of a DNN. In jumpout, the dropout rate p is a random variable sampled from a decreasing distribution, ensuring local smoothness in each linear model of a DNN. This approach results in a higher probability of choosing a smaller dropout rate, leading to decreased probability of smoothing polyhedra as points move farther away. Additionally, dropout can result in varying fractions of activated neurons in different layers, samples, and training stages, even with the same dropout rate. In jumpout, the effective neural-deactivation rate is normalized for each layer and training sample, ensuring consistency in deactivating activated neurons as training progresses. In jumpout, the dropout rate is adaptively normalized for each layer and training sample, ensuring consistent deactivation of neurons. The outputs are rescaled to maintain variance, allowing for compatibility with Batch Normalization. This enables the benefits of both dropout and BN in training a DNN. In jumpout, the dropout rate is adaptively normalized for each layer and training sample, ensuring consistent deactivation of neurons. It can be easily implemented and incorporated into existing architectures with only a minor modification to dropout code. Jumpout shows almost the same memory and computation costs as the original dropout in experiments on various benchmark datasets. Jumpout, a new approach to address the fixed dropout rate problem, adapts the dropout rate for each layer and training sample. It outperforms traditional dropout on tasks across various benchmark datasets like CIFAR10, CIFAR100, Fashion-MNIST, SVHN, STL10, and ImageNet-1k. Previous methods like \"standout\" have also been proposed to generate adaptive dropout rates. Different methods have been proposed to generate adaptive dropout rates for neural networks. BID1 introduced \"standout\" to adjust dropout rates for different layers and training stages using a binary belief network. BID24 extended this to learn adaptive dropout rates for individual neurons or groups. BID23 linked the Rademacher complexity of a DNN to dropout rates and suggested adapting them accordingly. In contrast, jumpout adjusts dropout rates without relying on additional models. The complexity of a DNN is bounded by a function related to dropout rate vectors. Jumpout adjusts dropout rates based on ReLU activation patterns without additional models. It introduces minimal computation and memory overhead and can be easily integrated into existing architectures. BID19 introduced Gaussian dropout as a faster convergence optimization method. Gaussian dropout, proposed in BID19, optimizes dropout directly for faster convergence. Variational dropout, introduced in BID9, connects global uncertainty with adaptive dropout rates for each neuron. BID11 extended variational dropout to reduce gradient estimator variance and achieve sparse dropout rates. Other recent dropout variants include Swapout BID16, combining dropout with other methods. In this paper, the focus is on modifications to the original dropout method without additional training costs or extra parameters. Jumpout is introduced to adjust dropout rates adaptively for each neuron. Jumpout is a modification of the original dropout method that aims to adjust dropout rates adaptively for each neuron in deep neural networks. It does not require extra training costs or introduce more parameters to learn. Jumpout can be applied alongside other dropout variants and targets different issues in dropout. The study focuses on a feed-forward deep neural network architecture and its formalization, which can represent various DNN architectures used in practice. It includes details on weight matrices, activation functions, input data points, output predictions, hidden nodes, and dimensionality. The DNN formalization discussed in the previous context covers various architectures used in practice, including fully-connected networks and convolutional operators. The weight matrices represent nodes after applying activation functions, with bias terms included at each layer. The convolution operator is likened to matrix multiplication, where filters are applied to different parts of the input. The convolution operator in neural networks is essentially a sparse weight matrix with tied parameters, achieved by applying filters to different parts of the input data. Average-pooling and max-pooling can be represented as matrix multiplication and activation functions, respectively. Residual network blocks can be represented by appending an identity matrix to retain input values. The residual network block can be represented by appending an identity matrix to retain input values. DNN with short-cut connections can be written as a piecewise linear function for ReLU activation functions. The DNN with ReLU activation functions can be represented as a piecewise linear function, where the activation pattern modifies the weight matrix to create a linear model around a given data point. The DNN with ReLU activation functions can be represented as a piecewise linear function. The activation pattern modifies the weight matrix to create a linear model around a given data point, eliminating all ReLU functions in the process. The resulting linear model is associated with specific activation patterns on all layers, defining a convex polyhedron. The linear model in Eqn. 2 is associated with activation patterns on all layers for a data input x, defining a convex polyhedron. The focus is on DNNs with ReLU activations due to their computational efficiency and wide applicability. The study will explore how dropout improves performance. The study focuses on how dropout improves the generalization performance of DNNs with ReLU activations. It considers how dropout generalizes each local linear model to nearby convex polyhedra, leading to three modifications that result in \"Jumpout\". The study explores how dropout enhances DNN generalization with ReLU activations, leading to three modifications known as \"Jumpout\". Dropout promotes neuron independence and diversity, trains smaller networks, and ensembles predictions during test/inference. Dropout improves generalization performance by promoting neuron independence and diversity, training smaller networks, and ensembling predictions during test/inference. It smooths each local linear model in a DNN with ReLUs by dividing the input space into convex polyhedra. For DNNs with ReLUs, the input space is divided into convex polyhedra, creating distinct local linear models for each training data point. The number of polyhedra can be exponential in the number of neurons, leading to dispersed training samples among different regions. The input space for DNNs with ReLUs is divided into convex polyhedra, creating distinct local linear models for each training data point. The number of polyhedra can be exponential in the number of neurons, leading to dispersed training samples among different regions. Nearby polyhedra may correspond to different linear models due to variations in activation patterns and weight matrices. Given the problems of dropout mentioned in Section 1.1, a proposal is made to sample a dropout rate from a distribution, where weight matrices are adjusted based on activation patterns to address the issue of DNN fragility and lack of smoothness in generalization. To address the fragility and lack of smoothness in generalization of DNNs, a proposal is made to sample a dropout rate from a truncated half-normal distribution, ensuring a positive value by taking the absolute value of the sampled rate. The dropout rate is further constrained within specified limits to stabilize the network's performance on new data. The proposal suggests sampling a dropout rate from a truncated half-normal distribution to address fragility and lack of smoothness in generalization of DNNs. The dropout rate is constrained within specified limits to stabilize network performance on new data, ensuring a monotone decreasing probability. The standard deviation \u03c3 is used as a hyper-parameter to control the amount of dropout. By utilizing a Gaussian-based dropout rate distribution with the standard deviation \u03c3 as a hyper-parameter, smaller dropout rates are sampled more frequently to enhance generalization performance. This approach encourages smoothness in the performance of local linear models, ensuring effectiveness on points within closer polyhedra. A Gaussian-based dropout rate distribution with a hyper-parameter \u03c3 encourages smoothness in the generalization performance of local linear models, especially for points in closer polyhedra. Tuning dropout rates separately for each layer can improve network performance, but it is often computationally expensive. Setting a single global dropout rate for all layers is a common but suboptimal approach due to the varying proportions of active neurons in each layer. Tuning dropout rates separately for different layers can improve network performance but is often computationally expensive. The proportion of active neurons in each layer varies significantly during training stages. Different dropout rates applied to different layers result in varying fractions of active neurons being deactivated. To better control dropout behavior across layers and training stages, the effective dropout rate of each layer is calculated as p_j * q_j, where p_j is the dropout rate of layer j and q_j is the fraction of active neurons in layer j. In order to better control dropout behavior across layers and training stages, the dropout rate is normalized by q + j, resulting in an actual dropout rate of p j = p j /q + j. This adjustment helps achieve a more consistent hamming distance between activation patterns and allows for precise tuning of the dropout rate as a single hyper-parameter. The dropout rate is normalized by q + j to achieve a consistent hamming distance between activation patterns and allow for precise tuning of the dropout rate as a single hyper-parameter. In standard dropout, neurons are scaled by 1/p during training, leading to incompatibility with batch normalization due to variance differences between training and test phases. Dropout and batch normalization (BN) are incompatible due to variance differences between training and test phases. Combining dropout layers with BN layers can lead to unpredictable behavior in DNNs. The combination of dropout and batch normalization (BN) in deep neural networks can lead to unpredictable behavior. In this setup, a linear computational layer is followed by a BN layer, ReLU activation layer, and dropout layer, affecting the values of neurons in subsequent layers. When applying dropout in deep neural networks with batch normalization, the scales of mean and variance of neurons change during training, impacting subsequent layers. The parameters of batch normalization are trained based on scaled mean and variance, which are not affected by dropout during test/inference. During training, dropout affects the scales of mean and variance of neurons in deep neural networks with batch normalization. This inconsistency can be fixed by rescaling the output to counteract dropout's impact on mean and variance scales. Rescaling factors differ for recovering mean and variance scales, with (1 \u2212 p j ) \u22121 for mean and (1 \u2212 p j ) \u22120.5 for variance if E(y j ) is small. During training, dropout affects the scales of mean and variance of neurons in deep neural networks with batch normalization. To recover the original scale of the mean, rescale the dropped neurons by (1 \u2212 p j ) \u22121, and for the variance, use (1 \u2212 p j ) \u22120.5 if E(y j ) is small. Consider the value of E[w j ] to scale the undropped neurons. However, this requires additional computation and memory cost. The scaling factor is correct for the variance of y j but for the mean, use (1 \u2212 p j ) \u22121. During training, dropout affects the scales of mean and variance of neurons in deep neural networks with batch normalization. To recover the original scale of the mean, rescale the dropped neurons by (1 \u2212 p j ) \u22121, and for the variance, use (1 \u2212 p j ) \u22120.5 if E(y j ) is small. However, computing information about w j requires additional computation and memory cost. No simple scaling method can resolve the shift in both mean and variance. The network is \"CIFAR10(s)\" (see Sec. 4). The left plot shows the empirical mean of y with dropout divided by the case without dropout. During training, dropout affects the scales of mean and variance of neurons in deep neural networks with batch normalization. The rescaling factor (1 \u2212 p) \u22120.75 provides a nice trade-off between mean and variance rescaling. The network used is \"CIFAR10(s)\". The left plot shows the empirical mean of y with dropout divided by the case without dropout, while the second plot shows the similar ratio for the variance. Ideally, both ratios should be close to 1. During training, dropout affects the scales of mean and variance of neurons in deep neural networks with batch normalization. The rescaling factor (1 \u2212 p) \u22120.75 provides a trade-off between mean and variance rescaling, balancing the impact of large and small mean values on the variance. During training, a rescaling factor of (1 \u2212 p) \u22120.75 is proposed to balance the impact of mean and variance rescaling in deep neural networks with batch normalization. This factor makes both the mean and variance consistent, as shown in figures comparing dropout methods. Using dropout with batch normalization in convolutional networks can potentially improve performance, with larger dropout rates leading to more significant improvements. The rescaling factor (1 \u2212 p) \u22120.75 is used to balance mean and variance rescaling in deep neural networks during training. The study explores the use of dropout with batch normalization in convolutional networks. It suggests that combining dropout with batch normalization can enhance performance, with larger dropout rates potentially leading to more significant improvements. However, using the original dropout with batch normalization can result in decreased accuracy when the dropout rate exceeds 0.15. In contrast, the proposed improved dropout method, called \"Jumpout,\" shows continuous performance improvement with increasing dropout rates until reaching 0.25, outperforming other configurations. The proposed improved dropout method, \"Jumpout,\" outperforms other configurations by continuously improving with increasing dropout rates until reaching 0.25. Jumpout samples from a decreasing distribution for a random dropout rate and normalizes it adaptively based on the number of active neurons. Jumpout is a novel dropout method that samples from a decreasing distribution for a random dropout rate and normalizes it adaptively based on the number of active neurons. It further scales the outputs by (1 \u2212 p) \u22120.75 during training to trade-off mean and variance shifts and synergize well with batchnorm operations. Jumpout introduces a novel dropout method that samples from a decreasing distribution for a random dropout rate and normalizes it adaptively based on the number of active neurons. It scales the outputs by (1 \u2212 p) \u22120.75 during training to balance mean and variance shifts and work well with batchnorm operations. Jumpout requires three hyperparameters, with \u03c3 being the main parameter controlling the standard deviation of the distribution. The auxiliary truncation hyperparameters (p min , p max ) bound the samples from the distribution, with recommended values of p min = 0.01 and p max = 0.6. Jumpout introduces a novel dropout method that samples from a decreasing distribution for a random dropout rate and normalizes it adaptively based on the number of active neurons. It requires three hyperparameters, with \u03c3 controlling the standard deviation. The input features of layer j are considered for one data point, and the estimation of q + j can be done separately for each data point or averaged over the mini-batch. Utilizing the latter option gives comparable performance with less computation and memory usage. In practice, jumpout has similar memory costs as original dropout, with minimal computation requirements for counting active neurons and sampling from a distribution. Dropout and jumpout are applied to various DNN architectures for comparison. In this section, dropout and jumpout are applied to different popular DNN architectures to compare their performance on six benchmark datasets at various scales. The architectures include small CNN with four convolutional layers applied to CIFAR10, WideResNet-28-10 applied to CIFAR10 and CIFAR100, \"pre-activation\" version of ResNet-20 applied to Fashion-MNIST, WideResNet-16-8 applied to SVHN and STL10, and ResNet-18 applied to other datasets. The study applied dropout and jumpout to various DNN architectures on different benchmark datasets. This included small CNN for CIFAR10, WideResNet-28-10 for CIFAR10 and CIFAR100, \"pre-activation\" ResNet-20 for Fashion-MNIST, WideResNet-16-8 for SVHN and STL10, and ResNet-18 for ImageNet. Standard settings and data preprocessing were followed for CIFAR and Fashion-MNIST experiments, while pre-trained ResNet18 models were used for ImageNet experiments. The study applied dropout and jumpout to various DNN architectures on different benchmark datasets. On ImageNet, pre-trained ResNet18 models were used, starting from a pre-trained model to avoid overfitting issues. Training two copies with dropout and jumpout respectively for the same number of epochs. The study compared dropout and jumpout on various DNN architectures and benchmark datasets. Jumpout consistently outperformed dropout on all datasets tested, including Fashion-MNIST and CIFAR10. Even on datasets with high test accuracy, jumpout still showed improvements. Additionally, jumpout achieved significant improvements on CIFAR100 and ImageNet. Jumpout consistently outperformed dropout on various DNN architectures and benchmark datasets, including Fashion-MNIST, CIFAR10, CIFAR100, and ImageNet. Even on datasets with high test accuracy, jumpout showed appreciable improvements, surpassing the need to increase model size for enhancement. A thorough ablation study confirmed the effectiveness of each proposed modification, highlighting the advantages of jumpout over original dropout. The ablation study confirmed the effectiveness of proposed modifications, showing that each one improves vanilla dropout. Combining all three modifications (jumpout) achieves the best performance. Learning curves and convergence plots in FIG5 demonstrate that jumpout with adaptive dropout rates per minibatch outperforms dropout in early learning stages. Jumpout with adaptive dropout rates per minibatch outperforms dropout in early learning stages, showing substantial advantages and reaching good accuracy faster. Future improvements may include a better learning rate schedule for jumpout to achieve final performance earlier. The study compares dropout and jumpout with adaptive rates per minibatch. Jumpout reaches final performance earlier than dropout, showing advantages in early learning stages. Dropout rescaling factor (1 \u2212 p) \u22120.75 gives a nice trade-off between mean and variance rescaling."
}