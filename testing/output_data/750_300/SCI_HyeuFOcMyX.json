{
    "title": "HyeuFOcMyX",
    "content": "Structural planning is crucial for generating long sentences in language models. This study introduces a planning phase in neural machine translation to control sentence structure. The model generates planner codes to guide the output words, improving translation performance by anticipating different sentence structures. The study introduces an end-to-end neural network with a discretization bottleneck to predict simplified part-of-speech tags in target sentences, improving translation performance by planning ahead and manipulating planner codes for different sentence structures. Evidence from linguists suggests that human speakers plan ahead to ensure grammatical and logical correctness during speech. In contrast to human speakers who plan ahead in discourse or sentence level, neural machine translation models lack a planning phase when generating sentences. The structural information in NMT models remains uncertain until concrete words are sampled, unlike human planning which ensures grammatical and logical correctness during speech. In neural machine translation models, structural information is uncertain until concrete words are sampled. To address this, a proposed framework inserts planner codes at the beginning of output sentences to plan the sentence structure before decoding actual words. In a proposed framework for neural machine translation models, planner codes are inserted at the beginning of output sentences to plan the sentence structure before decoding actual words. The input sentence already provides rich information about the target-side structure, reducing the need for uncertain planning. The proposed framework for neural machine translation models uses planner codes to plan the sentence structure before decoding words. The input sentence provides rich information about the target-side structure, reducing the need for uncertain planning. The planner codes help disambiguate uncertain information, such as the order of nouns and pronouns, to regulate the search space effectively during beam search. In this work, simplified POS tags are used to annotate sentence structure. Planner codes are learned through a network that reconstructs sentences with both input and codes. Experiments show improved translation performance with structural planning, allowing control over output sentence structure. In this section, structural planning is used to improve translation performance by controlling the output sentence structure through planner codes obtained from simplified POS tags. The goal is to reduce uncertainty in decoding by providing a \"big picture\" structural annotation for sentences. Beam search or the NMT model can efficiently handle uncertainty in local structures. In the decoding phase, a structural annotation is used to describe the sentence's \"big picture\" and reduce uncertainty. This annotation simplifies POS tags by mapping them to \"N\", \"V\", \"PRP\", \",\", and \".\". For example, \"He found a fox behind the wall\" becomes \"PRP V N\". In the decoding phase, a structural annotation simplifies POS tags by mapping them to \"N\", \"V\", \"PRP\", \",\", and \".\" to reduce uncertainty in sentence structure. The process involves converting annotations like \"PRP V N\" for sentences like \"He found a fox behind the wall\". The planner codes C Y are used to remove uncertainty in sentence structure during translation. The planner codes C Y are utilized to remove uncertainty in sentence structure during translation. The code learning model architecture involves computing discrete codes C 1 , .., C N based on simplified POS tags S 1 , ..., S T using a backward LSTM. These codes are then discretized into approximated one-hot vectors using the Gumbel-Softmax trick. The code learning model architecture involves encoding sequence S 1 , ..., S T with a backward LSTM and computing vectors C 1 , ..., C N. These vectors are discretized into one-hot vectors using the Gumbel-Softmax trick. Information from X and C initializes a decoder LSTM to predict S 1 , ..., S T. The model predicts the probability of emitting each tag S t. The code learning model architecture involves encoding sequences with a backward LSTM and computing vectors, which are then discretized into one-hot vectors. The model predicts the probability of emitting each tag, and the architecture includes an extra context input to the decoder. The parameters are optimized with crossentropy loss, and once trained, planner codes can be obtained for all target sentences in the training data. The code learning model architecture involves encoding sequences with a backward LSTM and computing vectors, which are then discretized into one-hot vectors. Once trained, planner codes can be obtained for all target sentences in the training data. These planner codes are used in training a regular NMT model, where beam search is employed during decoding. Methods have been proposed to enhance the syntactic correctness of translations. Recently, methods like BID19 and BID2 have been proposed to improve the syntactic correctness of translations in NMT models. BID19 restricts the search space of the NMT decoder using a lattice from a Statistical Machine Translation system, while BID2 uses a multi-task approach to combine parsing loss with the original loss. Other works incorporate target-side syntactic structures explicitly, such as interleaving CCG supertags with output words. Some works improve syntactic correctness in NMT models by combining parsing loss with the original loss. BID12 interleaves CCG supertags with output words, while Aharoni and Goldberg (2017) train NMT models to generate parse trees. BID20 proposes a model to generate words and parse actions simultaneously. Other methods learn discrete codes for different purposes, like compressing the structure before translation. The word and action prediction are conditioned on each other. Some works learn discrete codes for different purposes, like compressing word embeddings. BID7 breaks down word dependency with shorter code sequences for faster decoding. Models are evaluated on IWSLT 2014 Germanto-English and ASPEC Japanese-to-English tasks. Tokenization is done using Kytea for Japanese texts and moses toolkit for others. The models are evaluated on IWSLT 2014 Germanto-English and ASPEC Japanese-to-English tasks using Kytea for Japanese texts and moses toolkit for other languages. The code learning model uses bytepair encoding with 256 hidden units in all layers and is trained using Nesterov's accelerated gradient for 50 epochs. Different settings of code length N and number of code types K are tested, with the information capacity being N log K bits. The learned codes are evaluated for different settings in TAB1. The code learning model uses bytepair encoding with 256 hidden units and is trained using Nesterov's accelerated gradient for 50 epochs with a learning rate of 0.25. Different settings of code length N and number of code types K are tested, with the information capacity being N log K bits. Evaluation of the learned codes in TAB1 shows a trade-off between S Y accuracy and C Y accuracy, with N = 2, K = 4 being a balanced setting. The code learning model uses bytepair encoding with 256 hidden units and is trained using Nesterov's accelerated gradient for 50 epochs with a learning rate of 0.25. There is a trade-off between S Y accuracy and C Y accuracy when the code has more capacity. The NMT model uses 2 layers of bidirectional LSTM encoders and 2 layers of LSTM decoders, with 256 units for IWSLT De-En task and 1000 units for ASPEC Ja-En task. Key-Value Attention is applied in the first decoder layer, and residual connection BID3 is used to combine hidden states in two decoder layers. Dropout is applied outside of the recurrent function. The NMT models for IWSLT De-En and ASPEC Ja-En tasks use Key-Value Attention in the first decoder layer and residual connection BID3 to combine hidden states in two decoder layers. Dropout with a rate of 0.2 is applied outside of the recurrent function. The models are trained using the NAG optimizer with a learning rate of 0.25, annealed by a factor of 10 after 20K iterations if no improvement is observed. Conditioning word prediction on generated planner codes improves translation performance over a strong baseline. The NMT models for IWSLT De-En and ASPEC Ja-En tasks use Key-Value Attention and residual connection BID3 in decoder layers. Dropout rate of 0.2 is applied. NAG optimizer with LR of 0.25, annealed by 10x after 20K iterations if no improvement. Best parameters chosen on validation set. Conditioning word prediction on planner codes improves translation performance. Greedy search on JaEn dataset results in lower BLEU score compared to baseline. Beam search on planner codes followed by greedy search does not significantly change results. Simultaneously exploring multiple candidates with different structures is important for Ja-En task. The results suggest that exploring diverse candidates with different structures simultaneously is crucial for improving beam search performance on the Ja-En task. Manual selection of planner codes instead of relying solely on beam search can also be beneficial. The performance of beam search depends on candidate diversity. Manual selection of planner codes can be beneficial. Example translations conditioned on different planner codes are shown in Table 3. The process of AP .code 3 and AP .code 4 was described, showing translations with different structures by manipulating planner codes. The proposed method can generate paraphrased translations with high diversity. The distribution of assigned planner codes for English sentences in the ASPEC Ja-En dataset is shown in Figure 3. The proposed method can generate diverse paraphrased translations by manipulating planner codes. The distribution of codes learned for English sentences in the ASPEC Ja-En dataset is skewed, indicating room for improvement. Instead of discrete codes, structural annotations like POS tags can also be predicted directly. In this paper, a planning phase is added in neural machine translation to generate planner codes for controlling the output sentence structure, aiming to improve translation performance. In this paper, a planning phase is added in neural machine translation to generate planner codes for controlling the output sentence structure, aiming to improve translation performance. The proposed method involves designing an end-to-end neural network with a discretization bottleneck to predict simplified POS tags of target sentences, resulting in improved translation performance. Additionally, the planner codes allow for sampling translations with different structures, enhancing the decoding algorithm. The proposed method improves translation performance by using planner codes to control sentence structure. Sampling translations with different structures enhances the decoding algorithm by removing uncertainty. This framework can be extended to plan other latent factors like sentiment or topic."
}