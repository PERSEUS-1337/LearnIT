{
    "title": "ByBAl2eAZ",
    "content": "Deep reinforcement learning methods explore through noise injection in the action space. An alternative approach is adding noise to the agent's parameters for more consistent exploration. Combining parameter noise with traditional RL methods benefits both off- and on-policy methods, as shown in experimental comparisons of DQN, DDPG, and TRPO on various environments. Efficient exploration in deep reinforcement learning is a key challenge, as it prevents premature convergence to local optima. Adding noise to agent parameters benefits both off- and on-policy methods, as demonstrated in comparisons of DQN, DDPG, and TRPO on different environments. Efficient exploration in deep reinforcement learning is a challenge as it avoids premature convergence to local optima. Various methods have been proposed to address this issue in high-dimensional MDPs, often relying on complex structures. One approach to enhance exploration is by adding temporally-correlated noise, as seen in bootstrapped DQN. Adding temporally-correlated noise, such as in bootstrapped DQN, can enhance exploration in deep reinforcement learning algorithms. Parameter noise has also been shown to improve exploration by increasing the variety of behaviors in the policy. These approaches have limitations when only evaluated in on-policy settings with small function approximators or when disregarding temporal aspects. This paper explores combining parameter space noise with deep RL algorithms like DQN, DDPG, and TRPO to enhance exploratory behavior. Experiments demonstrate the effectiveness of this approach in both high-dimensional discrete environments and continuous control tasks. Experiments show that parameter noise enhances exploratory behavior in deep RL algorithms like DQN, DDPG, and TRPO. It outperforms traditional action space noise-based methods, especially in tasks with sparse reward signals. The standard RL framework involves an agent interacting with a fully observable environment modeled as a Markov decision process (MDP). The signal is sparse. The RL framework involves an agent in a fully observable environment modeled as an MDP. The environment has states, actions, initial state distribution, reward function, transition probabilities, time horizon, and discount factor. The agent aims to maximize the expected discounted return by following a policy parametrized by \u03b8. The agent's goal is to maximize the expected return by following a policy parametrized by \u03b8. Two popular off-policy algorithms considered are Deep Q-Networks (DQN) and Deep Deterministic Policy Gradients (DDPG). This paper discusses off-policy RL methods, focusing on Deep Q-Networks (DQN) and Deep Deterministic Policy Gradients (DDPG). DQN uses a deep neural network to estimate the optimal Q-value function, while DDPG is based on a deterministic policy gradient approach. The policy in DDPG is implicitly defined by the Q-value function, with the actor trained to maximize the critic's estimated Q-values. The Q-network predicts Q-values for actions and is updated using off-policy data from a replay buffer. This algorithm is applicable to continuous action spaces and uses a stochastic greedy or Boltzmann policy for exploration. DDPG is an algorithm for continuous action spaces where the critic estimates Q-values using off-policy data and the Bellman equation. The actor is trained to maximize the critic's Q-values through back-propagation. Exploration is achieved using a stochastic policy with action space noise. On-policy methods update function approximators based on the current policy. Trust Region Policy Optimization (TRPO) is an on-policy method that improves upon traditional policy gradient methods by computing an ascent direction that ensures a small change in the policy distribution. It solves a constrained optimization problem to update function approximators based on the current policy. TRPO improves upon REINFORCE by computing an ascent direction that ensures a small change in the policy distribution. It solves a constrained optimization problem with discounted state-visitation frequencies and advantage function. Policies are parameterized as neural networks. This work introduces policies parameterized as neural networks and explores structured exploration by sampling from a set of policies with additive Gaussian noise. The perturbed policy is sampled at the beginning of each episode and kept fixed for the entire rollout. State-dependent exploration involves adding noise to the parameter vector of the current policy. The perturbed policy is sampled at the start of each episode and remains fixed throughout. This exploration method distinguishes between action space noise and parameter space noise, with actions being sampled from a stochastic policy in the continuous action space case. When using Gaussian action noise in the continuous action space case, actions are sampled from a stochastic policy, leading to different actions for the same state in subsequent rollouts. In contrast, perturbing policy parameters at the start of each episode ensures consistency in actions for the same state sampled in the rollout. Perturbing policy parameters at the beginning of each episode ensures consistency in actions for the same state sampled in the rollout, introducing a dependence between the state and the exploratory action taken. Deep neural networks can be perturbed effectively by applying spherical Gaussian noise through a simple reparameterization technique. Neural networks can be perturbed by applying spherical Gaussian noise, but a simple reparameterization technique achieves this more effectively. Layer normalization BID2 is used between perturbed layers to normalize activations within a layer, allowing for consistent perturbation scale across all layers. Adaptive noise scaling parameter space noise requires selecting a suitable scale \u03c3, which can be challenging due to network architecture dependencies. Adaptive noise scaling in parameter space is crucial for neural networks, as different layers may have varying sensitivities to noise. Selecting the appropriate scale \u03c3 can be challenging due to network architecture dependencies and evolving parameter sensitivities. To address this, we propose a simple solution to adapt the scale of parameter space noise over time, providing a straightforward approach to managing noise in neural networks. Adapting the scale of parameter space noise over time can help manage noise in neural networks. By relating it to the variance in action space, a distance measure can be defined to adjust the noise level based on a threshold value. This approach addresses challenges in selecting the appropriate noise scale for neural networks with varying sensitivities. Parameter space noise can be adaptively adjusted based on a threshold value, with a scaling factor \u03b1 and threshold value \u03b4. In off-policy methods, noise is applied for exploration, while in on-policy methods, noise is used to perturb the policy. Parameter space noise can be incorporated in on-policy methods by perturbing the policy for exploration and training the non-perturbed network on collected data. This can be done using an adapted policy gradient approach as outlined by R\u00fcckstie\u00df et al. (2008). Incorporating parameter space noise in stochastic policy \u03c0 \u03b8 (a|s) with \u03b8 \u223c N (\u03c6, \u03a3) can benefit RL algorithms. The expected return is expanded using likelihood ratios and the re-parametrization trick for N samples i \u223c N (0, I) and \u03c4 i \u223c (\u03c0, p). The value of \u03a3 is fixed to \u03c3 2 I and scaled adaptively. This section explores the impact of parameter space noise on exploration in sparse reward environments compared to evolution strategies. The text discusses the benefits of incorporating parameter space noise in RL algorithms and its impact on exploration in sparse reward environments. Comparisons are made between parameter space noise exploration and evolution strategies for deep policies. Reference implementations of DQN and DDPG with adaptive parameter space noise are available online for evaluation. Parameter space noise is compared to action space noise in high-dimensional discrete-action environments and continuous control tasks using DQN, DDPG, and TRPO. The Arcade Learning Environment benchmark is used for discrete environments with a standard DQN implementation. A version of DQN with parameter noise is compared to a baseline DQN agent with -greedy action noise, with the scale adapting using a simple heuristic. We compare a baseline DQN agent with -greedy action noise against a version of DQN with parameter noise. The scale of parameter noise is adjusted based on a heuristic to ensure a fair comparison with action space noise. By reparametrizing the network to represent the policy \u03c0 implied by Q-values, perturbing the policy instead of Q results in more effective parameter noise implementation (see Section C.1 for details). This approach ensures a fair comparison between action space noise and parameter space noise without introducing additional hyperparameters. A fully connected layer followed by a softmax output layer is added to the network to predict a discrete probability distribution over actions given a state. To represent the policy \u03c0(a|s), a fully connected layer is added after the convolutional part of the network, followed by a softmax output layer. Perturbing the policy instead of Q leads to more meaningful changes, defining an explicit behavioral policy. The Q-network is trained following standard DQN practices, while the policy \u03c0 is trained to maximize the probability of outputting the greedy action based on the current Q-network. The policy is trained to mimic the behavior of running greedy DQN. The policy \u03c0 is trained to exhibit the same behavior as running greedy DQN by maximizing the probability of outputting the greedy action based on the current Q-network. Parameter space noise approach is compared against regular DQN and two-headed DQN with -greedy exploration, with actions randomly sampled for the first 50 thousand timesteps to fill the replay buffer before training. Combining parameter space noise with a bit of action space noise improves performance. The study randomly samples actions for the first 50 thousand timesteps to fill the replay buffer before training. Parameter space noise is found to perform better when combined with a bit of action space noise. 21 games of varying complexity are chosen for training, with each agent trained for 40 million frames. The overall performance is evaluated by running each configuration with three random seeds, and the median return is plotted. The study by BID4 compares the performance of parameter space noise and action space noise on a selection of games. Results show that parameter space noise often outperforms action space noise, especially on games requiring consistency. Learning progress starts sooner with parameter space noise. Parameter space noise outperforms action space noise, especially on games requiring consistency. Learning progress starts sooner with parameter space noise. Results confirm that a double-headed version of DQN with \u03b5-greedy exploration does not account for improved exploration. However, parameter space noise struggles to explore in extremely challenging games like Montezuma's Revenge. Sophisticated exploration methods like BID4 are likely more effective in such cases. The results confirm that parameter space noise is not responsible for improved exploration, especially in challenging games like Montezuma's Revenge. More sophisticated exploration methods like BID4 may be necessary for successful learning in these games. It would be interesting to evaluate the effect of combining parameter space noise with exploration methods. The text discusses the comparison between parameter noise and action noise in continuous control environments using DDPG as the RL algorithm. Proposed improvements to DQN are mentioned as orthogonal to the study's findings, leaving room for future experimental validation. In comparing parameter noise with action noise in continuous control environments using DDPG as the RL algorithm, different noise configurations were tested: (a) no noise, (b) uncorrelated additive Gaussian action space noise, (c) correlated additive Gaussian action space noise, and (d) adaptive parameter noise. Layer normalization was applied after each layer before the nonlinearity, proving to be beneficial for parameter space noise. We compare different noise configurations in continuous control tasks using DDPG: (a) no noise, (b) uncorrelated Gaussian action space noise, (c) correlated Gaussian action space noise, and (d) adaptive parameter space noise. Performance is evaluated on various tasks, with each agent trained for 1 million timesteps. In continuous control tasks, different noise configurations are compared using DDPG. Performance is evaluated on various tasks, with each agent trained for 1 million timesteps. Parameter space noise achieves higher returns on HalfCheetah compared to other configurations, which quickly converge to a local optimum. Parameter space noise outperforms other exploration schemes on HalfCheetah task by achieving higher returns and breaking out of sub-optimal behavior. It also surpasses correlated action space noise, indicating a significant difference between the two. However, on other environments, parameter space noise performs similarly to other exploration strategies. Space noise outperforms correlated action space noise on this environment, indicating a significant difference between the two. Parameter space noise performs similarly to other exploration strategies on other environments. DDPG can learn good policies even without noise, suggesting well-shaped reward functions in these environments. TRPO results are shown in FIG4, with parameter noise decreasing performance in the Walker2D environment. In the Walker2D environment, adding parameter noise reduces performance variance between seeds, aiding in escaping local optima. Parameter noise is evaluated for learning on environments with sparse rewards, where uncorrelated action noise fails. Parameter noise is evaluated on environments with sparse rewards to see if it enables existing RL algorithms to learn effectively. A toy example with a chain of states is used to demonstrate the difficulty of discovering rewards, with increasing challenge as the number of states grows. In a chain of states, the agent moves from state s2 with rewards r=0.001 in s1 and r=1 in state sN. Discovering the small reward in s1 is easier than the large reward in sN, with increasing difficulty as N increases. Different RL algorithms are compared on varying chain lengths N, with evaluations after each episode. Chain length N is varied with three different seeds trained and evaluated for each N. The problem is considered solved if one hundred subsequent rollouts achieve the optimal return. The median number of episodes before the problem is solved is plotted, with an abort after 2 thousand episodes if unsolved. Experimental details are available in Section A.3. Green indicates a solved problem, while blue indicates no solution within 2 K episodes. Parameter space noise outperforms action space noise and even the more computationally expensive bootstrapped DQN in solving the simple environment where the optimal strategy is always to go right. In a simple environment where the optimal strategy is always to go right, parameter space noise outperforms action space noise and even the more computationally expensive bootstrapped DQN. However, parameter space noise may not work as well in cases where the optimal action depends on the current state. In challenging continuous control environments with sparse rewards, exploration behavior is compared to action space noise. Parameter space noise may not guarantee optimal exploration in general cases. The environments only yield a reward after significant progress towards a goal, such as raising the paddle above a threshold or reaching an upright position. In challenging environments with sparse rewards, tasks include SparseCartpoleSwingup, SparseDoublePendulum, SparseHalfCheetah, SparseMountainCar, and SwimmerGather. Rewards are only given upon specific achievements like raising the paddle, reaching an upright position, crossing a target distance, or driving up a hill. DDPG and TRPO are used to solve these tasks with a time horizon of T = 500 steps. SwimmerGather involves reaching targets with positive or negative rewards. DDPG and TRPO are used to solve tasks with a time horizon of T = 500 steps. DDPG's performance is shown in FIG6, while TRPO results are in Appendix F. SparseDoublePendulum is relatively easy to solve for DDPG, even without noise. SparseDoublePendulum is easy for DDPG to solve, while SparseCartpoleSwingup and SparseMountainCar require parameter space noise for successful policies. SparseHalfCheetah shows non-zero reward but struggles to learn a successful policy. SwimmerGather proves challenging for all DDPG configurations. Parameter space noise can improve exploration behavior in off-the-shelf algorithms, but its benefits are not guaranteed for all cases. Evolution strategies also use noise in parameter space for exploration. Parameter space noise can enhance exploration behavior in algorithms, but its effectiveness varies. Evolution strategies utilize noise for exploration, but lack temporal information. Combining parameter space noise with traditional RL allows for temporal information integration and optimization through back-propagation gradients, improving exploratory behavior. Comparing ES and traditional RL methods. By combining parameter space noise with traditional RL algorithms, temporal information can be integrated while still benefiting from improved exploratory behavior. Performance comparison between ES and traditional RL with parameter space noise on 21 ALE games shows promising results. ES results were obtained after training on 1,000 M frames, while DQN used the same parameter space noise for exploration. Using the final policy without exploration, 10 episodes were run for each seed to compute median returns. DQN, trained on 40 M frames with parameter space noise, outperformed ES on 15 out of 21 Atari games, showcasing the combination of desirable exploration properties of ES with the sample efficiency of traditional RL. The study demonstrated that parameter space noise combines the exploration properties of ES with the sample efficiency of traditional RL. Various algorithms have been proposed to address exploration in reinforcement learning, but in real-world problems with continuous and high-dimensional state and action spaces, these algorithms become impractical. In the context of deep reinforcement learning, various techniques have been proposed to improve exploration, but they are often computationally expensive. Perturbing the parameters of a policy has been suggested as a way to enhance exploration in policy gradient methods. The idea of perturbing policy parameters for exploration in deep reinforcement learning has been proposed. This method has shown to outperform random exploration in policy gradient methods. The approach has been evaluated with different algorithms, but previous studies had limitations in terms of policy dimensionality and state spaces. In contrast, our method is applied and evaluated for both on and off-policy settings, using high-dimensional architectures. Our method is applied and evaluated for both on and off-policy settings, using high-dimensional policies and environments with large state spaces. It is closely related to evolution strategies and policy optimization methods. ES has been shown to work for high-dimensional environments like Atari and OpenAI Gym continuous control problems, but generally disregards temporal structure. Our approach perturbs network parameters directly for exploration, similar to Bootstrapped DQN but simpler and sometimes superior. Parameter space noise is proposed as a simpler and more effective method for exploration in deep reinforcement learning, compared to traditional action space noise. This approach perturbs network parameters directly, showing success when combined with various deep RL algorithms. Parameter space noise is suggested as a more effective alternative to traditional action space noise in deep reinforcement learning. It can be successfully integrated with DQN, DDPG, and TRPO algorithms, leading to improved performance, especially in environments with sparse rewards. This method offers a promising alternative to the commonly used action space noise. The experimental setup for ALE BID3 involves a network architecture with 3 convolutional layers and 1 hidden layer with ReLUs and layer normalization. This setup allows for solving environments with sparse rewards using parameter space noise as an alternative to action space noise. The network architecture for ALE includes 3 convolutional layers (8 filters of size 8x8, 64 filters of size 4x4, and 64 filters of size 3x3) followed by a hidden layer with 512 units and a linear output layer. ReLUs are used in each layer, with layer normalization in the fully connected part. Parameter space noise is included with a policy network using softmax output. Target networks are updated every 10,000 timesteps, and the Q-value network is trained with Adam optimizer, learning rate of 10^-4, and batch size of 32. The replay buffer can hold 1 million states. The Q-value network has the same architecture as the ALE network, with a softmax output layer. Target networks are updated every 10,000 timesteps. The Q-value network is trained with Adam optimizer, learning rate of 10^-4, and batch size of 32. The replay buffer can hold 1 million state transitions. Parameter space noise is adapted to have a similar effect in action space, ensuring maximum KL divergence between perturbed and non-perturbed \u03c0 is softly enforced. The policy is perturbed at the start of each episode, with the standard deviation adapted accordingly. The policy is perturbed at the beginning of each episode, with the standard deviation adapted every 50 timesteps. Layer normalization is included in the fully connected part of the network to prevent getting stuck. -greedy action selection with = 0.01 is used to avoid perturbed policy issues. 50 K random actions are performed to collect initial data for the replay. To prevent getting stuck, layer normalization is included in the network's fully connected part. -greedy action selection with = 0.01 is used. 50 K random actions are performed initially for the replay buffer. Observations are down-sampled to 84 \u00d7 84 pixels and converted to grayscale. The network receives a concatenation of 4 subsequent frames as input. Up to 30 noop actions are used at the start of each episode. The frames are down-sampled to 84 \u00d7 84 pixels and converted to grayscale. A concatenation of 4 subsequent frames is used as input. The network architecture for DDPG includes 2 hidden layers with 64 ReLU units each, with layer normalization applied to all layers. Target networks are soft-updated with \u03c4 = 0.001. The critic is trained with a learning rate of 10 \u22123 while the actor uses a learning rate of 10 \u22124. The critic and actor in DDPG are updated using the Adam optimizer with batch sizes of 128. The critic is regularized with an L2 penalty of 10 \u22122 and the replay buffer holds 100 K state transitions with \u03b3 = 0.99. Observation dimensions are normalized by an online estimate of mean and variance. Parameter space noise is scaled to match action space noise. TRPO uses a step size of \u03b4 KL = 0.01, a policy network with 2 hidden layers of 32 tanh units for nonlocomotion tasks, and 2 hidden layers of 64 tanh units for locomotion tasks. The Hessian calculation is subsampled with a factor of 0.1, and \u03b3 = 0.99. Action space noise is adjusted for dense environments with \u03c3 = 0.2 and for sparse environments with \u03c3 = 0.6. TRPO uses a step size of \u03b4 KL = 0.01, a policy network with 2 hidden layers of 32 tanh units for nonlocomotion tasks, and 2 hidden layers of 64 tanh units for locomotion tasks. The Hessian calculation is subsampled with a factor of 0.1, \u03b3 = 0.99, and the batch size per epoch is set to 5 K timesteps. The baseline is a learned linear transformation of the observations. Environments from OpenAI Gym and rllab are used for tasks with specific reward conditions. OpenAI Gym and rllab environments are utilized for tasks with specific reward conditions, such as Swimmer, Sparse tasks, and SparseDoublePendulum. The state encoding follows a proposed method using observation \u03c6(s t ) = (1{x \u2264 s t }). The agent is trained using DQN with a simple network structure and layer normalization before applying nonlinearity. Each agent undergoes training for up to 2K episodes, with varying chain lengths and three different seeds for evaluation after each episode. The agent is trained using DQN with layer normalization before nonlinearity. Each agent is trained for up to 2K episodes, with varying chain lengths and three seeds for evaluation. Performance is assessed by sampling trajectories without noise, and the problem is solved if optimal return is achieved in subsequent trajectories. The environment for testing exploratory behavior is shown in Figure 6. The environment for testing exploratory behavior is depicted in Figure 6. Different DQN variations are compared, including adaptive parameter space noise DQN, bootstrapped DQN, and -greedy DQN. Parameter space noise is adaptively scaled, and learning starts after 5 initial episodes. Target network and replay buffer settings are consistent across all cases. In the context of testing exploratory behavior, a single head is used with perturbed Q directly, with adaptive parameter space noise scaled to \u03b4 \u2248 0.05. The network is trained using Adam optimizer with a learning rate of 10^-3 and a batch size of 32, updating the target network every 100 timesteps. Stochastic policy \u03c0 \u03b8 (a|s) with \u03b8 \u223c N (\u03c6, \u03a3) is utilized for expected return expansion using likelihood ratios and the reparametrization trick for N samples i \u223c N (0, I) and \u03c4 i \u223c (\u03c0. This method also incorporates a variance-reducing baseline b i. The expected return is expanded using likelihood ratios and the reparametrization trick for N samples, incorporating a variance-reducing baseline. The proposed adaption method is used to re-scale \u03a3, with parameter space noise requiring a suitable scale \u03c3. The scale of action is highly dependent on the network architecture and varies over time as learning progresses. The proposed solution involves adapting the scale of parameter space noise over time by using a time-varying scale \u03c3 k, related to action space variance. This resolves limitations in an easy way, updating \u03c3 k accordingly. The parameter space noise is adapted over time using a time-varying scale \u03c3 k, related to action space variance. The scale is updated every K timesteps based on a heuristic involving a distance measure, rescaling factor \u03b1, and threshold value \u03b4. This approach is inspired by the Levenberg-Marquardt heuristic. The parameter space noise is adapted using a time-varying scale \u03c3 k, with \u03b1 rescaling factor and \u03b4 threshold value. The choice of distance measure depends on the policy representation, outlined for DDPG, TRPO, and DQN. In experiments, \u03b1 is always set to 1.01. For DQN, the policy is implicitly defined by the Q-value function, leading to pitfalls in measuring distance between Q-values. The policy in DQN is implicitly defined by the Q-value function, leading to challenges in measuring distance between Q-values. A naive distance measure may not accurately reflect policy equality, especially when perturbations are made to the final layer bias. Probabilistic formulations are used for both non-perturbed and perturbed policies to address this issue. The policy in DQN is defined by Q-values, making it challenging to measure distance accurately. A probabilistic formulation is used for both non-perturbed and perturbed policies, normalizing Q-values and avoiding previous problems. This allows for measuring distance in action space using Kullback-Leibler divergence. The distance in action space in DQN is measured using Kullback-Leibler divergence, effectively normalizing Q-values. This approach relates the distance measure to -greedy action space noise, avoiding the need for an additional hyperparameter \u03b4. The KL divergence between a greedy policy and an -greedy policy is calculated as D KL (\u03c0 \u03c0) = \u2212 log (1 \u2212 + |A| ), where |A| is the number of actions. The KL divergence between a greedy policy and an \u03b5-greedy policy is calculated as D KL (\u03c0 \u03c0) = \u2212 log (1 \u2212 \u03b5 + |A| ). This distance measure can be used to relate action space noise and parameter space noise in DDPG. The distance measure between non-perturbed and perturbed policy in DDPG is related to action space noise and parameter space noise by adaptively scaling \u03c3 to match the KL divergence between greedy and \u03b5-greedy policy. This results in effective action space noise with the same standard deviation. In TRPO, noise vectors are adapted to scale the noise for effective action space noise with the same standard deviation as regular Gaussian noise. This is achieved by computing a trust region around the noise direction using the conjugate gradient algorithm. The trust region around the noise direction is computed using the conjugate gradient algorithm to ensure the perturbed policy remains close to the original. Learning curves for 21 Atari games are provided, comparing ES and DQN performance after a certain number of frames. The final performance of ES after 1,000 M frames is compared to DQN with -greedy exploration and parameter space noise exploration after 40 M frames. Performance is estimated by running 10 episodes with exploration disabled. Adaptive parameter space noise achieves stable performance on InvertedDoublePendulum. Overall, performance is comparable to other exploration methods. The performance of TRPO with noise scaled according to the parameter curvature achieves stable results on InvertedDoublePendulum. No noise in action or parameter space yields comparable outcomes, suggesting these environments with DDPG are not ideal for exploration testing. The TRPO baseline uses action noise with a policy network outputting the mean of a Gaussian distribution. Adding parameter space noise improves the performance of TRPO on challenging sparse environments, as shown in FIG10. The baseline TRPO uses action noise with a policy network outputting the mean of a Gaussian distribution."
}