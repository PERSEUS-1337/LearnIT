{
    "title": "HkGv2NMTjQ",
    "content": "State of the art sound event classification uses neural networks to learn associations between class labels and audio recordings in a dataset. Ontologies define a structure that relates sound classes with abstract super classes, serving as domain knowledge representation. However, ontology information is often overlooked in modeling neural network architectures. Two ontology-based neural network architectures are proposed for sound event classification, designed to preserve an ontological structure. The networks are trained and evaluated using common sound event classification datasets, showing improved performance by incorporating ontological information. Sounds in environments can be categorized into abstract classes like humans, emergency vehicles, and home, represented by ontologies. The text discusses the importance of incorporating ontologies in sound event classification to improve performance. Ontologies provide a formal representation of domain knowledge through categories and relationships, aiding in the interpretation of sounds in different environments. Neural networks are commonly used for sound event classification but rarely consider ontologies in their design. Ontologies provide a formal representation of domain knowledge through categories and relationships, aiding in the interpretation of sounds in different environments. Neural networks, although state of the art for sound event classification, often do not consider ontologies in their design. Examples of datasets include ESC-50, UrbanSounds, DCASE, and AudioSet. Hierarchical relations in sound event classifiers, represented by super categories and subcategories, offer benefits by allowing the classifier to generalize. Taxonomies can be defined by nouns or verbs, interactions between objects and materials, actions and descriptors, or physical properties like frequency and time patterns. Examples of datasets include ESC-50, UrbanSounds, DCASE, and AudioSet. Hierarchical relations in sound event classifiers, such as super categories and subcategories, provide benefits like generalization, disambiguation of acoustically similar classes, and penalizing misclassifications differently. They can also serve as domain knowledge for modeling neural networks. Ontological information has been explored in computer vision as well. Ontological information has been evaluated in computer vision and music, but rarely used for sound event classification. Ontology-based network architectures have shown performance improvement and other benefits, such as replicating tree-like structures to model transformation from super classes to sub classes. Authors in BID16 proposed an ontology-based deep restricted Boltzmann machine for textual topic classification, replicating a tree-like structure with intermediate layers. They demonstrated improved performance and reduced overfitting in training data. Another approach in BID17 used a perceptron for each node in a hierarchy, showing enhanced performance through class disambiguation. These methods showcase the flexibility of adapting structures in deep learning models. In response to previous studies on ontology-based deep learning models for text classification, the authors propose their own ontology-based networks. They introduce a framework for incorporating ontological information into deep learning architectures, starting with a discussion on the types of ontologies used and their implications. The proposed model includes constraints and a Feed-forward structure to address class disambiguation and improve performance. The authors introduce a Feed-forward model with constraints to incorporate ontological information into deep learning architectures. They extend the model to compute ontology-based embeddings using Siamese Neural Networks, considering ontologies with two levels commonly found in sound event datasets. The framework introduced incorporates ontological information into deep learning architectures to compute ontology-based embeddings using Siamese Neural Networks. It is designed for ontologies with two levels, but can be generalized to more levels easily. The training data consists of audio representations associated with labels from the ontology, with each possible class mapped hierarchically to the next level. The ontology-based embeddings framework uses audio representations associated with labels from the ontology, where each possible class at one level is mapped hierarchically to the next level. The ontology in this case has two levels, with each element in the first level related to one element in the second level. The ontology-based embeddings framework uses audio representations associated with labels from a two-level ontology. Each element in the first level is related to one element in the second level, allowing for inference of labels between the two levels using a probabilistic formulation. The framework utilizes a Net linear + softmax formulation in a Feed-forward Network with an Ontological Layer. It involves estimating probabilities for different levels based on audio features and labels. Training the model may require additional knowledge beyond just the representation and label. The proposed framework involves using ontology-based neural network architectures to improve model performance by incorporating knowledge of different classes during training. The ontological layer utilizes the ontology structure in designing the architecture, specifically in a Feed-forward Network (FFN). The proposed framework involves designing ontology-based neural network architectures, specifically the Feed-forward Network (FFN) with an Ontological Layer. The architecture includes a base network, an intermediate vector z, and two outputs for each ontology level. The base network learns weights at every parameter update and utilizes audio features to generate probability vectors for different classes. The base network uses audio features to generate probability vectors for different classes. The ontological layer reflects the relation between super classes and sub classes. The FFN can predict any class in C1 and C2 for any input x. The ontological layer in the model reflects the relation between super classes and sub classes. Equation 3 describes how the layer is used, with M as the incidence matrix. The weights in the ontological layer are not trainable but are part of the training data. Equation 3 can be rewritten to define the weights of the ontological layer in the model. The weights are not trainable but are included in the training data. To train the model, a gradient-based method is applied to minimize the loss function, which is a combination of two categorical cross-entropy functions. The hyperparameter \u03bb \u2208 [0, 1] is tuned to adjust the balance between the two functions. When \u03bb = 1, the model is trained as a standard classifier using only information from the first level. In this section, we describe how we learned ontology-based embeddings using a Siamese neural network (SNN) to preserve the ontological structure. The network enforces samples of the same class to be closer while separating samples of different classes, aiming to create embeddings that maintain the ontological relationships. The hyperparameter \u03bb \u2208 [0, 1] is tuned to adjust the balance between the two categorical cross-entropy functions corresponding to different levels of the ontology. The Siamese neural network (SNN) preserves ontological structure by enforcing samples of the same class to be closer and separating samples of different classes. The architecture of the SNN includes a Feed-forward Network with Ontological Layer, where samples from different subclasses but the same superclass are closer than samples from different superclasses. The twin networks have the same base architecture with shared weights that are learned simultaneously. The Siamese neural network enforces samples of the same class to be closer and separates samples of different classes. The twin networks have the same base architecture with shared weights, and ontological embeddings are used to compute a Similarity metric. The distance between embeddings indicates how different samples are with respect to the ontology. The Siamese neural network enforces samples of the same class to be closer and separates samples of different classes based on ontological embeddings. The distance between embeddings indicates the difference between samples in terms of ontology. The output probabilities for different levels are evaluated for sound event classification performance. The Feed-forward Model with Ontological layer using Ontology-based embeddings is trained with three types of audio example pairs to minimize the classification performance of sound event using gradient-based method. The dataset for Making Sense of Sounds Challenge 2 - MSoS aims to classify the most abstract classes in its taxonomy with a two-level ontology. The dataset for Making Sense of Sounds Challenge 2 - MSoS aims to classify abstract classes at different hierarchy levels. It consists of 1500 audio files from various sources, divided into 5 categories with unbalanced sound types. The evaluation dataset has 500 audio files, with 100 files per category. The development dataset for Making Sense of Sounds Challenge 2 consists of 1500 audio files divided into five categories, with 300 files each. The evaluation dataset includes 500 audio files, with 100 files per category. All files are in single-channel 44.1 kHz, 16-bit .wav format, and are 5 seconds long. The dataset is designed to evaluate the classification of urban sounds. The official blind evaluation set of the challenge consisted of 500 files distributed among 5 classes. The Urban Sounds - US8K dataset is designed for urban sound classification, with a taxonomy adjusted to avoid redundant levels. The ontology has two levels, with 10 classes in level 1 and 4 classes in level 2, all sourced from real field recordings on Freesound. The dataset used for the study contained 8,732 audio files from Freesound, with a two-level ontology consisting of 10 classes at level 1 and 4 classes at level 2. The audio files were in a single-channel 44.1 kHz, 16-bit .wav format, and were divided into 10 subsets for analysis. The audio recordings were represented using state-of-the-art Walnet features BID1, with a 128-dimensional logmel-spectrogram vector computed for each audio and transformed using a convolutional neural network (CNN). The experiment utilized a feed-forward multi-layer perceptron network with 4 layers, including input, 2 dense layers, and output layer of dimensionality 128. The network was trained on a balanced set of AudioSet using Walnet features BID1 for audio representation. The experiment involved a feed-forward multi-layer perceptron network with 4 layers: input layer (1024), 2 dense layers (512 and 256), and output layer (128). The dense layers used Batch Normalization, dropout rate of 0.5, and ReLU activation function. Parameters were tuned in the Net box and for transforming z into p(y 1 |x). Baseline models were considered for different datasets, without ontological information. Base Network Architecture was used with an additional output. The baseline models for different datasets did not include ontological information and consisted of the Base Network Architecture with an added output layer for level 1 or level 2. For level 1, this is similar to training the Feed-forward model with Ontological Layer using \u03bb = 1. The loss function for level 2 is not considered with \u03bb = 1. In contrast, the baseline model for level 2 does not have a layer for predicting y 1. The baseline models for level 1 and level 2 did not include ontological information. The baseline performance for the MSoS challenge was 0.81 for level 2. Different values of \u03bb were used to train models, with FIG3 showing the effect on both data sets. The architecture presented in Section 2.2 was validated by training models with different values of \u03bb. The ontological layer improved classification performance, with the best results achieved using \u03bb = 0.8 in the MSoS dataset. This led to a 5.4% and 6% absolute improvement in accuracy for levels 1 and 2 respectively compared to baseline models. In the case of the MSoS dataset, the best performance was achieved with \u03bb = 0.8, resulting in accuracy of 0.74 and 0.913 for levels 1 and 2. This led to a 5.4% and 6% improvement over baseline models. On the US8K dataset, a smaller improvement was observed with \u03bb = 0.7, yielding accuracy of 0.82 and 0.86 for levels 1 and 2, a 2.5% and 0.2% improvement over baseline models. The best results were achieved with \u03bb = 0.8 on the MSoS dataset, showing a 5.4% and 6% improvement over baseline models. The ontology-based embeddings resulted in tighter clusters, as observed in the t-SNE plots. The architecture described in Section 2.3 was tested to evaluate the performance of ontology-based embeddings for sound event classification. t-SNE plots were included to show how the embeddings cluster at different levels using Walnet audio features. The Siamese neural network was trained with different super and sub class pairs to produce the embeddings, which were then passed to the base network for training. The Siamese neural network was trained with various super and sub class pairs to generate ontology-based embeddings, which were then fed into the base network for training. The SNN was trained for 50 epochs using the Adam algorithm and hyper-parameters were tuned for optimal performance. Testing different numbers of input training data pairs, it was found that 100,000 pairs yielded the best results. The loss function values from previous experiments were used, with a lambda value of 0.8. The Siamese neural network was trained with various pairs of input training data, with 100,000 pairs yielding the best performance. Different lambda values were used for classifiers in level 1 and level 2, as well as for the similarity metric. Results showed accuracy performance for MSoS and US8K, with conclusions drawn on the architecture's superior performance. The results in Table 1 compare the accuracy performance of MSoS and US8K, showing MSoS at 0.736 and 0.886 in level 1 and 2 respectively, and US8K at 0.818 and 0.856. The architecture outperformed the baseline but slightly underperformed without embeddings. Ontology-based embeddings improved grouping, as seen in t-SNE plots. The t-SNE plots of classes in level 1 and 2 showed that FF + Ontology vectors and ontology-based embeddings created clustered groups, with tighter clusters from the ontology-based embeddings. The performance on the US8K dataset was limited due to a similar number of sub classes and super classes, making the ontology contribution negligible. The study compared the number of sub classes to super classes in different datasets for sound event classification. The Feed-forward Network with Ontological Layer and ontological-embeddings outperformed the baseline accuracy on the blind evaluation set. The proposed framework utilized hierarchical neural networks for sound event classification. The study proposed a framework for sound event classification using hierarchical ontologies. Two methods were presented to incorporate this structure into neural networks without adding more parameters. A Feed-forward Network with an ontological layer and a Siamese neural Network with ontology-based embeddings were utilized, both outperforming the baseline accuracy. Our study introduced a Feed-forward Network with an ontological layer and a Siamese neural Network to improve sound event classification using hierarchical ontologies. Results showed enhanced performance over baselines, indicating potential for further exploration of ontologies in sound classification due to acoustic diversity and limited lexical terms."
}