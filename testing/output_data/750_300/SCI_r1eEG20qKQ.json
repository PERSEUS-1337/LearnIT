{
    "title": "r1eEG20qKQ",
    "content": "Hyperparameter optimization is a bilevel optimization problem where optimal parameters depend on hyperparameters. Regularization hyperparameters for neural networks are adapted by fitting approximations to the best-response function. Scalable best-response approximations for neural networks are constructed by modeling the best-response as a single network with gated hidden units. This approximation is justified by showing the exact best-response for a shallow linear network with L2-regularized Jacobian can be represented by a similar gating mechanism. The text discusses approximating the best-response function for neural networks using a single network with gated hidden units. This approximation is justified by showing a similar mechanism for a shallow linear network with L2-regularized Jacobian. The model is fitted using a gradient-based hyperparameter optimization algorithm that alternates between approximating the best-response and optimizing the hyperparameters. Unlike other approaches, this method does not require differentiating the training loss with respect to the hyperparameters, allowing for tuning of discrete hyperparameters, data augmentation hyperparameters, and dropout. Optimizing hyperparameters using an approximate best-response function without the need for differentiating training loss. Self-Tuning Networks (STNs) update hyperparameters online during training, outperforming other methods on deep learning problems. Our approach, Self-Tuning Networks (STNs), updates hyperparameters online during training and outperforms other methods on deep learning problems. Regularization hyperparameters like weight decay, data augmentation, and dropout are crucial for neural network generalization but challenging to tune. Popular hyperparameter optimization methods include grid search, random search, and Bayesian optimization. Approaches to hyperparameter optimization like grid search, random search, and Bayesian optimization are effective for low-dimensional spaces but treat the problem as a black-box, requiring many training runs. Formulating hyperparameter optimization as a bilevel optimization problem can exploit structure for faster convergence. Formulating hyperparameter optimization as a bilevel optimization problem involves defining parameters and hyperparameters, mapping them to training and validation losses, and aiming to solve for the best-response function. This approach offers faster convergence compared to traditional methods like grid search and Bayesian optimization. The validation loss can be minimized directly by gradient descent using Equation 2, offering speed-ups over black-box methods. Approximating the best-response w * with a parametric function \u0175 \u03c6 is proposed, optimizing \u03c6 and \u03bb jointly. Finding a scalable approximation \u0175 \u03c6 for the weights of a neural network is a significant challenge due to memory constraints. Constructing a compact approximation for the weights of a neural network by modeling the best-response of each row in a layer's weight matrix as a rank-one affine transformation of the hyperparameters. This approximation involves computing the activations of a base network plus a correction term dependent on the hyperparameters, offering a scalable solution to the memory overhead challenge. Self-Tuning Networks (STNs) update their own hyperparameters online during training, offering advantages over other optimization methods. They can be easily implemented by replacing existing modules in deep learning libraries with \"hyper\" counterparts. Self-Tuning Networks (STNs) have advantages over other hyperparameter optimization methods as they update hyperparameters online during training. They are easy to implement and do not require differentiating the training loss with respect to the hyperparameters. This online adaption ensures computational effort is not wasted and yields hyperparameter schedules that outperform fixed settings. The online adaption of Self-Tuning Networks (STNs) outperforms fixed hyperparameter settings. STNs do not require differentiating the training loss with respect to hyperparameters, allowing tuning of discrete hyperparameters. Empirical evaluation shows STNs outperform on deep-learning problems with Penn Treebank and CIFAR-10 datasets. Bilevel optimization involves solving upper-level and lower-level problems, with minimax problems being an example where the upper-level objective is the negative of the lower-level objective. Initially studied in economics for leader/follower dynamics, bilevel programs have practical applications in various fields. Bilevel programs, such as minimax problems, have practical applications in various fields including machine learning. These problems are strongly NP-hard even when objectives and constraints are linear. Hyperparameter optimization, GAN training, meta-learning, and neural architecture search are challenging tasks. Bilevel problems are strongly NP-hard, even with linear objectives and constraints. Most research focuses on restricted settings, while we aim to find local solutions in nonconvex, differentiable, and unconstrained settings. In nonconvex, differentiable, and unconstrained settings, the goal is to solve a problem with upper-and lower-level objectives and parameters. A gradient-based algorithm is preferred for speed, but simultaneous gradient descent may lead to incorrect solutions due to the dependence of parameters. The simplest optimization method is simultaneous gradient descent, but it can give incorrect solutions due to the dependence of parameters. A more principled approach is to use the best-response function to convert the problem into a single-level one. If the best-response function is differentiable, gradient descent can be used to minimize the equation. The best-response function can convert the problem into a single-level one, allowing for the use of gradient descent to minimize the equation. Conditions for unique optima and differentiability of the function are crucial but challenging to verify. Sufficient conditions are provided in a lemma for a neighborhood around a point where the problem is solved. Sufficient conditions for unique optima and differentiability of the function are provided in a lemma for a neighborhood around a point where the problem is solved. The gradient of F* decomposes into direct and response gradients. The gradient of F* decomposes into direct and response gradients, with the response gradient stabilizing optimization by converting the bilevel problem into a single-level one. This conversion ensures that the gradient vector field is preserved. Simultaneous gradient descent can stabilize optimization by converting the bilevel problem into a single-level one, ensuring a conservative gradient vector field. The solution to Problem 4b is a set, but uniqueness and differentiability of w* can lead to practical algorithms. Gradient-based hyperparameter optimization methods approximate the best-response w* or its Jacobian \u2202w*/\u2202\u03bb. Gradient-based hyperparameter optimization methods can approximate the best-response w* or its Jacobian \u2202w*/\u2202\u03bb, but they can be computationally expensive and struggle with discrete and stochastic hyperparameters. Promising approaches to approximate w* directly were proposed by Lorraine & Duvenaud (2018). Promising approaches to approximate w* directly were proposed by Lorraine & Duvenaud (2018). They introduced two algorithms: Global Approximation and Local Approximation. Global Approximation involves approximating w* as a differentiable function \u0175 \u03c6 with parameters \u03c6, while Local Approximation focuses on a more localized approach. Lorraine & Duvenaud (2018) proposed two algorithms: Global Approximation approximates w* as \u0175 \u03c6, while Local Approximation focuses on approximating w* in a neighborhood around the upper-level parameter \u03bb. The approach by Lorraine & Duvenaud (2018) involves approximating w* as \u0175 \u03c6 globally and focusing on a neighborhood around the upper-level parameter \u03bb locally. They use a factorized Gaussian noise distribution for p( |\u03c3) and an alternating gradient descent scheme to update \u03c6 and \u03bb. This method has shown success with L2 regularization on MNIST but its applicability to different regularizers or larger problems is uncertain. The requirement of \u0175 \u03c6 for high dimensional w poses a challenge. In this section, a memory-efficient best-response approximation \u0175 \u03c6 is constructed to scale to large neural networks. The method automatically adjusts the scale of the neighborhood \u03c6 and addresses challenges with high-dimensional w and setting \u03c3. The approach aims to adapt to discrete and stochastic hyperparameters. In this section, a best-response approximation \u0175 \u03c6 is constructed for large neural networks. The method automatically adjusts the neighborhood scale and handles discrete and stochastic hyperparameters. The resulting networks, called Self-Tuning Networks (STNs), update their own hyperparameters online during training. Self-Tuning Networks (STNs) update their hyperparameters online during training by approximating the best-response for weight matrices and biases through affine transformations of hyperparameters. This architecture adds a correction to the usual pre-activation of a layer to account for hyperparameters. The best-response architecture for Self-Tuning Networks involves a linear transformation of hyperparameters, making it tractable and memory-efficient. It enables parallelism by allowing independent perturbation of hyperparameters for different examples in a batch, improving sample efficiency. The best-response architecture for Self-Tuning Networks involves a linear transformation of hyperparameters, enabling parallelism and improving sample efficiency. This can be implemented by replacing existing modules in deep learning libraries with \"hyper\" counterparts. The compact representation of the best-response function from R^n to R^m is questioned, along with the reasonableness of equation 10 as an approximation. In this section, a model is presented where the best-response function can be exactly represented using a linear network with Jacobian norm regularization. The network's hidden units are modulated based on the hyperparameters, utilizing a 2-layer linear network to predict targets from inputs. The best-response function is represented by a linear network with modulated hidden units based on hyperparameters. A 2-layer linear network predicts targets from inputs using weights w = (Q, s) \u2208 R D\u00d7D \u00d7 R D. The squared-error loss is regularized with an L2 penalty on the Jacobian \u2202y /\u2202x, with penalty weight \u03bb in R +. Theorem 2 states properties of w 0 = (Q 0 , s 0 ) and the unregularized version of the problem. Theorem 2 discusses the properties of w 0 = (Q 0 , s 0 ) and the unregularized version of the problem, showing how y(x; w * (\u03bb)) can be implemented as a regular network with sigmoidal gating of hidden units. This architecture is inspired by a similar gating approach used to approximate best-response for deep, nonlinear networks. The architecture in FIG0 uses gating of hidden units to approximate best-response for deep, nonlinear networks. By replacing sigmoidal gating with linear gating for a narrow hyperparameter range, a smooth best-response function can be approximated by an affine function. This allows weights to be affine in the hyperparameters, particularly for quadratic lower-level objectives. The sigmoidal gating in the architecture is replaced with linear gating to approximate a smooth best-response function by an affine function for quadratic lower-level objectives. This ensures convergence to a local optimum in gradient descent. The effect of the sampled neighborhood is discussed, showing that a too small neighborhood leads to approximation issues. The descent on the objective function converges to a local optimum. The effect of the sampled neighborhood is crucial: if it is too small, the approximation may not match the best-response, while if it is too wide, the approximation may be insufficiently flexible. The gradient of the approximation will match the best-response if the sampled neighborhood is appropriate. The scale of the hyperparameter distribution controlled by \u03c3 affects the flexibility of the model. Entries of \u03c3 must be large enough to capture the shape locally but not too large to hinder flexibility. Smoothness of the loss landscape changes during training, impacting the model's performance. During training, adjusting \u03c3 based on sensitivity to hyperparameters can improve model performance. An entropy term weighted by \u03c4 is included to enlarge \u03c3 entries, resulting in an objective similar to variational inference. During training, adjusting \u03c3 based on sensitivity to hyperparameters can improve model performance. An entropy term weighted by \u03c4 is included to enlarge \u03c3 entries, resulting in an objective similar to variational inference. The objective interpolates between variational optimization and variational inference as \u03c4 ranges from 0 to 1. During training, adjusting \u03c3 based on sensitivity to hyperparameters can improve model performance. An entropy term weighted by \u03c4 is included to enlarge \u03c3 entries, resulting in an objective similar to variational inference. The objective interpolates between variational optimization and variational inference as \u03c4 ranges from 0 to 1. The STN training algorithm balances \u03c3 to avoid heavy entropy penalties while optimizing \u03bb for better training and representation learning. Performance evaluation is done at the deterministic current hyperparameter \u03bb 0. The STN training algorithm can tune hyperparameters that other gradient-based algorithms cannot, such as discrete or stochastic hyperparameters. It uses an unconstrained parametrization \u03bb \u2208 R n and maps \u03bb to the appropriate constrained space using a non-differentiable discretization for discrete hyperparameters. Training and validation losses, denoted as L T and L V, are evaluated at the deterministic current hyperparameter \u03bb 0. The STN training algorithm involves mapping hyperparameters from an unconstrained space to a constrained space using a non-differentiable discretization. Training and validation losses are functions of the hyperparameters and parameters. The algorithm alternates between updating parameters and hyperparameters to minimize different loss functions. The STN training algorithm involves updating parameters and hyperparameters alternately to minimize different loss functions. The algorithm can be implemented in code, and the non-differentiability of hyperparameters does not pose a problem. The derivative of the loss function can be estimated using the reparametrization trick. The algorithm for training involves updating parameters and hyperparameters alternately to minimize different loss functions. Derivatives of the loss function can be estimated using the reparametrization trick, with considerations for cases where regularization schemes may or may not directly depend on specific hyperparameters. Regularization schemes can be optimized using the reparametrization gradient when the loss function does not directly depend on certain hyperparameters. However, if the loss function explicitly relies on specific hyperparameters, the REINFORCE gradient estimator can be used. This approach is necessary for hyperparameters like the number of hidden units in a layer, which directly impact the validation loss. The method has been applied to convolutional networks and LSTMs, resulting in self-tuning CNNs (ST-CNNs) and self-tuning LSTMs (ST-LSTMs). We applied our method to convolutional networks and LSTMs, resulting in self-tuning CNNs (ST-CNNs) and self-tuning LSTMs (ST-LSTMs). STNs discovered hyperparameter schedules that outperformed fixed values, and were compared to other optimization methods on CIFAR-10 and PTB datasets. Self-tuning networks (STNs) outperform fixed hyperparameter values by discovering schedules online. STNs were compared to other optimization methods on CIFAR-10 and PTB datasets. An ST-LSTM was used to tune the output dropout rate, showing superior performance. The ST-LSTM discovered a schedule for output dropout rate on the PTB corpus, outperforming fixed hyperparameters with 82.58 vs 85.83 validation perplexity. This improvement is attributed to the schedule, not stochasticity from sampling hyperparameters during training. The improved performance of the ST-LSTM on the PTB corpus is attributed to a discovered schedule for output dropout rate, outperforming fixed hyperparameters. To rule out stochasticity from sampling hyperparameters, random Gaussian and sinusoid perturbations were tested, with STNs outperforming both methods. Limited capacity of\u0175 \u03c6 was also considered. The ST-LSTM outperformed perturbation methods on the PTB corpus and CIFAR-10 tasks. A standard LSTM trained with the output dropout schedule from the ST-LSTM showed similar performance, indicating the schedule's importance. The ST-LSTM outperformed perturbation methods on the PTB corpus and CIFAR-10 tasks, indicating the importance of the schedule. LSTM performed nearly as well as the STN, showing that the schedule itself was responsible for the improvement. Training a standard LSTM with the final dropout value from the STN did not perform as well as following the schedule. The STN discovered the same schedule regardless of initial hyperparameter values. The STN schedule implements a curriculum by using a low dropout rate early in training, aiding optimization, and then gradually increasing the dropout rate for better generalization. The hyperparameters adapt over a shorter timescale than the weights, leading to equilibration. Low regularization is best early in training, while higher regularization is better later on. The ST-LSTM was evaluated on the PTB corpus. The STN schedule implements a curriculum by using a low dropout rate early in training, aiding optimization, and gradually increasing the dropout rate for better generalization. An ST-LSTM was evaluated on the PTB corpus with 2-layer LSTM and 650 hidden units per layer. Seven hyperparameters were tuned, including variational dropout rates and DropConnect on the hidden-to-hidden weight matrix. The study involved tuning 7 hyperparameters for an ST-LSTM model with 650 hidden units per layer and 650-dimensional word embeddings. The hyperparameters included variational dropout rates, embedding dropout, DropConnect, and coefficients for activation regularization. The best results were obtained with a fixed perturbation scale of 1 for the hyperparameters. The comparison was made with grid search, random search, and Bayesian optimization. Additional details can be found in Appendix D. The study involved tuning 7 hyperparameters for an ST-LSTM model with 650 hidden units per layer and 650-dimensional word embeddings. The best results were obtained with a fixed perturbation scale of 1 for the hyperparameters. STNs outperform other methods in achieving lower validation perplexity more quickly. The schedules the STN finds for each hyperparameter are nontrivial, with some forms of dropout used to a greater extent at the start of training. The final validation and test perplexities achieved by each method are shown in Table 2. ST-CNNs were evaluated on the CIFAR-10 dataset using the AlexNet architecture. Different forms of dropout were utilized throughout training to prevent overfitting with high-capacity networks. The study evaluated ST-CNNs on the CIFAR-10 dataset using the AlexNet architecture. Various hyperparameters were tuned, including activation dropout, input dropout, and data augmentation parameters. A total of 15 hyperparameters were considered and compared to grid search, random search, and Bayesian optimization methods. The study evaluated ST-CNNs on the CIFAR-10 dataset using the AlexNet architecture and compared different hyperparameter optimization methods. STNs outperformed grid search, random search, and Bayesian optimization in finding better hyperparameter configurations in less time. The hyperparameter schedules found by STNs are shown in FIG3. The study evaluated ST-CNNs on the CIFAR-10 dataset using the AlexNet architecture and compared different hyperparameter optimization methods. STNs outperformed grid search, random search, and Bayesian optimization in finding better hyperparameter configurations in less time. The hyperparameter schedules found by the STN are shown in FIG3. Bilevel Optimization is discussed, with a focus on linear, quadratic, or convex objectives/constraints. Trust-region methods are loosely related to the work presented. The work discusses using KKT conditions as constraints for the upper-level problem and compares it to trust-region methods. Hypernetworks, introduced by Schmidhuber, are functions mapping to neural net weights, with applications in CNNs and RNNs. Hypernetworks are functions mapping to neural net weights, with applications in CNNs and RNNs. Ha et al. (2016) used hypernetworks to generate weights for modern CNNs and RNNs. Gradient-Based Hyperparameter Optimization involves approximating weights using gradient descent steps. Gradient-Based Hyperparameter Optimization involves two main approaches for approximating the best-response. The first approach uses gradient descent steps to approximate the derivative of the optimal weights with respect to hyperparameters. This method has been utilized by various researchers such as Maclaurin et al. (2015) and Franceschi et al. (2018). The second approach applies the Implicit Function Theorem to derive the derivative of optimal weights under specific conditions, initially introduced by Larsen et al. (1996) and further developed by Pedregosa (2016). The second approach, developed for hyperparameter optimization in neural networks, uses the Implicit Function Theorem to derive \u2202w * /\u2202\u03bb(\u03bb 0) under certain conditions. Similar methods have been applied in log-linear models, kernel selection, and image reconstruction. However, both approaches face challenges when differentiating gradient descent or training loss with respect to hyperparameters. Model-Based Hyperparameter Optimization involves Bayesian optimization to model the conditional probability of performance given hyperparameters and dataset. Differentiating gradient descent becomes costly with increasing descent steps, requiring Hessian-vector products to avoid direct Hessian computation. Model-Based Hyperparameter Optimization uses Bayesian optimization to model the conditional probability of performance based on hyperparameters and dataset. The process involves iteratively constructing a dataset D and selecting the next hyperparameter \u03bb to train on by maximizing an acquisition function. This approach balances exploration and exploitation without the need to train each model to completion, assuming certain learning curve behaviors. Inductive biases are incorporated into the model to improve performance estimation. Model-Based Hyperparameter Optimization uses Bayesian optimization with an acquisition function to balance exploration and exploitation. Model-free approaches like grid search and random search are also used for hyperparameter optimization. Model-free hyperparameter optimization methods like random search and grid search are commonly used. Successive Halving and Hyperband improve random search by allocating resources to promising configurations. These methods do not consider problem structure but can be easily parallelized and perform well in practice. Hyperparameter scheduling techniques like Population Based Training (PBT) are also used. Our method utilizes rich gradient information, unlike bandit techniques, which ignore problem structure. Model-free methods, such as Population Based Training (PBT), parallelize training by evaluating and replacing under-performing networks with better-performing ones. This approach also perturbs hyperparameters for improved performance. Self-Tuning Networks (STNs) efficiently approximate the best-response of parameters to hyperparameters by scaling and shifting hidden units, allowing for different hyperparameter settings during training. STNs replace the population of networks with a single best-response approximation and use gradients to tune hyperparameters in a single training run. Self-Tuning Networks (STNs) efficiently approximate the best-response of parameters to hyperparameters by scaling and shifting hidden units. This allows for gradient-based optimization to tune various regularization hyperparameters, including discrete ones. STNs can discover hyperparameter schedules that outperform fixed hyperparameters, leading to better generalization performance in less time on large-scale problems. STNs offer a promising approach for automated hyperparameter tuning in neural networks. Self-Tuning Networks (STNs) can achieve better generalization performance than competing approaches by efficiently approximating the best-response of parameters to hyperparameters. STNs offer a compelling path towards large-scale, automated hyperparameter tuning for neural networks. Self-Tuning Networks (STNs) aim to approximate the best-response of parameters to hyperparameters for improved generalization performance. The hyperparameter gradient is a combination of direct and response gradients of validation losses. The Jacobian of \u2202f /\u2202w decomposes as a block matrix with sub-blocks. The Jacobian of \u2202f /\u2202w decomposes as a block matrix with sub-blocks, and by the Implicit Function Theorem, there exists a unique continuously differentiable function w * : V \u2192 R m satisfying certain conditions. The Jacobian of \u2202f /\u2202w decomposes into sub-blocks, leading to a unique continuously differentiable function w * : V \u2192 R m. By utilizing second-order optimality conditions, w * (\u03bb) is the sole solution to Problem 4b for all \u03bb \u2208 U. This discussion is based on Hastie et al. (2001) and involves data matrix X, targets t, and SVD decomposition. The SVD decomposition of the data matrix X is represented by U, V, and D. Simplifying the function y(x; w) leads to standard L2-regularized least-squares linear regression. The optimal solution minimizing the equation is given by u*(\u03bb). The optimal solution to the unregularized version of the problem is given by u*. The change-of-basis matrix from the standard basis to the principal components of the data matrix is defined as Q0 = V. The solution s0 to the unregularized regression problem is found by solving for Q0s0 = u*, which implies s0 = D^-1U^t. There are multiple solutions to the problem, so any solution can be taken. The change-of-basis matrix from the standard basis to the principal components of the data matrix is defined as Q0 = V. The solution s0 to the unregularized regression problem is found by solving for Q0s0 = u*, where s0 = D^-1U^t. Multiple solutions exist for the problem, so any solution can be taken. The chosen functions Q*(\u03bb) = \u03c3(\u03bbv + c) row Q0 and s*(\u03bb) = s0 meet the criteria of best-response functions. The function f is quadratic, with matrices A, B, C, vectors d, e, and the derivative \u22022f/\u2202w2 = 0. By setting the derivative to 0 and using second-order conditions, we find the function \u0175\u03c6(\u03bb) = U\u03bb + b. The function f is quadratic with matrices A, B, C, vectors d, e, and \u22022f/\u2202w2 = 0. By setting the derivative to 0 and using second-order conditions, we find \u0175\u03c6(\u03bb) = U\u03bb + b. Simplifying further using linearity of expectation and other properties, we can differentiate f. The function f is quadratic with matrices A, B, C, vectors d, e, and \u22022f/\u2202w2 = 0. By setting the derivative to 0 and using second-order conditions, we find \u0175\u03c6(\u03bb) = U\u03bb + b. Simplifying further using linearity of expectation and other properties, we can differentiate f. Using matrix-derivative equalities, we find the best-response Jacobian \u2202w*/\u2202\u03bb(\u03bb) as given by Equation 34. Substituting U = C\u22121B into the equation gives w*(\u03bb0) \u2212 \u2202w*/\u2202\u03bb(\u03bb0). The derivative for \u2202f/\u2202U(\u03bb0, U, b, \u03c3) is set to 0, resulting in the best-response Jacobian \u2202w*/\u2202\u03bb(\u03bb) from Equation 34. Substituting U = C\u22121B into the equation gives w*(\u03bb0) \u2212 \u2202w*/\u2202\u03bb(\u03bb0), representing the first-order Taylor series of w* about \u03bb0. Model parameters were updated without changing hyperparameters, and training was stopped when the learning rate fell below 0.0003. Variational dropout was tuned for input, hidden state, and output of the LSTM, along with embedding dropout. We adjusted variational dropout on the input, hidden state, and output of the LSTM, along with embedding dropout. DropConnect was used to regularize the hidden-to-hidden weight matrix. Training was stopped when the learning rate dropped below 0.0003. DropConnect (zeroing out weights rather than activations) operates directly on weights, not mini-batch elements. A single DropConnect rate is sampled per mini-batch. Activation regularization (AR) penalizes large activations, while temporal activation regularization (TAR) is a slowness regularizer. Scaling coefficients \u03b1 and \u03b2 were tuned for AR and TAR. Hyperparameter ranges for baselines were [0, 0.95] for dropout rates and [0, 4] for \u03b1 and \u03b2. In the CNN experiments, 20% of the training data was held out for validation. The baseline CNN was trained using SGD with initial learning rate 0.01 and momentum 0.9 on mini-batches of size 128. The learning rate was decayed by 10 each time. Additional details on the CNN experiments include holding out 20% of the training data for validation. The baseline CNN was trained using SGD with an initial learning rate of 0.01 and momentum of 0.9 on mini-batches of size 128. The learning rate was decayed by 10 each time the validation loss failed to decrease for 60 epochs. The search spaces for hyperparameters in baselines-grid search, random search, and Bayesian optimization were defined. The ST-CNN's hyperparameters were optimized using Adam with a learning rate of 0.003. Training alternated between best-response approximation and hyperparameters following the schedule of ST-LSTM. Five epochs of warm-up were used for the model. The hyperparameters for the ST-CNN were optimized using Adam with a learning rate of 0.003. Training involved alternating between best-response approximation and hyperparameters following the schedule of ST-LSTM, with five epochs of warm-up for the model parameters. The entropy weight used was \u03c4 = 0.001, and cutout length and number of cutout holes were restricted to specific ranges. All dropout rates and data augmentation noise parameters were initialized to 0.05. The ST-CNN hyperparameters were initialized with specific values, including dropout rates and data augmentation noise parameters set to 0.05. The model was found to be robust to hyperparameter initialization, with low regularization aiding optimization in the early epochs. Curriculum learning BID2 is related to hyperparameter schedules and continuation methods for optimizing non-convex functions. Curriculum learning BID2 is a continuation method that optimizes non-convex functions by gradually increasing the difficulty of training criteria. This approach aids optimization by starting with a simpler version of the problem and gradually increasing the parameter \u03bb from 0 to 1. Hyperparameter schedules implement a form of curriculum learning by gradually increasing the difficulty of training criteria. Greedy hyperparameter schedules can outperform fixed settings throughout training. The schedule that increases dropout over time increases stochasticity, making the learning problem more difficult. Results from grid searches show that greedy hyperparameter schedules can outperform fixed values. A grid search was performed over 20 values each of input and output dropout, measuring validation perplexity in each epoch. The validation perplexity achieved by different dropout combinations at various epochs is shown in FIG8. Best validation loss is achieved with small dropout values at the start of training. At the start of training, small dropout values yield the best validation loss. As training progresses, larger dropout rates lead to improved validation performance. A grid search was conducted for output dropout, creating a schedule based on the best validation perplexity at each epoch. This schedule, shown in FIG9, demonstrates the benefits of greedy hyperparameter schedules. By conducting a fine-grained grid search for output dropout, a schedule was created based on the best validation perplexity at each epoch. The schedule, illustrated in FIG9, highlights the advantages of greedy hyperparameter schedules, showing that using small dropout values at the beginning of training leads to a rapid decrease in validation perplexity, while larger dropout values later on result in better overall validation perplexity. In Section 4.1, PyTorch code listings for HyperLinear and HyperConv2D classes used in constructing ST-LSTMs and ST-CNNs are provided, along with optimization steps for the training and validation sets. The perturbed values for output dropout were examined to determine if the improved performance of STNs is due to regularization or the schedule."
}