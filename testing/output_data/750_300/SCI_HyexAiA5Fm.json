{
    "title": "HyexAiA5Fm",
    "content": "Generative adversarial networks (GANs) are powerful neural generative models used for high-dimensional continuous measures. This paper introduces a scalable method for unbalanced optimal transport (OT) within the generative-adversarial framework. The approach involves learning a transport map and a scaling factor simultaneously to optimize the transfer of a source measure to a target measure. The proposed algorithm for solving this problem is based on stochastic alternating gradient updates. The paper introduces a scalable method for unbalanced optimal transport within the generative adversarial framework. It proposes an algorithm based on stochastic alternating gradient updates for transforming one measure to another in a cost-optimal manner. This methodology is demonstrated through numerical experiments in population modeling. The paper presents a scalable method for unbalanced optimal transport in the generative adversarial framework, using stochastic alternating gradient updates to transform measures in a cost-optimal way. This approach is demonstrated through numerical experiments in population modeling. The text discusses optimal transport methods for pushing a source distribution to a target distribution efficiently, with a focus on the Kantorovich formulation and the use of entropy regularization to improve solution efficiency. Stochastic methods based on the dual objective have been proposed for the continuous setting. Recently, optimal transport methods have been improved by regularizing the objective with an entropy term, making the dual problem more efficiently solvable using the Sinkhorn algorithm. Stochastic methods based on the dual objective have also been proposed for the continuous setting, with applications in various areas such as computer graphics, domain adaptation, and image translation using generative models like GANs. Generative models like GANs can be used for learning transport maps in various applications such as image translation, natural language translation, domain adaptation, and biological data integration. The objective is to model the transformation of a source population to a target population by training against an adversary. In this example, the objective is to learn a transport map and scaling factor to push the source population to the target population using deterministic or stochastic transport maps. Various strategies have been employed, including conditioning and cycle-consistency, to enforce correspondence between original and transported samples. Several methods using GANs have been developed to tackle mass conservation issues in transporting samples. However, these methods struggle with mass variation between source and target populations. Scaling algorithms have been proposed to extend optimal transport theory to handle unbalanced masses, providing a solution for optimal entropy-transport problems. Scaling algorithms have been developed to handle unbalanced masses in optimal transport theory, allowing for mass variation in applications such as computer graphics, tumor growth modeling, and computational biology. These algorithms approximate unbalanced transport plans between discrete measures by relaxing hard marginal constraints using divergences. Algorithms have been used for unbalanced transport plans in various applications like computer graphics, tumor growth modeling, and computational biology. Current methods cannot perform unbalanced optimal transport between continuous measures. A novel framework inspired by GANs is proposed to directly model mass variation in addition to transport in unbalanced optimal transport. The novel framework presented addresses high-dimensional transport problems by directly modeling mass variation in addition to transport. It proposes a Monge-like formulation of unbalanced optimal transport to learn a stochastic transport map and scaling factor for cost-optimal transport. Scalable methodology is developed for solving the relaxed problem, offering desirable theoretical properties. The relaxed Monge OT problem is addressed by BID7, leading to an alternative optimal entropy-transport problem by BID27 with scalable methodology for solution. The methodology is demonstrated in population modeling using various datasets, including MNIST, USPS, CelebA, and a single-cell RNA-seq dataset from zebrafish embryogenesis. Additionally, a new scalable method is proposed. The curr_chunk discusses the application of a new scalable method (Algorithm 2) for solving the optimal-entropy transport problem in the continuous setting, extending the work to unbalanced OT. It also introduces notation for probability measures and finite non-negative measures over topological spaces. The curr_chunk introduces notation for probability measures and finite non-negative measures over topological spaces, addressing the optimal transport problem in a cost-optimal manner. Monge (1781) formulated this problem. The curr_chunk discusses optimal transport using functions \u03c0X, \u03c0Y for projections, joint measure \u03b3, and the Monge and Kantorovich OT problems. Monge formulated the deterministic transport map search, while Kantorovich's OT problem is a convex relaxation using probabilistic transport plans. The Monge problem is non-convex and not always feasible, while the Kantorovich OT problem is a convex relaxation using probabilistic transport plans. Introducing entropic regularization simplifies the dual optimization problem. The relaxed Monge problem can be solved efficiently using the Sinkhorn algorithm with entropic regularization. Stochastic algorithms have been proposed for computing transport plans for continuous measures, and formulations for unbalanced optimal transport have been extended to handle mass variation. Existing numerical methods extend classical optimal transport to handle mass variation through optimal-entropy transport formulation. This formulation relaxes marginal constraints using divergences to find a measure that minimizes a cost function. Mass variation is allowed as the marginals of the measure are not constrained. The proposed algorithm for unbalanced optimal transport directly models mass variation, allowing for numerical methods to handle mass variation through optimal-entropy transport formulation. This algorithm is the first of its kind for unbalanced optimal transport in high-dimensional spaces. The proposed algorithm for unbalanced optimal transport directly models mass variation, allowing for numerical methods to handle mass variation through optimal-entropy transport formulation. This algorithm is the first of its kind for unbalanced optimal transport in high-dimensional spaces. Generalizing the Sinkhorn algorithm for computing regularized OT plans, this section introduces the first algorithm for unbalanced OT that directly models mass variation and can be applied to transport between high-dimensional continuous measures. The algorithm for unbalanced optimal transport aims to learn a stochastic transport map and scaling factor to push a source to a target measure in a cost-optimal manner. It involves penalizing the cost of mass transport and variation, with an equality constraint to ensure exact pushing of the source measure to the target measure. The algorithm for unbalanced optimal transport involves learning a stochastic transport map and scaling factor to push a source to a target measure in a cost-optimal manner. It penalizes the cost of mass transport and variation, with an equality constraint to ensure exact pushing of the source measure to the target measure. The more general case of stochastic maps is considered for practical problems, such as in cell biology where one cell can give rise to multiple cells in a target population. The unbalanced optimal transport algorithm involves learning a stochastic transport map and scaling factor to push a source to a target measure in a cost-optimal manner. It considers stochastic maps for practical problems, such as in cell biology where one cell can give rise to multiple cells in a target population. The transport map models the transformation from a source measure to a target measure, representing a population at two distinct time points. The transport map and scaling factor model the transformation of points from a source to a target measure, representing a population at different time points. Different transformation models are optimal based on the costs of mass transport and variation. The text discusses different transformation models for transporting points from a source to a target measure, considering costs of mass transport and variation. It highlights the importance of using an unbalanced transport map with a scaling factor to address class imbalances in the distribution. The text discusses using a scaling factor to address class imbalances in distribution by downweighing or upweighing samples. It introduces a relaxation method for optimization challenges in satisfying constraints, involving a divergence penalty instead of an equality constraint. This relaxation is a Monge-like version of the optimal-entropy transport problem. The text introduces a relaxation method for optimization challenges in satisfying constraints, involving a divergence penalty instead of an equality constraint. This relaxation is the Monge-like version of the optimal-entropy transport problem, using an appropriate choice of \u03c8 that satisfies specific requirements. In optimization challenges, the search space for formulations is limited by the choice of transport map T and scaling factor \u03be. Equivalence is only possible when restricting the joint measures \u03b3 to supp(\u00b5) \u00d7 Y. Equivalence can be established by restricting joint measures \u03b3 to supp(\u00b5) \u00d7 Y. Theoretical results for optimal entropy-transport can be derived from the analysis of the relaxed problem. Solutions of the relaxed problem converge to solutions of the original problem with a sufficiently large divergence penalty. Theoretical results for optimal entropy-transport can be derived from the analysis of the relaxed problem. Solutions of the relaxed problem converge to solutions of the original problem with a sufficiently large divergence penalty. Theorem 3.4 states that under certain conditions, solutions converge for a sequence 0 < \u03b6 DISPLAYFORM4. The Appendix discusses the convergence of joint measures in atomless probability spaces. The transport map and scaling factor can be learned using stochastic gradient methods for unbalanced Monge OT. The divergence term is minimized using an adversary function. The transport map and scaling factor can be learned through stochastic gradient methods, with the divergence term minimized using an adversary function. This optimization process involves alternating stochastic gradient updates and parameterizing neural networks for T, \u03be, and f. The procedure is akin to GAN training, resembling an adversarial game between (T, \u03be) and f. The optimization procedure involves gradient updates for parameterized neural networks T, \u03be, and f, similar to GAN training. T transports points from X to Y, \u03be assigns importance weights, and their objective is to minimize divergence from real samples. Cost functions c1 and c2 encourage cost-efficient strategies. The optimization procedure involves gradient updates for parameterized neural networks T, \u03be, and f, similar to GAN training. T transports points from X to Y, \u03be assigns importance weights, and their objective is to minimize divergence from real samples. Cost functions c1 and c2 encourage cost-efficient strategies. The probabilistic Monge-like formulation is related to the Kantorovich-like entropy-transport problem. Examples of divergences with corresponding entropy functions and convex conjugates are provided in Table 1 in the Appendix. Practical considerations for implementation and training are discussed in Appendix C. The probabilistic Monge-like formulation (6) is similar to the Kantorovich-like entropy-transport problem (3) in theory, but they result in different numerical methods in practice. Algorithm 1 solves the non-convex formulation (6) using neural networks to learn a transport map T and scaling factor \u03be, enabling scalable optimization with stochastic gradient descent. The networks are immediately useful for practical applications, requiring only a single forward pass for computation. The neural networks parameterize the transport map T and scaling factor \u03be, allowing scalable optimization with stochastic gradient descent. The networks are immediately useful for practical applications, requiring only a single forward pass for computation. However, due to the non-convexity of the optimization problem, Algorithm 1 may not find the global optimum. In contrast, the scaling algorithm of BID8 based on (3) solves a convex optimization. The scaling algorithm of BID8 is proven to converge for discrete problems but has limited scalability. A new stochastic method in the Appendix can handle transport between continuous measures, overcoming scalability issues. The new stochastic method in the Appendix generalizes the approach for handling transport between continuous measures and overcomes scalability limitations of BID8. However, the output is less interpretable for practical applications compared to Algorithm 1, making it unclear how to obtain a scaling factor or a stochastic transport map for generating samples outside the target dataset. The new stochastic method in the Appendix generalizes the approach for handling transport between continuous measures and overcomes scalability limitations of BID8. In the numerical experiments of Section 4, the advantage of directly learning a transport map and scaling factor using Algorithm 1 is shown. The problem of learning a scaling factor that \"balances\" measures \u00b5 and \u03bd arises in causal inference, where \u00b5 is the distribution of covariates from a control population and \u03bd is the distribution from a treated population. In causal inference, a scaling factor balances measures \u00b5 and \u03bd from control and treated populations. The goal is to eliminate selection biases in treatment effects by scaling the importance of different members based on their likelihood in the treated population. BID23 proposed a method for learning the scaling factor, but without considering transport. This section illustrates Algorithm 1 performing unbalanced OT. Algorithm 1 is used to perform unbalanced optimal transport between two modified MNIST datasets, focusing on population modeling. The source dataset contains regular MNIST digits with a specific class distribution, while the target dataset contains either regular or dimmed MNIST digits with a different class distribution. The source dataset contains regular MNIST digits with a specific class distribution, while the target dataset contains either regular or dimmed MNIST digits with a different class distribution. The class imbalance between the datasets imitates a scenario where certain classes become more popular while others become less popular, reflecting population drift. Algorithm 1 is evaluated for transporting the source distribution to the target distribution with a high cost of transport. Algorithm 1 is evaluated for transporting the source distribution to the target distribution, reflecting population drift and class imbalances. The scaling factor learned by Algorithm 1 can model growth or decline of different classes in a population. FIG4 illustrates the reweighting during unbalanced OT.MNIST-to-USPS. The scaling factor learned by Algorithm 1 reflects class imbalances and models growth or decline of different classes in a population. Unbalanced OT is applied from the MNIST dataset to the USPS dataset, simulating population evolution. The transport cost is the Euclidean distance between original and transported images. Algorithm 1 models the evolution of the MNIST distribution to the USPS distribution using unbalanced optimal transport. The transport cost is the Euclidean distance between original and transported images, visualized in FIG1. Arrows show the predicted appearance of MNIST images in the USPS dataset, reflecting scaling factors. The image reflects the scaling factor of the original MNIST image in the USPS dataset compared to the MNIST dataset. Euclidean distance is used to analyze likeness preservation during transport. MNIST digits with higher scaling factors appear brighter and cover a larger area of pixels. The model considers the prominence of MNIST digits based on scaling factors. Digits with higher scaling factors are brighter and cover more pixels. This is consistent with USPS digits being brighter and containing more pixels. Algorithm 1 was applied to the CelebA dataset for unbalanced OT from young to aged faces, simulating population transformation based on samples from two timepoints. The study applied Algorithm 1 to the CelebA dataset for unbalanced optimal transport (OT) from young to aged faces, modeling population transformation using samples from two timepoints. A variational autoencoder (VAE) was first trained on the dataset to encode all samples into the latent space before performing the unbalanced OT. The transport cost was based on the Euclidean distance in the latent space, and the results were visualized in FIG2. The study used Algorithm 1 on the CelebA dataset to perform unbalanced optimal transport from young to aged faces. The transport cost was based on Euclidean distance in the latent space. The results, visualized in FIG2, showed that most features were retained in the transported faces, with some exceptions like gender swaps. The study applied Algorithm 1 on the CelebA dataset for unbalanced optimal transport from young to aged faces. Results in FIG2 revealed that most features were preserved, with exceptions like gender swaps. Young faces with higher scaling factors were more likely to be male, indicating a predicted increase in male face prominence as the population ages. Our model predicts a growth in male face prominence compared to female faces as the CelebA population ages. There is a strong gender imbalance between young and aged populations, with the young population being predominantly female and the aged population predominantly male. The study focuses on lineage tracing of cells in zebrafish embryogenesis using single-cell gene expression data. The source population is from a late blastulation stage, and the target population is from an early gastrulation stage. The results of the transport are visualized in FIG3-c. Algorithm 1 analyzes single-cell gene expression data from two stages of zebrafish embryogenesis, comparing cells from late blastulation to early gastrulation. The results are visualized in FIG3-c after dimensionality reduction. Cells with higher scaling factors in the blastula stage were found to have upregulated genes, as determined by differential gene expression analysis and enrichment analysis using the GOrilla tool. The experiment analyzed single-cell gene expression data from two stages of zebrafish embryogenesis, comparing cells from late blastulation to early gastrulation. Cells with higher scaling factors in the blastula stage showed upregulated genes associated with differentiation and development of the mesoderm. This demonstrates the potential for biological discovery through analysis of scaling factors. A stochastic method for unbalanced OT based on the regularized dual formulation of BID7 is presented in this section. In this section, a stochastic method for unbalanced optimal transport (OT) is presented based on the regularized dual formulation of BID7. The dual formulation involves a constrained optimization problem that can be challenging to solve, but adding a strongly convex regularization term to the primal objective helps make the problem unconstrained. This regularization term, such as an entropic term, encourages transport plans with high entropy, leading to more meaningful biological discovery. Adding a strongly convex regularization term to the primal objective helps make the optimization problem unconstrained. This term encourages transport plans with high entropy, leading to more meaningful biological discovery. The dual of the regularized problem is given by a supremum over functions u, and the relationship between the primal optimizer \u03b3* and dual optimizer (u*, v*) is established. The relationship between the primal optimizer \u03b3* and dual optimizer (u*, v*) is given by DISPLAYFORM5. Rewriting equation (9) in terms of expectations, assuming access to samples from \u00b5, \u03bd, and their normalized measures, as well as normalization constants, leads to DISPLAYFORM6. Parameterizing u, v with neural networks u\u03b8, v\u03c6 and optimizing \u03b8, \u03c6 using stochastic gradient descent is described in Algorithm 2, a generalization of classical OT to unbalanced OT. The dual solution (u*, v*) learned from Algorithm 2 with neural networks can be used to reconstruct the primal solution \u03b3. The dual solution (u*, v*) learned from Algorithm 2 can be used to reconstruct the primal solution \u03b3 * which represents the amount of mass transported between points in X and Y. The marginals of \u03b3 * do not necessarily match \u00b5 and \u03bd, allowing for implicit mass variation in the problem. Additionally, an \"averaged\" deterministic mapping from X to Y can be learned using the barycentric projection T. The marginals of \u03b3 * with respect to X and Y may not be \u00b5 and \u03bd, allowing for implicit mass variation in the problem. An \"averaged\" deterministic mapping from X to Y can be learned using the barycentric projection T. A stochastic algorithm for learning such a map from the dual solution is presented in Algorithm 3. The objectives in (6) and (3) are equivalent if reformulated in terms of \u03b3 instead of (T, \u03be). The formulations are equivalent if the search space of (3) contains only joint measures specified by some (T, \u03be). This relation is formalized by Lemma 3.3, showing L \u03c8 (\u00b5, \u03bd) \u2265W c1,c2,\u03c8 (\u00b5, \u03bd) for any solution (T, \u03be) and defined \u03b3. Lemma 3.3 formalizes the relation between formulations by showing that L \u03c8 (\u00b5, \u03bd) \u2265W c1,c2,\u03c8 (\u00b5, \u03bd) for any solution (T, \u03be) and defined \u03b3. The inequality L \u03c8 (\u00b5, \u03bd) \u2264W c,\u03c81,\u03c82 (\u00b5, \u03bd) holds for any solution (T, \u03be) and \u03b3, where \u03b3 y|x is the pushforward measure of \u03bb under T x for all x \u2208 X. Proposition 9.1.2 and Theorem 13.1.1 in BID14 state the existence of a family of measurable functions {T x : Z \u2192 Y} x\u2208X such that \u03b3 y|x is the pushforward measure of \u03bb under T x for all x \u2208 X. Denoting T (x, z) : (x, z) \u2192 T x (z), a change of variables leads to the conclusion that (T, \u03be) satisfy certain relations. This implies the inequality L \u03c8 (\u00b5, \u03bd) \u2264W c,\u03c81,\u03c82 (\u00b5, \u03bd) holds for any solution (T, \u03be) and \u03b3. The proof shows that the joint measure \u03b3 specified by any minimizer of L \u03c8 (\u00b5, \u03bd) is unique, following the analysis of optimal entropy-transport by BID27. Several theoretical results for (6) can be derived from this analysis, such as the existence and uniqueness result when certain conditions are met. The joint measure \u03b3 specified by any minimizer of L \u03c8 (\u00b5, \u03bd) is unique under certain conditions, as shown in the analysis of optimal entropy-transport by BID27. The minimizer of L \u03c8 (\u00b5, \u03bd) also exists, and uniqueness is guaranteed when \u03c8 \u221e = \u221e. The minimizer of L \u03c8 (\u00b5, \u03bd) exists and is unique under certain conditions, as shown in the analysis of optimal entropy-transport by BID27. The product measure generated by the minimizers of L \u03c8 (\u00b5, \u03bd) is also unique, completing the proof. The uniqueness of \u03b3 follows from the proof of Corollary 3.6 in BID27. The product measure generated by the minimizers of L \u03c8 (\u00b5, \u03bd) is unique, completing the proof. L \u03c8 defines a proper metric between positive measures \u00b5 and \u03bd for certain cost functions and divergences, corresponding to the Hellinger-Kantorovich BID27 or the Wasserstein-Fisher-Rao BID9 metric. Theorem 3.4 states that solutions of the relaxed problem converge to solutions of the original problem. The convergence is shown through the pointwise convergence of \u03b6 k \u03c8(s) to the equality constraint \u03b9 = (s). This is supported by Lemma 3.9 in BID27. Theorem 3.4 proves that the relaxed problem solutions converge to the original problem solutions. The convergence is demonstrated by the pointwise convergence of \u03b6 k \u03c8(s) to the equality constraint \u03b9 = (s), supported by Lemma 3.9 in BID27. The sequence of minimizers \u03b3 k is bounded, as shown by Proposition 2.10 in BID27. The sequence of minimizers \u03b3 k is bounded and equally tight, leading to weak convergence to some \u03b3 by an extension of Prokhorov's theorem. The Markov inequality implies that \u03b3 is a minimizer of W c1,c2,\u03b9= (\u00b5, \u03bd) due to the bounded and equally tight sequence of minimizers \u03b3 k. The proof construction shows that \u03b3 k is related to the product measure induced by minimizers of L \u03b6 k \u03c8 (\u00b5, \u03bd), leading to a min-max problem formulation using the convex conjugate form of \u03c8-divergence. In this section, the convex conjugate form of \u03c8-divergence is used to rewrite the main objective as a min-max problem. For non-negative finite measures P, Q over T \u2282 R d, it holds that DISPLAYFORM0 where F is a subset of measurable functions. Equality holds if and only if certain conditions are met, and a simple proof of this result is provided. The convex conjugate form of \u03c8-divergence is used to rewrite the main objective as a min-max problem. A simple proof is provided for the conditions where the optimal function f belongs to the subdifferential of \u03c8( dP dQ ). This result has been used for generative modeling in previous studies. The optimal function f over the support of Q is obtained when dP dQ belongs to the subdifferential of \u03c8 * (f). The optimal function over the support of P \u22a5 is \u03c8 \u221e. Proposition B.1 provides conditions for the problem to be well-posed, with the cost of transport often being a measurement of correspondence between X and Y, such as the Euclidean distance after mapping to a common feature space. The cost of transport, c1, should be a measurement of correspondence between X and Y, like the Euclidean distance. For the cost of mass adjustment, c2, choose a convex function that vanishes at 1 and prevents \u03be from becoming too small or too large. The cost of mass adjustment, c2, should be a convex function that prevents \u03be from becoming too small or too large. Any entropy function from Table 1 can be used for Algorithm 1 to train generative models by matching a generated distribution P to a true data distribution Q. Jensen's inequality states that for any convex lower semi-continuous entropy function \u03c8, D \u03c8 (P |Q) is minimized when P = Q. The Jensen's inequality states that for any convex lower semi-continuous entropy function \u03c8, D \u03c8 (P |Q) is uniquely minimized when P = Q, where P, Q are probability measures. However, this does not generally hold when P, Q are not probability measures. In the original GAN paper, the discrminative objective corresponds to D \u03c8 (P |Q) with \u03c8(s) = s log s \u2212 (s + 1) log(s + 1) BID33. If P, Q are probability measures, this divergence is equivalent to the Jensen-Shannon divergence and is minimized when P = Q. If P, Q are non-negative measures with unconstrained total mass, the divergence is not minimized when P = Q. The divergence D \u03c8 (P |Q) with \u03c8(s) = s log s \u2212 (s + 1) log(s + 1) BID33 corresponds to the Jensen-Shannon divergence when P, Q are probability measures and is minimized when P = Q. When P, Q are not probability measures, an additional constraint on \u03c8 is needed to ensure divergence minimization matches P to Q. The divergence D \u03c8 (P |Q) is minimized when P = Q, with an additional constraint on \u03c8 needed to ensure minimization matches P to Q. If \u03c8(s) attains a unique minimum at s = 1 with specific conditions, then P = Q. Otherwise, P = Q in general when D \u03c8 (P |Q) is minimized. The divergence D \u03c8 (P |Q) is minimized when P = Q, with specific conditions needed for minimization. Examples of \u03c8 corresponding to common divergences are provided in Table 1. The choice of function f should map from Y to (\u2212\u221e, \u03c8 \u221e ]. Choice of function f for minimizing divergence D \u03c8 (P |Q) should belong to a class that maps from Y to (\u2212\u221e, \u03c8 \u221e ]. This can be achieved by parameterizing f using a neural network with an output activation layer. For neural architectures, fully-connected feedforward networks with ReLU activations were used in experiments, with a sigmoid function as the output activation layer for mapping final pixel brightness. For experiments in Section 4, fully-connected feedforward networks with 3 hidden layers and ReLU activations were used. The output activation layers were a sigmoid function for mapping pixel brightness to (0, 1) and a softplus function for mapping the scaling factor weight to (0, \u221e)."
}