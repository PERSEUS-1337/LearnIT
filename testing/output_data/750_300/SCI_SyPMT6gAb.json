{
    "title": "SyPMT6gAb",
    "content": "Off-policy learning involves evaluating and improving policies using historical data from a logging policy. This is important because on-policy evaluation is costly and has negative effects. A major challenge in off-policy learning is developing counterfactual estimators with low variance and generalization error. In this work, a new counterfactual learning principle for off-policy learning with bandit feedbacks is introduced. The method minimizes distribution divergence between logging policy and new policy, eliminating the need for sample variance regularization. End-to-end training algorithms using variational divergence minimization with neural network policies show significant improvement over conventional baseline algorithms. End-to-end training algorithms using variational divergence minimization showed significant improvement over conventional baseline algorithms for off-policy learning. This approach is important in scenarios where on-policy evaluation is expensive and risky, such as in clinical trials or online advertising A/B testing. Off-policy evaluation and learning are crucial in scenarios with adverse impacts, such as clinical trials and online advertising A/B testing. Utilizing historic data can enable safe exploration of policies before deployment. Various methods like Q learning and doubly robust estimator have been studied in the context of reinforcement learning and contextual bandits. Off-policy learning involves exploring policies safely before deployment using methods like Q learning and doubly robust estimator. Recent approaches include using logged interaction data with limited feedback, such as scalar rewards, in bandit feedback settings. This limits the information available about other possible actions and their potential rewards. In bandit feedback settings, limited feedback like scalar rewards restricts the information available about other possible actions and their potential rewards. This hinders off-policy learning using historic data. BID34 introduced a new counterfactual risk minimization framework to address the challenge of distribution mismatch in off-policy learning in bandit feedback settings. They added sample variance as a regularization term to improve empirical risk minimization, using linear stochastic models for policy parametrization. The new counterfactual risk minimization framework introduced by BID34 added sample variance as a regularization term to improve empirical risk minimization. However, the parametrization of policies as linear stochastic models has limited representation power, and computing sample variance regularization requires iterating through all training samples. Despite a first-order approximation technique proposed, deriving accurate and efficient training algorithms remains a challenge. The paper proposes a new learning principle for off-policy learning with bandit feedback, connecting to the generalization error bound of importance sampling. The policy is parametrized as a neural network to enable end-to-end training, minimizing distribution divergence between the new policy and the logging policy. This learning objective balances empirical risk and sample variance. The new policy minimizes distribution divergence from the logging policy, balancing empirical risk and sample variance. Parametrized as a neural network, it utilizes variational divergence minimization and Gumbel soft-max sampling. Experimental results show improved performance over conventional baselines, validating theoretical proofs. The evaluation on benchmark datasets demonstrates a significant performance improvement over conventional baselines. The framework of off-policy learning with logged bandit feedback is reviewed, where a policy maps inputs to structured outputs. Stochastic policies are used, defining a posterior distribution over the output space. In reinforcement learning, stochastic policies define a distribution over the output space given the input x. Actions are taken by sampling from this distribution, with each action having a probability of being selected. This allows for a significant performance improvement over conventional baselines in off-policy learning with logged bandit feedback. In reinforcement learning, stochastic policies correspond to a deterministic policy. Actions are taken by sampling from the distribution h(Y|x), with each action having a probability of h(y|x) being selected. Feedbacks \u03b4(x, y; y*) are observed in online systems, comparing the sampled action y to an underlying 'best' y*. The expected risk of a policy h(Y|x) is defined as DISPLAYFORM1, with the goal of off-policy learning being to find a policy with higher satisfaction. In off-policy learning, the goal is to find a policy with minimum expected risk on test data using data collected from a logging policy. The data includes observed loss feedback and logging probabilities, aiming to improve the policy for higher satisfaction. The goal in off-policy learning is to improve policy h(Y|x) with lower expected risks using data from a logging policy. Challenges include skewed distribution of logging policy and the need for empirical estimation due to finite samples, leading to generalization error. Off-policy learning aims to enhance policy h(Y|x) by reducing expected risks using data from a logging policy. Challenges include skewed distribution of logging policy, empirical estimation due to finite samples, and generalization error. To address these challenges, a propensity scoring approach using importance sampling can be used to account for distribution mismatch and rewrite the expected risk w.r.t h as the risk w.r.t h 0. With the historic dataset D, empirical risk R(h) can be estimated. The authors proposed a regularization term for sample variance derived from empirical Bernstein bounds to address flaws in the vanilla approach of counterfactual risk minimization. The modified objective function now includes the average of training data and variance regularization term. The authors proposed a regularization term for sample variance derived from empirical Bernstein bounds to improve counterfactual risk minimization. The modified objective function includes the average of training data and a variance regularization term, which is approximated using a first-order Taylor expansion to enable stochastic optimization. The authors introduced a regularization term for sample variance based on empirical Bernstein bounds to enhance counterfactual risk minimization. They utilized a first-order Taylor expansion for stochastic optimization, neglecting higher-order non-linear terms to reduce sample variance errors. The parametrized policy distribution h(Y|x) allowed for deriving a variance bound directly, avoiding empirical estimation from samples. The authors introduced a regularization term for sample variance based on empirical Bernstein bounds to enhance counterfactual risk minimization. Utilizing a first-order Taylor expansion for stochastic optimization, they reduced sample variance errors by neglecting higher-order non-linear terms. The parametrized policy distribution h(Y|x) enabled deriving a variance bound directly, avoiding empirical estimation from samples. The authors introduced a regularization term for sample variance based on empirical Bernstein bounds to enhance counterfactual risk minimization. Utilizing a first-order Taylor expansion for stochastic optimization, they reduced sample variance errors by neglecting higher-order non-linear terms. The parametrized policy distribution h(Y|x) enabled deriving a variance bound directly, avoiding empirical estimation from samples. Lemma 1 states an identity involving importance sampling weights and R\u00e9nyi divergence, leading to an upper bound for the second moment of the weighted loss. Theorem 1 provides conditions for random variables X and Y, along with a loss function, to derive an upper bound for the conditional divergence between two sampling distributions. Theorem 1 establishes conditions for random variables X and Y, along with a loss function, to derive an upper bound for the conditional divergence between two sampling distributions. This bound is similar to Eq. (4) but now considers a joint distribution over x, y. Theorem 2 provides a generalization bound between expected risk R(h) and empirical risk R(h) using distribution divergence function. Detailed proofs can be found in Appendix 1. Theorem 2 establishes a generalization bound between expected risk R(h) and empirical risk R(h) using distribution divergence function. The proof of this theorem involves the application of Bernstein inequality and the second moment bound, with detailed proofs in Appendix 7. This result highlights bias-variance trade-offs in empirical risk minimization problems. The proof of the theorem involves Bernstein inequality and the second moment bound, with detailed proofs in Appendix 7. It emphasizes bias-variance trade-offs in empirical risk minimization problems, motivating the minimization of variance regularized objectives in bandit learning settings. The model hyper-parameter \u03bb controls the trade-off between empirical risk and model variance. In bandit learning settings, minimizing variance regularized objectives is crucial. The model hyper-parameter \u03bb balances empirical risk and model variance, but setting it empirically is challenging. To address this, an alternative formulation of regularized ERM is explored through distributionally robust learning. This involves a constrained optimization formulation instead of a 'loss + regularizer' objective function. In light of recent success in distributionally robust learning, an alternative formulation of regularized ERM is explored. A constrained optimization approach is studied, with a regularization hyper-parameter \u03c1. The new formulation provides a good surrogate for the true risk, bounded by the regularization hyper-parameter \u03c1. The new objective function in distributionally robust learning provides a good surrogate for the true risk, bounded by the regularization hyper-parameter \u03c1. However, estimating the divergence function can be challenging with a parametrized distribution of h(y|x) and finite samples. Recent f-gan networks and Gumbel soft-max sampling techniques can help address this issue. Estimating the divergence function with a parametrized distribution of h(y|x) and finite samples is challenging. Recent f-gan networks and Gumbel soft-max sampling techniques can assist in variational divergence minimization. The need for stochasticity in the logging policy is emphasized for effective learning. Learning with a deterministic logging policy can be difficult due to limited exploration, resulting in an unbounded generalization bound that hinders counterfactual learning. Stochasticity in the logging policy is crucial for effective learning, and techniques like f-gan networks and Gumbel soft-max sampling can aid in minimizing variational divergence. The unbounded generalization bound in counterfactual learning is caused by the probability density of h 0 (y|x) being 0, making it impossible to minimize the square root of the condi- dy. The objective is to minimize D f (h||h 0 ; P(X)) with a convex function f (t) = t 2 \u2212 1. The calculation connects divergence with f-divergence measure BID25. By following the f-GAN method in BID26, a lower bound of the objective is reached. The dual formulation is obtained using Fenchel convex duality. In BID26, a lower bound of the objective is reached using Fenchel convex duality and the universal approximation theorem of neural networks BID15. The universal approximation theorem of neural networks BID15 allows neural networks to approximate continuous functions on a compact set with any desired precision. By choosing the family of neural networks as the family of functions, the final objective is a saddle point of a function that maps input pairs to a scalar value. The policy to be learned acts as a sampling distribution. The final objective is a saddle point of a function that maps input pairs to a scalar value, with the policy to be learned acting as a sampling distribution. The saddle point trained with mini-batch estimation is a consistent estimator of the true divergence, denoted as Df. The estimation error is decomposed in the proof. The proof decomposes the estimation error of the saddle point objective, with the first term coming from restricting the parametric family to neural networks and the second term involving the approximation error of empirical mean estimation. The universal approximation theorem ensures one term is zero, and the other term is rewritten using the true distribution. The proof decomposes the estimation error of the saddle point objective, with one term being zero due to the universal approximation theorem. The second term involves the approximation error of empirical mean estimation, which can be shown to approach zero using the strong law of large numbers. The proof utilizes the strong law of large numbers to show that one term approaches zero, while a generative-adversarial approach is applied to represent the T function as a discriminator network. The T function is represented as a discriminator network parametrized as T w (x, y), with the policy h(y|x) parametrized as a generator neural network h \u03b8 (y|x). Gumbel soft-max sampling is used for differential sampling from the distribution h(y|x) in structured output problems. The complete training procedure is listed for sampling from the logging policy h 0. The training procedure Alg. 1 is provided for sampling from the logging policy h 0, involving sampling from the distribution h(y|x) and updating the generator and discriminator functions iteratively to minimize divergence. Gumbel soft-max sampling is used for generating 'fake' samples in structured output problems. The training procedure involves sampling 'real' and 'fake' samples from D using Gumbel soft-max sampling. The algorithm presented solves the robust regularized formulation for counterfactual risk minimization from logged data. Additionally, training for the original ERM formulation is included. The algorithm presented in this section focuses on end-to-end learning for counterfactual risk minimization from logged data. It includes solving the robust regularized formulation and training for the original ERM formulation. The algorithm involves sampling from h0, regularization hyper-parameter \u03c1, and maximum iteration of divergence minimization steps I. The algorithm focuses on minimizing variance regularized risk through separate training steps: updating policy parameters to minimize reweighed loss and updating generator and discriminator parameters for regularization. The algorithm, set to I, focuses on minimizing variance regularized risk through separate training steps. It updates policy parameters to minimize reweighed loss and improves generalization performance by regularizing the variance of the new policy. Exploiting historic data is crucial in multi-armed bandit problems and its variants like contextual bandit. Various approaches, including doubly robust estimators, have been proposed to address this issue. Bandits problems, including contextual bandit, have wide applications. Doubly robust estimators have been proposed for these problems, and recent theoretical studies have explored the minimax risk lower bound. Bandits problems can be seen as single-state reinforcement learning problems, with techniques like Q function learning and temporal difference learning used for off-policy learning in RL. Recent works in deep RL have extended techniques like doubly robust estimators to RL domains. Q function learning and temporal difference learning are conventional methods for off-policy learning in RL. Deep RL studies have also addressed off-policy updates using methods such as multi-step bootstrapping and off-policy training of Q functions. Learning from log traces involves applying propensity scores to evaluate candidate policies, similar to treatment effect estimation in statistics. Off-policy training of Q functions involves learning from log traces using propensity scores to evaluate candidate policies, similar to treatment effect estimation in statistics. Unbiased counterfactual estimators have been derived for computational advertising, while variance regularization aims to improve generalization performance in bandit learning. Variance regularization in off-policy learning with bandit feedback aims to improve generalization performance by studying generalization bounds in importance sampling problems. The original purpose was to address covariate shift in supervised learning. Divergence minimization technique can be applied to supervised learning and domain adaptation problems to tackle distribution mismatch. Regularization for the objective function is closely related to distributionally robust optimization techniques. Regularization for the objective function in supervised learning and domain adaptation problems is closely related to distributionally robust optimization techniques. Wasserstein distance between empirical distribution and test distribution is a well-studied constraint for achieving robust generalization performance. Empirical evaluation of proposed algorithms involves converting from supervised learning to bandit feedback method. The distribution and test distribution constraint is crucial for achieving robust generalization performance. Empirical evaluation involves converting from supervised learning to bandit feedback method. Logging policy h0 is constructed for each sample x_i, with feedback collected as \u03b4(y*i, y_i). Conditional random field policy is used as the logging policy for benchmarks. Hamming loss is utilized as the loss metric. The logging policy h0 is used to collect feedback \u03b4(y*i, y_i) for each sample x_i. The conditional random field policy serves as the logging policy for benchmarks, with hamming loss as the loss metric. Bandit feedback datasets are created with samples passed four times to h0, actions recorded, and propensity scores calculated. Two evaluation metrics are used for the probabilistic policy h(Y|x), including expected loss (EXP). In evaluation, two types of metrics are used for the probabilistic policy h(Y|x): expected loss (EXP) and average hamming loss of maximum a posteriori probability (MAP). The expected loss measures the generalization performance, while MAP provides faster predictions based on regions with the highest probability. The MAP prediction is derived from the learned policy and is faster than sampling. However, it may lead to overfitting as it focuses on regions with high probability. Different policies with the same MAP performance can have varying generalization performance. Baselines include Vanilla importance sampling algorithms using IPS and counterfactual risk minimization. The comparison includes Vanilla importance sampling algorithms using IPS and counterfactual risk minimization (POEM) with L-BFGS and stochastic optimization solvers. Hyperparameters are chosen based on validation set performance. Neural network policies without divergence regularization (NN-NoReg) are also compared to assess variance effectiveness. The study compares Vanilla importance sampling algorithms using IPS and counterfactual risk minimization (POEM) with L-BFGS and stochastic optimization solvers. Neural network policies without divergence regularization (NN-NoReg) are also evaluated as baselines. Four multi-label classification datasets from the UCI machine learning repo are used, with detailed configurations provided in the Appendix. For benchmark comparison, a three-layer feed-forward neural network is used for policy distribution, and a two or three layer feed-forward neural network as the discriminator for divergence minimization. The networks are trained with Adam optimizer with learning rates of 0.001 and 0.01 for different parts. PyTorch and Nvidia K80 GPU cards are used for implementation and training. The neural networks were trained using Adam optimizer with different learning rates for reweighted loss and divergence minimization. PyTorch and Nvidia K80 GPU cards were used for implementation. Results show improved performance with neural network policies using Gumbel-softmax sampling schemes. The introduction of neural network policies with Gumbel-softmax sampling schemes significantly improved test performance compared to baseline CRF policies. Additional variance regularization further enhanced testing and MAP prediction loss. No significant difference was observed between the two Gumbel soft-max sampling schemes. The introduction of additional variance regularization term in neural network policies with Gumbel-softmax sampling schemes led to improvements in testing loss and MAP prediction loss. There was no significant difference observed between the two Gumbel soft-max sampling schemes. The effectiveness of variance regularization was studied quantitatively by varying the maximum number of iterations in each divergence minimization sub loop. By introducing additional variance regularization in neural network policies with Gumbel-softmax sampling, improvements in testing loss and MAP prediction loss were observed. Varying the maximum number of iterations in divergence minimization sub loops showed that models with regularization had lower test loss and faster convergence rates. By increasing the maximum iterations for divergence minimization, models with regularization show lower test loss and faster convergence rates. The regularization term helps policies generalize better to test sets, with stronger regularization leading to improved test performance. The training algorithm also converges faster with regularization, as indicated by the trend. Theoretical bounds suggest that algorithm generalization improves with more training samples. Regularization improves training algorithm convergence and generalization performance. Varying training data passes in bandit dataset shows increasing test performance with regularization. Regularized policies outperform models without regularization in generalization. In the bandit dataset, increasing samples lead to improved test performance for models with and without regularization, eventually stabilizing. Regularized policies demonstrate better generalization performance compared to non-regularized models. However, excessive training sample replay can lead to decreased MAP prediction performance, indicating potential overfitting. In experiments comparing two training schemes, stronger regularization leads to better generalization performance. However, excessive training sample replay can cause a decrease in MAP prediction performance, suggesting potential overfitting. The use of weighted loss and distribution divergence in the model with regularization slightly outperforms the model without regularization. In comparing two Gumbel-softmax sampling schemes, Gumbel-softmax and Straight-Through (ST) Gumbel-softmax, blending weighted loss and distribution divergence shows slight improvement over no regularization. However, balancing the gradient of the objective function makes training more challenging. There is no significant performance difference between the two sampling schemes. The impact of logging policies on learning performance and additional visualizations are discussed in this section. In this section, the effect of logging policies on learning performance is discussed. The algorithm's ability to improve policy depends on the stochasticity of the logging policy. Testing how this stochasticity affects learning involves modifying the parameter h 0 with a temperature multiplier \u03b1. For CRF logging policies, predictions are made by normalizing values of w T \u03c6(x, y), where w is the model parameter that can be adjusted by \u03b1 with w \u2192 \u03b1w. Additional visualizations of metrics can be found in Appendix 7. When testing the effect of logging policies on learning performance, the parameter h0 is modified with a temperature multiplier \u03b1. As \u03b1 increases, h0 becomes more deterministic. By varying \u03b1 in the range of 2 [-1, 1, ..., 8], it was observed that NN policies outperform logging policies when the stochasticity of h0 is sufficient. When testing the effect of logging policies on learning performance, NN policies outperform logging policies when the stochasticity of h0 is sufficient. However, as the temperature parameter increases beyond 2/3, it becomes harder and even impossible to learn improved NN policies. The drop in performance ratios is mainly due to the decreased loss of the logging policy h0. Additionally, NN policies with stronger regularization show slightly better performance. The drop in performance ratios is mainly due to the decreased loss of the logging policy h0. Policies with stronger regularization show slightly better performance, indicating the robustness of the learning principle. As h0 improves, models constantly outperform baselines, but the difficulty increases with the quality of h0. Regularization helps the model be more robust and achieve better generalization performance. Our regularization improves NN policy robustness and generalization performance. Models outperform baselines as h0 quality improves, but difficulty increases. Impact of logging policies on learned policies discussed, highlighting trade-offs between hamming loss and sampling biases. The study explores the trade-off between policy accuracy and sampling biases by varying the proportion of training data points used to train the logging policy. Improved policies outperform the logging policy, indicating they can address sampling biases. The study compares the performance of improved policies in addressing sampling biases. The increasing ratios of test expected loss to h0 performance indicate relative policy improvement. The paper proposes a new training principle inspired by learning bounds for importance sampling problems. The paper introduces a new training principle based on regularizing variance to enhance off-policy learning for logged bandit datasets. It combines importance reweighted loss with a regularization term measuring distribution divergence. Techniques like variational divergence minimization and Gumbel soft-max sampling are used to train neural network policies effectively. The paper introduces a new training principle based on regularizing variance to enhance off-policy learning for logged bandit datasets. By applying variational divergence minimization and Gumbel soft-max sampling techniques, neural network policies are trained end-to-end to minimize variance. Evaluations on benchmark datasets proved the effectiveness of the learning principle and training algorithm, with limitations mainly due to the need for propensity scores. Learning to estimate propensity scores and incorporating them into the training framework will improve the approach. The work discusses the limitations of needing propensity scores for off-policy learning from logged data. Estimating propensity scores and incorporating them into the training framework will enhance the algorithms' applicability. Directly learning importance weights is suggested as a potential extension. The techniques and theorems can be extended to general supervised learning and reinforcement learning. The work discusses the limitations of needing propensity scores for off-policy learning from logged data. Estimating propensity scores and incorporating them into the training framework will enhance the algorithms' applicability. Directly learning importance weights is suggested as a potential extension. The techniques and theorems may be extended to general supervised learning and reinforcement learning. The logging policy probability has comparable theoretical guarantees, which could be a good extension for the proposed algorithm. It will be interesting to study how Lemma 1 can be applied to importance sampling weight functions and loss functions. The work discusses the limitations of needing propensity scores for off-policy learning from logged data and suggests directly learning importance weights. By applying Lemma 1 and Bernstein's concentration bounds, bounds for importance sampling of bandit learning can be obtained. The algorithm aims to optimize the generator for approximate minimization of R(w) with a regularization hyper-parameter \u03bb. The algorithm minimizes variance regularized risk through co-training by updating the discriminator and generator iteratively. The generator is optimized to approximate the minimizer of R(w) with a regularization hyper-parameter \u03bb. The algorithm minimizes variance regularized risk through co-training by updating the generator with a mini-batch of samples from the dataset and estimating the gradient. The statistics of the datasets are reported in a table, showing the effect of stochasticity on test loss. As the logging policy becomes more deterministic, neural network policies still improve over the baseline in expected loss and loss with MAP predictions. The logging policy becoming more deterministic allows neural network policies to improve over the baseline in expected loss and loss with MAP predictions. There is no clear trend in the performance of MAP predictions, possibly due to the good MAP prediction performance of the h0 policy. Further investigation is warranted. Further investigation is needed as neural network policies struggle to beat baselines, especially with MAP predictions when the logging policy is well-trained."
}