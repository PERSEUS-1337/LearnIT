{
    "title": "rJg_NjCqtX",
    "content": "Chemical information extraction involves converting chemical knowledge in text into a chemical database by identifying and standardizing compound names. A framework is proposed in this paper to automatically standardize non-systematic names to systematic names using spelling error correction, byte pair encoding tokenization, and neural networks. Our framework proposes auto standardization from non-systematic names to systematic names using spelling error correction, byte pair encoding tokenization, and neural sequence to sequence model. The standardization accuracy on the test dataset is 54.04%, a significant improvement from previous results. There are over 100 million named chemical substances worldwide, each with unique names based on their structures. There are over 100 million named chemical substances worldwide, each with unique names based on their structures. The International Union of Pure and Applied Chemistry (IUPAC) defines rules for assigning systematic names to chemicals. In addition to systematic names, chemicals may also have common or trivial names for simplicity. Chemical substances can have multiple names for various reasons. Common names, like sucrose for sugar, are familiar and simple. In the pharmaceutical industry, new names are often created to differentiate products. In the pharmaceutical industry, producers create proprietary names for chemical substances to distinguish their products. Chemical information extraction research relies on standard chemical names stored in databases like PubChem and SciFinder. Extraction research converts chemical knowledge into databases using standard chemical names found in databases like PubChem and SciFinder. These databases store information such as chemical names, structures, and formulas, with ongoing work to update them by extracting information from chemical papers. If all substances are expressed using systematic names, it is easier to generate other information. The extraction research converts chemical knowledge into databases using standard chemical names from databases like PubChem and SciFinder. Systematic names make it easier to generate other information like SMILES and InCHI representations. OPSIN is a system that can convert systematic names to SMILES with high precision. Online systems like OPSIN are well developed for converting systematic names to SMILES strings with high precision. Non-systematic names can have errors like spelling, ordering, and common name errors. Non-systematic names can have errors such as spelling, ordering, common name, and synonym errors. For example, 2-(Acetyloxy)benzoic Acid has synonyms Acetylsalicylic Acid and Acetysal, sharing the same root word \"Acety\". The most common error in non-systematic names is the spelling error, which can coexist with ordering and synonym errors. This makes the task of converting non-systematic names to systematic names challenging. The framework proposed for converting non-systematic chemical names to systematic names includes spelling error correction, BPE tokenization, and a sequence to sequence model to address ordering and synonym errors. This task is challenging due to the mixed types of errors present. The framework proposed for standardizing chemical names includes a sequence to sequence model to address ordering and synonym errors. This approach differs from previous work like BID2, which relied heavily on chemical knowledge. The framework for standardizing chemical names utilizes a sequence to sequence model, different from previous methods that heavily rely on chemical knowledge. The model is based on neural machine translation, treating the task as similar to translating between source and target languages. The framework for standardizing chemical names uses a data-driven approach without external chemical knowledge. It achieves 54.04% accuracy on test data extracted from Chemical Journals with High Impact Factors. The corpus contains both non-systematic and systematic names of chemical substances. The corpus from Chemical Journals with High Impact Factors contains non-systematic and systematic chemical names. It includes 384816 data pairs, with an overview of the Levenshtein distance between the names. The experiment uses 80% training data, 19% test data, and 1% development data. In the experiment, the Levenshtein distance between non-systematic and systematic chemical names is analyzed using different data sets for training, testing, and development. The goal is to correct spelling errors by separating chemical names into elemental words and comparing them with vocabularies of systematic and non-systematic names. To correct spelling errors in chemical names, the experiment involves analyzing the Levenshtein distance between non-systematic and systematic names. This is done by separating names into elemental words and comparing them with vocabularies of systematic and non-systematic names. Two vocabularies are set up from the dataset: one for systematic elemental words and one for non-systematic elemental words. The non-systematic names are used to build an elemental vocabulary, keeping only the words that appear frequently. The experiment involves analyzing Levenshtein distance between non-systematic and systematic chemical names. Non-systematic names are used to build a vocabulary of common names or synonyms, which are combined with systematic names to create a final elemental vocabulary structured using BK-Tree. The Levenshtein distance between non-systematic and systematic chemical names is analyzed to create a final elemental vocabulary structured using BK-Tree, a tree structure widely used in spelling error correction. BK-Tree efficiently searches for vocabulary items with the smallest Levenshtein distance to a given word within a specified threshold. BK-Tree is used to correct spelling errors in non-systematic chemical names by efficiently searching for vocabulary items with the smallest Levenshtein distance within a specified threshold. It is scalable and easy to insert new training data, making it advantageous for this purpose. The BK-Tree is scalable and allows for easy insertion of new training data. It corrects spelling errors in non-systematic chemical names by breaking them into elemental words and using a sequence to sequence model. This process helps reduce noise and can partially correct non-systematic names. The BK-Tree, built from a dataset, uses Levenshtein distance to correct non-systematic chemical names by breaking them into elemental words. This process aids in training the sequence to sequence model by reducing noise. To apply the sequence-to-sequence model, chemical names are tokenized using Byte Pair Encoding (BPE) BID11. The symbol set is initialized by splitting names into characters and counting symbol pairs iteratively to replace the most frequent pair with a new symbol. The symbol set is initialized by splitting names into characters and counting symbol pairs iteratively to replace the most frequent pair with a new symbol. This process continues until a final symbol set is obtained, which is used for tokenization. The use of Byte Pair Encoding (BPE) helps in handling out-of-vocabulary problems and allows for the separation of names. After initializing the symbol set by splitting names into characters and counting symbol pairs, Byte Pair Encoding (BPE) is used for tokenization. BPE helps with out-of-vocabulary problems and separates names into meaningful subwords, allowing for the training of a sequence to sequence model. After initializing symbol set with character splitting and symbol pair counting, Byte Pair Encoding (BPE) is utilized for tokenization. Examples of BPE application to chemical names are shown in TAB1. The sequence to sequence model (BID12) is employed for machine translation, adapted from OpenNMT (BID5) with modifications. The model comprises an encoder generating a context vector H from non-systematic names split by BPE, and a decoder producing corresponding systematic names. The model utilizes a multilayers bidirectional LSTM encoder to generate a context vector H from non-systematic names split by BPE, which is then used by the decoder to produce corresponding systematic names. The model uses a bidirectional LSTM encoder to create a context vector H from non-systematic names split by BPE. The decoder generates systematic names based on this context vector. Parameters for spelling error correction include the threshold of the BK-Tree, while the BPE stage involves the number of merge operations. Different threshold values and merge operation numbers were tested in experiments. In the framework, parameters for spelling error correction include the threshold of the BK-Tree, while the BPE stage involves the number of merge operations. Different values were tested in experiments for both parameters. The sequence to sequence model uses 500 dimensions for word embeddings and hidden states, with a vocabulary size determined by basic characters and BPE merge operations. The encoder and decoder each have 2 layers, and spelling error correction is done for non-systematic names before training the model. The sequence to sequence model has 2 layers in both encoder and decoder. Spelling error correction is performed on non-systematic names before training. Parameters are trained jointly using SGD with a cross-entropy loss function. The minibatch size is 64, weights are initialized randomly, and the initial learning rate is 1.0 with a decay factor of 0.5. The loss is computed over a minibatch of size 64 and normalized. Weights are initialized with a random uniform distribution. Learning rate starts at 1.0 and decays by 0.5 every epoch after epoch 8 or when perplexity doesn't decrease. Dropout rate is 0.3, training lasts for 15 epochs, and beam size is set to 5 for decoding. Another experiment replaces the sequence to sequence model with a Statistical Machine Translation (SMT) model using the Moses system BID6. Training length is limited in this experiment. In the experiment, the training sequences are limited to 80 with a 3-grams language model using KenLM. Tokenization is done with BPE and 5000 merge operations. Data augmentation is used for spelling error correction and dealing with noisy data. Non-systematic names have errors inserted for comparison. Data augmentation is a technique used for neural model learning to handle noisy data, such as spelling errors. In an experiment, errors are inserted into non-systematic names with four different methods. Standardization quality is measured using accuracy and BLEU score BID10. In the experiment, four insertion methods are used to introduce errors into non-systematic names. Accuracy and BLEU score BID10 are used to measure standardization quality. The experiment results for different models on the test dataset are shown in TAB3. The experiment results for different models on the test dataset are shown in TAB3, with the combination of spelling error correction, BPE tokenization, and sequence to sequence model achieving the best performance. The framework shows significant improvement compared to the SMT model and the ChemHits system. The results for different numbers of BPE merge operations are shown in TAB4, with 5000 being the optimal value. The results show that BPE tokenization is useful, with 5000 merge operations being the best value. Spelling error correction is helpful, while data augmentation has some benefit but not as effective. Overcorrection may occur with large Levenshtein distance thresholds. The study demonstrates the effectiveness of spelling error correction in the framework, with data augmentation providing some benefit but not as effective. Overcorrection may occur with large thresholds. Examples in Table 6 show successful standardization of non-systematic names by the sequence to sequence model, correcting spelling and synonym errors. The sequence to sequence model successfully corrects spelling and synonym errors in non-systematic chemical names, as demonstrated in examples provided. The sequence to sequence model corrects errors in non-systematic chemical names, including spelling and ordering errors. An example is given with the visualization of attentions in FIG2, showing the relation between adenine and 9H-purin-6-amine. The seq2seq model corrects errors in non-systematic chemical names, including spelling and ordering errors. It can find the relation between adenine and 9H-purin-6-amine and place 9-methyl correctly. Examples of successfully standardized names are shown in Table 6. The section also analyzes failed standardization attempts. In this section, failed standardization attempts are analyzed by manually labeling error types. Synonym errors are the most confusing, while spelling errors are handled well by the system. Common errors are challenging due to the lack of rules between common and systematic names. Among the 100 samples analyzed, synonym errors were found to be the most confusing error type, while spelling errors were handled well by the system. Common errors posed a challenge due to the lack of rules between common and systematic names. Nearly half of the non-systematic names were not successfully standardized. The accuracy of systematic names varies based on length, with the framework performing best for names between 20 and 40 characters. However, it struggles with names longer than 60 characters, which make up 37% of the test dataset. The model does not consider chemical rules, leading to some names being generated incorrectly. The model performs well for systematic names between 20 and 40 characters but struggles with names longer than 60 characters, which account for 37% of the test dataset. The model does not consider chemical rules, leading to some incorrect name generations. The model proposes a framework to convert non-systematic chemical names to systematic names through spelling error correction and tokenization. It struggles with names longer than 60 characters and does not consider chemical rules, leading to some incorrect name generations. The framework proposed in this work aims to automatically convert non-systematic chemical names to systematic names using spelling error correction, tokenization, and a sequence to sequence model. It achieves an accuracy of 54.04% on the dataset, outperforming previous rule-based systems. The framework is end-to-end trained, data-driven, and independent of external chemical knowledge, opening up new possibilities for chemical information extraction. The framework proposed in this work enables practical use of chemical information extraction. It is trained end to end, data-driven, and independent of external chemical knowledge, starting a new research line in this field."
}