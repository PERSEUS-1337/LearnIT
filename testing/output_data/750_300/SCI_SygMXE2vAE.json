{
    "title": "SygMXE2vAE",
    "content": "BERT, a Transformer-based model, has achieved state-of-the-art results in various NLP tasks. This study presents a layer-wise analysis of BERT's hidden states to gain a better understanding of its internal functioning. Unlike previous research that focuses on attention weights, this analysis emphasizes the valuable information contained in hidden states. The study specifically examines how BERT models fine-tuned for Question Answering transform token vectors. Our analysis focuses on how QA models transform token vectors to find answers, using probing tasks to reveal information in each layer. Visualizations of hidden states in BERT show a reasoning process related to traditional pipeline tasks. Our qualitative analysis of hidden state visualizations in BERT reveals insights into the system's reasoning process. The transformations within BERT correspond to traditional pipeline tasks, allowing for implicit incorporation of task-specific information. Fine-tuning has minimal impact on semantic abilities, and prediction errors can be identified in early layer vector representations. Transformer models have gained popularity in Natural Language Processing for their advancements over RNNs. Transformer models have become prevalent in Natural Language Processing due to their improvements over RNNs. Prediction errors can be recognized in early layer vector representations. Large models and extensive pre-training have further enhanced their capabilities. Permission to use this work for personal or classroom use is granted without fee, as long as it is not used for profit. Copyrights for components owned by others must be respected. Republishing or redistributing requires permission. Contact permissions@acm.org. The paper discusses BERT BID8, a popular Transformer model in Natural Language Processing. It addresses the issue of black box models and highlights the model's significant improvements over previous state-of-the-art models in various benchmarks and tasks. BERT BID8 is a popular Transformer model that has shown significant improvements over previous models in various benchmarks and tasks. The problem of black box models in deep learning is a major concern due to the lack of transparency, reliability, and prediction guarantees. While Transformers are considered somewhat interpretable through attention values, recent research suggests this may not always be the case. The lack of transparency, reliability, and prediction guarantees in black box models is a major issue. While Transformers are thought to be somewhat interpretable through attention values, recent research questions this belief. This paper proposes a new approach to interpreting Transformer Networks by examining hidden states between encoder layers. It aims to address whether Transformers answer questions decompositionally like humans, if specific layers in a multi-layer network solve different tasks. This paper examines hidden states between encoder layers in Transformers to address questions about their decompositional answering, task-solving abilities across layers, influence of fine-tuning, and network failure analysis in Question Answering tasks. The paper examines hidden states in Transformers to understand their task-solving abilities and network failure analysis in Question Answering tasks. The study focuses on the BERT architecture and includes preliminary tests on the small GPT-2 model, showing similar results. The proposed layer-wise visualization of token representations provides insights into the internal state of the models. The study focuses on the BERT architecture and includes preliminary tests on the small GPT-2 model. A layer-wise visualization of token representations reveals information about the internal state of Transformer networks. General NLP Probing Tasks are applied, along with QA-specific tasks like Question Type Classification and Supporting Fact Extraction, to analyze BERT's abilities within its layers and the impact of fine-tuning. BERT, a Transformer network, is analyzed through NLP Probing Tasks and QA-specific tasks to understand how its layers are impacted by fine-tuning. The study shows that BERT's transformations follow similar phases across different tasks, with general language properties encoded in earlier layers and utilized in later layers for solving downstream tasks. The analysis focuses on BERT and Transformer models like GPT-2, which transform representations across network layers. GPT-2, an improved version of GPT, has shown proficiency in language modeling tasks. Other Transformer models may also be of interest for future analysis. Open-AI has decided not to release their pre-trained models for larger versions of BERT due to their proficiency in language modeling tasks. Other Transformer models like Universal Transformer and TransformerXL aim to improve flaws in the Transformer architecture. Interpretability and explainability of neural models are areas of increasing research interest. Interpretability and probing of neural models have gained significant research interest. Recent work focuses on applying probing tasks post-hoc to trained models, with BERT being a notable subject of investigation. Tenney et al. introduced an \"edge-probing\" framework with nine tasks to analyze BERT's capabilities. The majority of current works focus on creating or applying general purpose probing tasks, with BERT being a common subject of investigation. Tenney et al. introduced an \"edge-probing\" framework with nine tasks to analyze contextualized word embeddings of ELMo, BERT, and GPT-1. Other studies have added more probing tasks, with a specific focus on BERT architecture and analyzing it in the context of a Ranking task. The analysis of BERT in the context of a Ranking task involves probing attention values in different layers and measuring performance for representations built from various BERT layers. Other studies explore models through qualitative visual analysis or by studying single node activations in tasks like speech recognition. Zhang and Zhu BID41 survey different approaches limited to CNNs, Nagamine et al. BID24 study phoneme recognition in DNNs through single node activations for speech recognition. Hupkes et al. BID14 conduct qualitative analysis and train diagnostic classifiers to support hypotheses. Li et al. BID17 analyze word vectors and specific dimensions for sequence tagging and classification tasks. Liu et al. BID20 also perform layer-wise analysis of BERT's token representations. Our work on word vectors and specific dimensions for sequence tagging and classification tasks builds on previous studies by Liu et al. BID20, who analyzed BERT's token representations. Unlike Liu et al., we focus on models fine-tuned on downstream tasks and analyze the specific phases of BERT. Our motivation stems from Jain and Wallace BID15, who question the suitability of attention in solving certain issues. Our work is motivated by Jain and Wallace BID15, who question the effectiveness of attention in addressing explainability and interpretability issues. We propose evaluating hidden states and token representations instead of focusing on the general transferability of the network. Our analysis is centered on fine-tuned BERT models, examining the transformations applied to input tokens through two approaches. Our analysis focuses on fine-tuned BERT models and examines the transformations of input tokens. We qualitatively analyze token vectors in vector space and quantitatively evaluate language abilities on QA-related tasks. The architecture of BERT allows us to track token transformations throughout the network, providing insights into changes in token representations at each layer. The architecture of BERT and Transformer networks allows for tracking token transformations throughout the network. A qualitative analysis is conducted by collecting hidden states from each layer for randomly selected samples from the test set. This provides insights into the changes in token representations at each layer. BERT's pre-trained models use vector dimensions of 1024 (large model) and 512 (base model) for representing tokens. Dimensionality reduction is applied to visualize relations between tokens by fitting the vectors into two-dimensional space using T-distributed. To visualize relations between tokens in BERT's pre-trained models, dimensionality reduction techniques like PCA, t-SNE, and ICA are applied to reduce vector dimensions into two-dimensional space. K-means clustering is used to verify the distribution of clusters in 2D space. After applying dimensionality reduction techniques like PCA, t-SNE, and ICA to visualize token relations in BERT's pre-trained models, k-means clustering is used to verify cluster distribution in 2D space. The number of clusters chosen is based on observed clusters in PCA, with resulting clusters aligning with observations in 2D space. Semantic probing tasks are then applied to analyze information stored within transformed tokens at each layer to understand the model's abilities. After visualizing token relations in BERT's pre-trained models using dimensionality reduction techniques, semantic probing tasks are applied to analyze information stored within transformed tokens at each layer to understand the model's abilities. Edge Probing is used to translate NLP tasks into classification tasks for standardized probing mechanisms. The principle of Edge Probing, introduced by Tenney et al., translates NLP tasks into classification tasks to standardize probing mechanisms. Tasks like Named Entity Labeling, Coreference Resolution, Relation Classification, Question Type Classification, and Supporting Fact Identification are adopted for language understanding and reasoning. Named Entity Labeling involves predicting the correct entity category for a given span of tokens. The tasks added for language understanding and reasoning include Named Entity Labeling, Coreference Resolution, and Relation Classification. Named Entity Labeling requires predicting entity categories for token spans, Coreference Resolution predicts if mentions refer to the same entity, and Relation Classification identifies relationships between entities. The Coreference task involves predicting if two mentions in a text refer to the same entity, while Relation Classification requires predicting the relation type connecting two known entities. Source code for all experiments will be made publicly available. Question Type Classification is crucial for identifying the type of question being asked. The SemEval 2010 Task 8 dataset includes English web text and nine directional relation types. Question Type Classification is essential for identifying the type of question being asked, using the Question Classification dataset with 500 fine-grained question types. Supporting Facts extraction is crucial for answering questions. The text discusses fine-grained question types and the extraction of supporting facts for Question Answering tasks, particularly in multihop cases. It explores how BERT's token transformations help distinguish important context parts from distracting ones. A probing task is constructed to identify Supporting Facts by predicting if a sentence contains relevant information for a specific question. The text discusses constructing a probing task to identify Supporting Facts by predicting if a sentence contains relevant information for a specific question. This task tests the hypothesis that token representations contain information about their significance to the question. The text discusses constructing a probing task to identify Supporting Facts by predicting if a sentence contains relevant information for a specific question. This task tests the hypothesis that token representations contain information about their significance to the question. SQuAD does not require multi-hop reasoning, so the sentence containing the answer phrase is considered the Supporting Fact. Probing tasks are created for each dataset to evaluate their ability to recognize relevant parts. Samples are labeled as supporting facts or not. Input tokens are embedded with a fine-tuned BERT model for each probing task sample, across all layers. The text discusses creating a probing task to identify Supporting Facts by predicting if a sentence contains relevant information for a specific question. Input tokens are embedded with a fine-tuned BERT model for each probing task sample, considering all layers. Only tokens of \"labeled edges\" within a sample are used for classification, followed by feeding them into a Multi-layer Perceptron classifier for label-wise probability scores prediction. The text discusses using a Multi-layer Perceptron classifier to predict label-wise probability scores for tokens within a sample. This process is applied to pretrained BERT models without fine-tuning to understand the model's abilities. The aim is to analyze how BERT performs on complex downstream tasks like Question Answering, which require a combination of simpler tasks such as Coreference Resolution and Relation Modeling. The text discusses using a Multi-layer Perceptron classifier to predict label-wise probability scores for tokens within a sample. Our aim is to understand how BERT works on complex downstream tasks like Question Answering, which require a combination of simpler tasks. Three different Question Answering datasets are analyzed: SQUAD BID30, bAbI BID39, and HotpotQA BID40. The analysis aims to diversify the results. Currently, detention is a common punishment in schools in the UK, Ireland, and other countries. Students have to stay in school during specific times and may have to do work or sit quietly. HotpotQA is a Multihop QA task with 112,000 question-answer pairs. Detention is a common punishment in schools where students have to stay in school during specific times. HotpotQA is a Multihop QA task with 112,000 question-answer pairs, focusing on the distractor-task with a context of supporting and distracting facts. The distracting facts are reduced by a factor of 2.7 to accommodate the input size of the pre-trained BERT model. HotpotQA involves a context with supporting and distracting facts, averaging 900 words. To fit the BERT model's input size limit of 512 tokens, distracting facts are reduced by 2.7 times. The bAbI tasks are artificial toy tasks designed to test neural models' abilities, requiring reasoning over multiple sentences and including Positional Reasoning, Argument Relation Extraction, and Coreference Resolution. These tasks are distinct in their simplicity compared to other QA tasks. The tasks in this section require reasoning over multiple sentences and include Positional Reasoning, Argument Relation Extraction, and Coreference Resolution. The models analyzed are BERT BID8 and GPT-2 BID28, both based on Transformer architecture and incorporating ideas from previous models like ELMo and ULMFit. The models BERT BID8 and GPT-2 BID28 are based on Transformer architecture and incorporate ideas from previous models like ELMo and ULMFit. They have a similar architecture, with GPT-2 being the decoder half and BERT using both encoder and decoder. The models integrate into a probing setup and are trained using Pytorch implementation. BERT and GPT-2 models are based on Transformer architecture, with GPT-2 as the decoder half and BERT using both encoder and decoder. The models are fine-tuned on datasets with hyperparameters tuned through grid search. Training involves adjusting learning rate, batch size, and scheduling for 5 Epochs with evaluations on the development set. For training BERT and GPT-2 models, hyperparameters like learning rate and batch size are fine-tuned through grid search. Models are trained for 5 Epochs with evaluations on the development set. Input length is set at 384 tokens for bAbI and SQuAD tasks, and 512 tokens for HotpotQA tasks. The study evaluates BERT and GPT-2 models on various tasks such as SQuAD, HotpotQA, and bAbI. Different settings like Span prediction and Sequence Classification are considered. Span prediction involves appending all possible answers to the context for better results. In the study, BERT and GPT-2 models are evaluated on tasks like SQuAD, HotpotQA, and bAbI. Span prediction involves adding all possible answers to the context for improved performance. HotpotQA tasks are divided into Support Only (SP) and Distractor tasks to simplify the context and enhance token vector distinction. The HotpotQA Distractor task includes distracting sentences within the context to stay within the 512 token limit. Evaluation results show that models perform close to human level on SQuAD but struggle more with tasks derived from HotpotQA, especially in the distractor setting. The tasks derived from HotpotQA are more challenging, especially the distractor setting. GPT-2 performs better on bAbI tasks compared to BERT, with lower validation error. Tasks 17 and 19 in bAbI pose challenges requiring positional or geometric reasoning. GPT-2 shows improvement in reasoning capabilities in these areas. The qualitative analysis of vector transformations in the bAbI multi-task setting reveals recurring patterns that show GPT-2's improvement in reasoning capabilities, particularly in tasks 17 and 19. Results from probing tasks are compared in macro-averaged F1 over all network layers. Results from probing tasks on the bAbI task dataset and HotpotQA are compared in macro-averaged F1 over all network layers for different BERT models. PCA representations show the model going through multiple phases while answering questions in various QA tasks. BERT-large with 24 layers was fine-tuned on HotpotQA and PCA representations of tokens suggest the model goes through multiple phases when answering questions. These phases are observed in different QA tasks and supported by probing task results. The model clusters tokens semantically in early layers, as shown in Figures 4a and 5a. In BERT-based models, early layers group tokens into topical clusters, resembling Word2Vec embedding spaces. These initial layers lack task-specific information and perform poorly on semantic probing tasks. The middle layers show clusters of entities less connected by topical similarity. In BERT-based models, early layers group tokens into topical clusters similar to Word2Vec embedding spaces. Middle layers of neural networks show clusters of entities connected by relation within a specific input context, rather than topical similarity. Task-specific clusters include question-relevant entities, aiding in solving specific questions. The clusters in the neural networks show entities connected by relations within specific input contexts, aiding in solving specific questions. One cluster reveals the common punishment of detention in schools in the UK and Ireland, while another cluster highlights the relationship between Emily being a wolf and wolves being afraid of cats. The HotpotQA model also shows similar clusters with more coreferences. The HotpotQA model displays clusters of entities connected by relations within specific contexts. Emily's relation to wolves, including the plural form Wolves, is observed in these clusters. The model's ability to recognize Named Entities improves in higher network layers, while coreference resolution and relation recognition require input from additional layers. FIG7 visualizes these abilities. Named Entities are learned first in higher network layers, while coreference and relation recognition require input from additional layers until model performance peaks. BERT models excel at matching questions with supporting facts, crucial for Question Answering and Information Retrieval. BERT models excel at matching questions with supporting facts, crucial for Question Answering and Information Retrieval. The models transform tokens to match question tokens onto relevant context tokens, with the ability to distinguish relevant from irrelevant information. The BERT model excels at matching questions with supporting facts by transforming tokens to align question and context tokens in the vector space. Results show that the model's ability to distinguish relevant information improves in higher layers, as seen in FIG1 for SQuAD and bAbI datasets. However, the fine-tuned HotpotQA model in FIG2 does not show significant improvement, indicating a challenge in identifying correct Supporting Facts. The fine-tuned HotpotQA model in FIG2 is less distinct from the model without fine-tuning and does not reach high accuracy. This inability indicates why the BERT model does not perform well on this dataset as it is not able to identify the correct Supporting Facts. The vector representations enable us to tell which facts a model considered important, helping retracing decisions and making the model more transparent. Answer Extraction in the last network layers shows that the model separates the correct answer tokens from the rest of the tokens. The model dissolves previous clusters in the last network layers, separating correct answer tokens from the rest. The vector representation becomes task-specific during fine-tuning, leading to a performance drop in NLP tasks. This loss of information is especially noticeable in the large BERT model fine-tuned on HotpotQA. The performance drop in general NLP probing tasks is visualized in FIG7, with a focus on the loss of information in last-layer representations in the large BERT model fine-tuned on HotpotQA, as shown in FIG2. The fine-tuned model loses the ability to perform well on tasks like NEL or COREF, indicating a decline in performance. Analogies to Human Reasoning are drawn, comparing the phases of answering questions to the human reasoning process, highlighting the importance of semantic clustering and building relations between context parts for answering questions. The first phase of semantic clustering in NLP involves decomposing input into parts, followed by building relations between these parts to connect information for answering questions. Separating important from irrelevant information and grouping potential answer candidates are also key steps. Unlike humans who read sequentially, BERT can process all input parts simultaneously, allowing for concurrent phases depending on the task. BERT differs from human abstraction as it can process all input parts simultaneously, allowing for concurrent phases depending on the task. Comparing BERT to GPT-2, one major difference is that GPT-2 gives particular attention to the first token of a sequence. GPT-2 focuses on the first token of a sequence, leading to a separation of clusters during dimensionality reduction. This problem occurs in all layers except for the Embedding Layer, the first Transformer block, and the last one. The first token is masked during further analysis to address this issue. GPT-2 focuses on the first token of a sequence, leading to cluster separation during dimensionality reduction. The last layer's hidden state in GPT-2 shows separation of Supporting Facts and questions in the vector space, with an additional sentence extracted. Unlike BERT, the correct answer is not distinctly separated in GPT-2. These findings suggest that the analysis extends beyond the BERT architecture. The analysis of GPT-2 extends beyond the BERT architecture, showing separation of Supporting Facts and questions in the hidden state representations. Visualizations reveal failure states and task difficulty, with future work focusing on confirming these observations with more probing tasks. One aspect of explainable Neural Networks is to identify when, why, and how the network fails. Visualizations can show failure states and task difficulty, with insights gained by examining early layers for wrong predictions. When a wrong candidate answer is chosen with reasonable confidence, early layers can reveal reasons such as wrong Supporting Fact selection. If network confidence is low, transformations do not follow the same pattern. In early layers, the wrong Supporting Fact may be matched with the question. When network confidence is low, transformations do not go through the usual phases, resulting in little impact from fine-tuning. In some cases, when network confidence is low, the network maintains 'Semantic Clustering' similar to Word2Vec in later layers. Fine-tuning has little impact on the core NLP abilities of the model, as it already contains sufficient information about words and their relations for multiple downstream tasks. Fine-tuning only makes small weight changes and forces the model to forget some information to fit specific tasks, but it retains previously learned encoding for QA tasks. The positional embedding is crucial for Transformer networks' performance, addressing the lack of sequential information compared to RNNs. Visualizations confirm its importance, showing that it is added only once before the first layer. The positional embedding in Transformer networks addresses the lack of sequential information compared to RNNs. Visualizations show its effects are maintained even in late layers. Fine-tuning on SQuAD improves performance from layer 5 onwards, highlighting the relevance of resolving question types. The performance curves show that fine-tuning on SQuAD improves model performance from layer 5 onwards by resolving question types. In contrast, fine-tuning on bAbI tasks leads to a loss in the ability to distinguish question types due to the static structure of the samples. The study found that fine-tuning on bAbI tasks hinders the ability to distinguish question types, as the answer candidates can be recognized by sentence structure and word patterns. Surprisingly, fine-tuning on HotpotQA did not improve model performance compared to the model without fine-tuning. This suggests that BERT-large is pre-trained to recognize question types. The findings shed light on the inner workings of Transformer networks and their implications for future research. The qualitative analysis of token vectors in Transformer networks reveals interpretable information that can identify misclassified examples and model weaknesses. It also provides insights into which context parts are crucial for decision legitimization. This information was presented at CIKM '19 in Beijing, China. The information presented at CIKM '19 in Beijing, China discussed how Transformer models can help identify misclassified examples and model weaknesses, providing insights into important context parts for decision legitimization. It also highlighted the transferability of lower layers in certain problems and suggested individual layer depth selection for Transfer Learning tasks. Further research on skip connections in Transformer layers was recommended for examining direct information transfer between non-adjacent layers. The study suggests that specific layers in Transformer networks solve different problems, indicating a potential modularity that can be leveraged in training. It is proposed to tailor parts of the network to specific tasks during pre-training instead of using an end-to-end language model. Further research on skip connections in Transformer layers is recommended to explore direct information transfer between non-adjacent layers. Our work aims to reveal the internal processes within Transformer-based models and suggests further research to thoroughly understand state-of-the-art models for improving downstream tasks."
}