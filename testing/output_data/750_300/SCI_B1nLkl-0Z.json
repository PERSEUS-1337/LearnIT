{
    "title": "B1nLkl-0Z",
    "content": "State-action value functions, known as Q-values, are commonly used in reinforcement learning algorithms like SARSA and Q-learning. A new concept of action value is proposed, defined by a Gaussian smoothed version of the expected Q-value in SARSA. These smoothed Q-values still satisfy a Bellman equation, making them learnable from experience. The gradients of expected reward can be obtained from the gradient and Hessian of the smoothed Q-value function, allowing for the development of new algorithms to train a Gaussian policy directly from a learned Q-value. The gradient and Hessian of the smoothed Q-value function allow for training a Gaussian policy directly from a learned Q-value approximator. This approach achieves strong results on continuous control benchmarks by learning both mean and covariance during training. The ability to learn mean and covariance during training enables strong results on continuous control benchmarks in policy evaluation and improvement processes of model-free reinforcement learning algorithms. Different notions of Q-value have led to distinct families of RL methods. Different notions of Q-value have led to distinct families of RL methods, such as SARSA, Q-learning, Soft Q-learning, and PCL, each utilizing different forms of Q-value to update policies and maximize value functions. The choice of Q-value function significantly impacts the resulting algorithm's performance by influencing the types of policies that can be implemented. In this work, a new notion of action value is introduced: the smoothed action value function Q \u03c0, which associates a value with a distribution over actions rather than a specific action at each state. This concept differs from previous notions and has implications for policy implementation and exploration strategies in reinforcement learning algorithms. The smoothed Q-value function Q \u03c0 associates a value with a distribution over actions, unlike previous notions that associate a value with a specific action at each state. It is defined as the expected return of taking an action sampled from a normal distribution centered at a, then following actions sampled from the current policy. Smoothed Q-values have properties that make them attractive for use in reinforcement learning algorithms. The smoothed Q-value can be seen as a Gaussian-smoothed version of the expected Q-value, possessing properties that make it useful in reinforcement learning algorithms. It satisfies single-step Bellman consistency, allows for bootstrapping with function approximators, and can be used in optimization objectives for Gaussian policies. The gradient of the optimization objective with respect to the policy parameters is equivalent to the gradient and Hessian of the smoothed Q-value function. The optimization objective can be expressed using smoothed Q-values. The gradient of this objective with respect to the policy parameters is equivalent to the gradient and Hessian of the smoothed Q-value function. This leads to the Smoothie algorithm, which trains a policy using derivatives of a trained Q-value function, avoiding high variance in standard policy gradient algorithms. Smoothie algorithm proposes a policy training method using derivatives of a smoothed Q-value function, avoiding high variance in standard policy gradient algorithms. It utilizes a non-deterministic Gaussian policy for better exploration without excessive hyperparameter tuning. Smoothie algorithm introduces a Gaussian policy with mean and covariance parameters for exploratory behavior and reduced hyperparameter tuning. It can incorporate proximal policy optimization techniques by penalizing KL-divergence, improving stability and performance. Results on continuous control benchmarks are competitive, especially for challenging tasks with low data. Our formulation improves stability and performance in the standard DDPG algorithm for continuous control benchmarks, competing with state-of-the-art results, especially in low-data scenarios. The goal is to find an agent that maximizes cumulative discounted reward in a Markov decision process framework. The problem involves finding an agent that achieves maximal cumulative discounted reward in a Markov decision process (MDP) framework. The agent interacts with the environment by emitting actions and receiving reward feedback. The behavior of the agent is modeled using a stochastic policy \u03c0 that produces a distribution over feasible actions at each state. The optimization objective is the expected discounted return as a function of the policy. The agent in a Markov decision process interacts with the environment using a stochastic policy \u03c0 that produces action distributions. The optimization objective is the expected discounted return, expressed in terms of the action value function Q \u03c0 (s, a) and the Bellman equation. The discount factor \u03b3 \u2208 [0, 1] is used in the recursive definition of Q \u03c0 (s, a). Sampling and state transition distributions are often omitted for brevity. The policy gradient theorem expresses the gradient of the optimization objective with respect to the tunable parameters of a policy. Reinforcement learning algorithms balance variance and bias when estimating the action value function Q \u03c0 (s, a) using function approximation. In this paper, the focus is on multivariate Gaussian policies for continuous action spaces. The MDP state is represented as a feature vector, and the Gaussian policy is parametrized by mean and covariance functions. The paper focuses on multivariate Gaussian policies for continuous action spaces, representing the MDP state as a feature vector. The Gaussian policy is parametrized by mean and covariance functions. New RL training methods are developed for this family of policies, with potential generalization to other policy families. Prior work on learning Gaussian policies is reviewed. The paper reviews prior work on learning Gaussian policies, focusing on a new formulation called the deterministic policy gradient for Gaussian policies. This formulation applies when the policy covariance approaches zero, making the policy deterministic. The key observation is that under a deterministic policy, one can estimate the expected future return from a state. The key observation of BID21 is that under a deterministic policy, one can estimate the expected future return from a state. The gradient of the optimization objective for a parameterized policy can be expressed, and the Bellman equation can be re-expressed in the limit of the policy covariance approaching zero. The value function approximator can be optimized by minimizing the Bellman error for transitions sampled from interactions with the environment. In practice, algorithms like DDPG alternate between improving the value function through gradient descent and enhancing the policy. To improve sample efficiency, methods like BID5 and BID21 replace the on-policy state distribution with an off-policy distribution based on a replay buffer. This substitution affects the policy gradient identity, but prior work shows that it is effective. In practice, algorithms like DDPG alternate between improving the value function through gradient descent and enhancing the policy. To gain better sample efficiency, BID5 and BID21 replace the on-policy state distribution with an off-policy distribution based on a replay buffer. This substitution affects the policy gradient identity, but prior work shows that it is effective. Additionally, introducing smoothed action value functions in our method provides an effective signal for optimizing the parameters of a Gaussian policy. Smoothed Q-values, denoted Q \u03c0 (s, a), differ from ordinary Q-values in that they do not assume the first action. Smoothed action value functions are introduced, with gradients optimizing Gaussian policy parameters. Smoothed Q-values, denoted Q \u03c0 (s, a), differ by assuming only the mean of the first action is known. Expectation of Q \u03c0 (s, \u00e3) for actions \u00e3 near a is computed. The expected reward objective for a Gaussian policy \u03c0 is re-expressed. Smoothed action value functions are introduced, with gradients optimizing Gaussian policy parameters. Q \u03c0 (s, a) is directly learned as a function approximator, enabling direct bootstrapping of smoothed Q-values. Bellman consistency is maintained due to the form of Q \u03c0 (s, a). By directly learning a function approximator for Q \u03c0 (s, a), smoothed Q-values can be bootstrapped. The Bellman equation enables optimization of Q \u03c0, allowing for the learning of Gaussian policy parameters. By parameterizing a Gaussian policy \u03c0 \u03b8,\u03c6 with mean and covariance parameters, one can use derivatives of Q \u03c0 to learn \u00b5 and \u03a3. Estimating the derivative w.r.t. covariance parameters is more complex, but the second derivative of Q \u03c0 w.r.t. actions is sufficient for optimization. The second derivative of Q \u03c0 w.r.t. actions allows for exact computation of the derivative w.r.t. covariance parameters. The proof of this identity is provided in the Appendix using standard matrix calculus. The full derivative w.r.t. \u03c6 can be optimized in two ways using samples. Target values in residuals are treated as fixed for optimization of Q. The full derivative w.r.t. \u03c6 can be optimized in two ways using samples. One approach involves using a target network to achieve a fixed point, while the other approach uses a single function approximator for Q \u03c0 w (s, a). This simpler implementation is used in experimental evaluation. The second approach involves using a single function approximator for Q \u03c0 w (s, a) and optimizing it by minimizing a weighted Bellman error. Sampling from a replay buffer with knowledge of the sampling probability q(\u00e3 | s), a phantom action is drawn and the objective is optimized for a specific pair of state and action. The training procedure for Q \u03c0 w (s, a) involves minimizing a weighted Bellman error. It is unnecessary to track probabilities q(\u00e3 | s) when using a replay buffer with near-uniform distribution of actions. Other recent work has also benefited from ignoring importance weights. When using a replay buffer with a near-uniform distribution of actions, it is unnecessary to track probabilities q(\u00e3 | s). Policy gradient algorithms can be unstable in continuous control problems, leading to the development of trust region methods to mitigate this issue. Policy gradient algorithms are unstable in continuous control problems, leading to the development of trust region methods to mitigate this issue. Stabilizing techniques like constraining gradient steps within a trust region or adding a penalty on KL-divergence have not been applicable to deterministic policies like DDPG. However, a proposed formulation in this paper allows for trust region optimization by augmenting the objective with a penalty. The paper proposes a formulation that allows for trust region optimization in algorithms like DDPG, which have deterministic policies. By augmenting the objective with a penalty, the optimization becomes straightforward using the KL-divergence of two Gaussians. This approach builds on previous work using Q-value functions to learn a stable policy for approximating expected or optimal future value. The paper introduces a method that extends deterministic policy gradient approaches by incorporating policy covariance. This allows for trust region optimization in algorithms like DDPG, enhancing stability in learning policies for future value approximation. The proposed method extends deterministic policy gradient by incorporating policy covariance, leading to stable learning of policies for future value approximation. The approach differs from SVG by providing updates for the covariance and estimating the smoothed Q-value function instead of using noisy Monte Carlo samples for mean updates. The proposed method extends deterministic policy gradient by incorporating policy covariance for stable learning. Unlike SVG, it provides updates for covariance and estimates the smoothed Q-value function instead of using noisy Monte Carlo samples for mean updates. Methods for updating the covariance along the gradient of expected reward are essential for trust region and proximal policy techniques. Expected policy gradients (EPG) generalize DDPG by updating the mean and covariance of a stochastic Gaussian policy using gradients of an estimated Q-value function. EPG avoids approximate integrals by directly estimating the integral via the smoothed Q-value function. The novel training scheme proposed for learning the covariance of a Gaussian policy relies on properties of Gaussian integrals and avoids approximate integrals by directly estimating the integral via the smoothed Q-value function. The paper proposes a novel training scheme for learning the covariance of a Gaussian policy based on Gaussian integrals. It introduces a perspective where Q-values represent the averaged return of a distribution of actions, different from recent advances in distributional RL focusing on the distribution of returns of a single action. The paper introduces a new RL algorithm, Smoothie, which focuses on the averaged return of a distribution of actions. It utilizes insights from Gaussian policies and trains a parameterized Q \u03c0 w using gradients and Hessians. Smoothie is a new RL algorithm that focuses on the averaged return of action distributions. It trains a parameterized Q \u03c0 w using gradients and Hessians to train a Gaussian policy \u00b5 \u03b8 , \u03a3 \u03c6. See Algorithm 1 for the pseudocode. Smoothie is evaluated against DDPG as a baseline due to their similar use of gradient information for Q-value approximation. DDPG is known for its good, sample-efficient performance on continuous control benchmarks. The evaluation starts with a simple synthetic task. Smoothie is evaluated against DDPG as a baseline in a simple synthetic task with a reward function mixture of two Gaussians. Smoothie learns both the mean and variance during training. The policy mean is initialized centered on the worse Gaussian in a mixture of two Gaussians. Smoothie learns both mean and variance, while DDPG only learns the mean. DDPG struggles to escape local optima due to fixed exploratory noise scale. Smoothie successfully solves the task by pointing the policy mean towards the better Gaussian, unlike DDPG which struggles to escape local optima due to limitations in updating the mean. Smoothie successfully solves the task by adjusting the covariance during training. The policy mean is guided towards the better Gaussian, leading to near-zero covariance as it approaches the global optimum. Comparatively, DDPG struggles with local optima due to limitations in updating the mean. Smoothie adjusts policy variance to escape local optima and successfully solve tasks. Policy mean approaches global optimum with near-zero covariance. DDPG struggles with local optima due to mean updating limitations. Smoothie adapts policy variance based on reward function convexity/concavity. Standard continuous control benchmarks on OpenAI Gym BID3 using MuJoCo environment are also explored. Smoothie adjusts policy variance based on reward function convexity/concavity. Implementations utilize feed forward neural networks for policy and Q-values, with covariance parameterized as a diagonal. Exploration for DDPG is determined by an Ornstein-Uhlenbeck process. Additional implementation details are provided in the Appendix. Each plot displays average reward and standard deviation from six randomly seeded runs. Smoothie is competitive with DDPG, even outperforming it in tasks like Hopper, Walker2d, and Humanoid. TRPO is not sample-efficient enough to be a competitive baseline. During training, Smoothie shows advantages in final reward performance, especially in challenging tasks like Hopper, Walker2d, and Humanoid. TRPO is not sample-efficient for competitive baselines. Comparison results between Smoothie and DDPG are shown in FIG2, with hyperparameter search conducted for actor learning rate, critic learning rate, and reward scale. DDPG's search also includes exploratory noise parameters. Smoothie introduces an additional hyperparameter. Smoothie outperforms DDPG in various tasks, showing competitive or better performance. Smoothie has an advantage in Swimmer and Ant, with significant improvements in Hopper, Walker2d, and Humanoid tasks. The hyperparameter search for DDPG includes exploratory noise parameters, while Smoothie introduces an additional hyperparameter for KL-penalty weight determination. Smoothie outperforms DDPG in various tasks, exhibiting a slight advantage in Swimmer and Ant, with more significant improvements in Hopper, Walker2d, and Humanoid. The results for Humanoid are the best published for a method training on millions of environment steps, compared to TRPO which requires tens of millions of steps for comparable performance. Smoothie outperforms DDPG in various tasks, showing improvements in Swimmer, Ant, Hopper, Walker2d, and Humanoid. Smoothie requires millions of environment steps for training, while TRPO needs tens of millions for similar performance. The introduction of a KL-penalty in Smoothie enhances its performance, especially on harder tasks. This penalty for stability is not feasible in DDPG, making Smoothie a valuable solution. Smoothie's performance is enhanced by the introduction of a KL-penalty, particularly on harder tasks. This penalty is not possible in DDPG, making Smoothie a valuable solution to the instability in DDPG training. The use of proximal policy optimization, especially in Hopper and Humanoid, results in significant performance improvements without sacrificing sample efficiency. Additionally, a new Q-value function, Q \u03c0, is introduced, which offers advantages over the standard expected Q-value in terms of gradient and Hessian properties. Smoothie introduces a new Q-value function, Q \u03c0, which has advantages over the standard expected Q-value in terms of gradient and Hessian properties. This Gaussian-smoothed version allows for successful learning of both mean and covariance during training, leading to performance that can match or surpass DDPG, especially with a penalty on policy divergence. The success of Q \u03c0 is promising for improving performance without sacrificing sample efficiency. The success of Q \u03c0 in learning both mean and covariance during training can match or surpass DDPG, especially with a penalty on policy divergence. Smoothed Q-values make the reward surface smoother and have a direct relationship with the expected discounted return objective. Future work should explore applying these ideas to other policies. Q-values have a direct relationship with the expected discounted return objective. Future work should investigate these claims and techniques for applying Q \u03c0 motivations to other policies. The specific identity mentioned can be derived using standard matrix calculus. The text discusses deriving state using matrix calculus and simplifying equations for succinctness. It also mentions techniques for tackling different parts of a formula."
}