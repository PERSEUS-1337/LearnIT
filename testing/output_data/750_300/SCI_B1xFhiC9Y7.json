{
    "title": "B1xFhiC9Y7",
    "content": "Predicting structured outputs like semantic segmentation requires expensive per-pixel annotations for training convolutional neural networks. To address the challenge of generalizing to new domains without annotations, a domain adaptation method is proposed. This method involves learning discriminative feature representations of patches based on label histograms in the source domain and using adversarial learning to refine the features. The proposed method aims to learn discriminative feature representations of patches based on label histograms in the source domain and use adversarial learning to align the feature distributions between target and source patches for semantic segmentation. The framework integrates global alignment and patch-level alignment, achieving state-of-the-art performance on various benchmark datasets in different scenarios. Recent deep learning-based methods have shown significant progress in vision tasks like object recognition and semantic segmentation. However, models trained on annotated data often struggle to generalize to new domains. Domain adaptation methods have been developed to bridge this gap between source and target domains. Domain adaptation methods have been developed to improve generalization of learned models to new domains without annotations. While image classification has seen progress in this area, there is still room for improvement in pixel-level prediction tasks like semantic segmentation, where ground truth annotation is costly. Despite recent advancements in domain adaptation for pixel-level prediction tasks like semantic segmentation, there is still significant room for improvement. Domain adaptation is crucial for pixel-level predictions due to the high cost of annotating ground truth. Existing methods use feature-level or output space adaptation to align distributions between source and target domains using adversarial learning. Existing state-of-the-art methods for domain adaptation in pixel-level prediction tasks use feature-level or output space adaptation to align distributions between source and target domains. Global distribution alignment may not be effective due to differences in camera pose or field of view, leading to misalignment and incorrect bias during adaptation. Instead, the focus is on matching patches that are more likely to be shared across domains. In domain adaptation for pixel-level prediction tasks, existing methods focus on aligning distributions between source and target domains by matching patches that are likely shared, rather than globally aligning images. This approach aims to reduce misalignment and incorrect bias during adaptation by considering label histograms of patches. In domain adaptation for pixel-level prediction tasks, existing methods focus on aligning distributions between source and target domains by matching patches that are likely shared. Inspired by learning disentangled representations, the approach considers label histograms of patches to learn discriminative representations for patches. The approach focuses on learning discriminative representations for source patches and aligning them with target patches using adversarial modules to match global and patch-level distributions between domains. The approach utilizes adversarial modules to align global and patch-level distributions between domains by learning discriminative representations from pixel-level annotations in the source domain. The approach uses K-means clustering to group patch representations into clusters for training a classifier shared across domains. An adversarial loss is then used to align feature representations of target patches with the distribution of source patches in a clustered space. The approach utilizes K-means clustering to group patch representations for training a shared classifier across domains. An adversarial loss is employed to align target patch features with the source patch distribution in a clustered space. The representation learning is guided by label histograms and differs from methods using pre-defined factors like object pose. Experiments include synthetic-to-real and cross-city scenarios for pixel-level road-scene image segmentation. The proposed framework utilizes global and patch-level alignments for pixel-level road-scene image segmentation in various scenarios. Extensive ablation studies validate the components, showing favorable performance compared to state-of-the-art methods. The framework is general and could be applied to other structured outputs like depth in future work. The proposed framework for structured output prediction utilizes global and patch-level adversarial learning modules, discriminative representations guided by label histograms, and outperforms various baselines in terms of accuracy and visual quality. It is noted that the framework is general and could be applied to other forms of structured outputs in future work. In this work, a method is developed to learn discriminative representations guided by label histograms of patches through clustering, improving patch-level alignment. The proposed adaptation method outperforms various baselines and state-of-the-art methods in semantic segmentation. Domain adaptation approaches for image classification and pixel-level prediction tasks are discussed, along with algorithms for learning disentangled representations. Domain adaptation methods have been developed for image classification tasks by aligning feature distributions between source and target domains. Conventional approaches use hand-crafted features to minimize domain discrepancies, while recent algorithms utilize deep architectures to learn domain-invariant features. Adversarial learning schemes and Maximum Mean Discrepancy are commonly employed to achieve this, with various classifiers and loss functions being designed for different variants. One common practice in domain adaptation is to utilize deep architectures to learn domain-invariant features. Adversarial learning schemes and Maximum Mean Discrepancy are used to minimize domain discrepancies. Different classifiers and loss functions have been developed for variants. Recent work focuses on enhancing feature representations through pixel-level transfer and domain separation. Domain adaptation for structured pixel-level predictions, such as semantic segmentation for road-scene images, is not widely studied. The BID14 method introduces domain adaptation for semantic segmentation of road-scene images using adversarial networks to align global feature representations. BID36's CDA method applies SVM classifier for label capture without relying on inconsistent category-specific priors. The CDA method BID36 uses SVM classifier to capture label distributions on superpixels for domain adaptation, while BID4 performs class-wise domain adversarial alignment with pseudo labels. Object priors from Google Street View aid in alignment for static objects. These methods focus on global distribution alignment for structured output. Our proposed method focuses on learning discriminative representations for patches to aid in patch-level alignment without the need for additional priors or annotations. The network can be trained end-to-end, unlike previous methods that rely on global distribution alignment and class-specific priors for structured output. Our proposed method focuses on learning discriminative representations for patches to aid in patch-level alignment without the need for additional priors or annotations. The framework allows for end-to-end training and emphasizes learning patch-level representations to assist in the alignment process. Additionally, the approach of learning a latent disentangled space has shown benefits in various tasks such as facial recognition, image generation, and view synthesis. Learning a latent disentangled space has benefits for tasks like facial recognition, image generation, and view synthesis. Approaches use pre-defined factors to learn interpretable representations of images, such as graphic codes for 3D rendering and rotation factors for synthesizing 3D objects from a single image. Various image transformations, such as pose and lighting, are used for rendering 3D images. Methods like BID35 and AC-GAN focus on learning latent representations based on specific factors like rotation and image labels. To build on this research, we propose learning discriminative representations for patches to aid in domain adaptation tasks. Our proposed domain adaptation framework focuses on learning discriminative representations for patches to aid in the task. The framework utilizes label distributions as disentangled factors without the need for pre-defined factors. Additionally, an adversarial learning scheme is employed to align distributions across domains for predicting structured outputs. Our proposed domain adaptation framework focuses on aligning distributions across domains for predicting structured outputs. It utilizes discriminative representations for patches and employs an adversarial learning scheme. The goal is to align the predicted output distribution of target data with the source distribution. The proposed domain adaptation framework aims to align distributions between source and target data for predicting structured outputs. It incorporates a loss function for supervised learning on the source data and an adversarial loss to align global distributions. Additionally, a classification loss in a clustered space is used to learn patch-level discriminative representations from the source output distribution. An adversarial loss is also employed to align patch-level distributions between source and target data, pushing the target distribution closer to the source distribution. The adaptation task involves aligning patch-level distributions between source and target data using adversarial loss functions. The goal is to push the target distribution closer to the source distribution by utilizing global and patch-level adversarial loss functions. The adaptation task involves aligning patch-level distributions between source and target data using global and patch-level adversarial loss functions denoted as L g adv and L l adv, respectively, with weights \u03bb. The baseline model includes a supervised cross-entropy loss L s and an output space adaptation module using L g adv for global alignment. Figure 3 illustrates the main components and loss functions of the method. The baseline model includes a supervised cross-entropy loss Ls and an output space adaptation module using Lg adv for global alignment. The loss Ls can be optimized by a fully-convolutional network G that predicts the structured output with the loss summed over the spatial map indexed with h, w and the number of categories C. The adversarial loss Lg adv follows the practice of GAN training by optimizing G and a discriminator. The adversarial loss Lg adv follows the practice of GAN training by optimizing G and a discriminator D g that performs binary classification to distinguish source and target images. The min-max problem for G and D g is optimized with inputs dropped for simplicity. Transferable structured output representations are shared across source and target images. Based on the practice of GAN training, a min-max problem is optimized for a generator G and discriminator Dg to distinguish between source and target images. Transferable structured output representations are shared across the images, leading to the proposal of performing patch-level domain alignment. This involves clustering patches between domains rather than aligning all patches directly. The network architecture includes a generator G and a categorization module H for learning discriminative patch representations. The proposed network architecture includes a generator G and a categorization module H for learning discriminative patch representations through clustering patches from the source domain and guiding target patches to adapt to the disentangled space of source patch representations. The proposed method involves constructing prototypical patch patterns from source-domain examples with ground truth segmentation labels. Target domain patches then adapt to this disentangled space of source patch representations through adversarial guidance, selecting the closest cluster regardless of spatial location. Learning discriminative representations is challenging without class labels, so unsupervised clustering is applied. In this work, the authors utilize per-pixel annotations in the source domain to create a semantically disentangled space of patch representations by using label histograms for patches as the disentangled factor. Random sampling of patches from source images is done to achieve this goal. In this work, label histograms for patches are used as a disentangled factor to construct a semantically disentangled space of patch representations. Patches are randomly sampled from source images, and a 2-by-2 grid is used to extract spatial label histograms, which are then concatenated into a vector. K-means clustering is applied to these histograms, and a classification module is added to the network G for training on source data. In this approach, label histograms for patches are utilized to create a semantically disentangled space of patch representations. Patches from source images are sampled and spatial label histograms are generated, which are then clustered using K-means. A classification module is integrated into the network G during training on the source data to learn a discriminative representation. The learning process involves constructing a clustered space using a cross-entropy loss. Adversarial alignment is used to align target patches to the clustered space, regardless of their location. The learning process involves constructing a clustered space using a cross-entropy loss. Adversarial alignment is used to align target patches to the clustered space, regardless of their location, by reshaping the data and formulating an adversarial objective. The optimization process for training a GAN involves updating the discriminator Dg, discriminator Dl, and network G and H alternately while fixing the discriminators. NETWORK OPTIMIZATION involves updating the discriminator Dg and discriminator Dl to distinguish between source and target distributions, minimizing binary cross-entropy loss. The minimization problem in FORMULA6 involves updating the Discriminator Dg and Discriminator Dl to classify feature representations from the source or target domain, pushing the target distribution closer to the source distribution while maintaining good performance on main tasks using Network G and H. This combines supervised loss functions FORMULA1 and FORMULA4 with adversarial loss functions expressed as binary cross-entropy. The minimization problem in FORMULA6 involves combining supervised loss functions FORMULA1 and FORMULA4 with adversarial loss functions to update Discriminator Dg and Discriminator Dl. Updating H enhances feature representations in G, which is only required during testing, maintaining good performance on main tasks. The discriminator Dg and Dl have different architectures, with Dg using a spatial map O as input and Dl using a K-dimensional vector. Dg has 5 convolution layers with leaky ReLU activation, while Dl has 3 fully-connected layers with leaky ReLU activation. The generator G enhances feature representations and is only required during testing. The generator G with a categorization module H follows the DeepLab-v2 framework with ResNet-101 architecture pre-trained on ImageNet. A leaky ReLU activation is used in the discriminator Dl with 3 fully-connected layers. The proposed architecture adds a module H on the output prediction O using an adaptive average pooling layer to generate a spatial map. This map is then processed through two convolution layers to produce a feature map F with channel number K. The framework is implemented using the PyTorch toolbox on a single Titan X GPU with 12 GB memory. Training of the discriminators is done using the Adam optimizer with an initial learning rate of 10^-4. The proposed architecture includes a module H for output prediction O, processed through convolution layers to generate feature map F with channel number K. Implementation details involve using PyTorch on a Titan X GPU with 12 GB memory. Training involves Adam optimizer for discriminators and Stochastic Gradient Descent for the generator. Learning rates are adjusted using polynomial decay. Ablation study on GTA5-to-Cityscapes with ResNet-101 network is conducted, showing corresponding loss functions for each setting. The proposed architecture involves output prediction through convolution layers to generate feature maps. Training includes using Adam optimizer for discriminators and Stochastic Gradient Descent for the generator, with learning rates adjusted using polynomial decay. Ablation study on GTA5-to-Cityscapes with ResNet-101 network shows corresponding loss functions for each setting, with specific hyper-parameters detailed in the appendix. The proposed framework involves training the model initially with only the loss L s for 10K iterations to reduce noisy predictions, followed by training with all loss functions for 100K iterations. The hyper-parameters, including image and patch sizes, are detailed in the appendix. The framework is evaluated for domain adaptation on semantic segmentation, with an ablation study on GTA5-to-Cityscapes scenario and comparison against state-of-the-art approaches on benchmark datasets. The study evaluates a domain adaptation algorithm on semantic segmentation, comparing it against state-of-the-art methods on benchmark datasets. It includes adapting synthetic datasets like GTA5 and SYNTHIA to real datasets like Cityscapes, considering different scenarios and domain gaps. The training and testing sets are split following a specific method, and the algorithm is tested under various weather conditions in different cities. To adapt synthetic datasets like GTA5 and SYNTHIA to real datasets like Cityscapes, the study splits training and test sets following a specific method. It also adapts Cityscapes sunny images to the Oxford RobotCar BID23 dataset containing rainy scenes by selecting and annotating sequences for training and testing. 895 images are sampled for training, and 271 images are annotated with per-pixel semantic segmentation ground truth for evaluation. The dataset is annotated with the rainy tag and split into 7 sequences for training and 3 for testing. 895 images are used for training, and 271 images are annotated for per-pixel semantic segmentation ground truth evaluation. The intersection-over-union (IoU) ratio is used as the metric for evaluation. An ablation study is conducted on the GTA5-to-Cityscapes scenario to analyze the impact of different loss functions and design choices in the proposed framework. In an ablation study on the GTA5-to-Cityscapes scenario, different loss functions and design choices in the proposed framework are analyzed. Adding disentanglement without alignments improves performance, demonstrating enhanced feature representation. The method combining global and patch-level alignments achieves the highest IoU. Our method combining global and patch-level alignments achieved the highest IoU at 43.2%, demonstrating enhanced feature representation. The losses Ld and Ll adv are necessary for the alignment process, with a performance loss of 1.9% and 1.5% if either is removed. The losses Ld and Ll adv are crucial for alignment, with a performance drop of 1.9% and 1.5% if removed. Ld constructs a clustered space for Ll adv to perform patch-level alignment effectively. ReshapedF is used in module H to transform features for input to discriminator Dl, without which IoU performance drops by 2.4%. Without reshaping the features as independent data points, the performance drops by 2.4% in IoU. Visualization of feature representations using t-SNE shows that with adaptation, features are embedded into groups and source/target representations overlap. The clustered space visualization of patch-level features with adaptation shows well-overlapping source/target representations. Experimental comparisons with state-of-the-art algorithms in various scenarios, including synthetic-to-real cases, demonstrate the effectiveness of the proposed method. Experimental results for adapting GTA5 to Cityscapes are presented, comparing VGG-16 and ResNet-101 architectures. The proposed method outperforms state-of-the-art adaptations in feature, pixel-level, and output space alignments. It shows a 1.8% improvement in IoU and achieves the best IoU in 14 out of 19 categories. The proposed method utilizing ResNet-101 base network improves IoU by 1.8% and achieves the best IoU in 14 out of 19 categories. Results for adapting SYNTHIA to Cityscapes show similar improvements compared to state-of-the-art methods. Visual comparisons are shown in Figure 5, with more results in the appendix. Adapting between real images across different cities and conditions is highlighted as a significant scenario for practical applications. The proposed method improves IoU by 1.8% using ResNet-101 base network and achieves the best IoU in 14 out of 19 categories. Results for adapting SYNTHIA to Cityscapes are presented in Figure 5, with more comparisons in the appendix. Adapting between real images across different cities and conditions is crucial for practical applications. In a challenge case, Cityscapes is adapted to Oxford RobotCar with different weather conditions, showing improved segmentation details compared to the baseline method. The proposed method for domain adaptation in structured output improves IoU by 1.8% using ResNet-101 base network. Results show better segmentation details compared to the baseline method, with more comparisons provided in the appendix. The proposed method combines global and patch-level alignments to improve semantic segmentation performance. It involves constructing a clustered space of source patches and using adversarial learning to align target patch distributions with source ones. Extensive experiments validate the effectiveness of the approach under various challenges, showing superior performance compared to existing algorithms. The proposed method combines global and patch-level alignments to enhance semantic segmentation performance through extensive experiments validating its effectiveness under various challenges. The model is trained in an end-to-end manner by sampling one image from each domain in a training iteration and following a specific optimization strategy. Image and patch sizes during training and testing are shown in TAB3, maintaining the aspect ratio without cropping. The model is trained using an optimization strategy described in Section 3.4. Image and patch sizes are maintained without cropping. An entropy loss is used as regularization to push the target feature representation to one of the source clusters, achieving an IoU of 41.9%. The model achieves an IoU of 41.9% by adding entropy regularization to push target patches closer to the source distribution in a clustered space guided by label histogram. This is lower than the proposed patch-level adversarial alignment at 43.2%. The model learns discriminative representations for target patches by aligning them with the source distribution in a clustered space guided by label histogram. Example patches from source and target domains show high similarity in t-SNE visualization, demonstrating effective patch-level alignment. Results for adapting Cityscapes to Oxford RobotCar are presented in TAB4, comparing the proposed method with models without adaptation. In TAB4, the effectiveness of the proposed patch-level alignment is demonstrated by adapting Cityscapes to Oxford RobotCar in rainy conditions. A comparison is made with models without adaptation and the BID31 output space adaptation approach. Visual comparisons for different scenarios are provided in figures, showing that the proposed method often produces better segmentation outputs with more details and less noise. The proposed method demonstrates better segmentation outputs with more details and less noise compared to models without adaptation and the BID31 output space adaptation approach. The results are shown in Figure 9 to Figure 11, showcasing the effectiveness of the approach in adapting segmentation for different scenarios. Results of adapted segmentation for GTA5-to-Cityscapes and SYNTHIA-to-Cityscapes settings are shown in Figures 9 to 11. The proposed method outperforms models without adaptation and the BID31 output space adaptation approach, providing better segmentation outputs with more details and less noise."
}