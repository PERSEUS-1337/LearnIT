{
    "title": "r1eiqi09K7",
    "content": "Several first order stochastic optimization methods commonly used in the Euclidean domain have been adapted to certain Riemannian settings. However, popular optimization tools like Adam, Adagrad, and Amsgrad have not been generalized to Riemannian manifolds. The difficulty of generalizing adaptive schemes to Riemannian settings is discussed, along with algorithms and convergence proofs for geodesically convex objectives in the case of a product of Riemannian manifolds. Adaptivity is implemented across manifolds in the cartesian product. Our generalization extends adaptive schemes to the agnostic Riemannian setting, providing algorithms and convergence proofs for geodesically convex objectives on a product of Riemannian manifolds. Experimentally, Riemannian adaptive methods show faster convergence and lower train loss values compared to baselines when embedding the WordNet taxonomy in the Poincare ball. Developing powerful stochastic gradient-based optimization algorithms is crucial for various application domains, especially when dealing with a large number of parameters. Riemannian adaptive methods demonstrate faster convergence and lower train loss values compared to baselines when embedding the WordNet taxonomy in the Poincare ball. First order methods like ADAGRAD, ADADELTA, ADAM, and AMSGRAD are commonly used for optimizing parameters in engineering and computational sciences, particularly for deep neural networks and large vocabularies. These algorithms are designed for parameters in a Euclidean space, but recent work focuses on optimizing parameters in different geometries. Recent work has focused on optimizing parameters in a Riemannian manifold, allowing for non-Euclidean geometries. This family of algorithms has various applications such as solving Lyapunov equations, matrix factorization, geometric programming, dictionary learning, and hyperbolic taxonomy embedding. Some first-order stochastic methods have been adapted to this setting. The adaptivity of successful algorithms in non-Euclidean geometries, such as solving Lyapunov equations, matrix factorization, and dictionary learning, remains to find their Riemannian counterparts. Seminal methods like Riemannian stochastic gradient descent have been generalized, but further research is needed for convergence analysis in the geodesically convex case. In this work, the authors discuss the challenges of generalizing adaptive algorithms to Riemannian manifolds and propose new algorithms with convergence analysis. In this work, the authors propose generalizations of adaptive algorithms for Riemannian manifolds and provide convergence analysis, focusing on a product of manifolds for each \"coordinate\" of the adaptive scheme. They empirically support their claims with hyperbolic taxonomy embedding tasks. Their initial motivation was the learning of symbolic embeddings in non-Euclidean spaces. The authors developed Riemannian versions of ADAGRAD and ADAM for learning symbolic embeddings in non-Euclidean spaces, benefiting algorithms like GloVe. The absence of Riemannian adaptive algorithms hinders competitive optimization-based Riemannian embedding methods. The absence of Riemannian adaptive algorithms is a significant obstacle for competitive optimization-based Riemannian embedding methods, especially in hyperbolic spaces. Basic notions of differential geometry like manifold, tangent space, and Riemannian metric are essential for understanding these methods. A manifold M of dimension n is a space that can locally be approximated by a Euclidean space R n. The tangent space T x M at each point x \u2208 M is an n-dimensional vector space, serving as a first-order local approximation of M. A Riemannian metric \u03c1 is also defined in this context. A Riemannian manifold is a simple n-dimensional manifold with zero curvature. It consists of a collection of inner-products that define the geometry locally. A Riemannian metric induces a global distance function on the manifold, allowing for the calculation of geodesics. In a Riemannian manifold, a choice of metric induces a global distance function, allowing for the calculation of geodesics and defining Riemannian SGD for smooth functions. In a Riemannian manifold, Riemannian SGD is defined for smooth functions using a specific update formula involving the Riemannian gradient and a step-size. The exponential map allows for updates along the shortest path in the relevant direction while staying within the manifold. In Riemannian manifold, Riemannian SGD uses the exponential map for updates along the shortest path in the relevant direction while staying within the manifold. When exp x (v) is unknown, it can be approximated by a retraction map R x (v) = x + v. ADAGRAD rescales updates coordinate-wise based on past gradients, beneficial for sparse gradients or deep networks. ADAGRAD, introduced by BID5, rescales updates coordinate-wise based on past gradients, beneficial for sparse gradients or deep networks. ADAM, proposed by BID9, includes momentum and adaptivity terms in its update rule. When \u03b2 1 = 0, RMSPROP is similar to ADAGRAD but uses an exponential moving average instead of a sum for adaptivity. ADAM introduced a momentum term for \u03b2 1 = 0, leading to significant improvements. BID18 found a mistake in ADAM's convergence proof and proposed a fix. AMSGRAD was proposed as a modification to the ADAM algorithm to address the issue of accumulated squared gradients. BID18 identified a mistake in ADAM's convergence proof and suggested using AMSGRAD or ADAMNC for improvement. Coordinate-wise updates require a choice of coordinate system, but on a Riemannian manifold, this interpretation may differ. On a Riemannian manifold, coordinate-wise updates require a choice of coordinate system. The formalism allows working with local coordinate systems called charts, with different charts defined around each point. The formalism on a Riemannian manifold involves working with local coordinate systems called charts, allowing for intrinsic definitions of quantities that do not depend on the chosen chart. The RSGD update equation is intrinsically defined as it only involves exp and grad. The Riemannian gradient of a smooth function on a manifold can be intrinsically defined, but the Hessian is only defined at critical points. The RSGD update equation is intrinsic as it involves exp and grad. It is uncertain if Eqs. (3,4,5) can be expressed in a coordinate-free manner. A tempting solution involves fixing a canonical coordinate system in the tangent space and parallel-transporting it along the optimization trajectory on a Riemannian manifold. In Euclidean space, parallel transport between two points does not depend on the path, but in a Riemannian manifold, it depends on the path and curvature. In a Riemannian manifold, parallel transport depends on the chosen path and curvature, introducing a rotational component that disrupts the sparsity of gradients and adaptivity. The interpretation of adaptivity as optimizing different features at different speeds is lost, as the coordinate system for gradients depends on the optimization path. The interpretation of adaptivity in a Riemannian manifold is affected by the rotational component introduced by parallel transport, disrupting gradient sparsity. The coordinate system for gradients depends on the optimization path, and additional structure is assumed for the manifold. In a Riemannian manifold, adaptivity is challenging due to the absence of intrinsic coordinates. To address this, each component x i in the manifold can be seen as a \"coordinate\", simplifying the adaptation process. This approach allows for the design of meaningful adaptive schemes, overcoming difficulties posed by the lack of intrinsic coordinates. In a Riemannian manifold, adaptivity is challenging due to the absence of intrinsic coordinates. Each component x i in the manifold can be seen as a \"coordinate\", simplifying the adaptation process. This approach allows for the design of meaningful adaptive schemes, overcoming difficulties posed by the lack of intrinsic coordinates. In section 2, ADAGRAD, ADAM, and AMSGRAD were briefly presented. ADAM combines ADAGRAD with momentum and replaces the sum of past squared-gradients with an exponential moving average. ADAM is a combination of ADAGRAD with momentum and an exponential moving average. AMSGRAD is a modification of ADAM for improved convergence. ADAMNC is ADAM with a non-constant schedule for momentum parameters. The schedule proposed by BID18 for ADAMNC allows recovery of the sum of squared-gradients from ADAGRAD. ADAMNC has a non-constant schedule for momentum parameters, with a schedule proposed by BID18 allowing recovery of the sum of squared-gradients from ADAGRAD. Assumptions and notations involve geodesically complete Riemannian manifolds with sectional curvature lower bounded by \u03ba i \u2264 0. The product manifold of these manifolds is denoted as (M, \u03c1), with a set of feasible parameters defined as X := X 1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 X n. The product manifold (M, \u03c1) is formed by geodesically convex sets X i \u2282 M i. The projection operator \u03a0 Xi minimizes distance, with parallel transport, exponential, and log maps denoted by P i, exp i, and log i. A family of geodesically convex functions (f t) is considered, with bounded diameters and gradients for each X i \u2282 M i. In the product manifold (M, \u03c1), geodesically convex sets X i \u2282 M i are formed. A family of geodesically convex functions (f t) is considered, with bounded diameters and gradients for each X i \u2282 M i. Riemannian AMSGRAD algorithm is presented in FIG1, with comparisons to standard AMSGRAD algorithm. RADAM and ADAM are obtained by removing max operations. Riemannian AMSGRAD algorithm is compared to standard AMSGRAD in FIG1. RADAM and ADAM are derived by removing max operations. Convergence guarantees for RAMSGRAD and AMSGRAD are discussed, with similarities noted when (M i , \u03c1 i ) = R for all i. The convergence guarantee of AMSGRAD is shown in appendix C. When (M i , \u03c1 i ) = R for all i, convergence guarantees between RAMSGRAD and AMSGRAD coincide. The regret bound worsens at a speed of approximately 1 + D \u221e |\u03ba|/6 when the curvature is small but non-zero. Similar remarks apply to RADAMNC, with convergence guarantee shown in Theorem 2. Setting \u03b2 1 := 0 in Theorem 2 yields a convergence proof for RADAGRAD. The convergence guarantee of AMSGRAD is shown in appendix C. When (M i , \u03c1 i ) = R for all i, convergence guarantees between RAMSGRAD and AMSGRAD coincide. The regret bound worsens at a speed of approximately 1 + D \u221e |\u03ba|/6 when the curvature is small but non-zero. Similar remarks apply to RADAMNC, with convergence guarantee shown in Theorem 2. Setting \u03b2 1 := 0 in Theorem 2 yields a convergence proof for RADAGRAD. Theorem 1 (Convergence of RAMSGRAD) and Theorem 2 (Convergence of RADAMNC) provide convergence proofs for the respective algorithms. The role of convexity in the proofs is crucial. Convexity in Theorem 5 is replaced by geodesic convexity in Theorem 1. Functions f : R n \u2192 R and g : M \u2192 R are convex and geodesically convex if certain conditions hold. Regret bounds for convex objectives are typically obtained by bounding T t=1. The role of convexity in the proofs is crucial. Regret bounds for convex objectives are usually obtained by bounding T t=1 f t (x t ) \u2212 f t (x * ). In the Riemannian case, this term becomes \u03c1 xt (g t , \u2212 log xt (x * )). Using a cosine law, one can obtain a bound on g t , x t \u2212 x * in the particular case of an SGD update. The role of convexity is crucial in obtaining regret bounds for convex objectives. In the Riemannian case, the term becomes \u03c1 xt (g t , \u2212 log xt (x * )) and a bound on g t , x t \u2212 x * can be obtained using a cosine law. The update involves bounding two terms: a telescopic summation for the first term and a well-chosen decreasing schedule for the second term T t=1 \u03b1 t g t 2. In Riemannian manifolds, this step is generalized using lemma 6 in Alexandrov spaces, which includes geodesically convex subsets with lower bounded sectional curvature. The curvature dependent quantity \u03b6 of Eq. (10) is introduced in this context. The generalized step in Riemannian manifolds involves lemma 6 in Alexandrov spaces, allowing for bounding \u03c1 DISPLAYFORM11 using the curvature dependent quantity \u03b6 from Eq. (10). Improved bounds are seen for sparse gradients, where only a few words are updated at a time on the manifold M i. The convergence theorems do not require specifying \u03d5 i. The proposed algorithms RADAM, RAMSGRAD, and RADAGRAD are empirically assessed compared to the non-adaptive RSGD method. The regret bounds could potentially be improved by exploiting momentum/acceleration in the proofs for a particular choice of \u03d5 i. The proposed algorithms RADAM, RAMSGRAD, and RADAGRAD are empirically assessed compared to the non-adaptive RSGD method. The WordNet noun hierarchy is embedded in the n-dimensional Poincar\u00e9 model of hyperbolic geometry, known for better embedding tree-like graphs than Euclidean space. Each word is embedded in the same space of constant curvature -1. The choice of the Poincar\u00e9 model is justified by access to closed form expressions for all. The Poincar\u00e9 model is used for embedding the WordNet noun hierarchy in hyperbolic geometry, known for better embedding tree-like graphs. Each word is embedded in the same space of constant curvature -1, allowing for closed form expressions in algorithms. The model utilizes rescaled Euclidean gradients, distance functions, geodesics, exponential and logarithmic maps, and parallel transport along geodesics. The transitive closure of the WordNet taxonomy graph includes 82,115 nouns and 743,241 hypernymy Is-A relations. Words are embedded in a space of constant curvature -1, minimizing distances between connected words while maximizing distances otherwise. The model utilizes various mathematical functions and parallel transport along geodesics. The model minimizes distances between connected words in a space of constant curvature -1. It uses a loss function similar to log-likelihood and sampling of negative word pairs. The direction of edges in the graph is not considered in the loss function. The mean average precision (MAP) is reported for each directed edge. The study focuses on evaluating the loss value and mean average precision (MAP) for directed edges in a graph. Different settings like reconstruction and link prediction are considered, with a validation set used for link prediction. Training details include a \"burn-in phase\" for 20 epochs with specific learning rate and optimization method. The study specifically looks at 5-dimensional hyperbolic spaces. The study focused on evaluating loss value and mean average precision for directed edges in a graph in 5-dimensional hyperbolic spaces. Training details included a \"burn-in phase\" for 20 epochs with a fixed learning rate and optimization method. Negative word sampling strategy improved metrics, and RADAM was found to yield slightly better results than RAMS-GRAD. The study found that RADAM yielded slightly better results than RAMS-GRAD in optimizing metrics. Additionally, replacing the true exponential map with its first-order approximation led to convergence to lower loss values in both RSGD and adaptive methods. This could be due to retraction methods requiring fewer steps and smaller gradients to escape sub-optimal points on the ball border of Dn. Results show that \"retraction\"-based methods are not directly comparable to fully Riemannian analogues, as they require fewer steps and smaller gradients to escape sub-optimal points on the ball border of Dn. The study ran methods with different learning rates and displayed results for both \"exponential\" and \"retraction\" based methods in FIG2. Results from the study compared \"exponential\" and \"retraction\" based methods using different learning rates. The best settings for RSGD, RADAM, and RAMSGRAD were shown, with RADAM consistently achieving the lowest training loss. RADAGRAD was not reported due to inferior performance. In the study comparing different optimization methods, RADAM consistently achieved the lowest training loss with \u03b2 1 = 0.9 and \u03b2 2 = 0.999. It also outperformed other methods on the MAP metric for both reconstruction and link prediction in the full Riemannian setting. RAMSGRAD showed faster convergence for the link prediction task, indicating better generalization capability. RAMSGRAD is faster to converge in terms of MAP for link prediction, suggesting better generalization capability. Various first-order Riemannian methods have emerged after Riemannian SGD, including Riemannian SVRG and Riemannian accelerated gradient descent. Stochastic gradient Langevin dynamics was also generalized to improve optimization on the probability simplex. Various first-order Riemannian methods have emerged, including Riemannian accelerated gradient descent and stochastic gradient Langevin dynamics for optimization on the probability simplex. Riemannian counterparts of SGD with momentum and RMSprop have been proposed, but without convergence guarantees. The curr_chunk discusses the limitations of existing algorithms for optimization on manifolds, highlighting the lack of convergence guarantees and adaptivity across manifolds. It contrasts with previous versions of Riemannian ADAM and emphasizes the importance of preserving adaptivity in optimization algorithms. The curr_chunk proposes a method to generalize adaptive optimization tools for Cartesian products of Riemannian manifolds, addressing the lack of adaptivity and convergence analysis in existing algorithms. It aims to achieve similar convergence rates as Euclidean models and demonstrates superior performance in experiments. The curr_chunk introduces a method to extend adaptive optimization tools to Cartesian products of Riemannian manifolds, aiming for similar convergence rates as Euclidean models. Experimental results show superiority over non-adaptive methods like RSGD in hyperbolic word taxonomy embedding tasks. The curr_chunk discusses the application of inequalities in a mathematical proof, specifically focusing on lemma 6 and the use of Cauchy-Schwarz' and Young's inequalities. The text also mentions geodesic convexity and lemma 3 in the proof. The lemma discussed in the curr_chunk focuses on the cosine inequality in Alexandrov spaces, which is used to prove convergence of optimization algorithms for geodesically convex functions. It also mentions an analogue of Cauchy-Schwarz inequality. The lemma in the curr_chunk discusses the application of Cauchy-Schwarz inequality in Alexandrov spaces, which is used in convergence proofs for optimization algorithms. It also introduces an analogue lemma for non-negative real numbers."
}