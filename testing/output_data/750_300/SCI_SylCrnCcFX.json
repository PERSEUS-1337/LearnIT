{
    "title": "SylCrnCcFX",
    "content": "Deep networks aim to understand complex mappings through locally linear behavior. The challenge lies in stabilizing the derivatives of these mappings, especially for networks with piecewise linear activation functions. A new learning problem is proposed to promote stable derivatives over larger regions, focusing on provably stable linear approximations around points of interest. Our algorithm focuses on stabilizing derivatives in networks with piecewise linear activation functions. It includes an inference step to identify stable regions and an optimization step to expand them. A novel relaxation technique is proposed to scale the algorithm to realistic models, demonstrated on residual and recurrent networks with image and sequence datasets. The derivatives at points of interest play a crucial role in understanding complex mappings and sensitivity analysis. The derivatives at points of interest are crucial for understanding complex mappings and sensitivity analysis in networks with piecewise linear activation functions. They are used for local linearization to explain model predictions and guide learning through regularization of functional classes. The focus is on derivatives with respect to input coordinates rather than parameters. The derivatives discussed in this paper are crucial for understanding complex mappings and sensitivity analysis in networks with piecewise linear activation functions. State-of-the-art deep learning models are typically over-parametrized, leading to unstable functions and derivatives. The instability is reflected in both the function values and the derivatives due to unstable derivatives. State-of-the-art deep learning models are often over-parametrized, resulting in unstable functions and derivatives. This instability affects both function values and derivatives, leading to unreliable first-order approximations for explanations. Gradient stability is different from adversarial examples, with stable gradients remaining approximately invariant within a local region. Adversarial examples are small input perturbations that change predicted outputs. Adversarial examples are small perturbations of the input that change the predicted output. Robust estimation techniques focus on stable function values rather than stable gradients but can indirectly impact gradient stability. Ensuring gradient stability would involve finding maximally distorted derivatives and require access to approximate Hessians of deep learning models. In this paper, the focus is on deep networks with piecewise linear activations to ensure gradient stability. The special structure of these networks allows for inferring lower bounds on the maximum radius of p-norm balls around a point where derivatives are stable. The investigation specifically looks at the case of p = 2 due to its analytical solution. The study focuses on deep networks with piecewise linear activations to ensure gradient stability. It investigates lower bounds on the maximum radius of p-norm balls around a point where derivatives are stable, particularly for p = 2. The objective is to formulate a regularization problem to maximize the lower bound, which poses computational challenges in evaluating neuron gradients. The study proposes a novel perturbation algorithm for piecewise linear networks to evaluate neuron gradients efficiently without back-propagation, even when GPU memory constraints arise. Empirical examination is conducted with fully-connected and residual networks. The study introduces a novel perturbation algorithm for piecewise linear networks to efficiently evaluate neuron gradients without back-propagation. It examines inference and learning algorithms with various network architectures on image and time-series datasets. Key contributions include stable inference algorithms and a learning criterion for expanding regions of stable derivatives. The main contributions of this work include inference algorithms for identifying stable regions in neural networks with piecewise linear activation functions, a novel learning criterion for expanding stable derivatives, perturbation algorithms for high-dimensional data, and empirical evaluation with various network types. The focus is on neural networks with piecewise linear activation functions like ReLU BID15 and its variants. Neural networks with piecewise linear activation functions, such as ReLU BID15 and its variants, are the focus of this paper. The proposed approach involves a mixed integer linear representation of piecewise linear networks, including FC, CNN, RNN, and ResNet. The main contributions include inference algorithms for identifying stable regions and a novel learning criterion for expanding stable derivatives. The proposed approach involves a mixed integer linear representation of piecewise linear networks, including FC, CNN, RNN, and ResNet. Activation pattern BID20 encodes the active linear piece of the activation function for each neuron, leading to stable derivatives in the feasible set of the input space. The feasible set in the input space is a natural region where derivatives are stable, forming linear regions with the same end-to-end coefficients. Activation patterns define these regions, studied in various contexts like visualizing neurons and reachability of specific output values. The activation patterns in the input space form linear regions with stable derivatives, studied in various contexts like visualizing neurons and reachability of specific output values. This includes analyzing the number of linear regions in piecewise linear networks and exploring adversarial attacks or defense strategies. Note the distinction between locally linear regions of functional mapping and decision regions defined by classes. In contrast to quantifying the number of linear regions as a measure of complexity, this work focuses on expanding local linear regions through learning. The notion of stability considered here differs from adversarial examples, with different methods used to find exact adversarial examples. The methods for finding exact adversarial examples differ from stability considerations. Adversarial example computation is NP-complete and not scalable, while layer-wise relaxations provide bounds but are still intractable on ImageNet scale images. In contrast, the proposed inference algorithm certifies a 2 margin around a point by forwarding O(D) samples in parallel. The proposed inference algorithm certifies a 2 margin around a point by forwarding O(D) samples in parallel, even on high-dimensional images like ResNet on 299 \u00d7 299 \u00d7 3 dimensional images. The learning algorithm is based on the inference problem with 2 margins, resembling SVM objective but with a different purpose. The proposed learning algorithm for high-dimensional images like ResNet focuses on maximizing the 2 margin of linear regions around each data point, resembling SVM objective but with a different purpose. The objective is unsupervised and akin to transductive/semi-supervised SVM, extending the idea of margin to nonlinear classifiers in terms of decision boundaries. The curr_chunk discusses the development of a smooth relaxation of the margin and novel perturbation algorithms for gradient stability in deep learning models. This approach aims to improve interpretability and transparency of complex models by addressing the gradient as a key component for explanation methods. The curr_chunk focuses on the implications of gradient stability for interpretability and transparency in complex models. It addresses the instability of gradient-based explanations and the fundamental problem of establishing stability. The curr_chunk introduces approaches for establishing robust derivatives in neural networks with ReLU activations, focusing on inference and learning algorithms for networks with hidden layers and neurons. The proofs are provided in Appendix A. The curr_chunk introduces notation and algorithms for neural networks with hidden layers and neurons, using transformation matrices and biases. The curr_chunk discusses the computation process in neural networks using transformation matrices and biases, focusing on the piecewise linear property of the network's output. The curr_chunk explains the linear transformation of the last hidden layer in neural networks and the use of activation patterns. It focuses on the piecewise linear property of neural networks and the generic loss function used for training data. The curr_chunk discusses the activation pattern used in neural networks, defining it as a set of indicators for neurons that specify functional constraints. It also mentions the sub-gradient found by back-propagation and the feasible set of the activation pattern. The curr_chunk discusses the linear regions of activation patterns in neural networks, characterized by a set of linear constraints. The feasible set of the activation pattern is defined as a convex polyhedron, with a margin defined as the p margin of x subject to its activation pattern. The curr_chunk discusses the convexity of the input space and the definition of the p margin of x based on an activation pattern. It explores the feasibility of directional perturbations in the context of linear regions of activation patterns in neural networks. The feasibility of directional perturbations in neural networks can be determined by checking if a point x + \u00af\u2206x is within the feasible set S(x) based on the activation pattern O. This method can also be applied to 1-ball feasibility problems. The feasibility of directional perturbations in neural networks can be determined by checking if a point x + \u00af\u2206x satisfies the activation pattern O in set S(x). Proposition 5 extends this method to 1-ball feasibility, with the number of extreme points being linear to dimension D. However, for \u221e-balls, the number of extreme points is exponential in D, making it intractable. The feasibility of directional perturbations in neural networks can be determined by checking if a point x + \u00af\u2206x satisfies the activation pattern in set S(x). Proposition 5 extends this method to 1-ball feasibility, with the number of extreme points being linear to dimension D. However, for \u221e-balls, the number of extreme points is exponential in D, making it intractable. Certificates for directional perturbations and 1-balls can be found efficiently using binary searches, with details provided in Appendix B. The feasibility of 1-balls is tractable due to the convexity of S(x), while further exploiting the polyhedron structure allows for analytical certification of 2-balls. The minimum 2 distance between a point x and the union of hyperplanes can be certified analytically by exploiting the polyhedron structure of S(x). The 2 distance between x and a hyperplane induced by a neuron can be efficiently computed using forward passes. Visualization of certificates on 2 margins can be seen in FIG3, with linear region sizes related to their overall number in a bounded input space. The number of linear regions in f \u03b8 can be efficiently computed using forward passes in parallel. Certifying the number of complete linear regions (#CLR) among data points D x is proposed as an efficient way to capture the structure of the data manifold. This method is effective given a mild condition, unlike counting the number of linear regions on the whole space, which is intractable due to the combinatorial nature of activation patterns. Certifying the number of complete linear regions (#CLR) of f \u03b8 among data points D x is proposed as an efficient way to capture the data manifold structure. The number of complete linear regions is upper-bounded by the different activation patterns and lower-bounded by the different Jacobians. This section focuses on maximizing the 2 margin\u02c6 x,2. In this section, methods are discussed for maximizing the 2 margin\u02c6 x,2 in the context of certifying the number of complete linear regions of f \u03b8 among data points D x. The objective is formulated to maximize the margin, but the rigid loss surface may hinder optimization. A hinge-based relaxation approach is proposed to alleviate this issue. To address the rigid loss surface hindering optimization, a hinge-based relaxation approach is proposed to the distance function. This relaxation solves a smoother problem by incorporating a soft regularization method. FORMULA12 remains optimal for Eq. (4), and an upper bound can be obtained by relaxing constraints. An upper bound of Eq. FORMULA12 can be obtained by relaxing constraints through a smoother problem with soft regularization. This approach aims to maximize the margin in a linear model scenario. Neurons use TSVM loss with unannotated data to maximize margin in a linear model scenario. A 4-layer fully connected network is trained with different loss functions for binary classification. Piecewise linear regions are visualized to show the effect of the methods. The 4-layer fully connected network is trained with binary cross-entropy loss, distance regularization, and relaxed regularization. The distance regularization enlarges linear regions around each training point, while the relaxed regularization generalizes the property to the whole space with a smoother prediction boundary. The relaxed regularization in the 4-layer fully connected network generalizes the property to the whole space with a smoother prediction boundary. It includes a special central region where gradients are 0 to allow smooth direction changes, addressing the limitation of focusing only on the \"closest\" neuron. The generalized loss incorporates a set of neurons with high losses to the given point, denoted as \u00ce(x, \u03b3), to scale the effect to large networks. The generalized loss for learning RObust Local Linearity (ROLL) is defined as a relaxed loss with a set of neurons incurring high losses. The objective is to scale the effect to large networks, with a special case when \u03b3 = 100 simplifying the structure for stability in training and parallel computation. The nonlinear sorting step is simplified when \u03b3 = 100, leading to stable training and parallel computation. A parallel algorithm is developed without back-propagation by leveraging the functional structure of f \u03b8. The parallel algorithm developed leverages the functional structure of f \u03b8, avoiding back-propagation by constructing a linear network g \u03b8 with fixed linear activation functions to mimic f \u03b8 behavior in S(x). This allows for efficient computation of derivatives by forwarding two samples. The proposed approach constructs a linear network g \u03b8 with fixed linear activation functions to mimic the behavior of f \u03b8 in S(x). Derivatives of neurons can be computed by forwarding two samples, allowing for efficient computation. The complexity analysis assumes no overhead for parallel computation and a unit operation for batch matrix multiplication. The perturbation algorithm takes 2M operations to compute gradients for a batch of inputs, while back-propagation takes DISPLAYFORM0. The perturbation algorithm takes 2M operations to compute gradients for a batch of inputs, while back-propagation takes DISPLAYFORM0. Despite the parallelizable computation of \u2207 x z i j, it is challenging to compute the loss for large networks in a high dimension setting. An unbiased estimator of the ROLL loss in Eq. FORMULA17 is proposed when \u00ce(x, \u03b3) = I. In a high dimension setting, an unbiased estimator of the ROLL loss in Eq. FORMULA17 is proposed when \u00ce(x, \u03b3) = I. The sum of gradient norms can be efficiently computed using a decoupling method, allowing for parallel computation within GPU memory constraints. Sampling D input axes provides an unbiased approximation to Eq. (10) for computing partial derivatives. The proposed algorithms allow for efficient computation of gradient norms in high dimensions using GPU memory. Sampling D input axes provides an unbiased approximation for computing partial derivatives in deep learning models with affine transformations and piecewise linear activation functions. In this section, the authors compare their approach ('ROLL') with a baseline model ('vanilla') in deep learning models with affine transformations and piecewise linear activation functions. They suggest using average-pooling or convolution with large strides instead of maxpooling to avoid extra linear constraints. In this section, the authors compare their approach ('ROLL') with a baseline model ('vanilla') in deep learning models with affine transformations and piecewise linear activation functions. They suggest using average-pooling or convolution with large strides instead of maxpooling to avoid extra linear constraints. The experiments are conducted on a single GPU with 12G memory, evaluating accuracy, number of complete linear regions, and p margins of linear regions on a testing set. Parameter analysis was conducted on the MNIST dataset using a 4-layer FC model with ReLU activations. The testing data was evaluated at different percentiles, and experiments were done with a 55,000/5,000/10,000 split for training/validation/testing. The two models with the largest median were reported, given similar validation accuracy compared to the baseline model. Experiments were conducted on a 4-layer FC model with ReLU activations, and the results are shown in TAB1. The tuned models have specific parameters, and the ROLL loss achieves significantly larger margins compared to the vanilla loss. By sacrificing 1% accuracy, even larger margins can be achieved. The ROLL loss achieves larger margins compared to the vanilla loss, with a 10 times difference for most percentiles and up to 30 times by sacrificing 1% accuracy. The Spearman's rank correlation between testing data is at least 0.98. The lower #CLR in our approach indicates larger linear regions across different testing points. Points within the same linear region in the ROLL model with 98% accuracy have the same label. The ROLL model shows larger linear regions with points sharing the same label. Parameter analysis in Figure 2 reveals accuracy changes with C and \u03bb values. Higher \u03b3 values indicate less sensitivity to hyper-parameters. Running time for mini-batch gradient is measured to validate method efficiency. The accuracy decreases with an increased margin. Higher \u03b3 values reflect less sensitivity to hyper-parameters C and \u03bb. Efficiency of the proposed method is validated by measuring running time for mini-batch gradient descent. Comparison is made between different loss computations, showing comparable accuracy and margins for the approximate ROLL loss. The approximate ROLL loss computed by our perturbation algorithm shows comparable accuracy and margins to the full loss. Our approach is only twice slower than the vanilla loss, with the approximate loss being about 9 times faster. The perturbation algorithm achieves about 12 times empirical speed-up compared to back-propagation. The computational overhead of our method is minimal, achieved through the perturbation algorithm and the approximate loss. Our perturbation algorithm achieves a 12 times empirical speed-up compared to back-propagation. The computational overhead of our method is minimal, achieved through the perturbation algorithm and the approximate loss. Training RNNs for speaker identification on a Japanese Vowel dataset with variable sequence length and multiple channels. Training RNNs for speaker identification on a Japanese Vowel dataset from the UCI machine learning repository with variable sequence length and multiple channels using the state-of-the-art scaled Cayley orthogonal RNN. The results are reported in TAB3 with our approach leading to a model with about 4/20 times inferior ACC. The implementation details for preventing gradient vanishing/exploding with LeakyReLU activation are provided in Appendix H. Results in TAB3 show that our approach leads to a model with larger margins on testing data compared to the vanilla loss, with a Spearman's rank correlation of 0.98 between\u02c6 x,1 and\u02c6 x,2. Sensitivity analysis on derivatives identifies stability bounds at each timestamp and channel. The sensitivity analysis on derivatives identified stability bounds for each timestamp and channel, showing that the ROLL regularization had consistently larger stability bounds compared to the vanilla model. Experiments were conducted on Caltech-256 BID18 dataset using downsized images and an 18-layer ResNet model. Regularization is consistently larger than the vanilla model in experiments on Caltech-256 BID18 dataset. An 18-layer ResNet model is trained on downsized images using parameters pre-trained on ImageNet. The ROLL loss is used with 120 random samples per channel, and evaluation measures are challenging due to high input dimensionality. The implementation details involve splitting data into training, validation, and testing sets. Due to high input dimensionality, evaluating gradients is computationally challenging without GPUs. A sample-based approach is used to assess gradient stability for the ground-truth label in local regions. Comparing different models based on gradient prediction is problematic. The implementation involves splitting data into training, validation, and testing sets. Evaluating gradient stability across linear regions is the goal, using a sample-based approach due to high input dimensionality. Comparing models based on gradient prediction is challenging. The gradient distortion is evaluated in terms of expected and maximum distortion within an intersection. The gradient distortion is evaluated by maximizing the distortion over a norm ball with radius 8/256. Gradient-based optimization is not applicable due to the Hessian being either 0 or ill-defined. The maximum 1 distortion is computed using a genetic algorithm BID33 for black-box optimization due to the Hessian being either 0 or ill-defined. Results are presented in Table 4 with precision at 1 and 5 (P@1 and P@5). The ROLL loss shows more stable gradients compared to the vanilla loss, with slightly better precision. Only 40 and 42 out of 1024 images had their prediction labels changed in the ROLL and vanilla models, respectively. Visual examples in Figure 4 demonstrate the differences in gradient shapes and intensities between the two losses. This paper introduces a new learning problem to create locally transparent neural networks with stable gradients. The ROLL loss produces stable shapes and intensities of gradients, while the vanilla loss does not. More examples with integrated gradient attributions are provided in Appendix K. The paper introduces a new learning problem to construct locally transparent neural networks with stable gradients using a margin principle similar to SVM. The proposed ROLL loss expands regions with stable derivatives and generalizes the stable gradient property across linear regions. The feasible set of activation patterns is equivalent to satisfying linear constraints in the first layer. The paper introduces a new learning problem to construct locally transparent neural networks with stable gradients using a margin principle similar to SVM. The proposed ROLL loss expands regions with stable derivatives and generalizes the stable gradient property across linear regions. The feasible set of activation patterns is equivalent to satisfying linear constraints in the first layer. The proof follows by induction. Propositions 4, 5, and 6 provide conditions for directional feasibility, 1-ball feasibility, and 2-ball certificate in the context of constructing locally transparent neural networks with stable gradients. Proposition 6 discusses the 2-ball Certificate, stating that the minimum distance between a point x and a set S(x) is the maximum distance that ensures x is within S(x). This is optimal for a specific equation. The proof involves constructing a neural network feasible in Eq. (5) with the same loss as the optimal model in Eq. (4), utilizing parallel computation of gradients. The network g \u03b8 is built with the same weights and biases as f \u03b8 but with a well-crafted structure. The network g \u03b8 is constructed with the same weights and biases as f \u03b8 but with a linear activation function. Each layer in g \u03b8 is represented as a function of x, with fixed activation function \u00f4. This allows for the collection of partial derivatives with respect to an input. The activation function \u00f4 is fixed in the network g \u03b8, allowing for the computation of partial derivatives with respect to an input axis k by feeding zero and unit vectors. This procedure enables the computation of the derivative of all neurons to an input dimension with 2 forward passes. The proposed approach allows for the computation of the derivative of all neurons to an input dimension with 2 forward passes, without incurring overhead from parallel computation. The complexity analysis assumes a unit operation for batch matrix multiplication, with M operations for a typical forward pass up to the last hidden layer. The perturbation algorithm first takes a forward pass to obtain activation patterns for a batch of inputs. The perturbation algorithm allows for computing gradients of all neurons with 2 forward passes, totaling 2M operations. Back-propagation, on the other hand, requires sequential computation for each neuron's gradient. The perturbation algorithm computes gradients of all neurons with 2 forward passes, totaling 2M operations. Back-propagation requires sequential computation for each neuron's gradient, taking M i=1 2iN i operations in total. Dynamic programming can be used with the chain-rule of Jacobian to compute all gradients efficiently. The dynamic programming approach efficiently computes gradients using the chain-rule of Jacobian for fully connected networks, but is inefficient for convolutional layers due to the complexity of representing the convolutional operation. The dynamic programming approach efficiently computes gradients for fully connected networks, but is inefficient for convolutional layers due to the complexity of representing the convolutional operation. An introductory guide to derivations for maxout/max-pooling nonlinearity is provided, highlighting the feasibility of deriving inference and learning methods for a piecewise linear network with max-pooling nonlinearity. It is not recommended to use max-pooling neurons due to the introduction of new linear constraints; instead, convolution with large strides or average-pooling is suggested. The feasibility of deriving inference and learning methods for a piecewise linear network with max-pooling nonlinearity is highlighted. It is advised against using max-pooling neurons due to the introduction of new linear constraints. Instead, convolution with large strides or average-pooling is recommended. The target network is assumed to have a single nonlinearity mapping N neurons to 1 output. Once an activation pattern is fixed, the network reverts to a linear model as the nonlinearity in max-pooling disappears. The activation pattern in a piecewise linear network with max-pooling nonlinearity induces a feasible set in the input space where derivatives are stable. However, there may be cases where two activation patterns yield the same linear coefficients. The feasible set of a feasible activation pattern can be derived to check its correctness. In a piecewise linear network with max-pooling nonlinearity, activation patterns can yield the same linear coefficients. The feasible set of a feasible activation pattern can be derived to ensure correctness. The FC model has 4 fully-connected hidden layers with 100 neurons each, input dimension D is 2, and output dimension L is 1. Each max-pooling neuron induces N-1 linear constraints. The FC model consists of 4 fully-connected hidden layers with 100 neurons each, input dimension D is 2, and output dimension L is 1. The loss function is sigmoid cross entropy, trained for 5000 epochs with Adam optimizer. The regularization parameter \u03bb is tuned to 1 for both distance and relaxed regularization problems. Data is normalized with \u00b5 = 0.1307 and \u03c3 = 0.3081. The FC model consists of 4 fully-connected hidden layers with 300 neurons each. The regularization parameter \u03bb is tuned to 1 for both distance and relaxed regularization problems. Data is normalized with \u00b5 = 0.1307 and \u03c3 = 0.3081. The margin\u02c6 x,p in the normalized data is computed, and the scaled margin \u03c3\u02c6 x,p is reported in the table. The exact ROLL loss is computed during training without using approximate learning. The FC model consists of 4 fully-connected hidden layers with 300 neurons each. The activation function is ReLU. The loss function is cross-entropy with soft-max. Stochastic gradient descent with Nesterov momentum is used for training. The learning rate is 0.01, momentum is 0.5, and batch size is 64. Tuning involves grid search on \u03bb, C, \u03b3. The model is selected based on the best validation loss. Stochastic gradient descent with Nesterov momentum is used, with a learning rate of 0.01, momentum of 0.5, and batch size of 64. Tuning includes grid search on \u03bb, C, \u03b3 values. Data normalization is not performed, and the representation is learned with a single layer scoRNN. The representation is learned with a single layer scoRNN, where the state embedding from the last timestamp for each sequence is treated as the representation along with a fully-connected layer to produce a prediction. LeakyReLU is used as the activation function in scoRNN with hidden neurons set to 512. The loss function is cross-entropy with soft-max, and AMSGrad optimizer is used with a learning rate of 0.001 and batch size of 32 sequences. Tuning involves grid search on \u03bb values. The dimension of hidden neurons in scoRNN is set to 512. The loss function is cross-entropy with soft-max, and AMSGrad optimizer is used with a learning rate of 0.001 and batch size of 32 sequences. Tuning involves grid search on \u03bb values ranging from 2^-6 to 2^3, and C values ranging from 2^-5 to 2^7, with \u03b3 set to 100. Models with 1% less testing accuracy compared to the baseline model are reported. Training is done on normalized images with a bijective mapping established between normalized distance and distance in the original space. The model architecture of the pre-trained ResNet-18 is revised by replacing max-pooling with average-pooling after the first convolutional layer to reduce linear constraints. The bijection is applied to compute distances in the original space from normalized images. The model architecture of the pre-trained ResNet-18 is revised by replacing max-pooling with average-pooling after the first convolutional layer to reduce linear constraints. The model is trained with stochastic gradient descent with Nesterov momentum for 20 epochs, starting with an initial learning rate of 0.005. The model is trained with stochastic gradient descent using Nesterov momentum for 20 epochs. The initial learning rate is 0.005, adjusted to 0.0005 after 10 epochs. Batch size is 32. Tuning involves fixing C = 8, using 18 samples, and tuning \u03bb until significantly inferior validation accuracy is observed. To improve model accuracy, the learning process involves fixing C = 8, using 18 samples for approximate learning, and tuning \u03bb values. After identifying the highest plausible values for \u03bb and C, a genetic algorithm with 4800 populations and 30 epochs is implemented for further training. The genetic algorithm (GA) with 4800 populations and 30 epochs involves sampling 4800 samples in the domain for approximation. Samples are evaluated based on distance from the target x, with top 25% kept in the population. The remaining 75% are replaced with a random linear combination of pairs from the population. Updated samples undergo projection to the domain for further refinement. The genetic algorithm (GA) involves sampling 4800 samples in the domain for approximation. The top 25% samples are kept in the population, while the remaining 75% are replaced with a random linear combination of pairs from the population. Updated samples undergo projection to the domain for further refinement. The algorithm does not implement mutation due to computational reasons. The crossover operator is analogous to a gradient step where the direction is determined by other samples and the step size is determined randomly. The genetic algorithm (GA) involves sampling 4800 samples in the domain for approximation. The crossover operator is analogous to a gradient step where the direction is determined by other samples and the step size is determined randomly. Visualizations include original image, original gradient, adversarial gradient, image of adv. gradient, original int. gradient, and adversarial int. gradient. No optimization was performed to find the image. The integrated gradient attribution was visualized on the original image and the 'image of adv. gradient'. Derivatives were aggregated in each channel, normalized, and clipped to values between 0 and 1. No optimization was done to find the image yielding the maximum distorted integrated gradient. The integrated gradient attribution process involves aggregating derivatives in each channel, taking the absolute value, normalizing by the 99th percentile, and clipping values above 1. The resulting derivatives are then visualized as gray-scaled images, highlighting differences in settings. Examples from the Caltech-256 dataset are used to demonstrate different percentiles (P25, P50, P75). The integrated gradient is visualized to show differences in settings using examples from the Caltech-256 dataset, highlighting percentiles P25, P50, and P75 of maximum gradient distortions on the ROLL model. The exact values differ slightly from Table 4 due to interpolation methods. The visualization in Figure 5 and 6 displays examples from the Caltech-256 dataset showing the maximum 1 gradient distortions at percentiles P25, P50, and P75 on the ROLL model. The exact values differ slightly from Table 4 due to interpolation methods. The visualization in Figure 5 and 6 shows examples from the Caltech-256 dataset with maximum 1 gradient distortions at percentiles P50 and P100 on the ROLL model. The maximum distortions for the vanilla model are 893.3 for 'Projector', 1547.1 for 'Bear', and 5473.5 for 'Rainbow'. For the ROLL model, the maximum distortions are 1367.9 for 'Bear' and 3882.8 for 'Rainbow'."
}