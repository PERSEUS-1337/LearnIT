{
    "title": "Byg6QT3QqV",
    "content": "The development of explainable AI is crucial as AI becomes more integrated into our lives. Robotic teammates need to be able to generate explanations for their behavior, but current approaches often overlook the mental workload required for humans to understand these explanations. In this work, the focus is on generating online explanations during execution to reduce human mental workload. The challenge lies in the interdependence of different parts of an explanation, requiring a careful approach. Three implementations of online explanation generation are presented, based on a model reconciliation setting. Evaluation is done with human subjects in a standard planning scenario. Three different implementations of online explanation generation are presented, evaluated with human subjects in a standard planning competition domain and in simulation with ten different problems across two domains. The method is based on a model reconciliation setting introduced in prior work. As intelligent robots become more prevalent, human-AI interaction is crucial. Explanations from AI agents help convey decision-making reasoning, maintain trust, and enhance shared awareness. Prior work on explanation generation often overlooks the recipient's need to understand the explanation fully. Explanation generation in AI often focuses on supporting the agent's decision without considering the recipient's perspective. To address this, explanations should be generated from the human's viewpoint, taking into account model differences between the robot and human. This approach aims to reconcile discrepancies and improve shared awareness in human-AI interaction. In AI explanation generation, the focus is on supporting the agent's decision from the human's viewpoint, considering model differences between the robot and human. The robot should explain and reconcile model differences to align with the human's expectation, improving shared awareness in human-AI interaction. The robot should generate an explanation to reconcile model differences with the human's expectation, termed model reconciliation. One issue is the lack of consideration for the mental workload required for understanding explanations in AI interaction. In this work, the focus is on providing online explanations that intertwine with plan execution to reduce mental workload. Generating online explanations poses challenges due to the interdependence of different explanation parts. Online explanations intertwine with plan execution to reduce mental workload. Challenges arise from the interdependence of explanation parts, requiring a smooth spread of information to avoid cognitive dissonance. Illustrating with a scenario between friends Mark and Emma planning to study together for an exam. Online explanations reduce cognitive dissonance by presenting information smoothly. In a scenario between friends Mark and Emma planning to study together, Mark's relaxed study plan clashes with Emma's preference for a focused session followed by lunch. Mark's failure to explain his plan at the start leads to potential conflict. Mark and Emma have different study preferences. Mark prefers a relaxed study plan while Emma prefers a focused session followed by lunch. Mark fails to communicate his plan to Emma, leading to potential conflict. After studying for 60 minutes, Mark suggests going to lunch for energy, avoiding revealing his need for a walk to Emma. Mark explains to Emma the importance of taking a lunch break for energy, while gradually revealing his need for a walk to maintain his plan. This online communication strategy helps make his actions acceptable and understandable to Emma, despite their different values regarding study breaks. Mark explains the reasoning behind his plan to Emma, gradually revealing his need for a lunch break to maintain his energy levels. This communication strategy aims to make his actions acceptable and understandable to Emma, despite their differing values on study breaks. The new method developed in this paper intertwines explanation with plan execution to reduce mental workload and make interactions with Mark more straightforward for Emma. The new method developed in this paper is called online explanation, which breaks explanations into parts to be communicated at different times during plan execution. Three different approaches for online explanation generation were implemented, each focusing on different \"online\" properties such as matching the plan prefix. The new method developed in this paper, called online explanation, breaks explanations into parts to be communicated at different times during plan execution. Three different approaches were implemented for online explanation generation, focusing on matching the plan prefix, making the next action understandable, and matching the prefix of the robot's plan with an optimal human plan. This method reduces the mental workload for the recipient. The new method developed in this paper, called online explanation, breaks explanations into parts to be communicated at different times during plan execution. It ensures that earlier information communicated does not affect later parts of the explanation, reducing mental workload for the recipient. The approaches are evaluated with human subjects and in simulation. AI agents have limitations in operating as a teammate, requiring transparency to other members. AI agents have been used in various fields like transportation, medicine, finance, and military, but they are limited in their ability to operate as a teammate. Explainable AI is essential for human-AI collaboration as it helps improve human trust and understanding of the AI agent's decision-making process. Explainable AI is crucial for human-AI collaboration, enhancing trust and shared awareness by accurately modeling human perception and other agents' expectations. This allows the AI agent to generate understandable motions, plans, and assistive actions. The model of other agents allows an AI agent to infer their expectations, generate legible motions, plans, and assistive actions. The agent can signal its intention before execution to improve human understanding and explain its behavior by using the model. The model allows an AI agent to signal its intention before execution and explain its behavior by generating explanations based on the recipient's perception model. This helps improve human understanding and maintain optimal behavior. In prior work, the focus was on generating explanations based on the recipient's perception model. However, the mental workload required for understanding an explanation is often overlooked. The ordering of information in an explanation can influence its perception. It is argued that complex explanations sometimes need to be provided in an online fashion. In this work, it is argued that complex explanations may need to be provided online, intertwining explanation generation with plan execution. The idea is to convey a minimal amount of information that explains part of the plan currently of interest. This approach is based on the model reconciliation setting from prior work. Our problem definition is based on the model reconciliation setting defined in prior work. A planning problem is defined as a tuple (F, A, I, G) using PDDL, similar to STRIPS. F is the set of predicates used to specify the state of the world and A is the set of actions used to change the state of the world. Actions are defined with preconditions, add and delete effects. The planning problem is defined as a tuple (F, A, I, G) using PDDL, similar to STRIPS. F represents predicates for the world state, A for actions to change the state. Actions have preconditions, add and delete effects. The robot's plan, \u03c0 * I,G, must be optimal according to M R, considering rational agents and human model M H. The optimal plan cost under M R is determined by the initial and goal state pair. Model reconciliation involves aligning the robot's behavior with human expectations. Explanation generation aims to bring models M H and M R closer together by updating M H. Explanation generation in a model reconciliation setting involves updating model M H to align with model M R, ensuring the robot's plan is fully explainable in the human's model. A mapping function converts a planning problem into a set of features to specify the problem. Explanation generation in a model reconciliation setting involves updating model M H to align with model M R, ensuring the robot's plan is fully explainable in the human's model. The explanation generation problem is defined as a tuple (\u03c0 * I,G , M R , M H ), where an explanation consists of unit feature changes to reconcile the two models by reducing the cost difference between the human's expected plan and the robot's plan. Feature changes to DISPLAYFORM2 in model M H aim to reconcile two models by reducing the cost difference between human's and robot's plans. A complete explanation in the context of model reconciliation ensures the robot's plan is optimal in the human's model. A minimal complete explanation (MCE) is defined as the explanation with the minimum number of unit feature changes. Online explanation generation is introduced to address the mental workload requirement of humans for understanding explanations during plan execution. The goal is to provide a minimal amount of information to explain the part of the plan that is of interest and not explainable. Online explanation generation is a method introduced to reduce the mental workload of humans during plan execution by providing minimal information to explain the part of the plan that is of interest and not explainable. It involves creating a set of sub-explanations in an online fashion, where each sub-explanation represents a set of unit features to be made at a specific step in the plan. Online explanation generation aims to reduce human mental workload during plan execution by providing minimal information to explain the part of the plan that is of interest and not explainable. The robot can split an explanation into multiple parts, generated online as the plan is executed. Three approaches are discussed: OEG with Plan Prefix matching, OEG with Next Action matching, and OEG with any prefix matching. Approaches for online explanation generation focus on different methods for generating sub-explanations during plan execution. The planning process must consider how model changes affect human expectations. The problem of explanation generation is converted to a model search challenge in the space of possible models, where model changes may not be independent. The problem of explanation generation is converted to a model search challenge in the space of possible models. Model changes may not be independent, and the search involves finding the largest set of model changes that ensure plan prefixes would not change after further sub-explanations. The problem of explanation generation is converted to a model search challenge in the space of possible models. Model changes may not be independent, and the search involves finding the largest set of model changes that ensure plan prefixes would not change after further sub-explanations. The search process is illustrated in FIG1, where an OEG-PP is a set of subexplanations that are recursively processed for each sub-explanation. The explanation generation problem is approached as a model search challenge, aiming to find the optimal plan based on human and robot models. The search process involves recursively processing sub-explanations to match plan prefixes between models. The search starts from the robot model and stops when plan prefixes align with the updated human model. Our approach in the explanation generation problem involves starting the search from the robot model and matching plan prefixes with the updated human model. This process, similar to MME, requires multiple runs but allows for better computation efficiency compared to MCE and MME. Our approach involves running the process multiple times, allowing us to beat both MCE and MME in terms of computation efficiency. The dotted line represents the border of the maximum state space model modification in the robot model, reconciling the two models up to the current plan execution. The approach involves running the process multiple times to beat MCE and MME in computation efficiency. It finds the largest set of model changes to the robot model such that the plan execution matches up to step t2-1. The complement set of changes will be E1 for the next recursive step, starting from action t1 with the human model MHE1. To maintain the prefix for future steps, later plans are forced to be compatible with it. The recursive search algorithm for model space OEG is presented in Algorithm 1 for finding ek given Ek\u22121. Using a recursive model reconciliation procedure on the model space, we start by finding the difference between models M DISPLAYFORM1 and MR, and modify MR with respect to MH. The algorithm for model space OEG involves finding the difference between models M DISPLAYFORM1 and MR, and modifying MR with respect to MH to match the human's plan with the robot's plan. The goal is to ensure that both plans have the same prefix during plan execution. The goal of explanation generation in the OEG-PP algorithm is to ensure that the robot and human plans match at each step of plan execution, even if the human is not interested in earlier actions. The robot only needs to reconcile between MR and MH to match the very next action in the plan, regardless of earlier actions in the plan prefix. This approach is motivated by the human's prefix setup. The robot focuses on reconciling between MR and MH to match the next action in the plan, regardless of earlier actions. This is motivated by the human's limited cognitive memory span. The agent explains the differences between the most recent human plan and the generated plan through a recursive model reconciliation procedure. The differences between the most recent human plan and the robot plan are reconciled by focusing on matching the next action in the plan, without comparing the entire plan prefix. The search is performed from MH \\ MR for computational efficiency, and only the immediate next action that does not match is explained by the agent. The search process of OEG is similar to minimally monotonically explanation (MME) in BID6, but executed multiple times online. Algorithms combine search from MH and MR for better performance. The OEG-PP approach involves model space search to reconcile human and robot plans, assuming a set of optimal plans. The robot does not need to explain if a human plan shares the same prefix as its own plan. The OEG-PP approach involves model space search to reconcile human and robot plans. The goal of OEG is to satisfy the human optimal plan generated from the original human model. A compilation approach is implemented to check plan prefixes efficiently. The OEG-PP approach involves model space search to reconcile human and robot plans efficiently. A compilation approach is implemented to ensure that the robot's plan prefix matches with the human's model, reducing computational costs. The compilation approach ensures that the robot's plan prefix aligns with the human's model, reducing computational costs by adding predicates to actions in the compiled model. The compilation approach aligns the robot's plan prefix with the human's model by adding predicates to actions, reducing computational costs. The search for e k involves a recursive model reconciliation process on the model space, similar to Algorithm 1, to find differences between models and check for human optimal plans with the same plan prefix. After aligning the robot's plan prefix with the human's model using the compilation approach, a recursive model reconciliation process is used to find differences between models and check for human optimal plans with the same plan prefix. The agent iteratively updates the models and identifies new sub-explanations until an optimal human plan matching the robot's plan is found. The process of identifying an optimal human plan that matches the robot's plan involves model space search. This approach was evaluated for online explanation generation with human subjects and in simulation, comparing results with the Minimally Complete Explanation (MCE) BID6 approach. The evaluation was conducted on different problems in the rover and barman domains to compare online explanation with MCE in terms of information needed and computation time. The evaluation of the online explanation generation approach compared to the Minimally Complete Explanation (MCE) BID6 approach was conducted on various problems in the rover and barman domains. The goal was to assess differences in information requirements and computation time. The human subject study aimed to confirm the benefits of online explanation generation, hypothesizing that it would reduce mental workload and improve task performance. The study evaluated online explanation generation in a rover domain to reduce mental workload and improve task performance. The rover's goal is to explore Mars, take rock and soil samples, calibrate its camera, and communicate results to the base station. The rover must calibrate its camera before taking images and collect rock or soil samples. It can only store one sample at a time and must drop the current sample to take another. The robot's goal is to serve a desired set of drinks in a barman role in the domain. The robot in the barman domain serves drinks using drink dispensers, glasses, and a shaker. Constraints include grabbing one object with an empty hand, grabbing one object with one hand, and ensuring a glass is empty and clean before filling it with a drink. Simulation results compare explanations for rover and barman domains. The simulation results compare explanations for rover and barman domains. OEG-PP and OEG-NA may have more model features in some cases than MCE explanations. In some cases, OEG-PP and OEG-NA explanations have more model features than MCE, due to the focus on generating minimal information at each time step. The reason for sharing more information in OEG-PP and OEG-NA lies in the dependence between features and planner behavior. The OEG-PP and OEG-NA approaches provide more information compared to MCE, focusing on the dependence between features and planner behavior. OEG-AP shows the advantage of considering all optimal plans, but there remains a distance between robot and human plans in terms of action distance. OEG-NA only considers the immediate next action, while OEG-AP has no guarantee of optimal plans. The distance between the robot's plan and the human's plan in terms of action distance remains in OEG-NA due to only considering the immediate next action. OEG-AP, on the other hand, considers all optimal human plans but does not guarantee matching the robot's plan. The plan distance gradually decreases in OEG approaches as execution and explanation are intertwined. In our implementation, model updates are sorted based on feature size, with a focus on smallest changes from the robot's side. Backtracking is performed when needed. In our implementation, model updates are sorted based on feature size, focusing on the smallest changes from the robot's side. Backtracking is done when necessary. A human study was designed to compare three approaches for online explanation generation with minimally complete explanation (MCE) BID6. The search process takes advantage of the fact that later information often does not affect previous sub-explanations. In a human study, three approaches for online explanation generation were compared with minimally complete explanation (MCE) BID6. An additional approach, MCE-R, randomly breaks MCE during plan execution to test performance. The experiment was conducted using Amazon Mechanical Turk with 3D simulation, where subjects were given a rover task with a 30-minute time limit. Explanations were in plain English, and rover actions were shown using GIF images. The subjects in the experiment were introduced to the rover domain and given a 30-minute time limit to complete a task. Explanations were provided in plain English, and GIF images from a 3D simulated scenario were used to depict rover actions. The human subjects acted as the rover's commander on Mars, observing the rover's plan and determining the validity of its actions with explanations from OEG approaches or MCEs. In the experiment, human subjects act as the rover's commander on Mars, assessing the rover's actions with explanations from OEG approaches or MCEs. Additional spatial puzzles were included to increase cognitive demand and observe the effect on mental workload. Certain information was deliberately omitted to test subjects' ability to create correct plans without explanations. In the experiment, spatial puzzles were added to increase cognitive demand. Certain information was deliberately hidden, such as limited storage and memory, camera calibration requirements, and objective alignment. This hidden information created differences between M H and M R models, requiring explanations in scenarios. In the model reconciliation setting, hidden information creates differences between M H and M R models, leading to the need for explanations. The robot in the MCE setting shares all information at the beginning, while in MCE-R, information is communicated at different steps. The robot uses various approaches for online explanation generation, intertwining explanation communication with plan execution. In MCE-R, the robot communicates information at different steps using various online explanation generation approaches. Subjects evaluate the robot's actions and efficiency of explanation approaches using the NASA Task Load standard questionnaire. The study evaluated the efficiency of different explanation approaches by using the NASA Task Load Index (TLX) questionnaire. NASA TLX is a tool to assess subjective workload in human-machine interface systems, measuring mental workload through various variables. NASA TLX is a widely used tool to measure mental workload, calculating an overall score based on sub-scales like mental demand, physical demand, temporal demand, performance, effort, and frustration. The experiment excluded physical demand questions and recruited 150 subjects to assess mental demand using an academic survey on MTurk. The study recruited 150 subjects on MTurk to assess mental demand using an academic survey. After filtering out invalid responses, 94 valid responses were obtained. The age range of subjects was between 18 and 70, with 29.8% being female. The study had 94 valid responses from subjects aged 18 to 70, with 29.8% being female. The research focused on how well humans understand a robot's plan by comparing distances across different settings. The distance metric calculated the similarity between human and robot plans based on questionable actions. The study compared human understanding of a robot's plan using a distance metric based on questionable actions. Results showed OEG approaches reduced mental workload better than MCE approaches, as seen in NASA TLX measures. The study found that OEG approaches were more effective in reducing human mental workload compared to MCE approaches, as evidenced by better performance in NASA TLX measures. OEG approaches also led to fewer questionable actions, indicating higher trust in robots. The study compared OEG approaches to MCEs in terms of reducing mental workload and trust in robots. OEG approaches had fewer questionable actions and higher accuracy in identifying correct actions. OEG-AP had the least questionable actions and highest accuracy. Statistical analysis showed a significant difference in mental load between OEG approaches and MCEs. The study compared OEG approaches to MCEs in terms of mental workload and task completion time. Statistical analysis revealed a significant difference in mental workload between the two groups. The average time taken to complete the task varied among the categories, with OEG-NA being the quickest and OEG-PP being the slowest. In a study comparing OEG approaches to MCEs, the average time taken for task completion varied among categories, with OEG-NA being the quickest and OEG-PP the slowest. The accuracy of the secondary task did not show significant differences. A novel approach for explanation generation was introduced to reduce mental workload during human-robot interaction by breaking down complex explanations into smaller parts. The study introduced a novel approach for explanation generation to reduce mental workload during human-robot interaction by breaking down complex explanations into smaller parts. Three different approaches were provided, focusing on generating easily understandable explanations intertwined with plan execution. Evaluation was done using simulation and human subjects, showing promising results for achieving explainable AI. The study introduced three approaches for generating easily understandable explanations intertwined with plan execution to reduce mental workload during human-robot interaction. Evaluation using simulation and human subjects showed improved task performance and reduced mental workload, a crucial step towards achieving explainable AI."
}