{
    "title": "HJcjQTJ0W",
    "content": "Massive data on user local platforms pose challenges for deep neural network (DNN) training due to resource constraints. To address privacy risks in cloud-based training, a method using intermediate representations of data is proposed. This involves splitting DNNs between local platforms and the cloud, with local NN generating feature representations. The local NN is derived from pre-trained NNs to protect data privacy, while the cloud NN is trained based on these representations for the learning task. The idea of DNN splitting is validated by analyzing the relationship between privacy loss and classification accuracy based on local NN topology. PrivyNet is proposed to optimize accuracy for image classification tasks while considering privacy loss, local computation, and storage constraints. It efficiently determines the local neural network topology using the CIFAR-10 dataset. Cloud-based services offer an alternative for deep model training but raise privacy concerns due to excessive user data collection. Cloud-based services offer an alternative for deep model training but raise privacy concerns due to excessive user data collection. To protect user data privacy, different data pre-processing schemes are proposed, where transformed representations are generated locally and uploaded for learning tasks. The requirements for these representations are utility and privacy, ensuring accurate task completion while limiting private information leakage. The transformation scheme should be flexible for platforms with varying computation and storage capabilities. The transformation scheme for protecting user data privacy in cloud-based services should be flexible for different platforms and data types. Various methods, such as syntactic anonymization, have been proposed to balance privacy and utility in data processing. Various privacy protection methods, such as k-anonymity, l-diversity, and t-closeness, aim to anonymize quasi-identifiers and sensitive attributes in databases. However, differential privacy offers a more formal guarantee by adding noise to data. Existing works often require local platforms for deployment, and non-invertible transformations are also used for data anonymization. In the backward propagation process, non-invertible linear and non-linear transformations are proposed for data anonymization. Linear transformations rely on covariance or linear discriminant analysis for filtering training data, but suffer from limited privacy protection. Nonlinear transformations like minimax filter or Siamese networks offer better privacy protection but require an iterative training scheme between cloud and local platforms. The proposed PrivyNet framework aims to achieve a fine-grained control of the trade-off between privacy and utility in DNN training. It divides the model into local and cloud parts, with the local NN generating intermediate representations and the cloud NN trained for the learning task. Privacy protection is ensured through non-linear transformations like convolution and pooling in the local NN. The PrivyNet framework splits the DNN model into local and cloud parts, with the local NN derived from pre-trained NNs to protect privacy while maintaining utility. Key operations include convolution and pooling, enabling fine-grained control over privacy loss. The framework aims to balance privacy and utility in cloud-based training. PrivyNet enables cloud-based training with fine-grained privacy control by using a CNN as the local NN. Three key factors affecting privacy and utility trade-off are identified. A hierarchical strategy optimizes the local NN topology considering computation, storage, and privacy constraints. PrivyNet is validated using CNN-based image classification, showcasing efficiency and effectiveness in leveraging pre-trained NNs for intermediate representation generation. The characterization flow involves generating feature representations using a pre-trained NN, training an image classification network (ICN) based on these features, and training an image reconstruction network (IRN) to reconstruct original images. Utility is measured by task accuracy and privacy by the distance between reconstructed and original images. The IRN is trained assuming knowledge of original images and features, but not the transformation (FEN). This aligns with an adversarial model discussed in later sections. The adversarial model, detailed in Sections 3 and 4, involves a collection of training instances represented by images with label indicators. A transformation function t is induced by the FEN with output feature representations of depth D and dimension W \u00d7 H. The utility is evaluated by learning a classifier based on transformed representations. The utility of transformed representations is evaluated by learning a classifier to minimize empirical risk for the target learning task. Privacy is assessed by minimizing the distance between reconstructed images and original images, measured by peak signal-to-noise ratio (PSNR). The privacy loss of transformed representations is measured using peak signal-to-noise ratio (PSNR) compared to original images. Larger PSNR indicates higher privacy loss. The impact of FEN topology on privacy and utility is characterized, with settings described in detail. FEN is derived from VGG16 pre-trained on Imagenet, using CNN for image classification and reconstruction tasks. The FEN topology is determined by the number of layers and depth. In this section, the architectures of VGG16, ICN, and IRN are evaluated for the reconstruction task. The FEN topology is determined by the number of layers, output depth, and selected channels. Changes in FEN topology impact utility and privacy, with different behaviors observed. Privacy loss is measured by PSNR, showing less privacy loss with reduced output depth. The study evaluates the impact of FEN topology on privacy and utility in VGG16, ICN, and IRN architectures. Privacy loss is measured by PSNR, showing less loss with reduced output depth. Trade-offs between accuracy and PSNR are observed, guiding the PrivyNet framework. When privacy loss is high (blue line, large PSNR), FEN with different topologies show similar utility. However, when privacy loss is low (red line, small PSNR), FEN with more layers tend to provide better utility. The selected subset of output channels also impacts privacy and utility. Comparing utility and privacy loss for each single channel in a FEN with 4 VGG16 layers, the best channel achieves around 4X the utility of the worst. Detailed statistics are provided in Table 4. When comparing utility and privacy for single channels in FEN with 4 VGG16 layers, the best channel achieves 4X the utility of the worst channel, with a privacy loss difference of 6 dB. The impact of output channel selection, number of FEN layers, and output depth is also evaluated, showing changes in privacy and utility. The privacy and utility of representations generated from a pre-trained CNN-based transformation can be controlled by adjusting the number of FEN layers, output channel depth, and output channel selection. Larger dependence is observed for the number of FEN layers and output channel depth in influencing both privacy and utility. Leveraging a pre-trained CNN to build the FEN allows for exploring the trade-off between utility and privacy. In the next section, the framework PrivyNet is proposed to optimize utility under privacy constraints by determining the FEN topology. The FEN topology impacts local computation and storage, especially on lightweight platforms like mobile devices. Consideration of these constraints is crucial in designing the FEN for optimal utility. Our PrivyNet framework optimizes utility under privacy constraints by determining the FEN topology for mobile devices. Privacy characterization and NN performance profiling are conducted to consider constraints on privacy, local computation, and storage. Channel pruning based on private data is done to remove ineffective channels, and the FEN topology is determined accordingly. The FEN topology is determined after randomly selecting output channels. The assumption of original image availability is crucial for worst-case privacy evaluation. The attackers are assumed to be unaware of the FEN transformation to limit privacy loss. The FEN anonymity protection is crucial to prevent attackers from accessing the architecture and weights of pre-trained NNs. The pre-characterization stage involves performance and storage profiling on local platforms and cloud-based privacy characterization for the pre-trained NNs. This profiling is essential due to different computation capabilities and storage configurations across platforms, which directly impact the FEN topology. The FEN anonymity protection is vital to prevent attackers from accessing pre-trained NNs. Privacy characterization is done using cloud-based services, and the reconstruction network is trained on publicly available data. Experiments show that less than 1000 samples are needed for characterization, with similar PSNR changes observed for different FEN topologies. In PrivyNet, the number of FEN layers and output channel depth impact privacy and accuracy. The topology is determined based on pre-characterization results, considering local constraints. Less than 1000 samples are needed for accurate characterization with data augmentation. PSNR values may be less accurate for privacy tasks. The number of FEN layers and output channel depth in PrivyNet impact privacy and accuracy. The topology is determined based on pre-characterization results, considering local constraints on computation, storage, and privacy loss. When privacy requirements are high, a deep FEN layer is selected, while for low privacy requirements, a shallow FEN is chosen. The output depth is determined by the privacy constraints in both cases. When determining the FEN layers and output channel depth in PrivyNet, the privacy and utility trade-off is considered. By selecting a shallow FEN with the required privacy level, the output channel depth is adjusted accordingly to minimize local computation and storage consumption. Different privacy loss requirements can be achieved with varying FEN configurations, leading to the selection of specific parameters to optimize utility. Output channel selection is crucial for balancing utility and privacy, as shown in Figure 4. After determining the FEN layers and output depth in PrivyNet, output channel selection is crucial for balancing utility and privacy. Large variances in utility and privacy are observed when selecting channels directly from the whole set, leading to the necessity of channel pruning. The correlation between utility and privacy loss for a single channel is negligible, as shown in Figures 4 and 10. In Figure 10, among the top 32 channels with the largest privacy loss, 4 channels also have the worst utility. In PrivyNet, after determining FEN layers and output depth, channel pruning is essential for balancing utility and privacy. Figure 10 shows that among the top 32 channels with the largest privacy loss, 4 channels also have the worst utility. This observation allows for optimizing utility while suppressing privacy loss simultaneously. Fisher's LDA analysis is used to identify channels with the worst utility. Fisher's LDA scheme measures the distance between representations of images within the same class and different classes using the covariance matrix. It is effective in identifying ineffective channels by analyzing the output representation. Fisher's LDA scheme analyzes representations to identify ineffective channels for pruning, improving accuracy in learning tasks. The algorithm computes Fisher's discriminability for each channel, pruning those with the worst utility. The effectiveness of the LDA-based supervised channel pruning algorithm is verified through experiments. The experimental setup validates the LDA-based supervised channel pruning algorithm's effectiveness by leveraging Fisher's discriminability to identify and prune ineffective channels in VGG16 layers. Results show a significant reduction in bad channels compared to random pruning methods. The experimental results demonstrate the effectiveness of supervised channel pruning using Fisher's discriminability in VGG16 layers. Pruning 32 or 64 channels with the worst utility shows similar results, with minimal extra computation introduced. The complexity of the pruning process scales with the number of samples, indicating its efficiency. To demonstrate the effectiveness of supervised channel pruning, the layer of FEN was set to 6 with an output depth of 8. Three settings were compared for privacy and utility of released representations: random selection, channel pruning based on privacy and utility characterization followed by random selection, and channel pruning based on privacy characterization and LDA followed by random selection. Pruning involved removing 64 channels with the worst utility and 32 channels with the largest privacy loss. Results showed that random selection without pruning had an average PSNR of 18.3 dB, while the PSNRs for the pruned settings were lower. After pruning, better utility and less privacy leakage were achieved simultaneously. The LDA-based pruning strategy showed 1.1% better accuracy and 1.25 dB smaller PSNR compared to random selection without pruning. The method also had similar accuracy to the characterization-based pruning strategy but with slightly less privacy loss. The effectiveness of the supervised pruning strategy was verified through utility and privacy comparison for the three settings. In this section, the paper discusses the adversarial model adopted, focusing on protecting the anonymity of the FEN derived from pre-trained NNs. Strategies are provided to prevent more powerful attacks and enhance privacy protection. Two methods are considered for FEN protection: building a pool of pre-trained NNs and enabling FEN. In this framework, strategies are proposed to protect the anonymity of the FEN derived from pre-trained NNs. Two methods are suggested: building a pool of pre-trained NNs and applying channel selection procedures to enhance privacy protection. The goal is to make it harder for attackers to guess how the FEN is derived, without sacrificing utility or increasing privacy loss. In the framework to protect the anonymity of the FEN derived from pre-trained NNs, strategies involve channel selection procedures to enhance privacy without sacrificing utility. Empirical verification is done by gradually reducing channel depth in convolution layers, showing minimal impact on privacy and utility with a significant reduction in runtime. After reducing channel depth in convolution layers, the privacy and utility of the FEN remain similar with a significant decrease in runtime. PrivyNet is a flexible framework for cloud-based training with fine-grained privacy protection, beneficial for resource-constrained platforms like hospitals storing patient data for disease diagnosis and prevention. PrivyNet is a flexible framework for cloud-based training with fine-grained privacy protection. It can be used in modern hospitals to release informative features from patient data for disease diagnosis and treatment. It is also beneficial for mobile platforms to upload collected data to the cloud while protecting private information. PrivyNet is a flexible framework for cloud-based training with fine-grained privacy protection. It enables mobile platforms to upload data to the cloud while safeguarding private information. The framework is platform-aware and allows for control over privacy and utility, making it applicable for various end-users and situations. The characterization uses CIFAR-10 and CIFAR-100 datasets, with detailed information on the image classes and sizes. Additionally, the framework utilizes pre-trained models like VGG16 for privacy and accuracy assessment. The framework PrivyNet utilizes pre-trained models like VGG16 for privacy and accuracy assessment. VGG16 architecture is used for image classification and reconstruction tasks, with CNN for h and generative NN based on ResNet blocks for g. The image reconstruction network (IRN) is constructed following ResNet block clusters, with 8 ResNet blocks in each cluster. Gradient descent optimizer is used in the training process. In experiments, Tensorflow example is followed using gradient descent optimizer. Learning rates and batch sizes are set for image reconstruction and classification tasks. Data augmentation includes normalization, brightness, and contrast modifications. IRN's image recovery capability depends on ResNet block clusters. Topology of IRN is determined before characterization for accurate privacy evaluation. In experiments, the image recovery capability of IRN is determined by the number of ResNet block clusters. The quality of reconstructed images saturates with an increase in ResNet block clusters. Performance profiling of VGG16 on different CPUs and storage requirements with increasing VGG16 layers are also analyzed. The performance profiling of VGG16 on mobile and server class CPUs, along with storage requirements increasing with VGG16 layers, shows a rapid rise in local computation and storage needs. Convolution layers contribute most to computation, while fully connected layers dominate storage, especially with larger input image sizes. Different platforms may face varying bottlenecks, highlighting the need for a flexible framework considering local computation and storage differences. The necessity for a flexible framework considering local computation and storage differences is highlighted. The complexity of extra computation is determined by the number of samples and output dimensions. The computation complexity is influenced by factors like the characteristics of the FEN and the learning task. Small N LDA values are usually sufficient for good pruning results, resulting in minimal computation overhead."
}