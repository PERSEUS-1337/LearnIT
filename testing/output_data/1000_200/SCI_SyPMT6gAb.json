{
    "title": "SyPMT6gAb",
    "content": "Off-policy learning involves evaluating and improving policies using historical data from a logging policy. This is important because on-policy evaluation is costly and has negative effects. A major challenge in off-policy learning is developing counterfactual estimators with low variance and generalization error. In this work, a new counterfactual learning principle for off-policy learning with bandit feedbacks is introduced. The method minimizes distribution divergence between logging policy and new policy to regularize generalization error, eliminating the need for sample variance regularization. End-to-end training algorithms using variational divergence minimization with neural network policies show significant improvement over conventional baseline algorithms. Off-policy learning is crucial for evaluating deterministic policies using historical data, as on-policy evaluation is costly and has negative impacts. Off-policy evaluation is essential for assessing policies using historical data to avoid costly and risky on-policy evaluations. Various methods like Q learning, doubly robust estimator, and self-normalized approaches have been studied in the context of reinforcement learning and contextual bandits. A new direction in off-policy learning involves using logged interaction data for safe exploration of policy hypotheses before deployment. In off-policy learning with bandit feedback, limited feedback is observed in the form of scalar rewards for actions taken. Historical data is used to address challenges in counterfactual evaluation. In off-policy learning with bandit feedback, historical data is used to address challenges in counterfactual evaluation. BID34 introduced a new framework for counterfactual risk minimization, incorporating sample variance as a regularization term. However, the linear stochastic model used for policy parametrization limits representation power, and efficient training algorithms remain a challenge. Our contribution in this paper is three-fold: 1. We propose a new learning principle for off-policy learning with bandit feedback by minimizing the distribution divergence between the new policy and the logging policy. 2. We parametrize the policy as a neural network and solve the divergence minimization problem using variational divergence minimization and Gumbel soft-max sampling. 3. Experimental evaluation on benchmark datasets shows significant performance improvement over conventional baselines. Our experiment evaluation on benchmark datasets shows significant improvement in performance over conventional baselines, and case studies also corroborate the soundness of our theoretical proofs. The framework of off-policy learning with logged bandit feedback is reviewed, where a policy maps an input to a structured output. In off-policy learning, a policy h(Y|x) is used to select actions by sampling from a distribution. Feedback is observed by comparing the selected action to an underlying 'best' action y*. The goal is to minimize expected risk on test data. In offline logged learning, only feedback on sampled actions is available. In off-policy learning, the goal is to find a policy h(Y|x) with minimum expected risk on test data. In offline logged learning, data is collected from a logging policy h0(Y|x) and the aim is to improve this policy to have lower expected risks. Challenges include skewed distribution of the logging policy and the need for empirical estimation due to finite samples, leading to generalization error. In off-policy learning, the goal is to minimize the expected risk on test data by finding a policy h(Y|x). Challenges include skewed distribution of the logging policy and the need for empirical estimation due to finite samples, leading to generalization error. To address this, a propensity scoring approach using importance sampling is used to account for distribution mismatch. Counterfactual risk minimization aims to regularize variance by introducing a regularization term derived from empirical Bernstein bounds. The authors proposed a regularization term for sample variance derived from empirical Bernstein bounds to address the challenges in off-policy learning. They approximated the regularization term via first-order Taylor expansion to enable stochastic training, but this neglects non-linear terms and introduces approximation errors. Instead of estimating variance empirically from samples, they considered deriving a variance bound directly from the parametrized policy. The authors aim to derive a variance bound directly from the parametrized policy h(Y|x) by utilizing importance sampling weights. They introduce a lemma regarding the R\u00e9nyi divergence and importance sampling weights, which allows for the derivation of an upper bound. The authors derive an upper bound for the second moment of the weighted loss using importance sampling weights and the R\u00e9nyi divergence. This bound relates to the expected risk and empirical risk of a new policy on a loss function. The authors provide an upper bound for the second moment of the weighted loss using importance sampling weights and the R\u00e9nyi divergence. This bound relates to the bias-variance trade-offs in empirical risk minimization problems, motivating the minimization of variance regularized objectives in bandit learning settings. In bandit learning settings, minimizing variance regularized objectives is crucial due to the bias-variance trade-offs in empirical risk minimization problems. An alternative formulation involves a constrained optimization approach with a pre-determined constant \u03c1 as the regularization hyper-parameter. This new formulation aims to address the challenge of setting \u03bb empirically and optimizing the objective effectively. The new formulation, DISPLAYFORM0, introduces a regularization hyper-parameter \u03c1. The robust objective R d(h||h0\u2264\u03c1) (h) serves as a good surrogate for the true risk R(h), with their difference bounded by \u03c1. When dealing with parametrized distributions and finite samples, estimating the divergence function becomes challenging. Recent f-gan networks and Gumbel soft-max sampling techniques can aid in this task. The stochasticity of the logging policy is emphasized for counterfactual learning. The stochasticity of the logging policy is crucial for counterfactual learning. A deterministic logging policy with peaked masses and zeros in unexplored regions makes learning difficult. The derived variance regularized objective requires minimizing the square root of a certain integral, leading to an unbounded generalization bound in cases where counterfactual learning is not possible. The derived variance regularized objective requires minimizing the square root of a certain integral, leading to an unbounded generalization bound in cases where counterfactual learning is not possible. By connecting the divergence to the f-divergence measure, a lower bound of the objective can be reached using the f-GAN for variational divergence minimization method. The second equality is derived using Fenchel convex duality for a convex function f. The third inequality arises from restricting T to a family of functions. The universal approximation theorem of neural networks allows for the use of neural networks to satisfy the equality condition. The final objective involves a saddle point of a function mapping input pairs to a scalar value. The final objective is a saddle point of a function T(x, y) mapping input pairs to a scalar value. The policy h(Y|x) acts as a sampling distribution, and the saddle point trained with mini-batch estimation is a consistent estimator of the true divergence. The true divergence is denoted by Df = sup T T dhdx - f*(T)dh0dx, with empirical estimators \u0125 and h0 obtained by sampling from the two distributions. The estimation error is decomposed into terms related to the parametric family of neural networks and the approximation error of empirical mean estimation. The error in the estimation is decomposed into terms related to the parametric family of neural networks and the approximation error of empirical mean estimation. By applying the strong law of large numbers and optimality conditions, the error terms approach zero. Generative-adversarial approaches can also be utilized for further refinement. Applying SLLN and Theorem 5, we can conclude that the error terms approach zero. A generative-adversarial approach can be used by representing the T function as a discriminator network and parametrizing the policy distribution h(y|x) as a generator neural network. Gumbel soft-max sampling is employed for differential sampling, and a complete training procedure is outlined for optimizing the generator. The algorithm presented optimizes a generator distribution to minimize divergence from the initial distribution, using mini-batch sampling and Gumbel soft-max. It includes a training algorithm for variance regularization and counterfactual risk minimization from logged data. The algorithm optimizes a generator distribution to minimize divergence from the initial distribution, using mini-batch sampling and Gumbel soft-max. It includes a training algorithm for variance regularization and counterfactual risk minimization from logged data. The algorithm works in two separate training steps: updating the generator and discriminator for divergence minimization. The algorithm minimizes variance regularized risk in two separate training steps: updating policy parameters to minimize reweighed loss, and updating generator and discriminator to improve generalization performance. Historic data exploitation is crucial in bandit problems, with approaches like doubly robust estimators proposed. Recent theoretical studies have explored minimax risk lower bounds and adaptive learning algorithms for bandit problems. Recent works in deep RL have addressed off-policy updates using methods like multi-step bootstrapping and off-policy training of Q functions. Learning from logs traces involves applying propensity scores to evaluate candidate policies, similar to treatment effect estimation in statistics. Techniques such as Q function learning and temporal difference learning are used for off-policy learning in RL by considering the Markov property of the decision process. The problem of treatment effect estimation in observational studies is addressed using unbiased counterfactual estimators and variance regularization for off-policy learning with bandit feedback. Techniques like Q function learning and temporal difference learning are used in deep RL for off-policy updates. The distributional shift in supervised learning, also known as covariate shift, is discussed along with variance regularized empirical risk minimization for convex objective functions. The divergence minimization technique can be applied to supervised learning and domain adaptation problems to address distribution matching issues. Regularization for the objective function is closely related to distributionally robust optimization techniques, where the empirical risk is minimized over an ellipsoid uncertainty set. The Wasserstein distance between empirical and test distributions is a well-studied constraint that achieves robust generalization performance. The empirical distribution and test distribution are key constraints for achieving robust generalization performance. Algorithms are evaluated by converting supervised learning to bandit feedback. Bandit feedback datasets are created by passing samples through a logging policy and recording actions, loss values, and propensities. In evaluation, two types of metrics are used for the probabilistic policy h(Y|x): expected loss (EXP) and average hamming loss of maximum a posteriori probability (MAP). MAP predictions are faster but may not capture the diversity of predictions, leading to potential differences in generalization performance among policies with the same MAP performance. The comparison of different algorithms for probabilistic policies is discussed, highlighting the importance of considering generalization performance along with MAP performance. Baselines like IPS and POEM are compared using various optimization solvers, with hyperparameters selected based on validation set performance. The effectiveness of variance regularization is verified by comparing with NN-NoReg baselines. Without divergence regularization (NN-NoReg), compared as baselines to verify the effectiveness of variance regularization. Four multi-label classification datasets from the UCI machine learning repo are used, with a three-layer feed-forward neural network for policy distribution and a two or three layer feed-forward neural network for divergence minimization. Separate training version 2 is used for benchmark comparison, trained with Adam optimizer with learning rates of 0.001 and 0.01 for reweighted loss and divergence minimization. PyTorch is used for implementation and training. By implementing neural network policies with Gumbel-softmax sampling schemes, test performance significantly improves compared to baseline CRF policies. The introduction of additional variance regularization further enhances results. The pipelines were implemented using PyTorch and trained on Nvidia K80 GPU cards. Results from 10 experiment runs are reported in TAB0, showcasing the effectiveness of the neural network parametrization. Codes for reproducing the results and preprocessed data are available for download. The introduction of additional variance regularization in neural network policies with Gumbel-softmax sampling schemes leads to improvements in testing loss and MAP prediction loss. Different iterations in divergence minimization sub loops were studied quantitatively to analyze the effectiveness of variance regularization. Results were compared using the yeast dataset, showing that models without regularization had higher expected loss in test sets. The introduction of additional variance regularization in neural network policies with Gumbel-softmax sampling schemes improves testing loss and MAP prediction loss. Models without regularization had higher test loss, while adding regularization led to faster convergence and better generalization to test sets. Increasing the number of divergence minimization steps further improved test performance. Theoretical bounds suggest that algorithm generalization improves with more training samples. Our algorithm's performance improves with an increase in training samples. Varying the number of passes of training data and applying regularization leads to better generalization performance. However, excessive training sample replay can result in overfitting, as indicated by a decrease in MAP prediction performance. In this section, experiments compare two training schemes: cotraining in Alg. 3 and an easier version Alg. 2. The figures show that blending weighted loss and distribution divergence slightly improves performance. However, balancing the gradient of the objective function makes training more challenging. There is no significant difference between two Gumbel-softmax sampling schemes. The discussion also touches on the impact of logging policies. In this section, the impact of logging policies on learning performance is discussed. The stochasticity of the logging policy affects the algorithm's ability to learn an improved policy. By modifying the parameter h 0 with a temperature multiplier \u03b1, the logging policy can become more deterministic. Varying \u03b1 in the range of 2 [\u22121,1,...,8] shows how the expected test loss ratio changes. The stochasticity of the logging policy impacts learning performance. Varying the temperature parameter \u03b1 affects the policy's determinism. NN policies outperform logging policies with sufficient stochasticity in h 0. Stronger regularization in NN policies shows better performance. The regularization in NN policies improves performance against weaker regularization. As the quality of h0 improves, models consistently outperform baselines, but face increasing difficulty. The impact of logging policies on learned improved policies is discussed, highlighting the trade-off between policy accuracy and sampling biases. The study explores the trade-off between policy accuracy and sampling biases introduced by logging policies. By varying the proportion of training data points, improved policies outperform the logging policy, addressing sampling biases. Regularizing variance in off-policy learning for logged bandit datasets is proposed to enhance generalization performance. The study proposes regularizing variance to improve generalization performance in off-policy learning for logged bandit datasets. A new training principle combining importance reweighted loss and distribution divergence regularization is introduced. Variational divergence minimization and Gumbel soft-max sampling techniques are used to train neural network policies effectively. Evaluations on benchmark datasets validate the effectiveness of the learning principle and algorithm, with limitations related to the need for propensity scores. Further case studies confirmed the theoretical discussion. Limitations include the requirement for propensity scores, which may not always be available. Learning to estimate propensity scores and incorporating them into the training framework will enhance the applicability of the algorithms. Directly learning importance weights has comparable theoretical guarantees and could be a valuable extension. The techniques and theorems can be extended to general supervised learning and reinforcement learning. Applying Lemma 1 to importance sampling weight function and loss function yields promising results. The text discusses the application of Lemma 1 to importance sampling weight functions and loss functions, yielding promising results. It also mentions the use of Bernstein's concentration bounds to establish bounds for importance sampling in bandit learning. The goal is to optimize a generator through iterative updates while ensuring convergence. The text discusses optimizing a generator through iterative updates to ensure convergence. It includes updating the discriminator and generator, sampling fake and real samples, estimating generator gradients, and minimizing variance regularized risk. The statistics of datasets are reported, and the effect of stochasticity on test loss is analyzed. The text discusses improving expected loss and MAP predictions over h0 policy. While NN policies can pick up patterns easily, beating baselines is difficult. Further investigation is needed on this phenomenon."
}