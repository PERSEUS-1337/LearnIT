{
    "title": "Skh4jRcKQ",
    "content": "Training activation quantized neural networks involves minimizing a piecewise constant training loss using a straight-through estimator (STE) in the backward pass. The concept of STE is theoretically justified in this paper by explaining why searching in its negative direction minimizes the training loss. The STE-modified chain rule provides a coarse gradient, and the proper choice of STE is crucial for optimization. The proper choice of STE in training activation quantized neural networks is crucial for optimization. The expected coarse gradient correlates positively with the population gradient, and its negation is a descent direction for minimizing the population loss. A poor choice of STE can lead to instability near certain local minima, as verified with CIFAR-10 experiments. Deep neural networks have been successful in various machine learning applications. Recent efforts have focused on training coarsely quantized deep neural networks to achieve memory savings and energy efficiency during inference. This involves solving a challenging optimization problem of minimizing a nonconvex empirical risk function subject to a discrete set-constraint. Weight quantization of DNN is a widely studied problem in the literature. Training activation quantized DNN faces challenges due to the gradient being almost everywhere zero. To address this issue, a non-trivial search direction is constructed by modifying the chain rule, replacing zero derivatives with related surrogates. The straight-through estimator (STE) is a proxy derivative used in the backward pass for quantized activation functions. It originates from the perceptron algorithm and is based on a modified chain rule. Other approaches include stochastic neurons and the target propagation algorithm for learning binary activated networks. The straight-through estimator (STE) is a proxy derivative used in the backward pass for quantized activation functions. It has been extensively discussed in the literature and has been applied to train multi-layer networks with binary activations. Different variants of STE have been proposed, including using the derivative of the sigmoid function and substituting the derivative of the signum activation function. This technique has been employed in training deep neural networks with various types of quantized activations. The straight-through estimator (STE) has been used in training deep neural networks with quantized activations, including ReLU. Limited theoretical understanding exists despite empirical success. Recent studies have explored scenarios where certain layers are not ideal for back-propagation, proposing alternative approaches for improving generalization accuracy. In recent studies, Wang et al. and Athalye et al. proposed methods to enhance the generalization accuracy of deep neural networks. Athalye et al. introduced a backward pass differentiable approximation to overcome adversarial defenses, breaking defenses at ICLR 2018. The \"coarse gradient\" concept was introduced to describe the derivative of the loss function with respect to weight variables using the STE-modified chain rule. The mismatch between backward and forward passes makes the coarse gradient not a standard gradient, but searching in its negative direction minimizes training loss. The text discusses the significance of the gradient of the loss function and the choice of Straight-Through Estimator (STE) in minimizing training loss. It explores different STEs for learning networks with binary activation and Gaussian data, proving that proper STE choices lead to descent training algorithms. The text explores the impact of different Straight-Through Estimators (STE) on training algorithms for binary activation networks. It proves that proper STE choices result in descent training algorithms, with clipped ReLU STE showing the best performance on deeper networks like VGG-11 and ResNet-20. In CIFAR experiments, using identity or ReLU STE can lead to unstable training with higher loss and decreased accuracy, indicating poor STEs generate coarse gradients. Convergence guarantees for perceptron and Convertron algorithms were proven for identity STE, but do not apply to networks with two trainable layers. Identity STE is considered a poor choice in this case. In this paper, the identity STE is deemed a poor choice for networks with two trainable layers. The quantized activation function's monotonicity is crucial for coarse gradient descent, with all three STEs leveraging this property. The clipped ReLU STE is highlighted as a superior option to avoid instability issues. The study focuses on the energy landscape of a two-linear-layer network with binary activation and Gaussian data, comparing empirical performances of different STEs in activation quantization. In section 4, empirical performances of different STEs in 2-bit and 4-bit activation quantization are compared, highlighting instability issues associated with poor STEs in CIFAR experiments. Technical proofs and figures are deferred to the appendix. Notations include Euclidean norm, spectral norm, zero vector, one vector, identity matrix, inner product, and Hadamard product. The model outputs predictions for input Z using trainable weights w and v in linear layers. The prediction for input Z is made using trainable weights w and v in linear layers. The first layer acts as a convolutional layer, with each row of Z being a patch and the weight filter w shared among all patches. The second layer serves as the classifier, generating labels based on the parameters v* and w*. The activation function used is a binary function, not ReLU. The entries of Z are assumed to be i.i.d. sampled from a Gaussian distribution. The text discusses the learning task of minimizing population loss by assuming Gaussian distribution of input data. It mentions the challenge of not having access to the gradient of the objective function for network training, and introduces the idea of replacing zero components with a related non-trivial function. The text introduces the idea of replacing zero components with a related non-trivial function in order to train a two-linear-layer convolutional neural network (CNN) using the STE \u00b5 method. This method gives rise to a coarse gradient descent for learning the network. Additionally, preliminaries about the landscape of the population loss function f (v, w) are presented, defining the angle between w and w * as \u03b8(w, w * ). The text elaborates on the landscape of the population loss function f(v, w) by defining the angle between w and w* as \u03b8(w, w*). It discusses the analytic expressions of f(v, w) and \u2207f(v, w), presenting Lemmas 1 and 2 regarding the population loss function. It highlights that (v, 0m) cannot be a local minimizer, and identifies possible minimizers of the model (2) as stationary points and non-differentiable points. The text discusses the global minimizers of the population loss function and proves that stationary points can only be saddle points. It also identifies potential spurious local minimizers and shows the Lipschitz continuity of the population gradient. The focus is on the complex case where both saddle points and spurious local minimizers exist. The text focuses on the complex case with both saddle points and spurious local minimizers. Algorithm 1 using ReLU or clipped ReLU derivatives converges to a critical point, while using the identity function does not. Theorem 1 states that with a sufficiently small learning rate, the objective sequence decreases monotonically, converging to a saddle point or local minimizer. The sequence {f(vt, wt)} monotonically decreases, converging to a saddle point or local minimizer. The convergence guarantee for coarse gradient descent is established under the assumption of infinite training samples. With few data, empirical loss descends roughly along the negative coarse gradient direction. As sample size increases, the loss gains monotonicity and smoothness, explaining the success of STE in deep learning with large datasets. The text discusses the effectiveness of STE with large datasets in deep learning. It mentions that the Gaussian assumption on input data can be weakened to a rotation-invariant distribution. The mathematical analysis for the main results is outlined, including the plots of empirical loss with different sample sizes. The expected coarse gradient with ReLU as the STE is also explained. The text discusses the effectiveness of STE with large datasets in deep learning, focusing on the Gaussian assumption on input data and a rotation-invariant distribution. Lemma 5 highlights the correlation between coarse and population gradients, forming a descent direction for minimizing population loss. The significance of the estimate (12) in guaranteeing the descent property of Algorithm 1 is emphasized. The text emphasizes the importance of estimate (12) in ensuring the descent property of Algorithm 1. It discusses convergence to critical points using ReLU and clipped ReLU STE, highlighting the correlation between coarse and true partial gradients for population loss minimization. The coarse partial gradient using clipped ReLU STE generally correlates positively with the true partial gradient of the population loss. It vanishes only at critical points and converges when Algorithm 1 converges. The derivative of the identity function yields different results compared to Lemmas 5 and 6. The coarse gradient derived from the identity function does not vanish at local minima, leading to Algorithm 1 potentially never converging there. Lemmas 9 and 10 provide conditions where the coarse gradient descent may not converge near spurious minimizers. The coarse gradient derived from the identity function may not vanish at local minima, causing Algorithm 1 to potentially never converge there. The iterates {(v t , w t )} may initially move towards a local minimizer, but as they approach it, the training loss begins to increase due to the descent property not holding. Empirical performances of vanilla and clipped ReLUs on deeper nets differ, with clipped ReLU showing potential as the best performer for 2-bit or 4-bit quantized activations. The instability issue of the clipped ReLU is also discussed. The clipped ReLU is considered the best performer for quantized activations, with a focus on carefully choosing the resolution \u03b1 to maintain accuracy. A modified batch normalization layer is used to determine the best \u03b1, which is then fixed during training. The modified LeNet-5 model includes batch normalization prior to each activation layer. The optimizer used is stochastic gradient descent with momentum = 0.9. Training consists of 50 epochs for LeNet-5 on MNIST, and 200 epochs for VGG-11 and ResNet-20 on CIFAR-10. Parameters are initialized from pre-trained full-precision models, and the learning rate schedule is specified in the appendix. Experimental results are summarized in Table 1, showing training losses and validation accuracies. The experimental results in Table 1 show that the derivative of clipped ReLU performs the best among the three STEs, followed by vanilla ReLU and then the identity function. Clipped ReLU is the top performer for deeper networks, while vanilla ReLU shows comparable performance on the shallow LeNet-5 network. The use of the identity function on ResNet-20 with 4-bit activations leads to instability, as predicted in Theorem 1. Gradient descent algorithms using vanilla and clipped ReLUs converge to minima with validation accuracies. The coarse gradient descent algorithms using different STEs converge to neighborhoods of minima with varying validation accuracies. Training with the identity STE leads to a worse minimum compared to using other STEs. The training with identity STE results in a worse minimum due to the coarse gradient not vanishing at good minima. Similarly, ReLU STE also performs poorly on 2-bit activated ResNet-20 due to instability at good minima. The coarse gradient descent using identity STE is repelled from good minima on ResNet-20 with 4-bit activations. The first theoretical justification for STE is that it leads to a descent training algorithm. The study explored three different Straight-Through Estimators (STEs) for training a two-linear-layer CNN with binary activation. It was found that negative expected coarse gradients based on vanilla and clipped ReLUs are effective for minimizing population loss, while the identity STE is not suitable due to generating incompatible coarse gradients. CIFAR experiments confirmed issues with improper STE choices. Future work aims to understand coarse gradient descent for large-scale optimization problems with intractable gradients. The study investigated Straight-Through Estimators (STEs) for training a two-linear-layer CNN with binary activation. Negative expected coarse gradients from vanilla and clipped ReLUs were effective in minimizing population loss, while the identity STE generated incompatible coarse gradients. CIFAR experiments highlighted issues with improper STE choices. Future work aims to understand coarse gradient descent for large-scale optimization problems with intractable gradients. The text discusses the use of polar representation for Gaussian random variables and provides proofs for various identities and inequalities related to Gaussian random vectors. The study aims to understand coarse gradient descent for large-scale optimization problems with intractable gradients. The text discusses inequalities and proofs related to Gaussian random vectors in polar representation, focusing on coarse gradient descent for optimization problems with intractable gradients. Key details include Cauchy-Schwarz inequality, projection of vectors, population loss function, and partial gradients. The text discusses the validation of claims related to Gaussian random vectors in polar representation. It shows the proof of Lemma 2, the local optimality of stationary points, and the identification of saddle points in optimization problems. The Hessian matrix is shown to be indefinite, indicating saddle points. The Hessian matrix is indefinite, indicating saddle points in optimization problems. The perturbed objective value is analyzed, showing that for small non-zero changes in variables, the objective function increases. Additionally, the proof of Lemma 3 is discussed, showing the existence of Lipschitz constant for differentiable points. Lemma 4 states the expected partial gradient of (v, w; Z) with respect to v and the expected coarse gradient with respect to w. The proof involves invoking Lemmas 11 and 14, leading to the validation of the claim. Lemma 5 states the inner product between expected coarse and true gradients w.r.t. w. If certain conditions are met, there exists a constant A relu > 0. The proof involves Lemmas 2 and 4, showing the relationship between w and w*. Lemma 7 discusses the inner product between expected coarse and true gradients with certain conditions met. The proof involves computing E Z g crelu (v, w; Z) and showing the relationship between w and w*. Lemma 12 states that if \u03b8(w, w*) = 0, \u03c0, then the inner product between E Z g crelu (v, w; Z) and the estimate from FORMULA2 is bounded. The proof is similar to Lemma 6, with q(\u03b8, w) being non-negative and equal to 0 only at \u03b8 = 0, \u03c0. Lemma 9 shows the expected coarse partial gradient w.r.t. w. In the last equality, the identity (I m + 1 m 1 m )1 m = (m + 1)1 m was used twice. If w = 0 n and \u03b8(w, w * ) \u2208 (0, \u03c0), then the inner product between the expected coarse and true gradients w.r.t. w is calculated."
}