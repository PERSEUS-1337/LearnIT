{
    "title": "Byg6QT3QqV",
    "content": "The development of explainable AI is crucial as AI becomes more integrated into our lives. Robotic teammates need to be able to generate explanations for their behavior, but current approaches often overlook the mental workload required for humans to understand these explanations. In this work, the importance of providing online explanations during execution to reduce mental workload is emphasized. The challenge lies in the interdependence of different parts of an explanation, requiring a careful approach to generating online explanations. Three different implementations are presented, based on a model reconciliation setting, and evaluated with human subjects in a planning competition domain and in simulation with various problems. As intelligent robots interact more with humans, providing explanations for their decisions is crucial for maintaining trust and shared awareness. Prior work on explanation generation often overlooks the recipient's perspective, but a good explanation should consider the differences between human and robot models. To address discrepancies between human and robot models in explanation generation, the robot should reconcile model differences by adjusting its behavior to align with the human's expectations. This process, termed model reconciliation, ensures that the robot's actions make sense to the human. However, one remaining issue is the neglect of mental workload. The general decision-making process of an agent in the presence of model differences is called model reconciliation. One challenge is the ignorance of the mental workload required for understanding an explanation. Explanations, especially complex ones, should be provided online to reduce mental workload. Generating online explanations must consider the interdependence of different parts of the explanation. The online explanation generation process involves spreading out information to be communicated while ensuring smooth perception without cognitive dissonance. This is illustrated through a scenario between friends Mark and Emma planning a study session, highlighting the importance of considering the interdependence of different parts of an explanation. Mark prefers to keep his study session in one go and have lunch afterwards. He doesn't initially share his plan with Emma, but eventually explains that they need energy to continue studying, leading them to go for lunch. Mark also refrains from mentioning his need for a walk, as he reveals his reasoning gradually to maintain his plan. This example emphasizes the importance of providing explanations in an online fashion. In this paper, a new method for explanation generation is developed, intertwining explanation with plan execution. The new form of explanation, called online explanation, considers the mental workload of the receiver by breaking it into multiple parts. This approach aims to spread out information throughout the plan execution, reducing the mental workload and making interactions more straightforward. The new method for explanation generation, known as online explanation, focuses on reducing the mental workload of the receiver by breaking explanations into multiple parts communicated at different times during plan execution. Three approaches were implemented, each emphasizing different \"online\" properties to ensure a smoother interaction between human and robot teammates. The goal is to match plan prefixes, make the next action understandable, and align the robot's plan with a possible optimal human plan. This method significantly reduces mental workload and enhances the recipient's experience. The explanation generation method, called online explanation, aims to reduce the mental workload of the recipient by breaking explanations into parts communicated at different times. Explainable AI is crucial for human-AI collaboration, improving trust and maintaining shared situation awareness. AI helps improve human trust in the AI agent by explaining its decision-making process and modeling the human's perception accurately. This allows the agent to generate understandable actions, plans, and assistive actions, prioritizing explicability over cost optimality. Additionally, the AI agent can signal its intentions before execution to provide additional context information. The model can be used by an AI agent to signal its intention before execution and explain its behavior by generating explanations. Research focuses on generating the \"right\" explanations based on the recipient's perception model, but the mental workload required for understanding an explanation is often ignored. In prior work, the mental workload for understanding explanations is often overlooked. The ordering of information in an explanation can affect how it is perceived. This study suggests that explanations should sometimes be generated online, especially for complex scenarios. Online explanation generation involves providing the minimum information needed to explain a specific part of a plan, intertwining explanation with plan execution. The problem definition is based on the model reconciliation setting from previous work, closely related to planning problems. The problem in this work is closely related to planning problems, defined as a tuple (F, A, I, G) using PDDL. Actions have preconditions, add and delete effects, with an initial and goal state. The robot's optimal plan is based on cost under M R, considering the human's model M H in the model reconciliation setting. The robot's plan must be optimal in the model reconciliation setting, considering both M R and M H. When the robot's behavior matches the human's expectations, the models are reconciled. Explanation generation involves updating M H to make the robot's plan fully explainable. A mapping function in BID6 converts planning problems into a feature space. Explanation generation in the context of model reconciliation involves converting planning problems into a feature space using a mapping function. An explanation is a set of unit feature changes that reconcile the human's expected plan with the robot's plan, making the cost difference smaller after model updates. A complete explanation is one that ensures the robot's plan is optimal in the human's model, with a minimal complete explanation defined as satisfying a certain cost criteria. Explanation generation in model reconciliation involves converting planning problems into a feature space using a mapping function. A complete explanation ensures the robot's plan is optimal in the human's model, with a minimal complete explanation defined as satisfying a certain cost criteria. Online explanation generation introduces a method to provide minimal information during plan execution to explain the part of the plan that is of interest and not explainable. Online explanation generation in model reconciliation involves providing sub-explanations (e k , t k ) during plan execution. The robot splits the explanation into parts based on human expectations, with three approaches discussed: Plan Prefix matching, Next Action matching, and any prefix matching. The planning process generates sub-explanations ({e k }) to explain the plan effectively. The planning process for online explanation generation involves generating sub-explanations to explain the plan effectively. Model changes must be considered to ensure that future changes do not render a mismatch in previously reconciled plan prefixes. This is achieved by searching for the largest set of model changes from M R to M H to maintain plan prefixes in M H. The planning process for online explanation generation involves generating sub-explanations to explain the plan effectively. To maintain plan prefixes in M H, a search is conducted from M R to M H to find the largest set of model changes. This process ensures that the plan prefix remains unchanged even after further sub-explanations. Our approach in online explanation generation involves starting the search for sub-explanations from the robot model, matching plan prefixes with the updated human model. This differs from previous approaches where the search starts from the human model. Despite the need to run the process multiple times, our method outperforms others in terms of computation by considering only a small set of changes at a time. The approach in online explanation generation involves starting the search for sub-explanations from the robot model, matching plan prefixes with the updated human model. The maximum state space model modification in the robot model reconciles the two models up to the current plan execution. The largest set of model changes to the robot model corresponds to minimum updates to the human model. The process ensures that the prefix of a plan using the corresponding model matches with that of the optimal plan up to a certain step. The next recursive step involves starting from a specific action and updating the human model accordingly to maintain compatibility with future steps. This approach guarantees that an optimal plan exists that satisfies the requirements without affecting the solution for online explanation generation. The recursive search algorithm for model space OEG is presented in Algorithm 1 for finding e k given E k\u22121. We use a recursive model reconciliation procedure on the model space to find the largest set of model changes that can satisfy constraints. The goal of explanation generation is to ensure that the robot and human plan have the same prefix at any step of plan execution. The goal of explanation generation is to ensure that the robot and human plan have the same prefix at any step of plan execution, with a focus on reconciling between M R and M H to match the very next action in the plan. This approach is motivated by the human's limited cognitive memory span, where the agent explains the next action that differs between the human plan and the generated plan. The explanation generation process focuses on reconciling differences between the human plan and the robot plan by comparing the very next action. The search procedure starts from M H \\M H for computational efficiency and only explains the immediate next action that does not match, maintaining the match between plan prefixes. This process is similar to minimally monotonically explanation (MME) in BID6 but must be executed multiple times for online explanation generation (OEG). The OEG search process is similar to MME in BID6 but must be executed multiple times due to its online nature. Algorithms combine search from M H and M R for better performance. OEG-PP assumes the robot has only one plan and aims to reconcile human and robot plans through model space search. Relaxing this assumption allows for multiple optimal plans, where the robot doesn't need to explain if a human plan matches its own up to the current action. The goal of OEG is to satisfy the human optimal plan. The OEG approach aims to reconcile human and robot plans by checking if the robot's plan prefix matches a prefix in the human's model. This is done by compiling the human's model into a new problem where the robot's plan prefix is always a prefix of the human's plan. If the cost of the human's optimal plan in this new domain model matches the cost before compilation, then there exists an optimal plan in the human's model that matches the prefix. Otherwise, an explanation is needed. The key is to ensure that a plan prefix is always satisfied in the compiled model by adding predicates to actions as effects and prerequisites. The search for e k involves a recursive model reconciliation process on the model space. The main difference in this approach is that the agent checks for a human optimal plan after each model update. The main difference in this approach is that after each model update, the agent checks for a human optimal plan that matches the robot's plan. This process continues until an optimal human plan exists that matches the robot's plan. The approach was evaluated for online explanation generation with human subjects and in simulation, comparing the results with the Minimally Complete Explanation (MCE) BID6 approach. The study evaluated an online explanation generation approach for human subjects and in simulation, comparing it with the Minimally Complete Explanation (MCE) BID6 approach. The goal was to see the differences in information needed and computation time between online explanation and MCE. The approach was tested on various problems in the rover and barman domains. The human subject study aimed to confirm the benefits of online explanation generation, hypothesizing that it would reduce mental workload and improve task performance. In the rover domain, the robot explores Mars to collect samples and communicate results. It must calibrate its camera, have empty storage for samples, and can only store one sample at a time. To take multiple samples, it must drop the current one. In the barman domain, the robot serves drinks using specific objects with constraints on grabbing items. The robot in the barman domain serves drinks using specific objects with constraints on grabbing items. Simulation results compare different approaches for explanations in the rover and barman domains, showing differences in the number of shared model features and total features between approaches. In the rover domain, the OEG-PP and OEG-NA explanations share more model features than MCE, due to OEG focusing on minimal information per time step. OEG-AP considers all optimal plans, showing an improvement in information but still has a distance from the human's plan. The distance between the robot's plan and the human's plan in terms of action distance is due to OEG-NA only considering the immediate next action. OEG-AP considers all optimal human plans but doesn't guarantee a match with the robot's plan. The plan distance gradually decreases during execution, leading to a smoother adjustment for M H and potentially reducing the human's mental workload. In our study, we evaluated the effect of different approaches for online explanation generation on human mental workload. We sorted model updates based on feature size and checked for consistency, backtracking when necessary. A human study compared three explanation generation approaches, including minimally complete explanation (MCE) and randomly breaking MCE during plan execution (MCE-R). The experiment was conducted using Amazon Mechanical Turk with 3D simulation. The experiment conducted on Amazon Mechanical Turk with 3D simulation involved human subjects acting as rover commanders on Mars. They were tasked with determining the rover's actions' validity using explanations provided by different approaches, including MCEs and MCE-R. Each subject had a 30-minute time limit to complete the task. The experiment involved human subjects acting as rover commanders on Mars, tasked with determining the rover's actions' validity using explanations provided by different approaches. Additional spatial puzzles were added to observe mental workload effects more clearly. Hidden information introduced differences between model reconciliation settings, requiring explanations to be provided. In the experiment, human subjects acted as rover commanders on Mars, determining the validity of the rover's actions using different explanation approaches. Hidden information in model reconciliation settings led to the need for explanations. The robot shared all information at the beginning in MCE setting, while information was broken up in MCE-R. Different online explanation generation approaches were used in OEG settings, intertwining explanation communication with plan execution. Subjects were asked to assess the robot's actions throughout the task. The study evaluated the efficiency of different explanation approaches by using the NASA Task Load Index (TLX) questionnaire. NASA TLX is a tool to assess workload in human-machine interface systems, measuring mental workload through various variables. The questionnaire calculates an overall mental workload score based on sub-scales such as mental demand, performance, effort, and frustration. Physical demand was not included in the experiment. The study used the NASA Task Load Index questionnaire to evaluate different explanation approaches. The questionnaire measures mental workload through variables like mental demand, performance, effort, and frustration. The experiment involved 150 human subjects on MTurk, with criteria for worker selection. After filtering out invalid responses, there were 94 valid responses. The subjects' age range was 18-70, with 29.8% female participants. The study examined how well subjects understood the robot's plan with different explanations. The study evaluated different explanation approaches using the NASA Task Load Index questionnaire. It involved 150 human subjects aged 18-70, with 29.8% being female. The distance between the robot's plan and the human's expected plan was computed to measure understanding. Overall, OEG approaches were found to reduce mental workload better than MCE approaches, as shown by NASA TLX measures. The OEG approaches were found to reduce mental workload better than MCE approaches, as indicated by NASA TLX measures. OEG approaches resulted in better performance in almost all NASA TLX measures, creating more temporal demand due to intertwining the explanation process with plan execution. The number of questionable actions was significantly lower in OEG approaches, indicating higher trust towards robots. OEG-AP had the least number of questionable actions and the highest accuracy in identifying correct actions. The study compared OEG approaches with MCE approaches in terms of mental workload, time analysis, and accuracy of secondary tasks. OEG approaches showed a significant reduction in mental workload and had lower questionable actions. OEG-AP had the highest accuracy and the least number of questionable actions. Time analysis showed varying completion times for different categories, with OEG-NA being the fastest. Overall, the novel OEG approach aimed to reduce the mental workload for human interpretation. The paper introduces a novel approach for explanation generation in human-robot interaction to reduce mental workload. It breaks down complex explanations into smaller parts and integrates them with plan execution. Three different approaches were provided, focusing on generating easily understandable explanations. Evaluation with simulation and human subjects showed improved task performance and reduced mental workload."
}