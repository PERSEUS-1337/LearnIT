{
    "title": "HJlfAo09KX",
    "content": "We study model recovery for data classification using a one-hidden-layer neural network. Under Gaussian inputs, the empirical risk function shows strong convexity and smoothness, allowing gradient descent to converge linearly to the ground truth with near-optimal sample complexity. This is the first global convergence guarantee for empirical risk minimization using cross entropy via gradient descent for learning one-hidden-layer neural networks. Neural networks have gained research interest for their success in practical domains like computer vision and artificial intelligence. Efforts are being made to understand the theoretical underpinnings behind their success, including which functions can be represented by deep neural networks and why they generalize well. Model recovery for data classification using one-hidden-layer neural networks is a key area of research, with a recent study providing a global convergence guarantee for empirical risk minimization using cross entropy via gradient descent. One important research area focuses on model recovery in neural networks to understand why they generalize well. Studies have looked at different types of data generations, such as regression problems with Gaussian inputs and weight vectors for neurons. Previous research has examined single-neuron models, one-hidden-layer multi-neuron networks, and two-layer feedforward networks. BID28, BID38, and BID19 studied different neural network models under various activations. The models considered single-neuron, one-hidden-layer multi-neuron, and two-layer feedforward networks. The studies focused on regression and classification problems using gradient descent over squared loss for parameter recovery. Previous research provided statistical guarantees for model recovery using squared loss. In previous studies, statistical guarantees were provided for model recovery using squared loss in neural network models. BID38 showed that the Hessian of the empirical loss function is positive definite in the local neighborhood of the ground truth, requiring fresh samples at each iteration for gradient descent to converge. On the other hand, studies like BID21 and BID28 established uniform geometry like strong convexity, eliminating the need for per-iteration resampling for gradient descent to have guaranteed linear convergence. However, these guarantees have only been shown for the squared loss function. This paper aims to develop a strong statistical guarantee without per-iteration resampling. This study aims to provide a strong statistical guarantee for recovery of one-hidden-layer neural networks using the cross entropy loss function, specifically for multi-neuron classification problems with sigmoid activations. The empirical risk function based on the cross entropy loss is shown to be uniformly strongly convex in a local neighborhood of the ground truth, with specific sample size requirements based on input dimension and number of neurons. The study provides a statistical guarantee for recovering one-hidden-layer neural networks with sigmoid activations using the cross entropy loss function. The gradient descent converges linearly to a critical point with a sample complexity of O(dK 5 log 2 d), and the recovery of W is up to certain statistical accuracy. The convergence rate is O(dK 9/2 log n/n) in the Frobenius norm, and computational complexity for -accuracy is O(ndK 2 log(1/ )). The tensor method in BID38 is used for initialization in the neighborhood. The proof in the current chunk introduces a new technique for analyzing the cross-entropy loss function in neural networks with sigmoid activations. It utilizes statistical information on geometric curvatures, gradients, and Hessians to guarantee uniform concentrations. The method also extends to the classification problem with squared loss, providing similar performance guarantees. The current chunk focuses on the theoretical and algorithmic aspects of learning shallow neural networks via nonconvex optimization. It highlights the relevance of parameter recovery in non-convex learning for signal processing problems. The statistical model for data generation allows for a focus on average-case performance, enabling global convergence of simple local search algorithms. The studies on one-hidden-layer network models are categorized into landscape analysis and model evaluation. The studies focus on properties enabling global convergence of local search algorithms for shallow neural networks. Landscape analysis shows no spurious local minima with large network size compared to data input. Model recovery examines spurious bad local minima even at the population level with multiple neurons. Characterizations for local Hessian in regression problems are provided for various activation functions. In the regression problem, the local Hessian's characteristics are analyzed for different activation functions. Studies show convergence properties of gradient descent with ReLU activation and bounded derivatives. Sample complexity requirements are discussed for linear convergence in both regression and classification tasks. The sample complexity for the classification problem using squared loss is O(d log 2 d) with sub-Gaussian inputs. The study analyzes the cross entropy loss function with a different form and focuses on model recovery classification under the multi-neuron case. Previous work on neural networks with different structures and loss functions is not directly comparable to this study. The paper discusses local geometry and convergence of gradient descent in neural networks. Sections cover problem formulation, results, initialization, numerical examples, and conclusions. Not directly comparable to previous work due to different networks and loss functions. Symbols and notations are defined throughout the paper. The paper discusses local geometry and convergence of gradient descent in neural networks. It describes the generative model for training data and the gradient descent algorithm for learning network weights. Training samples are drawn i.i.d., with a sigmoid activation function used in a one-hidden layer neural network model for classification. The paper discusses the classification setting in neural networks, using a one-hidden layer model with a sigmoid activation function. The goal is to estimate the weights W by minimizing the empirical risk function through gradient descent. To avoid local minima, a well-designed initialization scheme is implemented. The update rule for gradient descent is provided with a step size \u03b7. The gradient descent algorithm with a well-designed initialization scheme is described in detail in Section 4. The update rule is given as DISPLAYFORM0, where \u03b7 is the step size. The algorithm is summarized in Algorithm 1. Throughout the execution, the same set of training samples is used, unlike existing work such as BID38 which employs resampling. An important quantity regarding \u03c6(z) is introduced to capture the geometric properties of the loss function. The local strong convexity of the empirical risk function is characterized in a neighborhood of the ground truth W. The Hessian of the risk function is guaranteed to be positive definite with high probability for the classification model with sigmoid activation function. The Hessian of the empirical risk function is positive definite in the local neighborhood of W with high probability for the classification model with sigmoid activation function. Theorem 1 guarantees this property as long as certain conditions are met, and it applies to all column permutations of W. The bounds in Theorem 1 for the sigmoid activation depend on network dimension parameters (n and K), activation function, and ground truth. For a special case with orthonormal columns in W, Theorem 1 guarantees near-optimal sample complexity as long as n = \u2126(dK 5 log 2 d). The classification problem with quantized labels shows that W may not be a critical point of f n (W), but a unique local minimizer W n exists in the local neighborhood of W due to strong convexity of the empirical risk function. Theorem 2 guarantees the existence of a unique critical point W n in B(W , r) for the classification model with sigmoid activation function. Gradient descent converges linearly to W n with high probability, as long as certain conditions are met. The proof of Theorem 2 guarantees the existence of a critical point W n in B(W , r) that converges to W at a rate of O(K 9/4 d log n/n). Gradient descent converges linearly to W n at a linear rate if initialized in the basin of attraction. The computational complexity for achieving -accuracy is O(ndK 2 log (1/ )). The initialization method follows the tensor method proposed in BID38. The tensor method introduced in BID38 defines a product \u2297 for vectors in R^d and the identity matrix I. It involves defining P2 and P3, initializing W using Algorithm 2 with two major steps: estimating the direction of each column of W and reducing a third-order tensor to output estimates. The tensor method in BID38 involves non-orthogonal tensor decomposition on R3 to estimate siVi, where si is a random sign. Linear system of equations is used to approximate the magnitude of wi and si. Technical assumptions are made for the activation function in the classification problem, without requiring the homogeneous assumption. Assumptions are also made on the curvature of the activation function around the ground truth. The initialization algorithm guarantees accurate estimation of the direction and norm of W for the classification model under specific assumptions on the activation function and sample size. The proof involves showing the accuracy of estimating W's direction and norm separately. Our proof involves showing the accuracy of estimating the direction and norm of W for the classification model. We implement gradient descent to verify the strong convexity of the empirical risk function around W. Multiple random initializations converge to the same critical point Wn with the same training samples. Variance of the output of gradient descent is calculated to quantify the standard deviation of the estimator Wn. The output of the th run is denoted as w DISPLAYFORM0, quantifying the standard deviation of the estimator Wn under different initializations. An experiment is successful if SDn \u2264 10 \u22122. Gradient descent converges to the same local minima with high probability when the sample complexity is large enough. The statistical accuracy of the local minimizer for gradient descent is shown if initialized close to the ground truth. The average estimation error decreases as sample size increases in model recovery of a one-hidden-layer neural network. Cross entropy loss with gradient descent outperforms squared loss in classification problems. In this paper, the model recovery of a one-hidden-layer neural network using cross entropy loss in a multi-neuron classification problem is studied. The sample complexity for guaranteeing local strong convexity near the ground truth is characterized, ensuring gradient descent convergence to the ground truth with high probability. Future work may extend the analysis to different activation functions and network structures. The population loss function is denoted as DISPLAYFORM0, and the proof of Theorem 1 involves showing the smoothness and convexity properties of the Hessian \u2207 2 f (W). Lemmas 1, 2, and 3 establish these properties for both the population and empirical loss functions, leading to the conclusion that the Hessian of the population risk is smooth enough. The Hessian of the population risk function is shown to be smooth and locally strongly convex around W. Lemmas 1, 2, and 3 establish these properties for sigmoid activations, allowing for bounding of the Hessian in a neighborhood around the ground truth. The Hessian of the empirical loss function is also shown to be close to the population loss function in a uniform sense. The Hessian of the empirical loss function is close to the Hessian of the population loss function in a uniform sense. For sigmoid activations, there exists a constant C such that as long as n \u2265 C \u00b7 dK log dK, with probability at least 1 \u2212 d \u221210, certain conditions hold. The proof can be found in Appendix D.4. Combining Lemma 3 and Lemma 1 leads to Theorem 1, showing that f n (W) is strongly convex in a certain neighborhood, implying the existence of at most one critical point. Theorem 1 states there is at most one critical point in B(W, r). The proof of Theorem 2 involves showing the gradient concentrates around \u2207f(W) in B(W, r) and then proving the existence of a critical point Wn in B(W, r). Additionally, it is shown that Wn is close to W and gradient descent converges linearly to Wn with a properly chosen step size. Lemmas 3 and 4 are used to guarantee the uniqueness of the critical point in B(W, r) due to local strong convexity. The text discusses the existence of a critical point in B(W, r) due to local strong convexity. Lemmas 3 and 4 ensure uniqueness of this critical point. The proof involves showing the gradient concentrates around \u2207f(W) in B(W, r) and establishing the existence of a critical point Wn in B(W, r). Additionally, it is shown that Wn is close to W and gradient descent converges linearly to Wn with a properly chosen step size. The proof of gradient descent convergence to the local minimizer Wn involves two parts. Part (a) ensures accurate estimation of the direction of W, similar to previous arguments. Part (b) does not require a homogeneous condition for the activation function, based on a mild condition in Assumption 2. The proof details a tensor operation for matrices A, B, and C. The proof involves defining a tensor operation for matrices A, B, and C. It shows that for regression problems, estimation error is bounded using Bernstein inequality. This result also applies to classification problems with minor adjustments in the proof. In the classification problem, Bernstein inequality is applied to all neurons together, with bounded labels y i. The proof for estimating w i does not require homogeneous conditions on the activation function, but assumes a relaxed condition in Assumption 2. Quantity Q 1 is defined based on the first non-zero index l 1. The text discusses the estimation of w i using Quantity Q 1, which is defined based on the first non-zero index l 1. By solving an optimization problem, an estimate for w i can be obtained using the given equations and assumptions. The sign of \u03b2 i can help in estimating s i correctly. The text introduces definitions and results related to the norms of random variables, specifically the sub-gaussian and sub-exponential norms. These norms are crucial for the proofs discussed in the text. The sub-gaussian and sub-exponential norms of a random variable X are defined as X \u03c82 and X \u03c81 respectively. If X \u03c82 is upper bounded, then X is a sub-gaussian random variable. Calculations of the gradient and Hessian of E [ (W ; are provided, along with the evaluation of \u2206 j,l. The hessian block is concisely written as DISPLAYFORM6, with g j,l (W ) = \u03be j,l (W ) (p(W )(1\u2212p(W ))) 2 \u2208 R. The calculation of \u2206 j,l is shown as DISPLAYFORM8, leading to the final result DISPLAYFORM9. The text discusses calculations involving the variable W, including the calculation of \u2206 j,l and upper bounding E T 2 j,l,k. It also mentions the use of the Cauchy-Schwarz inequality and provides a lemma for the sigmoid activation function. The text concludes with the derivation of DISPLAYFORM15 and the relationship between e and DISPLAYFORM16. The text presents upper and lower bounds for the Hessian of the population risk at ground truth, and applies Lemma 1 to obtain a uniform bound in the neighborhood of W. It discusses inequalities involving \u03c6 and provides an upper bound for \u22072f(W). The conclusion is drawn that if DISPLAYFORM17 holds for a constant C, then certain conditions are met. The text discusses inequalities involving \u03c6 and provides upper bounds for \u22072f(W). It concludes that certain conditions are met if DISPLAYFORM17 holds for a constant C. The proof of Lemma 3 is adapted from a previous analysis, bounding terms P(A t), P(B t), and P(C t). The text provides upper bounds for terms P(A t), P(B t), and P(C t) separately. Lemma 7 states that G i \u03c81 is upper bounded by a constant C."
}