{
    "title": "BJeOioA9Y7",
    "content": "In this paper, a new approach called knowledge flow is developed to transfer knowledge from multiple deep nets (teachers) to a new deep net model (student). The teachers and student can have different structures and be trained on different tasks. The student becomes independent of the teachers after training, outperforming fine-tuning and other knowledge exchange methods in various learning tasks. Research communities have developed various deep net architectures for different tasks, with some architectures trained from scratch and others fine-tuned using structurally similar deep nets. In reinforcement learning, different approaches involving teachers have been explored, such as progressive neural nets, PathNet, 'Growing a Brain', and Actor-mimic models. In reinforcement learning, various techniques like 'Growing a Brain' BID30, Actor-mimic BID20, and Knowledge distillation BID9 are used to fine-tune neural networks or distill knowledge from large models. However, these methods have limitations such as computational intensity and reliance on a single pretrained model. Knowledge flow is a new approach that addresses the limitations of fine-tuning methods like 'Growing a Brain' BID30 and actor-mimic BID20 by allowing multiple teachers to transfer knowledge to a student during training. This method ensures the student becomes independent regardless of the number of teachers used, and the resulting student net size remains constant. It offers flexibility in choosing teacher models and is applicable to various tasks in reinforcement learning and fully-supervised training. In reinforcement learning, the goal is to maximize future rewards by finding a policy that guides decision-making. The asynchronous advantage actor-critic (A3C) formulation is used, where a policy mapping and value function are approximated by deep neural networks. The discount factor \u03b3 is considered in evaluating knowledge flow across tasks. The policy and value function in reinforcement learning are approximated by deep neural networks. The policy optimization involves a loss function based on negative log-likelihood and entropy regularization. The value function optimization commonly uses squared loss. The goal is to maximize future rewards by finding an optimal policy. The proposed framework, knowledge flow, transfers knowledge from multiple 'teachers' to a 'student' deep net during training. The student's weights gradually shift from relying heavily on teacher one to becoming independent. This process is illustrated on example deep nets in FIG0. The knowledge flow framework transfers knowledge from multiple teachers to a student deep net during training. The student's weights gradually shift, becoming independent as it is trained using parameters from pre-trained teacher models. Teacher representations are added to the student net through transformation and scaling, enhancing its learning process. The knowledge flow framework transfers knowledge from multiple teachers to a student deep net during training. Teacher representations are added to the student net through transformation and scaling, with normalized weights encoding which representations to trust at each layer. The student model should eventually perform well on the target task without relying on teachers, achieved by encouraging a high normalized weight on the student representation as training progresses. During early stages, the student heavily relies on teacher knowledge for better performance. During training, the student initially relies on teacher knowledge for better performance but gradually becomes more independent. Two additional loss functions are introduced to encourage this transition: dependency loss measures reliance on teachers, and KL loss ensures stable behavior as teacher influence decreases. These modifications help the student master tasks independently. The student's reliance on teacher influence decreases gradually during training. Additional loss functions, including dependency loss and KL loss, are introduced to encourage independence. Parameters \u03bb 1 and \u03bb 2 control the strength of teacher influence, with a low \u03bb 1 allowing the student to rely on teachers initially. The student's reliance on teacher influence decreases gradually during training by adjusting the weight for teacher layers. Despite differences in objectives, students can benefit from low-level knowledge transfer from teachers. Modifications to deep nets and loss functions dep and KL are used to decrease teacher influence in the student model. The student model gradually decreases reliance on teacher influence by adjusting weights for teacher layers. Candidate sets are defined for each layer in the student model, and normalized weights determine which representation to trust. Combined intermediate representations are obtained using matrices. The number of introduced matrices in the framework is limited. Not every student layer is linked to every teacher network layer. In practice, it is recommended to link one teacher layer to one or two student layers to avoid introducing too many matrices. Additional trainable parameters are introduced but not part of the resulting student network. In the final stage of training, the student becomes independent and no longer relies on additional parameters Q and w introduced in the framework. The influence of teachers is gradually decreased during training to encourage the student to become more independent. This is achieved by minimizing the dependence cost to increase weights for the student's layers. During the final stage of training, the student becomes independent by minimizing dependence cost to increase weights for its layers. To prevent degrading performance, a Kullback-Leibler regularizer is used to gradually decrease the influence of teachers and avoid rapid changes in the student's output distribution. In supervised learning, a Kullback-Leibler regularizer is used to yield good results. In reinforcement learning, a different approach is taken. Knowledge flow is evaluated on both reinforcement and supervised learning tasks, with results reported using only the student model to avoid influence from teacher nets. Knowledge flow is evaluated on reinforcement learning using Atari games, where the agent learns to predict actions based on rewards and input images from the environment. The agent chooses an action every four frames, with the last action repeated on skipped frames. The A3C model architecture is used for all teacher and student models. The A3C model architecture is used for all teacher and student models in reinforcement learning tasks with Atari games. The model has three hidden layers with specific filter sizes and strides. Different hyper-parameter settings are used compared to BID17, including the use of Adam with shared statistics for better training results. The learning rate is gradually decreased to zero for all experiments. The learning rate is set to 10 \u22124 and gradually decreased to zero for all experiments. To select \u03bb 1 and \u03bb 2 in our framework, we randomly sample values and repeat each experiment 25 times with different random seeds. Evaluation metrics involve playing each game for 30 episodes and following the 'no-op' procedure. In experiments evaluating a transfer reinforcement learning framework, results show that a student model trained with one teacher outperforms PathNet in 11 out of 14 experiments. Compared to PNN, the student model with two teachers has fewer parameters but achieves higher scores in five out of seven experiments, demonstrating effective knowledge transfer. The results show that knowledge flow effectively transfers knowledge from teachers to students. Increasing the number of teachers from one to two significantly improves student performance across all experiments. Different combinations of environment/teacher settings were tested, with positive results compared to other methods like PathNet and progressive neural network. In experiments, knowledge flow with expert teachers outperforms A3C baseline, showing successful knowledge transfer. Knowledge flow with non-expert teachers also outperforms fine-tuning due to the student model learning from multiple teachers and avoiding negative impacts from insufficiently pretrained teachers. In knowledge flow, the student benefits from learning from multiple teachers, avoiding negative impacts from insufficiently pretrained teachers. Training curves are shown in FIG5, with the student model achieving scores ten times larger than learning without a teacher and fine-tuning from a teacher. The student model achieves scores ten times larger than learning without a teacher and fine-tuning from a teacher on various image classification benchmarks like CIFAR-10, CIFAR-100, STL-10, and EM-NIST. Evaluation metrics include reporting top-1 error rate on the test set of each dataset. CIFAR-10 and CIFAR-100 datasets consist of colored images of size 32 \u00d7 32 with 10 and 100 classes respectively. Training and test sets contain 50,000 and 10,000 images. All experiments are performed with standard data augmentation using Densenet. The experiments involve training teachers on CIFAR-10, CIFAR-100, and SVHN datasets, followed by training a student model using different teacher combinations. Fine-tuning from CIFAR-100 improves performance by 4% over the baseline, while fine-tuning from SVHN performs worse. Knowledge flow improves by 13% over the baseline when exposed to both good and inadequate teachers. The study demonstrates that knowledge flow can benefit from exposure to both good and inadequate teachers, improving performance by 13% over the baseline. Results on the CIFAR-100 dataset show similar trends, with additional findings detailed in the appendix. The approach contrasts with PathNet BID6 by utilizing multiple pre-trained teacher nets to avoid catastrophic forgetting. Progressive Net BID23 introduces lateral connections to previously learned features to avoid catastrophic forgetting. The discussed method also uses lateral connections but ensures the student's independence during training. Distral BID26 combines distillation and transfer learning for joint training of multiple tasks, promoting consistency between policies. In contrast, knowledge flow focuses on a single task while multi-task learning addresses multiple tasks simultaneously. Knowledge flow is a single-task learning framework that leverages information from multiple teachers to help a student learn a new task. It differs from multi-task learning, where information from different tasks is shared to boost performance. Other related work includes actor-mimic, policy distillation, domain adaptation, and lifelong learning. A general knowledge flow approach allows training a deep net from any number of teachers, with results shown for reinforcement learning. The study presents a general knowledge flow approach for training deep nets from multiple teachers, showing improvements in reinforcement and supervised learning tasks. They aim to optimize teacher selection and swapping during student training. Experiments on MNIST, CIFAR-100, and ImageNet demonstrate the effectiveness of distilling knowledge from larger to smaller models. The student model, with hidden layers of 800 units, follows the structure of the teacher model but with halved output channels. The distilled student model's test error is summarized in TAB4, showing consistently better performance than KD. The 'EMNIST Letters' dataset consists of 26 balanced classes of handwritten letters, with training and test sets containing 124,800 and 20,800 images respectively. The 'EMNIST Letters' dataset has 26 balanced classes of letters, with 124,800 training and 20,800 test images. The 'EMNIST Digits' dataset has 10 classes with 240,000 training and 40,000 test images. The study compares student learning with expert, semi-expert, and non-expert teachers, showing improved results compared to baseline and fine-tuning methods. In our experiment, student learning with expert, semi-expert, and non-expert teachers showed better performance compared to baseline and fine-tuning methods. The STL-10 dataset consists of colored images with 10 classes, and we used 5,000 labeled images for training. Teachers were trained on CIFAR-10 and CIFAR-100, and results were compared to fine-tuning and baseline methods. Fine-tuning a model using weights pretrained on CIFAR-10 and CIFAR-100 reduces test errors by more than 10%. Student model training in the framework further reduces test error by 3%. Results are obtained using fewer data and may not be directly comparable to other approaches. In Fig. 5, accuracy over training epochs is shown, comparing to Distral BID26, a multi-task reinforcement learning framework. Experiments on Atari games involve three tasks, with teachers provided for task 2 and task 3. Our model is trained for 40M steps, while Distral is trained for 120M steps. Results show that Distral is suboptimal for very different target tasks due to its multi-task approach. Our framework can decrease a teacher's influence to reduce negative transfer. In the C10 experiment, the normalized weight (p w) for teachers and the student is plotted, showing that the C100 teacher has a higher p w value than the SVHN teacher. Ablation study confirms the student benefits from teacher knowledge, even when using untrained teacher models. The ablation study confirmed that students benefit from teacher knowledge, even with untrained teacher models. Learning with knowledgeable teachers resulted in higher rewards compared to untrained teachers in different environments and settings. The KL term helps maintain the student's output distribution when the teachers' influence decreases. The KL term prevents drastic changes in student's output distribution when teachers' influence decreases. Ablation study with KL coefficient set to zero shows performance drop without KL term. Training with KL term achieves higher rewards compared to training without it. Additional experiments use different architectures for teacher models as suggested by a reviewer. In additional experiments, the teacher model is based on BID16 with 3 convolutional layers and a hidden fully connected layer, while the student model is based on BID17 with 2 convolutional layers and a hidden fully connected layer. The models are linked in specific ways for the experiments. In the experiment, teachers with different architectures are linked to the student's layers for the target task of KungFu Master. Results show that learning with teachers of varying architectures can achieve similar performance as those with the same architecture, with higher rewards enabled through knowledge flow. Using an average network to obtain \u03b8 old achieves similar performance as using a single model, as shown in FIG0. For the target task of Boxing with a Riverraid expert teacher, the average reward is 96.2 at the end of training. Using an average network achieves similar performance as using a single model in the target task of Boxing with a Riverraid expert teacher. Various techniques for knowledge transfer have been explored, such as fine-tuning, PathNet, 'Growing a Brain', actor-mimic, and learning without forgetting. PathNet allows multiple agents to train the same deep net while reusing parameters and avoiding catastrophic forgetting. The discussed method introduces scaling with normalized weights to ensure independence of the student during training, addressing a limitation in previous methods. Distral, a combination of 'distill & transfer learning', involves joint training of multiple tasks with a shared 'distilled' policy encoding common behavior. Multiple teacher nets are considered for transfer learning to avoid catastrophic forgetting. The BID26 method involves joint training of multiple tasks with a shared 'distilled' policy to encourage consistency between policies. In contrast, Knowledge distillation BID9 distills information from a larger deep net into a smaller one, assuming both nets are trained on the same dataset. Our technique enables knowledge transfer between different domains, while Actor-mimic BID20 helps agents learn multiple tasks simultaneously. It uses a combination of feature regression and cross entropy loss to guide the learning process. Learning without forgetting BID13 allows adding new tasks without losing original capabilities by utilizing data only from the new task. To add a new task to a deep net without losing original capabilities, data from the new task is used while retaining old capabilities by recording the old network's output on the new data. This technique contrasts with transferring knowledge from teacher networks more explicitly."
}