{
    "title": "BJInEZsTb",
    "content": "In this paper, a deep autoencoder network is introduced for studying representation learning and generative modeling of geometric data represented as point clouds. The learned representations outperform the state of the art in 3D recognition tasks and enable shape editing applications. Different generative models, including GANs and Gaussian mixture models, are studied, with GMMs trained in the latent space of the autoencoders producing samples of the best fidelity and diversity. The paper introduces a deep autoencoder network for generative modeling of geometric data represented as point clouds. It proposes measures of fidelity and diversity for evaluating generative models. Different encodings like view-based projections and volumetric grids are discussed, highlighting the limitations in semantics. These raw representations are not ideal for classic statistical methods in generative modeling. Recent advances in deep learning, such as autoencoders and Generative Adversarial Networks, offer a data-driven approach for designing new objects without the need for complex parametric models. Deep learning tools can learn complex data representations and generate realistic samples from underlying distributions, eliminating the manual feature crafting process. Deep learning architectures for 3D point clouds have emerged in the literature, focusing on their expressive surface geometry representation. Point clouds are commonly generated by range-scanning devices like the Kinect and iPhone. Only a few deep architectures, like PointNet, have been developed for point clouds, successfully handling classification and segmentation tasks. Deep architectures for 3D point clouds have gained attention in the literature, with PointNet BID17 addressing classification and segmentation tasks. Generative models for point clouds are a focus, with GANs BID10 introducing challenges in training and evaluation. The text discusses the evaluation of generative models for point clouds, focusing on fidelity and coverage. It introduces a new AE architecture for learning compact representations with high reconstruction quality and generative models capable of generating point clouds similar to the training data. The text introduces generative models for point clouds, emphasizing fidelity and coverage. It proposes a workflow involving training an AE with a compact bottleneck layer, followed by training a GAN in the fixed latent representation. Latent GANs are easier to train and achieve superior reconstruction. Multi-class GANs are also discussed. The text discusses the performance of GMMs trained in the latent space of fixed AEs and the effectiveness of multi-class GANs. It also introduces new metrics for evaluating generative models, such as fidelity and coverage metrics based on optimal matching between generated samples and a test set. The paper discusses the use of autoencoders (AE) for generating point clouds and introduces evaluation metrics for the models. Autoencoders aim to reproduce their input by learning a low-dimensional representation in a bottleneck layer. The Encoder compresses data into a latent representation, while the Decoder reconstructs the data. The models are evaluated quantitatively and qualitatively in Section 4. The bottleneck layer in autoencoders learns a low-dimensional representation for the dataset. Generative Adversarial Networks (GANs) are state-of-the-art models where a generator synthesizes samples to mimic real data, while a discriminator distinguishes between real and synthesized samples. The parameters for the discriminator and generator networks are denoted as \u03b8 (D) and \u03b8 (G) respectively. Additionally, an improved Wasserstein GAN formulation is used. Point clouds pose unique challenges for network architecture due to their lack of grid-like structure, making them harder to encode than images or voxel grids. Recent work bypasses this issue by avoiding 2D convolutions and addressing the unordered nature of point clouds. The improved Wasserstein GAN formulation is utilized for stability during training. Point clouds are unordered, making comparisons between them complex. Two metrics for comparing unordered point sets are Earth Mover's distance (EMD) and Chamfer distance (CD). EMD transforms one set to another, while CD measures the difference between point sets. The Earth Mover's distance (EMD) measures the difference between two point sets by transforming one set to another using a bijection. It is differentiable almost everywhere. On the other hand, the Chamfer distance (CD) calculates the squared distance between each point in one set to its nearest neighbor in the other set, making it more computationally efficient. These metrics are used to evaluate representations and generative models by comparing reconstructed or synthesized point clouds to their ground truth counterparts. To measure the faithfulness and diversity of a generative model and potential mode-collapse, metrics like Coverage, COV-CD, COV-EMD, MMD-CD, and MMD-EMD are used. Coverage measures how well a point-cloud distribution matches a ground truth distribution, while MMD captures fidelity by matching every point cloud with the minimum distance. The text discusses measuring the faithfulness and diversity of generative models using metrics like Coverage, COV-CD, COV-EMD, MMD-CD, and MMD-EMD. MMD measures distances in pairwise matchings to correlate with the realism of elements in point clouds. Jensen-Shannon Divergence is used to compare marginal distributions in 3D space. The text also describes autoencoder and GAN architectures tailored to point-cloud data. Our approach involves designing autoencoder and GAN architectures specifically for point clouds. The autoencoder encodes each point independently using 1-D convolutional layers, resulting in a k-dimensional vector. This vector serves as the basis for our generative models, including a GAN tailored to point-cloud data and a simpler model based on Gaussian Mixtures. The autoencoder utilizes 1-D conv layers with ReLU and batch-norm layers, leading to a k-dimensional latent vector. The decoder consists of fully connected layers to generate a 2048 \u00d7 3 output. Two distinct AE models, AE-EMD and AE-CD, are explored using EMD-distance and Chamfer-Distance as structural losses. The appropriate latent-space size is determined by training 8 AEs with different bottleneck sizes and evaluating under the two losses. The generative model operates on raw point clouds of a single object class, using a GAN with a discriminator architecture similar to the autoencoder. The generator takes a 128-dimensional noise vector and maps it to a 2048 \u00d7 3 output through 5 FC-ReLU layers. In the l-GAN model, a pre-trained autoencoder is used to process the input data before the generator and discriminator operate on the bottleneck variable. The architecture is simpler compared to the r-GAN, with shallow designs producing realistic results. Additionally, Gaussian Mixture Models are trained alongside the l-GANs. The study utilizes a pre-trained autoencoder in the l-GAN model for processing input data, leading to realistic results. Gaussian Mixture Models are also trained on the latent spaces learned by the autoencoders, which can be used as point-cloud generators. Shapes from the ShapeNet repository are reconstructed using class-specific AEs, with models trained per class. The study uses a pre-trained autoencoder in the l-GAN model to process input data and generate realistic results. Class-specific AEs are trained on shapes from the ShapeNet repository, with models split into training/testing/validation sets. The performance of the latent features computed by the AE is evaluated using linear models on supervised datasets. In the experiment, a larger bottleneck of 512 was used, along with an increased number of neurons and batch-norm applied to the decoder. Features for an input 3D shape are obtained by feeding its point-cloud forward to the network and extracting a 512-dimensional bottleneck layer vector. A linear classification SVM trained on ModelNet BID32 is used on this feature. The 512-dimensional feature is compared to the previous state of the art BID31, showing it to be more intuitive and parsimonious. Different losses are evaluated on ModelNet10 and ModelNet40, with CD producing better results when variation within the collection increases. The experiment used a larger bottleneck of 512 and increased neurons with batch-norm in the decoder. Features for a 3D shape are obtained by feeding its point-cloud to the network and extracting a 512-dimensional vector. CD produces better results with increased variation within the collection. The experiment also demonstrates the domain-robustness of learned features through qualitative evaluation. The experiment demonstrated the domain-robustness of learned features through qualitative evaluation. The learned latent representation enables shape editing applications like interpolations, part editing, and analogies. Generalization ability is shown through reconstruction of unseen shapes and comparable quality on training vs. test splits. Five generative models were trained on point-cloud data of the chair category with 128-dimensional bottleneck AEs referred to as AE-CD and AE-EMD. The study trained five generative models on point-cloud data of the chair category using 128-dimensional bottleneck AEs, referred to as AE-CD and AE-EMD. The models included l-GANs in the AE-CD and AE-EMD latent spaces, a l-GAN with Wasserstein objective in the AE-EMD space, GMMs, and an r-GAN directly on the point cloud data. Model selection was based on how well the synthetic results matched the ground-truth distribution. The study trained five generative models on point-cloud data of the chair category using 128-dimensional bottleneck AEs. Model selection was based on how well the synthetic results matched the ground-truth distribution, using metrics like JSD and MMD-CD. The optimal number of Gaussian components for GMM was found to be 32, with full covariance matrices performing better than diagonal ones. The study compared generative models using different selection criteria and found that full covariance matrices performed better than diagonal ones. The optimal number of Gaussians for GMM was 40, and models were evaluated based on their ability to generate synthetic samples resembling the ground truth distribution. In experiments, synthetic point clouds were generated using models to mimic train and test data distributions. Results were compared against the ground truth distribution, with average classification probabilities reported for samples recognized as chairs. Evaluations were conducted on 5 generators using minimal JSD selection on validation data, with GMM-32-F representing a GMM with 32 Gaussian components with full covariances. In experiments, synthetic point clouds were generated using models to mimic train and test data distributions. Results were compared against the ground truth distribution, with average classification probabilities reported for samples recognized as chairs. Evaluations were conducted on 5 generators using minimal JSD selection on validation data, with GMM-32-F representing a GMM with 32 Gaussian components with full covariances. The experiment repeated with three pseudo-random seeds and average measurements reported in TAB10 for various comparison metrics. Training a Gaussian mixture model in the latent space of the EMD-based AE yielded the best results in terms of fidelity and coverage. The achieved fidelity and coverage were close to the reconstruction baseline, with comparable MMD-EMD values between AE-EMD and GMMs. In experiments, synthetic point clouds were generated to mimic train and test data distributions. The models' generalization ability was established by comparing performance on training vs. testing splits. Synthetic datasets were generated for testing and validation splits, with sizes three times bigger than the ground truth dataset to reduce sampling bias when measuring MMD or Coverage statistics. The MMD-CD distance to the test set was noted in TAB10 for comparison metrics. The MMD-CD distance to the test set was relatively small for the r-GANs in TAB10, but qualitative inspection of the results showed otherwise. The inadequacy of the chamfer distance to distinguish pathological cases was highlighted with examples in Fig. 3, showcasing differences between r-GANs and l-GANs in generating synthetic point clouds. The chamfer distance metric fails to accurately assess the quality of r-GAN results due to the concentration of points in specific areas, leading to misleading comparisons with ground truth. This limitation highlights the need for a more comprehensive evaluation metric. The CD metric is limited in assessing r-GAN results due to its \"blindness\" to partial matches between shapes, resulting in inflated coverage metrics compared to EMD. EMD promotes one-to-one mapping and correlates more strongly with visual quality, penalizing r-GAN in terms of MMD and coverage. Training trends show r-GAN struggles to provide good coverage of the test set regardless of the metric used. The l-GAN (AE-CD) performs better in terms of fidelity with fewer epochs, but its coverage remains low due to the CD promoting unnatural topologies. Switching to an EMD-based AE (l-GAN, AE-EMD) results in a dramatic improvement in coverage and fidelity, despite both l-GANs suffering from mode collapse. Switching to a latent WGAN largely eliminates mode collapse issues in GANs on point-cloud data, improving coverage and fidelity. Comparisons to voxel-based methods show promising results in terms of JSD on the training set of the chair category. The authors converted voxel grid output into a point-set with 2048 points using farthest-point-sampling. They isolated the largest connected component from the isosurface and compared results with BID31. The r-GAN outperformed BID31 in diversity and realism, while l-GANs performed even better with less training time. The l-GAN outperforms the r-GAN in generating point clouds with higher quality and less noise, showcasing the advantage of its smaller architecture and dimensionality. The synthetic results from the l-GAN and a 32-component GMM trained on AE-EMD latent space demonstrate the strength of the learned representation, allowing the simple GMM model to perform well. The l-GAN produces clearer results with less noise compared to the r-GAN, showing the benefit of using a good structural loss on the pre-trained AE. Synthetic point clouds were generated by l-GAN and a 32-component GMM trained on AE-EMD latent space. Experiments were conducted with an AE-EMD trained on a mixed set of point clouds from 5 categories, achieving good results. The multi-class AE and class-specific AEs were trained for different epochs and evaluated using MMD-CD measurements. The l-WGANs based on the multi-class AE performed similarly to the dedicated class-specifically trained ones, with minimal sacrifice in visual quality. The limitations of the models include failure cases where chairs with rare geometries are not faithfully decoded, missing high-frequency geometric details, and struggling to create realistic shapes for certain classes. Designing more robust raw-GANs for point clouds is suggested for future work. Designing more robust raw-GANs for point clouds is an interesting avenue for future work. Training Gaussian mixture models in the latent space of an autoencoder is closely related to VAEs. One issue with VAEs is over-regularization, which can affect reconstruction quality. Fixing the AE before training generative models yields good results, with novel architectures for 3D point-cloud representation learning and generation showing good generalization to unseen data. The study explores 3D point-cloud representation learning and generation, with generative models producing faithful samples without memorizing examples. A Gaussian mixture model trained in the fixed latent space of an autoencoder showed promising results. The encoder used in the experiments had specific filter configurations, and batch normalization was applied. In experiments, an encoder with varying filter sizes and a decoder with specific neuron configurations were used. Batch normalization was applied, and online data augmentation was performed. Different autoencoder setups did not show significant advantages over the \"vanilla\" architecture. Adding drop-out layers resulted in worse reconstructions. The discriminator in the r-GAN model consists of 1D-convolutions with specific filter sizes and leaky-ReLU activation. The generator includes FC-ReLU layers with varying neuron configurations. The model was trained using Adam optimizer with specific hyperparameters and a noise vector drawn from a Gaussian distribution. The discriminator in the r-GAN model consists of 1D-convolutions with filter sizes and leaky-ReLU activation, while the generator includes FC-ReLU layers with varying neuron configurations. The noise vector for the generator is drawn from a spherical Gaussian distribution. In the l-Wasserstein-GAN, a gradient penalty regularizer is used with specific training parameters, and a linear SVM classifier is employed for classification experiments. The training parameters of SVMs for the Structural Loss ModelNet40 and ModelNet10 datasets are shown in Table 5, including C-penalty, intercept loss, and optimization parameters. The reconstruction quality of the autoencoders (CD and EMD-based) is compared using JSD on the training and test datasets, indicating generalization ability. The learned embedding is used for shape editing applications. The autoencoders show good generalization ability in shape editing applications across different object classes. Using the learned embedding, vector arithmetic in the latent space allows for editing parts in point clouds, such as tuning car appearance, adding armrests to chairs, and removing handles from mugs. Shape annotations are used as guidance to modify shapes within object categories. The latent representation in autoencoders allows for editing parts in point clouds, like adding armrests to chairs and removing handles from mugs. By interpolating between latent representations, structural differences between object sub-categories can be modeled. The latent representation in autoencoders enables morphing between shapes by interpolating between shapes and finding analogous shapes through linear manipulations and nearest neighbor searching in the latent space. The text discusses finding shape analogies through linear manipulations and nearest-neighbor searching in the latent space using a combination of point-cloud generators and voxel-based autoencoders. The approach involves adding a difference vector between shapes and searching for the nearest neighbor in the latent space. The method is demonstrated with images from meshes used to derive point-clouds for visualization. In experiments, a full-GMM model with 32 centers was used on ShapeNet's chair class, comparing with point-cloud generators. Results show latent AE-based GMM models outperform voxel-based GANs significantly. See Table 7 for quantitative results. The latent representation (voxel GMM) provides a significant improvement over raw voxel GAN architecture. Performance of 64^3 voxel-based GMM is comparable to 32^3 resolution, indicating fidelity is not affected by high-frequency details. Point-cloud-based models outperform voxel-based models in fidelity to ground truth, as measured by MMD. The coverage boost of voxel-based latent-space models compared to MMD is likely due to how the coverage metric is computed. The voxel-based latent-space models show a boost in coverage compared to MMD due to the way the coverage metric is computed, matching all generated shapes against the ground truth regardless of quality. The histogram analysis indicates poor quality matchings in voxel-based models, mostly covering very poor quality partial shapes. The voxel-based latent-space models exhibit improved coverage compared to MMD, as confirmed by the point-cloud output. The covering mainly consists of poor quality partial shapes. The models use GMMs with full covariances and different dimensional latent codes. The voxel-based AEs are fully-convolutional with specific layer parameters. The voxel-based AE models use specific layer parameters for the encoders and decoders, with a stride of 2 for most layers. The models are trained for 100 epochs with Adam optimizer and binary cross-entropy loss. Reconstruction quality is compared to a state-of-the-art method on the ShapeNetCars dataset. Our dense voxel-based AE is compared to the state-of-the-art method BID28 on the ShapeNetCars dataset. Reconstruction quality is measured by the intersection-over-union between input and synthesized voxel grids. The GMM-generator is evaluated against a model that memorizes training data for the chair class, with coverage/fidelity slightly lower than expected. The generative models show slightly lower coverage/fidelity compared to memorization, indicating good validity of metrics. Learning representation enables compact data representation and novel shape generation. Despite some mode collapse, the generative models achieve excellent fidelity. Further comparisons with BID32 are provided for major ShapeNet classes in Tables 10, 11, 12. The text discusses comparisons between generative models and BID32 for major ShapeNet classes, showing excellent fidelity. Generalization error of various GAN models is also analyzed using JSD and MMD-CD metrics. The text discusses comparisons between generative models and BID32 for major ShapeNet classes, showing excellent fidelity. Evaluation of GMM models with varying Gaussians and covariance types trained on latent space. Full covariance models achieve smaller JSD than diagonal covariance models. 30 or more clusters are sufficient for minimal JSD. Evaluation of five generators on chair data using minimal MMD-CD selected epochs/models. Evaluation of five generators on chair data using minimal MMD-CD selected epochs/models. The models were evaluated based on synthetic point-clouds matching the ground truth dataset in terms of MMD-CD, showing consistent quality across different evaluation metrics."
}