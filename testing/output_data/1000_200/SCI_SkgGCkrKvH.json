{
    "title": "SkgGCkrKvH",
    "content": "Decentralized training of deep learning models is crucial for data privacy and on-device learning. Communication compression is proposed to overcome network bandwidth limitations. Choco-SGD achieves linear speedup with high compression ratios on non-convex functions and non-IID training data. The algorithm's practical performance is demonstrated in training models over decentralized user devices and in datacenters. Distributed machine learning has enabled successful applications in research and industry, offering computational scalability. Decentralized training using distributed optimization algorithms has enabled successful applications in research and industry. These methods offer computational scalability and data-locality, with recent theoretical results showing decentralized schemes can be as efficient as centralized approaches. Gradient compression techniques have been proposed to reduce data sent over communication links in the network. Decentralized training using distributed optimization algorithms has enabled successful applications in research and industry. Gradient compression techniques have been proposed (Tang et al., 2018; Wen et al., 2017; Lin et al., 2018b; Wangni et al., 2018; Stich et al., 2018) to reduce data sent over communication links in the network. CHOCO-SGD, introduced for convex problems (Koloskova et al., 2019), overcomes constraints of previous algorithms for deep neural network training. Evaluation focuses on generalization performance on standard machine learning benchmarks. In this study, CHOCO-SGD is evaluated for training in challenging peer-to-peer and datacenter settings, showing speed-ups over decentralized baselines with less communication overhead. The decentralized communication patterns in datacenter settings allow for better scalability, improving time-to-accuracy on large tasks like ImageNet training. Communication efficient CHOCO-SGD improves time-to-accuracy on large tasks like ImageNet training. Decentralized algorithms face challenges scaling to a larger number of nodes and often do not reach the same performance as centralized schemes. Reporting these findings can spur further research on decentralized training schemes. CHOCO-SGD converges at a rate of O 1 / \u221a nT + n /(\u03c1 4 \u03b4 2 T ) on non-convex smooth functions, highlighting its theoretical advancements. CHOCO-SGD with momentum is analyzed for practical performance in on-device and datacenter training scenarios. The decentralized schemes face challenges when scaling to a larger number of nodes. In this paper, the performance of decentralized learning schemes when scaling to larger node numbers is systematically investigated. Various methods for training in communication restricted settings are discussed, including decentralized schemes, gradient compression, asynchronous methods, and multiple local SGD steps before averaging. This research focuses on learning over decentralized data, building on the federated learning literature for centralized algorithms. The paper advocates for combining decentralized SGD schemes with gradient compression, focusing on gossip averaging approaches. Results show convergence rates similar to centralized mini-batch SGD, with the spectral gap affecting smaller terms. Similar results have been observed for related schemes involving quantization. Theoretical guarantees for communication compression with quantization have been established for schemes with unbiased and biased compression. Schemes with error correction show the best practical performance and theoretical guarantees. Proximal updates and variance reduction have also been studied in combination with quantized updates. Decentralized optimization with quantization has shown that gossip averaging can diverge. The CHOCO-SGD algorithm can handle high compression rates in decentralized optimization, unlike gossip averaging which can diverge in the presence of quantization noise. Other adaptive schemes exist with varying compression accuracy and communication costs for convergence. Tang et al. proposed DCD and ECD algorithms for deep learning applications with constant compression ratios. The CHOCO-SGD algorithm can handle high compression rates in decentralized optimization, achieving higher test accuracy compared to DeepSqueeze. It has been introduced for non-convex functions, showing a convergence rate of \u03b4 > 0. In this section, the decentralized optimization problem is formally introduced along with compression operators and the CHOCO-SGD algorithm. The optimization problems are distributed across n nodes with local distributions for sampling data. Communication is limited to local neighbors defined by a weighted graph. Positive weights are assigned to edges based on local node degrees. Compression operators are used in decentralized optimization on weighted graphs, with weights based on local node degrees. The operators aim to transmit compressed messages, supporting a larger class of compression schemes compared to quantization operators. Specific compression schemes are discussed in Section 5. Compression operators in decentralized optimization on weighted graphs support a larger class of compression schemes compared to quantization operators. CHOCO-SGD algorithm involves private variables updated by stochastic gradient and gossip averaging steps, preserving averages of iterates even with quantization noise. Nodes communicate with neighbors using compressed updates, making variables publicly available. The CHOCO-SGD algorithm involves private variables updated by stochastic gradient and gossip averaging steps on weighted graphs. Nodes communicate with neighbors using compressed updates, making variables publicly available. The communication and gradient computation can be executed in parallel, with each node storing at most 3 vectors. The analysis is extended to non-convex problems with technical assumptions on bounded variance of stochastic gradients. The CHOCO-SGD algorithm involves private variables updated by stochastic gradient and gossip averaging steps on weighted graphs. The algorithm converges asymptotically with a linear speed-up compared to SGD on a single node. Experimental comparisons with relevant baselines are conducted using commonly used compression operators, leveraging momentum in all implemented algorithms. The CHOCO-SGD algorithm utilizes compression operators and momentum in all implemented algorithms. The momentum version of CHOCO-SGD is presented as Algorithm 2, incorporating weight decay factor \u03bb and momentum factor \u03b2. Experimental setup involves a ring topology with 8 nodes training ResNet20 on the Cifar10 dataset, implementing DCD, ECD, and DeepSqueeze with momentum. In the experimental setup, various compression schemes are applied to ResNet20 layers separately. The learning rate is fine-tuned and warmed up gradually for different algorithms, with decay at specific epochs and training stopping at 300 epochs. Top-1 test accuracy is evaluated on each node individually, and the average is reported. Hyper-parameter tuning details can be found in Appendix F. Compression schemes are applied to every layer of ResNet20 separately, with evaluation of top-1 test accuracy on each node over the dataset. Two unbiased schemes include quantization and random sparsification, while biased schemes involve selecting weights based on magnitude or compressing to sign scaled by vector norm. Detailed scheme definitions can be found in Appendix C. DCD and ECD analysis is limited to unbiased schemes. Results from the experiments show that unbiased compression schemes like ECD and DCD perform well only at low compression ratios, sometimes diverging at high ratios. This aligns with previous theoretical and experimental findings. Additionally, biased compression schemes like CHOCO-SGD and DeepSqueeze have only been studied for biased schemes, but unbiased schemes can be scaled down to meet specifications for the experiments. The performance of DCD with biased top sparsification is better than with unbiased random counterpart, despite lacking theoretical support. CHOCO-SGD shows good generalization in all scenarios with minimal accuracy drop. Sign compression achieves high accuracy with significantly fewer bits per weight. Decentralized real-world scenarios pose challenges for centralized methods in training machine learning models. Decentralized scenarios in machine learning involve limited communication bandwidth, unknown network topology, and privacy concerns. Training data is split between nodes without shuffling, ensuring privacy on each device. This setting is motivated by applications like sensor networks or hospitals, where each device has access to local data and the number of connected devices is large. Scaling properties of CHOCO-SGD with sign compression are studied for decentralized deep learning on the Cifar10 dataset. Centralized baseline comparison involves all nodes routing updates to a central coordinator. CHOCO-SGD with sign compression, decentralized SGD without compression, and centralized SGD without compression are compared. Training is done on 4, 16, 36, and 64 nodes to analyze scaling. In studying the scaling properties of CHOCO-SGD, training was conducted on 4, 16, 36, and 64 nodes using decentralized algorithms on different topologies. Results showed that CentralizedSGD outperformed CHOCO-SGD due to graph topology and communication compression influences. The influence of graph topology and communication compression on training performance was observed in decentralized algorithms. Despite increasing the number of epochs, the gap between centralized and decentralized algorithms could not be fully closed. The focus in real decentralized scenarios is on reducing communication costs rather than minimizing epochs. Testing accuracy was compared with a fixed number of transmitted bits, showing CHOCO-SGD performance. Experiments on a Real Social Network Graph were conducted using CHOCO-SGD to reduce mobile data costs. Torus topology is beneficial for large node networks, while decentralized and centralized SGD require more bits for accuracy. Training was done on a Davis Southern women social network with 32 nodes using a ResNet20 model on Cifar10. We trained a ResNet20 model on the Cifar10 dataset for image classification and a three-layer LSTM architecture on WikiText-2 for a language modeling task. Results show that the decentralized algorithm outperformed the centralized and quantized decentralized algorithms in terms of training accuracy. After training ResNet20 on Cifar10 and a three-layer LSTM on WikiText-2, the decentralized algorithm outperformed the centralized and quantized decentralized algorithms in terms of training accuracy. In testing, the decentralized algorithm performed best, followed by the centralized and quantized decentralized. CHOCO-SGD outperformed the exact decentralized scheme in test accuracy for the same transmitted data. CHOCO-SGD also had the best performance in test perplexity compared to the centralized SGD. When considering perplexity for a fixed data volume, CHOCO-SGD performs best, followed by exact decentralized and centralized algorithms. Large-scale training with Resnet-50 on ImageNet-1k in a datacenter setting shows benefits of CHOCO-SGD, especially when scaling to more nodes. Decentralized optimization methods address scaling issues even in well-connected environments like datacenters with fast connections. Recent studies have shown decentralized schemes can outperform centralized ones, with impressive speedups for training on multiple GPUs. Main differences between these algorithms and CHOCO-SGD include asynchronous gossip updates. The study demonstrates impressive speedups for training on 256 GPUs with asynchronous gossip updates and decentralized communication in a ring topology. The experiments were conducted on 8 machines with Resnet-50 on ImageNet-1k, showcasing the benefits of their setup compared to CHOCO-SGD. The study shows speedups for training on 256 GPUs using decentralized communication in a ring topology. CHOCO-SGD outperforms All-reduce in terms of time per epoch with a slight accuracy loss. Our study demonstrates a 20% time gain over the common all-reduce baseline using CHOCO-SGD for decentralized deep learning training in bandwidth-constrained environments. The algorithm shows theoretical convergence guarantees and linear speedup with the number of nodes, with empirical performance on image classification and language modeling tasks. Our study introduces CHOCO-SGD for decentralized deep learning training in communication-restricted environments, demonstrating theoretical convergence guarantees and linear speedup with node count. The proof of Theorem 4.1 is presented, showing performance under high communication compression and data-locality constraints. In Theorem A.2, CHOCO-SGD is analyzed for arbitrary stepsizes \u03b7, leading to Theorem 4.1 as a special case. The proof structure follows Koloskova et al. (2019), showing convergence of algorithms with stochastic gradient updates and averaging among nodes. Linear convergence of the averaging scheme in CHOCO-SGD is established, based on the estimate from (Koloskova et al., 2019). Decentralized SGD with arbitrary averaging is discussed in Algorithm 3, where an averaging scheme is assumed to preserve the average of iterates and converge with a linear rate. Exact Averaging is mentioned as a consensus averaging algorithm with a mixing matrix W, converging at a rate defined by an eigengap of W. The CHOCO-SGD algorithm is also referenced in the context of recovering algorithms like D-PSGD. Decentralized SGD with arbitrary averaging is discussed in Algorithm 3, where an averaging scheme is assumed to preserve the average of iterates and converge with a linear rate. The CHOCO-SGD algorithm is referenced in the context of recovering algorithms like D-PSGD. The order of communication and gradient computation parts in Algorithm 1 is exchanged to illustrate their independence and parallel execution. The proof of Lemma A.1 shows that the iterates of Algorithm 3 with constant stepsize \u03b7 satisfy a recursion verifying convergence. Under certain assumptions, the averaged iterates of Algorithm 3 exhibit a linear speedup compared to SGD on one node, with the convergence rate affecting only the second-order term. CHOCO-SGD with CHOCO-GOSSIP averaging converges at a specific rate, with a dependence on the eigengap of the mixing matrix W. The CHOCO-GOSSIP averaging scheme converges at a rate of 2, with a dependence on the eigengap of the mixing matrix W. The proof of Theorem A.2 involves L-smoothness and bounding terms to establish guarantees for the averaged vector of parameters x. In a decentralized setting, averaging all parameters across multiple machines can be costly and challenging, especially with a large number of machines and model size. In a decentralized setting, it is costly and challenging to average all parameters across multiple machines, especially with a large number of machines and model size. Theorem A.4 shows that Algorithm 3 converges at a speed determined by the convergence rate of the underlying averaging scheme, with a dependence on the stepsize and consensus stepsize. The convergence rate of Algorithm 3 in a decentralized setting depends on the underlying averaging scheme. Theorem A.4 proves this, with a focus on the stepsize and consensus stepsize. The proof involves L-smoothness and bounding terms, leading to Corollary A.5 on the convergence of local weights. Additionally, Lemma B.1 and B.3 provide insights on vector and matrix inequalities. Algorithm 4, CHOCO-SGD, introduces Error Feedback and mixing matrix W for initialization. Algorithm 4 CHOCO-SGD introduces Error Feedback and mixing matrix W for initialization. It combines CHOCO-SGD with weight decay and momentum, adapting Nesterov momentum for a decentralized setting. CHOCO-SGD works as an error feedback algorithm, saving quantization errors into internal memory for the next iteration. In CHOCO-SGD, the difference x (t) i \u2212 x (t\u22121) i is transmitted to represent the evolution of local variable x i at step t. Internal memory is used to correct errors before compressing the value. The model training procedure and hyper-parameter tuning are detailed. CHOCO-SGD with sign compression is compared with decentralized SGD and centralized SGD without compression. Two models are trained: ResNet20 for image classification on the Cifar10 dataset and a three-layer LSTM architecture. For image classification on the Cifar10 dataset, a ResNet20 model with 0.27 million parameters was used. For a language modeling task on WikiText-2, a three-layer LSTM architecture with 28.95 million parameters was employed. The LSTM had a hidden dimension size of 650, with gradient clipping at 0.4 and dropout at 0.4 applied only on the output. Training lasted for 300 epochs with a per node mini-batch size of 32. The learning rate followed a linear scaling rule. The learning rate of CHOCO-SGD is proportional to the node degree and momentum is only applied on ResNet20 training. The initial learning rate is gradually warmed up from 0.1 to the fine-tuned value for the first 5 epochs. It is then decayed by a factor of 10 at 50% and 75% of training epochs. The optimal learning rate per sample is determined by a linear scaling rule based on the node degree and mini-batch size. The best performance is ensured to be in the middle of the pre-defined grid. The optimal learning rate and consensus stepsize for CHOCO-SGD are fine-tuned using a pre-defined grid, ensuring the best performance is in the middle of the grids. Tables demonstrate the hyperparameters for training ResNet-20 on Cifar10 and a social network topology with 32 nodes. Training data is split between nodes with a fixed partition, no shuffling, a per node mini-batch size of 32, and a maximum node degree of 14. The training data is split between nodes with a fixed partition, no shuffling, a per node mini-batch size of 32, and a maximum node degree of 14. Additionally, the social network topology has 32 nodes where each node can only access a disjoint subset of the dataset. Test accuracy of the averaged model and consensus among local models is depicted towards the end of optimization. During optimization, local models initially diverge from the averaged model but reach consensus towards the end, with test performances aligning. This behavior is consistent with previous findings."
}