{
    "title": "Byl5NREFDr",
    "content": "We study model extraction in natural language processing, where an adversary can reconstruct a victim model using only query access. The attacker can successfully mount the attack without real training data or meaningful queries, using random word sequences and task-specific heuristics. This exploit is enabled by the shift towards transfer learning methods in NLP. With a query budget of a few hundred dollars, the attacker can extract a model that performs slightly worse than the victim model. Machine learning models are valuable intellectual property, often only accessible through web APIs. Malicious users may attempt to extract models through \"model stealing\" or \"model extraction\" attacks. Attackers can replicate existing models served by APIs with a query budget of a few hundred dollars, achieving similar performance to the victim model. Defense strategies like membership classification and API watermarking can be effective against some adversaries but may be circumvented by more sophisticated attackers. Contextualized pretrained representations like ELMo and BERT have become popular for NLP APIs. Adversaries can steal models by training a local copy using (input, output) pairs from API queries. This can lead to intellectual property theft, leakage of sensitive data, or the creation of adversarial examples. In this paper, it is demonstrated that NLP models fine-tuned from a pretrained BERT model can be extracted without access to training data or well-formed queries. Extraction attacks are possible with randomly sampled word sequences and simple heuristics, contrasting with prior work that required access to relevant data. Extraction attacks on NLP models can be conducted without access to training data or well-formed queries. By using randomly-sampled sentences and paragraphs from Wikipedia, the attacker can improve extraction performance. These attacks are cost-effective, with the most expensive one estimated at around $500. The attacker samples words to form queries, sends them to the victim BERT model, and fine-tunes their own BERT model using the victim's outputs as labels. This process works even without access to relevant data. The attacker uses randomly-sampled queries to extract information from a victim BERT model, fine-tuning their own model without access to relevant data. Pretraining on the attacker's side makes model extraction easier. Simple defenses like membership classification and API watermarking are effective against naive adversaries but fail against clever ones. The study evaluates defenses against model extraction attacks, highlighting their effectiveness against naive adversaries but failure against more sophisticated ones. The research aims to inspire further exploration into stronger defense mechanisms and a deeper understanding of vulnerabilities in models and datasets. Previous work on model extraction, particularly in computer vision, is referenced, along with the relevance of zero-shot distillation and NLP system input studies. The study evaluates defenses against model extraction attacks, highlighting their effectiveness against naive adversaries but failure against more sophisticated ones. Previous work on model extraction in computer vision is referenced, along with the relevance of zero-shot distillation and NLP system input studies. In contrast, recent work attempts extraction on NLP systems using BERT-large models for tasks expecting pairwise inputs like question answering. The study discusses prior work on data-efficient distillation and model extraction, focusing on generating data impressions from a teacher model. Previous research has shown successful extraction on SVMs and 1-layer networks using i.i.d noise, but scaling this to deeper neural networks remains a challenge. Unnatural text inputs have been shown to elicit overly confident model predictions. BERT, a 24-layer transformer model, is studied for model extraction. It converts word sequences into contextualized vector representations. The model's parameters are learned through masked language modeling on unlabelled data. BERT, a 24-layer transformer model, revolutionized NLP with state-of-the-art performance on various tasks. It uses contextualized vector representations learned through masked language modeling. In extraction attacks, a victim model g T is a black-box API for task T. Extraction attacks involve reconstructing a local copy of a victim model g T by using a task-specific query generator to create nonsensical word sequences as queries. The attacker fine-tunes a public release of f bert,\u03b8 * on the resulting dataset {x i , g T (x i )} to obtain g T for various NLP tasks. The curr_chunk discusses different input and output spaces for various NLP tasks, including binary sentiment classification, ternary natural language inference classification, extractive question answering, and boolean question answering. It also mentions two query generators, RANDOM and WIKI, used to generate input queries for these tasks. The curr_chunk discusses the insufficiency of RANDOM and WIKI query generators for tasks requiring complex interactions between different parts of the input space. Task-specific heuristics are applied, such as replacing words in the premise for MNLI and sampling words from the passage for SQuAD/BoolQ questions. The curr_chunk discusses query generation for tasks like SQuAD/BoolQ, using sampled words from passages to form questions. Evaluation is done with different query budgets, and cost estimates are provided using Google Cloud Platform's Natural Language API calculator. The curr_chunk discusses the high accuracy of extracted models on the original development set, even when trained with nonsensical inputs. Despite seeing only nonsensical questions during training, extracted SQuAD models recover 95% of original accuracy on WIKI. Extracted SQuAD models maintain 95% accuracy despite training with nonsensical questions. However, their agreement is only slightly better than accuracy, especially on held-out sets like WIKI and RANDOM. This discrepancy suggests poor functional equivalence between victim and extracted models. An ablation study with alternative query generation heuristics for SQuAD and MNLI is also conducted. The text discusses the impact of hiding the full probability distribution in model extraction attacks. Results show that access to the output probability distribution is not crucial for successful extraction. Query efficiency is also measured, showing that extraction can be successful even with small query budgets. In this section, an analysis is performed to understand why nonsensical input queries are effective for extracting NLP models based on BERT. The questions raised include the properties of these queries, the consistency of answers from different victim models, and the interpretability of nonsensical queries to humans. In this section, the analysis focuses on the effectiveness of nonsensical input queries for extracting NLP models based on BERT. The study examines the RANDOM and WIKI extraction configurations for SQuAD to determine if certain queries are more representative of the original data distribution. Different victim models show varying levels of agreement on answers to nonsensical queries, with higher agreement on SQuAD training and development set queries compared to WIKI and RANDOM queries. The study examines the effectiveness of nonsensical input queries for extracting NLP models based on BERT. High-agreement queries are found to be more useful for model extraction, showing large F1 improvements compared to random and low-agreement subsets. This suggests that agreement between victim models is a good indicator of the quality of input-output pairs for extraction. The study explores the use of nonsensical input queries for extracting NLP models based on BERT. High-agreement queries are more effective for model extraction, indicating that agreement between victim models is a good indicator of input-output pair quality. Investigating if high-agreement nonsensical textual inputs have human interpretations, human annotators were asked to answer SQuAD questions with unanimous victim model agreement. In a study on extracting NLP models based on BERT, human annotators answered SQuAD questions with unanimous victim model agreement. Annotators used a word overlap heuristic to select answer spans, resulting in lower accuracy on WIKI and RANDOM subsets compared to original SQuAD questions. The study also discusses inter-annotator agreement and the use of pretrained BERT-large models in practical scenarios. In practical scenarios, the attacker may not have information about the victim's architecture when fine-tuning a different base model. The extraction accuracy depends on the pretraining setup, with BERT-large yielding higher accuracy. Different configurations of BERT models were tested, showing better accuracy when the attacker starts from BERT-large, even if the victim was initialized with BERT-base. The attacker benefits from starting with the same model as the victim, such as BERT-base to BERT-base. Fine-tuning BERT gives attackers an advantage due to the good language representation. Training a QANet model without pretraining shows high accuracy with original SQuAD inputs and BERT-large labels. QANet achieves high accuracy with original SQuAD inputs and BERT-large labels, but F1 drops significantly with nonsensical queries. Better pretraining allows models to start with a good language representation, simplifying extraction. BERT-based models are vulnerable to model extraction, leading to the investigation of defense strategies that preserve API utility and remain undetectable to attackers without requiring re-training. The defense strategies explored in the study aim to protect against model extraction by identifying nonsensical inputs or adversarial examples and issuing random outputs to eliminate extraction signals. Membership inference is used as a binary classification problem, labeling original training examples as real and extraction examples as fake. The study explores defense strategies against model extraction by labeling original training examples as real and extraction examples as fake. Classifiers trained using model confidence scores and rare word representations transfer well to a balanced development set with the same distribution as their training data. The classifiers are robust to query generation processes and watermarking is another defense strategy discussed. Watermarking is a defense strategy against model extraction, where a fraction of queries are modified to return incorrect outputs. This method aims to detect extracted models by memorizing watermarked queries, making them vulnerable to detection if deployed publicly. Evaluation on MNLI and SQuAD shows promising results in detecting extracted models. The accuracy of predicting watermarked output and original labels on watermarked queries is crucial. Watermarking a small percentage of queries minimizes API performance drop. Models perform similarly on the development set with or without watermarking, but differences emerge on watermarked training data. Watermarking can detect extracted models but has limitations in its application. Watermarking can detect extracted models but has limitations in its application, as attackers can take steps to prevent detection. Model extraction attacks against NLP APIs serving BERT-based models are effective at extracting good models with low query budgets. Model extraction attacks against NLP APIs serving BERT-based models are surprisingly effective, even with nonsensical input queries. Fine-tuning large pretrained language models simplifies the extraction process for attackers. Existing defenses are generally inadequate, requiring further research for robust defenses against adaptive adversaries. Future directions include leveraging nonsensical inputs for model distillation, using query efficiency to diagnose dataset complexity, and exploring victim model agreement for proximity identification. In this study, the researchers focused on diagnosing dataset complexity using query efficiency as a proxy and investigating victim model agreement for proximity identification in model extraction attacks against NLP APIs. They utilized Google Cloud Platform's cost estimate for Natural Language APIs and extrapolated costs for tasks not covered by the APIs. In this paper, the costs of entity analysis and sentiment analysis APIs for natural language inference (MNLI) and reading comprehension tasks (SQuAD, BoolQ) were estimated. It is challenging to provide a precise cost estimate for issuing queries, as some API providers offer free queries. Attackers could potentially exploit multiple accounts to gather data. Most APIs are used on webpages and can be accessed freely by web users. Without proper precautions, attackers could scrape information from APIs at a large scale without cost. API costs can vary depending on computing infrastructure and revenue models. It is important to focus on the low costs needed to extract datasets rather than actual estimates. For example, it costs -$430.56 to extract a large speech recognition dataset and $2000.00 for 1 million translation queries. Input generation algorithms for each dataset are detailed in this section. In this section, input generation algorithms for different datasets are explained. For example, a vocabulary is built using wikitext103, with the top 10000 tokens preserved. Tokens are randomly sampled from this vocabulary for translation queries. The process involves replacing words in the wikitext103 vocabulary randomly to generate hypotheses for different datasets like MNLI and SQuAD. The final paragraphs are constructed by sampling tokens from the wikitext103 vocabulary based on chosen lengths. The process involves sampling tokens from the wikitext103 vocabulary to generate questions for datasets like SQuAD and BoolQ. Questions are constructed by randomly selecting paragraph tokens and appending them with a question starter word. In this section, additional query generation heuristics are studied by comparing extraction datasets for SQuAD 1.1. Findings show that starting questions with common question starter words like \"what\" helps, especially with RANDOM schemes. A similar ablation study on MNLI is presented, showing that when the lexical overlap between the premise and hypothesis is low, the model almost always predicts. Recent work on MNLI (McCoy et al., 2019) found that the model's predictions are influenced by the lexical overlap between the premise and hypothesis. Low overlap leads to neutral or contradiction predictions, high overlap leads to entailment predictions, and a few different words result in balanced datasets with strong extraction signals. Using frequent words aids extraction. Human studies involved fifteen annotators who annotated sets of questions, ensuring diversity in the annotators. The study involved English-speaking graduate students who annotated sets of questions to measure inter-annotator agreement. Different question sets were used, including original SQuAD questions, WIKI questions with high and low agreement among victim models, and RANDOM questions with high and low agreement. The results showed that the average pairwise F1 followed a specific order, reflecting the closeness to the actual input distribution. In an ablation study on input features for the membership classifier, the last layer representations were found to be more effective than the logits in distinguishing between real and fake inputs. However, the best results were achieved by using both feature sets. The ablation study showed that the last layer representations were more effective in classifying real or fake inputs, but the best results were obtained by using both feature sets."
}