{
    "title": "B1nLkl-0Z",
    "content": "State-action value functions, such as Q-values, are essential in reinforcement learning algorithms like SARSA and Q-learning. A new concept of action value, defined by a Gaussian smoothed version of expected Q-value, is proposed. This smoothed Q-value still satisfies a Bellman equation, making it learnable from sampled experience. Gradients of expected reward with respect to a Gaussian policy's mean and covariance can be obtained from the smoothed Q-value function. New algorithms are developed for training a Gaussian policy directly from a learned Q-value approximator, allowing for learning both mean and covariance during training. The ability to learn both mean and covariance during training allows for strong results in continuous control benchmarks. Model-free reinforcement learning involves policy evaluation and improvement processes. Different notions of Q-value have led to distinct families of RL methods, such as SARSA and Q-learning. In this work, a new notion of action value is introduced: the smoothed action value function Q\u03c0. Unlike previous notions, which assign a value to a specific action at each state, the smoothed Q-value assigns a value to a specific distribution over actions. The smoothed Q-value assigns a value to a distribution over actions, allowing for single-step Bellman consistency and optimization with Gaussian policies in RL algorithms. Smoothie is an algorithm that trains a policy using derivatives of a trained Q-value function, avoiding high variance in updates. Unlike DDPG, it utilizes a non-deterministic Gaussian policy for exploratory behavior. Smoothie utilizes a non-deterministic Gaussian policy with mean and covariance parameters, reducing the need for hyperparameter tuning. It can incorporate proximal policy optimization techniques by penalizing KL-divergence, improving stability and performance. Results on continuous control benchmarks are competitive, especially for challenging tasks with limited data. In the standard model-free RL framework, an agent interacts with a stochastic environment to maximize cumulative discounted reward. This is formulated as a Markov decision process with state and action spaces. The agent's behavior is modeled using a stochastic policy, and the optimization objective is expressed in terms of the expected action value function. The optimization objective in reinforcement learning is to maximize cumulative discounted reward by using a stochastic policy. The policy gradient theorem expresses the gradient of the objective function with respect to the policy parameters. Various algorithms balance variance and bias when estimating the action value function. In this paper, the focus is on multivariate Gaussian policies for continuous action spaces. The policy is parametrized by mean and covariance functions mapping the observed state to a Gaussian distribution. New RL training methods are developed for this parametric policy family, with potential generalization to other policy families. The paper focuses on multivariate Gaussian policies for continuous action spaces, parametrized by mean and covariance functions. A new formulation called the deterministic policy gradient is presented for Gaussian policies with a policy covariance approaching zero. This leads to a deterministic policy where the policy gradient theorem can be characterized, and the Bellman equation can be re-expressed. The paper introduces a deterministic policy gradient theorem for Gaussian policies with a policy covariance approaching zero. This allows for the re-expression of the Bellman equation and optimization of the value function approximator Q \u03c0 w by minimizing the Bellman error. Off-policy data is utilized to improve sample efficiency in practice. In this paper, smoothed action value functions are introduced to optimize Gaussian policy parameters efficiently using off-policy data. Smoothed Q-values do not assume the first action is fully specified, allowing for a more effective signal for optimization. This approach differs from prior work by considering actions drawn in the vicinity of a. The approach introduces smoothed Q-values for optimizing Gaussian policy parameters efficiently using off-policy data. By directly learning a function approximator for Q \u03c0 (s, a), the method enables direct bootstrapping of smoothed Q-values, leveraging Bellman consistency for optimization. The text discusses utilizing derivatives of Q \u03c0 to learn policy parameters efficiently. Parameterizing a Gaussian policy \u03c0 \u03b8,\u03c6 in terms of mean and covariance parameters enables direct optimization of Q \u03c0. The gradient of the objective w.r.t. mean parameters follows from the policy gradient theorem. Estimating the derivative w.r.t. covariance parameters involves observing that the second derivative of Q \u03c0 w.r.t. actions allows for exact computation of the derivative w.r.t. \u03a3. The text discusses optimizing Q \u03c0 using matrix calculus and two different approaches. The first approach involves optimizing Q \u03c0 with fixed target values, while the second approach uses a single function approximator for Q \u03c0. Sampling from a replay buffer with knowledge of the sampling probability allows for optimization of Q \u03c0 by minimizing a weighted Bellman error. Optimizing Q \u03c0 involves drawing a phantom action and minimizing a weighted Bellman error. The training procedure reaches an optimum when Q \u03c0 satisfies the Bellman equation recursion. It is unnecessary to track probabilities q(\u00e3 | s) if the replay buffer provides a near-uniform distribution of actions. Saving the probability of sampled actions during interactions with the environment is possible. Policy gradient algorithms are unstable in continuous control problems, leading to the development of trust region methods and stabilizing techniques. These techniques have not been applicable to algorithms like DDPG due to deterministic policies, but a proposed formulation in this paper allows for trust region optimization by augmenting the objective with a penalty. The paper proposes a trust region optimization method by augmenting the objective with a penalty, allowing for stable learning of a policy using Q-value functions. This method is a generalization of deterministic policy gradient, with the ability to train a policy using gradient information from the Q-value function. The proposed method is a generalization of the deterministic policy gradient, with updates for training the Q-value approximator and policy mean similar to DDPG. Stochastic Value Gradient (SVG) also trains stochastic policies but lacks updates for the covariance and estimates the mean update with a noisy Monte Carlo sample. Our approach estimates the smoothed Q-value function to avoid noisy estimates and discusses the importance of updating the covariance for subsequent trust region optimization. The proposed method, a generalization of deterministic policy gradient like DDPG, introduces updates for training the Q-value approximator and policy mean. It avoids noisy estimates by estimating the smoothed Q-value function and emphasizes the importance of updating the covariance for trust region optimization. The method simplifies updates by directly estimating integrals and relies on neural network function approximators instead of quadratic Taylor expansions. The novel training scheme proposed in this paper relies on neural network function approximators to estimate the smoothed Q-value function and learn the covariance of a Gaussian policy. This perspective differs from recent advances in distributional RL, as it focuses on the averaged return of a distribution of actions rather than the distribution of returns of a single action. The paper introduces a new RL algorithm called Smoothie, which utilizes Gaussian policies. It maintains a parameterized Q function and trains a Gaussian policy using gradient and Hessian updates. The algorithm is outlined in a simplified pseudocode in Algorithm 1. Smoothie is evaluated against DDPG on a simple synthetic task with a reward function of two Gaussians. The policy mean is initialized at the worse Gaussian to study its behavior in a restricted setting. The function is a mixture of two Gaussians, with Smoothie learning both mean and variance, while DDPG learns only the mean. DDPG struggles to escape the local optimum due to updates based on the derivative of Q \u03c0 w at the current mean. Smoothie, on the other hand, performs better in this scenario. Smoothie successfully solves the task by adjusting the covariance during training, leading \u00b5 \u03b8 towards the better Gaussian. The smoothed reward function helps in this process, with \u03a3 \u03c6 decreasing initially and then increasing before approaching the global optimum. Comparing Smoothie and DDPG on a synthetic task, DDPG keeps the standard deviation constant while Smoothie learns both mean and standard deviation. The reward function and its Gaussian-smoothed version are shown for the task. Smoothie successfully adjusts the covariance during training to improve performance. It adapts both mean and standard deviation, escaping local optima. Implementation details for continuous control benchmarks are provided, utilizing neural networks for policy and Q-values. Exploration in DDPG is determined by an Ornstein-Uhlenbeck process. Average reward and standard deviation plots are shown for six randomly seeded runs. Smoothie is competitive with DDPG, learning optimal noise scale during training and outperforming in difficult tasks like Hopper, Walker2d, and Humanoid. TRPO is not sample-efficient. Results compared in FIG2 with hyperparameter search for actor learning rate, critic learning rate, and reward scale. DDPG's exploratory noise scale and damping also considered. Smoothie outperforms DDPG in various tasks, showing a slight advantage in Swimmer and Ant, and significant improvements in Hopper, Walker2d, and Humanoid. The average reward for Hopper is doubled, and Humanoid achieves the best published results with only millions of environment steps. Smoothie learns exploration without supervision, while DDPG uses a hyperparameter search for exploration. Smoothie's performance surpasses DDPG in various tasks, with notable improvements in Hopper and Humanoid. The introduction of a KL-penalty enhances Smoothie's performance, particularly on challenging tasks. This penalty promotes stability, addressing the inherent instability in DDPG training. Smoothie's use of a learnable covariance and proximal policy optimization method proves beneficial, showcasing better results with fewer environment steps compared to TRPO. The Smoothie algorithm, utilizing a new Q-value function Q \u03c0, shows significant performance improvements in tasks like Hopper and Humanoid compared to DDPG. By learning both mean and covariance during training, Smoothie can match or surpass DDPG's performance, especially with a penalty on policy divergence. Learning Q \u03c0 is considered more sensible than Q \u03c0, as it leads to a smoother reward surface. The success of Q \u03c0 is encouraging as it leads to a smoother reward surface, making it easier to learn and having a more direct relationship with the expected return objective. Future work should investigate these claims and apply the underlying motivations to other policies. A proof of a specific identity related to Gaussian integrals is provided, derived using standard matrix calculus. The text discusses the simplification of equations for a symmetric matrix A, focusing on the left-hand side and right-hand side of a specific formula."
}