{
    "title": "HkGv2NMTjQ",
    "content": "State of the art sound event classification uses neural networks to learn associations between class labels and audio recordings in a dataset. Ontologies define a structure that relates sound classes with more abstract super classes, serving as a source of domain knowledge representation. However, the use of ontology information in modeling neural network architectures is often overlooked. Two ontology-based neural network architectures are proposed for sound event classification, showing improved performance by incorporating ontological information. Humans can recognize various sounds in their environment, which can be categorized into more abstract classes like humans, emergency vehicles, and home. Ontologies provide a structure for representing these relationships, but current sound event classification methods do not fully utilize this information. Ontologies can enhance sound event classification by providing structure through categories and relationships. Neural networks, although state of the art for SEC, often do not consider ontologies. Taxonomies can be based on abstraction hierarchies defined by linguistics, such as nouns or verbs. Examples of datasets include ESC-50, UrbanSounds, DCASE, and AudioSet. Another taxonomy can be based on interactions between objects and materials, actions, and descriptors. Hierarchical relations in sound event classifiers offer multiple benefits, allowing for back-off to more general categories, disambiguating acoustically similar classes, penalizing classification differently, and serving as domain knowledge for neural networks. Ontological information has been evaluated in computer vision and music but is rarely used for sound. Ontological information has been used in computer vision and music but is rarely utilized for sound event classification. Ontology-based network architectures have shown performance improvements and reduced overfitting in training data. Authors have proposed various approaches, such as using a perceptron for each node in the hierarchy to improve class disambiguation. The proposed ontology-based networks aim to improve performance in sound event classification by utilizing ontological information in deep learning architectures. The framework includes a Feed-forward model with ontological constraints and extends the learning model to compute ontology-based embeddings using Siamese Neural Networks. This approach aims to adapt structures in the deep learning model to better align with the ontology structure. The framework utilizes ontological information in deep learning architectures to compute ontology-based embeddings using Siamese Neural Networks. It considers ontologies with two levels and can be generalized to more levels. Training data consists of audio representations associated with labels from the ontology, where each class is mapped hierarchically. The framework utilizes ontological information in deep learning architectures to compute ontology-based embeddings using Siamese Neural Networks. It considers ontologies with two levels and can be generalized to more levels. Training data consists of audio representations associated with labels from the ontology, where each class is mapped hierarchically. The architecture of the Feed-forward Network with Ontological Layer is illustrated, showing the relationship between elements in different ontology levels. The probabilistic formulation allows for inferring labels between levels based on known information. The text discusses using ontological information in deep learning architectures to compute ontology-based embeddings. It explains how to estimate probabilities for different levels in the ontology and the importance of utilizing knowledge during training to improve model performance. The proposed framework is used to design ontology-based neural network architectures, including the introduction of an ontological layer. The proposed framework utilizes an ontological layer in the architecture, incorporating a Feed-forward Network (FFN) to generate outputs for different ontology levels. The base network learns weights for audio features input x to produce probability vectors for classes C1 and C2. The ontological layer reflects the ontology's super classes and sub classes relationship, enhancing prediction accuracy. The ontological layer in the architecture connects super classes and sub classes to predict classes C1 and C2 for input x. Equation 3 represents this relationship as a directed graph with an incidence matrix M. The weights in the ontological layer are fixed and part of the training data. The model is trained using a gradient-based method to minimize the loss function L, a combination of two categorical cross-entropy functions for p(y1|x) and p(y2|x). The architecture includes a Siamese neural network (SNN) for learning ontology-based embeddings that preserve the ontological structure. The SNN enforces samples of the same class to be closer and separates samples of different classes, with the goal of creating embeddings that reflect the ontological relationships. The model uses a convex combination of two categorical cross-entropy functions, with a hyperparameter \u03bb \u2208 [0, 1] to be tuned. When \u03bb = 1, the model trains a standard classifier using only information from the first level of the ontology. The Siamese neural network (SNN) architecture includes a Feed-forward Network with Ontological Layer shown in FIG1. It uses ontological embeddings to compute a Similarity metric (Euclidean Distance) for samples from different super classes. The distance between embeddings indicates the similarity between samples based on subclass and super class distinctions. The Feed-forward Model with Ontological layer uses ontological embeddings to compute similarity metrics for samples from different subclasses and superclasses. The red rows show output probabilities for different levels, and the model is trained using gradient-based methods. Evaluation of sound event classification performance is done using ontological-based neural network architectures on datasets with ontologies. The dataset for Making Sense of Sounds Challenge 2 aims to classify the most abstract classes in its taxonomy. The dataset for Making Sense of Sounds Challenge 2 (MSoS) aims to classify the most abstract classes in its taxonomy. It consists of 1500 audio files from various sources, divided into five categories with 300 files each. The evaluation dataset has 500 audio files, 100 per category, all in a standardized format. The files are 5 seconds long with periods of silence, and the set is partitioned for training, tuning parameters, and testing. The official blind evaluation set includes 500 files. The Urban Sounds -US8K dataset contains 8,732 audio files from Freesound database, with a taxonomy adjusted to avoid redundant levels. The files are in a standardized format of single-channel 44.1 kHz, 16-bit .wav files, divided into 10 subsets for training and tuning parameters, with one subset for testing. The dataset contains 8,732 audio files in 44.1 kHz, 16-bit .wav format, divided into 10 subsets for training and testing. The audio recordings were represented using state-of-the-art Walnet features BID1 and transformed via a convolutional neural network (CNN) with a feed-forward multi-layer perceptron network architecture. The network consists of 4 layers: an input layer of dimensionality 1024, 2 dense layers of dimensionality 512 and 256, and an output layer of dimensionality 128. The dense layers utilize Batch Normalization, a dropout rate of 0.5, and the ReLU activation function. The dense layers in the neural network utilize Batch Normalization, a dropout rate of 0.5, and the ReLU activation function. Parameters were tuned in the Net box and for transforming z into p(y 1 |x). Baseline models were considered for different data sets at levels 1 and 2, without ontological information. For level 1, the baseline model is equivalent to training the Feed-forward model with Ontological Layer using \u03bb = 1. Results of baseline models are shown in Table 1. The baseline model differs from the Feed-forward model with \u03bb = 0 as it lacks a layer for predicting y1. Results of baseline models for MSoS and US8K data sets at levels 1 and 2 are shown in Table 1. The best performance in MSoS data set was achieved with \u03bb = 0.8, obtaining 0.74 and 0.913 accuracy in levels 1 and 2 respectively. The ontological layer improves classification performance. Using the ontological structure, the best performance was achieved with \u03bb = 0.8, resulting in 5.4% and 6% improvement in accuracy for levels 1 and 2 compared to baseline models in the MSoS dataset. In the US8K dataset, the best performance was with \u03bb = 0.7, showing a smaller improvement of 2.5% and 0.2% for levels 1 and 2. The MSoS t-SNE plots illustrate the impact of ontology-based embeddings on class samples at different levels. The ontology-based embeddings resulted in tighter and better defined clusters for sound event classification. The Siamese neural network was trained with Walnet audio features using different super and sub class pairs to produce the embeddings. The SNN was trained for 50 epochs using the Adam algorithm and hyper-parameters were tuned for good performance. The Siamese neural network was trained for 50 epochs using the Adam algorithm and hyper-parameters were tuned for good performance. Different numbers of pairs for input training data were tried, with 100,000 pairs yielding the best performance. The loss function values were adjusted, affecting overall performance. Results showed improved accuracy compared to the baseline, but slightly underperformed the method without the ontology-based embeddings. The performance of the architecture with ontology-based embeddings is better than the baseline but slightly underperformed compared to the method without embeddings. Ontology-based embeddings showed better grouping with tighter and well-defined clusters in t-SNE plots. However, the US8K dataset had limited performance due to a similar number of sub classes and super classes. The study compared two approaches for sound event classification using hierarchical ontologies. The Feed-forward Network with Ontological Layer achieved 0.88 accuracy, while using ontological embeddings achieved 0.89, both outperforming the baseline. The framework proposed a simple way to incorporate ontology into deep learning models without adding more parameters. Our study utilized a Feed-forward Network with an ontological layer and a Siamese neural Network to improve sound event classification. Results showed improved performance over baselines, indicating the potential for further exploration of ontologies in this field."
}