{
    "title": "HyerxgHYvH",
    "content": "The proposed solution for evaluating mathematical expressions involves a Lego bricks style architecture, where smaller neural networks are trained independently to perform specific operations. Eight fundamental operations are identified and learned using feed forward neural networks, which can then be reused to create larger networks for more complex tasks like n-digit multiplication and division. The bottom-up strategy involves reusing smaller neural networks to create larger and more complex networks for tasks like n-digit multiplication, division, and cross product. This approach allows for generalization up to 7 digit numbers and works for both positive and negative numbers. Unlike traditional methods, this solution focuses on the success of feed-forward Artificial Neural Networks in learning and adapting to specific tasks, with a key emphasis on generalization for improved performance on unseen data. The performance of Artificial Neural Networks (ANNs) degrades on unseen data due to lack of generalization. Techniques like Domain Adaptation can help, but ANNs primarily rely on memorization and lack quantitative reasoning. In contrast, living species, like children, exhibit numerical extrapolation and quantitative reasoning in their learning process. The key to generalization in learning is understanding how to reuse memorized examples, as seen in children's ability to extrapolate arithmetic operations. Complex numerical reasoning in Artificial Neural Networks can be developed by identifying and learning fundamental operations that can be reused to create complex functions. Fundamental arithmetic operations are learned using simple neural networks, which are then reused to create larger networks for solving more complex problems like n-digit multiplication and division. This approach is the first to propose a generalized solution for arithmetic operations, working for both positive and negative numbers. Neural networks are known for their ability to approximate mathematical functions. Neural networks can approximate arithmetic functions for both positive and negative numbers. Recent works have focused on training networks to generalize over minimal training data using different architectures like Resnet, highway networks, and dense networks. EqnMaster uses generative recurrent networks to approximate arithmetic functions, but still lacks generalization over unseen data. EqnMaster uses generative recurrent networks to approximate arithmetic functions, but struggles with generalization beyond 3-digit numbers. The Neural Arithmetic Logic Unit (NALU) utilizes linear activations to predict arithmetic function outputs, highlighting extrapolation challenges in end-to-end learning tasks. In contrast, a simple Feed Forward Network can solve arithmetic expressions like multiplication, albeit with less efficiency in network architecture. Using digital circuitry as a reference, neural networks can be designed to efficiently solve basic arithmetic problems. Our work builds on Binary Multiplier Neural Networks' premise to predict arithmetic function outputs for both positive and negative decimal numbers. Our proposed network can predict arithmetic function outputs for both positive and negative decimal integers, unlike existing models that only work on limited digits. We train smaller networks for different sub tasks like signed multiplication and division, then combine them to perform complex tasks. We also use a loop unrolling strategy to generalize solutions from 1-digit to n-digit arithmetic operations. Our proposed network can predict arithmetic function outputs for positive and negative decimal integers, using a loop unrolling strategy to generalize solutions from 1-digit to n-digit arithmetic operations. Multiplication and division can be performed as repeated addition and subtraction, implemented on digital circuits known for accurate arithmetic operations. Neural networks can simulate digital circuits, with n-digit multiplication involving 1-digit multiplication, addition, place value shifter, and sign calculator. Division requires subtraction operation in addition to other operations. Neural networks designed for arithmetic operations involve fundamental blocks like addition, subtraction, multiplication, place value shifter, and sign calculators. Complex functions like multiplication and division can be achieved using these basic blocks. Neurons in the network perform a sum transform by multiplying inputs with weights and passing them through an activation function to produce the final output. The addition and subtraction modules in the neural network are implemented using single neurons with specific weights. The multiplication process involves shifting and adding digits from right to left, with a place value shifter used to position the output correctly before final addition. The neural network's multiplication module uses fixed weights for each digit, with 82 possible outcomes for 1-digit integer multiplication. It has 2 input neurons, 1 hidden layer with 30 neurons, and an output layer of 82 neurons. The model computes the absolute value of a single number and selects the highest-ranked prediction as the output. The neural network's multiplication module computes the absolute value of a single number using 2 hidden layers. It includes an input sign calculator and an output sign calculator for determining the sign of the result. The neural network's multiplication module uses 2 hidden layers to compute the absolute value of a single number and determine the sign of the result. It performs multiplication between 2 n digit signed integers using fundamental operations. The neural network's multiplication module computes the absolute value of signed integers using fundamental operations. It converts numbers to positive integers, extracts input sign, and performs multiplication using single digit operations. The final output is obtained by combining the results for each token of the multiplier. The multiplication module computes absolute values of signed integers using single digit operations. Results are combined for each token of the multiplier to form a single number. The final output is assigned a sign using 1-digit sign multiply. The division model separates sign and magnitude during pre-processing, inspired by long division model. The division model in the architecture involves multiplying the n-digit divisor with single digit multipliers and subtracting from the dividend. The smallest non-negative integer is selected using additional layers, with the selected node representing the remainder and quotient. The quotient is combined over iterations and the remainder is carried over to the next digit in the divisor. The architecture of the multiplication network is shown in Figure 2(b,d). The division model is based on digital circuitry for decimal digits. Comparisons are made with signed arithmetic operations, as the paper only implements addition, subtraction, and multiplication. The study compares the results of signed arithmetic operations using Neural Arithmetic and Logic Unit (NALU) implementation. Testing is done on a dataset ranging from 0 to 30 uniform numbers, with results rounded to integers. The model is tested on 2 to 7-digit integers, including negative integers, showing superior performance compared to recurrent and discriminative networks. Our model outperforms recurrent and discriminative networks, achieving 100% accuracy within the testing range of the dataset. The signed multiplication feature is exclusive to our model. Comparison with the Neural Arithmetic and Logic Unit (NALU) model shows superior performance, especially in division operations. The NALU model fails drastically outside the range of 10-20. Many complex tasks can be divided into smaller sub-tasks, which can be solved by training independent small neural networks. Fundamental arithmetic operations are learned using simple feed forward neural networks, which are then combined to solve more complex tasks. The proposed work involves using smaller neural networks to learn fundamental arithmetic operations, which are then combined to solve complex tasks like n-digit multiplication and division. One limitation is the use of float operation in the tokenizer, but it does not hinder the current work as only pre-trained smaller networks are used. Future work includes resolving this issue and testing the accuracy of a cross product network. Additionally, there are plans to develop a point cloud segmentation algorithm using a larger number of identical smaller networks."
}