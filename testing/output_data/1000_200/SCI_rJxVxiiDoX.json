{
    "title": "rJxVxiiDoX",
    "content": "We propose quantization-aware training to reduce computational cost of deep neural network based keyword spotting. Experimental results show that this approach can recover performance models quantized to lower bits representations. Additionally, combining quantization-aware training with weight matrix factorization significantly reduces model size and computation for small-footprint keyword spotting while maintaining performance. Quantization-aware training is used to optimize keyword models for low-power devices by considering quantized weights in full precision representation. This approach enables successful training of 8 bit and 4 bit quantized keyword spotting models. The paper introduces the keyword spotting system and quantization methods, emphasizing the use of dynamic quantization techniques. The paper discusses training KWS models using quantization-aware 32 training. Dynamic quantization is used for DNN weight matrices, with inputs quantized row-wise on the fly. The keyword 'Alexa' is chosen for experiments using a 500 hrs far-field corpus. Evaluation is done using DET curves and AUC. Training is done using GPU-based distributed methods. The paper discusses training KWS models using quantization-aware training. Training is organized into 3 stages, with the performance of quantized models shown in a table and DET curves for different models compared. The DET curves for 16 bit and 8 bit quantized models are not significantly different from the full-precision model."
}