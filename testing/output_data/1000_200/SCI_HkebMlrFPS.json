{
    "title": "HkebMlrFPS",
    "content": "Most deep learning for NLP uses single-mode word embeddings, but this new approach introduces multi-mode codebook embeddings for phrases and sentences. These codebook embeddings capture different semantic facets of the phrase's meaning and outperform strong baselines on benchmark datasets. Codebook embeddings provide a more interpretable semantic representation and outperform strong baselines on various NLP tasks. Unlike single-mode embeddings, codebook embeddings capture different semantic facets of phrases and sentences, offering a more comprehensive analysis. Word sense induction methods and multi-mode word embeddings are used to represent target words in multiple points or regions in a semantic space by clustering neighboring words. This approach helps in capturing different senses of words, as illustrated with the example of \"real property\" having different meanings in legal and philosophical contexts. In contrast to topic modeling like LDA, word sense induction methods use clustering to discover different senses of words. However, extending these representations to phrases or sentences faces efficiency challenges due to the large number of unique sequences in a corpus. Our compositional model aims to predict cluster centers' embeddings from the target phrase's word sequence to reconstruct the co-occurring distribution efficiently. This approach addresses the challenge of sparseness in co-occurring statistics and avoids overfitting by learning the compositional meaning of each sequence without excessive parameters. In this work, a neural encoder and decoder are used to compress redundant parameters in local clustering problems. Instead of clustering co-occurring words at test time, a mapping between target sequences and cluster centers is learned during training. This allows for direct prediction of cluster centers using a single forward pass of the neural network during testing. The proposed model uses a neural network to generate cluster centers for unseen input sequences during testing. A nonnegative and sparse coefficient matrix is used to match predicted cluster centers with observed word embeddings. Gradients are back-propagated to train the model jointly and end-to-end. The model outperforms baseline methods in capturing compositional meanings and can measure asymmetric relations without supervision. The model proposed uses neural networks to generate cluster centers for input sequences. It outperforms baselines in capturing compositional meanings and measuring asymmetric relations without supervision. The multimode representation is shown to be superior in sentence representation, as demonstrated in an extractive summarization experiment. The training setup, objective function, and architecture of the prediction model are detailed in Sections 2.1, 2.2, and 2.3, respectively. The model uses neural networks to generate cluster centers for input sequences, outperforming baselines in capturing compositional meanings and measuring asymmetric relations without supervision. It reconstructs embeddings of co-occurring words while avoiding predicting common topics. The training signal is to reconstruct neighboring words based on a fixed window size for sentence representation. The model uses neural networks to generate cluster centers for input sequences, outperforming baselines in capturing compositional meanings and measuring asymmetric relations without supervision. It reconstructs embeddings of co-occurring words while focusing on semantics rather than syntax. The training signal is to reconstruct neighboring words based on a fixed window size for sentence representation, with the goal of clustering words that could possibly occur beside the sequence. The model uses neural networks to generate cluster centers for input sequences, focusing on semantics over syntax. It reconstructs embeddings of co-occurring words in a pre-trained word embedding space, ignoring word order information. The predicted cluster centers of input sequences are represented as matrices, improving the capturing of compositional meanings and measuring asymmetric relations without supervision. The model utilizes neural networks to generate cluster centers for input sequences, focusing on semantics. It reconstructs embeddings of co-occurring words in a pre-trained word embedding space, using a fixed number of clusters to simplify the prediction model design. The reconstruction loss of k-means clustering in the word embedding space is discussed, along with the adoption of non-negative sparse coding for relaxation of constraints. The model uses neural networks to generate diverse cluster centers for input sequences, focusing on semantics. It adopts non-negative sparse coding to relax constraints, allowing positive coefficient values. This approach is smoother and easier to optimize for neural networks compared to k-means clustering. The reconstruction error is defined with a hyper-parameter controlling sparsity, ensuring the coefficient values remain \u2264 1 to prevent the neural network from learning to predict centers. The reconstruction error is defined with a hyper-parameter controlling sparsity to avoid predicting centers with small magnitudes. The proposed loss minimizes L2 distance in a pre-trained embedding space and efficiently estimates M Ot using convex optimization. This allows for end-to-end training without predicting the same global topics regardless of input. Our method involves back-propagating gradients to the neural network for end-to-end training. The loss function is defined to prevent predicting the same global topics regardless of input. The neural network architecture is similar to a transformation-based seq2seq model, using an encoder to map sentences with similar word distribution closer together. The encoder aims to map sentences with similar word distribution closer together, using a decoder that predicts embeddings instead of words. Different linear layers are used to capture different aspects of codebook embeddings. Removing attention on contextualized word embeddings from the encoder increases validation loss for sentence representation. The encoder focuses on mapping sentences with similar word distribution closer together, using a decoder to predict embeddings. Removing attention on contextualized word embeddings from the encoder increases validation loss for sentence representation. The attention does not significantly affect phrase representation, allowing for flexibility in replacing encoder and decoder architectures. Other input features can also be incorporated, and cluster centers predicted by the model can be visualized. The model visualizes cluster centers to summarize target sequences effectively. Codebook embeddings capture semantic facets of phrases or sentences, improving unsupervised semantic tasks. Pre-trained GloVe embeddings are used for sentence and phrase representation. The model is trained on Wikipedia 2016, removing stop words from co-occurring words. Noun phrases are considered in phrase experiments. Our model is trained on Wikipedia 2016 with stop words removed from co-occurring words. Noun phrases are considered in experiments, and sentence boundaries and POS tags are detected using spaCy. The models do not require additional resources like PPDB and are compared with baselines using raw text. The embedding size is set to GloVe size (300), and models are trained on one GPU within a week. The models trained on a modern GPU for a week underfit the data due to their small size. Comparing with BERT, which has more parameters and resources, our models show unsupervised performance based on cosine similarity. The unsupervised performances of models based on cosine similarity are evaluated using standard benchmarks like Semeval 2013 and Turney 2012. BiRD and WikiSRS datasets provide ground truth phrase similarities. Semeval 2013 distinguishes similar phrase pairs, while Turney aims to identify the most similar unigram to a query bigram. BiRD and WikiSRS measure phrase relatedness and similarity. Our model evaluates two scoring functions for measuring phrase similarity: one averages contextualized word embeddings from a transformer encoder and computes cosine similarity, labeled as Ours Emb. We also calculate a symmetric distance SC by comparing normalized codebook embeddings of phrases. When ranking for similar phrases, negative distance represents similarity. Performance is compared with 5 baselines including GloVe Avg and Word2Vec Avg, which compute cosine similarity between averaged word embeddings. Our models significantly outperform all baselines in the 4 datasets, including GloVe Avg and Word2Vec Avg. The results show the effectiveness of non-linearly composing word embeddings and the incorporation of word order information in producing phrase embeddings. The results indicate the effectiveness of non-linearly composing word embeddings to predict co-occurring word embeddings. Performance of Ours (K=1) is slightly better than Ours (K=10), supporting previous findings. Ours (K=10) remains strong compared to baselines, showing similarity performance is not sensitive to the number of clusters. STS benchmark is a widely used sentence similarity task. The STS benchmark is a widely used sentence similarity task. Different models are compared based on their performance on lower half datasets with less similarity. Various methods like BERT, GloVe, word mover's distance, and cosine similarity are evaluated. Arora et al. (2017) propose a method to weight words in sentences. The STS benchmark evaluates sentence similarity using different models like BERT, GloVe, word mover's distance, and cosine similarity. Arora et al. (2017) suggest weighting words in sentences based on their probability in the corpus. Post-processing methods like GloVe SIF and GloVe Prob_avg are used to enhance performance. Considering word embeddings along with sentence embeddings improves sentence similarity measurement. The importance of considering word embeddings along with sentence embeddings for measuring sentence similarity is highlighted. Multi-facet embeddings allow for estimating word importance by computing cosine similarity with predicted codebook embeddings. This approach enhances the accuracy of sentence similarity measurement compared to other methods. The importance of word embeddings for measuring sentence similarity is emphasized. Our method, which uses attention weighting, outperforms other methods like WMD and BERT Avg, especially in STSB Low. Multi-mode representation and attention weighting significantly improve performance, especially in STSB Low. The variant of the method using bi-LSTM as encoder and LSTM as decoder performs worse than the transformer alternative. It outperforms ST Cos, showing the effectiveness of ignoring word order in the loss function. The model is applied to HypeNet for hypernymy detection based on the assumption that co-occurring words are less related to some hyponyms. The predicted codebook embeddings of a hyponym cluster co-occurring words better than its hypernym. Asymmetric scoring function defined for detecting hypernyms outperforms baselines. Our methods show better accuracy in detecting hypernym direction compared to symmetric similarity measurement. The objective is to discover a summary with normalized embeddings that best reconstructs the document. The extractive summarization method aims to generate multiple codebook embeddings to represent each sentence in the document, optimizing a specific equation. The model selects sentences greedily to create a summary with normalized embeddings that reconstruct the document effectively. The extractive summarization method generates codebook embeddings to represent sentences in the document, comparing different approaches like average word embeddings and using all words in sentences as aspects. Results are evaluated on the CNN/Daily Mail testing set using F1 of ROUGE. The extractive summarization method compares different approaches for sentence representation using codebook embeddings. Results are evaluated on the CNN/Daily Mail testing set using F1 of ROUGE, comparing methods that do or do not use sentence position information. In evaluating unsupervised sentence embeddings, larger cluster numbers (K) lead to better results, with K=100 performing best after selecting 3 sentences. This approach allows for setting a large K to address efficiency challenges. Neural networks have been explored for discovering coherent topics, offering flexibility in incorporating various input features. Neural networks have been used to discover semantically coherent topics by Srivastava & Sutton (2017). Sparse coding on word embedding space is employed to model multiple aspects of a word (Faruqui et al., 2015; Arora et al., 2018). Parameterizing word embeddings with neural networks helps test hypotheses and save storage space (Han et al., 2018; Shu & Nakayama, 2018). Words are represented as single or multiple regions in Gaussian embeddings to capture asymmetric relations (Vilnis & McCallum, 2015; Athiwaratkun & Wilson, 2017). Challenges of extending these methods to longer sequences remain unaddressed. The challenges of designing a neural decoder for sets rather than sequences are addressed in studies focusing on measuring distances between ground truth and predicted sets. Various matching loss options, such as Chamfer distance, are utilized in auto-encoder models for point clouds. The goal is to reconstruct a set using fewer bases, achieving permutation invariance for neural networks. Our goal is to efficiently predict clustering centers that reconstruct observed instances, overcoming computational challenges in learning multi-mode representations for long sequences. Using a neural encoder to model target sequence meaning and a neural decoder to predict a set of codebook. The proposed model uses a neural encoder and decoder to predict codebook embeddings as representations for sentences or phrases. It utilizes a non-negative sparse coefficient matrix to match predicted embeddings to observed words, allowing for interpretable clustering centers. Experimental results show that the model outperforms BERT, skip-thoughts, and GloVe in unsupervised benchmarks, especially for sequences with multiple aspects. The experimental results suggest that multi-facet embeddings perform best for sequences with many aspects, while single-facet embeddings perform similarly well for sequences with one aspect. Future plans include training a single model to generate multi-facet embeddings for both phrases and sentences, evaluating it as a pre-trained embedding approach for supervised or semi-supervised settings, and applying it to other unsupervised learning tasks. The model is kept simple due to computational constraints, with training loss nearly converged after 1 or 2 epochs, and hyper-parameters not fine-tuned. The model is smaller than BERT but effective in architecture. The model uses a smaller architecture compared to BERT, with similar hyper-parameters. The sparsity penalty weight is set at 0.4, sentence size limited to 50 words, and co-occurring words restricted to 30. Transformer dimensions are set to 300, with 5 layers for sentence representation and 1 layer for phrase representation. The model architecture uses smaller dimensions compared to BERT, with a sparsity penalty weight of 0.4. The number of transformer layers for sentence representation is 5, while for phrase representation it is 2 with a dropout on attention of 0.5. The window size is set to 5, and hyperparameters are determined by validation loss. The number of codebook embeddings is chosen based on training data performance, with larger K needing longer training time. The model uses smaller dimensions compared to BERT, with a sparsity penalty weight of 0.4. The number of transformer layers for sentence representation is 5, while for phrase representation it is 2 with a dropout on attention of 0.5. The window size is set to 5, and hyperparameters are determined by validation loss. Larger K requires longer training time, which may affect model performance. The hidden embedding for skip-thoughts is set to 600, and the model is retrained in Wikipedia 2016 for 2 weeks. Comparisons with BERT Large show differences in performance across various tasks. The model uses smaller dimensions compared to BERT, with differences in performance across various tasks. BERT performs better in similarity tasks, but worse in hypernym detection. Increasing model size boosts BERT's performance, especially in phrase similarity tasks. Our method outperforms BERT in most cases, possibly due to BERT's training on predicting masked words in input sequences, which may not be ideal for short sequences like phrases. Comparisons with other baselines show potential issues with sentence representation methods. The bad performances of W Emb (*) methods may be due to selecting shorter sentences. Our method outperforms W Emb (GloVe) and Sent Emb (GloVe) when summaries have similar length. W Emb (*) usually outperforms Sent Emb (*) with similar length summaries, but this comparison may not be fair as W Emb (*) can select more sentences. Preventing the selection of many short sentences may be preferable for fluency in extractive summarization. In extractive summarization, it is important to balance the number of sentences used to cover topics for fluency. Our method performs well compared to other methods like W Emb (GloVe) and Sent Emb (GloVe) for summaries of similar length. When summary length is less than 50 words, Ours (K=100) is the best choice, while W Emb (BERT) is better for longer summaries. Combining our method with BERT could lead to improved performance. The study visualizes predicted embeddings from sentences in the validation set using a GloVe embedding space. The format of the file is similar to Table 1, with the first line being the preprocessed input sentence. The embeddings are visualized by the nearest five neighbors and their cosine similarities."
}