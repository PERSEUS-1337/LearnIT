{
    "title": "r1eEG20qKQ",
    "content": "Hyperparameter optimization is a bilevel optimization problem where optimal parameters depend on hyperparameters. Regularization hyperparameters for neural networks are adapted by fitting approximations to the best-response function. Scalable best-response approximations for neural networks are constructed by modeling the best-response as a single network with gated hidden units. This model is fitted using a gradient-based hyperparameter optimization algorithm. Self-Tuning Networks (STNs) update hyperparameters online during training without the need for differentiating the training loss. This approach allows for tuning discrete hyperparameters, data augmentation hyperparameters, and dropout probabilities. It outperforms other hyperparameter optimization methods on large-scale deep learning problems, discovering hyperparameter schedules that can surpass fixed values. Regularization hyperparameters like weight decay, data augmentation, and dropout are crucial for neural network generalization but challenging to tune. Hyperparameters like weight decay, data augmentation, and dropout are essential for neural network generalization but difficult to tune. Popular hyperparameter optimization methods include grid search, random search, and Bayesian optimization. Formulating hyperparameter optimization as a bilevel optimization problem can lead to faster convergence. In hyperparameter optimization, functions L V and L V map parameters and hyperparameters to training and validation losses. The goal is to solve a single-level problem by approximating the best-response w * with a parametric function \u0175 \u03c6. This approach offers faster convergence compared to black-box methods but faces challenges in finding a scalable approximation for neural network weights. Finding a scalable approximation for neural network weights is a significant challenge. A compact approximation can be constructed by modeling the best-response of each row in a layer's weight matrix/bias as a rank-one affine transformation of the hyperparameters. This approach, known as Self-Tuning Networks (STNs), updates hyperparameters online during training and offers advantages over other optimization methods. Self-Tuning Networks (STNs) update hyperparameters online during training, offering advantages over other optimization methods. They are easy to implement and adapt hyperparameters online, leading to improved performance compared to fixed settings. STN training algorithm does not require differentiating the training loss with respect to hyperparameters, allowing tuning of discrete hyperparameters. The bilevel optimization problem involves two sub-problems, the upper-level and lower-level problems, where the upper-level problem must be solved subject to optimality of the lower-level problem. Bilevel programs were first studied in economics to model leader/follower firm dynamics and have since found uses in various fields. In economics, bilevel programs model leader/follower dynamics. In machine learning, they are used for hyperparameter optimization, GAN training, meta-learning, and neural architecture search. Bilevel problems are NP-hard, and most work focuses on linear, quadratic, and convex functions. This study aims to find local solutions in nonconvex, differentiable, and unconstrained settings. The text discusses solving a bilevel optimization problem using a gradient-based algorithm. Simultaneous gradient descent is not ideal due to the dependence of parameters. The best-response function is proposed as a more principled approach to finding optimal solutions. The text introduces the best-response function as a method to solve a bilevel optimization problem by converting it into a single-level problem. Conditions for the uniqueness of the optimum solution are discussed, along with a lemma providing sufficient conditions for differentiability in a neighborhood of a given point. The gradient of F* decomposes into direct and response gradients, stabilizing optimization by converting the bilevel problem into a single-level one. This ensures a conservative gradient vector field, avoiding pathological issues. The solution to Problem 4b is a set, but assuming uniqueness and differentiability of w* can lead to practical algorithms. Gradient-based hyperparameter optimization methods approximate the best-response w* or its Jacobian \u2202w*/\u2202\u03bb. However, these methods can be computationally expensive and struggle with discrete and stochastic hyperparameters. Lorraine & Duvenaud (2018) proposed promising approaches to directly approximate w*. Lorraine & Duvenaud (2018) proposed algorithms for approximating w* in Problem 5. The first algorithm uses a global approximation with a differentiable function \u0175 \u03c6, while the second algorithm employs a local approximation around the upper-level parameter \u03bb. The approach involves perturbing the upper-level parameter \u03bb and minimizing an objective function to find \u03c6. An alternating gradient descent scheme is used, showing success with L2 regularization on MNIST. Challenges include handling high-dimensional w, setting \u03c3, and adapting to different regularizers or scales. A memory-efficient best-response approximation \u0175 \u03c6 is constructed in this section. In this section, a best-response approximation \u0175 \u03c6 is constructed for large neural networks. The algorithm automatically adjusts the scale of the neighborhood \u03c6 is trained on and handles discrete and stochastic hyperparameters. The resulting networks, called Self-Tuning Networks (STNs), update their own hyperparameters online during training. The best-response for a layer's weight matrix and bias is approximated as an affine transformation of the hyperparameters. The architecture computes elementwise multiplication and row-wise rescaling, incorporating a correction for hyperparameters. It is memory-efficient and enables parallelism, improving sample efficiency in batch processing. The best-response approximation requires a specific number of parameters to represent weights and biases, allowing for online adjustment of hyperparameters in Self-Tuning Networks. In this section, a model is presented where the best-response function can be exactly represented using a linear network with Jacobian norm regularization. The network's hidden units are modulated based on the hyperparameters, utilizing a 2-layer linear network to predict targets from inputs. The model presented utilizes a 2-layer linear network with modulated hidden units based on hyperparameters. It predicts targets from inputs using weights w = (Q, s) \u2208 R D\u00d7D \u00d7 R D. The squared-error loss is regularized with an L2 penalty on the Jacobian \u2202y /\u2202x, with a penalty weight \u03bb in R+. The architecture shown in FIG0 uses gating of hidden units to approximate best-response for deep, nonlinear networks. Linear gating can replace sigmoidal gating for a narrow hyperparameter range, allowing for an affine approximation of the best-response function. This approach ensures correct best-response Jacobian for quadratic lower-level objectives. The architecture in FIG0 uses gating of hidden units to approximate best-response for deep, nonlinear networks. Minimizing E \u223cp( |\u03c3) [f (\u03bb + ,\u0175 \u03c6 (\u03bb + ))] ensures gradient descent on the objective F (\u03bb,\u0175 \u03c6 (\u03bb)) converges to a local optimum. The sampled neighborhood size affects the approximation quality, with \u03c3 controlling the hyperparameter distribution scale for \u03c6 training. The entries of \u03c3 control the scale of the hyperparameter distribution on which \u03c6 is trained. Adjusting \u03c3 during training based on the sensitivity of the upper-level objective to the sampled hyperparameters is proposed. An entropy term weighted by \u03c4 \u2208 R + enlarges the entries of \u03c3 in the objective function. The objective is to interpolate between variational optimization and variational inference by balancing the first term to avoid heavy entropy penalties. Performance evaluation is done at the deterministic current hyperparameter \u03bb 0. The STN training algorithm can tune hyperparameters that other gradient-based algorithms cannot, such as discrete or stochastic hyperparameters. It uses an unconstrained parametrization \u03bb \u2208 R n and involves a non-differentiable discretization for discrete hyperparameters. STNs are trained by a gradient descent scheme alternating between updating \u03c6 for T train steps. The STN training algorithm tunes hyperparameters, including discrete ones, using an unconstrained parametrization \u03bb \u2208 R n. It involves a non-differentiable discretization for discrete hyperparameters and is trained by updating \u03c6 for T train steps. The algorithm can be implemented in code as shown in Appendix G. The STN training algorithm tunes hyperparameters using an unconstrained parametrization \u03bb \u2208 R n. For regularization schemes, the reparametrization gradient is used if L V does not depend on \u03bb i directly. If L V explicitly relies on \u03bb i, the REINFORCE gradient estimator is used. This approach is needed for hyperparameters like the number of hidden units in a layer. The method was applied to convolutional networks and LSTMs, resulting in self-tuning CNNs (ST-CNNs) and self-tuning LSTMs (ST-LSTMs). The STN training algorithm tunes hyperparameters using an unconstrained parametrization \u03bb \u2208 R n. It was applied to convolutional networks and LSTMs, resulting in self-tuning CNNs (ST-CNNs) and self-tuning LSTMs (ST-LSTMs). STNs discover schedules for adapting hyperparameters online, outperforming fixed values. STNs were compared to other optimization methods on CIFAR-10 and PTB datasets, showing superior performance. An ST-LSTM was used to tune the output dropout rate on the PTB corpus. The ST-LSTM used an ST-LSTM to tune the output dropout rate on the PTB corpus, outperforming fixed rates with a discovered schedule. The improved performance was not due to stochasticity from sampling hyperparameters, as shown by comparison with perturbation methods on a standard LSTM. The ST-LSTM outperformed sinusoid perturbations and fixed dropout rates on the PTB word-level language modeling task and CIFAR-10 image-classification task. Training a standard LSTM from scratch using the schedule for output dropout discovered by the ST-LSTM showed similar performance to the STN, indicating the importance of the hyperparameter schedule. Using the final dropout value found by the STN did not yield as good results as following the schedule. The STN schedule implements a curriculum by using a low dropout rate early in training, aiding optimization, and then gradually increasing the dropout rate for better generalization. Hyperparameters adapt faster than weights, leading to equilibration during training. Low regularization is best early on, while higher regularization is better later. ST-LSTM performed well on the PTB corpus, showcasing the importance of hyperparameter schedules. The ST-LSTM was evaluated on the PTB corpus with a 2-layer LSTM and 650 hidden units per layer. 7 hyperparameters were tuned, including variational dropout rates and coefficients for activation regularization. STNs were compared to grid search, random search, and Bayesian optimization. The experimental setup and hyperparameter roles are detailed in Appendix D. STNs were compared to other optimization methods, showing superior performance in achieving lower validation perplexity. The schedules for each hyperparameter in STNs are nontrivial, with varying dropout forms used throughout training. ST-CNNs were evaluated on the CIFAR-10 dataset using the AlexNet architecture, with hyperparameter tuning. ST-CNNs were evaluated on the CIFAR-10 dataset using the AlexNet architecture. Hyperparameters were tuned, including activation dropout, input dropout, scaling noise, data augmentation parameters, and noise levels for hue, saturation, brightness, and contrast. STNs outperformed grid search, random search, and Bayesian optimization in finding optimal configurations. The experimental setup details are provided in Appendix E. The final validation and test losses for each method are shown in FIG2. STNs find better hyperparameter configurations in less time. Bilevel Optimization involves solving problems with linear, quadratic, or convex constraints. Our work resembles trust-region methods and evolutionary techniques used by Sinha et al. Hypernetworks were first considered by Schmidhuber in 1993. In related work, Sinha et al. (2013) used evolutionary techniques for estimating best-response functions iteratively. Hypernetworks, introduced by Schmidhuber in 1993, map to neural net weights. Ha et al. (2016) applied hypernetworks to generate weights for CNNs and RNNs. BID6 used hypernetworks for global best-response approximation in architecture search. Gradient-Based Hyperparameter Optimization involves approximating w * (\u03bb 0 ) using w T (\u03bb 0 , w 0 ) after T steps of gradient descent on f with respect to w. Gradient-Based Hyperparameter Optimization involves approximating w * (\u03bb 0 ) using w T (\u03bb 0 , w 0 ) after T steps of gradient descent on f with respect to w. Two approaches are discussed: one proposed by BID13 and used by various researchers, and another using the Implicit Function Theorem under certain conditions. These approaches have been applied in hyperparameter optimization for neural networks, log-linear models, kernel selection, and image reconstruction, but struggle with certain hyperparameters. Model-Based Hyperparameter Optimization involves using Bayesian optimization to model the conditional probability of performance given hyperparameters and dataset. The process iteratively constructs a dataset and selects the next hyperparameters to train on by maximizing an acquisition function. This approach balances exploration and exploitation without the need to train each model to completion. Model-Based Hyperparameter Optimization involves selecting the next hyperparameters to train on by maximizing an acquisition function that balances exploration and exploitation. Model-free approaches like random search and Successive Halving extend random search by adaptively allocating resources to promising configurations using multi-armed bandit techniques. These methods ignore structure in the network when used for hyperparameter optimization. Population Based Training (PBT) considers schedules for hyperparameters by training a population of networks in parallel. The under-performing networks are replaced by better-performing ones, with their hyperparameters copied and randomly perturbed for training the new network clone. This allows a single model to experience different hyperparameter settings over the course of training, implementing a schedule. Self-Tuning Networks (STNs) efficiently approximate the best-response of parameters to hyperparameters by scaling and shifting hidden units. They use gradient-based optimization to tune various regularization hyperparameters, including discrete ones, during a single training run. STNs discover hyperparameter schedules that outperform fixed hyperparameters, achieving better generalization performance in less time on large-scale problems. STNs offer a compelling path towards automated hyperparameter tuning for neural networks. Self-Tuning Networks (STNs) offer a compelling path towards large-scale, automated hyperparameter tuning for neural networks, achieving better generalization performance in less time. The parameters respond to hyperparameters through direct and response gradients, leading to improved optimization. The Jacobian of \u2202f /\u2202w decomposes as a block matrix with sub-blocks. f is C 2 in a neighborhood of (\u03bb 0 , w 0 ), so \u2202f /\u2202w is continuously differentiable. The Hessian \u2202 2 f /\u2202w 2 is positive definite and invertible at (\u03bb 0 , w 0 ). By the Implicit Function Theorem, there exists a unique continuously differentiable function w * : V \u2192 R m such that \u2202f /\u2202w(\u03bb, w * (\u03bb)) = 0 for \u03bb \u2208 V and w * (\u03bb 0 ) = w 0. This discussion is based on Hastie et al. (2001). The SVD decomposition of the data matrix X is discussed, leading to a simplified function y(x; w) and the optimal solutions for regularized and unregularized versions of the problem are presented. The optimal solution to the unregularized version of Problem 19 is given by u* = D^-1 U^t, where Q0 = V and s0 solves the regression problem. Best-response functions Q*(\u03bb) and s*(\u03bb) are chosen as \u03c3(\u03bbv + c) row Q0 and s0, meeting the criteria. The function f is quadratic, with matrices A, B, C, vectors d, e. The function f is quadratic with matrices A, B, C, vectors d, e. By assuming \u2202 2 f /\u2202w 2 0, we find the derivative equal to 0 using second-order sufficient conditions. Substituting and simplifying, we get equation 36. Simplifying further using linearity of expectation, we differentiate f. The function f is quadratic with matrices A, B, C, vectors d, e. By assuming second-order sufficient conditions, we find the derivative equal to 0. Differentiating f using matrix-derivative equalities, we obtain the best-response Jacobian. The model parameters were updated, but hyperparameters were not. Training was terminated when the learning rate dropped below 0.0003. Variational dropout was tuned on the input to the LSTM. We tuned variational dropout on the input, hidden state, and output of the LSTM. We also adjusted embedding dropout and regularized the hidden-to-hidden weight matrix using DropConnect. Activation regularization penalizes large activations, while temporal activation regularization promotes slowness. For the CNN experiments, 20% of the training data was held out for validation. The baseline CNN was trained using SGD with initial learning rate 0.01 and momentum 0.9 on mini-batches of size 128. The learning rate was decayed by 10 when the validation loss didn't decrease for 60 epochs, and training ended if the learning rate fell below 10^-5 or validation loss didn't decrease. The ST-CNN's hyperparameters were optimized using Adam with a learning rate of 0.003. Training alternated between the best-response approximation and hyperparameters following the same schedule as the ST-LSTM. Five epochs of warm-up were used for model parameters. The ST-CNN hyperparameters were optimized using Adam with a learning rate of 0.003. Training involved 2 steps on the training set and 1 step on the validation set. Entropy weight was set to \u03c4 = 0.001, cutout length ranged from 0 to 24, and number of cutout holes ranged from 0 to 4. Dropout rates and data augmentation noise parameters were initialized to 0.05. The model was found to be robust to hyperparameter initialization, with low regularization aiding optimization in the early epochs. Connections were drawn between hyperparameter schedules and curriculum learning. In the early epochs, hyperparameter schedules were connected to curriculum learning, where optimization starts with simpler criteria and gradually increases difficulty. This method is hypothesized to aid optimization and generalization. Hyperparameter schedules can implement curriculum learning by adjusting parameters like dropout over time to make the learning problem more challenging. Hyperparameter schedules can implement curriculum learning by adjusting parameters like dropout over time to make the learning problem more challenging. Grid searches help understand the effects of different hyperparameter settings, showing that greedy schedules can outperform fixed values. Validation perplexity is measured for different dropout combinations, with smaller values being better at the start and larger values later on. Greedy hyperparameter schedules show potential benefits. The benefits of greedy hyperparameter schedules are demonstrated through a fine-grained grid search for dropout values. By adjusting dropout rates over epochs, better validation performance is achieved. Using smaller dropout values initially leads to a fast decrease in perplexity, while larger values later on result in overall improved validation perplexity. This approach outperforms fixed hyperparameter values from the initial grid search. The PyTorch code listings for constructing ST-LSTMs and ST-CNNs, including the HyperLinear and HyperConv2D classes, are provided in this section along with optimization steps for the training and validation sets. The improved performance of STNs is attributed to the regularization effect rather than the schedule, as shown in the perturbed values for output dropout in FIG10."
}