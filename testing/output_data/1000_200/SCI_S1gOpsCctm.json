{
    "title": "S1gOpsCctm",
    "content": "Recurrent neural networks (RNNs) are used for control policies in reinforcement and imitation learning. A new technique called Quantized Bottleneck Insertion is introduced to learn finite representations of memory vectors and observation features in RNN policies. This allows for better analysis and understanding of memory use and behavior. Results on synthetic environments and Atari games show that the finite representations can be surprisingly small, with as few as 3 memory states and 10 observations for a perfect Pong policy. This approach leads to improved interpretability in deep reinforcement learning and imitation learning. In this paper, the authors introduce a technique called Quantized Bottleneck Insertion to learn compact memory representations in RNN policies. This approach aims to improve the interpretability of deep reinforcement learning and imitation learning models. The results show that finite memory representations can be small, with as few as 3 memory states and 10 observations for a perfect Pong policy. The paper introduces Quantized Bottleneck Insertion to learn compact memory representations in RNN policies, aiming to improve interpretability. It hypothesizes that continuous memory captures discrete concepts, motivating quantization for better understanding. The approach transforms RNN policies to capture powerful memory usage in a more direct way. The approach introduces Quantized Bottleneck Insertion to transform RNN policies into Moore Machine Networks with quantized memory and observations, improving interpretability and capturing powerful memory usage in a direct way. The approach introduces Quantized Bottleneck Insertion to transform RNN policies into Moore Machine Networks with quantized memory and observations, improving interpretability and capturing powerful memory usage in a direct way. Training quantized networks is challenging, but a simple approach using \"straight through\" gradient estimators is effective. Experiments in synthetic domains and benchmark grammar learning problems show accurate extraction of MMNs, providing insights into RNN memory use. Additionally, experiments on 6 Atari games using RNNs achieve state-of-the-art performance, with near-equivalent and surprisingly small MMNs extracted. Our work introduces Quantized Bottleneck Insertion to transform RNN policies into Moore Machine Networks with quantized memory and observations, improving interpretability and capturing powerful memory usage. The extracted MMNs provide insights into RNN memory use, revealing cases of purely reactive control and open-loop controllers. This work is related to learning finite-state representations of recurrent neural networks, with experiments showing accurate extraction of MMNs and achieving state-of-the-art performance in Atari games. Our work is related to learning finite-state representations of recurrent neural networks. Previous approaches involve extracting Finite State Machines (FSMs) from recurrent networks trained to recognize languages, but these methods do not directly apply to learning policies. Our approach involves inserting discrete elements into the RNN to create Moore Machines, providing a more accurate representation of RNN behavior. Our approach involves directly inserting discrete elements into the RNN to create Moore Machines, allowing for fine-tuning and visualization using standard learning frameworks. This method transforms any pre-trained recurrent policy into a finite representation, extending previous work on learning FSMs from recurrent networks. Our approach involves inserting discrete elements into RNNs to create Moore Machines for interpretability. Unlike prior work on binary networks, we focus on discrete representations of memory and observations while allowing arbitrary activations and weights. RNNs are commonly used in reinforcement learning for policies requiring internal memory. Our approach involves inserting discrete elements into RNNs to create Moore Machines for interpretability. RNNs are commonly used in reinforcement learning for policies requiring internal memory, where an RNN maintains a continuous-valued hidden state that influences action choice based on observation features extracted from the current observation. The transition to a new state is determined by a transition function implemented via gating networks like LSTMs or GRUs. The high-dimensional nature of the hidden state and observation features can make interpreting memory challenging, motivating the goal of extracting compact quantized representations. Our goal is to extract compact quantized representations of hidden states and observations in RNNs by introducing Moore Machines. Moore Machines are finite state machines labeled by output values, corresponding to actions, with hidden states, observations, actions, transition functions, and policies. The transition function determines the next hidden state based on the current state and observation. A Moore Machine Network (MMN) uses deep networks to represent the transition function and policy, mapping continuous observations to a finite discrete space. It considers quantized state and observation representations with discrete vectors for hidden states and observations. An MMN is like a traditional RNN with memory composed of k-level activation units and environmental observations transformed to a k-level representation before being fed to the recurrent module. Learning MMNs from scratch can be challenging for non-trivial problems, even though RNNs can be learned easily. Training high-performing MMNs from scratch has proven difficult. Training high-performing MMNs from scratch for complex problems like Atari games is challenging. A new approach involves using trained RNNs to learn quantized bottleneck networks (QBNs) for embedding continuous features into a k-level quantized representation. These QBNs are inserted into the original recurrent net with minimal behavior changes, resulting in a network that consumes quantized features and maintains quantized state, effectively creating an MMN. A QBN is an autoencoder with a constrained k-level activation bottleneck for discretizing a continuous space. It involves a multilayer encoder E mapping inputs to a latent encoding E(x) and a decoder D. The QBN output is quantized using 3-level quantization of +1, 0, and -1. The QBN involves a multilayer encoder E and a decoder D, with the output quantized using 3-level quantization. To support 3-valued quantization, a specific activation function is used. The introduction of the quantize function makes b(x) non-differentiable, but the straight-through estimator is effective in addressing this issue. The straight-through estimator effectively treats the quantize function as the identity during back-propagation in the QBN. Training involves using L2 reconstruction error for autoencoder training. Recurrent policy generates training sequences for observation, observation feature, and hidden state. Two QBNs, b f and b h, are trained on observed features and states respectively. The approach involves training two QBNs, b f and b h, on observed features and states respectively. The QBNs act as \"wires\" that propagate input to output in the RNN, with some noise due to imperfect reconstruction. Inserting these wires into the RNN allows it to be viewed as an MMN, with b f between feature computation units and b h between output and input to the recurrent block. The RNN can be viewed as an MMN with bottlenecks providing a quantized representation of features and states. Fine-tuning the MMN by training on original RNN data helps match the softmax distribution over actions. Training in this way is more stable than simply training the MMN. The MMN is trained to match the softmax distribution over actions produced by the RNN, which is more stable than training it to output the same action. Visualization tools can be used to analyze the memory and its feature bits for a semantic understanding. Another approach is to create a Moore Machine over atomic state and observation spaces using the MMN, allowing for analysis of different machine states and their relationships. The Moore Machine is created by running the learned MMN to produce a dataset of consecutive pairs of quantized states, quantized features, and actions selected. The state-space corresponds to distinct quantized states, while the observation-space is unique quantized feature vectors. The transition function is constructed from the data to create a transaction table capturing transitions. Minimization techniques are applied to arrive at a minimal Moore Machine, reducing the number of distinct states. In this section, standard Moore Machine minimization techniques are applied to reduce the number of distinct states and observations in the machine. The experiments aim to extract MMNs from RNNs without significant loss in performance and investigate the interpretability of recurrent policies. Two domains are considered: a synthetic environment called Mode Counter and benchmark grammar learning problems. These environments allow for varying levels of memory required by a policy. The Mode Counter Environments (MCEs) allow for varying memory requirements in policy learning. MCEs are a type of Partially Observable Markov Decision Process with transitions between modes, each requiring different memory for past events and internal counters. Agents must infer the mode through observations and memory use to receive a reward for taking the correct action associated with the active mode. The Mode Counter Environments (MCEs) require inferring the mode through observations and memory use to achieve optimal performance. Three MCE instances test different memory usage types: 1) Amnesia - optimal actions based on current observation without memory, 2) Blind - memory needed to track mode sequence for optimal actions, 3) Tracker - memory use for determining optimal actions. The Mode Counter Environments (MCEs) require memory to track mode sequences for optimal actions. The MCE instances use a recurrent architecture with feed-forward and GRU layers to achieve 100% accuracy on the imitation dataset. In the MCE environments, trained RNNs achieve 100% accuracy on the imitation dataset and produce optimal policies. QBN training with bottleneck sizes of Bf \u2208 {4, 8} and Bh \u2208 {4, 8} was fast compared to RNN training, as QBNs do not need to learn temporal dependencies. QBNs with bottleneck sizes of Bf \u2208 {4, 8} and Bh \u2208 {4, 8} were embedded into RNNs to create discrete MMNs. Performance of the MMNs was measured before and after fine tuning, with most cases not requiring fine tuning due to low reconstruction error. Fine-tuning resulted in perfect MMN performance in all cases except for Tracker (Bh = 4, Bf = 4), which achieved 98% accuracy. Inserting one bottleneck at a time yielded perfect performance, indicating that combined error accumulation of both bottlenecks reduced performance. After fine-tuning, most MMNs achieved perfect performance except for Tracker. Inserting one bottleneck at a time showed that combined error accumulation reduced performance. Moore Machine Extraction revealed more states and observations before minimization, indicating MMN learning does not always result in minimal representations. However, after minimization, exact minimal machines were obtained for each MCE domain. After fine-tuning, most MMNs achieved perfect performance except for Tracker. Inserting one bottleneck at a time showed that combined error accumulation reduced performance. However, in all but one case, exact minimal machines were obtained for each MCE domain after minimization. The MMNs learned via QBN insertions were equivalent to the true minimal machines, showing optimal performance. The exception occurred when the MMN did not achieve perfect accuracy. Examining these machines allows understanding of memory use and how the policies' actions are determined by the current observation. In evaluating our approach over 7 Tomita Grammars, each grammar defines binary strings for acceptance or rejection. The focus is on policy learning problems, treating grammars as environments with 'accept' and 'reject' actions. The RNN for each grammar consists of a one-layer GRU with 10 hidden units and a fully connected softmax layer with 2 nodes. Imitation learning is used to train each RNN with an Adam optimizer and learning rate of 0.001. The training dataset includes an equal number of accept/reject strings with lengths uniformly sampled. The RNNs were trained using the Adam optimizer with a learning rate of 0.001 on a dataset of accept/reject strings. Test results showed high accuracy, except for grammar #6. MMNs were created without a bottleneck encoder, with bottleneck b h for hidden memory state. Experiments were conducted with B h values of 8 and 16, and performance before and after fine-tuning is shown in TAB1. MMNs were able to maintain RNN performance in most cases. The performance of Moore Machine Networks (MMNs) before and after fine-tuning is shown in TAB1, where MMNs were able to maintain RNN performance in most cases. Results for MM extraction and minimization in reducing state-space while maintaining performance are also presented in TAB1. The technique applied to RNNs does not directly result in minimal machines but achieves equivalent results. In this section, the technique is applied to RNNs learned for six Atari games using the OpenAI gym. Unlike previous experiments, the ground truth minimal machines were not known for Atari, making it unclear what to expect due to the complexity of input observations. While other efforts have been made to understand Atari agents, this work focuses on extracting finite state representations for Atari policies. All Atari agents have the same recurrent architecture, with input observations preprocessed as image frames. The RNN training for Atari agents involves preprocessing input observations as image frames, using a specific recurrent architecture with convolutional and GRU layers, and applying the A3C RL algorithm for training. The performance of the trained RNN is reported on six games. The trained RNN performance on six games using Generalized Advantage Estimation (\u03bb = 1.0) BID20 is reported in TAB2. The encoder and decoder sizes were adjusted to match the dimension of continuous observation features. Training data for bottlenecks was generated using noisy rollouts to increase diversity and robust learning of QBNs. Bottlenecks were trained for B h \u2208 {64, 128} and B f \u2208 {100, 400}. The training data for bottlenecks was generated using noisy rollouts to increase diversity and robust learning of QBNs. Bottlenecks were trained for B h \u2208 {64, 128} and B f \u2208 {100, 400}, with values significantly larger than earlier experiments due to the complexity of Atari. Each bottleneck was trained to saturation of training loss and inserted into the RNN to create an MMN for each Atari game. MMN performance was evaluated before and after fine-tuning, showing that for 4 games, the MMNs achieved identical or very close scores to the RNN. The MMNs achieved similar scores to the RNN for Pong, Freeway, Bowling, and Boxing after fine-tuning, demonstrating the ability to learn complex games. However, for Breakout and Space Invaders, the MMNs scored lower due to poor reconstruction in certain game scenarios. After clearing the first board, the policy fails to press the fire-button, resulting in a lower score. This prompts the need for more intelligent training approaches for QBNs to capture critical information in rare states. Minimization of MMNs reduces the number of states and observations significantly, making them easier to analyze manually. This analysis may be challenging for complex policies. Understanding Memory Use. In Atari, three types of memory use were observed for Pong, Bowling, and Freeway. Pong's policy has three states and 10 observation symbols, transitioning to the same state regardless of the current state. Bowling and Freeway have only one observation symbol in the minimal Markov Model, simplifying analysis. The policy in Pong is similar to the Amnesia MCE, while Bowling and Freeway have open-loop policies that ignore input images. Freeway always takes the Up action, and Bowling has a structured initial action sequence followed by a repeated loop. The MM extraction approach successfully identified these policy behaviors. The MM extraction approach successfully identified policy behaviors in Atari games like Breakout, Space Invaders, and Boxing, which use memory and observations. Future work will involve semantic analysis and visualization tools to better understand memory use in RNN policies. The approach involves training Quantized Bottleneck Networks to produce binary encodings of continuous RNN memory and input features. The approach involves training Quantized Bottleneck Networks to produce binary encodings of continuous RNN memory and input features, and inserting those bottlenecks into the RNN to extract a discrete Moore machine. Results show accurate extraction of ground truth in known environments and similar performance to original RNN policies in Atari games, providing insight into memory usage. The machines provide insight into memory usage of policies, revealing small memory states and observations. Different policies show varying reliance on memory and observations. Future work includes developing tools for interpreting observations and states, analyzing finite-state machine structure, and parameterizing MCEs. The MCE is defined by mode number M, transition function P, lifespan mapping \u2206(m), and count set C. The hidden state is (m t , c t ), where m t is the current mode and c t is the consecutive time-steps in mode m t. Mode changes occur when lifespan is reached, with the next mode generated by transition distribution P. The agent receives continuous-valued observations o t based on the current state. The MCE involves continuous-valued observations o t based on the current state (m t , c t ). Observations determine the mode when the mode count is in C, requiring the agent to remember the current mode and track how long it has been active. Experiments are conducted with different MCE instances, such as Amnesia with \u2206(m) = 1 for all modes and C = {0}, where an optimal policy does not use memory to track past information. The curr_chunk discusses different scenarios to test the ability to use MMN extraction in determining if a policy is reactive or uses memory. It includes tests with random initial distributions, deterministic distributions, and larger mode life spans. The goal is to see if an optimal policy can be achieved by using memory or ignoring observations. The most general instance of the environment involves using memory to keep track of modes and mode count, which can result in challenging problems as the number of modes and their life-spans increase. Accept and reject states are denoted by 'A' and 'R', respectively, with machines being 100% accurate except for Grammar 6."
}