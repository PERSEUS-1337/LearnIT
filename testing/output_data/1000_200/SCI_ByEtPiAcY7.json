{
    "title": "ByEtPiAcY7",
    "content": "Knowledge extraction techniques aim to make neural networks more understandable by converting them into symbolic descriptions. The challenge is to find explanations that are simpler than the original model but still accurate. Some believe that the complexity of deep networks makes it impossible to explain hidden features in a way that humans can understand. However, a new method using \\textit{M-of-N} rules shows promise in balancing complexity and accuracy in explaining hidden features in Convolutional Neural Networks (CNNs). The experiments in this paper reveal an optimal trade-off between comprehensibility and accuracy in hidden features of a Convolutional Neural Network (CNN). Each latent variable follows an optimal \\textit{M-of-N} rule for behavior description. Rules in the first and final layers are highly explainable, while those in the second and third layers are less so. This sheds light on rule extraction from deep networks and the importance of decompositional knowledge extraction for explainability in AI. In the past decade, there have been significant advancements in neural network models, particularly deep networks that rely on distributed representations for accurate data modeling. Unlike symbolic AI or symbolic Machine Learning, distributed representations do not necessarily correlate with easily identifiable data features. Knowledge extraction aims to enhance the explainability of neural networks by uncovering implicit knowledge in their weights. Knowledge extraction in neural networks aims to increase explainability by translating them into symbolic rules or decision trees. Rule extraction techniques have been developed over decades, using either decompositional or pedagogical approaches. The main challenge lies in the complexity of the extracted rules. The complexity of extracted rules from neural networks is a challenge due to distributed representations found in the networks. Important concepts are not always represented by single neurons but by patterns of activity over many neurons, leading to difficulties in knowledge extraction. In this paper, a method for empirically examining the explainability of latent variables in neural networks is developed. Rule extraction is used by searching through a space of M-of-N rules to describe a latent variable, measuring the error and complexity of each. The study explores the explainability of latent variables in neural networks using rule extraction to find accurate rules for each variable. Different error/complexity trade-offs reveal a landscape of rule extraction, showing varying levels of accuracy in describing network behavior. A 'critical point' indicates an ideal rule for each variable, with explainability trends differing between layers and architectures. The study delves into the explainability of latent variables in neural networks through rule extraction, revealing varying levels of accuracy in describing network behavior. Different error/complexity trade-offs show that rules in convolutional layers are more complex compared to fully connected layers. Rules in the first and final layers have near 0% error, while those in the second and third layers have around 15% error. The paper also provides an overview of previous algorithms used for knowledge extraction and outlines the experimental results of the rule extraction process. The study explores rule extraction for neural networks, focusing on accuracy and complexity of M-of-N rules. Previous algorithms like KBANN used hidden variable weights to extract symbolic rules. More recent algorithms generate binary trees representing M-of-N rules, which can be reduced to propositional logic sentences. These algorithms aim to select an M-of-N configuration for better interpretability. The more recent algorithms for rule extraction in neural networks focus on selecting an M-of-N rule based on maximum information gain. These methods treat the model as a black box and do not explain latent variables, allowing for rule extraction through querying the model. Other eclectic methods combine pedagogical and decompositional approaches, while some opt for visually oriented techniques. Various methods have been developed to address the 'black-box' problem of neural networks. The curr_chunk discusses the challenges of using decompositional techniques to explain the features of deep neural networks. It highlights the complexity that arises from multiple hidden layers and the impracticality of producing a hierarchy of rules to explain arbitrary hidden features. Despite these challenges, experiments in the paper show some promising results. The curr_chunk discusses the potential of rule extraction as a tool for explaining deep network models, highlighting the explainability of certain layers and extracted rules. It introduces the formal definition of logical rules in programming. The curr_chunk explains the formal definition of logical rules in programming, where a rule is of the form A \u2190 B, with A as the head and B as the body. It discusses how rules can be used to explain neural networks by defining literals based on the states of neurons, whether binary or continuous. In neural networks, latent variables are described by M-of-N rules, which soften the conjunctive constraint on logical rules. M-of-N rules offer a compact representation reflecting input/output dependencies in a neural network fan-in. M-of-N rules provide a compact representation of input/output dependencies in neural networks and share structural similarity with neural networks, acting as 'weightless perceptrons'. They cannot represent all propositional formulas, such as XOR, and offer a different approach to rule extraction compared to a simple lookup table. This paper reintroduces M-of-N rules for explainability in neural networks, focusing on setting bias and weights for input neurons. Continuous activation values require choosing splitting values for rule extraction based on information gain. The goal is to generate literals for target neurons by selecting splits that maximize information gain with respect to output labels. The paper reintroduces M-of-N rules for explainability in neural networks, focusing on setting bias and weights for input neurons. The goal is to generate literals for target neurons by selecting splits that maximize information gain with respect to output labels. Input literals are then generated by choosing splits that maximize information gain with respect to the target neuron. Each target literal in a layer will have its own set of input literals, corresponding to the same set of input neurons but with different splits. In convolution layers, each feature map corresponds to a group of neurons with different input patches. Only the neuron with the optimal split that has the maximum information gain with respect to the network output is tested, resulting in a single rule for each feature map. In rule extraction from neural networks, accuracy and comprehensibility are key metrics. Accuracy is defined in terms of the expected difference between rule predictions and network outputs. By using the network to compute neuron states from input states, we can determine the truth of rules. This process involves using rules to determine variable values based on input states. When extracting rules from neural networks, accuracy and comprehensibility are crucial. Accuracy is measured by comparing rule predictions to network outputs. By using input states to determine variable values, we can assess the discrepancy between rule and network outputs. Comprehensibility is evaluated based on the complexity of a rule, determined by the length of its body in disjunctive normal form. The complexity of a rule is measured by the length of its body in disjunctive normal form (DNF). The normalized complexity measure is calculated by normalizing with respect to a maximum complexity. An example is given with a perceptron and a rule h = 1 \u21d0\u21d2 1-of{x 1 = 1, \u00ac(x 2 = 1)}. The most complex rule for 2 variables is a 1 \u2212 of \u2212 2 rule. The complexity of a rule is determined by the length of its body in disjunctive normal form (DNF). A parameter \u03b2 in the loss function balances soundness and complexity. By varying \u03b2, the relationship between rule complexity and accuracy can be explicitly determined. For \u03b2 = 0, the rule with minimum loss is the one with minimum error, while for large \u03b2, the rule with minimum loss has 0 complexity. Trivial rules like always predicting true or false are examples of rules with 0 complexity. When generating rules for neurons, splits are created based on input neuron weights. The search for M-of-N rules minimizes L(R) by considering the magnitude of the weights. The search procedure relies on the ordering of variables X_i to generate splits for neuron h. The search procedure for generating M-of-N rules relies on the ordering of variables X_i. Splits are created based on input neuron weights to define literals for each neuron. The assumption is made that accurate rules use literals corresponding to neurons with the strongest weights. This assumption is justified by the conditional independence of hidden units in all layers except the final one. The search procedure for generating M-of-N rules relies on ordering variables X_i based on input neuron weights to define literals. The conditional independence of hidden units justifies using literals corresponding to neurons with the strongest weights. Experimental results show high accuracy when ordering literals by weight, reducing the search space to a polynomial one. To handle large datasets, the algorithm was implemented in Spark on IBM cloud services for efficient rule extraction. The algorithm for rule extraction was implemented in Spark on IBM cloud services to extract rules accurately. The accuracy of the rules was evaluated using examples from the training set, with a focus on the network's output. By running the search in parallel, the accuracy/complexity graph for 50 hidden neurons could be mapped in a few hours. The number of examples used in the accuracy calculation was limited to 1000 to reduce processing time. The extraction process for the first hidden feature in a CNN trained on the fashion MNIST dataset was demonstrated using 1000 random input examples from the training set. In the extraction process for the first hidden feature in the CNN trained on the fashion MNIST dataset, 1000 random input examples are used to compute neuron activations and predicted labels. Neuron 96 with an information gain of 0.015 is selected based on optimal splitting value. This neuron corresponds to the image patch centered at (3, 12). Neuron 96 with an information gain of 0.015 is selected based on optimal splitting value at (3, 12). Input splits are defined to maximize information gain with respect to variable H. M-of-N rules are searched to explain H for error/complexity tradeoffs, resulting in three different rules visualized in Figure 1. The first image shows neuron weights and three rules of decreasing complexity. A 5-of-13 rule with 0.025 error agrees with the network 97.5% of the time. Adding penalties for complexity results in simpler rules with higher errors. Applying the technique to the DNA promoter dataset demonstrates the results on a simple network. In a study on a simple network suitable for rule extraction, a feed forward network with 100 nodes in a hidden layer was trained on the DNA promoter dataset. The relationship between complexity and error in the first layer was found to be exponential, indicating an optimal tradeoff. A rule in the output layer achieved 100% fidelity to the network. Rules for the hidden layer were defined based on information gain with respect to the output, resulting in M-of-N rules extracted from the input layer with no complexity penalty. The network's output can be predicted by hierarchical rules, with errors propagating through layers due to different splits chosen for the same layer. To replace the network with a set of hierarchical rules, a single set of splits for each layer must be decided by moving down one layer at a time and selecting input splits based on the context. In order to provide an idealized complexity/error curve for rule extraction with M-of-N rules, experiments are conducted layer by layer independently. This approach allows for examining the rule extraction landscape of a neural network trained on a practical example, such as a basic CNN on fashion MNIST in tensorflow. The CNN architecture includes a 32 feature convolutional layer with a 5 \u00d7 5 filter. The layerwise rule extraction search was tested on a basic CNN trained on fashion MNIST in tensorflow. The CNN had a standard architecture with 32 and 64 feature convolutional layers followed by max pooling layers and a final hidden fully connected layer. 1000 random inputs from the fashion MNIST training data were chosen to test extracted rules against the network. In the third layer, the search was limited to rules with 1000 literals or less to save time. The final layer output was tested with 10 neurons, each undergoing the rule searching procedure for 5 different values of \u03b2. This resulted in 5 sets of extracted rules with varying error/complexity trade-offs. Averaging the complexities and errors of rules extracted for each target neuron produced a graph showing the error/complexity landscape for rules extracted from each layer. The first and final layers extracted accurate rules with near 0 error, while the second and third layers had a different trade-off. The extracted rules from each layer show varying accuracy and complexity trade-offs. The first and final layers achieved near 0 error with accurate rules, while the second and third layers had a trade-off between accuracy and complexity. The third layer performed similarly to the second layer despite having more input nodes. The final layer provided more accurate rules with less complexity compared to the first layer. The results show a critical point where error increases rapidly as complexity penalty increases, indicating a natural set of rules for explaining latent features. Current rule extraction algorithms do not consider complexity in optimization, leading to uncertainty in where rules will land on the Error/Complexity graph. This paper introduces rule complexity as a key factor in extraction algorithms, with empirical evaluation essential for validation. While some CNN layers have complex rules with 15% error, others show simple rules with near 0% error in capturing output neuron behavior. The final layer in the CNN can accurately capture output neuron behavior with simple rules, suggesting selective use of decompositional algorithms based on the layer. The black box problem of neural networks hinders their deployment, as explainability remains a challenge despite efforts in knowledge extraction. Knowledge extraction in neural networks has attracted attention, but remains difficult to interpret. Critics argue that the distributed nature of neural networks makes rule extraction unfeasible. A novel search method for M-of-N rules was applied to explain latent features in a CNN, showing that an 'optimal' rule can represent an ideal error/complexity trade-off. Rule complexity was included in the search for extracted rules, revealing a large trade-off discrepancy between neurons in different layers. Rule complexity was explicitly measured in the search for extracted rules, showing a discrepancy in trade-off between neurons in different layers. While rule extraction may not adequately describe all latent variables, simplifying explanations without reducing accuracy can be useful. Decompositional rule extraction remains important for understanding network behavior. Further research could explore the impact of using different transfer functions, data sets, architectures, and regularization techniques on the accuracy and interpretability of networks."
}