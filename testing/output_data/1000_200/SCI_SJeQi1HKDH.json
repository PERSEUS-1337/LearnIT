{
    "title": "SJeQi1HKDH",
    "content": "In this work, the social influence is integrated into reinforcement learning to enable agents to learn from both the environment and peers. The Interior Policy Differentiation (IPD) algorithm encourages agents to develop unique policies while solving tasks, resulting in improved performance and diverse behaviors. This approach is inspired by cognition and animal studies. Reinforcement Learning (RL) involves learning through interaction with the environment to maximize rewards. Behavioral diversity is crucial for species evolution, and previous works have focused on encouraging diversity in RL by designing rich environments. However, designing complex environments manually is challenging and limits diversity. In this work, the focus is on improving the diversity of RL agents while maintaining their ability to solve tasks. The concept of social influence in reinforcement learning is inspired by animal society dynamics. The goal is to increase behavioral diversity by encouraging agents to explore beyond just maximizing rewards. The concept of social influence in reinforcement learning is explored to increase behavioral diversity among agents. A learning scheme is illustrated, where agents differentiate actions to be unique from others. Social influence is implemented as social uniqueness motivation, treated as a constrained optimization problem. A policy distance metric is defined to compare agent similarity, and an optimization constraint is developed for immediate feedback in the learning process. Interior Policy Differentiation (IPD) is introduced as a novel method. The proposed metric introduces immediate feedback in the learning process through Interior Policy Differentiation (IPD). It aims to encourage agents to perform well in tasks while taking unique actions compared to other agents. The method is shown to be effective in learning diverse and well-behaved policies for locomotion tasks using the Proximal Policy Optimization (PPO) algorithm. The Variational Information Maximizing Exploration (VIME) method by Houthooft et al. (2016) addresses sparse reward problems in reinforcement learning by adding an intrinsic reward term based on information gains. Curiosity-driven methods by Pathak et al. (2017) and Burda et al. (2018a) define intrinsic rewards based on prediction errors of neural networks. Random Network Distillation (RND) proposed by Burda et al. (2018b) quantifies intrinsic reward by comparing prediction differences between two networks. The Task-Novelty Bisector (TNB) learning method introduced by Zhang et al. (2019) aims to optimize the trade-off between external rewards and intrinsic rewards in reinforcement learning. It updates the policy based on the angular bisector of the two reward types. The TNB method optimizes external and intrinsic rewards by updating the policy along the angular bisector of the gradients. However, the foundation of this joint optimization is not solid, requiring additional neural networks for evaluating novelty, leading to extra computation expenses. DPPO method by Heess et al. (2017) enables agents to learn complex skills in diverse environments, showcasing impressive and effective results in navigating terrains. The research explores how different RL algorithms converge to various policies for the same task. While some algorithms tend to reach the same local optimum, others learn sophisticated strategies. The focus is on learning diverse policies through a single algorithm and avoiding local optima. Kurutach et al. (2018) maintain model uncertainty in collected data. The research focuses on learning diverse policies in reinforcement learning by avoiding local optima. Kurutach et al. (2018) maintain model uncertainty in collected data using an ensemble of deep neural networks. A metric is defined to measure policy differences, which is essential for the proposed algorithm. The learned policies are denoted as {\u03c0 \u03b8i ; \u03b8 i \u2208 \u0398, i = 1, 2, ...}, with \u03b8 i representing policy parameters. A metric in a metric space should satisfy identity, symmetry, and triangle inequality properties. In a metric space, the Total Variance Divergence is used to measure policy distance. This method aims to maximize the uniqueness of new policies by considering existing policies. The calculation of policy differences is based on D \u03c1 T V. Our method maximizes the uniqueness of a new policy by calculating policy differences using Monte Carlo estimation. In continuous state cases, obtaining enough samples efficiently is challenging. To improve sample efficiency, we propose an approximation method that requires similar domains of possible states. Our method aims to maximize policy distinctiveness by calculating policy variances through Monte Carlo estimation. To enhance sample efficiency in continuous state scenarios, we suggest an approximation approach that necessitates comparable state domains. Our method focuses on maximizing policy distinctiveness by estimating policy variances using Monte Carlo methods. The objective is to improve sample efficiency in continuous state scenarios by ensuring comparable state domains. The estimation of \u03c1 \u03b8 (s) using a single trajectory \u03c4 is unbiased, and the next step involves developing an efficient learning algorithm that considers both reward from the primal task and policy uniqueness. The traditional RL paradigm aims to maximize the expectation of cumulative rewards g = t=0 \u03b3 t r t, where the trajectory \u03c4 is sampled from the policy \u03b8 using Monte Carlo methods. Behavioral diversity of different agents is enhanced by incorporating both reward from the primal task and policy uniqueness into the learning objective. To enhance agent diversity, the learning objective in Monte Carlo methods should consider both primal task rewards and policy uniqueness. Previous approaches directly combine these rewards with a weight parameter \u03b1, impacting the agent's performance. The selection of \u03b1 and the formulation of intrinsic reward r int are crucial, as they affect the agent's ability to solve the primal task effectively. To address the challenge of selecting \u03b1 in multi-objective optimization, the approach shifts towards a constrained optimization problem by incorporating social uniqueness as a constraint rather than an additional target. This transformation involves introducing a threshold r 0 for minimal permitted uniqueness and a penalty method with penalty term r int and coefficient 1\u2212\u03b1 \u03b1 > 0. The difficulty lies in determining the optimal \u03b1 value, as discussed in further detail in the appendix. The constrained optimization problem in Eq.(7) with penalty term r int and coefficient 1\u2212\u03b1 \u03b1 > 0 is addressed by Zhang et al. (2019) using Task Novel Bisector (TNB) in the form of Feasible Direction Methods (FDMs). This work proposes solving Eq.(7) using Interior Point Methods (IPMs) instead. The solution involves reforming the problem to an unconstrained form with an additional barrier term in the objective, leading to the solution of Eq.(7) as \u03b1 approaches 0. Refer to Appendix G for more discussion on novel policy seeking methods and constrained optimization. In our proposed RL paradigm, the learning process is influenced by peers, allowing for a more natural way to bound collected transitions in the feasible region. By terminating new agents that step outside the feasible region, all valid samples collected during training are kept inside, reducing computational challenges and numerical instability. During training, valid samples are kept inside the feasible region, ensuring uniqueness in the new policy. This eliminates the need to balance intrinsic and extrinsic rewards, leading to a more robust learning process. The approach, named Interior Policy Differentiation (IPD) method, is demonstrated on MuJoCo-based environments in OpenAI Gym. The gym's physics engine is based on MuJoCo, and experiments are conducted on three locomotion environments. Different policies can be produced by selecting different random seeds before training. The proposed method is demonstrated on PPO and compared with other popular algorithms. In this work, the proposed method based on PPO is demonstrated and compared with TNB and WSR approaches for combining task goals and uniqueness motivation. The uniqueness metric is used directly in learning new policies without reshaping. WSR, TNB, and the proposed method are implemented in the same experimental settings, training 10 different policies sequentially to be unique. The proposed method based on PPO is compared with TNB and WSR for combining task goals and uniqueness motivation. The uniqueness metric is used in learning new policies without reshaping. Experimental results show the uniqueness and performance of policies trained sequentially. Visualizations demonstrate the acceleration process and motion patterns of agents clearly. The experimental results show that the proposed method outperforms other methods in Hopper and HalfCheetah tasks. However, in Walker2d, both WSR and the proposed method improve policy uniqueness but cannot surpass the performance of PPO. Detailed comparisons are provided in Table 1, with visualizations in Fig. 5, Fig. 6, and Fig. 7 in Appendix C. Success rate is also used as a metric for performance evaluation. In this work, success rate is used as a metric to compare different approaches. A policy is considered successful if it outperforms the baseline (average final performance of PPO) during training. The method consistently surpasses the baseline, ensuring performance improvement in Hopper and HalfCheetah environments. Our method shows performance improvements in Hopper and HalfCheetah environments by preventing policies from getting stuck in local minima. It encourages exploration of different action patterns, leading to enhanced performance compared to traditional RL schemes. In the HalfCheetah environment, our learning scheme prevents agents from acting randomly by receiving termination signals from peers, leading to improved performance. Our method involves agents learning to terminate early to avoid control costs, then adapting for higher rewards. The learning process acts as an implicit curriculum, with increasing difficulty in finding unique policies as more are learned. A study on social influence's impact on performance shows a more significant decrease in Hopper due to its smaller action space. In this work, an efficient approach called Interior Policy Differentiation (IPD) is developed to motivate RL to learn diverse strategies inspired by social influence. The method defines policy uniqueness and treats the problem as a constrained optimization problem. Experimental results show that IPD can learn various well-behaved policies, helping agents avoid local minimums and acting as an implicit curriculum for learning. The text discusses the importance of policy uniqueness in reinforcement learning and how it can act as an implicit curriculum for learning diverse strategies. The results show that optimizing for uniqueness can sometimes lead to better performance, but careful hyper-parameter tuning and reward shaping are necessary to balance task-related performance. The implementation details include using deterministic policies for calculating uniqueness and using a specific network structure for actor models in PPO. The text discusses the importance of policy uniqueness in reinforcement learning and how it can act as an implicit curriculum for learning diverse strategies. In the calculation of DTV, Gaussian noise on the action space is removed in PPO, using MLP with 2 hidden layers for actor models. The choice of unit number in the second layer is detailed in ablation studies. Training timesteps are fixed at 1M for Hopper-v3, 1.6M for Walker2d-v3, and 3M for HalfCheetah. The constraint threshold r0 can control the magnitude of policy uniqueness in the proposed method. Threshold Selection: The constraint threshold r0 in the proposed method allows for flexible control over policy uniqueness. Different thresholds result in varying policy behaviors, with a larger threshold leading to more distinct agent actions and a smaller threshold imposing a lighter constraint. Constraints are based on cumulative uniqueness rather than individual actions to focus on long-term differences. Testing with different threshold values shows varying agent performance levels. The implementation of Eq. (7) does not use constraints to force every action of a new agent to be different from others. Instead, cumulative uniqueness is used as constraints, applied after a certain number of timesteps. The WSR, TNB, and IPD methods correspond to different approaches in constrained optimization problems. The optimization of policy is based on batches of trajectory samples and implemented with stochastic gradient descent. The Penalty Method simplifies the optimization problem by incorporating constraints into a penalty term and solving it iteratively. The Feasible Direction Method finds a direction that satisfies the constraints. The final solution heavily depends on the selection of a fixed weight term \u03b1. The Feasible Direction Method (FDM) considers constraints by finding a direction that satisfies them. The TNB method selects a direction based on the bisector of gradients. The optimization result in TNB heavily relies on the shape of the function g. Using a barrier term can also influence the objective function. The solution of the objective with a barrier term will approach the primal objective as \u03b1 decreases. To address computational challenges, transitions can be bounded within a feasible region using previously trained policies. This implicitly constrains new agents during training by terminating those that step outside the region. During training, new agents are constrained within a feasible region by terminating those that step outside it. This ensures that all collected samples are unique and less likely to appear in previous policies. As a result, the learning process is more robust, eliminating the need to balance intrinsic and extrinsic rewards. The pseudo code of IPD based on PPO includes additions to the primal algorithm for this purpose."
}