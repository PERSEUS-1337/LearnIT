{
    "title": "SJxDDpEKvH",
    "content": "Deep generative models can emulate complex image datasets, providing a latent representation of the data. Manipulating this representation for meaningful transformations remains challenging without supervision. A non-statistical framework based on identifying modular network organization through counterfactual manipulations is proposed. Experiments show modularity between channel groups in various generative models, enabling targeted interventions like style transfer and assessing robustness to contextual changes in patterns. Deep generative models like GANs and VAEs have been successful in creating realistic images in various domains. Efforts have been made to produce models with disentangled latent representations for controlling interpretable properties of images, although these models may not be mechanistic or causal in nature. Generative models like GANs and VAEs aim to create realistic images with disentangled latent representations for interpretability. However, these models may lack a mechanistic or causal understanding, hindering the ability to attribute interpretable properties to specific parts of the network architecture. Access to a modular organization in generative models would enhance interpretability and enable extrapolations, similar to human representational capabilities. Extrapolations are crucial for adaptability to environmental changes and robust decision-making. In this paper, a causal framework is proposed to explore modularity in generative models, aiming to allow for direct interventions in the network without affecting other mechanisms. This approach enhances interpretability and adaptability to environmental changes. The paper proposes a causal framework to assess the modularity of generative models, allowing for individual modifications without affecting other mechanisms. This approach enhances interpretability and adaptability to environmental changes, using counterfactuals to analyze the role of internal variables in deep generative models. Empirical results show that VAEs and GANs trained on image databases exhibit modularity in their hidden units, enabling counterfactual editing of generated images. Generative models like VAEs and GANs show modularity in their hidden units, allowing for counterfactual editing of images. The interpretability of convolutional neural networks has been extensively studied in discriminative architectures, while our work focuses on intrinsic disentanglement to uncover the internal organization of networks. Our approach introduces the concept of intrinsic disentanglement in networks, which differs from previous works by being fully unsupervised and flexible in handling arbitrary continuous transformations. This contrasts with other approaches that require semantic information or rely on group representation theory. Additionally, an interventional approach to disentanglement has been explored in classical graphical models. An interventional approach to disentanglement was taken by Suter et al. (2018) focusing on extrinsic disentanglement in a graphical model setting. They developed measures of interventional robustness based on labeled data. A general framework was introduced to precisely formulate the notion of disentanglement and connect it to causal concepts. The theory section will be presented informally for a high-level understanding, with mathematical details provided in Appendix A. The framework considers a generative model M that maps a latent space Z to a manifold Y M where data points live in ambient Euclidean space Y. The text discusses the use of a generative model M to map a latent space Z to a manifold Y M, where data points exist in ambient Euclidean space Y. A representation mapping r from Y M to a representation space R is defined, with M being the latent representation of the data. A Causal Generative Model (CGM) is introduced to implement the generative model using a non-recurrent neural network, represented by a causal graphical model. Additionally, endogenous variables can be chosen to compute the mapping g M. The text discusses using a generative model M to map a latent space Z to a manifold Y M, with endogenous variables chosen to compute the mapping g M. The internal representation of the network is defined by mild conditions ensuring g M is left-invertible. The text discusses defining the internal representation of the network using a generative model M to map a latent space Z to a manifold Y M. The V k 's are constrained to subsets of smaller dimensions. The CGM framework allows defining counterfactuals in the network following Pearl (2014), with unit-level counterfactuals being the output of the interventional model M h. Counterfactuals induce a transformation based on potential outcomes. The unit-level counterfactuals from the generative model induce a transformation, with a focus on faithfulness to the original model. Non-faithful counterfactuals may lead to artifactual outputs or extrapolation to unseen data. The concept of disentangled representation in latent variables is also discussed. The classical notion of disentangled representation posits that latent variables sparsely encode real-world transformations. Supervised approaches manipulate relevant transformations explicitly, while unsupervised learning approaches aim to learn these transformations from unlabeled data. State-of-the-art approaches enforce conditional independence between latent factors, leading to issues with the prior distribution constraints. Enforcing conditional independence between latent factors in statistical approaches poses issues with prior distribution constraints, as it may not accurately represent real-world properties. Current unsupervised methods struggle to disentangle complex real-world data beyond synthetic or well-calibrated datasets like CelebA. This challenge hinders the development of inductive biases for learning representations that benefit downstream tasks. Disentangled generative models struggle on complex real-world datasets, with visual quality below non-disentangled models like BigGAN. A non-statistical definition of disentanglement involves a transformation T on the data manifold YM, where T acts on a single variable zk using transformation f, leaving other latent variables free to encode other properties. Two transformations T1 and T2 are considered disentangled when they modify independently. The notion of extrinsic disentanglement involves transformations T1 and T2 modifying different components of the latent representation, following the causal principle of independent mechanisms. This functional definition is agnostic to subjective property choices and statistical independence, requiring statistical independence between disentangled factors in the latent space. The functional notion of disentangled transformation in a space where components are i.i.d. distributed entails statistical independence between disentangled factors. The extension of this definition allows for transformations of internal variables to be intrinsically disentangled with respect to a subset of endogenous variables. This concept is illustrated in the CGM of Fig. 2b, where properties encoded by endogenous variables may not be statistically independent due to a common latent cause but can still be intervened on independently. Intrinsic disentanglement of transformations in internal representation space is achieved when a subset of endogenous variables remains unaffected by a transformation T. Modularity of endogenous variables allows for arbitrary disentangled transformations to be implemented within their input domain. Intrinsic disentanglement of transformations in internal representation space is achieved when a subset of endogenous variables remains unaffected by a transformation. Modularity allows for arbitrary disentangled transformations within the input domain, leading to a disentangled representation through partitioning of intermediate modules. Our framework suggests that a disentangled representation requires partitioning latent variables into modules, challenging the assumption of independent scalar variables. This insight is relevant for artificial and biological systems, where grouping neurons into modules at a \"mesoscopic\" level allows for independent intervention. The functional definition of disentanglement raises questions on finding relevant transformations, addressed by Prop. 1 and 2. The functional definition of disentanglement, as discussed in the previous section, raises questions on how to find relevant transformations. Prop. 1 and 2 suggest that once a modular structure is identified in the network, a wide range of disentangled transformations become available. Transformations that remain within their input domain are considered good candidates for disentanglement, and counterfactual interventions implicitly define transformations. To achieve faithful counterfactuals, a constant value v0 is assigned to a subset of endogenous variables E. Sampling from the joint marginal distribution of the variables in E is used to avoid characterizing VEM. This approach is illustrated using a standard feed-forward multilayer neural network with endogenous variables defined as the collection of all output activations of channels. The hybridization procedure in a neural network involves selecting endogenous variables from output activations of channels in a layer. Two original examples are generated using latent variables, and a tuple of values is memorized for each example. By identifying a modular structure, different aspects of generated images can be encoded in the tuples, allowing for the creation of hybrid examples mixing these features. The counterfactual hybridization framework assesses how a module affects the generator's output by generating pairs of latent vectors and creating hybrid outputs. The influence map is estimated by quantifying the causal effect through the mean absolute effect calculation. The counterfactual hybridization framework evaluates the impact of a module on the generator's output by generating hybrid outputs from pairs of latent vectors. The approach calculates the unit-level causal effects and averages them over different interventions, resulting in a grayscale heat-map pixel map. The individual influence of module E is quantified by averaging the influence map across output pixels. Selecting subsets E to intervene on poses a challenge in the hybridization approach. The hybridization approach evaluates the impact of module E by averaging influence maps across output pixels. Selecting subsets to intervene on is a challenge, especially with networks containing many units or channels per layer. A fine to coarse approach is used to group influence maps by similarity, defining modules at a coarser scale. The influence maps suggest functionally segregated channels, grouped into modules dedicated to specific aspects of the output. Clustering channels using EIMs as feature vectors involves pre-processing and NMF algorithm for grouping. The Non-negative Matrix Factorization (NMF) algorithm is used to cluster template patterns from a matrix S, with weights in W representing the contribution of each pattern to individual maps. The choice of NMF is justified by its success in isolating meaningful parts of images. The approach will also be compared to the classical k-means clustering algorithm, and a toy generative model is introduced to further justify the NMF based approach. A toy generative model is introduced, involving a neural network with hidden layers and random choice for model parameters. The model involves a neural network with hidden layers where coefficients are sampled from a distribution. The assumption is that certain areas in the image are influenced by specific modules. The identifiability result shows that the hidden layer vectors correspond to a disentangled representation. This justifies the use of NMF on the influence map matrix for generating a binary matrix. The use of NMF on the influence map matrix justifies generating a binary matrix summarizing significant influences. Sliding window application enforces similarity between influence maps in the same module, favoring low-rank matrix factorization. Investigating modularity of generative models on CelebA dataset involved EIM calculations, clustering channels into modules, and hybridizing generator samples using these modules. The study by Higgins et al. (2017) involved running EIM calculations, clustering channels into modules, and hybridizing generator samples. Setting the number of clusters to 3 resulted in interpretable cluster templates for background, face, and hair. Cluster stability analysis confirmed the consistency of these clusters. The clustering was performed twice on two-thirds of the data, with clusters matched to maximize label consistency. Results showed that 3 clusters were a reasonable choice with high consistency (>90%), dropping for 4 clusters. NMF-based clustering outperformed k-means. Robustness was also assessed using cosine distance between templates, showing an average similarity of .9. The results of clustering showed high consistency with 3 clusters having an average cosine similarity of .9. Applying a hybridization procedure to the resulting modules led to replacement of features while maintaining the overall image structure. Exemplary influence maps also reflected the observation that some maps may spread over different image locations. The \u03b2-VAE is designed for extrinsic disentanglement but may not be optimal compared to other approaches. Further investigation is needed to determine if better extrinsic disentanglement could also benefit intrinsic disentanglement. It is important to study intrinsic disentanglement in models where disentanglement is not explicitly enforced, such as GAN-like architectures. These architectures typically outperform VAE-like approaches in terms of sample quality in complex image datasets. The results mentioned can be replicated in the official tensorlayer DCGAN implementation with a similar architecture. After replicating results in the official tensorlayer DCGAN implementation, the study tested a pretrained Boundary Equilibrium GAN (BEGAN) for generating high-resolution face images. The BEGAN's simple generator architecture allowed for minimal modifications to test the hypothesis, resulting in noticeable effects on counterfactuals with interventions on channels. The study tested a pretrained BEGAN for generating high-resolution face images. Interventions on channels from the same cluster in two successive layers resulted in a selective transfer of features. The counterfactual images were evaluated using FID, showing clear hair transfer and encoding of different face features in modules. The study evaluated counterfactual images using FID to compare image quality with original samples. The BigGAN-deep architecture was used to generate high-resolution images with complex datasets, showing the ability to scale and generalize to various objects. The study used the BigGAN-deep architecture to generate high-resolution images with complex datasets, showing the ability to scale and generalize to various objects. Counterfactual images were evaluated using FID to compare image quality with original samples. Hybrids were generated by mixing features of different classes, and examples showed the effectiveness of intervening on two successive layers within a Gblock to generate counterfactuals. High-quality counterfactuals with modified backgrounds while keeping a similar object in the foreground were demonstrated, even with objects of different nature like a teddy bear in a tree or a \"teddy-koala\" merging textures on a uniform indoor background. The study investigated the use of counterfactual images to improve classifier robustness. Different pretrained classifiers were compared in recognizing original classes like teddy bears or koalas. Recognition rates tended to increase with layer depth, with Inception resnet performing better at intermediate blocks 5-6. At intermediate blocks 5-6, Inception resnet outperformed other classifiers in recognizing original classes like teddy bears or koalas. Different classifiers rely on different aspects of image content for decision-making. A mathematical definition of disentanglement was introduced to characterize representation in deep generative architectures, revealing interpretable modules of internal variables in four different models trained on real-world datasets. This framework enhances understanding of complex generative architectures and their applications. The research introduces a framework to understand generative models and their applications, such as style transfer and object recognition. It aims to enhance interpretability of deep neural networks and utilize them for tasks they were not originally trained for, potentially leading to more sustainable AI research in the future. In the future, Artificial Intelligence could be advanced by using trained generator architectures as mechanistic models that can be manipulated independently. Structural causal models (SCMs) with structural equations (SEs) can represent these models, allowing for the study of variables and exogenous influences. SEs remain valid even after interventions, enabling the modeling of operations in AI research. The Causal Generative Model (CGM) M captures computational relations between input latent variables, generator's output Y, and endogenous variables in a directed acyclic graph G. This model can decompose the generator's output into two steps: {Z k } \u2192 {V k } \u2192 Y. The CGM M decomposes the generator's output into two steps: {Z k } \u2192 {V k } \u2192 Y, where V k represents a channel of the output. The model comprises a directed acyclic graph G and a set of structural equations. The graph includes endogenous variables, latent inputs, and the output, aligning with Pearl's definition of a deterministic structural causal model. Specificities of CGMs include variable assignments that may or may not involve latent/exogenous variables, which is uncommon in causal inference. The CGM M decomposes the generator's output into two steps: {Z k } \u2192 {V k } \u2192 Y, where V k represents a channel of the output. The model allows for feed-forward networks with latent inputs and deterministic operations. Endogenous variables are assigned once z is chosen, ensuring unambiguous assignments. Internal variables and outputs typically live on manifolds of smaller dimension than their ambient space. The distribution covers the whole latent space Z, with internal variables and outputs living on smaller dimension manifolds. Functions assign Y from latent and endogenous variables, called latent and endogenous mappings. A CGM satisfying these assumptions is called an embedded CGM. The example in Fig. 2b contains exactly two layers. The embedded CGM in Fig. 2b has two layers and defined image sets constrained by parameters. The image set Y M should approximate the data distribution. Learning generator parameters to match Y M with the data distribution is a key goal for generative models. The support of the target data distribution is a major goal for generative models. Transformations that respect the topology of Y M are used, with embeddings as the basic structure allowing inversion of g M. Injectivity of g M is a key requirement for embedded CGMs, where if Z of CGM M is compact, then M is embedded if and only if g M is injective. Generative models based on uniformly distributed latent variables are embedded CGMs if they are injective. Generative models with uniformly distributed latent variables, if injective, are embedded CGMs. Restricting VAEs' latent space to compact intervals can approximate the original CGM for most samples. The CGM framework allows defining counterfactuals in the network following Pearl (2014). Unit level counterfactuals involve replacing structural assignments for endogenous variables with specific assignments based on the latent variables. The interventional CGM involves replacing structural assignments for variables with specific assignments based on latent variables. Counterfactuals induce a transformation of the generative model's output, relating to disentanglement and intrinsic disentanglement within the network. Intrinsic disentanglement in generative models involves a transformation of endogenous variables that only affects specific variables, leading to a causal interpretation of the model's structure. This concept relates to robustness to perturbations and counterfactuals, with a focus on the compactness of latent variables and the Hausdorff property of the model's codomain. The proof of equivalence between faithful and disentangled transformations is discussed. It is shown that a transformation is disentangled if it is an endomorphism of Y M, and a faithful transformation implies disentanglement with respect to E in M. The absence of a common latent ancestor between E and E ensures unambiguous values in both subsets. The proof shows that a transformation is disentangled if it is an endomorphism of Y M, ensuring unambiguous values in subsets. The subsets of endogenous variables are modular and form a disentangled representation. The hidden layer partition ensures a disentangled representation with injective mapping and counterfactual hybridization. The rank K binary factorization of matrix B is guaranteed by conditions on I k's and thresholding approach. The uniqueness of factorization is ensured by classical NMF identifiability results. The \u03b2-VAE architecture is similar to DCGAN with specified hyperparameters. The architecture used for the model is similar to DCGAN with specified hyperparameters and skip connections for image sharpness. The pretrained model was taken from Tensorflow-hub and consists of ResBlocks for the generator. The BigGan-deep architecture by Brock et al. (2018) was used as a pre-trained model on 256x256 ImageNet without retraining. The architecture used for the model is based on DCGAN with skip connections for image sharpness. The pretrained model from Tensorflow-hub, following the BigGan-deep architecture by Brock et al. (2018) on 256x256 ImageNet, was not retrained. It consists of ResBlocks for the generator, with BatchNorm-ReLU-Conv Layers and upsampling transformations. Skip connections bring fresh signal from the input to every ResBlock. Influence maps were generated by a VAE on the CelebA dataset, showing variance and perturbation influence. FID analysis of BEGAN hybrids measured distances between different pairs of classes, normalized by the FID between real data. Hybrids created through interventions on cluster k show small distances to generated data and each other, indicating visually plausible images. Entropy values are higher for modules with poorer quality, particularly in Gblock 6. Hybrids based on interventions at a more abstract level have smaller entropy values. The module with poorer quality leads to larger entropy values. Hybrids based on interventions on Gblock number 4 have smaller entropy values, suggesting that object texture is key for the classifier's decision. The hybrids for the BIGAN between classes \"cock\" and \"ostrich\" show interventions on different Gblocks with fixed modules. The entropy is computed using probabilistic output for the top 10 classes across all hybrids. The entropy values are computed for hybrids generated by interventions on Gblock 5, showing large entropy for the first module. The classification outcome of discriminative models for koala+teddy hybrids is analyzed to assess classifier robustness. The resultant hybrids resemble a teddy bear in a koala context, emphasizing the importance of object recognition over contextual information. The study analyzes the classifier robustness using koala+teddy hybrids, showing nasnet large is more contextually robust."
}