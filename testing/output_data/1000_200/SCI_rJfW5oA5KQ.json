{
    "title": "rJfW5oA5KQ",
    "content": "Generative Adversarial Networks (GANs) have shown impressive results in learning complex real-world distributions, but recent works have highlighted issues such as lack of diversity and mode collapse. Theoretical work by Arora et al. (2017a) suggests a dilemma regarding GANs' statistical properties: powerful discriminators can lead to overfitting, while weak discriminators may not detect mode collapse. In contrast to previous works on GANs, this paper demonstrates that GANs can learn distributions in Wasserstein distance with polynomial sample complexity by using discriminators with strong distinguishing power against specific generator classes. The designed discriminators can approximate the Wasserstein distance and/or KL-divergence, ensuring that the learned distribution closely matches the true distribution and avoids dropping modes. Preliminary experiments on synthetic datasets show a strong correlation between the test Integral Probability Metric (IPM) and KL divergence. The lack of diversity in GANs may be due to optimization sub-optimality rather than statistical inefficiency. Various methods have been proposed to enhance GAN quality, but the understanding of GANs is still limited. Recent research has highlighted concerns about mode collapse and lack of diversity in GANs, suggesting that distributions learned by GANs may miss significant modes of the target distribution. The paper proposes that designing discriminators with strong distinguishing power against specific families of generators, such as special subclasses of neural network generators, can alleviate mode collapse. The focus is mainly on the Wasserstein GAN formulation in this paper. The paper focuses on the Wasserstein GAN formulation, specifically the F-Integral Probability Metric (F-IPM) between distributions. It sets up families of generators and discriminators to learn the data distribution by optimizing the Wasserstein-1 distance. Parametric families of functions, like neural networks, are used to approximate Lipschitz functions for gradient-based optimization. The paper discusses optimizing the objective of GANs through gradient-based algorithms using parameterized samplers in the family G. One major concern is \"mode collapse,\" where the learned distribution generates high-quality but low-diversity examples due to the weakness of IPM compared to W 1. This issue arises when a distribution q can fool IPM but not W 1, leading to mode-dropped distributions that lack diversity. The complexity measure R(F) is discussed in relation to Rademacher complexity, with the issue of mode collapse in GANs due to the weakness of IPM compared to W 1. Increasing the strength of the discriminator to larger families like all 1-Lipschitz functions is suggested, but the Wasserstein-1 distance is noted to have poor generalization properties. Even when the distributions p and q are equal, the empirical Wasserstein distance may not reflect this accurately, leading to challenges in learning success. This paper addresses the challenge of mode collapse in GANs by proposing a discriminator class F that is strong against a specific generator class G. The discriminator class F has restricted approximability with respect to the generator class G and the data distribution p, ensuring that it can distinguish between p and any q in G effectively. The paper focuses on discriminator class F having restricted approximability with respect to generator class G and data distribution p. It aims to approximate the Wasserstein distance W1 for p and any q in G, with specific functions \u03b3L(t) = t^\u03b1 and \u03b3U(t) = t. The framework considers both realizable and non-realizable cases. A discriminator class F with restricted approximability aims to resolve mode collapse by ensuring that if the IPM between p and q is small, then p and q are also close in Wasserstein distance. This allows for passing from population-level guarantees to empirical-level guarantees, expanding on the statistical properties of Wasserstein GANs. The paper introduces a theoretical framework for analyzing Wasserstein GANs with polynomial samples, focusing on the statistical properties of the distance W F. It develops techniques for designing discriminator class F with restricted approximability to address mode collapse, providing empirical-level guarantees for generator classes like mixtures of Gaussians and distributions generated by invertible neural networks. The paper introduces a theoretical framework for analyzing Wasserstein GANs with polynomial samples, focusing on the statistical properties of the distance W F. It develops techniques for designing discriminator class F with restricted approximability to address mode collapse, providing empirical-level guarantees for generator classes like mixtures of Gaussians and distributions generated by invertible neural networks. The generator classes include simple classes like mixtures of Gaussians, exponential families, and more complicated classes like distributions generated by invertible neural networks. Properly chosen F provides diversity guarantees such as inequalities eq. (5). Simple families of distributions G, such as Gaussian distributions and exponential families, allow for direct design of F to distinguish pairs of distributions in G. For Gaussians, one-layer neural networks with ReLU activations suffice as discriminators, while for exponential families, linear combinations of sufficient statistics are used. The family of distributions generated by invertible neural networks is studied in Section 4, where a special type of neural network discriminators with one additional layer than the generator has restricted approximability. The special type of neural network discriminators with one additional layer than the generator has restricted approximability, guaranteeing certain properties for the learned distribution q. However, the invertibility assumption limits the distributions to those supported on the entire space, which may not align with the low-dimensional manifold where natural images reside. The KL-divergence is infinite if the estimated distribution's support does not coincide with the distribution p's support. The KL-divergence is not a suitable measurement for statistical distance when both distributions have low-dimensional supports. The paper focuses on approximating Wasserstein distance using IPMs for generators with low-dimensional supports, showing the advantage of GANs over MLE in learning such distributions. Tools are developed to approximate the log-density of a neural network generator, demonstrated in synthetic and controlled settings. The paper focuses on approximating Wasserstein distance using IPMs for generators with low-dimensional supports, showing the advantage of GANs over MLE in learning such distributions. The test IPM could serve as an alternative for measuring diversity and quality in complex settings where KL-divergence or Wasserstein distance is not measurable. The lack of diversity in GAN experiments may be due to sub-optimal optimization rather than statistical inefficiency. Various tests for diversity, memorization, and generalization have been developed, indicating that while \"memorization\" is not an issue with most GANs, diversity may be lacking. The lack of diversity in GAN experiments is a common issue, with various proposed solutions to address mode collapse. Different architectures and algorithms have been suggested to improve diversity, but there are no provable solutions in general. Some studies have shown promising results, such as training GANs with quadratic discriminators when generators are Gaussians. The IPM is a proper metric under a mild regularity condition, providing a KL-divergence bound with finite samples. The work develops statistical guarantees in Wasserstein distance for distributions like injective neural network generators. Liang (2017) discusses GANs in a non-parametric setup, showing that the sample complexity for learning GANs improves with the smoothness of the generator family. The sample complexity for learning GANs improves with the smoothness of the generator family, but the rate is non-parametric unless the Fourier spectrum decays fast. Invertible generator structure in Flow-GAN shows GAN training increases KL on real dataset. Successful GAN training implies learning in KL-divergence with invertible neural net. Real data may not be generated by an invertible neural network. If data can be generated by an injective neural network, closeness can be bounded. Our theory implies that data generated by an injective neural network can be bounded in closeness to the true distribution in Wasserstein distance. The notion of IPM includes statistical distances like TV and Wasserstein-1 distance. The F-IPM, or neural net IPM, refers to the distance between distributions when F is a class of neural networks. KL divergence and Wasserstein-2 distance are also important distances between distributions. The KL divergence and Wasserstein-2 distance are important statistical measures between distributions. The Rademacher complexity and training IPM loss play a role in the generalization of the Wasserstein GAN. Theoretical results show that data generated by an injective neural network can be closely bounded to the true distribution in Wasserstein distance. Theorem 2.1 states that neural networks with ReLU activation can distinguish Gaussian distributions with restricted approximability guarantees. The discriminators considered have restricted approximability induced by the IPM W F. The IPM W F induced by discriminators with restricted approximability guarantees for neural networks with ReLU activation can distinguish Gaussian distributions. The upper bound for W F (p, q) is on the order of W 1 (p, q)/ \u221a d when p, q have spherical covariances. The proof of the order of W 1 (p, q)/ \u221a d for Gaussian distributions with spherical covariances is deferred. An extension to mixture of Gaussians is also discussed, along with the design of a discriminator family F for exponential families. Linear combinations of sufficient statistics are shown to be discriminators with restricted approximability. Theorem 3.2 states conditions for exponential families and discriminators, assuming certain properties hold for the log partition function. The log partition function log Z(\u03b8) is convex, with assumptions on curvature. Geometric assumptions are needed for the bound eq. (8) due to Wasserstein distance dependence on underlying geometry. Proof of eq. FORMULA15 follows standard theory, while eq. (8) proof requires further development in Section 4. In Section 4, discriminators with restricted approximability for neural net generators are designed. The focus is on invertible neural networks generators with proper densities in Section 4.1, and the extension to injective neural networks generators in Section 4.2, where distributions no longer have densities. The generators are parameterized by invertible neural networks. The family of neural networks G is parameterized by invertible neural networks, allowing each hidden dimension to have a different impact on the output distribution. The set of invertible neural networks G\u03b8 consists of standard-layer feedforward nets with invertible parameters and activation functions. The assumption is made that the generators are invertible. Assuming invertible generators with specific parameters, neural networks G\u03b8 are considered, ensuring invertibility and using activation functions like smoothed Leaky ReLU. Imposing assumptions on generator networks is necessary to prevent them from implementing pseudo-random functions. Neural networks can implement pseudo-random functions that are indistinguishable from random functions. A family of neural networks with specific activation functions can compute log p \u03b8 with a limited number of layers and parameters. The exact form of the neural network family may not be crucial in practice, as other families could also provide good approximations. The proof involves a change-of-variable formula and the observation that G\u03b8 is a feedforward network. The proof involves the change-of-variable formula log p \u03b8 (x) = log \u03c6 \u03b3 (G \u22121) and the observation that G \u22121 \u03b8 is a feedforward neural net with layers. The log-det of the Jacobian involves computing the determinant of the weight matrices, which can be represented by adding a bias on the final output layer. The discriminator class F has restricted approximability w.r.t. G for any p, q \u2208 G. The proof of Theorem 4.2 involves the discriminator class F having restricted approximability with respect to G. Lemma 4.3 relates the KL divergence to the IPM when log densities exist in the family of discriminators. The proof sketch outlines the use of the discriminator class implementing log p - log q for any p, q in G. The goal is to lower bound the quantity by the Wasserstein distance and upper bound W F (p, q) by the Wasserstein distance. To establish the proof of Theorem 4.2, the discriminator class F's restricted approximability with respect to G is crucial. The goal is to lower bound the quantity by the Wasserstein distance and upper bound W F (p, q) by the Wasserstein distance. This involves proving transportation inequalities using Lemma D.3 and utilizing Lipschitz functions to ensure sub-Gaussian random variables. Additionally, two workarounds are proposed to handle the lack of global Lipschitzness in F. Theorem D.2 extends the results in (Polyanskiy & Wu, 2016) by providing two workarounds for handling the lack of global Lipschitzness in the discriminator class F. The training success with small expected IPM implies closeness between the estimated distribution q and the true distribution p in Wasserstein distance. Corollary 4.4 states that with high probability over the choice of training data, if the training process returns a distribution, the training error is measured by the expected IPM over the randomness of the learned distributions. In this section, injective neural network generators are discussed, which generate distributions on a low dimensional manifold. A novel divergence between two distributions, sandwiched by Wasserstein distance, is designed to be optimized as IPM. The invertibility of G \u03b8 is limited to the image of G \u03b8, a k-dimensional manifold. In this section, a variant of the IPM is designed to approximate the Wasserstein distance between distributions generated by neural nets. The smoothed F-IPM between p, q is optimized with an additional variable \u03b2, showing that for certain discriminator classes, it approximates the Wasserstein distance. Theorem E.1 states that for distributions p, q in class G, a discriminator class F exists such that if d(p^n, q^n) is small for n poly(d), then mode collapse is avoided. The discriminator family F must have restricted approximability with respect to generator family G to prevent mode collapse. Certain specific discriminator classes are designed to guarantee this. In practice, the Wasserstein distance between distributions (p, q) is bounded by a specific discriminator class with restricted approximability. Synthetic experiments confirm that this holds true in GAN training, where the optimization difficulty rather than statistical inefficiency may be the main challenge. In practice, GAN training may face optimization difficulty rather than statistical inefficiency. Experiments show good statistical behaviors on \"typical\" discriminator classes. Synthetic experiments with WGANs demonstrate correlation between IPM and Wasserstein distance, as well as with KL divergence. Training GANs on various curves in two dimensions is explored. In synthetic experiments with WGANs, the unit circle and \"swiss roll\" curve are trained, showing strong correlation between IPM and Wasserstein distance. The ground truth distributions are not covered in Theorems 4.2 and 4.5, but evidence suggests restricted approximability holds. Standard two-hidden-layer ReLU nets are used as generators and discriminators. The study uses standard two-hidden-layer ReLU nets for both generator and discriminator, with specific architectures. The RMSProp optimizer is employed with set learning rates. Two metrics, neural net IPM W F and Wasserstein distance W 1, are compared between ground truth distribution and learned distribution during training. The study compares neural net IPM and Wasserstein distance for learning distributions using GANs. Results show close alignment between the learned generator and ground truth distribution at iteration 10000, with correlation between IPM and Wasserstein distance. At iteration 500, generators have not fully learned the distributions yet. The study presents sample complexity bounds for learning various distributions with convergence guarantees. The study presents sample complexity bounds for learning various distributions using GANs with convergence guarantees in Wasserstein distance or KL divergence. The analysis involves designing discriminators with restricted approximability tailored to the generator class, aiming to extend techniques to other distribution families with tighter sample complexity bounds. The study explores sample complexity bounds for learning distributions with GANs, focusing on Wasserstein distance and KL divergence. Discriminators with restricted approximability are designed for the generator class to improve sample complexity bounds for various distribution families. The study focuses on sample complexity bounds for learning distributions with GANs, specifically Wasserstein distance and KL divergence. Discriminators with restricted approximability are designed for the generator class to enhance sample complexity bounds for different distribution families. The linear discriminator is expressed as the sum of two ReLU discriminators, with mathematical representation t = \u03c3(t) \u2212 \u03c3(\u2212t). The neuron distance between two Gaussians is computed, and perturbation bounds are utilized for further analysis. The study focuses on sample complexity bounds for learning distributions with GANs, specifically Wasserstein distance and KL divergence. Using perturbation bounds, the lower bound for the W 2 distance between two Gaussians is established. The W 2 distance is used to bridge the KL and F-distance, leading to the need to bound the growth of \u2207 log p 1 (x) 2. The study establishes lower bounds for the W 2 distance between two Gaussians using perturbation bounds. To bound the growth of \u2207 log p 1 (x) 2, the exponential family property is recalled, and Wasserstein bounds are shown. The proof involves bounding the growth of \u2207 log p 1 (x) 2 and utilizing the Rademacher contraction inequality. The text discusses perturbation bounds to establish lower bounds for the W 2 distance between two Gaussians. It involves bounding the growth of \u2207 log p 1 (x) 2, utilizing Wasserstein bounds, and applying the Rademacher contraction inequality. The study also considers a mixture of k identity-covariance Gaussians and the use of neural networks for learning. The text discusses the Gaussian concentration result and the restricted approximability of discriminator functions in the context of perturbation bounds for the W 2 distance between two Gaussians. It also considers the regularity properties of distributions and the mixture components of Gaussians. The text discusses the regularity properties of distributions and mixture components of Gaussians. It focuses on bounding the Rademacher complexity of a neural network and utilizing a one-step discretization bound to show Lipschitz continuity in the \u03c1 metric. The covering number of a set under \u03c1 is also considered for perturbation bounds. The text discusses bounding the covering number of a set under \u03c1 for perturbation bounds. It shows Lipschitz continuity in the \u03c1 metric and utilizes a one-step discretization bound. The Rademacher complexity of a neural network is also considered. Theorem D.2 provides upper bounds for f-contrast by Wasserstein distance for distributions on R^d. It involves a truncation argument and utilizes the Lipschitz property of f. The proof involves a coupling and Cauchy-Schwarz inequality. This result extends a previous proposition by Polyanskiy & Wu (2016). The proof of Theorem D.2 involves a coupling and Cauchy-Schwarz inequality, extending a previous proposition by Polyanskiy & Wu (2016). It shows upper bounds for f-contrast by Wasserstein distance for distributions on R^d. The inverse network G^-1\u03b8 can be computed as G\u03b8(z), with \u03b8 being a -layer feedforward net with activation \u03c3^-1. The problem of representing log p\u03b8(x) by a neural network is considered, with the log density having a specific formula. The log density of Z \u223c N(0, diag(\u03b3^2)) can be computed using an inverse network with specific layers and parameters. By adding branches to the network, the log determinant of the Jacobian can also be calculated. This involves recursive backward steps and activation functions. By adding branches to the inverse network, the log density of Z \u223c N(0, diag(\u03b3^2)) and the log determinant of the Jacobian can be computed. This results in a neural network that computes log p \u03b8 (x) with limited layers and parameters, satisfying the Gozlan condition. The network G \u03b8 is L-Lipschitz, and the random variable DISPLAYFORM5 is L2-sub-Gaussian, satisfying the Gozlan condition with \u03c32 = L2. The Lipschitzness of log p \u03b8 (x) for all \u03b8 \u2208 \u0398 can be upper bounded through Wasserstein distances. The text discusses the Lipschitzness of the network G \u03b8 and the upper bounding of log p \u03b8 (x) through Wasserstein distances for all \u03b8 \u2208 \u0398. The W 2 bound and W 1 bound are analyzed, showing the differentiability of log p \u03b8 (x) and the tail bound for X 2. The text discusses the Lipschitzness of the network G \u03b8 and upper bounding of log p \u03b8 (x) through Wasserstein distances for all \u03b8 \u2208 \u0398. It also analyzes the tail bound for X 2. The constant C(\u03b8) is the sum of normalizing constant for Gaussian density and log det(W i ). A parameter K = K(\u03b8) is created within the range [0, d R W]. Define the metric DISPLAYFORM13 for any reparametrized \u03b8. The one-step discretization bound is discussed using the Rademacher process. Two lemmas address discretization error and expected max over a finite set. Lemma D.7 states that there are constants \u03bb 0 and C depending on certain factors. By substituting previous Lemmas into the bound equation, it is shown that for certain conditions, the generalization error dominates. The text discusses the Lipschitzness of hidden layers in a network. The text discusses the Lipschitzness of hidden layers in a network, showing bounds for each layer and addressing the Lipschitz property of the network as a whole. The text discusses the Lipschitzness of hidden layers in a network, showing bounds for each layer and addressing the Lipschitz property of the network as a whole. Fixing any \u03b8 \u2208 \u0398, the random variable DISPLAYFORM32 is shown to be sub-exponential. The term h , A \u03b3 h is a quadratic function of a sub-Gaussian random vector, hence is subexponential with a mean bounded by E[ A \u03b3 op h 2 2 ] \u2264 Cd/\u03b4 2. The text discusses the Lipschitzness of hidden layers in a network, showing bounds for each layer and addressing the Lipschitz property of the network as a whole. The sub-Gaussian parameter of h leads to Cd/\u03b4^2. The random variable Y \u03b8 is mean-zero sub-exponential with a MGF bound. The covering number of \u0398 is bounded by the product of independent covering numbers. Jensen's inequality is used to bound the expected maximum.\u03bb \u2264 \u03bb0\u03b4^2n is a key parameter for the theorem. The text discusses the Lipschitzness of hidden layers in a network, showing bounds for each layer and addressing the Lipschitz property of the network as a whole. Towards stating the theorem more quantitatively, relevant quantities of the generator class are specified. The distribution p\u03b2\u03b8(x) is obtained by adding Gaussian noise with variance \u03b2^2 to a sample from G\u03b8, and truncating the distribution to a high-probability region. The text introduces regularity conditions for the family of generators G, including bounds on partial derivatives of f and assumptions on the inverse activation function. Asymptotic notation is used for convenience, hiding dependencies on certain variables. The main theorem states that for certain F and d, F approximates the Wasserstein distance. The main theorem states that for certain F and d, F approximates the Wasserstein distance. Theorem E.1 and Theorem E.2 provide conditions for the generator class G and a family of functions F that can approximate the log density of p \u03b2 for every p \u2208 G. The approach involves approximating p \u03b2(x) using Laplace's method of integration, with F networks of size poly(1/\u03b2, d) that approximate log p for typical x and act as a lower bound of p globally. The integral calculation is dominated by its maximum value, with a greedy \"inversion\" procedure used for typical x and providing a lower bound for atypical x. This leads to proving Theorem E.1, assuming the correctness of Theorem E.2. The proof of Theorem E.1 involves neural networks N1 and N2 approximating log p \u03b2 and log q \u03b2, respectively. By using certain equations and the Bobkov-G\u00f6tze theorem, a lower bound is established. For the upper bound, setting \u03b2 = W 1/6 suffices. The optimal coupling C of p and q is considered for the claim. The proof of Theorem E.1 involves neural networks approximating log p \u03b2 and log q \u03b2. By setting \u03b2 = W 1/6, an optimal coupling C of p and q is considered for the claim. The rest of the section focuses on proving Theorem E.2, with helper lemmas and a generalization claim following analogously to Lemma D.5. The proof involves neural networks approximating log p \u03b2 and log q \u03b2. By setting \u03b2 = W 1/6, an optimal coupling C of p and q is considered for the claim. The proof by reverse induction shows the iterative production of estimates, leading to the completion of the claim. The size/Lipschitz constant of the neural network is also discussed. The algorithm presented approximates the integral needed and can be implemented by a small, Lipschitz network. Parameters are defined, and the output of the \"invertor\" circuit is calculated. The algorithm approximates the integral using a small, Lipschitz network. Parameters are defined, and the output of the \"invertor\" circuit is calculated to find approximate eigenvector/eigenvalue pairs. Algorithm 1 approximates integrals using a small, Lipschitz network. Parameters are defined, and the output of the \"invertor\" circuit is calculated to find approximate eigenvector/eigenvalue pairs. Matrices are chosen from a set, and matrices with eigenvalues separated by a factor of \u03b2 are selected. The proof of Theorem E.9 is considered, and a universal bound on T \u03b2 is applied. Synthetic WGAN is performed. Theorem E.9 provides a universal bound on T \u03b2, with synthetic WGAN experiments using invertible neural net generators and discriminators designed with restricted approximability. The goal is to show the correlation between empirical IPM W F (p, q) and the KL-divergence on synthetic data. Data is generated from a ground-truth invertible neural net generator, X = G \u03b8 (Z), where Z is a spherical Gaussian. The generator network uses an invertible neural net with Leaky ReLU activation and well-conditioned weight matrices. The discriminator architecture is chosen for restricted approximability, and a trainable neural network models the non-differentiable function. Training involves generating batches from ground-truth and trained generators to solve a min-max problem. To train the generator and discriminator networks, stochastic batches are generated from both the ground-truth generator and the trained generator with a batch size of 64. The min-max problem in the Wasserstein GAN formulation is solved, with 10 updates of the discriminator in between each generator step. Various regularization methods are used for discriminator training. The RMSProp optimizer is employed as the update rule. Evaluation metrics include the KL divergence and training loss (IPM W F train) to assess distributional closeness. The KL divergence is considered a strong criterion for distributional closeness. The training loss (IPM W F train) is the unregularized GAN loss balanced carefully between discriminator and generator steps. The neural net IPM (W F eval) involves separately optimizing the WGAN loss to find an approximate maximizer for contrast. In experiments, a two-layer neural net is used as the generator in 10 dimensions. The discriminator is trained using Vanilla WGAN or WGAN-GP with the same ground-truth generator. Results are compared from 6 different random initializations. The study used a two-layer neural net as the generator in 10 dimensions and compared results from Vanilla WGAN and WGAN-GP with the same ground-truth generator. The main findings show that WGAN training with a discriminator design of restricted approximability can learn the true distribution in KL divergence, indicating that GANs are finding the true distribution without mode collapse. The W F (eval) and KL divergence are highly correlated, with adding a gradient penalty improving optimization significantly. The W F curve can serve as a good metric for monitoring convergence. The W F curve can serve as a good metric for monitoring convergence, reflecting improvements seen in the KL curve. Results show that IPM with vanilla discriminators also correlates well with KL-divergence, indicating the effectiveness of the discriminator design. The W F curve is a good metric for monitoring convergence and correlates well with the KL-divergence. The inferior performance of the WGAN-Vanilla algorithm in terms of KL divergence is due to training performance rather than statistical properties of GANs. The correlation between p and its perturbations is tested by comparing KL divergence and neural net IPM on pairs of perturbed generators. In testing the correlation between p and its perturbations, pairs of generators are generated with small Gaussian noise added. The KL divergence and neural net IPM between the generators are computed, showing a positive correlation. Outliers with large KL divergence are attributed to perturbations causing poorly conditioned weight matrices. Outliers with large KL divergence are caused by perturbations leading to poorly conditioned weight matrices. Experiments with vanilla fully-connected discriminator nets show good convergence in KL divergence, but weaker correlation compared to restricted approximability. This suggests that vanilla discriminator structures can be effective for generating good results, with room for improvement in the quality of the distance W F. The KL-divergence between true and learned distributions, estimated IPM, and training loss are shown in figures. The correlation between KL and neural net IPM is computed with different discriminator structures, showing similar results."
}