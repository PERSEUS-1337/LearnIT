{
    "title": "HkxStoC5F7",
    "content": "The paper introduces a new framework called ML-PIP for data efficient and versatile learning. It extends existing probabilistic interpretations of meta-learning and introduces \\Versa{}, which uses a flexible amortization network for few-shot learning. This network outputs a distribution over task-specific parameters in a single forward pass, eliminating the need for optimization at test time. The paper introduces \\Versa{}, a new framework for few-shot learning that sets new state-of-the-art results on benchmark datasets. It can handle arbitrary numbers of shots and classes at train and test time, demonstrating its power through a challenging ShapeNet view reconstruction task. The approach addresses the need for data-efficient learning in various applications, emphasizing information sharing across related tasks. Despite recent advances in meta-learning, there is still a lack of general purpose methods for flexible learning. In this paper, a framework called ML-PIP is developed for meta-learning approximate probabilistic inference for prediction. It extends existing point-estimate probabilistic interpretations of meta-learning to cover a broader class of methods, incorporating shared statistical structure between tasks via hierarchical probabilistic models. The framework ML-PIP leverages shared statistical structure between tasks and meta-learning for probabilistic inference. A new method called VERSA substitutes optimization procedures with forward passes through inference networks, resulting in faster test-time performance. VERSA employs a flexible amortization network for few-shot learning, outputting task-specific parameters in a single pass. It can handle arbitrary shots and classes for classification. The framework includes a multi-task probabilistic model and a method for meta-learning probabilistic inference, focusing on discriminative models and shared statistical structure between tasks. The standard multi-task directed graphical model employs shared parameters for all tasks and task-specific parameters. The goal is to meta-learn fast and accurate approximations to the posterior predictive distribution for unseen tasks. This framework provides a simple reframing and extension of existing approaches. Point estimates are used for the shared parameters. The framework for meta-learning approximate inference involves using point estimates for shared parameters and distributional estimates for task-specific parameters. The probabilistic solution for few-shot learning includes forming a posterior distribution over task-specific parameters and computing the posterior predictive. Emphasis is on quick approximation at test time. The framework for meta-learning approximate inference involves forming a posterior predictive distribution by learning an inference network with parameters \u03c6 that approximates the posterior predictive distribution. Amortization enables fast predictions at test time, with the form of distributions identical to those used in amortized methods. Additional approximation such as Monte Carlo sampling may be required for this step. The training method involves meta-learning the approximate posterior predictive distribution using a factorized Gaussian distribution. The goal is to minimize the KL-divergence between the true and approximate posterior predictive distribution, returning parameters that best approximate the distribution in an average KL sense. The training method involves meta-learning the approximate posterior predictive distribution using a factorized Gaussian distribution to return parameters that best approximate the distribution in an average KL sense. The procedure scores the approximate inference procedure by selecting a task at random, sampling training data, forming the posterior predictive, and computing the log-density. This process is repeated many times to provide an unbiased estimate of the objective for optimization. The training method involves optimizing the posterior predictive distribution by simulating approximate Bayesian log-likelihood evaluation. Unlike standard variational inference, the objective function focuses on minimizing KL divergence between the posterior predictive distribution and the inference network. The end-to-end stochastic training procedure aims to maximize predictive performance by optimizing shared parameters \u03b8. The training method involves optimizing the posterior predictive distribution through an end-to-end stochastic training objective for shared parameters \u03b8 and \u03c6. This approach maximizes predictive performance without requiring an explicit prior distribution over parameters. The approach for Meta-Learning Probabilistic Inference for Prediction (ML-PIP) does not need an explicit prior distribution over parameters. It unifies existing approaches and supports versatile learning for rapid and flexible inference. The approach for Meta-Learning Probabilistic Inference for Prediction (ML-PIP) enables rapid inference with deep neural networks without retraining, using sets as inputs for flexibility in processing variable numbers of observations. It is inspired by previous work on few-shot image classification. For few-shot image classification, a probabilistic model inspired by early work and recent extensions to deep learning is used. A shared feature extractor feeds into task-specific linear classifiers. Amortization of the approximate posterior is proposed to model the distribution over weight matrices, allowing for context-independent specification of weight vectors for each class. Amortization network is proposed for few-shot image classification to specify weight vectors for each class independently. The network maps image/angle examples to stochastic inputs and reduces the number of learned parameters by operating directly on extracted features. End-to-end training is employed to backpropagate to the generator. The implementation involves end-to-end training by backpropagating through the inference network. The classification matrix is constructed by performing feed-forward passes. The context independent approximation addresses limitations of naive amortization. The context independent approximation addresses limitations of naive amortization by reducing the number of parameters needed for inference, allowing meta-training with varying numbers of classes, and accommodating variable class numbers at test-time. It is applied to Few-Shot Image Reconstruction for complex output spaces, involving multi-output regression tasks. The generative model resembles a GAN or VAE decoder, using a latent vector for image generation. The generative model, similar to a GAN or VAE decoder, uses a latent vector and angle representation to produce images at specified orientations. Parameters of the generator network are global, while latent inputs are task-specific. A Gaussian likelihood in pixel space is used for generator outputs, with a sigmoid activation for output means between zero and one. An amortization network processes image representations and view orientations before instance-pooling. The amortization network processes image representations and view orientations before instance-pooling. ML-PIP unifies various meta-learning approaches, including gradient-based and metric-based variants, as well as amortized MAP inference and conditional modeling approaches. This process is illustrated in FIG5 and connects previous approaches to VERSA. The curr_chunk discusses gradient-based meta-learning and the concept of semi-amortized inference. It highlights the recovery of Model-agnostic meta-learning and provides a perspective on the update choice as one of amortization. The episodic meta-train/meta-test splits are not directly addressed in this perspective. VERSATILE (VERSA) is a distributional approach that simplifies inference by treating both local and global parameters without the need for back-propagation during training or gradient computation at test time. Amortized point estimates for task-specific parameters are constructed by averaging top-layer activations for each class. Amortized MAP inference method proposed for predicting weights of classes from activations of a pre-trained network to support online learning and transfer tasks. Hyper-networks are used to amortize learning about weights, with VERSA being a distributional approach that simplifies inference by treating both local and global parameters. The ML-PIP framework utilizes hyper-networks to amortize learning about weights and transfer from high-shot to low-shot classification tasks. VERSA goes beyond point estimates, supporting full multi-task learning by sharing information between tasks. Amortization network is part of the model specification for conditional models trained via maximum likelihood. The ML-PIP training procedure for \u03c6 and \u03b8 is equivalent to training a conditional model via maximum likelihood estimation. Comparison to Variational Inference (VI) shows that VERSA significantly improves few-shot classification and outperforms standard VI. In Section 5.2, VERSA's few-shot classification results on Omniglot and miniImageNet datasets show high accuracy across varying shot and way settings. Additionally, in Section 5.3, VERSA's performance on a one-shot view reconstruction task with ShapeNet objects is examined. An experiment is conducted to investigate the amortized posterior inference achieved by VERSA through generating data from a Gaussian distribution with varying means across tasks. In two experiments with N \u2208 {5, 10} train observations and M = 15 test observations, an inference network is introduced for amortizing inference. The learnable parameters are trained with an objective function, and the model is trained with Adam using mini-batches of tasks. Posterior distributions are inferred for unseen test sets, showing accurate recovery despite minimizing predictive KL divergence. The evaluation of VERSA on few-shot classification tasks shows accurate recovery of posterior distributions over \u03c8. The implementation follows specific sections and an approximate inference scheme. Training is episodic, with the approximate posterior closely resembling the true posterior for both Omniglot and miniImageNet datasets. The training for few-shot classification tasks using VERSA closely resembles the true posterior for both Omniglot and miniImagenet datasets. The approach involves episodic training with specific sections and an approximate inference scheme. The results are compared with competitive approaches in a tabular format. VERS achieves state-of-the-art results on 5-way -5-shot classification on miniImageNet and 20-way -1 shot Omniglot benchmarks using a convolution-based network architecture. It also performs competitively on other benchmarks. VERS achieves state-of-the-art results on various benchmarks, including 5-way -5-shot classification on miniImageNet and 20-way -1 shot Omniglot. VERSA adapts only the weights of the top-level classifier, outperforming amortized VI and non-amortized VI in terms of log-likelihood and accuracy. VI tends to under-fit, especially with small data points, while non-amortized VI improves performance but is slower in forming the posterior. Using non-amortized VI improves performance substantially, but does not reach the level of VERSA. Forming the posterior is significantly slower as it requires many forward/backward passes through the network. VERSA allows for varying the number of classes and shots between training and testing, demonstrating high accuracy across different conditions. VERSATILE (VERSA) demonstrates flexibility and robustness at test-time conditions, achieving 94% accuracy with 100-way conditions. It outperforms MAML in speed, taking only 53.5 seconds compared to 302.9 seconds for 1000 test tasks. VERSA also shows a 4.26% accuracy improvement over MAML. The dataset used includes 37,108 objects from 12 object categories in ShapeNetCore v2 BID5. VERSATILE (VERSA) uses 12 object categories with 37,108 objects. The dataset is split for training, validation, and testing. Each object has 36 views generated for evaluation. VERSA is compared to a conditional variational autoencoder (C-VAE) in an episodic training manner. Views of unseen objects from the test set are generated and compared to ground truth views. VERSATILE (VERSA) and a C-VAE are compared in generating views of unseen objects from the test set. VERSA produces sharper images with more detail, showing superior performance in quantitative metrics compared to the C-VAE. As the number of shots increases, VERSA's performance improves further. ML-PIP is a probabilistic framework for meta-learning that unifies various methods and suggests alternative approaches. Building on ML-PIP, VERSA is a few-shot learning algorithm that avoids gradient-based optimization at test time by amortizing posterior inference of task-specific parameters. VERSA demonstrated state-of-the-art performance on few-shot learning tasks, including a challenging 1-shot view reconstruction task. Prototypical Networks perform better when trained on higher \"way\" than that of testing, achieving 68.20% accuracy when trained on 20-way classification and tested on 5-way. The new inference framework in Section 2 is based on Bayesian decision theory, providing a recipe for making predictions for unknown test variables by combining information from observed training data. The text discusses Bayesian decision theory (BDT) and its application in meta-learning probabilistic inference. It introduces a stochastic variational objective for training that focuses on returning a full predictive distribution over unknown test variables. The quality of the predictive distribution is evaluated using a distributional loss function. The text introduces amortized variational training for optimizing the expected distributional loss with q constrained to a distributional family Q. It discusses forming quick predictions at test time and learning parameters by minimizing average expected loss over tasks using shared variational parameters. The optimal variational parameters are found by minimizing the expected distributional loss across tasks. The text discusses minimizing expected distributional loss across tasks using amortized variational training with q constrained to a distributional family Q. It involves sampling tasks and partitioning data for episodic minibatch training, emphasizing meta-learning. The log-loss function is employed, optimizing q to be closest to the true predictive distribution p(\u1ef9|D) in a KL sense. The text discusses the optimal q \u03c6 being the closest member of Q to the true predictive distribution p(\u1ef9|D) in a KL sense. It explores alternative scoring rules and task-specific losses for future work. The approximate predictive distribution is specified by replacing the true posterior with an approximation. The context-independent approximation is justified through density ratio estimation, considering the conditional density of each class. The optimal softmax classifier can be expressed in terms of conditional densities for each class, constructed independently similar to training a naive Bayes classifier. The approximation uses q \u03c6 (w|{x n |y n = c}) for each class in a given task, under ideal conditions. The experiment aims to evaluate the context-independent assumption by randomly generating fifty tasks from a dataset and performing free-form variational inference on the weights for each task. The goal is to see if the distribution of weights for a specific class remains similar regardless of other classes in the task. In an experiment evaluating the context-independent assumption, fifty tasks were randomly generated from a dataset for 5-way classification in the MNIST dataset. The model achieved 99% accuracy on test examples, and t-SNE plots showed clustering of class weights in 2-dimensional space with some overlap between classes. The t-SNE plots in the experiment showed clustering of class weights with some overlap between classes. For tasks containing both class '1' and '2', class '2' weights were located away from their cluster, suggesting independence of class-weights. A VI-based objective was derived for the probabilistic model, with \"amortized\" VI parameterized by a neural network and \"non-amortized\" VI optimized independently for each new task. The text discusses the derivation of an objective function for a probabilistic model using \"amortized\" and \"non-amortized\" Variational Inference (VI). The objective function includes an evidence lower bound (ELBO) and a stochastic estimator for optimization. The key differences between the two VI methods are highlighted, with the \"non-amortized\" VI not explicitly encouraging model generalization. The text provides details on few-shot classification experiments using the Omniglot dataset, which consists of 1623 handwritten characters from 50 alphabets. Images are resized and augmented with rotations for training, validation, and test sets. Training is done in an episodic manner for C-way, k c -shot classification tasks. Training for few-shot classification tasks is done episodically, with each iteration consisting of one or more tasks. C classes are randomly selected for each task, with k c character instances used for training and 15 for testing. The validation set is used for monitoring progress and model selection, while the final evaluation is done on 600 randomly selected tasks from the test set. The Adam BID25 optimizer with a learning rate of 0.0001 is used, with 16 tasks per batch. Models are trained for different iterations based on the number of classes and shots. The models are trained on the miniImageNet dataset with different iterations based on the number of classes and shots. The dataset consists of 60,000 color images divided into 100 classes. Training is done episodically with the Adam optimizer and a constant learning rate of 0.0001. The model is trained using different iterations based on the number of classes and shots. The neural network architectures for the feature extractor, amortization network, and linear classifier are detailed. The amortization network provides Gaussian parameters for the weight distributions of the linear classifier. The local-reparameterization trick is used when sampling from the weight distributions. The feature extraction network is shared with the pre-processing phase of the amortization network. The feature extraction network is shared with the pre-processing phase of the amortization network to reduce the number of learned parameters. The network architecture includes convolutional layers with dropout and pooling for both Omniglot and miniImageNet few-shot learning tasks. Batch Normalization and dropout with a keep probability of 0.5 are used throughout the process. The network architecture for ShapeNetCore v2 BID5 includes convolutional layers with dropout and pooling, resulting in a dataset of 37,108 objects from 12 categories. The dataset consists of 37,108 objects from 12 categories. Objects are randomly shuffled and split for training, validation, and testing. Each object has 36 views, with images converted to grayscale and resized to 32x32 pixels. The model is trained episodically on a single view per task, using the remaining views for evaluation. The model is trained on a single view selected randomly from 36 views per object. The remaining views are used to evaluate the system. A modified amortization network is used to generate 36 views of the object. Network architectures for encoder, amortization, and generator networks are described in Tables E.2 to E.4. Training is done with Adam BID25 optimizer, a learning rate of 0.0001, 24 tasks per batch, and 500,000 training iterations. The model architecture consists of multiple convolutional layers with pooling, followed by a fully connected layer with ReLU activation."
}