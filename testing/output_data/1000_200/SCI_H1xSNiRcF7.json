{
    "title": "H1xSNiRcF7",
    "content": "There is a growing interest in geometrically-inspired embeddings for learning hierarchies, partial orders, and lattice structures, with applications to transitive relational data. Recent work has extended these ideas to probabilistically calibrated models for learning from uncertain supervision and inferring soft-inclusions among concepts. The Box Lattice model by Vilnis et al. (2018) showed promising results in modeling soft-inclusions through high-dimensional hyperrectangles. However, the hard edges of the boxes present optimization difficulties. This work presents a novel hierarchical embedding model inspired by the Box Lattice model. In this work, a novel hierarchical embedding model is introduced, inspired by a relaxation of box embeddings into parameterized density functions using Gaussian convolutions. The approach provides an alternative surrogate to the original lattice measure, improving optimization robustness in the disjoint case. Performance improvements are demonstrated on various tasks, especially in sparse data scenarios. Embedding methods play a crucial role in machine learning by converting semantic problems into geometric representations. Embedding methods in machine learning convert semantic problems into geometric representations. Recent advancements include structured or geometric representations like Gaussian embeddings, order embeddings, and box embeddings, which express ideas of asymmetry through complex geometric structures. In this work, the focus is on the probabilistic Box Lattice model of Box embeddings (BE), which show strong empirical performance in modeling transitive relations and complex joint probability distributions. They replace vector lattice ordering with overlapping boxes, providing a more general notion for geometric representations in machine learning. The focus is on the Box Lattice model of Box embeddings, which use overlapping boxes for geometric representations in machine learning. The \"hard edges\" of boxes can cause issues for gradient-based optimization, especially with sparse data. BID22 introduce an ad-hoc surrogate function to address the disjoint case, while the current study looks at the problem differently. The current study introduces a new model inspired by relaxing hard edges of boxes into smoothed density functions using Gaussian convolution. It demonstrates superior performance in modeling transitive relations on various datasets compared to existing methods. The approach shows significant improvements in the pseudosparse regime and outperforms state-of-the-art results. The current study introduces a new model that extends the box lattice embedding model of BID22. Another hyperrectangle-based generalization of order embeddings was proposed by BID18. These models, along with hyperbolic space embeddings, optimize an energy function for learning hierarchical embeddings. The current study introduces a new model for learning hierarchical embeddings, focusing on embedding orderings and transitive relations within knowledge graphs. The model utilizes Gaussian convolution to smooth the energy landscape, a technique common in mollified optimization methods. This approach differs from previous nonprobabilistic models like order embeddings and box embeddings, as it aims to optimize an energy function for learning non-treelike DAGs. Our focus is on embedding orderings and transitive relations in knowledge graphs using a probabilistic approach. We aim to learn an embedding model that maps concepts to subsets of event space, suited for transitive relations and fuzzy concepts of inclusion and entailment. We introduce methods for representing ontologies as geometric objects, including vector and box lattices. The non-strict partially ordered set is defined by reflexivity, antisymmetry, and transitivity properties. A poset is a binary relation that generalizes the concept of a totally ordered set, allowing for incomparable elements. Posets are useful for representing acyclic directed graph data with transitive relations. A lattice, a type of poset, has unique least upper and greatest lower bounds for any subset of elements. In a bounded lattice, additional elements denote the least upper bound and greatest lower bound of the entire set. Lattices have binary operations for join and meet, satisfying specific properties. The extended real numbers form a bounded lattice and a totally ordered set. A bounded lattice satisfies properties such as unique least upper and greatest lower bounds for any subset of elements. The extended real numbers form a bounded lattice under min and max operations. A semilattice has only a meet or join. A vector lattice, also known as a Riesz space, is a vector space with specific properties. A vector lattice, also known as a Riesz space, is a vector space with a lattice structure. Order Embeddings of BID20 represent partial orders as vectors using the reverse product order. Objects become more specific as they move away from the origin in the vector lattice representation. In a two-dimensional example, Order Embedding vector lattice represents a simple ontology with a box lattice structure. Each concept in a knowledge graph is associated with two vectors, creating a natural partial order and lattice structure. The box lattice structure involves maximum and minimum coordinates of an axis-aligned hyperrectangle, allowing for representation of boxes and scalar coordinates. The lattice meet is the largest box contained within both x and y, while the lattice join is the smallest box containing both x and y. Marginal probabilities of events are given by the volume of boxes under a suitable probability measure. The probability measure is compatible with the meet semilattice. When using gradient-based optimization to learn box embeddings, an issue arises when two concepts are incorrectly labeled as disjoint, leading to zero gradient signal flow due to the meet being zero. This problem is especially problematic in sparse lattices where most boxes have little to no intersection. The authors propose a surrogate function to optimize when intervals are disjoint in sparse lattices, aiming to avoid gradient sparsity issues. They suggest a more principled framework to develop alternate measures for optimization and model quality improvement. The authors propose a relaxation of the standard box embeddings to address gradient sparsity issues in sparse lattices. They suggest using kernel smoothing functions with infinite support to replace indicator functions for better optimization and model quality improvement. The authors propose using kernel smoothing functions with infinite support to replace indicator functions in sparse lattices for better optimization. This approach involves convolution with a normalized Gaussian kernel and applying the diffusion equation to the original embeddings. The solution to the integral is given by a closed form solution involving the standard normal CDF and the softplus function. The solution to equation 2 involves the standard normal CDF and the softplus function. In the zero-temperature limit, the formula simplifies to equation 1. This is expected from convolution with a zero-bandwidth kernel. The Gaussian-smoothed indicators do not provide a valid meet operation on a function lattice due to idempotency issues. A modification of equation 3 allows for a function p(x \u2227 x) = p(x) while maintaining the smooth optimization properties of the Gaussian model. This is in contrast to the hinge function m h, which has different properties than the softplus function. In the zero-temperature limit, equations 3 and 7 are equivalent, but equation 7 is idempotent for overlapping intervals. This leads to defining probabilities using equation 7 instead of equation 3, satisfying idempotency in the one-dimensional box case. In the one-dimensional box case, probabilities are defined using equation 7 to satisfy idempotency. Softplus is used to upper-bound the hinge function, allowing for values greater than 1 that need to be normalized. Two normalization approaches are used in experiments, either dividing by measured sizes or projecting onto the unit hypercube and normalizing. The final probability is calculated as the product over dimensions. The probability function p(x) is calculated as the product over dimensions. It is not a valid probability measure on the entire joint space of events, similar to the Gaussian model. A comparison of different functions in FIG2 shows that the softplus overlap performs better for disjoint boxes while preserving the meet property. The Gaussian model requires lowering its temperature to achieve high overlap, leading to vanishing gradients in the tails. The Gaussian model must lower its temperature to achieve high overlap, causing vanishing gradients in the tails. Experiments on the WordNet hypernym prediction task show that the smoothed box model performs nearly as well as the original box lattice in terms of test accuracy. The smoothed box model performs nearly as well as the original box lattice in terms of test accuracy, but may perform better in a task with higher sparsity. Further experiments are conducted using different numbers of positive and negative examples from the WordNet mammal subset to compare the box lattice, the smoothed approach, and order embeddings as a baseline. The training data contains 1,176 positive examples, while the dev/test sets contain 209 positive examples. Negative examples are generated randomly. The experiments conducted on the WordNet mammal subset and Flickr entailment dataset show that the Smoothed Box model outperforms OE and Box models on imbalanced data. The F1 scores of the models are compared for different levels of label imbalance, with the Smoothed Box model consistently performing better. This superior performance is crucial for real-world entailment graph learning, where negatives greatly outnumber positives. The Flickr entailment dataset contains 45 million image caption pairs. Experimental details are provided, including KL divergence and Pearson correlation results. Performance gains are seen, especially on unseen captions. The method is also applied to a market-basket task using the MovieLens dataset. In a market-basket task using the MovieLens dataset, the goal is to predict users' preference for movie A given they liked movie B. Pairs of user-movie ratings above 4 points are collected from the MovieLens-20M dataset, resulting in 8545 movies. The conditional probability P(A|B) is calculated and compared with various baselines. Separate embeddings are used for target and conditioned movies, with an additional vector for the \"imply\" relation in the complex bilinear model. Evaluation is done on the test set. The study used separate embeddings for target and conditioned movies, with an additional vector for the \"imply\" relation in the complex bilinear model. Results show that the smoothed box embedding method outperforms the original box lattice and other baselines, especially in Spearman correlation. The model is easier to train due to fewer hyper-parameters. The study introduced a smoothed box embedding model that outperformed other baselines, especially in Spearman correlation. This model is easier to train with fewer hyper-parameters and is effective for sparse data and poor initialization. The research highlights the challenges in learning from geometrically-inspired embedding models and the need for further exploration in function lattices and constraint-based approaches. The study introduced a smoothed box embedding model that outperformed other baselines, especially in Spearman correlation. This model is easier to train with fewer hyper-parameters and is effective for sparse data and poor initialization. The research also explores function lattices and constraint-based approaches to learning. The Gaussian overlap formula is evaluated for lattice elements x and y with smoothed indicators f and g, using a normalized Gaussian kernel. The integral of interest is evaluated using Fubini's theorem, leading to optimization by the smoothed model on the MovieLens dataset. The MovieLens dataset is suitable for optimization by the smoothed model due to a large proportion of small probabilities. Additional experiments are conducted to test the model's robustness to initialization, specifically focusing on the distribution of boxes with minimum coordinates and positive widths. The smoothed box model shows resilience to disjoint initialization, unlike the original box model, which degrades significantly. Methodology and hyperparameter selection details for each experiment are provided, with code available for reproduction on GitHub. The WordNet experiments involve evaluating the model on the development set every epoch. The experiments conducted in this section involve training baseline models using BID22 parameters and a smoothed model with hyperparameters determined on the development set. Negative examples are randomly generated based on a ratio for each batch of positive examples. A parameter sweep is done for all models to select the best result for each model. The experimental setup uses the same architecture as BID22 and BID9, with a single-layer LSTM producing box embeddings parameterized by min and delta. The model architecture includes a single-layer LSTM that reads captions and generates box embeddings. Hyperparameters are determined on the development set, with the model evaluated every 50 steps. The best development model is used to score the test set."
}