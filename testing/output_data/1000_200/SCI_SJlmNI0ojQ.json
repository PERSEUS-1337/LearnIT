{
    "title": "SJlmNI0ojQ",
    "content": "End-to-end acoustic-to-word speech recognition models are popular for their ease of training, scalability, and lack of a lexicon requirement. Contextual acoustic word embeddings can be constructed directly from these models using attention distribution. These embeddings perform competitively on standard sentence evaluation tasks and match text-based embeddings in spoken language understanding tasks. In spoken language understanding tasks, embeddings from speech recognition match text-based embeddings. Methods like word2vec, GLoVE, CoVe, and ELMo are popular in natural language processing. Speech recognition faces challenges due to variability in speakers, acoustics, and microphones. In contrast to prior work aligning speech and text or segmenting input speech into fixed-length segments, this study focuses on constructing individual acoustic word embeddings based on utterance-level acoustics. Our work focuses on constructing individual acoustic word embeddings from utterance-level acoustics using an attention-based sequence-to-sequence model. This model automatically segments and classifies input speech into words without the need for pre-defined boundaries. The acoustic word embeddings are learned in the context of their containing sentence and show promise in non-transcription tasks. In this paper, the authors demonstrate the usability of attention for aligning words to acoustic frames and constructing Contextual Acoustic Word Embeddings (CAWE). They show that CAWE is competitive with text-based word2vec embeddings on standard sentence evaluation benchmarks. Additionally, they highlight the utility of CAWE for Spoken Language Understanding tasks, suggesting that pretrained speech models could be used for transfer learning. The text discusses the training of A2W models using CTC and S2S models, emphasizing the need for large training data and vocabulary. Solutions for generating out-of-vocabulary words involve using smaller units like characters or sub-words. A study presents an S2S model for pure-word A2W recognition with a 300-hour Switchboard corpus. The text discusses the training of A2W models using CTC and S2S models, emphasizing the need for large training data and vocabulary. A study presents an S2S model for pure-word A2W recognition with a 300-hour Switchboard corpus, exploring ways to learn acoustic word embeddings. BID6 uses a supervised Convolutional Neural Network for speech recognition with short speech frames as input. BID4 proposes an unsupervised method for learning speech embeddings using a fixed context of words. Research has also focused on learning contextualized word embeddings for text-based tasks. Research has progressed into learning contextualized word embeddings BID2 BID3 from a fully trained machine translation model for downstream tasks. Our work connects A2W speech recognition with learning contextual word embeddings from speech using a S2S model similar to the Listen, Attend and Spell model. The encoder in our model is a pyramidal multi-layer bi-directional Long Short Term Memory (BLSTM) network, while the decoder is an LSTM network that models the output distribution over the next target based on previous predictions. The decoder network in the model is an LSTM network that learns to model the output distribution over the next target based on previous predictions. It uses an attention mechanism to generate targets from the input. The model follows the same experimental setup as word-based models but learns 300 dimensional acoustic feature vectors instead of 320 dimensional ones. The model described in the previous work BID12 learns 300 dimensional acoustic feature vectors. Acoustic word embeddings are obtained from an end-to-end trained speech recognition system using hidden representations from the encoder and attention weights from the decoder. The method is similar to CoVe BID2 for text embeddings but addresses the challenge of aligning input speech with output words using location-aware attention. The location-aware attention mechanism assigns higher probability to specific frames, allowing for automatic segmentation of continuous speech into words. This segmentation is used to generate word embeddings by weighing the importance of acoustic frames based on their attention weights. This process creates word representations that correspond to the acoustic context, as illustrated in Figure 1. The attention mechanism assigns higher probability to specific frames for automatic segmentation of speech into words, generating word embeddings by weighing the importance of acoustic frames. The hidden representations and attention weights are colored according to their correspondence with a particular word. The model obtains mappings of words to acoustic frames, and describes three ways of using attention to obtain acoustic word embeddings. Contextual Acoustic Word Embeddings (CAWE) are derived from three techniques: unweighted Average, Attention weighted Average, and maximum attention. These techniques use attention scores to map acoustic frames to words in a speech recognition setup using the Switchboard corpus. The A2W model utilizes the Switchboard corpus and a subset of the How2 dataset for speech recognition. It achieves word error rates of 22.2% on Switchboard and 36.6% on CallHome from the Switchboard Eval2000 test set, and 24.3% on the dev5 test set of How2. The embeddings are evaluated in 16 sentence evaluation tasks covering Semantic Textual Similarity, classification, and sentiment analysis. The curr_chunk discusses sentence evaluation tasks covering various aspects such as Semantic Textual Similarity, classification, sentiment analysis, entailment, and semantic relatedness using the SICK dataset. The evaluation measures Spearman's coefficient of correlation and classification accuracies. Logistic regression is used for classification in downstream evaluations. The SentEval toolkit BID26 is utilized for evaluation. The curr_chunk discusses the use of logistic regression for classification tasks, specifically comparing CAWE-M and CAWE-W embeddings on Switchboard and How2 datasets. CAWE-M outperforms CAWE-W on average performance for STS tasks, attributing the difference to the construction method of the embeddings. The curr_chunk discusses the construction methods of embeddings, comparing CAWE-M and U-AVG to CAWE-W. CAWE-M uses the most confident attention score, while U-AVG weights all encoder hidden representations equally. The datasets for downstream tasks are the same as described in Section 5.1, and training details involve comparing embeddings from the speech recognition model's training set to text-based word embeddings trained on all transcripts. This ensures a fair comparison between supervised and unsupervised techniques. The performance of CAWE is competitive with word2vec CBOW despite the A2W model's limited vocabulary. Evaluations show that CAWE-M outperforms word2vec embeddings on 10 out of 16 tasks when concatenated with text embeddings. This indicates that CAWE-M adds more information that improves performance. The concatenated embedding of CAWE-M outperforms word2vec embeddings on 10 out of 16 tasks, showing improved performance. The gains are more noticeable in Switchboard compared to How2 dataset due to the nature of the speech. Additionally, CAWE is evaluated on the ATIS dataset for Spoken Language Understanding, which is similar in domain to Switchboard. Our model architecture for the speech-based downstream evaluation task is similar to a simple RNN-based model with an embedding layer, RNN variant, dense layer, and softmax. We train the model for 10 epochs with RMSProp and different seed values, showing that speech-based word embeddings can match text-based word embeddings in performance. The study compares the performance of speech-based word embeddings to text-based word embeddings in a downstream task. Contextual acoustic word embeddings are learned from a speech recognition model, showing competitive results with word2vec text embeddings. Two variants of these embeddings outperform the simple average method by up to 34% in semantic textual similarity tasks. The speech-based embeddings also match the performance of text-based embeddings in spoken language understanding. The study shows that contextual audio embeddings outperform the simple average method by up to 34% in semantic textual similarity tasks and match the performance of text-based embeddings in spoken language understanding. The model has the potential to improve downstream tasks similar to text embeddings, despite the complexity of noisy audio input. Future work will focus on scaling the model to larger corpora and vocabularies, and comparing with non-contextual acoustic word embedding methods."
}