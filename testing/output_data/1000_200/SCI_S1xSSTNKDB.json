{
    "title": "S1xSSTNKDB",
    "content": "Existing public face image datasets are biased towards Caucasian faces, leading to inconsistent classification accuracy for non-White race groups. To address this bias, a new balanced face image dataset with 108,501 images representing 7 race groups was created. The model trained on this dataset showed improved accuracy and consistency across different race and gender groups. The model trained on a new balanced face image dataset showed improved accuracy and consistency across different race and gender groups. Comparison with commercial computer vision APIs also reported balanced accuracy across gender, race, and age groups. Numerous large-scale face image datasets have fostered research and development for automated face detection, alignment, recognition, generation, and modification. Existing public face datasets are biased towards Caucasian faces, with other races significantly underrepresented. This bias can lead to inaccurate results and models that do not generalize well across different groups without calibration. Biased data in models can lead to ethical concerns about fairness in automated systems, as seen in studies critiquing commercial computer vision systems for their accuracy disparities across sub-demographics. Unwanted biases in image datasets can easily occur due to biased selection, capture, and negative sets. To address biases in image datasets, a novel face dataset with balanced race composition was proposed, containing 108,501 facial images from various sources. The dataset includes 7 race groups and is well-balanced among them. The paper highlights three main contributions, including empirical evidence of the dataset's effectiveness. The dataset proposed contains 7 well-balanced race groups, including East Asian, Southeast Asian, Middle Eastern, and Latino. It outperforms existing datasets on novel data and is the first to include Latino and Middle Eastern faces, expanding its applicability in various fields. The inclusion of major racial groups in face attribute recognition datasets significantly expands the applicability of computer vision methods. Existing datasets are mostly dominated by the White race, but efforts are being made to include more diverse demographics. Face attribute recognition is used in tasks like face verification and person re-identification. Computer vision tasks like face verification and person re-identification must perform well across different gender and race groups to maintain public trust. Incidents of racial bias, such as Google Photos mistaking African American faces for Gorillas, have led to service termination or feature removal. Commercial providers have stopped offering race classifiers due to these issues. Face attribute recognition is used for demographic surveys in marketing and social science research. Social scientists use images to infer demographic attributes and analyze behaviors, with the cost of unfair classification being significant. The cost of unfair classification is significant as it can lead to over- or under-estimation of specific sub-populations, impacting policy decisions. The AI and machine learning communities are increasingly focusing on algorithmic fairness and addressing biases in datasets and models. Research in fairness aims to ensure balanced accuracy, where attribute classification is independent of race and gender, to produce fair outcomes like loan approvals. Studies in algorithmic fairness involve auditing existing bias in datasets and systems. The paper focuses on addressing bias in datasets and systems for fairness in gender classification from facial images. Previous research has shown biases in commercial gender classification systems, particularly towards dark-skinned females. The study falls into the categories of discovering bias in datasets and creating better datasets. The paper aims to mitigate biases in existing databases by collecting more diverse face images from non-White race groups, improving generalization performance to novel datasets. Their dataset includes Southeast Asian and Middle Eastern races, addressing discrimination by including major race groups often excluded. The dataset defines 7 race groups: White, Black, Indian, East Asian, Southeast Asian, Middle Eastern, and Latino. Race is based on physical traits while ethnicity is based on cultural similarities. Latino is considered a race based on facial appearance. The paper aims to address discrimination by including major race groups often excluded in datasets. In the dataset, 7 race groups are defined based on physical traits. Latino is considered a race, not just an ethnicity, judged by facial appearance. Subgroups like Middle Eastern, East Asian, Southeast Asian, and Indian are also included. Skin color is used as a proxy for racial or ethnicity grouping in some studies, but it has limitations due to lighting conditions affecting skin color. The Pilot Parliaments Benchmark dataset used profile photographs of government officials taken in controlled lighting conditions, making it non-in-the-wild. Skin color variations and the one-dimensional nature of skin color make it difficult to differentiate between race groups. Race is a multidimensional concept, so human annotators are used to annotate physical race in addition to skin color. The curr_chunk discusses the use of race and skin color in face datasets sourced from public figures like politicians and celebrities. The selection of these populations may introduce biases due to factors like age and attractiveness. Some datasets are collected through web searches using keywords, potentially leading to stereotypical faces or prioritizing celebrities. The curr_chunk discusses minimizing selection bias in dataset collection by starting from a large public image dataset and incrementally increasing dataset size. The dataset is smaller but more balanced on race, with 7,125 faces randomly sampled and annotated. The dataset was initially sampled with 7,125 faces, and demographic compositions of each country were estimated. To avoid bias towards the White race, adjustments were made in the number of images collected. The minimum face size detected was set at 50 by 50 pixels, allowing for recognizable attributes. Only images with specific Creative Commons licenses were used, and Amazon Mechanical Turk was utilized for annotations. We used images with \"Attribution\" and \"Share Alike\" Creative Commons licenses for annotation of race, gender, and age. Three workers were assigned per image, with ground-truth values determined by agreement. Annotations were refined using a model and manually verified. Skewed race compositions were measured, and datasets with race annotations were used for statistics. The study measured dataset skewness in race composition and found bias towards White race in face attribute datasets. Gender balance was relatively better, ranging from 40%-60% male ratio. Model performance was compared using ResNet-34 architecture trained on each dataset with ADAM optimization. Face detection was done using dlib's CNN-based face detector, and attribute classification was performed in PyTorch. The study compared dataset skewness in race composition and gender balance using ResNet-34 architecture trained on different datasets. Faces were detected with dlib's CNN-based face detector and attributes classified in PyTorch. FairFace dataset defined 7 race categories but only 4 were used for comparison. CelebA lacked race annotations and was only used for gender classification. Cross-dataset classifications were performed by alternating training and test sets. The study compared dataset skewness in race composition and gender balance using ResNet-34 architecture trained on different datasets. Faces were detected with dlib's CNN-based face detector and attributes classified in PyTorch. FairFace dataset defined 7 race categories but only 4 were used for comparison. CelebA lacked race annotations and was only used for gender classification. Cross-dataset classifications were performed by alternating training and test sets. The classification results for race, gender, and age on the datasets across subpopulations were shown in Tables 2 and 3. The model's accuracy was highest on some variables on the LFWA+ dataset, which is the most biased, while also being very close to the leader in other cases. The generalization performance of the models was tested on three novel datasets. To test the generalization performance of the models, three novel datasets were considered. These datasets were collected from different sources than the data from Flickr and were not used in training. The test datasets contain people in various locations, including images from geo-tagged Tweets and photographs from online media outlets. The study utilized public datasets of tweet IDs from known media accounts and a protest dataset collected from Google Image search. 8,000 faces were randomly sampled from each dataset and annotated for gender, race, and age. The study randomly sampled 8,000 faces from different datasets and annotated them for gender, race, and age. The FairFace model outperformed other models in accuracy for race, gender, and age on novel datasets, even with smaller training image sizes. This suggests dataset size is not the only factor influencing model performance. The study found that models trained with fewer images (9k and 18k) outperformed larger datasets like CelebA, indicating dataset size isn't the sole factor for performance. The model showed consistent results across different race groups for race, gender, and age classification. Fair classification was measured using standard deviations of accuracy on various sub-populations. The FairFace model achieves the lowest maximum accuracy disparity in gender classification, with less than 1% accuracy discrepancy between male and female, and White and non-White groups. Other models show a strong bias towards males, yielding lower accuracy on females. The study found that most models exhibit a bias towards males in gender classification, with lower accuracy on females and non-White groups. The largest gender performance gap was observed in the LFWA+ dataset. The unbalanced representation in training data is likely the cause of these biases. Additionally, data diversity was measured using t-SNE visualization of facial embeddings from various datasets. The facial embedding used in the study was based on biased datasets like FaceScrub and VGG-Face, resulting in loosely separated race groups in the FairFace dataset. The diversity of faces in datasets like LFWA+ and UTKFace was measured by examining pairwise distances between faces. The study examined pairwise distances between faces in different datasets using a 128-dimensional facial embedding from dlib. UTKFace had tightly clustered faces, while LFWA+ showed diverse faces despite mostly white faces. The diversity was attributed to the training data, not actual appearance. Distribution of pairwise distances of faces in 3 datasets measured by L1 distance on face embedding. Compared to previous studies on face analytic models, this research used FairFace images for gender classification testing with various online APIs. The dataset is more diverse in terms of race, age, expressions, head orientation, and photographic conditions, making it a better benchmark for bias measurement. 7,476 random samples from FairFace were used, excluding children under 20 due to ambiguity in gender determination. The experiments on gender classification accuracy of face analytic models were conducted using diverse FairFace images, excluding children under 20. Results from Microsoft, Face++, IBM, and FairFace APIs were compared, showing varying levels of accuracy. Table 6 displays gender classification accuracies of tested APIs, with Amazon Rekognition detecting all faces. Two sets of accuracies are reported, including a model trained with their dataset for comparison. Results show a bias towards male category, consistent with previous reports. The results show gender classifiers favoring males, especially with dark-skinned females having higher error rates. Skin color alone is not a reliable indicator of bias. Face detection also introduces gender bias, with Microsoft's model failing to detect male faces. The paper proposes a balanced face image dataset for studying model bias. This paper introduces a novel face image dataset balanced on race, gender, and age, derived from the Yahoo YFCC100m dataset. The dataset shows improved generalization classification performance for gender, race, and age compared to existing datasets. It can be used for training new models and verifying balanced accuracy of existing classifiers, emphasizing the importance of algorithmic fairness in AI development. The novel dataset proposed in this paper aims to mitigate race and gender bias in computer vision systems, promoting algorithmic fairness. It can be used for training new models and verifying classifier accuracy. The dataset addresses concerns about transparency in large-scale image datasets and aims to improve societal acceptance of AI systems."
}