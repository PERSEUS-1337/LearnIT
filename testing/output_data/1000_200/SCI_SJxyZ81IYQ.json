{
    "title": "SJxyZ81IYQ",
    "content": "Mainstream captioning models often face issues like irrelevant semantics, lack of diversity, and poor generalization. This paper introduces a new image captioning paradigm with two stages: extracting explicit semantic representation from the image and constructing the caption recursively. This approach preserves semantic content better and follows a natural human language structure. The current text chunk discusses the limitations of the sequential model in image captioning, which fails to reflect the hierarchical structures of natural languages. Despite its effectiveness on benchmarks, this model lacks the ability to capture inherent language structures. The sequential model in image captioning struggles to capture hierarchical structures of natural languages, leading to captions that are syntactically correct but semantically irrelevant. This model relies on n-gram statistics and lacks the ability to generalize due to the entanglement of syntactic rules and semantics. To address these issues, a new paradigm for image captioning is proposed, focusing on semantics extraction and construction. The proposed paradigm for image captioning aims to address the limitations of sequential models by decomposing semantics extraction and caption construction into two stages. It involves deriving a semantic representation of the image using noun-phrases and recursively composing captions. The proposed paradigm for image captioning involves decomposing semantics extraction and caption construction into two stages using parametric modular nets. This approach aims to better preserve semantic content and make caption generation easier to interpret and control. The proposed paradigm for image captioning involves a recursive composition procedure that reflects natural language structures and captures hierarchical dependencies among words and phrases. It increases caption diversity, preserves semantic correctness, generalizes well to new data, and maintains good performance with limited training data. The literature in image captioning has evolved from early bottom-up, detection-based approaches to more advanced neural network methods. Recent works on image captioning have shifted towards using convolutional neural networks for image representation and recurrent neural networks for caption generation. Different approaches have been proposed, such as using a single feature vector for image representation with LSTM for word generation, or using a set of feature vectors with an attention mechanism to extract relevant image information. These advancements aim to improve caption diversity, semantic correctness, and generalization to new data. Recent approaches in image captioning involve attention mechanisms in the recurrent decoder to extract relevant image information. Various adjustments have been made, such as attending to generated text, adding an additional LSTM for better control, and reformulating latent states as 2D maps. Some methods directly extract phrases or semantic words from input images, while others predict frequent training words occurrences. Different strategies include treating noun-phrases as hyper-words and using a hierarchical approach with one LSTM deciding on phrases to produce. The authors proposed a hierarchical approach in BID21 where one LSTM decides on phrases to produce, while the second-level LSTM generates words for each phrase. However, sequential caption generation tends to favor frequent n-grams, leading to issues like incorrect semantic coverage and lack of diversity. In contrast, the proposed paradigm represents the input image with noun-phrases and constructs captions through a recursive composition procedure, preserving semantics effectively, requiring less data, and resulting in more diverse captions. The proposed compositional paradigm in the current chunk allows for the generation of captions with multiple objects, unlike previous work by Kuznetsova et al. The recursive composition procedure used in the paradigm enables the generation of diverse and semantically rich captions with less data. The proposed two-stage framework for image captioning involves deriving noun-phrases as a semantic representation and constructing captions in a bottom-up manner using a recursive compositional procedure called CompCap. This approach allows for the generation of diverse and semantically rich captions with multiple objects. The CompCap framework for image captioning involves deriving noun-phrases as a semantic representation and constructing captions in a bottom-up manner. This approach allows for generating diverse and semantically rich captions with multiple objects, capturing nonsequential dependencies among words and phrases in a sentence. In the CompCap framework for image captioning, noun-phrases are extracted as a semantic representation to generate diverse and semantically rich captions. The task of noun-phrase extraction is formalized as a multi-label classification problem, with a list of distinct noun-phrases derived from training captions. In the CompCap framework for image captioning, noun-phrases are extracted as a semantic representation to generate diverse and semantically rich captions. The noun-phrases are selected based on frequency and treated as classes for binary classification using visual features extracted from images. The caption is constructed recursively using a compositional procedure called CompCap, with a pruning step to remove semantically similar concepts. The CompCap framework for image captioning uses a recursive compositional procedure to construct captions by connecting noun-phrases with a Connecting Module. The module generates longer phrases by concatenating sequences of words and selects the phrase with the maximum connecting score as the new phrase. An Evaluation Module is then applied to assess the new phrase. The CompCap framework uses a recursive compositional procedure to construct captions by connecting noun-phrases with a Connecting Module. The module selects the phrase with the maximum connecting score as the new phrase, which is then evaluated by an Evaluation Module to determine if it is a complete caption. The process continues until a complete caption is obtained or only a single phrase remains in the pool. The C-Module in the CompCap framework focuses on generating connecting phrases as a classification problem due to the larger space of incomplete captions. This approach differs from using an LSTM for decoding intermediate words in captions. The limited number of distinct connecting phrases is observed, as semantic words like nouns and adjectives are not included in these phrases. For instance, in MS-COCO BID4, there are around 1 million samples collected for the connecting module, containing only about 1,000 distinct phrases. The connecting module in the CompCap framework focuses on generating connecting phrases as a classification problem, with a limited number of distinct phrases mined from training captions. A two-level LSTM model is used to encode left and right phrases, interacting with visual features for attention control. The CompCap framework uses a two-level LSTM model to encode phrases and interact with visual features for attention control. The connecting module generates connecting phrases as a classification problem, with different parameters for encoding phrases based on their position in the ordered pair. The softmax output values are used as connecting scores to determine the best connecting phrase. A virtual neg class is added for pairs that cannot be connected, and scores for phrases are computed using the C-Module. The Evaluation Module (E-Module) determines if a phrase is a complete caption by encoding it into a vector and evaluating its probability. It can also check other properties like caption quality using a caption evaluator. The framework for generating diverse captions can be extended by using beam search or probabilistic sampling to avoid local minima. User preferences and other conditions can also be incorporated into the framework. The framework for generating diverse captions can be extended to incorporate user preferences or other conditions. Operations are interpretable and controllable, allowing for influence on resultant captions by filtering noun phrases or modulating scores. Experimental examples are provided using MS-COCO and Flickr30k datasets. The vocabulary sizes for MS-COCO and Flickr30k datasets are 9,487 and 7,000 respectively, with training captions limited to 18 words. Ground-truth captions are parsed into trees using NLP toolkit BID31 for training data. The C-Module and E-Module are separately trained for classification tasks, making the compositional procedure less sensitive to training statistics and better at generalizing. Each step of the procedure during testing involves two forward passes, with 2 or 3 steps typically needed to obtain a complete caption. Various methods are compared in the experiments. The comparison of different methods for image captioning includes Neural Image Captioner (NIC) BID2, AdapAtt, TopDown BID1, and LSTM-A5 BID19. All methods are trained using the same hyperparameters and ResNet-152 BID16 pretrained on ImageNet. During training, ResNet-152 is used to extract image features without finetuning, and a fixed learning rate of 0.0001 is set for all methods. Best performing parameters are selected for caption generation during testing. CompCap selects 7 noun-phrases with top scores to represent the input image. Beam-search of size 3 is used for baselines and pair selection. No beam-search is used for connecting phrase selection. The quality of generated captions is compared. The quality of generated captions is compared using various metrics on the offline test set of MS-COCO and Flickr30k. CompCap with predicted noun-phrases performs best under the SPICE metric but lags behind in CIDEr, BLEU-4, ROUGE, and METEOR. These results reflect the differences between sequential and compositional caption generation methods. CompCap's compositional generation procedure preserves semantic content effectively, but may include n-grams not in the training set. A study on its components showed a boost in metrics when using groundtruth noun-phrases from associated captions. This indicates CompCap's ability to generate better captions with improved semantic understanding of input images. CompCap generates improved captions by integrating ground-truth noun-phrases in composing order, boosting metrics except for SPICE. The compositional paradigm separates semantics and syntax, allowing CompCap to handle out-of-domain content effectively and requiring less data to learn. Studies confirm its ability to generate better captions with enhanced semantic understanding. In two studies, CompCap was tested on different datasets, showing competitive results when trained on in-domain and out-of-domain data. The ability to disentangle semantics and syntax allows for improved performance across datasets. The distribution of syntax is stable across datasets, while semantics varies. CompCap can generate diverse captions by varying noun-phrases or composing order. Diversity is analyzed using five metrics, including novel and unique captions ratio, vocabulary usage, and pair-wise editing distances. The diversity of captions generated by CompCap is quantified using pair-wise editing distances. Two metrics are used: diversity at the dataset level and diversity at the image level. CompCap achieved the best results in all metrics, indicating diverse and novel captions. CompCap obtained the best results in all metrics for computing diversity at the image level, suggesting diverse and novel captions. Error analysis revealed that misunderstandings of input visual content were the main cause of errors in captions generated by CompCap, which could be addressed with more sophisticated techniques in noun-phrase extraction. The proposed method for image captioning involves a compositional approach, extracting noun-phrases from the input image and assembling them into captions hierarchically. This method preserves semantics effectively, requires less training data, generalizes well across datasets, and produces more accurate captions. The proposed compositional approach for image captioning involves extracting and assembling semantically similar noun-phrases to improve caption diversity and accuracy. This method utilizes encoders in the C-Module to determine semantic similarity based on central nouns and input images. The C-Module uses encoders to compute normalized euclidean distances between noun-phrases to determine semantic similarity based on input images. The sum of distances is compared to a threshold for similarity assessment. The C-Module uses encoders with independent parameters to compute distances between noun-phrases for semantic similarity. Additional hyperparameters for CompCap can be adjusted, such as beam search sizes, but have minor influence on performance."
}