{
    "title": "HyexAiA5Fm",
    "content": "Generative adversarial networks (GANs) are neural generative models successful in modeling high-dimensional continuous measures. A scalable method for unbalanced optimal transport (OT) is presented using the generative-adversarial framework. The approach involves learning a transport map and a scaling factor to push a source measure to a target measure in a cost-optimal manner. The formulation is theoretically justified and an algorithm based on stochastic alternating gradient updates is proposed. Numerical experiments show its application in population modeling. The methodology discussed involves unbalanced optimal transport for population modeling, focusing on transforming one measure to another using mass variation and transport. It considers the evolution of features and local mass variations to model sub-populations in the target population. Modern approaches are based on the Kantorovich formulation, seeking optimal probabilistic coupling between measures. Recent advancements in optimal transport methods have focused on the Kantorovich formulation, aiming to find the optimal probabilistic coupling between measures. Regularizing the objective with an entropy term has improved efficiency in solving the dual problem using the Sinkhorn algorithm. Applications of optimal transport include computer graphics, domain adaptation, image translation, natural language translation, and domain adaptation. Generative models like GANs are used to learn transport maps in cases where transport costs are unavailable. Recent advancements in optimal transport methods have focused on the Kantorovich formulation, aiming to find the optimal probabilistic coupling between measures. Applications of optimal transport include computer graphics, domain adaptation, image translation, natural language translation, and biological data integration. GANs are used to learn transport maps in cases where transport costs are unavailable, with strategies such as conditioning or cycle-consistency employed to enforce correspondence between original and transported samples. Several formulations have been proposed for extending the theory of optimal transport to handle mass variation, including algorithms for approximating solutions to optimal entropy-transport problems. These algorithms have been used in applications such as computer graphics and tumor growth modeling. The novel framework presented aims to solve unbalanced optimal transport by directly modeling mass variation in addition to transport. It proposes a Monge-like formulation to learn a stochastic transport map and scaling factor for cost-optimal transport between source and target measures. This generalizes the unbalanced Monge OT problem and builds on recent successes of GANs for high-dimensional transport problems. The framework aims to solve unbalanced optimal transport by modeling mass variation and proposing a Monge-like formulation for cost-optimal transport. It generalizes the unbalanced Monge OT problem and introduces a scalable methodology for solving the relaxed problem. The methodology is demonstrated in population modeling using various datasets, and a new scalable method is proposed for solving the optimal-entropy transport problem in the continuous setting. The proposed scalable method (Algorithm 2) in the Appendix addresses the optimal-entropy transport problem in the continuous setting, providing a scalable alternative for large datasets. It extends the work on unbalanced optimal transport and introduces key notation for probability measures and pushforward operators. Optimal transport aims to find cost-optimal ways to transport measures, with functions projecting onto X and Y for marginals. Optimal transport (OT) deals with transporting measures in a cost-optimal way. Monge formulated the problem as a search over deterministic transport maps. The Kantorovich OT problem is a convex relaxation of the Monge problem, formulating OT as a search over probabilistic transport plans. Conditional probability distributions specify stochastic maps from X to Y, a \"one-to-many\" version of the deterministic map. The relaxed problem is a linear program that is always feasible and can be solved in O(n^3) time for discrete measures. The relaxed problem in optimal transport is a linear program that can be solved efficiently. Entropic regularization simplifies the dual optimization problem, leading to the use of the Sinkhorn algorithm. Stochastic algorithms have been proposed for computing transport plans that can handle continuous measures. Various formulations have extended classical optimal transport to handle mass variation. Existing numerical methods are based on optimal-entropy transport, which relaxes marginal constraints using divergences. The first algorithm for unbalanced optimal transport directly models mass variation and can be applied to high-dimensional continuous measures. It builds on the state-of-the-art iterative scaling algorithms for discrete settings and proposes a Monge-like formulation. The development of an algorithm for unbalanced optimal transport involves a Monge-like formulation that aims to learn a stochastic transport map and scaling factor to push a source to a target measure in a cost-optimal manner. The algorithm considers the cost of transport and mass variation, seeking a transport map and scaling factor that satisfy specific constraints. The unbalanced Monge optimal transport problem involves finding a stochastic transport map and scaling factor to push a source measure to a target measure in a cost-optimal way. This model is more suitable for practical problems where one source can give rise to multiple targets. The algorithm considers transport costs, mass variation, and specific constraints to achieve this optimization. The transport map T models the movement of points from a source to a target measure, while the scaling factor \u03be represents growth or shrinkage of these points. Different transformation models are optimal based on mass transport costs and variation. An unbalanced transport map with a scaling factor can address class imbalances by adjusting sample weights in the source distribution. The scaling factor in the transport map helps balance class distributions by adjusting sample weights. A relaxation technique using a divergence penalty replaces the equality constraint, leading to a Monge-like version of the optimal-entropy transport problem. This reformulation involves a joint measure \u03b3 and changes the objective function for optimal-entropy transport. The objective function for optimal-entropy transport is obtained by using \u03b3 instead of (T, \u03be). The search space is different between the formulations, as not all joint measures \u03b3 \u2208 M + (X \u00d7 Y) can be specified by (T, \u03be). In the asymmetric Monge formulation, all mass transported to Y must come from within the support of \u00b5. Equivalence is established by restricting the support of \u03b3 to supp(\u00b5) \u00d7 Y. Theorem 3.4 states that solutions of the relaxed problem converge to solutions of the original problem under certain conditions.\u03b3 k is the joint measure specified by a minimizer of L \u03b6 k \u03c8 (\u00b5, \u03bd). The joint measure \u03b3 k converges weakly to \u03b3, specified by a minimizer of L(\u00b5, \u03bd). The transport map and scaling factor can be learned using stochastic gradient methods, optimizing the objective with neural networks. The optimization procedure involves parameterizing T, \u03be, and f with neural networks to minimize divergence between transported and real samples. The objective is similar to GAN training, with T transporting points from X to Y, \u03be determining importance weights, and f acting as an adversary. Cost functions encourage finding a cost-efficient strategy. The probabilistic Monge-like formulation involves solving a non-convex optimization problem using neural networks to learn a transport map and scaling factor. This approach enables scalable optimization and practical applications with efficient computation of point transport. The neural architectures imbue their function classes with a specific structure for effective implementation. The neural architectures of T, \u03be imbue their function classes with a particular structure for effective learning in high-dimensional settings. Algorithm 1 may not find the global optimum due to non-convexity, while the scaling algorithm of BID8 solves a convex optimization problem but has limited scalability. A new stochastic method in the Appendix can handle transport between continuous measures, overcoming the scalability limitations of BID8. The new stochastic method in the Appendix generalizes the approach for handling transport between continuous measures, overcoming scalability limitations. However, the output is less interpretable compared to Algorithm 1, making it unclear how to obtain a scaling factor or a stochastic transport map. The problem of learning a scaling factor arises in causal inference, aiming to balance distributions from control and treated populations. In causal inference, \u00b5 represents the distribution of covariates from a control population, while \u03bd represents the distribution from a treated population. The goal is to scale the importance of different members from the control population based on their likelihood to be present in the treated population, to eliminate selection biases in treatment effect inference. Algorithm 1 is used for unbalanced optimal transport, illustrated with applications in population modeling like MNIST-to-MNIST datasets. The source dataset contains regular MNIST digits, while the target dataset contains either regular or dimmed MNIST digits. The class distribution imbalance between the two datasets mimics a scenario where certain classes become more popular while others become less popular. Algorithm 1 was evaluated for transporting the source distribution to the target distribution, with a high cost of transport enforced. The scaling factor for each digit class reflects its imbalance ratio between the two distributions. The scaling factor learned by Algorithm 1 reflects class imbalances and models growth or decline of different classes in a population. Unbalanced OT is applied from the MNIST dataset to the USPS dataset to simulate evolution over time. Transport cost is based on Euclidean distance between original and transported images. Unbalanced transport is visualized in FIG1. The unbalanced transport from the MNIST dataset to the USPS dataset is visualized in FIG1, showing the predicted appearance of images with scaling factors indicating prominence changes. Despite limitations, many MNIST digits retained their likeness during the transport, with brighter digits generally having higher scaling factors and covering a larger area of pixels. The study applied Algorithm 1 on the CelebA dataset to model the transformation of young faces to aged faces using unbalanced optimal transport. This involved training a variational autoencoder on the dataset and encoding samples into a latent space before performing the transport based on Euclidean distances. The study utilized Algorithm 1 on the CelebA dataset to transform young faces into aged faces using unbalanced optimal transport. Faces in the young population were encoded into a latent space, and the transport cost was based on Euclidean distance. The transported faces generally retained key features, with exceptions like gender swaps. Interestingly, faces with higher scaling factors were more likely to be male. The study applied Algorithm 1 on the CelebA dataset to transform young faces into aged faces using unbalanced optimal transport. Faces with higher scaling factors were more likely to be male, indicating a gender imbalance between the young and aged populations. In zebrafish embryogenesis, lineage tracing of cells between different developmental stages is of great interest in biology. The study applied Algorithm 1 to zebrafish embryogenesis data, showing cells in earlier stages more poised for later stage development. Using single-cell gene expression data, cells with higher scaling factors were enriched for genes related to differentiation and mesoderm development. The study applied Algorithm 1 to zebrafish embryogenesis data, showing cells in earlier stages more poised for later stage development. Cells with higher scaling factors were significantly enriched for genes associated with differentiation and mesoderm development. A stochastic method for unbalanced OT based on the regularized dual formulation of BID7 is presented, which can lead to meaningful biological discoveries. The dual formulation involves a constrained optimization problem that can be made unconstrained by adding a strongly convex regularization term to the primal objective. The regularized dual formulation of BID7 is applied to zebrafish embryogenesis data, showing cells in earlier stages more poised for later stage development. A stochastic method for unbalanced OT involves a constrained optimization problem with a smoothing effect on the transport plan. The dual optimizer can be optimized using neural networks and stochastic gradient descent. The dual optimizer can be optimized using neural networks and stochastic gradient descent. Algorithm 2 describes the parameterization of u, v with neural networks and optimization using SGD for Unbalanced OT. The dual solution learned from Algorithm 2 can be used to reconstruct the primal solution based on the transport map indicating mass transported between points in X and Y. The dual optimizer can be optimized using neural networks and stochastic gradient descent. Algorithm 2 parameterizes u, v with neural networks and optimizes using SGD for Unbalanced OT. The dual solution learned can reconstruct the primal solution based on the transport map indicating mass transported between points in X and Y. The relation in (10) defines \u03b3 * as a transport map showing mass transport between every pair of points in X and Y. The marginals of \u03b3 * with respect to X and Y may not be \u00b5 and \u03bd, allowing for mass variation. An \"averaged\" deterministic mapping from X to Y can be learned from \u03b3 * using the barycentric projection T : X \u2192 Y. A stochastic algorithm for learning such a map from the dual solution is presented in Algorithm 3. The formulations are equivalent when considering \u03b3 instead of (T, \u03be). Lemma 3.3 formalizes the relation between the formulations. The proof shows that L\u03c8(\u00b5, \u03bd) \u2265 Wc1,c2,\u03c8(\u00b5, \u03bd) and L\u03c8(\u00b5, \u03bd) \u2264 Wc,\u03c81,\u03c82(\u00b5, \u03bd). The inequality holds for any (T, \u03be), taking the infimum over the left-hand side yields L\u03c8(\u00b5, \u03bd) \u2264 Wc,\u03c81,\u03c82(\u00b5, \u03bd). By the disintegration theorem, there exists a family of probability measures {\u03b3 y|x} such that \u03b3 y|x is the pushforward measure of \u03bb under Tx for all x \u2208 X.\u03c0X#\u03b3 is restricted to the support of \u00b5, i.e. \u03c0X#\u03b3 \u226a \u00b5. Let \u03be be the Radon-Nikodym derivative d\u00b5. It follows that (T, \u03be) satisfy the same relation as in (12). The proof shows that under certain conditions, the joint measure specified by any minimizer of L\u03c8(\u00b5, \u03bd) is unique. This result follows from the analysis of optimal entropy-transport by BID27. The equivalence of formulations leads to theoretical results for (6), such as the existence and uniqueness result mentioned. The proof establishes the uniqueness of the joint measure determined by any minimizer of L\u03c8(\u00b5, \u03bd) under specific conditions. This result is derived from the study of optimal entropy-transport by BID27, leading to theoretical implications for (6), including the existence and uniqueness outcomes. Based on the theoretical analysis of BID27 and constrained optimization results, it is shown that solutions of the relaxed problem converge to solutions of the original problem. The convergence is proven by showing that the penalty term converges to the equality constraint, leading to the conclusion that the minimizer of the relaxed problem is greater than or equal to the minimizer of the original problem. Lemma 3.9 in BID27 states that the limit of Wc1,c2,\u03b6k\u03c8(\u00b5, \u03bd) is greater than or equal to Wc1,c2,\u03b9=(\u00b5, \u03bd). The sequence of minimizers \u03b3k is bounded and equally tight, leading to the existence of a weak subsequence by an extension of Prokhorov's theorem. Inequality: For any \u03bb > 0, there exists a subsequence of \u03b3k that weakly converges to \u03b3, a minimizer of Wc1,c2,\u03b9=(\u00b5, \u03bd). The convex conjugate form of \u03c8-divergence is used to rewrite the main objective as a min-max problem. Lemma B.2 states a condition for non-negative finite measures P, Q over T \u2282 Rd. Lemma B.2 provides conditions for non-negative finite measures P, Q over T \u2282 Rd. It states that equality holds under certain conditions, with a simple proof provided. This result has been used in generative modeling and a rigorous proof can be found in a referenced source. The support of P \u22a5 is equal to \u03c8 \u221e, completing the proof. Proposition B.1 gives conditions on cost functions c1, c2 for wellposed problems. It is convenient for c1 to measure correspondence between X and Y, such as Euclidean distance. For c2, choose a convex function that vanishes at 1 and prevents \u03be from becoming too small or too large. Various entropy functions can be used for \u03c8-divergences. The text discusses the use of \u03c8-divergences and associated entropy functions for training generative models. It explains how Jensen's inequality applies to probability measures P and Q, but not to non-probability measures. The example of the original GAN paper is used to illustrate this concept. The text discusses \u03c8-divergences and entropy functions for training generative models. The Jensen-Shannon divergence is minimized when P = Q for probability measures. For non-probability measures, an additional constraint on \u03c8 is needed to match P to Q. If \u03c8(s) attains a unique minimum at s = 1 with \u03c8(1) = 0 and \u03c8 \u221e > 0, then D \u03c8 (P |Q) = 0 \u21d2 P = Q. The text discusses the use of \u03c8-divergences and entropy functions for training generative models. It explains that for non-probability measures, an additional constraint on \u03c8 is needed to match P to Q. Examples of activation layers and neural architectures are provided for implementing this constraint. In Section 4 experiments, fully-connected feedforward networks with 3 hidden layers and ReLU activations were used. The output activation layers were a sigmoid function for T and a softplus function for \u03be."
}