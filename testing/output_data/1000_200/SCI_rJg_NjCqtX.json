{
    "title": "rJg_NjCqtX",
    "content": "Chemical information extraction involves converting chemical knowledge in text into a chemical database, heavily relying on identifying and standardizing chemical compound names. A framework is proposed in this paper for auto standardization from non-systematic names to systematic names using spelling error correction, tokenization, and neural sequence to sequence model. The framework is fully data-driven and achieves a standardization accuracy of 54.04% on the test dataset, a significant improvement from previous results. The International Union of Pure and Applied Chemistry (IUPAC) defines rules for assigning systematic names to chemical substances. However, besides systematic names, there are also common names or trivial names for chemicals that are more familiar to people. For example, sucrose is commonly known as sugar. In chemistry, besides systematic names assigned by IUPAC, there are also common names or trivial names for chemicals like sucrose (sugar). In the industry, proprietary names are created to distinguish products, like Aspirin for 2-Acetoxybenzoic acid. Chemical information extraction relies on standard chemical names for databases. Chemical databases like PubChem and SciFinder store chemical information using standard chemical names. Extracting chemical information from papers is ongoing work to update these databases. Systematic names can be converted to SMILES and InCHI representations for generating structural formulas with high precision. Chemical databases store chemical information using standard names. Extracting information from papers updates these databases. Non-systematic names can have errors like spelling, ordering, common name, and synonym errors. Non-systematic chemical names can contain errors such as spelling, ordering, common name, and synonym errors. The most common error type is synonym error, where words in non-systematic names differ from systematic names but share the same root word. Errors can be mixed, making conversion to systematic names challenging. A framework is proposed to automatically correct spelling errors, tokenize using Byte pair encoding (BPE), and convert non-systematic names to systematic names. The framework proposed aims to standardize chemical names by correcting spelling errors, tokenizing using Byte pair encoding (BPE), and utilizing a sequence to sequence model to address ordering, common name, and synonym errors. Previous work by BID2 focused on chemical name standardization but was limited by its reliance on chemical knowledge. In contrast, the sequence to sequence model is applied due to its effectiveness in neural machine translation. The sequence to sequence model is adopted for standardizing chemical names, similar to machine translation. The framework is fully data-driven, achieving 54.04% accuracy on test data from a corpus of chemical names extracted from high impact Chemical Journals. The corpus from Chemical Journals with High Impact factors (CJHIF) contains 384816 data pairs of non-systematic and systematic names of chemical substances. The Levenshtein distance distribution between these names is shown in FIG1. The experiment uses 80% training data, 19% test data, and 1% development data to correct spelling errors in chemical names by separating them into elemental words. The experiment uses data from Chemical Journals with High Impact factors to correct spelling errors in chemical names by separating them into elemental words. This involves creating vocabularies of systematic and non-systematic elemental words, combining them to get a final elemental vocabulary. The experiment corrects spelling errors in chemical names by creating vocabularies of systematic and non-systematic elemental words, combining them to form a final elemental vocabulary. BK-Tree is used for efficient spelling error correction by rapidly identifying vocabulary items with the smallest Levenshtein distance from a given word. The BK-Tree is utilized for efficient spelling error correction in chemical names by identifying vocabulary items with the smallest Levenshtein distance from a given word. It allows for easy insertion of new training data, making it scalable. The process involves separating chemical names into elemental words, inputting them into the BK-Tree, correcting errors, and combining the elemental words to form the full name. This method helps reduce noise in training sequence to sequence models. The sequence-to-sequence model utilizes Byte Pair Encoding (BPE) for tokenization of chemical names. The symbol set is initialized by splitting names into characters and counting symbol pairs iteratively. This method helps reduce noise in training models. The symbol set is initialized by splitting names into characters and counting symbol pairs iteratively. Each merge operation produces a new symbol. The final symbol set size is equal to the initial character size plus the number of merge operations. Byte Pair Encoding (BPE) is chosen for tokenization as it can handle out-of-vocabulary problems and separate names into meaningful subwords. Examples of applying BPE to chemical names are shown in TAB1. The sequence to sequence model, based on BPE tokenization of chemical names, utilizes a bidirectional LSTM encoder to generate context vectors for decoding systematic names. This model, adapted from OpenNMT, is commonly used in machine translation. The encoder in the sequence to sequence model uses a bidirectional LSTM to create context vectors for decoding. The decoder calculates the probability of output sequences. Parameters for spelling error correction include the threshold of the BK-Tree, while parameters for the BPE stage include the number of merge operations. Various threshold and merge operation values were tested in experiments. In experiments, values of 2500, 5000, 10000, and 20000 merge operations were tested for the BPE stage. The sequence to sequence model uses 500 dimensions for word embeddings and hidden states, with 2 layers in both encoder and decoder. Spelling error correction is done before training. Parameters are trained jointly using SGD with a cross-entropy loss function. Initial weights are randomly initialized, and the learning rate starts at 1.0. The model is trained over a minibatch size of 64 with weights initialized using a random uniform distribution. The learning rate starts at 1.0 and decays by a factor of 0.5 every epoch after epoch 8 or when perplexity does not decrease. Dropout rate is 0.3, training lasts for 15 epochs, and beam size is set to 5 for decoding. Another experiment replaces the sequence to sequence model with Statistical Machine Translation using Moses system BID6. Training sequences are limited to 80 with a 3-grams language model and BPE tokenization with 5000 merge operations. Data augmentation is used to handle noisy data. Data augmentation is used with BPE for neural model learning to handle noisy data, specifically spelling errors. Four types of error insertion methods are applied with equal probability. Standardization quality is measured using accuracy and BLEU score BID10, with accuracy being a strong performance metric. The experiment results show that the combination of spelling error correction, BPE tokenization, and sequence to sequence model achieves the best performance. The framework has a significant improvement compared to other models. The results also highlight the usefulness of BPE in improving accuracy. The results demonstrate the effectiveness of BPE tokenization in a character-level sequence to sequence model for spelling error correction. Data augmentation also provides some benefit, although not as much as spelling error correction. Overcorrection may occur with large thresholds, impacting standardization quality. Examples in Table 6 showcase the model's ability to correct non-systematic names, including fixing non-alphabet spelling errors and synonym errors. The sequence to sequence model can correct non-alphabet spelling errors and synonym errors in chemical names. Examples include fixing the order of compounds and standardizing proprietary names. Visualization of attentions in an example demonstrates the model's effectiveness in correcting common name errors and ordering errors. The seq2seq model can correct non-alphabet spelling errors and synonym errors in chemical names, as demonstrated in examples of fixing compound order and standardizing names. The model effectively identifies common name errors and ordering errors, as shown in Figure 6. The system also analyzes failed standardization attempts, with synonym errors being the most challenging error type. The distribution of error types in chemical name standardization is analyzed, with synonym errors being the most challenging. The system performs well on spelling errors but struggles with common name errors. Among 100 samples, some are nearly correct, some are totally incorrect, and the rest are partially correct. Nearly half of non-systematic names are not successfully standardized. The accuracy for systematic names of different lengths is also discussed. Our framework achieves the best performance for systematic names of length between 20 and 40 but performs poorly for names longer than 60. The model does not consider chemical rules, leading to some generated names that do not follow the rules. Examples of failed attempts include \"5-bromo-2-methylbenzene-1-sulfonyl chloride\" being predicted as \"4-bromo-2-methylbenzene-1-sulfonyl chloride\" and \"choline dicarbonate\" being incorrectly predicted as \"(2-hydroxyethyl)trimethylazanium\". In this work, a framework is proposed to automatically convert non-systematic chemical names to systematic names using spelling error correction, tokenization, and a sequence to sequence model. The framework achieves 54.04% accuracy, outperforming previous rule-based systems and enabling practical extraction of chemical information. The advantage of this framework is its end-to-end training, data-driven approach, and independence from external chemical knowledge. This work introduces a new research line for chemical information extraction, utilizing an end-to-end, data-driven framework independent of external chemical knowledge."
}