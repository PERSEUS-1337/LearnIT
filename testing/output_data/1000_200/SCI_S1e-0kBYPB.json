{
    "title": "S1e-0kBYPB",
    "content": "To ensure public acceptance of AI systems, we need methods to explain decisions of black-box models like neural networks. Current explanatory methods face issues with different explanation perspectives and lack validation on complex models. A verification framework for explanatory methods is introduced to address these challenges. The curr_chunk discusses a verification framework for explanatory methods in neural networks, focusing on the feature-selection perspective. It aims to provide guarantees on the inner workings of a non-trivial neural network architecture. The framework is validated by highlighting the failure modes of current explainers and aims to offer an off-the-shelf evaluation for explanations. The curr_chunk discusses different perspectives on explanations in models, specifically feature-additivity and feature-selection. These perspectives lead to fundamentally different explanations for predicting a single input. Explanatory methods adhering to these perspectives are being compared in practice. For example, L2X is compared with LIME and SHAP. The curr_chunk discusses the comparison of different explanatory methods, highlighting the challenges of comparing feature-selection and feature-additivity explainers. It also raises questions about the reliability of explanatory methods when the target model has less dramatic biases due to the unknown decision-making process of neural networks. The curr_chunk discusses evaluating explainers for neural networks by assuming reasonable behavior of target models, despite potential reliance on spurious correlations in datasets. The curr_chunk proposes a framework to evaluate explainers for neural networks by generating tests under the feature-selection perspective. It aims to identify tokens that have zero contribution to the model's prediction and test if explainers rank them higher than relevant tokens. The framework was applied to three pairs of (target model, dataset) for multi-aspect sentiment analysis. The framework proposed evaluates explainers for neural networks by generating tests under the feature-selection perspective. It tests if explainers rank irrelevant tokens higher than relevant ones. The framework was applied to three pairs of (target model, dataset) for multi-aspect sentiment analysis, highlighting the need for evaluation tests with guarantees on the target model's behavior. The study introduces an automatic evaluation test for explainers that does not rely on speculation about the target model's behavior. LIME and SHAP perform better than L2X on the test, raising awareness of potential failure modes in explanatory methods. In Section 5, the study discusses reasons for believing L2X is less effective. Error rates of explanatory methods highlight possible failure modes in feature-selection explanations. Findings reveal instances where explainers misidentify relevant tokens. A generic evaluation test will be released for community use. Feature-based explainers explain predictions based on input unit-features like tokens or super-pixels. Feature-based explainers provide explanations for predictions by assigning weights to input features, either additively or selectively. Other types of explanations include example-based, which identify relevant instances in the training set, and human-level explanations that aim to mimic human reasoning. In this work, the focus is on verifying feature-based explainers, which are commonly used in current works. Evaluations are typically done on interpretable target models like linear regression and decision trees to assess the faithfulness of the explainer. However, these simple models may not be representative of the actual target model. In evaluating feature-based explainers, simple models like linear regression and decision trees are commonly used. However, these models may not represent the complexity of real-world neural networks. Synthetic setups and assuming reasonable behavior are other evaluation methods used to assess explainers. In evaluating feature-based explainers, simple models like linear regression and decision trees are commonly used. Synthetic setups and assuming reasonable behavior are other evaluation methods used to assess explainers. Crowd-sourcing evaluation is often performed to check if the explainer's features align with the model's predictions, but neural networks may rely on unexpected artifacts even with high accuracy, making this evaluation unreliable. Another evaluation involves humans predicting a model's behavior based on explanations provided by different explainers. In contrast to manual evaluation methods, our automatic framework assesses explainers on neural networks with guaranteed inner-workings, similar to a sanity check introduced by Adebayo et al. (2018). Our framework assesses explainers on neural networks with guaranteed inner-workings, challenging the fidelity of the explainer to the target model. Explanatory methods adhere to the Feature-additivity perspective, where explanations approximate the model prediction by summing contributions from each feature. The explainers on neural networks aim to approximate model predictions by summing feature contributions. Various methods, such as LIME and Shapley values, follow this feature-additivity perspective. Shapley values from game theory provide feature contributions by averaging over a neighborhood of the instance. Shapley values from game theory provide feature contributions by averaging over a neighborhood of the instance. The choice of the neighborhood is critical, and it is an open question what neighborhood is best to use in practice. Various methods aim to approximate model predictions by summing feature contributions. Shapley values from game theory provide feature contributions by averaging over a neighborhood of the instance. L2X learns S(x) by maximizing mutual information between S(x) and the prediction, but assumes the number of important features per instance is known, which is not always the case. The model may not always rely on a subset of features, as opposed to using all features, but this can be true for certain tasks like sentiment analysis. Instance-wise explanations for a hypothetical sentiment analysis regression model are provided to understand the differences between perspectives. In a hypothetical sentiment analysis regression model, the feature-additive explanation highlights the relevance of specific tokens in the input, such as \"nice\" and \"good,\" for determining the sentiment score. This demonstrates the differences between perspectives in understanding feature contributions. The feature-additive perspective emphasizes the importance of \"nice\" and \"good\" in determining sentiment scores, while the feature-selective perspective focuses on specific tokens like \"very\" and \"good\" in isolation. This highlights the differences in understanding feature contributions between the two perspectives. In the proposed verification framework for feature-selection perspective of instance-wise explanations, the RCNN model architecture is leveraged to identify relevant and irrelevant features in the dataset. Metrics are introduced to measure explainers' ranking failures. The RCNN model consists of a generator and an encoder, both using recurrent convolutional neural networks. The generator selects a subset of tokens from the input text, which is then used by the encoder to make predictions. The model is trained jointly with no direct supervision on subset selection. The RCNN model consists of a generator and an encoder trained jointly with no direct supervision on subset selection. The generator is encouraged to select a short sub-phrase and fewer tokens, using a REINFORCE-style procedure for gradient estimation. This facilitates the existence of tokens that do not contribute to the final prediction, potentially leading to an emergent communication protocol called a handshake. The RCNN model may have learned an internal communication protocol called a handshake, where non-selected tokens encode information via selected tokens. The goal is to eliminate handshakes by gathering a dataset where non-selected tokens have zero contribution to the model's prediction. The model's behavior is illustrated in an example, showing how eliminating handshakes affects the prediction score. Equation 7 captures the handshake in the example provided. Non-selected tokens are considered irrelevant or zero-contribution. Pruning the dataset ensures that non-selected tokens have no impact on the prediction. The goal is to eliminate handshakes by retaining instances where S Sx = S x. After pruning non-selected tokens, it is important to ensure that there is at least one selected token clearly relevant for the prediction. This is done by checking if removing the token results in a significant change in prediction. After pruning non-selected tokens, it is crucial to verify the relevance of selected tokens by checking if their removal causes a significant change in prediction. If the absolute change in prediction exceeds a threshold \u03c4, the token is considered clearly relevant. Tokens are then partitioned into clearly relevant (SR) and uncertain (SDK) categories. It is important to note that a token may be relevant in combination with others, even if it alone does not meet the threshold for prediction change. The procedure ensures that tokens causing a significant change in prediction are ranked higher. Only datapoints with at least one clearly relevant token are retained. Evaluation metrics focus on ranking features, with the most important token in SR x. The error metrics evaluate explainers providing rankings over features, measuring instances where relevant tokens are not selected or ranked lower than non-relevant tokens. The study evaluates explainers by measuring errors in ranking relevant tokens, focusing on metrics (A), (B), and (C). The framework is applied to the RCNN model trained on the BeerAdvocate corpus, consisting of 100K human-generated beer reviews with aspects like appearance, aroma, and palate. The RCNN predicts ratings rescaled between 0 and 1. The study evaluates explainers by measuring errors in ranking relevant tokens for the RCNN model trained on the BeerAdvocate corpus. Three separate RCNNs are trained for each aspect independently, with ratings rescaled between 0 and 1. A threshold of \u03c4 = 0.1 is chosen to identify clearly relevant tokens, with statistics provided in Appendix A. In Appendix A, statistics of the datasets are provided, including average lengths of reviews and selected tokens. The threshold of 0.1 for identifying relevant tokens is strict, with 1 or 2 relevant tokens per datapoint. Three popular explainers, LIME, SHAP, and L2X, were tested with default settings, except for L2X where word embeddings dimension was set to 200. In the evaluation of explainers, LIME and SHAP outperformed L2X on most metrics, despite L2X being a feature-selection explainer. One limitation of L2X is the need to know the number of important features per instance, which is usually unknown in practice. In testing L2X, the average number of tokens highlighted by human annotators was used as K, with values of 23, 18, and 13 for different aspects. Evaluation metrics showed that explainers like LIME and L2X sometimes ranked zero-contribution tokens higher than relevant features, indicating failures in the predicted ranking. The evaluation metrics show that SHAP and L2X have differences in how they rank zero-contribution tokens compared to relevant features. SHAP places fewer zero-contribution tokens ahead of relevant ones, while L2X places more. Qualitative analysis using heatmaps illustrates these rankings visually. The evaluation metrics reveal differences in how SHAP and L2X rank tokens, with SHAP placing fewer zero-contribution tokens ahead of relevant ones, while L2X places more. Qualitative analysis using heatmaps visually illustrates these rankings. Both explainers tend to attribute importance to nonselected tokens, with LIME and SHAP even ranking tokens like \"mouthfeel\" and \"lacing\" as most important. L2X prioritizes \"taste\", \"great\", \"mouthfeel\", and \"lacing\" as key tokens, while \"gorgeous\" is not ranked highly. In this work, an evaluation test for post-hoc explanatory methods was introduced, highlighting the distinction between two perspectives of explanations. The framework offers guarantees on the behavior of real-world neural networks, showcasing error rates of popular explanatory methods. The methodology, initially applied to natural language processing, is adaptable to other tasks like computer vision. The evaluation test for post-hoc explanatory methods is adaptable to various tasks like computer vision. The core algorithm in current explainers is domain-agnostic, providing a representative view of their limitations. Statistics of the dataset are provided in Table 2, showing the number of instances retained, average review length, and average numbers of selected tokens. The average numbers of selected tokens, selected tokens with a prediction difference of at least 0.1 when eliminated individually, and non-selected tokens are |S x |, |SR x |, and |N x | respectively. The percentage of instances eliminated from the dataset due to a potential handshake and further eliminated datapoints are provided. The proof involves the impact of non-selected tokens on the final prediction. The proof discusses the impact of non-selected tokens on the final prediction in the context of eliminating instances from the dataset due to a potential handshake. It concludes that if the model gives the same prediction after eliminating non-selected tokens, then there was no handshake in the instance. The proof shows that if the model's prediction remains the same after removing non-selected tokens, then there was no handshake in the instance. The beer, described as a nice brown \"grolsch\" like bottle, has a dark yellow color with fruity aromas and a smooth taste, better than most American lagers. The beer is poured from a nice brown \"grolsch\" like bottle, with a dark yellow color and fruity aromas. It has a smooth taste, better than most American lagers. The beer has a smooth taste with fruity aromas, better than most American lagers. It has a low alcohol content and a slight warming sensation."
}