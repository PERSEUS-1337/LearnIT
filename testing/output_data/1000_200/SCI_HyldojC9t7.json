{
    "title": "HyldojC9t7",
    "content": "The D2KE methodology constructs positive definite kernels from dissimilarity measures on structured inputs like time series, strings, histograms, and graphs. It utilizes random features to build a kernel, improving generalizability over universal Nearest-Neighbor estimates. D2KE offers better generalizability than universal Nearest-Neighbor estimates by subsuming representative-set method and generalizing Random Features methods to complex structured inputs. Our proposed framework excels in classification experiments across various domains such as time series, strings, and histograms for texts and images. It outperforms existing distance-based learning methods in terms of testing accuracy and computational time. It is often easier to specify a dissimilarity function between instances than to construct a feature representation, especially for structured inputs like real-valued time series, strings, histograms, and graphs. There are well-developed dissimilarity measures available, such as Dynamic Time Warping for time series. Standard machine learning methods are designed for vector representations, with less focus on distance-based methods for structured inputs. Nearest-Neighbor Estimation (NNE) is a common distance-based method, but it can be unreliable with high variance when neighbors are far apart. This is especially true for inputs with large intrinsic dimensions implied by the distance measure. Distance-based machine learning methods can be unreliable with high variance when neighbors are far apart, especially for inputs with large intrinsic dimensions. Research has focused on developing global distance-based methods by using similarity functions or kernel methods. One approach is to treat the data similarity matrix as a kernel Gram matrix and apply standard kernel-based methods like Support Vector Machines or kernel ridge regression. However, most similarity measures do not provide a positive-definite kernel, posing a challenge for empirical risk minimization. One approach to address the challenge of non-positive-definite kernels in similarity measures is to estimate a positive-definite Gram matrix that approximates the similarity matrix. This can be done by modifying eigenvalues or learning a PD approximation. However, these modifications may lead to information loss and inconsistency between training and testing samples. Another common approach is to select a subset of training samples as a representative set for distance or similarity calculations. In this paper, a novel general framework called D2KE (Distance to Kernel and Embedding) is proposed to construct a family of positive-definite kernels from a dissimilarity measure on structured inputs. This approach draws from the literature of Random Features to build novel kernels that outperform representative-set methods in various application domains. The D2KE framework proposes constructing kernels from a dissimilarity measure, creating a feature embedding for better generalization in classification and regression tasks across different domains. The framework outperforms existing distance-based learning methods in terms of testing accuracy. Our proposed framework constructs a family of PD kernels via Random Features from a given distance measure for structured inputs, outperforming existing distance-based learning methods in testing accuracy and computational time. It generalizes Random Features methods to complex structured inputs of variable sizes, accelerating kernel machines on structured inputs. This study introduces a novel approach using Random Features to accelerate kernel machines on structured inputs of variable sizes, outperforming existing distance-based methods. The method constructs PD kernels from a given distance measure, improving testing accuracy and computational efficiency across various domains such as time-series, strings, and histograms. The study introduces a novel approach using Random Features to accelerate kernel machines on structured inputs of variable sizes, outperforming existing distance-based methods. It constructs PD kernels from a given distance measure, improving testing accuracy and computational efficiency across various domains such as time-series, strings, and histograms. Additionally, various approaches are discussed for building PD kernels on structured inputs like text and time-series, modifying distance functions into kernels. Interest in approximating non-linear kernel machines using randomized feature maps has surged in recent years due to a significant reduction in training and testing times for kernel based learning algorithms. Various explicit nonlinear random feature maps have been constructed for different types of kernels, including Gaussian and Laplacian Kernels, intersection kernels, additive kernels, dot product kernels, and semigroup kernels. Among them, the Random Fourier Features (RFF) method approximates a Gaussian Kernel function. The Random Fourier Features (RFF) method approximates a Gaussian Kernel function using a Gaussian random matrix. To accelerate RFF on high-dimensional input data matrices, methods leveraging structured matrices have been proposed. However, existing RF methods only consider inputs with vector representations, while D2KE takes structured inputs of different sizes and computes the RF using a structured distance metric. D2KE differs from existing RF methods by using structured inputs of varying sizes and computing the RF with a structured distance metric. Unlike other RF approaches that rely on user-defined kernels, D2KE constructs a new PD kernel through a random feature map. It offers a unified framework for various structured inputs, including strings, histograms, and graphs, beyond the limitations of previous methods like BID49. The unified framework of D2KE allows for structured inputs like strings, histograms, and graphs, going beyond previous methods like BID49. It involves estimating a target function from samples, using a dissimilarity measure between input objects instead of a feature representation. The size of structured inputs can vary widely, such as strings with variable lengths or graphs. The dissimilarity measure in BID4 can vary widely for structured inputs like strings or graphs. An ideal dissimilarity measure for learning a target function should imply small differences in the function and small expected distances among samples. Lipschitz Continuity is an important assumption in this context. The text discusses the importance of Lipschitz Continuity in learning a target function with small expected distances among samples. It introduces the concept of Lipschitz-continuity constant and covering number to measure the size of the space implied by a given dissimilarity measure. The impact of these quantities on the estimation error of a Nearest-Neighbor Estimator is also explored. The text extends the analysis of the estimation error of k-nearest-neighbor to structured input spaces, defining effective dimension. An example is provided for measuring the space of Multiset using ground distance and covering numbers. The text introduces the concept of effective dimension in analyzing the estimation error of k-nearest-neighbor in structured input spaces. It discusses constructing coverings of sets with elements from V, providing a bound on the estimation error of the k-Nearest-Neighbor estimate of f(x). The proof is similar to standard analysis of k-NN's estimation error, with space partition number replaced by covering number and dimension replaced by effective dimension. In structured input spaces, the estimation error of k-NN decreases slowly with n when p X,d is large. To address this, an estimator based on a RKHS derived from the distance measure is developed with better sample complexity for higher effective dimension problems. A method called D2KE constructs positive-definite kernels from a given distance measure by introducing a family of kernels DISPLAYFORM0 where \u03c9 \u2208 \u2126 is a random structured object. In structured input spaces, a family of kernels is constructed from a structured input domain X and a distance measure d(., .). The kernels are parameterized by a distribution p(\u03c9) over random objects \u03c9 \u2208 \u2126 and a feature map \u03c6 \u03c9 (x) derived from the distance of x to all \u03c9 \u2208 \u2126. The kernel can be interpreted as a soft version of the distance substitution kernel, with a soft minimum function defined by p(\u03c9) and \u03b3. The kernel in Equation FORMULA13 is always PD by construction. To approximate it, draw R samples from p(\u03c9) to get {\u03c9 j } R j=1 and solve for a R-dimensional feature embedding a\u015d. Random Feature Approximation can be used to evaluate the kernel analytically. The kernel can be approximated using Random Features (RF) BID39, allowing for efficient use in large-scale settings with a large number of samples. By learning a target function as a linear function of the RF feature map, a domain-specific empirical risk can be minimized. This approach is different from a recent work that selects random features in a supervised setting, and could be extended to develop a supervised D2KE method. The RF based empirical risk minimization for D2KE kernels is outlined in Algorithm 1, where random feature embeddings are computed using a structured distance measure. This method contrasts with traditional RF approaches and will be further analyzed in Section 5. The relationship to the Representative-Set Method is discussed, showing a connection to the naive choice of p(\u03c9) in our approach. The approach outlined in Algorithm 1 for RF based empirical risk minimization for D2KE kernels is related to the representative-set method. By interpreting Equation (8) as a random-feature approximation to the kernel, a generalization error bound is obtained even as R approaches infinity. By interpreting Equation (8) as a random-feature approximation to the kernel, a much nicer generalization error bound is obtained even as R approaches infinity. The choice of p(\u03c9) in the kernel plays a crucial role, with \"close to uniform\" distributions giving better performance in various domains compared to the Representative-Set Method. For example, in the time-series domain with DTW dissimilarity, a distribution corresponding to random time series of uniform length and Gaussian-distributed elements outperforms RSM. The distribution p(\u03c9) corresponding to random time series, strings, and sets of vectors with specific characteristics outperforms the Representative-Set Method in various classification tasks. The synthetic nature of p(\u03c9) allows for an unlimited number of random features, resulting in a better approximation to the exact kernel. The synthetic nature of p(\u03c9) allows for an unlimited number of random features, resulting in a better approximation to the exact kernel. This framework analyzes error decomposition in the RKHS corresponding to the kernel, focusing on the population risk minimizer subject to the RKHS norm constraint. The RKHS corresponding to the kernel in Equation (4) is analyzed for error decomposition, focusing on the population risk minimizer subject to the RKHS norm constraint. The function approximation error is discussed, showing that any function in the RKHS is Lipschitz-continuous with respect to the given distance. The RKHS is Lipschitz-continuous with respect to the given distance, and additional smoothness is imposed via the RKHS norm constraint and kernel parameter. The estimation error is discussed, with a focus on setting the tuning parameter \u03bb as small as possible for better estimation error. The estimation error for a RKHS estimator has a better dependency on n (i.e. n \u22121/2) compared to the k-nearest-neighbor method, especially for higher effective dimension. Analyzing the error for the kernel in Equation FORMULA12 is challenging due to the lack of an analytic form. The error from Random Feature Approximation can be bounded using the same estimation error bound. The error from Random Feature Approximation can be bounded using the same estimation error bound as the empirical risk function. Analyzing the kernel approximation error involves uniform convergence, and the optimal solution of empirical risk minimization is considered. The approximation error in terms of kernel evaluation is discussed, leading to a corollary. The Representer theorem states that to guarantee a small error in empirical risk minimization, the number of Random Features needed is proportional to the effective dimension. This framework can achieve suboptimal performance, with small constants A and M for most loss functions. The Representer theorem states that the number of Random Features needed for small error in empirical risk minimization is proportional to the effective dimension. The estimated function from the random feature approximation based ERM estimator is denoted as R, and the desired target function is denoted as f *. The target function f * lies close to the population risk minimizer f C in the RKHS spanned by the D2KE kernel. The proposed method is evaluated in various domains such as time-series, strings, texts, and images, using different dissimilarity measures and distance-based methods. The study utilized various dissimilarity measures for different data types, including Dynamic Time Warping for time-series, Edit Distance for strings, Earth Mover's distance for Bags of Words, and (Modified) Hausdorff distance for Bags of Visual Words. C-MEX programs were adapted or implemented for computationally demanding distance measures with quadratic complexity. Four datasets were selected for each domain, with multivariate time-series data from the UCI Machine Learning repository. The datasets used in the study include multivariate time-series, string, text, and image data. The time-series data vary in length from 2 to 205 observations, string data have alphabets of size 4 to 8 and lengths ranging from 34 to 198, text data have document lengths from 9.9 to 117, and image data have SIFT feature vector sizes ranging from 1 to 914. The datasets were divided into 70/30 train and test subsets. The study divided datasets into 70/30 train and test subsets. The SIFT feature vector sizes of image data range from 1 to 914. Comparison was made against 5 state-of-the-art baselines with different kernel constructions and complexities. The study compared a new method, D2KE, with 5 baseline methods in terms of computational complexity. D2KE has linear complexity in both the number of data samples and sequence length, outperforming the baselines with quadratic complexity. Parameters were optimized using 10-fold cross validation, with random samples generated for D2KE to achieve close to exact kernel performance. Linear SVM with LIBLINEAR was employed for classification. The study compared D2KE with 5 baseline methods in terms of computational complexity. D2KE outperformed the baselines in classification accuracy while requiring less computation time. D2KE performed better than KNN and achieved better performance than DSK_RBF, DSK_ND, and KSVM methods. Our method outperforms DSK_ND and the KSVM method by achieving better performance with a representation induced from a truly PD kernel. RSM is closest to our method in feature matrix construction, but D2KE's random object sampling performs significantly better. Our proposed framework derives a positive-definite kernel and feature embedding function from dissimilarity measures for structured input domains like sequences, time-series, and sets. Our framework extends existing approaches for creating embeddings of structured objects based on distance to random objects, with a potential for deep architecture integration. The function g(t) = exp(\u2212\u03b3t) is Lipschitz-continuous, and our goal is to bound the magnitude of Hoefding's inequality for input pairs in X. Our framework extends existing approaches for creating embeddings of structured objects based on distance to random objects, with a potential for deep architecture integration. To get a bound that holds for all input pairs in X, we find a covering E of X with size N. By applying Hoefding's inequality and the Lipschitz-continuity of exp(\u2212\u03b3t), we can derive a result by choosing an appropriate parameter \u03b3. The process involves optimizing parameters using an approximate kernel and performing cross-validation to find the best parameters for each method. The text discusses the process of optimizing parameters for different methods, such as DSK_RBF, DSK_ND, KSVM, RSM, and D2KE, using 10-fold cross validation. Different techniques are employed, such as using an exact RBF kernel, squared distance, random selection for data samples, and linear SVM implementation. The goal is to achieve performance close to an exact kernel by generating random samples. All datasets used are collected from popular public websites. The study optimized parameters for various methods using 10-fold cross validation. Datasets were collected from public websites for Machine Learning research. Computation was done on a DELL system with Intel Xeon processors. Multithreading with 12 threads was used to speed up distance computations. DTW was employed for time-series data. In all experiments, various distance computations were optimized using 10-fold cross-validation. D2KE consistently outperformed other baselines in classification accuracy for multivariate time-series data, requiring less computation time. Notably, D2KE achieved 26.62% higher performance than KNN on IQ_radio due to KNN's sensitivity to data noise and poor performance for high-dimensional datasets like Auslan. Our method outperforms KNN, DSK_RBF, DSK_ND, and KSVM on high-dimensional data sets like Auslan. Compared to RSM, our method samples random time series for better performance. Our method utilizes a representation induced from a truly p.d. kernel, making better use of the data than indefinite kernels. Our method samples random time series to denoise and find patterns in data, offering a more abundant feature space compared to RSM. Levenshtein distance is used as the distance measure for string data, with parameters optimized for \u03b3 in the range [1e-5 1]. Results show that D2KE outperforms other distance-based baselines, including DSK_RBF with Levenshtein distance. D2KE offers a clear advantage over baseline methods, achieving better performance on large datasets with less computation. On large datasets, D2KE outperforms other baselines with less computation. It uses earth mover's distance for text data and google pretrained word vectors for document representation. D2KE outperforms other baselines on all four datasets by using distance based kernel methods and SVM over KNN for text data classification. Random documents of short length are effective for \"topic\" learning tasks. D2KE achieves significant speedup with random features for datasets with large documents. Hausdorff distance used for image data with SIFT-descriptors. Parameters optimized for \u03b3 and length of SIFT-descriptor sequence. D2KE outperforms other baselines in various cases, showing better performance than KNN and RSM. The quadratic complexity of other methods makes it hard to scale to large data, while D2KE remains effective. The use of random features may be due to the ineffectiveness of underlying SIFT features in finding patterns quickly in images."
}