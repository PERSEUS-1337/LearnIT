{
    "title": "HymuJz-A-",
    "content": "The text discusses the limitations of modern machine vision algorithms in learning visual relations, as demonstrated through experiments that strain convolutional neural networks. It also highlights the challenges faced by relational networks in solving visual question answering problems. The argument is made that feedback mechanisms like working memory and attention are crucial for abstract visual reasoning, drawing inspiration from biological vision. Feedback mechanisms like working memory and attention are essential for abstract visual reasoning, as demonstrated by the success of biological vision. Deep convolutional neural networks have shown remarkable feats in image classification tasks, surpassing human accuracy in some cases. The importance of these mechanisms is highlighted through experiments that strain CNNs and relational networks in solving visual tasks. The CNN struggles to learn the concept of \"sameness\" in images, despite excelling at classifying objects like a flute. This difficulty highlights the importance of feedback mechanisms in abstract visual reasoning, as demonstrated by biological vision. The difficulty of recognizing visual relations in images, such as the concept of \"sameness,\" is a challenge for contemporary computer vision algorithms, including CNNs and relational networks (RNs). RNs have shown success on visual question answering benchmarks but have limitations when tested on tasks like the same-different task. This failure is surprising given the ability of animals, including humans and primates, to recognize visual relations. The failure of modern computer vision algorithms to recognize visual relations across the animal kingdom is evident. Previous studies have shown that black-box classifiers and CNN architectures struggle with visual reasoning tasks, despite extensive training data. There has been a lack of systematic exploration of the limits of contemporary machine learning algorithms on relational reasoning problems. Recent work by BID4, BID12, and BID26 demonstrated the limitations of CNNs and other visual reasoning networks on visual-relation tasks. These experiments questioned whether the failure of feedforward neural networks in solving visual-relation problems is due to hyperparameters or a systematic issue within the model class. The study aimed to systematically probe the boundaries of CNNs and RNs in handling visual-relation tasks, showing that CNNs struggle with these tasks and RNs do not alleviate these limitations. The limitations of CNNs and RNs in handling visual-relation tasks are demonstrated through controlled experiments. The study suggests that brain mechanisms like working memory and attention are crucial for reasoning about visual relations. Current computer vision models need to incorporate these mechanisms to efficiently solve complex visual reasoning tasks. The study also introduces a novel visual-relation challenge and analyzes CNN performance on twenty-three SVRT problems, revealing a dichotomy between hard same-different problems and easy spatial-relation problems. The study introduces a novel visual-relation challenge, the SVRT challenge, consisting of twenty-three binary classification problems. It demonstrates that CNNs solve same-different tasks through rote memorization and suggests rethinking visual question answering challenges by drawing inspiration from neuroscience and cognitive science. The SVRT challenge involves twenty-three binary classification problems where CNNs show lower accuracies on same-different tasks compared to spatial-relation tasks. CNNs were trained on different hyper-parameter combinations, with the best-performing network's accuracies ranked for each problem. The high-throughput analysis tested CNNs of varying depths and receptive field sizes on twenty-three problems. The networks were trained on 2 million examples split into training and test sets, resulting in a total of n = 207 conditions. The study trained nine networks on twenty-three problems using code provided by BID6. The best networks' accuracy was sorted by problem type - red for Same-Different (SD) problems and blue for Spatial-Relation (SR) problems. This dichotomy is evident in the results. The study trained nine networks on twenty-three problems, with SD problems colored red and SR problems colored blue. CNNs performed much worse on SD problems compared to SR problems, with some SD problems resulting in accuracy barely above chance. SD tasks present a significant challenge to CNNs, matching earlier evidence of a visual-relation dichotomy. Hyperparameter search showed SR problems were consistently learned well across all network configurations, while larger networks yielded higher accuracy on SD problems. The study trained nine networks on twenty-three problems, with CNNs performing worse on SD problems compared to SR problems. Larger networks yielded higher accuracy on SD problems, while feedforward models performed poorly on visual-relation problems. The SVRT challenge has limitations in its sample of visual relations. The SVRT challenge includes a sample of visual relations that are arbitrary and difficult to compare directly due to different image structures and generation methods. Comparing visual-relation problems in the SVRT challenge is challenging due to different image structures and unique generation methods. Problems vary in required image configurations, such as object size and number, making direct comparisons difficult. To improve comparison, defining problems on the same set of images would be more effective. Additionally, using simple closed curves as items in SVRT images hinders the ability to control image variability based on generation parameters. The PSVRT challenge aimed to address issues with SVRT by creating a new dataset with two problems: Spatial Relations (SR) and Same-Different (SD). This dataset allowed for better quantification of image variability and its impact on task difficulty. The PSVRT challenge introduced two problems: Spatial Relations (SR) and Same-Different (SD). SR classifies images based on horizontal or vertical arrangement, while SD determines if images contain identical items. The image generator creates grayscale images using square binary bit patterns on a blank background. Parameters like item size, image size, and number of items control image variability. The item size, image size, and number of items control image variability. When k \u2265 3, the SD category label is determined by the presence of at least 2 identical items, and the SR category label is determined by the average orientation of item displacements. The number of possible images in a dataset is quantified as O(P n 2 ,k 2 km 2 ). The Parametric SVRT test, or PSVRT, quantifies the number of possible images in a dataset using parameters related to permutations. Images are generated by sampling class labels and items, with variations based on the labels. Items are randomly placed in an image with specific spacing. The experiment aimed to assess the difficulty of learning PSVRT problems with varying image parameters. A baseline architecture was able to learn both same-different and spatial-relation PSVRT problems for specific parameter settings. Training sessions were conducted for different combinations of item size, image size, and item number to measure problem difficulty using training-to-acquisition (TTA) as a metric. In the session, the number of training examples needed for the architecture to reach 95% accuracy was measured (TTA). The network was trained from scratch in various conditions without a holdout test set. Different image parameters were varied separately to assess their impact on learnability in three sub-experiments. The baseline CNN was trained in each condition with 20 million training images. Only the best-case results are reported. The baseline CNN was trained with 20 million images in different conditions, using a batch size of 50. It had four convolution and pool layers, followed by four fully-connected layers. The network used dropout in the last fully-connected layer and an ADAM optimizer with a base learning rate of 10^-4. The effect of network size on learnability was also examined. The baseline CNN was trained with 20 million images using a batch size of 50 and an ADAM optimizer with a base learning rate of 10^-4. The effect of network size on learnability was examined, showing a strong dichotomy in learning curves with a sudden rise in accuracy termed the \"learning event\". Training runs that exhibited this event almost invariably reached 95% accuracy within 20 million training images. The training runs showed a strong bi-modality in final accuracy, either at chance-level or close to 100%. In one condition, the learning event occurred immediately after training began and reached 95% accuracy soon after. However, in another condition, a significant straining effect was found from two image parameters, image size and number of items, making the learning event less likely as image size increased. Increasing image size and the number of items in an image had a significant impact on the network's ability to learn a problem. The network struggled to learn when image size was increased, with no learning events occurring at 150x150 and above. Additionally, having 3 or more items in an image also hindered the network's learning. Even when a more relaxed rule was applied to consider items as the same if they were congruent up to a 90-degree rotation, the CNN still did not learn for any parameter configuration. Increasing image size and the number of items in an image significantly impacted the network's ability to learn. The strict same-different rule quadrupled the number of matching images, straining CNNs due to increased image variability. The exponential relationship between image size and item number led to a quadratic-rate increase in variability with size and an exponential-rate increase with more items. This effect was consistent across CNNs of different widths, with a constant shift in the TTA curve. Increasing image size and the number of items in an image significantly impacted the network's ability to learn. The straining effect on CNNs due to increased image variability was consistent across different network widths, with a constant rightward shift in the TTA curve over image sizes. However, increasing item size did not produce a visible straining effect on CNNs. Learnability remained stable over the range of item sizes considered, suggesting the possibility of constructing feedforward feature detectors that can generalize to coordinated item variability. When CNNs learn a PSVRT condition, they build a feature set tailored for a specific data set rather than learning the rule itself. The features learned are not invariant rule-detectors but a collection of templates covering a particular distribution in the image space. The CNN's ability to learn decreases with increasing image variability, indicating sensitivity to irrelevant image variations. The Relational Network (RN) is an architecture designed to detect visual relations and outperforms a baseline CNN on various visual reasoning problems, including the \"sort-of-CLEVR\" VQA task. The RN sits on top of a CNN and learns a map from pairs of high-level CNN feature vectors to answers for relational questions, which can be provided as natural language or hardcoded binary strings. The entire system (CNN+LSTM+RN) can be trained end-to-end and was found to be effective in tasks with simple 2D items in images. The Relational Network (RN) outperformed a CNN on the \"sort-of-CLEVR\" VQA task by solving relational and non-relational questions using simple 2D items in images. The task lacks item variability and encourages rote memorization, hindering the learning of the concept of sameness. The RN model was trained on a two-item sort-of-CLEVR same-different task and PSVRT stimuli to measure its ability to transfer concepts and solve relational problems. The architecture details included a convolutional network with four layers and kernel sizes of 5x5, without pooling, and a stride of 3 in the first two layers. The model consisted of a convolutional network with four layers using 5x5 kernel sizes and ReLu activations, followed by a 4-layer MLP with 256 units per layer and a 3-layer MLP with 256 units per layer. The system was trained with a cross-entropy loss using an ADAM optimizer with a base learning rate of 2.5 x 10^-4 and Xavier initialization for weights. The model used a convolutional network with four layers and a multi-layer perceptron for training. Different versions of the sort-of-CLEVR dataset were created, each missing a color+shape combination. The CNN+RN architecture was trained to detect sameness in images with two items, but did not generalize well to left-out color+shape combinations. The CNN+RN model trained on the sort-of-CLEVR task does not generalize well to left-out color+shape combinations, with training accuracy reaching 90% but validation accuracy remaining at chance levels. The CNN+RN model trained on the sort-of-CLEVR task struggles to generalize to left-out color+shape combinations, with validation accuracy remaining low despite high training accuracy. The system performs well with image sizes up to 120 pixels but fails to learn with larger sizes. The CNN+RN model achieves over 95% accuracy with image sizes up to 120 pixels but fails to learn with larger sizes, indicating a limitation in representational capacity for visual-relation problems. Learning templates for arrangements of objects becomes intractable due to the combinatorial explosion in the number of templates needed. This difficulty in representing stimuli with a combinatorial structure has been acknowledged by cognitive scientists. Current computer vision scientists seem to overlook the difficulty in representing stimuli with a combinatorial structure, which has long been acknowledged by cognitive scientists. Biological visual systems excel at detecting relations, as humans can learn complicated visual rules and generalize them from just a few training examples. For instance, humans could learn a complex visual rule involving reflection around the perpendicular bisector of shapes with only about 6 examples, a task that is challenging for CNNs. The best performing network in high-throughput search struggled to excel in visual reasoning ability, unlike humans, birds, and primates who can quickly learn and generalize same-different relations with minimal training examples. Ducklings have shown the ability to perform a one-shot version of a visual learning experiment from birth. Ducklings demonstrated a preference for novel objects obeying the relationship observed in training, suggesting rapid learning of abstract concepts. In contrast, CNN+RN showed no ability to transfer the concept of same-different to novel objects even after extensive training. Feedback signals beyond feedforward processes play a crucial role in visual-relation detection. The visual cortex can perform certain recognition tasks without cortical feedback, but object localization in clutter requires attention. Neuroscience evidence suggests that processing spatial relations in a cluttered scene also requires attention. The processing of spatial relations in a cluttered scene requires attention, even when objects can be detected pre-attentively. Working memory plays a role in visual relation processing, particularly in tasks involving spatial reasoning. Attention and working memory allow for flexible representations of relations to be dynamically constructed at run-time. Humans can easily detect visual relations and construct structured descriptions about the visual world around them by dynamically constructing flexible representations of relations at run-time through attention shifts, avoiding the capacity overload associated with storing templates in neural networks. This ability highlights the importance of exploring attentional and mnemonic mechanisms in computational understanding of visual processing. The exploration of attentional and mnemonic mechanisms is crucial for computational understanding of visual reasoning, surpassing modern computers in detecting visual relations."
}