{
    "title": "SJlPZlStwS",
    "content": "Recent studies have shown that convolutional neural networks (CNNs) are vulnerable to various attacks. A new framework called EdgeGANRob has been proposed to improve the robustness of CNNs by focusing on shape/structure features and using a generative adversarial network (GAN) to reconstruct images. Additionally, a robust edge detection approach called Robust Canny has been introduced to reduce sensitivity to adversarial perturbations. EdgeGANRob is compared to a simplified version called EdgeNetRob for further insights. Edge detection approach Robust Canny improves model robustness. Comparison with EdgeNetRob shows EdgeNetRob boosts robustness but reduces clean model accuracy. EdgeGANRob enhances clean model accuracy without losing robustness benefits. Extensive experiments demonstrate EdgeGANRob's resilience in various learning tasks. Convolutional neural networks (CNNs) have achieved state-of-the-art performance in various tasks but are vulnerable to adversarial examples and data poisoning attacks. Recent studies suggest that CNNs focus on surface statistical regularities instead of high-level abstractions, hindering generalization. Recent studies explore the vulnerability of CNNs to adversarial examples, attributing it to the focus on superficial patterns rather than high-level abstractions. Improving the general robustness of DNNs remains a challenge, with suggestions to train classifiers on \"robust features\" and consider global object shapes over local patterns for human recognition. Recent studies suggest that CNNs are biased towards local patterns, while humans focus on global object shapes for recognition. This bias makes CNNs vulnerable to adversarial examples and backdoor attacks. Human object recognition relies more on object shapes than textures. The paper proposes using edge information to improve CNN robustness against adversarial attacks and backdoor attacks. This approach, called EdgeGANRob, leverages structural information in images to enhance CNN performance. The paper introduces EdgeGANRob, a new approach to enhance CNN robustness by leveraging structural information in images. A simplified version, EdgeNetRob, extracts structural information through edge detection, improving CNNs' robustness by focusing on shape rather than texture/color. However, challenges remain, such as vulnerability to attacks on edge detection algorithms. To address this, a robust edge detection algorithm, Robust Canny, is proposed. The paper introduces EdgeGANRob, a new approach to enhance CNN robustness by leveraging structural information in images. A robust edge detection algorithm, Robust Canny, is proposed to improve the robustness of EdgeGANRob against attacks. EdgeNetRob improves CNNs' robustness by focusing on shape but decreases clean accuracy due to missing texture/color information, motivating the development of EdgeGANRob to refill texture/colors based on edge images. The main contributions include proposing a unified framework, EdgeGANRob, to improve CNNs' robustness against multiple tasks simultaneously. The paper introduces EdgeGANRob, a unified framework that enhances CNN robustness by extracting edge/structure information from input images and refilling textural information with GAN. A robust edge detection approach, Robust Canny, is proposed to reduce sensitivity to adversarial perturbations. Evaluation on EdgeNetRob and EdgeGANRob shows effectiveness against adversarial attacks and distribution shifting. The paper introduces EdgeGANRob, a framework enhancing CNN robustness by extracting edge/structure information and refilling textural information with GAN. Evaluation on EdgeNetRob and EdgeGANRob shows effectiveness against adversarial attacks and distribution shifting. Defense methods against adversarial examples are discussed, highlighting the importance of evaluating against adaptive attacks. Defense methods should be evaluated against strong adaptive attacks. Distribution shifting is more common and general in real-world applications compared to adversarial examples. CNNs have a tendency to learn superficial statistical cues, but a method has been proposed to robustify CNNs by penalizing the predictive power of local representations. Benchmark datasets have been proposed for evaluating model robustness under common perturbations. The backdoor attack is a type of poisoning attack that works by injecting a pattern into training data. The backdoor attack is a poisoning attack that injects a pattern into training data to make models predict a specific target class. Various methods have been proposed to detect and protect models from such attacks, including using robust statistics and neuron pruning. Recent research has shown a connection between recognition robustness and visual features, with CNNs relying more on textures and humans relying more on shape structure. Adversarially robust models tend to capture global structure of objects, while non-robust features in natural images also exist. In this work, the authors propose using edge features as a robust feature for image classification. They introduce a new classification pipeline called EdgeGANRob, which extracts edge/structure features from images, reconstructs them using a GAN, and then feeds them into a classifier. The method aims to improve robustness by focusing on edge features. The EdgeNetRob pipeline is a simplified backbone of EdgeGANRob, focusing on edge detection and image classification. It aims to make classifiers less sensitive to local textures by training on extracted edge maps. This approach enhances robustness by forcing decisions based solely on edges, even when using pre-trained classifiers on original training data. EdgeNetRob simplifies image classification by training on edge maps, making classifiers less sensitive to textures. However, it degrades CNN performance on clean data due to missing texture/color information. To address this, EdgeGANRob fills edge maps with texture/colors, improving clean accuracy. The robustness of this system depends on the edge detector used, motivating the proposal of a robust edge detection algorithm named Robust Canny. The text discusses the proposal of a robust edge detection algorithm named Robust Canny to address the low accuracy of neural network-based edge detectors. The algorithm aims to improve the robustness of the traditional Canny edge detector by truncating noisy pixels in its intermediate stages. The proposed Robust Canny edge detection algorithm enhances the traditional Canny detector by addressing noise reduction, gradient computation, noise masking, non-maximum suppression, double thresholding, and edge tracking by hysteresis. The Robust Canny edge detection algorithm improves the traditional Canny detector by incorporating noise reduction, gradient computation, noise masking, non-maximum suppression, double thresholding, and edge tracking by hysteresis. The algorithm maps pixels to strong, weak, and non-edge levels, detects edge pixels through connectivity, and mitigates perturbation noise by setting low gradient magnitudes to zero. Parameters like the standard deviation of the gaussian filter and thresholds also impact robustness. In addition to the masking operation, the parameters of Canny (e.g. standard deviation of gaussian filter \u03c3, thresholds \u03b8 l , \u03b8 h ) affect the robustness level. Larger \u03c3 and higher thresholds \u03b8 l , \u03b8 h result in better robustness but may reduce clean accuracy. Careful parameter selection is crucial for a robust edge detector. Training a Generative Adversarial Network (GAN) in EdgeGANRob involves generating color images from edge maps using the image-to-image translation framework (pix2pix). Training an inpainting GAN involves two stages: first, a conditional GAN is trained with adversarial and feature matching losses, and in the second stage, the GAN is fine-tuned along with a classifier to generate realistic RGB images for high classification accuracy. The method improves robustness under adversarial attack, distribution shifting, and backdoor attack by leveraging edge features in generated images by inpainting GAN. The approach focuses on shape structure to enhance model generalization ability and accuracy. EdgeGANRob leverages edge features to improve model generalization ability and is less sensitive to distribution changes during testing. Extracting edges acts as a data sanitization step to prevent backdoor attacks by removing malicious patterns. The robustness of the method is evaluated, with EdgeNetRob showing unique advantages in certain settings as a robust recognition method. It is listed as an independent method to compare with EdgeGANRob. The method EdgeNetRob is evaluated for its robustness against adversarial attacks, distribution shifting, and backdoor attacks on Fashion MNIST and CelebA datasets. MNIST and CIFAR-10 datasets were not chosen due to their limitations in providing informative benchmarks for the study. The EdgeNetRob method is evaluated for robustness against adversarial attacks on Fashion MNIST and CelebA datasets using \u221e adversarial perturbation constraints. Different perturbation budgets are used for each dataset. The robustness is measured against white-box attacks using the BPDA attack. The study evaluates the robustness of EdgeNetRob against white-box attacks using the BPDA attack. Three edge detection methods are compared: RCF, Canny, and Robust Canny, with results reported for Fashion MNIST in Table 1. Using edges generated by RCF is not robust, as accuracy drops near 0 under strong adaptive attacks. The study compares EdgeNetRob and EdgeGANRob against white-box attacks, achieving higher clean accuracy than the baseline model. EdgeGANRob outperforms EdgeNetRob on the CelebA dataset, highlighting the importance of using GANs on complex datasets. The study demonstrates the effectiveness of EdgeNetRob and EdgeGANRob against strong adaptive attacks, showing robustness levels comparable to adversarial training baselines. EdgeNetRob, in particular, stands out for its time efficiency due to not using adversarial training. Generalization ability is tested under distribution shifting using perturbed Fashion MNIST and CelebA datasets with various transformations. The study compares EdgeNetRob and EdgeGANRob with state-of-the-art method PAR on MNIST and CelebA datasets with different patterns. Results show significant accuracy improvement with EdgeNetRob and EdgeGANRob on various patterns, while maintaining high accuracy on greyscale images. Edge features are found to aid CNN generalization under distribution shifting, serving as a defense against backdoor attacks. The results demonstrate that edge features improve CNN generalization under distribution shifting and can be used as a defense against backdoor attacks. Invisible watermark patterns were embedded in images for Fashion MNIST and CelebA datasets, with qualitative results shown in figures. Different attack and target pairs were selected for each dataset, with varying poisoning ratios. A comparison with the baseline method Spectral Signature was conducted, showing results in tables. Our method successfully attacks the vanilla Net with high poisoning accuracy on CelebA and Fashion MNIST datasets. Spectral Signature does not perform well with invisible watermark patterns, while EdgeNetRob and EdgeGANRob consistently have low poisoning accuracy. EdgeGANRob achieves better clean accuracy compared to EdgeNetRob. The effect of invisible watermark patterns can be removed by the edge detector. Our method combines a robust edge feature extractor with a generative adversarial network to improve model robustness against adversarial attacks and distribution shifting. It also enhances resistance to backdoor attacks. The importance of using shape information for model robustness is emphasized, showing promising directions for future research. Data pre-processing involves resizing CelebA images to 128x128 using bicubic interpolation and using 10% of images as test data. Data normalization is done for both datasets. For data pre-processing, CelebA images are resized to 128x128 and normalized to [-1, 1]. A LeNet-style CNN is used for Fashion-MNIST, while a standard ResNet with depth 20 is used for CelebA. Models are trained using stochastic gradient descent with momentum, PGD, and CW attacks. Different attack scenarios are evaluated with varying step sizes and distances. Robust Canny is used for evaluating adversarial robustness with specific hyper-parameters. The hyper-parameters used in Robust Canny for evaluating adversarial robustness are reported. Different parameters are set for Fashion MNIST and CelebA datasets. The last three steps in the algorithm involve non-differentiable transformations, which can be replaced with differentiable approximations in a white-box attack scenario. The Backward Pass Differentiable Approximation (BPDA) technique is used to create stronger attacks by finding a differentiable approximation of the Robust Canny algorithm. The transformation is divided into two stages: C1 (steps 1-3) and C2 (steps 4-6). C2 is non-differentiable, producing a masked version of the input. A differentiable approximation of R-Canny is obtained by approximating C2(C1(x)) as M(C1(x)) \u2297 C1(x). The text discusses the differentiable approximation of the Robust Canny algorithm using the Backward Pass Differentiable Approximation (BPDA) technique. It focuses on backpropagating gradients through C1(\u00b7) and not M(\u00b7), showing changes in test accuracy under radial and random mask transformations. Visualization results for CelebA and backdoor attacks on Fashion MNIST are also presented."
}