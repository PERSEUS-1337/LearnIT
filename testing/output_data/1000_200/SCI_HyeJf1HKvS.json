{
    "title": "HyeJf1HKvS",
    "content": "This work introduces a two-stage neural architecture for learning structural correspondences between graphs. It utilizes localized node embeddings from a graph neural network to rank soft correspondences initially, followed by synchronous message passing networks to iteratively re-rank the correspondences for a matching consensus in local neighborhoods. The message passing scheme computes a well-founded measure of consensus for corresponding neighborhoods, guiding the re-ranking process. The architecture scales well to large inputs and demonstrates practical effectiveness in computer vision and entity alignment tasks. Graph matching is crucial for various real-world applications like cheminformatics, bioinformatics, social network analysis, and computer vision. It involves establishing structural correspondences between nodes in graphs by considering node and edge similarities. The method presented in this work improves upon the current state-of-the-art in computer vision and entity alignment tasks. Various neural architectures have been proposed to tackle the task of graph matching, which is crucial for applications in cheminformatics, bioinformatics, social network analysis, and computer vision. The problem of graph matching has been extensively studied in theory and practice, often related to domain-agnostic distances and formulated as NP-hard problems. However, purely combinatorial approaches may not be tractable for large-scale instances and do not consider continuous node embeddings. Graph matching approaches have been proposed using neural architectures, but they often lack continuous node embeddings and struggle with scalability and generalization to unseen graphs. The problem is typically formulated as an edge-preserving quadratic assignment problem. Graph matching is formulated as an edge-preserving, quadratic assignment problem with the intuition of finding correspondences based on neighborhood consensus. This approach aims to prevent adjacent nodes in the source graph from being mapped to different regions in the target graph. The problem is addressed in supervised and semi-supervised settings, incorporating the concept of neighborhood consensus as an inductive bias into the model. The proposed deep graph matching architecture consists of two stages: local feature matching and iterative refinement using synchronous message passing networks. The feature matching step computes initial correspondence scores based on node embeddings, while the refinement strategy aims to reach neighborhood consensus. The deep graph matching architecture involves two stages: initial correspondence score computation based on local node embeddings and iterative refinement for neighborhood consensus using a differentiable validator for graph isomorphism. The method is scalable to large, real-world inputs. The deep graph matching architecture involves initial correspondence score computation based on node embeddings and obtaining soft correspondences using sinkhorn normalization. The method is trained in a supervised fashion against ground truth. The deep graph matching architecture involves training a Graph Neural Network (GNN) to obtain node representations for potential correspondences in a supervised manner. The GNN follows a neural message passing scheme to update node features based on localized information aggregation. Various operators from geometric deep learning and relational representation learning are utilized in this process. The deep graph matching architecture utilizes graph neural networks to detect violations of neighborhood consensus criteria in local neighborhoods and iteratively refine correspondences. The operators from deep learning and relational representation learning provide precise control over extracted features. The proposed algorithm utilizes a soft correspondence matrix to pass node functions between domains, enabling neighborhood consensus detection and refinement through synchronous message passing on graph neural networks. The proposed algorithm utilizes a shared graph neural network to compare results and measure neighborhood consensus between node pairs. This consensus stage iteratively improves consensus by updating correspondence scores based on an MLP. The process resolves ambiguities and false matchings using purely local operators, emphasizing the importance of the two-stage approach. The proposed algorithm uses a two-stage approach to test neighborhood consensus between node pairs. Theorems 1 and 2 demonstrate the effectiveness of measuring local neighborhood matching using a soft correspondence between graphs. A permutation equivariant and injective GNN is crucial for accurate node embeddings. A GNN satisfying criteria in Theorems 1 and 2 provides equal node embeddings. Permutation equivariance and injectivity are easily fulfilled by common GNN architectures. Injectivity can be achieved by using a powerful GNN like the Weisfeiler & Lehman heuristic. The proposed approach relates to classical graph matching techniques. The proposed approach can be related to classical graph matching techniques such as the graduated assignment algorithm. It involves iteratively computing new solutions by solving a linear assignment problem and implementing the softassign operator with sinkhorn normalization. The gradient Q is closely linked to a neighborhood consensus scheme in a non-trainable GNN instantiation. The proposed approach involves solving a linear assignment problem with sinkhorn normalization and utilizing trainable neural networks for updating correspondence scores. It is a deep parameterized generalization of the graduated assignment algorithm and supports continuous node and edge features through established GNN models. The benefits of using trainable neural networks are experimentally verified. The proposed approach involves utilizing trainable neural networks for updating correspondence scores and optimizing the algorithm for large input domains. Initial correspondences are sparsified by filtering out low score correspondences before neighborhood consensus, reducing memory footprint and time complexity. The proposed approach utilizes neural networks to update correspondence scores and optimize for large input domains by sparsifying initial correspondences. This involves filtering out low score correspondences before neighborhood consensus to reduce memory footprint and time complexity. Additionally, node indicator functions are replaced with randomly drawn node functions to improve computational efficiency. The proposed approach utilizes neural networks to update correspondence scores and optimize for large input domains by sparsifying initial correspondences. This involves filtering out low score correspondences before neighborhood consensus to reduce memory footprint and time complexity. Additionally, node indicator functions are replaced with randomly drawn node functions to improve computational efficiency. Functions are still guaranteed to be injective. Theorem 1 holds without restrictions on the function space L(G s). Theorem 2 may not hold, but a refinement strategy is expected to resolve ambiguities by re-sampling R (l) s in each iteration. Softmax normalization is proposed as a relaxation of sinkhorn normalization to avoid inconsistent integer solutions and vanishing gradients. Row-wise softmax normalization is applied to \u015c (l) to naturally resolve violations. The proposed approach utilizes neural networks to update correspondence scores and optimize for large input domains by sparsifying initial correspondences. Row-wise softmax normalization is applied to resolve violations and encourage convergence with fewer refinement iterations. Decreasing the number of refinement iterations for training does not affect convergence abilities during testing. The method is validated on various tasks including synthetic graph analysis, supervised keypoint matching in images, and cross-lingual knowledge graph alignment. Implementation is in PYTORCH using PYTORCH GEOMETRIC and KEOPS libraries for efficient processing with GPU acceleration. Optimization is done with ADAM using a fixed learning rate. In experiments, optimization is done via ADAM with a fixed learning rate. Similar architectures are used for \u03a8 \u03b81 and \u03a8 \u03b82, omitting dropout in \u03a8 \u03b82. Hits@k is reported to evaluate the model on synthetic graphs, aiming to learn matching for pairs of graphs in a supervised fashion. Each pair consists of an undirected Erd\u0151s & R\u00e9nyi graph G s and a target graph G t constructed from G s by removing edges with probability p s without disconnecting any nodes. Training and evaluation are done on 1,000 graphs for different configurations of p s. In experiments, the graph neural network operators \u03a8 \u03b81 and \u03a8 \u03b82 are implemented with three layers of the GIN operator for distinguishing raw graph structures. The MLPs have 2 layers and a hidden dimensionality of 32 with ReLU activation. Input features are one-hot encodings of node degrees. A Jumping Knowledge style concatenation is used to compute final node representations. The procedure is trained and tested with 10 and 20 refinement iterations, respectively. The study employs a Jumping Knowledge style concatenation to compute final node representations. The matching accuracy is evaluated for different choices of parameters, showing a decrease in performance with increasing structural noise. However, a two-stage architecture is proposed that can recover all correspondences regardless of the noise level. This highlights the benefits of matching consensus and scalability enhancements. The study demonstrates the benefits of matching consensus and scalability enhancements by showing that increasing the number of iterations during testing allows the procedure to converge even when training does not. Additionally, the refinement strategy performs well on sparsified correspondences, converging to the perfect solution with increasing k. This makes it a viable option for scaling the algorithm to large graphs. The study shows that increasing the number of iterations during testing helps the algorithm converge, even when training does not. The refinement strategy works well on sparsified correspondences, making it a good option for scaling the algorithm to large graphs. Experiments were conducted on the PASCALVOC and WILLOW-OBJECTCLASS datasets, with specific criteria for dataset filtering and keypoint requirements. The PASCALVOC dataset contains instances with varying scale, pose, and illumination, while the WILLOW-OBJECTCLASS dataset has consistent orientations for each category. The model is pre-trained on PASCALVOC and fine-tuned on 20 random splits with 20 images per class for training. Graphs are constructed using Delaunay triangulation of keypoints, and input features are based on a pre-trained VGG16 model. The SPLINECNN architecture is adopted as the graph neural network operator. The SPLINECNN architecture, based on a pre-trained VGG16 model, utilizes B-spline based kernel functions conditioned on edge features for graph neural network operations. Results are evaluated for isotropic and anisotropic cases, showing high accuracy across different layers. The SPLINECNN architecture utilizes edge features for graph neural network operations, with a kernel size of 5, hidden dimensionality of 256, and ReLU non-linearity function. The network consists of two convolutional layers, dropout, and a linear layer. Training involves forming pairs between training examples and evaluating with test graph pairs. The model is trained using negative log-likelihood for superior performance. The experimental setup of Wang et al. (2019b) involves training models using negative log-likelihood for superior performance. Evaluation includes isotropic and anisotropic GNNs for different values of L, with ablation results using \u03a8 \u03b81 = MLP for local node matching. Results show significant improvement over competing methods, especially on the WILLOW-OBJECTCLASS dataset where the refinement stage reduces errors by half across all categories. The second stage of the approach shows crucial improvements, especially from a weaker initial feature matching baseline. Initial matchings help further enhance performance, using task-specific isotropic or anisotropic GNNs. The model's generalization capabilities are tested on the PASCALPF dataset, where synthetic graph pairs are generated for training. The model is trained on synthetic graph pairs with added noise and outliers. It outperforms the state-of-the-art results on the PASCALPF dataset, showcasing the benefits of the consensus architecture. The model, trained on synthetic graph pairs with added noise and outliers, outperforms state-of-the-art results on the PASCALPF dataset, showcasing the benefits of the consensus architecture. Additionally, the model's performance on the DBP15K datasets, linking entities of different knowledge graphs, demonstrates the effectiveness of the method even without visual information. The model, trained on synthetic graph pairs with added noise and outliers, outperforms state-of-the-art results on the PASCALPF dataset. It also showcases the benefits of the consensus architecture and demonstrates effectiveness on the DBP15K datasets, linking entities of different knowledge graphs without visual information. The approach involves aligning word embeddings separately for each language and then aligning them into the same vector space. The graph neural network operator used is similar to Xu et al. (2019d), with ReLU and dropout for non-linearity, and a three-layer GNN for obtaining initial similarities and refining alignments. Training is done using negative log likelihood in a semi-supervised manner. In the fashion domain, the model trains on synthetic graph pairs with added noise and outliers, outperforming state-of-the-art results on the PASCALPF dataset. It aligns word embeddings for each language and then into the same vector space using a graph neural network operator. The model improves Hits@1 and Hits@10 metrics significantly, with gains of up to 9.38 percentage points, showcasing the effectiveness of the refinement strategy in aligning initial correspondences. Our refinement strategy significantly improves Hits@1 of initial correspondences by a large margin, with results for Hits@10 being shared due to operating on top 10 initial correspondences. The scalability allows for multiple refinement iterations while maintaining large hidden feature dimensionalities. Experimental results show the effectiveness of our approach in solving real-world problems, but it inherits limitations related to the expressive power of GNNs and the WL heuristic for graph isomorphism testing. One limitation is the potential failure to converge when two nodes are assigned the same color by WL. Identifying correspondences between nodes in graphs has been extensively studied in various domains. Related problems include maximum common subgraph, network alignment, graph edit distance, and graph matching. The approach may fail to converge if two nodes are assigned the same color by the Weisfeiler-Lehman (WL) heuristic due to equal neighborhood sets. Adding noise to initial correspondence distributions could resolve ambiguities, but real-world datasets typically have enough noise to prevent this scenario. Graph neural networks have become a focus of research for deep graph matching techniques, with a two-stage neural architecture proposed for learning node correspondences between graphs. The approach aims to reach a neighborhood consensus between matchings and resolve violations iteratively. Our approach in fashioning a two-stage neural architecture for deep graph matching aims to reach a neighborhood consensus between matchings and resolve violations iteratively. We proposed enhancements to scale our algorithm to large input domains and evaluated it on real-world datasets, consistently outperforming the state-of-the-art. The final optimized algorithm is detailed in Algorithm 1. The text discusses the power of distinguishing graph structures using vectorial representations and isomorphisms. It highlights the importance of injective node colorings and the identification of submatrices describing isomorphisms between subgraphs. The text also mentions the contradiction of injectivity requirements in certain scenarios. The algorithm described in the text is a generalization of the graduated assignment algorithm, incorporating trainable parameters to improve results. By using trainable neural networks, the algorithm consistently outperforms fixed-function message passing schemes in experiments. This approach allows for learning meaningful similarities between node and edge features, which is challenging with fixed-function pipelines. Our approach improves upon fixed-function message passing schemes by learning to utilize node and edge features for refining procedures. It offers flexibility in selecting task-specific GNN operators and has potential for higher-order GNNs. Experimental validation includes synthetic experiments with Erd\u0151s & R\u00e9nyi graphs for robustness testing. Our approach improves upon fixed-function message passing schemes by learning to utilize node and edge features for refining procedures. It offers flexibility in selecting task-specific GNN operators and has potential for higher-order GNNs. Experimental validation includes synthetic experiments with Erd\u0151s & R\u00e9nyi graphs for robustness testing. The consensus stage in our neural architecture is robust to node additions or removals, while the first stage struggles with finding the right matching due to unmatched nodes not influencing the neighborhood consensus error. The architecture can detect and decrease false positive influence of nodes in the refinement stage. Identifying correspondences between nodes in graphs is a common problem studied in graph theory. The maximum common subgraph isomorphism problem is NP-hard, even in trees, unless the common subgraph is required to be connected. Most variants of the problem are difficult to approximate with guarantees. Different techniques have been developed for specific problem variants in cheminformatics. In cheminformatics, exact polynomial-time algorithms are available for specific problem variants. Bioinformatics and computer vision use different techniques for network alignment or graph matching. Graph matching involves minimizing a function for two graphs with adjacency matrices. Recent survey by Yan et al. (2016) provides important related work in graph matching. In graph matching, research has focused on minimizing a specific equation using various algorithms and techniques. The applicability of relaxation and projection methods is still not well understood, with limited theoretical results available. The WL heuristic and Frank-Wolfe algorithm have been used to distinguish between graphs and obtain partitions. The Frank-Wolfe algorithm can be modified to obtain the WL partition for graph matching. Various relaxations and approaches, such as spectral relaxations and random walks, have been studied. Graph matching is closely related to the quadratic assignment problem (QAP), with Equation (1) interpreted as Koopmans-Beckmann's QAP. Graph matching is a well-studied problem in operations research. Recent literature considers weighted versions, such as Lawler's QAP, which involves a large affinity matrix. Different approaches, like factorizing the matrix and incorporating global constraints, have been proposed. Kernelized graph matching, using kernels for similarities, is also explored. Lagrangean decompositions and dual ascent methods have been studied for solving the graph matching problem. Swoboda et al. (2017) studied Lagrangean decompositions for MAP inference in conditional random fields, using dual ascent algorithms for graph matching. Message passing schedules and update mechanisms leading to state-of-the-art performance were identified. Functional representation for graph matching aims to avoid constructing the affinity matrix. Graph edit distance measures the cost to transform one graph into another by adding, deleting, and substituting vertices and edges, with computation being NP-hard. The graph edit distance problem, proposed over 30 years ago, is NP-hard and related to the maximum common subgraph and quadratic assignment problems. Various algorithms have been developed, with heuristics often used for larger graphs. Greedy strategies can reduce the running time, and linear time is achievable for specific cost functions. Network alignment is defined similarly but involves additional considerations. Network alignment is a problem defined similarly to the graph edit distance problem, involving a similarity function between pairs of nodes. Algorithms typically compute a node-to-node similarity matrix and then solve the assignment problem to find an alignment. ISORANK, proposed by Singh et al. (2008), uses the adjacency matrix of the product graph and PAGERANK to compute the similarity matrix. Kollias et al. (2012) proposed an efficient approximation of ISORANK using decomposition techniques. In network alignment, algorithms aim to find an optimal correspondence between graphs based on node and edge similarities. Various techniques have been proposed, such as efficient approximations of ISORANK, extensions supporting vertex and edge similarities, and message passing algorithms for sparse network alignment. These methods focus on solving the assignment problem to align graphs according to a defined objective function. In network alignment, algorithms aim to find an optimal correspondence between graphs based on node and edge similarities. Recently, methods have been proposed to learn similarity functions for specific tasks, such as graph edit distance. Deep graph matching procedures have been explored, including refining local feature matchings and enforcing neighborhood consistency. The functional maps framework addresses similar problems for manifolds. Deep graph matching has been heavily investigated recently, with various approaches such as supervised deep graph matching networks and compositional message passing algorithms. Different methods have been proposed, including using node-wise features, dense node-to-node cross-graph affinities, and sinkhorn normalization for linear assignment. Our matching procedure is fully-learnable, unlike other models that use unlearnable spectral graph matching solvers. Our approach in deep graph matching involves a supervised method for sets of graphs, utilizing optimal transport between nodes. Unlike previous approaches, we address inconsistent neighborhood assignments and noise in graphs through enhanced optimal transport objectives. Our approach involves supervised deep graph matching using Gromov-Wasserstein barycenter. It generalizes to unseen graph instances and addresses network alignment through optimal transport between nodes. Various methods like CYCLEGANs, NODE2VEC embeddings, and graph neural networks have been used for network alignment in recent studies. In a follow-up work, Bai et al. (2018) proposed using shared graph neural networks to approximate the graph edit distance between two graphs. They fine-tuned the network output using a histogram of correspondence scores. Wang et al. (2019b) and Xu et al. (2019d) enhanced GNN operators by aggregating information from local neighbors and similar embeddings in the other graph through cross-graph matching procedures. In a related study, Wang & Solomon (2019) address the challenge of identifying an unknown rigid motion between point clouds by framing it as a point cloud matching issue followed by a differentiable SVD module. They utilize a Transformer module to pass intra-graph node embeddings before conducting feature matching based on inner product similarity scores. These methods, while not focused on achieving consistent matching, can enhance the initial feature matching process, complementing advancements in the field. Neighborhood consensus methods improve local feature matching results efficiently. A deep neural network using 4D convolution was proposed for this purpose, but it cannot be directly applied to the graph domain. Our algorithm infers errors on the product graph but performs computations on the original graphs. The algorithm infers errors on the product graph but computes on the original graphs with O(n^2) nodes and O(n^4) edges, utilizing the functional maps framework for defining continuous maps between function spaces on manifolds."
}