{
    "title": "rkhlb8lCZ",
    "content": "Convolutional Neural Networks advance 2D and 3D image classification. Wavelet Pooling reduces feature dimensions and overfitting, outperforming traditional pooling methods in benchmark datasets. Our proposed method outperforms or performs comparably with other pooling methods in benchmark classification datasets. CNNs are the standard in image and object classification due to their ability to consistently classify at a higher accuracy rate than vector-based deep learning techniques. Researchers constantly evaluate and upgrade CNN components like the convolutional and pooling layers to improve accuracy and efficiency beyond previous benchmarks. Pooling, rooted in predecessors like Neocognitron and Cresceptron, undergoes modifications to enhance CNN performance. Pooling in CNNs, rooted in predecessors like Neocognitron and Cresceptron, involves subsampling convolutional layer results to reduce spatial dimensions, parameters, increase efficiency, and regulate overfitting. Popular methods include max pooling and average pooling, but newer approaches like mixed pooling and stochastic pooling use probabilistic methods to address weaknesses in deterministic pooling operations. The proposed wavelet pooling algorithm utilizes second-level wavelet decomposition to subsample features, aiming to minimize artifacts and improve network regularization. It is compared to other pooling methods like max, mean, mixed, and stochastic pooling to demonstrate its effectiveness. The proposed wavelet pooling algorithm aims to minimize artifacts and improve network regularization by utilizing second-level wavelet decomposition for subsampling features. It is compared to other pooling methods like max, mean, mixed, and stochastic pooling on benchmark image classification datasets. Pooling, another term for subsampling, condenses the dimensions of the output of the convolutional layer by summarizing regions into one neuron value. The convolutional layer output is condensed through pooling, where regions are summarized into one neuron value. Max pooling selects the maximum value of a region, while average pooling calculates the average value. Both methods are effective for dimensionality reduction in neural networks. Researchers have developed probabilistic pooling methods like mixed pooling, which combines max and average pooling by randomly selecting one method over the other during training to address the shortcomings of traditional pooling methods. Mixed pooling combines max and average pooling by randomly selecting one method during training. It can be applied in three different ways: for all features within a layer, mixed between features within a layer, or mixed between regions for different features within a layer. Stochastic pooling, another probabilistic method, improves upon max pooling by sampling from neighborhood regions based on activation probabilities. The proposed stochastic pooling method samples activations within a region based on probabilities, avoiding the limitations of max and average pooling. It selects activations with the highest probabilities but can choose any activation. By using wavelets, it reduces dimensions effectively. The proposed pooling method utilizes wavelets to reduce feature map dimensions and minimize artifacts in image interpolation. It discards first-order subbands to capture data compression organically, reducing jagged edges and other artifacts. The wavelet pooling scheme performs a 2nd order decomposition in the wavelet domain using the fast wavelet transform (FWT), which is an efficient implementation of the two-dimensional discrete wavelet transform (DWT). The 2nd order wavelet transform involves applying the FWT twice on images, first on rows and then on columns, to obtain detail and approximation subbands at each decomposition level. The image features are reconstructed using only the 2nd order wavelet subbands, pooling them by a factor of 2 with the IFWT based on the IDWT. The proposed wavelet pooling algorithm pools image features by a factor of 2 using the IFWT based on the IDWT. Backpropagation involves 1st order wavelet decomposition, upsampling detail coefficient subbands, and reconstructing the image feature for further backpropagation. CNN experiments use MatConvNet and stochastic gradient descent for training. The wavelet pooling algorithm utilizes Haar wavelet basis for even, square subbands. Experiments are conducted on a 64-bit operating system with specific hardware configurations. Different CNN structures are used for various datasets, with experiments involving Dropout and Batch Normalization for regularization. Pooling methods are compared using a 2x2 window for consistency. Our proposed method outperforms all other pooling methods on various datasets, including MNIST. Max pooling shows signs of overfitting, while mixed and stochastic pooling have a fluctuating performance. Average and wavelet pooling demonstrate a smoother learning curve and error reduction. In experiments with pooling methods on CIFAR-10 dataset, our proposed method ranks second in accuracy, showing resistance to overfitting compared to max pooling. Dropout and batch normalization are used in the second set of experiments to observe their effects over more epochs. The method's learning rate adjustment prevents overfitting and results in a slower descent in error reduction. The experiments involved testing different pooling methods on the SHVN dataset. Two sets of experiments were conducted, one without dropout layers and the other with dropout layers. The network structure for the experiments is shown in FIG0. The training and test data were sourced from the SHVN dataset, with different image sets used for the experiments. In the experiments, different pooling methods were tested on the SHVN dataset with and without dropout layers. The proposed method showed the second lowest accuracy, with max and wavelet pooling slightly overfitting the data. Mixed, stochastic, and average pooling maintained a slow progression of learning. The KDEF dataset was used for another set of experiments with dropout included in the pooling methods. The KDEF dataset contains 4,900 images of 35 people displaying seven basic emotions using facial expressions at five poses. Errors in the dataset are fixed by mirroring missing or corrupted images and manually cropping them to 762 x 562 dimensions. The data is shuffled, with 3,900 images used for training and 1,000 for testing. Images are resized to 128x128 due to memory and time constraints. The data set consists of 3,900 training images and 1,000 test images, resized to 128x128. Dropout layers help regulate the network, with wavelet pooling showing resistance to overfitting. Stochastic pooling and wavelet pooling exhibit consistent learning progression. The proposed method ranks second in accuracy. Wavelet pooling is presented as a proof-of-concept due to computational inefficiency. Our construction and implementation of wavelet pooling is not efficient, presented as a proof-of-concept to show potential and validity for future improvements in computational efficiency. The code implementation is not optimized, with accuracy results and novelty serving as a starting point for further enhancements by researchers. Efficiency is calculated based on mathematical operations utilized by each pooling method. The text discusses different pooling methods and their computational efficiency. Average pooling requires the least number of computations, followed by mixed pooling and max pooling. Stochastic pooling is the least efficient method, using about 3 times more mathematical operations than average pooling. Wavelet pooling is presented as a proof-of-concept for future improvements in computational efficiency. Wavelet pooling is the least computationally efficient method, using 54 to 213x more mathematical operations than average pooling. Despite this, with improvements in coding practices, GPUs, and an enhanced FTW algorithm, wavelet pooling can be a viable option. Various enhancements to the FTW algorithm, such as multidimensional wavelets, lifting, and parallelization, aim to improve efficiency in speed and memory. Wavelet pooling has the potential to outperform traditional methods in CNNs, showing superior performance in the MNIST dataset and competitive results in the CIFAR-10 and KDEF datasets. The addition of dropout and batch normalization improves network regularization. Results confirm that no single pooling method is superior, with performance varying depending on the dataset and network structure. Results confirm that no single pooling method is superior, with some performing better depending on the dataset and network structure. Networks often alternate between different pooling methods to maximize effectiveness. Future work could involve varying wavelet basis and adjusting upsampling/downsampling factors for better image feature reduction. Retaining discarded subbands for backpropagation may improve accuracy. Improving the computational efficiency of the FTW method is also crucial. Analyzing the structural similarity of wavelet pooling versus other methods could further validate its effectiveness."
}