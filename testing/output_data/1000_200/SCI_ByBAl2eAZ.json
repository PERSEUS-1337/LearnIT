{
    "title": "ByBAl2eAZ",
    "content": "Deep reinforcement learning methods can use noise injection in the action space for exploratory behavior. Alternatively, adding noise directly to the agent's parameters can lead to more consistent exploration. Combining parameter noise with traditional RL methods benefits both off- and on-policy methods. This approach helps prevent premature convergence to local optima and enhances exploration in deep RL. Reinforcement learning aims to prevent premature convergence to local optima by enabling efficient exploration, which is challenging in high-dimensional MDPs. Various methods have been proposed, including adding temporally-correlated noise or parameter noise to enhance exploration and obtain a policy with a larger variety of behaviors. This paper explores combining parameter space noise with deep RL algorithms like DQN, DDPG, and TRPO to improve exploratory behavior. Experiments demonstrate its effectiveness in both high-dimensional discrete environments and continuous control tasks, using on-and off-policy methods. Parameter noise is shown to outperform traditional action space noise in tasks with sparse rewards in both high-dimensional discrete environments and continuous control tasks. The standard RL framework involves an agent interacting with a fully observable environment modeled as a Markov decision process. The goal is to maximize the expected discounted return by optimizing a policy parametrized by \u03b8. The agent's goal is to maximize the expected return by optimizing a policy parametrized by \u03b8, which can be deterministic or stochastic. Off-policy RL methods like Deep Q-Networks (DQN) and Deep Deterministic Policy Gradients (DDPG) are considered in this paper. DQN uses a deep neural network to estimate the optimal Q-value function, while DDPG focuses on learning based on data captured by arbitrary policies. The policy in Deep Deterministic Policy Gradients (DDPG) is implicitly defined by the Q-value function, which encourages exploration through a stochastic policy. The actor-critic algorithm in DDPG updates the Q-value function using off-policy data and aims to maximize the critic's estimated Q-values. Exploration in DDPG is achieved through a stochastic policy with sampling noise in the action space. Trust Region Policy Optimization (TRPO) is an extension of traditional policy gradient methods that improves upon REINFORCE by ensuring a small change in the policy distribution. It solves a constrained optimization problem using discounted state-visitation frequencies induced by the policy. TRPO solves a constrained optimization problem using discounted state-visitation frequencies induced by the policy, represented as parameterized functions. Policies are sampled by adding Gaussian noise to the parameter vector, ensuring structured exploration. State-dependent exploration is discussed in the context of action space noise versus parameter space noise. While Gaussian action noise leads to different actions for the same state, perturbing policy parameters results in the same action being taken consistently for a given state. Perturbing deep neural networks with spherical Gaussian noise can ensure consistency in actions and introduce a dependence between state and exploratory action. This can be achieved through a reparameterization of the network with layer normalization between perturbed layers, allowing for a consistent perturbation scale across all layers. Adaptive noise scaling in parameter space allows for a consistent perturbation scale across all layers, adapting over time to the variance in action space induced by the noise. Parameter space noise can be applied in both off-policy and on-policy methods for exploration and training, with a scaling factor \u03b1 and threshold value \u03b4 determining the perturbation level. In off-policy methods, data collected off-policy can be used for perturbing the policy, while in on-policy methods, parameter noise can be incorporated for exploration. Parameter space noise can be incorporated in on-policy methods for exploration and training. The expected return can be expanded using likelihood ratios and the re-parametrization trick for N samples. The value of \u03a3 is fixed to \u03c3^2I and scaled adaptively. This section addresses the benefits of incorporating parameter space noise in state-of-the-art RL algorithms and its effectiveness in exploring sparse reward environments. RL algorithms can benefit from incorporating parameter space noise for exploring sparse reward environments effectively. Comparison with evolution strategies shows the added value of parameter space noise over action space noise in both discrete and continuous environments. Reference implementations of DQN and DDPG with adaptive parameter space noise are available online for evaluation. Incorporating parameter space noise in RL algorithms can enhance exploration in sparse reward environments. A comparison between DQN agents with action noise and parameter noise shows the benefits of the latter. The scale of parameter noise is adjusted based on KL divergence, ensuring a fair comparison with action noise. Reparameterizing the network to represent the greedy policy improves the effectiveness of parameter perturbation. The policy \u03c0 is represented by a fully connected layer after the network's convolutional part, predicting a probability distribution over actions. Perturbing the policy instead of Q-values leads to more meaningful changes, training it to output the greedy action according to the current Q-network. This approach is compared against regular DQN and two-headed DQN to ensure consistent behavior. The study compares parameter space noise approach with regular DQN and two-headed DQN, using -greedy exploration. Actions are randomly sampled for the first 50 thousand timesteps to fill the replay buffer. Combining parameter space noise with a bit of action space noise improves performance. Experimental details are provided in Section A.1. 21 games of varying complexity are chosen for training, with learning curves shown in FIG2. Each agent is trained for 40 M frames, and performance is evaluated with three random seeds, plotting the median return. The study compares parameter space noise with regular DQN and two-headed DQN using -greedy exploration. Results show parameter space noise often outperforms action space noise, especially on games requiring consistency. Learning progress starts sooner with parameter space noise. Comparison with double-headed DQN confirms improved exploration is not due to architecture change. Full results available in Appendix D. Parameter space noise is effective for improved exploration, especially in challenging games like Montezuma's Revenge. More sophisticated exploration methods like BID4 may be necessary for successful learning in these games. Proposed improvements to DQN such as double DQN, prioritized experience replay, and dueling networks are likely to further enhance results. Experimental validation of combining parameter space noise with exploration methods is suggested for future work. The text discusses comparing parameter noise with action noise in continuous control environments using DDPG as the RL algorithm. Different noise configurations are evaluated for performance improvement. In continuous control tasks, parameter space noise outperforms other exploration schemes, achieving higher returns and preventing convergence to local optima. This is demonstrated in the HalfCheetah environment where parameter space noise shows superior performance compared to uncorrelated Gaussian action space noise. In continuous control tasks, parameter space noise outperforms other exploration schemes by preventing convergence to local optima. It performs significantly better than correlated action space noise, indicating a clear difference between the two. Additionally, DDPG is capable of learning good policies even without noise present, suggesting that some environments do not require extensive exploration due to well-shaped reward functions. In continuous control tasks, parameter space noise is effective in preventing convergence to local optima and outperforms correlated action space noise. Adding parameter noise in the Walker2D environment reduces performance variance between seeds, aiding in escaping local optima. Parameter noise enables existing RL algorithms to learn in environments with sparse rewards, where uncorrelated action noise fails. The agent navigates a chain of N states starting at state s2, with rewards of 0.001 in s1 and 1 in sN. The difficulty of finding rewards increases with N. Different DQN methods are compared with varying chain lengths. Performance is evaluated after each episode, with the problem considered solved if optimal return is achieved in one hundred subsequent rollouts. Median episodes before problem resolution are plotted. The experiment evaluates different DQN methods in navigating a chain of states with increasing rewards. The problem is considered solved if optimal return is achieved in one hundred subsequent rollouts. Parameter space noise outperforms action space noise and bootstrapped DQN in this simple environment. In continuous control environments with sparse rewards, parameter space noise may not guarantee optimal exploration compared to action space noise. The environments only yield a reward after significant progress towards a goal, such as raising a paddle above a threshold or reaching an upright position. In sparse reward environments, tasks like SparseDoublePendulum and SparseHalfCheetah only reward specific achievements. DDPG and TRPO are used to solve these tasks with a time horizon of T = 500 steps. SparseDoublePendulum appears easier to solve with DDPG. In sparse reward environments, DDPG is used to solve tasks like SparseDoublePendulum and SparseHalfCheetah. Parameter space noise improves exploration behavior, but success is not guaranteed for all cases. Parameter space noise can improve exploration behavior in reinforcement learning algorithms like Evolution Strategies (ES). ES introduces noise in the parameter space to enhance exploration, leading to better performance in certain cases. By combining parameter space noise with traditional RL algorithms, temporal information can be included while still benefiting from improved exploratory behavior. Performance comparison between ES and traditional RL with parameter space noise is conducted on 21 ALE games. In comparing ES and traditional RL with parameter space noise, performance on 21 ALE games is evaluated. DQN with parameter space noise outperforms ES on 15 out of 21 Atari games, demonstrating the combination of exploration properties of ES with the sample efficiency of traditional RL. The problem of exploration in reinforcement learning has been extensively studied, with algorithms proposed to guarantee near-optimal solutions in a polynomial number of steps. However, in real-world scenarios with continuous and high-dimensional state and action spaces, these algorithms become impractical. Various techniques have been proposed in deep reinforcement learning to improve exploration, but they are often computationally expensive. Perturbing policy parameters has been shown to outperform other methods in policy gradient algorithms. The authors proposed perturbing policy parameters for policy gradient methods, showing it outperforms random exploration. Their method was evaluated with REINFORCE and Natural Actor-Critic BID24 algorithms. In contrast to previous work, their method is applied to both on and off-policy settings, uses high-dimensional policies, and environments with large state spaces. Their work is related to evolution strategies and policy optimization techniques. In policy optimization, ES has been shown to work for high-dimensional environments like Atari and OpenAI Gym. However, ES lacks temporal structure consideration and suffers from sample inefficiency. Our approach perturbs network parameters directly for exploration, similar to Bootstrapped DQN but simpler and sometimes superior. Another similar approach by BID8 also utilizes parameter perturbations for efficient exploration. Our proposal of parameter space noise is a straightforward yet effective alternative. In this work, parameter space noise is proposed as a simple and effective replacement for traditional action space noise in deep reinforcement learning algorithms. It has shown improved performance in various algorithms such as DQN, DDPG, and TRPO, especially in environments with sparse rewards. Parameter space noise is suggested as a viable alternative to action space noise, which is commonly used in reinforcement learning applications. The experimental setup for ALE BID3 includes a network architecture with 3 convolutional layers followed by a hidden layer and a linear output layer. ReLUs are used in each layer, with layer normalization in the fully connected part. A second head for parameter space noise is included, and target networks are updated every 10 K timesteps. The Q-value network is trained using the Adam optimizer with a learning rate of 10 \u22124 and a batch size of 32. The replay buffer can hold 1 M state transitions. The Q-value network is trained using the Adam optimizer with a learning rate of 10 \u22124 and a batch size of 32. The replay buffer can hold 1 M state transitions. Parameter space noise is scaled adaptively to enforce maximum KL divergence between perturbed and non-perturbed \u03c0. The policy is perturbed at the beginning of each episode, with the standard deviation adapted every 50 timesteps. To avoid getting stuck, -greedy action selection is used with = 0.01. The network architecture for DDPG includes layer normalization. Initial data for the replay buffer is collected with 50 K random actions. Observations are down-sampled to 84 \u00d7 84 pixels and converted to grayscale. The actor and critic both use 2 hidden layers with 64 ReLU units each. Layer normalization is applied to the critic. The actor and critic in DDPG use 2 hidden layers with 64 ReLU units each. Layer normalization is applied to all layers. The target networks are soft-updated with \u03c4 = 0.001. The critic is trained with a learning rate of 10 \u22123 while the actor uses a learning rate of 10 \u22124. Both actor and critic are updated using the Adam optimizer with batch sizes of 128. The critic is regularized using an L2 penalty with 10 \u22122. The replay buffer holds 100 K state transitions and \u03b3 = 0.99 is used. Parameter space noise is scaled adaptively to match the action space noise. Action space noise with \u03c3 = 0.2 is used for dense environments, and \u03c3 = 0.6 for sparse environments. TRPO uses action space noise with \u03c3 = 0.2 for dense environments and \u03c3 = 0.6 for sparse environments. It has a step size of \u03b4 KL = 0.01, a policy network with 2 hidden layers of 32 tanh units for nonlocomotion tasks, and 2 hidden layers of 64 tanh units for locomotion tasks. The Hessian calculation is subsampled with a factor of 0.1, \u03b3 = 0.99, and a batch size of 5 K timesteps per epoch. The baseline is a learned linear transformation of observations. Various environments from OpenAI Gym and rllab are used for tasks with specific reward conditions. The curr_chunk discusses different environments like DISPLAYFORM2, DISPLAYFORM3, DISPLAYFORM4, and DISPLAYFORM5 with specific reward conditions. DQN is used with a simple network for training agents in these environments. Each agent is trained for up to 2K episodes with varying chain lengths N and different seeds. The curr_chunk discusses training agents with different algorithms in a scalable environment. Agents are trained for up to 2K episodes with varying chain lengths N and different seeds. The performance of the policy is evaluated after each episode, and the problem is considered solved if optimal return is achieved in one hundred subsequent trajectories. Different algorithms like adaptive parameter space noise DQN, bootstrapped DQN, and \u03b5-greedy DQN are compared in this setting. The curr_chunk discusses adaptive parameter space noise for perturbing Q directly, with \u03b4 \u2248 0.05. Training details include \u03b3 = 0.999, 100 K state transitions in the replay buffer, learning starting after 5 episodes, target network updates every 100 timesteps, Adam optimizer with lr of 10^-3, batch size of 32. Stochastic policy \u03c0 \u03b8 (a|s) with \u03b8 \u223c N (\u03c6, \u03a3) is used, with likelihood ratios and reparametrization trick for expected return expansion. Variance-reducing baseline b i t is subtracted, \u03a3 := \u03c3^2 I is set, and a proposed adaption method is used for re-scaling. The challenge lies in selecting a suitable scale \u03c3 for parameter space noise. The proposed adaption method involves adjusting the scale of parameter space noise over time using a time-varying scale \u03c3 k. This adaptation resolves limitations related to the scale dependency on network architecture and parameter sensitivity. The scale \u03c3 k is updated every K timesteps based on a simple heuristic, related to action space variance, and updated accordingly. The scale \u03c3 k is updated every K timesteps using a simple heuristic based on action space variance, with \u03b1 = 1.01 always used. Different distance measures are outlined for methods that use behavioral policies (DDPG and TRPO) and those that do not (DQN). The perturbed policy changes bias in the final layer, affecting Q-values. A probabilistic formulation is used to measure distance in action space, avoiding normalization issues. The perturbed policy changes bias in the final layer, affecting Q-values. A probabilistic formulation using KL divergence effectively normalizes Q-values and compares -greedy action space noise without the need for an additional hyperparameter \u03b4. The KL divergence between greedy and -greedy policies can be used to relate action space noise and parameter space noise by adaptively scaling \u03c3 to match the distance measure. The distance measure between non-perturbed and perturbed policies is used to relate action space noise and parameter space noise by adaptively scaling \u03c3. This results in effective action space noise with the same standard deviation as regular Gaussian action space noise. For TRPO, noise vectors are scaled by computing a natural step H \u22121 \u03c3. The noise vectors for TRPO are scaled by computing a natural step H \u22121 \u03c3 to ensure the perturbed policy remains close to the non-perturbed version. Learning curves for 21 Atari games are provided in FIG8, and the final performance of ES is compared to DQN in TAB2. Performance is evaluated by running 10 episodes with exploration disabled, with results from Salimans et al. (2017) for ES and DQN. The performance of TRPO with noise scaled according to the parameter curvature is shown in FIG10. Adaptive parameter space noise achieves stable performance on InvertedDoublePendulum. Overall, performance is comparable to other exploration approaches, indicating that these environments with DDPG are not ideal for testing exploration. Adding parameter space noise aids in learning more consistently on challenging sparse environments, as shown in FIG10. The TRPO baseline uses action noise with a policy network outputting the mean of a Gaussian distribution."
}