{
    "title": "SylCrnCcFX",
    "content": "Deep networks aim to understand complex mappings through locally linear behavior. The challenge lies in the instability of derivatives, especially in networks with piecewise linear activation functions. A new learning problem is proposed to promote stable derivatives over larger regions. The algorithm involves identifying stable linear approximation regions and expanding them. This approach is demonstrated on image and sequence datasets using residual and recurrent networks. The paper introduces a novel relaxation technique to scale algorithms for realistic models, focusing on derivatives with respect to input coordinates rather than parameters in deep learning models. The challenge lies in the instability of these derivatives, especially in networks with piecewise linear activation functions. The approach involves identifying stable linear approximation regions and expanding them, demonstrated on image and sequence datasets using residual and recurrent networks. The challenge in deep learning models is the instability of derivatives, leading to unstable functions and derivatives. Gradient stability differs from adversarial examples, with stable gradients remaining approximately invariant within a local region. Adversarial examples are small perturbations that change predicted outputs. A large local gradient, stable or not, can contribute to finding adversarial examples. In this paper, the focus is on deep networks with piecewise linear activations to address the challenge of unstable derivatives. The special structure of these networks allows for inferring lower bounds on the maximum radius of p-norm balls around a point where derivatives are stable. The study specifically looks at the case of p = 2 to determine these lower bounds. The study focuses on deep networks with piecewise linear activations to determine lower bounds on the maximum radius of p-norm balls around a point where derivatives are stable. Specifically, the case of p = 2 is investigated, leading to a regularization problem with a rigid and non-smooth objective. To address the computational challenge of evaluating neuron gradients, a novel perturbation algorithm is proposed for piecewise linear networks. The study introduces inference and learning algorithms for neural networks with piecewise linear activations, focusing on stability and expansion of stable derivatives. Novel perturbation algorithms are developed to handle high-dimensional data, with empirical evaluation on various network types. The study introduces inference and learning algorithms for neural networks with piecewise linear activations, focusing on stability and expansion of stable derivatives. The proposed approach is based on a mixed integer linear representation of piecewise linear networks, encoding the active linear piece of the activation function for each network type. The approach is based on a mixed integer linear representation of piecewise linear networks, encoding the active linear piece of the activation function for each neuron. The feasible set corresponding to an activation pattern in the input space is a natural region where derivatives are stable. The feasible set induced by an activation pattern is called a linear region, and a complete linear region is a maximal connected subset of the input space with the same derivatives of the network. Activation patterns have been studied in various contexts, such as visualizing neurons and reachability of specific output values. In contrast to quantifying the number of linear regions as a measure of complexity, the focus is on local linear regions and expanding them via learning. The notion of stability considered differs from adversarial examples, with different methods used for finding exact adversarial examples. The methods for finding exact adversarial examples differ from stability considerations. Adversarial example computation is NP-complete and not scalable, while layer-wise relaxations provide bounds but are still intractable on ImageNet scale images. In contrast, the proposed inference algorithm certifies a margin around a point by forwarding samples in parallel, scaling to high-dimensional settings like ResNet on large images. The proposed learning algorithm aims to develop an unbiased estimation using a sub-sampling procedure that scales to ResNet on large images. It focuses on maximizing the margin of linear regions around each data point, similar to transductive/semi-supervised SVM. This approach differs from traditional SVM training by emphasizing the margin of linear regions rather than data points. In contrast to traditional SVM training, the proposed algorithm focuses on maximizing the margin of linear regions around each data point. It addresses the instability of gradient-based explanations and aims to establish robust derivatives for complex models. The proposed algorithm addresses the instability of gradient-based explanations by focusing on establishing robust derivatives for complex models, specifically neural networks with ReLU activations. The approach is developed under the notation of FC networks and introduces inference and learning algorithms for neural networks with multiple hidden layers. The text discusses the computation process in neural networks with FC architecture and ReLU activations. It explains how the output is a linear transformation of the last hidden layer and can be further processed for classification. The focus is on the piecewise linear property of neural networks and the use of a generic loss function. The activation pattern used in the paper is defined as a p-ball around x with radius. The activation pattern BID20 used in this paper is defined as a set of indicators for neurons that specify functional constraints. Each linear region of the neural network is characterized as a convex polyhedron with linear constraints in the input space. The activation pattern BID20 defines functional constraints for neurons in the neural network, characterizing each linear region as a convex polyhedron with linear constraints in the input space. The feasible set S(x) is a convex polyhedron with linear constraints, and the p margin of x, denoted as \u02c6 x,p, is a lower bound subject to a derivative specification. Directional feasibility is checked by exploiting the convexity of S(x) for a directional perturbation. Directional feasibility is determined by checking if a point x + \u00af\u2206x is within the feasible set S(x) using activation patterns. This method can be applied to 1-balls, with a linear number of extreme points. However, for \u221e-balls in high dimensions, the number of extreme points becomes exponential, making it impractical. Binary searches can be used to find certificates for directional perturbations and 1-balls. Binary searches can be used to find certificates for directional perturbations and 1-balls. The feasibility on 1-balls is tractable due to convexity of S(x) and its certification is efficient by a binary search. Proposition 6 states that given a point x, the minimum 2 distance between x and the union of hyperplanes can be computed efficiently. Visualization of the certificates on 2 margins can be seen in FIG3. The sizes of linear regions are related to their overall number. The number of linear regions in a function f \u03b8 is difficult to count due to the combinatorial nature of activation patterns. To address this, we propose certifying the number of complete linear regions (#CLR) among data points D x, which is efficient to compute under certain conditions. This certification is based on the cardinality of activation patterns and ensures an upperbound on the number of complete linear regions. In this section, methods are discussed to maximize the 2 margin\u02c6 x,2 in a regularization problem to increase the margin. A hinge-based relaxation is applied to the distance function to address the rigid loss surface hindering optimization. If certain conditions are not met, an upper bound of the objective function is still valid. An upper bound of Eq. FORMULA12 can be obtained due to constraints by deriving a relaxation that solves a smoother problem with a soft regularization approach. This relaxation involves maximizing TSVM losses among neurons and computing the margin in a linear model scenario. The proposed methods aim to maximize the margin in a linear model scenario by using distance regularization and relaxed regularization. These techniques result in piecewise linear regions and prediction heatmaps, with the relaxed regularization providing a smoother prediction boundary and a central region with 0 gradients for smooth gradient direction changes. The relaxed regularization in the context of maximizing margin in a linear model scenario involves a special central region with 0 gradients to allow smooth gradient direction changes. A generalization is made to the relaxed loss with a set of neurons incurring high losses to a given point, denoted as \u00ce(x, \u03b3). The final objective is to learn RObust Local Linearity (ROLL) with a special case when \u03b3 = 100, simplifying the structure and stabilizing the training process. The relaxed regularization for maximizing margin in a linear model involves a central region with 0 gradients for smooth direction changes. When \u03b3 = 100, the nonlinear sorting step disappears, stabilizing training and allowing for an approximate learning algorithm. A parallel algorithm is developed without back-propagation by exploiting the linear structure of hidden neurons. The proposed approach involves constructing a linear network g \u03b8 identical to f \u03b8 in S(x) with fixed linear activation functions. Derivatives of neurons can be computed by forwarding two samples. The complexity analysis assumes no overhead for parallel computation. The perturbation algorithm for computing gradients takes 2M operations, while back-propagation takes more. Despite parallelizable computation, computing \u2207 x z i j remains challenging. The proposed approach involves constructing a linear network g \u03b8 identical to f \u03b8 in S(x) with fixed linear activation functions. Derivatives of neurons can be computed by forwarding two samples. The perturbation algorithm for computing gradients takes 2M operations, while back-propagation takes more. Despite parallelizable computation, computing \u2207 x z i j remains challenging. The detailed analysis is also in Appendix C. It is still challenging to compute the loss for large networks in a high dimension setting, where even calling D + 1 forward passes in parallel as used in \u00a74.1 is infeasible due to memory constraints. An unbiased estimator of the loss in Eq. FORMULA17 when \u00ce(x, \u03b3) = I is proposed. The sum of gradient norms is decoupled for efficient computation. Uniformly sampling D input axes provides an unbiased approximation to Eq. (10), requiring D + 1 times memory for computing all partial derivatives. The proposed algorithms allow for unbiased approximation of Eq. (10) by uniformly sampling D input axes, requiring D + 1 times memory for computing all partial derivatives. The approach can be applied to deep learning models with affine transformations and piecewise linear activation functions. It does not immediately generalize to nonlinearity of maxout/max-pooling, but suggestions are provided in Appendix E. Comparisons with a baseline model ('ROLL') are made in this section. In this section, the approach ('ROLL') is compared with a baseline model ('vanilla') in various scenarios using accuracy, number of complete linear regions, and p margins of linear regions as evaluation measures on a testing set. Experiments are conducted on a 4-layer FC model with ReLU activations using a split of the MNIST dataset for training/validation/testing. The experiments on a 4-layer FC model with ReLU activations using a split of the MNIST dataset show that the tuned models achieve larger margins with a tradeoff of 1% accuracy. The Spearman's rank correlation between x1 and x2 among testing data is at least 0.98 for all cases. The ROLL loss achieves about 10 times larger margins for most percentiles compared to the vanilla loss. The Spearman's rank correlation between x1 and x2 among testing data is high at 0.98 for all cases. The lower #CLR in our approach compared to the baseline model indicates larger linear regions spanning different testing points. Points within the same linear region in the ROLL model with ACC=98% share the same label, while visually similar digits may be in the same region in another ROLL model. Parameter analysis in Figure 2 shows that as C and \u03bb increase, accuracy decreases with an increased margin. Higher \u03b3 values result in less sensitivity to hyper-parameters C and \u03bb. Running time for a complete mini-batch gradient descent step is measured to validate the efficiency of the proposed method. The efficiency of the proposed method is validated by measuring the running time for a complete mini-batch gradient descent step. A comparison is made between different loss computation methods, showing that the approximate ROLL loss is about 9 times faster than the full loss. The perturbation algorithm achieves about 12 times empirical speed-up compared to back-propagation. Overall, the computational overhead of the method is minimal compared to the vanilla loss. The computational overhead of the method is minimal compared to the vanilla loss, achieved through the perturbation algorithm and approximate loss. RNNs are trained for speaker identification on a Japanese Vowel dataset with variable sequence length and channels. The network utilizes the state-of-the-art scaled Cayley orthogonal RNN for preventing gradient vanishing/exploding. The implementation details of training RNNs for speaker identification using orthogonal matrices to prevent gradient issues are provided in Appendix H. Results show that the proposed approach leads to larger margins on testing data compared to the vanilla loss, with a Spearman's rank correlation of 0.98. Sensitivity analysis on derivatives and visualization results are also presented, showing the stability bounds of the ROLL regularization method. The visualization in FIG4 compares the vanilla model with our ROLL model, showing a stability bound of the ROLL regularization consistently larger. Experiments are conducted on Caltech-256 BID18 dataset using a downsized 18-layer ResNet trained with ROLL loss. Evaluation measures are challenging due to high input dimensionality, requiring a sample-based approach for stability evaluation. The text discusses evaluating the stability of gradients in a local region using a sample-based approach. It focuses on the gradient distortion and compares different models using labeled data. The evaluation is done in terms of expected distortion and maximum distortion within an intersection of an \u221e-ball and image domain. The text also mentions visualizations of examples in Caltech-256 dataset showing maximum gradient distortions on the ROLL model. In Figure 4, examples in Caltech-256 dataset are visualized to show maximum gradient distortions on the ROLL model. The adversarial gradient is obtained through optimization using a genetic algorithm due to the complexity of the loss function. Implementation details are provided in Appendix J, with 8000 samples used to approximate the expected distortion. The evaluation is limited to 1024 random images in the testing set for both maximum and expected gradient distortions, with an \u221e-ball radius set to 8/256. The ROLL loss shows more stable gradients and slightly better precision compared to the vanilla loss. Only 40 and 42 out of 1024 images had their prediction labels changed in the ROLL and vanilla models, respectively. The ROLL loss produces stable shapes and intensities of gradients, while the vanilla loss does not. This paper introduces a new learning problem to create locally transparent neural networks. This paper introduces a new learning problem to endow deep learning models with robust local linearity by constructing locally transparent neural networks with stable derivatives. The proposed ROLL loss expands regions with stable derivatives and generalizes the stable gradient property across linear regions. The proof of directional feasibility and 1-ball feasibility is presented, showing the linear properties of the feasible sets in relation to given points and balls. Additionally, a 2-ball certificate is introduced to determine the minimum distance between points and hyperplanes within convex sets. The 2-ball certificate establishes the minimum distance between points and hyperplanes in convex sets, ensuring that hyperplanes induced from linear constraints are at least 2 units away from the point x. This is proven by constructing a neural network feasible model with the same loss as the optimal model. The neural network g \u03b8 is constructed with the same weights and biases as f \u03b8 but with a linear activation function o i j = max(0, o i j ) \u2208 {0, 1}. Each layer in g \u03b8 is represented as: We collect partial derivatives with respect to an input axis k by feeding zero and unit vectors to g \u03b8. The derivative of each neuron z i j with respect to x k can be computed. The derivative of all neurons to an input dimension can be computed with 2 forward passes, taking a total of 2M operations. The complexity analysis assumes no overhead for parallel computation and a unit operation for batch matrix multiplication. The complexity of computing gradients using forward and backward passes is compared to back-propagation. Forward passes with perturbations require 2M operations, while back-propagation needs Mi=1 2iNi operations. Chain-rule of Jacobian is used for dynamic programming to compute gradients efficiently. The Jacobian is used for dynamic programming to compute gradients efficiently in neural networks. The approach is efficient for fully connected networks but inefficient for convolutional layers due to the expensive linear transformation. An introductory guide is provided for deriving inference and learning methods for maxout/max-pooling nonlinearity. It is feasible to derive methods for piecewise linear networks with max-pooling nonlinearity, but using convolution with large strides is suggested over max-pooling neurons. The text discusses the suggestion to use convolution with large strides or average-pooling instead of max-pooling nonlinearity in neural networks. It explains how activation patterns can lead to a linear model and discusses the feasible set of activation patterns in the input space. The feasible set of activation patterns in the input space can be derived for a feasible activation pattern. Linear constraints are evident, allowing for the application of inference and learning algorithms with convex polyhedron feasible sets. The model consists of fully-connected hidden layers with specific dimensions and a sigmoid cross entropy loss function. Training is done with an Adam optimizer over 5000 epochs, selecting the model based on training loss. The model is trained for 5000 epochs using the Adam optimizer. The regularization parameters are tuned, and the data is normalized. The fully-connected model has 4 hidden layers with 300 neurons each and ReLU activation. The loss function is cross-entropy with soft-max. The model has hidden layers with 300 neurons each and ReLU activation. The loss function is cross-entropy with soft-max. Stochastic gradient descent with Nesterov momentum is used, with a learning rate of 0.01 and a batch size of 64. Tuning involves a grid search on \u03bb, C, \u03b3. The data is not normalized, and the representation is learned with a single layer scoRNN. The representation is learned with a single layer scoRNN, using LeakyReLU activation functions and a hidden neuron dimension of 512. The loss function is cross-entropy with soft-max, optimized with AMSGrad. Tuning involves a grid search on \u03bb, C, \u03b3, with models achieving similar testing accuracy to the baseline model reported. Training is done on normalized images with a bijective mapping between normalized and original distances. The model architecture of the pre-trained ResNet-18 is revised by replacing max-pooling with average-pooling after the first convolutional layer and enlarging the receptive field of the last pooling layer to output 512 dimensions. The bijection between normalized and original distances is applied to compute the vanilla loss along each channel. The ResNet-18 model is adapted for larger images than the original ImageNet data. It is trained using stochastic gradient descent with Nesterov momentum for 20 epochs, starting with a learning rate of 0.005 and adjusting to 0.0005 after 10 epochs. The model is tuned with a fixed C value of 8 and \u03bb values ranging from 10^-6 to 0.001. The batch size is 32, and the best model based on validation loss is selected. The ResNet-18 model is trained with a fixed C value of 8 and \u03bb set to 0.001. A genetic algorithm with 4800 populations and 30 epochs is used for training with 360 random samples. Samples are evaluated based on gradient distance, sorted, and updated through crossover and projection steps to ensure feasibility. The genetic algorithm used for training the ResNet-18 model involves updating samples in P by performing an \u221e -projection to ensure feasibility. Mutation was not implemented due to computational reasons, and the crossover operator is used analogous to a gradient step. Various images are visualized, including the original image, original gradient, adversarial gradient, image of adversarial gradient, original integrated gradient attribution, and adversarial integrated gradient attribution. The integrated gradient attribution and adversarial integrated gradient are visualized by aggregating derivatives in each channel, taking the absolute value, normalizing, and clipping values above 1. The derivatives are then visualized as a gray-scaled image to highlight differences in settings. The integrated gradient attribution and adversarial integrated gradient are visualized by aggregating derivatives in each channel, taking the absolute value, normalizing, and clipping values above 1. The element-wise product between the grayscaled integrated gradient and the original image is visualized to highlight differences in settings. Examples from the Caltech-256 dataset that yield different percentiles of maximum gradient distortions on the ROLL model are shown in Figures 5 and 6. The exact values of the maximum gradient distortion for each image are displayed in the captions. Figures 5 and 6 show examples from the Caltech-256 dataset that yield different percentiles of maximum gradient distortions on the ROLL model. The maximum 1 gradient distortion values for the vanilla model and ROLL model are provided for various images such as 'Projector', 'Bear', and 'Rainbow'."
}