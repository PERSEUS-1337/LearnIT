{
    "title": "rJxlc0EtDr",
    "content": "Recent research on neural network architectures with external memory has focused on the bAbI question and answering dataset, which presents challenging tasks requiring reasoning. A classic associative inference task from human neuroscience literature was used to assess the reasoning capacity of existing memory-augmented architectures. Current architectures struggle with reasoning over long distance associations, as shown in tasks involving finding the shortest path between nodes. A novel architecture, MEMO, was developed to address this issue by introducing two new components. MEMO is a novel architecture designed to reason over longer distances by introducing two new components: a separation between memories/facts stored in external memory and an adaptive retrieval mechanism. It can solve novel reasoning tasks and all 20 tasks in bAbI. This ability is crucial for making judgments that require connecting facts acquired at different times and experiences. Inferential reasoning involves recombining single experiences to infer relationships, supported by the hippocampus. Memories are stored independently to minimize interference, allowing for specific recall of events. Recent research has shown that the integration of separated experiences occurs at the point of retrieval through a recurrent mechanism, allowing for inference by interacting multiple pattern separated codes. This insight is used to explore enhancing inferential reasoning in neural networks, such as the Differential Neural Computer and end-to-end memory networks. Recent research has focused on enhancing inferential reasoning in neural networks by utilizing external memory and attention mechanisms. A new task called Paired Associative Inference (PAI) has been introduced to overcome limitations in traditional neural networks when faced with repetitive tasks. This task is derived from neuroscientific literature and aims to capture the essence of inferential reasoning. The PAI task, derived from neuroscientific literature, aims to capture inferential reasoning by forcing neural networks to learn abstractions for solving unseen associations. It is followed by tasks involving finding the shortest path and bAbi to investigate memory representations supporting memory-based reasoning. Previous models have used fixed memory representations combining word embeddings with positional encoding transformations. MEMO is a new approach that retains all facts in memory and uses a linear projection with a recurrent attention mechanism for flexible weighting. It addresses the issue of prohibitive computation time in neural networks by growing computation based on input size rather than problem complexity. In standard neural networks, computation is adjusted based on input size, either by padding with extra values or dropping values. The amount of computation is typically hand-tuned, but a new approach inspired by human memory models aims to adapt compute time to task complexity. In a new approach inspired by human memory models, a neural network adjusts its computation time based on task complexity. The network uses a halting policy to determine if it has settled into a fixed point, terminating the process when it does. This is implemented using techniques like adaptive computation time and reinforcement learning to optimize the number of computation steps. Our approach utilizes reinforcement learning to adjust weights based on the optimal number of computation steps. By adding an extra term to the REINFORCE loss, we minimize the expected number of computation steps, encouraging the network to prefer representations that require less computation. Our contributions include a task emphasizing reasoning and an investigation into memory representations that support this task. The curr_chunk discusses the essence of reasoning and an investigation into memory representations supporting inferential reasoning. It also introduces a REINFORCE loss component for optimal learning iterations and presents empirical results on various tasks. The setup is based on End-to-End Memory Networks and focuses on a multilayer, tied weight variant. The curr_chunk focuses on the architecture of the End-to-End Memory Networks (EMN) and its application in predicting answers from knowledge inputs and queries. It involves embedding words, using key, value, and query matrices, and positional encoding for inferential reasoning. The End-to-End Memory Networks (EMN) utilize embedding matrices for key, values, and query, along with positional encoding for inferential reasoning. EMN calculates weights over memory elements and produces outputs based on queries. The model is trained via cross entropy loss on the final answer. MEMO embeds inputs differently by deriving a common embedding for each input matrix. MEMO adapts input matrices by deriving a common embedding for each, without using hand-coded positional embeddings. Multiple heads are used to attend to the memory, with each head having a different view of the inputs. This approach allows for flexible capturing of input sentence parts. MEMO uses embedding matrices for key, values, and query without hand-coded positional embeddings. It employs multi-head attention, DropOut, and LayerNorm for improved generalization and learning dynamics. The attention mechanism in MEMO allows for flexible recombination of stored items. MEMO utilizes DropOut and LayerNorm to enhance generalization and learning dynamics. The attention mechanism involves matrices for transforming logits and queries, with an output MLP producing the answer. While inspired by Vaswani et al. (2017), MEMO differs by preserving queries separate from keys and values, reducing computational complexity. MEMO preserves queries separate from keys and values, reducing computational complexity. It uses gated recurrent units and a MLP to learn the number of computational steps required to answer a query effectively. The input to the network is formed by the Bhattacharyya distance between attention weights. The network in MEMO is trained using REINFORCE and adjusts parameters using n-step look ahead values. The objective is to minimize a term called L Hop, which follows from the fact that \u03c0 is a. The objective function of the network in MEMO is to minimize L Hop, a term that encourages minimizing the expected number of hops in computation. This term is similar in motivation to previous work but directly promotes representations that require less computation. Using REINFORCE for training discrete random variables can lead to high variance, but in the case of a binary halting random variable, the variance is just p(1 \u2212 p). The variance of binary halting random variables is bounded by p(1 \u2212 p), where p is the probability of halting. The reward structure is defined by the target answer a and the prediction \u00e2 from the network. The final layer of M LP R is initialized with bias init to increase the probability of producing a probability of 1. The network has a maximum number of hops, N, before stopping. No gradient sharing between the hop network and the main MEMO network. In recent years, there has been a growing interest in memory-augmented networks for abstract and relational reasoning tasks. The Differential Neural Computer (DNC) is an influential model that operates sequentially on inputs, learning to read and write to a memory store. While the DNC can solve algorithmic problems, scaling to higher-dimensional domains is challenging. An extension incorporating sparsity into the DNC has shown improved performance on larger-scale tasks like the bAbI task suite. Several memory-augmented architectures have been developed since the publication of the DNC, including the Dynamic Memory Network, Recurrent Entity Network, Working Memory Network, and RelationNet. These models enable relational reasoning over memory contents and have shown good performance on various tasks. Several memory-augmented architectures have been developed to enable relational reasoning over memory contents, including models like the Dynamic Memory Network, Recurrent Entity Network, Working Memory Network, and RelationNet. These models have shown good performance on various tasks. New approaches like Adaptive Computation Time (ACT) and Adaptive Early Exit Networks have been introduced to dynamically adjust computational steps based on task complexity. Conditional computation techniques have been explored in neural networks, such as using REINFORCE to adjust computation steps dynamically. This approach has been applied to recurrent neural networks to decide on activating the next layer or skipping steps. Other variations include jumping in sequence to reduce processed inputs and incorporating external memory. These methods aim to optimize computation efficiency based on task complexity. Graph Neural Networks (GNNs) involve iterative message passing to propagate embeddings in a graph. Neural networks aggregate functions over graph components for various learning tasks. The message passing process is similar to attention mechanisms. Our work introduces a model that differs from Graph Neural Networks (GNNs) by performing adaptive computation to modulate the number of message passing steps and does not require message passing between memories. The model allows input queries to attend directly to memory slots. The paper introduces a task called paired associative inference (PAI) to probe the reasoning capacity of neural networks by testing their ability to appreciate distant relationships among elements in memory. This task is derived from neuroscience and involves associating random pairs of images to simulate reasoning processes. The paired associative inference (PAI) task involves randomly associating images to test reasoning capacity. Agents are presented with pairs of images and later tested with direct and indirect queries to assess episodic memory and inference abilities. The study compared MEMO with other memory-augmented architectures like End to End Memory Networks (EMN), DNC, and Universal Transformer (UT) in the paired associative inference task. The right answer in the task can only be produced by understanding the association between images A and C, similar to recognizing a connection between two people walking with the same little girl. For detailed batch creation information, refer to the appendix. MEMO was compared with other memory-augmented models like DNC and UT in the paired associative inference task. MEMO outperformed other models on harder inference queries and longer sequences, showing superior accuracy and efficiency in solving complex tasks. In the paired associative inference task, MEMO outperformed DNC by requiring fewer pondering steps to achieve the same accuracy. MEMO retrieved memory slots efficiently to form associations and avoid interference, showcasing superior performance in complex tasks. In the paired associative inference task, MEMO outperformed DNC by requiring fewer steps for accurate performance. MEMO efficiently retrieved memory slots to form associations and avoid interference, showcasing superior performance in complex tasks. In a specific example, MEMO retrieved memories in multiple hops to support correct inference decisions, following a pattern predicted by computational models of the hippocampus and observed in neural data. The algorithm used in MEMO for inference tasks depends on the number of hops the network takes, with more hops initially used for over-parametrization and then automatically reduced for efficiency. Ablation experiments confirmed that specific memory representations and recurrent attention mechanisms are crucial for successful inference in MEMO. The study compared the adaptive computation mechanism in MEMO with ACT (Graves, 2016) and found MEMO to be more data efficient for inference tasks. The algorithm in MEMO for inference tasks relies on specific memory representations and recurrent attention mechanisms. This conclusion was valid only for inference queries, not for direct queries, which test episodic memory with a single memory look-up. Table 2 displays the accuracy of models in synthetic reasoning experiments on randomly generated graphs. DNC, Universal Transformer, and MEMO showed perfect accuracy in predicting intermediate shortest path nodes on a small graph. However, MEMO outperformed EMN in predicting the first node on more complex graphs. MEMO outperformed EMN and DNC in predicting nodes on complex graphs with high connectivity. Universal Transformer showed different performance in predicting the first and second nodes of the shortest path. In comparison to MEMO, EMN and DNC, Universal Transformer exhibited varied performance in predicting nodes on complex graphs with high connectivity. Additionally, MEMO achieved high accuracy on the bAbI question answering dataset, solving all tasks with lower error compared to other models. MEMO successfully solved all tasks in the 10k training regime, matching the performance of previous models but with lower error. Ablation experiments highlighted the importance of memory representations and recurrent attention. The use of layernorm in the recurrent attention mechanism improved stability and performance. Test results on the bAbI task showed competitive performance compared to DNC and Universal Transformer. In an in-depth investigation of memory representations for inferential reasoning, MEMO, an extension to existing memory architectures, showed state-of-the-art results in paired associative inference and graph traversal tasks. It was the only architecture to solve long sequences and matched the performance of current state-of-the-art results on the bAbI dataset. Flexible weighting of memory was key to achieving these results. Our analysis supported the hypothesis that flexible weighting of individual elements in memory, achieved by combining separated storage of facts with a recurrent attention mechanism, led to state-of-the-art results on the bAbI dataset. The task was made challenging by starting from the ImageNet dataset and creating three distinct datasets with sequences of varying lengths. Each dataset contained a large number of training, evaluation, and testing images with no repetition in each sequence. Each dataset contains 1e6 training images, 1e5 evaluation images, and 2e5 testing images with sequences randomly generated. Batches are built with memory, query, and target entries by selecting N sequences from the pool. Memory content includes pair-wise associations between items in the sequence, resulting in 32 rows for S = 3. Queries consist of a cue, match, and lure images, with the cue and match from the sequence and the lure from a different sequence. The cue, match, and lure images are used in direct and indirect queries to test episodic memory. Direct queries involve retrieving an episode experienced, while indirect queries require inference across multiple episodes. The queries are presented as image embedding vectors concatenated together. The cue, match, and lure images are used in direct and indirect queries to test episodic memory. The queries are presented as concatenated image embedding vectors. The position of the match and lure images is randomized to avoid degenerate solutions. The task requires appreciating the correct connection between images and avoiding interference from other items in memory. The batch is balanced with half direct queries and half indirect queries. The network predicts the class of matches from memory store, with longer sequences providing both direct and indirect queries requiring different levels of inference. Inputs for EMN and MEMO use memory and query naturally, while DNC embeds stories and query similarly. In the case of DNC (section G), stories and query are embedded similarly to MEMO. Memory and query are presented sequentially to the model, followed by blank inputs for final prediction. For UT, stories and query are embedded like MEMO, using the encoder described in Section H. Results are from the evaluation set at the end of training, each containing 600 items. Graph generation for shortest path experiments follows the method of Graves et al. (2016), sampling two-dimensional points from a unit square to generate graphs for network training. Graphs for network training are generated by sampling two-dimensional points from a unit square. The graph description includes tuples of integers representing connections between nodes, with queries and target paths also represented as tuples of integers. During training, a mini-batch of 64 graphs with queries and target paths is sampled. During training, a mini-batch of 64 graphs is sampled with associated queries and target paths. Queries are represented as a matrix of size 64 \u00d7 2, targets as 64 \u00d7 (L \u2212 1), and graph descriptions as 64 \u00d7 M \u00d7 2. The upper bound M is fixed to the maximum number of nodes multiplied by the out-degree of the nodes in the graph. Networks are trained for 2e4 epochs with 100 batch updates each. For EMN and MEMO, the graph description is set to the contents of their memory, and the query is used as input to answer the sequence of nodes in the target path. The model predicts answers for nodes sequentially, with MEMO using predicted answers as queries for the next node, while EMN uses ground truth answers. Weights for each answer are not shared. The Universal Transformer embeds queries and graph descriptions, concatenates them, and uses the encoder. The Universal Transformer embeds queries and graph descriptions, concatenates them, and uses the encoder to provide an answer. For DNC, the graph description tuples are presented first, followed by the query tuple, and pondering steps are used to output the sequence of nodes for the proposed shortest path. Training is done using Adam with a cross-entropy loss against sampled target sequences. The models are trained using Adam with cross-entropy loss against sampled target sequences. Evaluation involves sampling a batch of 600 graph descriptions, queries, and targets to calculate mean accuracy over all nodes of the target path. DNC and UT have a 'global view' on the problem, allowing them to reason and work backwards from the end node to achieve better performance. MEMO has a 'local view' on the problem, where the answer to the second node depends on the answer about the first node. Comparing MEMO versus EMN, an experiment was conducted using ground truth answers and model predictions for the first node as queries for the second node. Results showed that providing MEMO with the ground truth for node 1 as a query for node 2 improved performance compared to using the prediction of the first node. In an experiment comparing MEMO and EMN, providing MEMO with the ground truth for node 1 as a query for node 2 improved performance. EMN performed poorly when using the same training regime as MEMO. The study used the English Question Answer dataset with specific pre-processing steps. \u2022 The text explains how commas in answers are not ignored, leading to each answer having its own label. Questions are separated from the text and used as queries. During training, a mini-batch of 128 queries and corresponding stories are sampled. The queries are a matrix of 128 \u00d7 11 tokens, while sentences are of size 128 \u00d7 320 \u00d7 11. EMN and MEMO use stories and queries as inputs in their architecture. In the case of DNC and UT, stories and queries are embedded similarly to MEMO. The model processes stories and queries in sequence, followed by blank inputs for final prediction. Optimization steps using Adam are performed for all models in the experiments, with specific hyperparameters. MEMO includes a column vector to account for temporal context in bAbI tasks. MEMO was trained for 2e4 epochs with 100 batch updates. Evaluation involved sampling a batch of 10,000 elements and computing mean accuracy over examples. The model used polynomial learning rate decay and cross entropy loss to predict class ID in tasks. In training MEMO, batch sizes were set at 64 for PAI and shortest path tasks, and 128 for bAbI. The network predicted class ID, node ID, and word ID for each respective task. The halting policy network parameters were updated using RMSProp with a specific learning rate. The temporal complexity of MEMO is O(n s \u00b7 A \u00b7 N \u00b7 H \u00b7 I \u00b7 S \u00b7 d), where various factors determine the complexity. MEMO is a memory-based model with linear complexity compared to the quadratic complexity of the Universal Transformer. The spatial complexity of MEMO is O(I \u00b7 S \u00b7 d), where the size of memory is fixed. The halting unit h is defined according to Graves (2016) in the implementation of MEMO. In our experiments, the size of memory is fixed. We implement ACT based on Graves (2016) and define the halting unit h differently than the original ACT. This change aims to increase fairness in comparison and enable more powerful representations. The halting probability is defined as in the original work, with a fixed value of 0.01. In our experiments, the halting mechanism's feasibility is explored by defining the halting probability and using the same architecture as described in previous works by Graves et al. (2016) and Dehghani et al. (2018). Hyperparameters were searched for to optimize the model's performance. The hyperparameters used in training the model were described in tensor2tensor/models/research/universal_transformer.py. A search was conducted to optimize the model's performance on various tasks."
}