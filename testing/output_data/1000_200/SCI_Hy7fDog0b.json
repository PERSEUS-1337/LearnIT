{
    "title": "Hy7fDog0b",
    "content": "Generative models are useful for modeling complex distributions, but current training techniques require fully-observed samples. In scenarios where obtaining full samples is costly or impossible, learning from partial, noisy observations is more practical. A new method called AmbientGAN is proposed for training Generative Adversarial Networks (GANs) using lossy measurements of samples. This approach can recover the true underlying distribution even with per-sample information loss, leading to significant improvements in generative models on benchmark datasets. Generative models trained with the AmbientGAN method show substantial improvements in qualitative and quantitative performance, achieving higher inception scores compared to baselines. This approach allows for training generative models directly from noisy or incomplete samples, overcoming the challenge of collecting enough data for training. The method involves passing the generator's output through a random measurement function, with the discriminator distinguishing between real and generated measurements. The framework is able to recover the unknown distribution from various types of measurements, demonstrating its effectiveness in scenarios where full samples are costly or unavailable. The AmbientGAN method allows for training generative models directly from noisy or incomplete samples by using a new way of training GANs. The discriminator distinguishes between real and simulated measurements of a generated image, enabling the construction of good generative models from extremely noisy observations and low dimensional projections. Our method can construct generative models from noisy observations and low dimensional projections, demonstrated through qualitative visual quality and quantitative comparison to baseline methods. Theoretical results show that noisy measurements uniquely determine the distribution of original images, leading to the need for a generative model that matches the true distribution. The distribution of measured images uniquely determines the distribution of original images, leading to the need for a generative model that matches the true distribution. Different measurement models, such as dropout and random projection, are considered. Empirical results show challenges in inpainting individual images in the celebA dataset, leading to significant artifacts when learning a GAN on the cleaned data. Incorporating the measurement process into GAN training improves sample quality. Learning from noisy, blurred images in celebA dataset leads to poor results. Models denoised by Wiener deconvolution produce cleaner samples. Generative model on MNIST dataset learns from 1D projections, improving sample quality. AmbientGAN is able to recover underlying structure from random lines projected onto images. Two variants are considered, with the first variant unable to identify distribution up to rotation or reflection. Neural network based generative models can be constructed using autoregressive or adversarial approaches. The adversarial framework is powerful for modeling complex data distributions like images, video, and 3D models. Generative models have various applications explored in different papers. Generative models, such as GANs, are used in various applications like image translation and creating realistic synthetic data. Training stability can be improved by operating generators against discriminators on different low-dimensional projections of data. This concept is related to creating 3D object shapes from 2D projections in the AmbientGAN framework. The authors' work is related to creating 3D object shapes from 2D projections in the AmbientGAN framework. They use notation to denote real and generated distributions, underlying space, and measurements. Lossy measurements are performed on samples from a real underlying distribution, with each measurement being an output of a stochastic measurement function parameterized by \u03b8. The authors use a distribution p\u03b8 for measurement functions, where measurements y = f\u03b8(x) are obtained. They aim to create a generative model of distribution prx using samples from distribution pry, combining measurement process with adversarial training. The model involves a random latent vector Z \u223c pz for sampling. The training framework involves learning a generator G to match distributions pgx and prx using random measurements on generated objects Xg. A random measurement function f\u0398 is sampled to obtain measurements y = f\u0398(Xg), with the discriminator distinguishing real from fake measurements. The discriminator predicts if y is from real or generated distribution. Quality function q(x) defines objective based on discriminator output. AmbientGAN objective requires differentiable f\u03b8. G and D are implemented as neural networks. Model is end-to-end differentiable and trained using gradient-based GAN procedure. Sampling Z, \u0398, Yr in each iteration to compute stochastic gradients for G and D parameters. Alternating between steps in training. Our approach involves using stochastic gradients to update parameters in G and D alternately. It is compatible with various GAN improvements and can incorporate additional information like sample labels. We experiment with different GAN models and describe the measurement models used for theoretical and empirical results, focusing on 2D images. The measurement models used for theoretical and empirical results in our approach focus on 2D images. These models include Block-Pixels, Convolve+Noise, Block-Patch, Keep-Patch, and Extract-Patch. The AmbientGAN learning framework is versatile and can be applied to various data formats and measurement models. The measurement functions for 2D images in the AmbientGAN framework include Keep-Patch, Extract-Patch, Pad-Rotate-Project, PadRotate-Project-\u03b8, and Gaussian-Projection. These functions manipulate image patches and projections to extract information for recovery. The AmbientGAN framework includes various measurement functions for 2D images such as Keep-Patch, Extract-Patch, Pad-Rotate-Project, PadRotate-Project-\u03b8, and Gaussian-Projection. These functions manipulate image patches and projections to extract information for recovery. In the context of random Gaussian vector projection onto measurements, it is shown that the true underlying distribution can be recovered for certain measurement models. The unique distribution consistent with observed measurements is invertible, providing a consistency guarantee with the AmbientGAN training procedure. The AmbientGAN framework uses various measurement functions for 2D images to extract information for recovery. The framework shows that the true underlying distribution can be recovered under certain measurement models, such as Gaussian-Projection, Convolve+Noise, and Block-Pixels. The uniqueness of the distribution consistent with observed measurements provides a guarantee for recovering the true underlying distribution with the AmbientGAN framework. The AmbientGAN framework utilizes different measurement functions for 2D images to recover the true underlying distribution. The framework guarantees recovery under measurement models like Gaussian-Projection, Convolve+Noise, and Block-Pixels, with a unique distribution consistent with observed measurements. The setting involves Gaussian blurring kernel with additive Gaussian noise and assumes a finite discrete set of pixel values. Sample complexity results are provided for learning distributions in the AmbientGAN framework. The experiments used three datasets: MNIST for handwritten digits, CelebA for celebrity faces, and CIFAR-10 for images from 10 classes. Two GAN models were used for MNIST: a conditional DCGAN and another model with specific architectures detailed in the appendix. For the MNIST dataset, two GAN models were used: a conditional DCGAN and an unconditional Wasserstein GAN with gradient penalty. For the celebA dataset, an unconditional DCGAN was utilized. The CIFAR-10 dataset employed an Auxiliary Classifier Wasserstein GAN with gradient penalty. Different discriminator architectures were used for 2D and 1D projections. The fully connected discriminators used for the MNIST dataset were 25-25-1 and for the celebA dataset were 100-100-1. Baseline approaches were implemented to evaluate the AmbientGAN framework's performance, including an \"ignore\" baseline and a stronger baseline based on invertible measurement functions. The AmbientGAN setting violates assumptions of invertible measurement functions. To address this, an approximate inverse function is used to train a generative model with estimated inverse samples. This approach aims to improve the approximation of the generative model. For various measurement models, approximate inverse functions were obtained using different methods. For Block-Pixels measurements, blurring the image or using total variation inpainting was used. Convolve+Noise measurements utilized Wiener deconvolution. Block-Patch measurements employed Navier Stokes based inpainting. Other measurement models did not have clear methods for obtaining approximate inverse functions. For the Pad-Rotate-Project-\u03b8 measurement model, inverting the Radon transform is a common technique, but it is challenging due to limited projections. Inverting Pad-Rotate-Project measurements is even more difficult as it lacks information about \u03b8. Therefore, only results with AmbientGAN models are reported in this subset of experiments. Samples generated by baselines and our models are presented for each experiment. The samples from the dataset, baselines, and our models are shown for each experiment. Results on MNIST are in the appendix. Results on celebA with DCGAN and CIFAR-10 with ACW-GANGP are presented. Our models produce images with good visual quality, while baselines struggle due to the degraded measurement process. Our models use Gaussian kernel and IID Gaussian noise. Results on celebA with DCGAN show that measurements are noisy, but our models can create coherent faces. Different measurement models show signal degradation. On MNIST with DCGAN, our model learns rotation and reflection without explicit incentive. Our method demonstrates the ability to generate images of digits using only 1D projections, showing challenges in learning complex distributions and the need for better training methods for GANs. Our method shows challenges in learning complex distributions with 1D projections and the need for better GAN training methods. Inception scores are used to evaluate generative models in the AmbientGAN framework on CIFAR-10 and MNIST datasets. The Inception score was computed using the network described. A plot of the scores as a function of p is shown in Fig. 7 (left). As p increases, AmbientGAN models outperform baseline models. For Convolve+Noise measurements on MNIST, varying the noise level \u03c3 showed that AmbientGAN models maintained a high inception score even with increased noise levels. The AmbientGAN models maintain high inception scores even as noise levels increase. The Pad-Rotate-Project model performs poorly with an inception score of 4.18, while the model with Pad-Rotate-Project-\u03b8 measurements achieves a score of 8.12. The vanilla GAN model trained with fully-observed samples scores 8.99. The second model, trained on 1D projections, comes close to the fully-observed case. In Fig. 8 (left), the inception score vs the probability of blocking pixels p is shown for the BlockPixels measurement model on CIFAR-10. Generative models require high-quality datasets, but our approach learns from incomplete, noisy measurements. In Fig. 8, we compare our method's performance on CIFAR-10 with baselines, showing superiority. The inception score trends similarly to MNIST, demonstrating the effectiveness of our approach. Our approach learns from incomplete, noisy measurements to construct new generative models of distributions. If the Discriminator D is optimal in the vanilla GAN model, the induced measurement distribution matches the given measurement distribution. The measurement model covers all possible directions for projections, including the projection vector \u0398. The measurement model includes the projection vector \u0398, matching all 1D marginals to converge to the true underlying distribution. The Convolve+Noise measurement model with specific conditions leads to a unique distribution that induces the measurement distribution. The Convolve+Noise measurement model with specific conditions leads to a unique distribution that induces the measurement distribution. This is achieved through a bijective map between X and Z, where the pdfs of X and Z are related by a Jacobian function. The pdf of Y, being a sum of two random variables, is a convolution of their individual probability density functions. The proof involves the reverse map from the measurement distribution to the sample distribution, uniquely determining the true underlying distribution. Theorem 1 from BID11 is stated for the discrete setting, with the empirical version of the vanilla GAN objective defined for a dataset of measurement samples. The empirical version of the vanilla GAN objective is defined for a dataset of measurement samples {y1, y2, ..., ys}, with optimal discriminator and generator conditions. The proof involves the ERM version of the loss and a unique solution for the Block-Pixels measurement model under certain conditions. The Block-Pixels measurement model involves a unique distribution p x over [t], with a transition matrix A \u2208 R t\u00d7t. If A is invertible, p x can be recovered from the distribution p y. The sample complexity depends on the minimum eigenvalue magnitude of A. The distribution p x can be recovered from p y if A is invertible. The sample complexity is determined by the minimum eigenvalue magnitude of A. For Block-Pixels measurement, images are divided into classes based on the number of zero pixels, and a transition matrix A is considered. The transition matrix A for images sorted by class number is lower triangular due to the blocking of pixels independently with probability p. The diagonal entries of A are strictly positive with a minimum value of (1 - p) n, making A invertible. The DCGAN and WGANGP models on MNIST follow specific architectures. The DCGAN model uses a noise input with 100 dimensions sampled uniformly, while the generator and discriminator both use concatenated labels. The WGANGP model takes a latent vector of 128 dimensions. Both models incorporate batch-norm and have distinct layer structures. The WGANGP model on MNIST uses a latent vector of 128 dimensions with one linear and three deconvolutional layers in the generator, and three convolutional layers followed by one linear layer in the discriminator. The unconditional DCGAN model on celebA has a latent vector of 100 dimensions with batch-norm used in both generator and discriminator. The ACWGANGP model on CIFAR-10 uses a latent vector of 128 dimensions with a residual architecture. The CIFAR-10 model utilizes a latent vector of 128 dimensions with a residual architecture in the generator. It includes conditional batch normalization, nonlinearity, and upconvolution layers in the residual blocks. The discriminator consists of convolutional layers and a final linear layer. The analysis assumes known parametric form of the measurement function for simulating the measurement process. In the experiment, the AmbientGAN approach is shown to be robust to mismatches in the parameter distribution of the measurement function. The Block-Pixels measurement model is used with the MNIST dataset, where pixels are blocked with a certain probability to create measurements. AmbientGAN models are trained with different blocking probabilities, and their performance is evaluated after training. After training AmbientGAN models with various blocking probabilities, the inception score peaks at p = 0.5, indicating robustness to parameter distribution mismatch. The generator learned through AmbientGAN captures data distribution well and is used for compressed sensing on MNIST with p = 0.5. Reconstruction error vs the number of measurements is plotted. Using the generator learned from AmbientGAN trained with corrupted samples at p = 0.5, a reduction in the number of measurements for compressed sensing on MNIST is observed compared to Lasso, as shown in the reconstruction error plot."
}