{
    "title": "SJg9z6VFDr",
    "content": "Recently, a new model called graph ordinary differential equation (GODE) has been proposed for graph data, inspired by neural ordinary differential equation (NODE) for Euclidean data. GODE uses continuous-depth models and two efficient training methods: indirect back-propagation with the adjoint method and direct back-propagation through the ODE solver. Direct backprop has shown better performance in experiments. Additionally, a family of bijective blocks is introduced to reduce memory consumption. GODE outperforms the adjoint method, introduces bijective blocks for $\\mathcal{O}(1)$ memory consumption, and improves accuracy in various graph neural networks. It achieves continuous modeling in time, memory efficiency, accurate gradient estimation, and generalizability. CNNs excel in tasks like image classification and segmentation but are limited to Euclidean grid data, hindering their application in irregular datasets like graphs. Graph data structures represent objects as nodes and relations as edges, widely used for irregularly structured data like social networks, protein interaction networks, and citation graphs. Traditional methods like random walk and graph embedding have limitations in modeling graphs, leading to the development of graph neural networks (GNN) that generalize convolution operations to capture local features. Graph neural networks (GNN) generalize convolution operations to graphs to capture local information. There are two types of methods for convolution on a graph: spectral and non-spectral. Spectral methods compute the graph Laplacian for filtering, while non-spectral methods directly perform convolution in the graph domain. GraphSAGE learns a convolution kernel in an inductive manner. All existing GNN models have discrete layers. The proposed GODE extends NODE to model message propagation on graphs as an ODE, allowing for adaptive evaluation and continuous invertible models. In this work, the authors propose a memory-efficient framework for accurate gradient estimation in free-form continuous invertible models. They show that the inferior performance of NODEs compared to discrete-layer models in image classification tasks is due to errors in gradient estimation during training. The proposed framework improves the performance of NODE and GODE models in benchmark tasks by accurately estimating gradients. The authors propose a memory-efficient framework for accurate gradient estimation in free-form continuous invertible models, improving performance on benchmark classification tasks. They generalize ODE to graph data with GODE models and demonstrate improved performance on various datasets. Previous efforts have viewed neural networks as differential equations, with NODE treating the network as a continuous ODE. The authors proposed a stable architecture based on ODE analysis and introduced neural ordinary differential equation (NODE) for generative models. Various studies have focused on training NODE, with recent advancements like augmented neural ODEs. However, gradient estimation issues persist, leading to inferior performance compared to discrete-layer models in classification tasks. GNNs are categorized into spectral and non-spectral methods, with spectral GNNs operating in the Fourier domain of a graph. Spectral and non-spectral methods are two categories of GNNs. Spectral GNNs operate in the Fourier domain of a graph, requiring information of the whole graph for filtering. Non-spectral GNNs only consider message aggregation around neighbor nodes, leading to localized and computationally efficient models. Various spectral methods have been introduced to address the computation burden and improve filter localization. Defferrard et al. (2016) and Kipf & Welling (2016) introduced fast localized spectral filtering and first-order graph convolution, respectively, for efficient graph processing. Other methods like MoNet, GraphSAGE, graph attention networks, and GIN have also been proposed to improve performance in graph-related tasks. Invertible blocks in neural networks, used in normalizing flow, ensure bijective mapping for accurate reconstruction. They have been applied in building invertible networks for memory-efficient backpropagation without storing activation. Invertible blocks in neural networks allow for memory-efficient backpropagation by discarding activation of middle layers, as each layer's activation can be reconstructed from the next layer. This approach is used in discrete-layer models with residual connections, leading to neural ordinary differential equations. The forward pass of a model with discrete layers involves applying an output layer after multiple layers, while a NODE forward pass integrates states over time and applies an output layer at the end. Integration can be done using various ODE solvers like Euler Method or Runge-Kutta Method. The adjoint method, used in optimal process control and functional analysis, involves model parameters denoted as \u03b8 and the adjoint defined as the hidden state solved in reverse-time. The reverse-time solution can be numerically unstable, causing errors in gradient calculations. The reverse-time solution can be numerically unstable, causing errors in gradient calculations. Direct back-propagation through ODE solver involves saving evaluation time points during forward pass and re-building the computation graph during backward pass. This allows for accurate reconstruction of the hidden state and evaluation of the gradient. Gradient descent is then performed to optimize \u03b8 to minimize the loss function. Eq. 6 represents a reverse-time integration that can be solved with any ODE solver, requiring determination of z(t) by solving Eq. 2 in reverse-time. Storing z(t) during forward pass would lead to large memory consumption. In the forward pass, Eq. 2 is solved forward in time, while in the backward pass, Eq. 2 and 6 are solved reverse in time. The reverse-time ODE solver can lead to inaccurate gradients in adjoint methods due to the mismatch between hidden states solved forward-time and reverse-time. This error in gradient calculation can impact the optimization process. The reverse-time ODE error in gradient calculation can impact optimization. Proposition 1 states that unstable ODEs have eigenvalues with non-zero real parts, affecting accuracy. Large real parts make ODEs sensitive to numerical errors, impacting solution accuracy and gradient computation. The adjoint method may be affected by numerical errors in reverse-time ODE solving. The accuracy of the computed gradient can be affected by numerical errors in reverse-time ODE solving. To address this, direct back-propagation through the ODE solver is proposed, ensuring accurate hidden states at evaluated time points. This can be achieved by saving activation values or reconstructing them at specific time points, guaranteeing accurate back-propagation regardless of the stability of the equations. Algorithm 1 outlines a method for accurate gradient estimation in ODE solvers for free-form functions. It involves defining a model with a free-form function, selecting an initial step size, and iteratively updating the state while checking for error tolerance. The algorithm ensures accurate back-propagation through the ODE solver, regardless of equation stability. The algorithm outlined in Algorithm 1 focuses on accurate gradient estimation in ODE solvers for free-form functions. It involves adaptive stepsize integration during the forward pass and rebuilding the computation graph during the backward pass for efficient memory usage. During backward pass, the solver rebuilds the computation graph by directly evaluating saved time points without adaptive searching. Our method reduces memory consumption by deleting all middle activations during the forward pass and not searching for an optimal stepsize in the backward pass. During the backward pass, the solver reduces memory consumption by using a step-wise checkpoint method and handling invertible blocks, allowing for more memory-efficient computation. During the backward pass, memory consumption is reduced using a step-wise checkpoint method and invertible blocks. Bijective blocks are introduced, allowing for accurate reconstruction of x from y without storing activations, making it memory-efficient. Differentiable neural networks F and G are used with a bijective function \u03c8 for various tasks. Theorem 1 states that the block defined is a bijective mapping. Graph neural networks with discrete layers are introduced, extending to graph ordinary differential equations (GODE) in the continuous case. Graph neural networks are introduced with discrete layers and extended to the continuous case with graph ordinary differential equations (GODE). GNNs are represented with nodes and edges, where message passing is used for information exchange between nodes. Differentiable functions parameterized by neural networks are utilized in a 3-stage model for each node in the graph. A GNN can be seen as a 3-stage model for a node u: message passing, where neighbors send information to u, message aggregation, where u combines messages from neighbors, and update, where node states are adjusted. GODE converts discrete-time GNN to continuous-time by using a message passing process, depicted in a diagram. The process converts discrete-time GNN to continuous-time using graph ordinary differential equation (GODE), which captures highly non-linear functions and can outperform discrete-layer models. The asymptotic stability of GODE is related to over-smoothing phenomena, and graph convolution is a special case of Laplacian smoothing. The continuous smoothing process of the symmetrically normalized Laplacian results in asymptotically stable trajectories, leading to similar features among nodes over time. Experiments with a CNN-NODE on image classification tasks and benchmark graph datasets show the impact of integration time on classification accuracy. The study evaluated a CNN-NODE method on various benchmark graph datasets and image classification tasks without pre-processing the raw data. The NODE model was derived from modifying a ResNet18 for image classification tasks. The function for each block consisted of conv-bn-relu layers, similar to the residual branch in ResNet. GODE can be applied to any graph neural network by replacing f with corresponding structures or F, G with other structures. GODE was tested on various GNN architectures including GCN, GAT, ChebNet, and GIN with different depths of layers for a fair comparison. For a fair comparison, different graph neural network models were trained with varying depths of layers. The models used the same hyper-parameters such as channel numbers. Channel numbers were set to 32 for hidden layers in graph classification tasks and 16 for node classification tasks. Different number of hidden layers (1, 2, 3) were experimented with, and the mean and variance of accuracy were calculated. The adjoint method and direct back-propagation were compared. Direct back-propagation outperformed the adjoint method in experiments with different numbers of hidden layers. Modifications were made to a ResNet18 for image classification tasks, resulting in NODE18 with improved performance. Additionally, a GODE model with a GCN was trained for graph networks, showing consistent superiority of direct back-propagation over the adjoint method. Our training method significantly improves the performance of NODE18 on CIFAR10 and CIFAR100 datasets, outperforming deeper networks like ResNet101. It also outperforms the adjoint method on various benchmark graph datasets. The method shows robustness to different orders of ODE solvers and supports NODE and GODE models with free-form functions. Our method demonstrates robustness to different orders of ODE solvers and supports NODE and GODE models with free-form functions. Bijective blocks defined in Eq. 8 can be easily generalized, with F and G as general neural networks. Different \u03c8 functions showed similar behavior on node classification tasks, validating the effectiveness of GODE models. The effectiveness of GODE models was validated as they outperformed their discrete-layer counterparts significantly. Different \u03c8 functions behaved similarly on node classification tasks, highlighting the importance of the continuous-time model. Lower memory cost was also confirmed. Results for various models on graph classification tasks were summarized in Table 4, showing that GODE models performed significantly better in most experiments. Integration time influence was tested for NODE and GODE models during inference, with results summarized in Table 5. The continuous process model is important for graph models, with integration time influencing inference results. Short integration time leads to lack of information gathering, while long integration time causes oversmoothing issues. GODE enables continuous diffusion process modeling on graphs with a memory-efficient back-propagation method for gradient determination. The paper addresses gradient estimation for NODE and improves accuracy on benchmark tasks. The over-smoothing of GNN is related to the asymptotic stability of ODE. Our paper focuses on improving gradient estimation for NODE, achieving accuracy comparable to state-of-the-art discrete layer models. We conduct experiments on various datasets including citation networks, social networks, and bioinformatics datasets. The structure of invertible blocks is explained with important modifications for bijective blocks. We propose a parameter state checkpoint method for bijective blocks, allowing for accurate inversion. The algorithm is summarized in Algo. 2, with pseudo code for forward and backward functions in PyTorch. Our bijective block is memory efficient, demonstrated by training a GODE model. Our bijective block is memory efficient, as demonstrated by training a GODE model and comparing memory consumption with a conventional method. Results show that our method only slightly increases memory usage with depth, while the conventional method shows a significant increase. Our memory-efficient bijective block only slightly increases memory consumption compared to conventional methods when training a GODE model. The block takes O(1) memory by storing outputs in cache and deleting activations of middle layers. The increased memory usage is minimal as states of F and G need to be cached, but this step consumes minimal memory compared to input data. The bijective block in GODE model increases memory consumption slightly but is memory-efficient. It stores outputs in cache and deletes activations of middle layers, requiring minimal memory compared to input data. The block ensures stability by having non-positive real part eigenvalues for both forward and reverse mappings. The forward mapping is proven to be bijective by showing injectivity and surjectivity. The bijective block in GODE model ensures stability with non-positive real part eigenvalues for both forward and reverse mappings. The forward mapping is proven to be bijective by showing injectivity and surjectivity. The computation graph is used to derive gradients for the loss function. The gradient of parameters in a neural-ODE model is derived from an optimization perspective, extending from continuous to discrete cases. Notations include hidden states, parameters, input, target, predicted output, and loss function. The continuous model follows an ODE with a differentiable function represented by a neural network. The training process is formulated as an optimization problem. The forward pass and loss function in a neural network are defined. The training process is formulated as an optimization problem using the Lagrangian Multiplier Method. The Karush-Kuhn-Tucker (KKT) conditions are necessary for an optimal solution. The derivative with respect to \u03bb is derived at the optimal point. The derivative with respect to \u03bb is derived at the optimal point using calculus of variation. The continuous perturbation of \u03bb(t) is considered, leading to the condition dz(t)/dt - f(z(t), t, \u03b8) = 0 for all t in (0, T). In the discrete case, the ODE condition is transformed into a finite sum representation, corresponding to previous analyses."
}