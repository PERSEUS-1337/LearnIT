{
    "title": "H1xQVn09FX",
    "content": "Efficient audio synthesis is a challenging machine learning task due to human perception sensitivity to global structure and waveform coherence. GANs can generate high-fidelity and locally-coherent audio by modeling log magnitudes and instantaneous frequencies with sufficient resolution. Extensive empirical investigations on the NSynth dataset show that GANs outperform WaveNet baselines in generating audio faster and more efficiently. Neural audio synthesis is a challenging task that requires modeling temporal scales over a wide range. Autoregressive models like WaveNet focus on fine scales but have slow sampling speeds. Research has been done to speed up generation, but methods introduce overhead. Large models like GANs have been successful in generating high-resolution images efficiently. However, adapting these architectures for audio synthesis has not yet achieved the same level of fidelity due to memory constraints and the need for local latent structure modeling. Attempts to adapt image GAN architectures for waveform generation have not reached the same level of fidelity as with images. Frame-based techniques struggle with phase alignment due to differences in periodicity and output stride, making it challenging to preserve phase coherence. Filters in STFT must cover all frequencies and phase alignments for phase coherence. Instantaneous radial frequency can be derived from unwrapping phase and taking its derivative. GAN progress in image modeling starts with focused datasets like CelebA for realistic texture generation. Later models generalize to broader domains. The NSynth dataset was introduced for audio, containing individual notes from musical instruments with aligned and cropped data for fine-scale details. Various models have been explored for conditional generation, including autoregressive WaveNet autoencoders and bottleneck spectrogram autoencoders. This work introduces adversarial training and explores effective representations for noncausal convolutional generation in audio waveforms, focusing on maintaining the regularity of periodic signals over short to intermediate timescales. Maintaining the regularity of periodic signals over short to intermediate timescales is crucial due to discontinuities and irregularities in waveforms. Phase precession occurs when the stride of frames does not align with a waveform's periodicity, posing a challenge for synthesis networks to learn frequency and phase combinations. This phenomenon is similar to the short-time Fourier transform and occurs when filterbanks overlap. Phase precession occurs when filterbanks overlap, causing a precessing phase to grow linearly. The derivative of the unwrapped phase with respect to time equals the angular difference between the frame stride and signal periodicity, known as the instantaneous frequency (IF). This phenomenon is crucial for maintaining the regularity of periodic signals over short to intermediate timescales. In this paper, the interplay of architecture and representation in synthesizing coherent audio with GANs is investigated. Generating log-magnitude spectrograms and phases directly with GANs can produce more coherent waveforms than directly generating waveforms with strided convolutions. Estimating IF spectra leads to more coherent audio than estimating phase, and it is important to prevent harmonics from overlapping by increasing the STFT frame size and switching to mel frequency scale. In this study, GANs show superior performance on the NSynth dataset, generating examples much faster than WaveNet. Global conditioning on latent and pitch vectors allows for smooth timbre interpolation and consistent timbral identity across pitch. The NSynth dataset contains 300,000 musical notes from 1,000 instruments, with labels for pitch, velocity, instrument, and acoustic qualities. Each sample is four seconds long and sampled at 16kHz. The NSynth dataset contains 300,000 musical notes from 1,000 instruments, with labels for pitch, velocity, instrument, and acoustic qualities. Each sample is four seconds long, sampled at 16kHz, and has 64,000 dimensions. The study focused on training with acoustic instruments and fundamental pitches ranging from MIDI 24-84. A new 80/20 test/train split was created from shuffled data to improve the training process for generating audio spectra using GANs. The study adapts progressive training methods from BID16 to generate audio spectra using GANs. The model samples a random vector and uses transposed convolutions to upsample and generate output data. Gradient penalty and pixel normalization are used for training. Both progressive and nonprogressive training variants show comparable quality, with slightly better convergence time and sample diversity seen in progressive training. Our method involves conditioning on an additional source of information by appending a one-hot representation of musical pitch to the latent vector. An auxiliary classification BID24 loss is added to the discriminator to predict the pitch label. STFT magnitudes and phase angles are computed using TensorFlow's built-in implementation with specific parameters. TensorFlow's built-in implementation is used to compute STFT magnitudes and phase angles with specific parameters. The resulting \"image\" has a size of (256, 512, 2) with magnitude and phase in two channels. Magnitudes are log-scaled and normalized to -1 to 1, while phase angles are also scaled to match the tanh output nonlinearity. Optionally, the phase angle can be unwrapped and finite differences taken to create \"instantaneous frequency\" models. Increasing the STFT frame size and stride doubles the spectral image size to (128, 1024, 2) for better frequency resolution at lower frequencies. To improve frequency resolution, the STFT frame size and stride are doubled, resulting in spectral images with size (128, 1024, 2). Log magnitudes and instantaneous frequencies are transformed to a mel frequency scale without compression, known as \"IF-Mel\" variants. WaveGAN is adapted for waveform generation with pitch conditioning. Our own GAN models achieve similar results. WaveNet is the state of the art in generative audio modeling, using WaveNet autoencoder to interpolate between sounds. Strong WaveNet baselines are created by adapting the architecture to accept pitch conditioning signals. The 8-bit model outperforms the 16-bit model in stability and performance. The 8-bit mu law model outperforms the 16-bit mixture of logistics model in stability and performance. Evaluation of generative models for audio generation is challenging due to the subjective nature of audio quality. Human evaluation is used as the gold standard for assessing audio quality. The study used Amazon Mechanical Turk to compare models in audio generation. Participants evaluated audio quality on a Likert scale. 3600 ratings were collected, with each model involved in 800 comparisons. The Number of Statistically-Different Bins (NDB) metric was used to measure diversity in generated examples. The study clustered generated examples into 50 Voronoi cells using k-means in log-spectrogram space. Inception Score (IS) is a metric for evaluating GANs based on the mean KL divergence between imageconditional output class probabilities and the marginal distribution. The Inception Score (IS) metric evaluates GANs based on class probabilities. Pitch Accuracy (PA) and Pitch Entropy (PE) measure distinct pitches in models. Fr\u00e9chet Inception Distance (FID) uses 2-Wasserstein distance for GAN evaluation based on Gaussian distributions. The study evaluates audio quality and diversity using a pretrained Inception classifier. Quality decreases as output representations move from IF-Mel to Waveform. Human evaluation shows IF-Mel model is comparable to real data. WaveNet baseline occasionally breaks down, resulting in a score comparable to IF GANs. Sample diversity correlates with audio quality, as seen in NDB score improvement with high frequency resolution. The NDB score improves with high frequency resolution, correlating with sample diversity and audio quality. WaveNet baseline receives the worst NDB score due to lack of diversity in oscillation types. FID scores also reflect sample quality, with IF models performing better with high frequency resolution. Phase models have high FID scores, indicating poor sample quality. Mel scaling has less effect on FID compared to listener study. Phase models show high FID due to poor sample quality. Models perform well on classifier metrics like IS, Pitch Accuracy, and Pitch Entropy. High-resolution models generate examples similar to real data. Discriminative information on sample quality from high scores is limited due to distribution differences. Metrics indicate low frequency models and baselines have less reliable pitch generation. The WaveGAN and PhaseGAN models show phase irregularities in waveform, while IFGAN is more coherent. Real data and IF models have strong consistent colors in Rainbowgrams. Listen to audio examples for a better understanding. The Rainbowgrams show that real data and IF models have coherent waveforms with strong consistent colors for each harmonic, while PhaseGAN has phase discontinuities and WaveGAN is irregular. The visualization helps to see clear phase coherence in the harmonics of real data and IFGAN. The visualization highlights the phase coherence of harmonics in real data and IFGAN, with consistent colors. In contrast, PhaseGAN shows discontinuities as speckled noise, and WaveGAN appears largely incoherent. GANs allow conditioning on the same latent vector for the entire sequence, unlike WaveNet autoencoders which have limited scope and structure that must be modeled. Interpolating between examples in raw waveform, distributed latent code, and global code of IF-Mel GAN is explored using a pretrained WaveNet autoencoder. Interpolating waveforms involves mixing amplitudes of sounds. WaveNet improves by mixing in timbre space, but linear interpolation doesn't match latents' prior, leading to oscillations. GAN model with gaussian prior allows for global interpolation. The IF-Mel GAN has global conditioning for smooth interpolation in perceptual attributes, producing high-fidelity audio examples. Linear interpolation in WaveNet lacks a compact prior, leading to less realistic sounds with feedback harmonics. Spherical interpolation stays aligned with the prior, resulting in smooth perceptual changes during interpolation. In the audio examples, timbre is interpolated between random points in latent space using pitches from Bach's Suite No. 1 in G major. The timbre smoothly morphs between instruments while pitches follow the composed piece. The GAN maintains its timbral identity across different pitches, creating a unique instrument identity. The IF-Mel GAN allows for parallel processing of training and generation, significantly reducing latency compared to WaveNet. This opens up the possibility for real-time neural network audio synthesis on devices, making it much faster and more efficient. The IF-Mel GAN enables real-time neural network audio synthesis on devices, offering a broader range of expressive sounds compared to traditional methods. This approach differs from speech synthesis models that focus on handling variable length conditioning and often rely on recurrent or autoregressive models. In comparison to speech, music audio generation is relatively under-explored, with autoregressive models showing potential for synthesizing musical instrument sounds. The text discusses the use of GANs for audio generation, focusing on improving training stability and generation quality. It also mentions the NSynth dataset and the use of WaveNet autoencoders for audio synthesis. The NSynth dataset was used with WaveNet autoencoders for timbre interpolation, but with slow sampling speeds. BID23 improved this with an adversarial domain confusion loss for timbre transformations across audio sources. BID5 achieved faster sampling speeds by training a frame-based regression model for pitch and instrument labels. Their architecture lacks phase coherency and handles multimodal distributions poorly. The architecture for audio generation with GANs lacks phase coherency and struggles with multimodal distributions, requiring a large number of channels. However, by controlling the audio representation, high-quality audio generation on the NSynth dataset has been achieved, surpassing WaveNet in fidelity and speed. Further research is needed to validate and expand these results to other types of natural sound, with potential applications in domain transfer and adversarial losses in audio. Issues of mode collapse and diversity common to GANs also exist for audio generation. Further research is needed to explore combining adversarial losses with encoders or regression losses to capture the full data distribution effectively. Training models with different learning rates and weights for the auxiliary classifier loss showed that a learning rate of 8e-4 and classifier loss of 10 performed the best across all variants. The architecture for audio generation with GANs includes box upscaling/downscaling and pixel normalization in the generators. The generators in GANs use box upscaling/downscaling and pixel normalization. The discriminator appends the standard deviation of minibatch activations as a scalar channel. Real data is normalized before passing to the discriminator. Training involves using a Tanh output nonlinearity for the generator and shifting and scaling log-magnitudes and phases. Training each GAN variant takes 4.5 days on a single V100 GPU with a batch size of 8. For nonprogressive models, training is on approximately 5M examples, while progressive models train on 1.6M examples per stage (7 stages). The training process involves using a single V100 GPU with a batch size of 8. For nonprogressive models, training is on approximately 5M examples, while progressive models train on 1.6M examples per stage (7 stages). The WaveNet baseline utilizes a Tensorflow implementation with a decoder composed of 30 layers of dilated convolution. The conditioning stack operates on a one-hot pitch conditioning signal distributed in time. The conditioning stack in the encoder operates on a one-hot pitch conditioning signal distributed in time. It consists of 5 layers of dilated convolution, increasing to 25, and then 3 layers of regular convolution, all with 512 channels. The conditioning signal is passed through a 1x1 convolution for each layer of the decoder and added to the output of each layer. WaveNets converged to 150k iterations in 2 days with 32 V100 GPUs trained with synchronous SGD with batch size 1 per GPU."
}