{
    "title": "rygVV205KQ",
    "content": "In this work, imitation learning is used to tackle challenges in high-dimensional sparse reward tasks for reinforcement learning agents. Adversarial imitation is shown to be effective in learning a representation of the world from pixels and exploring efficiently with rare reward signals. The adversary, acting as the reward function, can be small with just 128 parameters and trained using a basic GAN formulation. This approach overcomes limitations of other imitation methods, requiring only video demonstrations and sparse rewards to solve complex tasks like robot manipulation. The proposed agent can solve a challenging robot manipulation task of block stacking from video demonstrations and sparse reward, outperforming non-imitating agents and learning faster than competing approaches. A new adversarial goal recognizer allows the agent to learn stacking purely from imitation in some cases. GAIL can handle high-dimensional pixel observations with the right features and a single layer discriminator network. In this work, GAIL can handle high-dimensional pixel observations with the right features and a single layer discriminator network. The efficiency can be improved by using a D4PG agent with a replay buffer. Various types of features like self-supervised embeddings, random projections, and value network features can be successfully used with a tiny, single-layer adversary. Additionally, GAIL can be modified for off-policy D4PG agents with experience replay. The paper introduces a 6-DoF Jaco robot arm agent that learns block stacking from demonstration videos and sparse reward. It achieves a 94% success rate on a simulated Jaco arm, outperforming previous approaches using hand-crafted rewards. The 6-DoF Jaco robot arm agent learns block stacking from demonstration videos and sparse reward, achieving a 94% success rate on a simulated Jaco arm. An adversary-based early termination method improves task performance and learning speed. The agent also learns with no task reward using an auxiliary goal recognizer adversary, achieving 55% stacking success from video imitation only. Ablation experiments on Jaco stacking and a 2D planar walker benchmark are conducted to understand the reasons for improvement in the agent. The Markov Decision Process (MDP) is defined by states, actions, reward function, transition distribution, and discount factor. The goal is to find a policy that maximizes the expected sum of discounted rewards. DDPG is an actor-critic method using neural networks to represent the policy and action-value function. New transitions are added to a replay buffer by sampling from the policy. The policy network, represented by neural networks with parameters \u03c6, is trained via gradient descent to maximize the action-value function using deterministic policy gradient. DDPG agent is enhanced with subsequent improvements in D4PG. GAIL involves learning a reward function by training a discriminator network D(s, a) to distinguish. In GAIL, a reward function is learned by training a discriminator network to distinguish between agent and expert transitions. The objective is closely related to MaxEnt inverse reinforcement learning. A D4PG agent is used for off-policy training with experience replay. The reward function is jointly trained with actor and critic updates using a modified equation. The replay buffer changes the original expectation over the agent policy, requiring the discriminator to distinguish expert transitions from those produced by previous agents. The reward function in GAIL is learned by training a discriminator to distinguish between agent and expert transitions. The reward function interpolates imitation reward and a sparse task reward, using the sigmoid of the logits to bound the imitation reward between 0 and 1. This allows for intuitive values for early termination of episodes in the actor process. The actor process in GAIL includes early termination based on discriminator score to prevent drifting from expert trajectories. Multiple CPU actor processes run in parallel with a single GPU learner process. The discriminator network's capacity is a critical design choice for the reward function. The type of network used in the discriminator is a critical design choice in the agent. The discriminator's capacity must be balanced to distinguish between agent and expert without making it too easy or too difficult for the agent to fool. Expert demonstrations provide valuable data for feature learning due to their coverage of state space regions needed to solve the task. Access to expert actions is not assumed, ruling out behavior cloning for feature learning. Contrastive predictive coding (CPC) is a representation learning technique that maps observations into a latent space for long-term predictions. It uses a probabilistic contrastive loss with negative sampling, making it a suitable option for feature learning without expert actions or pixel prediction objectives. CPC utilizes a probabilistic contrastive loss for training the encoder and autoregressive model jointly without needing a decoder. One approach to removing task rewards is to replace them with a neural network goal recognizer trained on expert trajectories. However, this can lead to blind spots that allow the agent to exploit the system for imitation rewards. To address this issue, an alternative to sparse task rewards is suggested. To address the issue of imitation rewards in training, a secondary goal discriminator network can be used to replace sparse task rewards. This network detects if the agent reaches a goal state defined as the latter 1/M proportion of expert demonstrations. The modified reward function includes this discriminator, which operates on the same feature space as the primary discriminator. Training the goal discriminator involves sampling expert states from the latter 1/M portion of each demonstration trajectory. This approach enhances the performance of the agent beyond pure imitation learning methods like GAIL. By training a second discriminator to recognize goal states, an agent can surpass the demonstrator in reaching goals faster. Environments include a Kinova Jaco arm with 9 degrees of freedom and hand-crafted reward functions. Demonstrations are collected using a SpaceNavigator 3D motion controller. The hand-crafted reward functions for collecting demonstrations using a SpaceNavigator 3D motion controller. Human operators controlled the Jaco arm to gather 500 episodes of demonstrations for each task. Another 500 trajectories were gathered for validation, and a dataset of 30 \"non-expert\" trajectories was collected for diagnostics. Additionally, a 2D walker environment was used to collect demonstrations for training a D4PG agent. The agents used 64x64 pixel observations of 200 expert demonstrations. Trained a D4PG agent from proprioceptive states to match a target velocity using 64x64 pixel observations of expert demonstrations. Compared the imitation method to D4PG and GAIL agents, showing favorable results. CPC models future observations well for expert sequences but not non-expert. Conditioning on k-step predictions improves performance on stacking tasks when using CPC embeddings. Our method shows superior performance with sparse rewards compared to D4PG with sparse rewards struggling due to exploration complexity. The agent using value network features learns quickly, outperforming CPC features. GAIL from pixels performs poorly, while GAIL with tiny adversaries on random projections has limited success. The discriminator network has 128 parameters with CPC features and 2048 with value network features. The discriminator network has 128 parameters with CPC features and 2048 with value network features. GAIL value features worked due to norm clipping in the critic optimizer. Using CPC temporal predictions as input did not improve performance in Jaco or Walker2D environments. The expert state visualizes CPC features learned from training trajectories. Jaco stacking ablation experiments show that adding layers to the discriminator network does not improve performance. The optimal choice for imitation learning is determined by comparing a tiny discriminator to a deeper network on features. The effect of the number of layers on the discriminator network is explored in relation to imitation learning. Early termination criterion improves learning by stopping episodes when the discriminator score is low. The discriminator's ability to distinguish expert from agent trajectories affects episode length during training. The study evaluates the data efficiency of the proposed method in terms of expert demonstrations. Results show that even with 60 demonstrations, good performance can be achieved. In the planar walker experiment, the conventional GAIL methods did not learn, but the proposed method using value network features and random projections successfully learned to run. Videos of the trained agent are available in the supplementary materials. The proposed method using value network features and random projections successfully learned to run without any task rewards. Two out of five runs achieved a success rate of 55%, with the agent demonstrating efficient stacking and exploitation techniques. Videos of the trained agent are available in the supplementary materials. The curr_chunk discusses the use of expert demonstrations to improve agent performance in robotics, specifically in the context of Q-learning for cart-pole swing up tasks. The task differs from previous work as it only relies on pixel observations, not states and actions. The text also mentions the extension of deep learning into interactive environments and the attractiveness of imitation learning. The curr_chunk discusses extending deep learning into interactive environments through imitation learning, specifically focusing on one-shot imitation using expert demonstrations to replicate behaviors in new problem instances. This approach has been successful in tasks such as stacking blocks with a simulated Fetch robot. Another method mentioned involves gradient-based meta learning for one-shot learning of observed behaviors. The curr_chunk introduces a new approach for one-shot learning of observed behaviors using a gradient-based meta learning approach. Unlike traditional methods like behavioral cloning, this approach focuses on the agent learning through interaction with the environment rather than supervised learning. It aims to overcome limitations such as cascading failures in different states and the need for a large number of demonstrations. The curr_chunk discusses various approaches like inverse reinforcement learning (IRL), deep Q-Learning from demonstration (DQfD), and deterministic policy gradients from demonstration (DPGfD) for training agents using expert demonstrations. These methods aim to optimize learned rewards and improve agent training without relying solely on expert actions. Generative Adversarial Networks (GANs) have been successful in image generation, and GAIL applies adversarial learning to imitation tasks. Despite various adaptations to handle high-dimensional input spaces and sparse rewards, challenges remain. A new approach using minimal adversaries on learned features shows promise in solving sparse reward tasks effectively. Another line of work focuses on learning compact representations for imitation learning without expert actions, bridging the domain gap. Both BID30 and BID3 use self-supervised features from third person observations to bridge the domain gap. They track a single expert trajectory, while we aim to generalize all possible initializations of a hard exploration task using GAIL. We utilize static and dynamic self-supervised features to train block stacking agents successfully from sparse rewards on pixels. The behavior cloning model, utilizing a residual network pixel encoder architecture followed by LSTM and linear layers, achieves a stacking accuracy of approximately 15%. Additionally, a visualization of CPC on video data is shown, consisting of an encoder and autoregressive model for latent representation mapping. The encoder maps observations to latent representations, while the autoregressive model summarizes past latents into a context vector. Both optimize the same loss function by maximizing mutual information between the context and target, resulting in compact representations. This is useful for extracting slow features, especially when the context and target are far apart in time. Our proposed approach involves model learning through contrastive predictive coding (CPC) and training an agent using CPC future predictions. Reward functions are modified from BID37, with dense staged rewards defined in five stages and sparse rewards in two stages. The actor and critic share a residual network with twenty convolutional layers. The actor and critic share a residual network with twenty convolutional layers. Both actor and critic use independent three-layer fully connected networks with exponential linear units. Distributional Q functions are adopted instead of a scalar state-action value function. The text chunk discusses the computation of a bootstrap target with N-step returns using fixed atoms bounded between V min and V max. A categorical projection is adopted to handle the different supports of the bootstrap target. The loss function for training distributional value functions is defined using cross entropy, and distributed prioritized experience replay is utilized for increased stability and learning efficiency."
}