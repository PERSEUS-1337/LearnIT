{
    "title": "HyPpD0g0Z",
    "content": "When training a deep neural network for supervised image classification, latent features can be broadly divided into \"core\" features that are conditionally invariant across domains and \"style\" features that can vary across domains. It is important to focus on using the conditionally invariant features to guard against future adversarial domain shifts. The domain itself is considered a latent variable in this approach. When training deep neural networks for image classification, focus on using \"conditionally invariant\" features to guard against domain shifts. The domain is treated as a latent variable, making it impossible to directly observe feature distribution changes across domains. Data augmentation involves generating multiple images from an original image, with an ID variable indicating the relevant original image. Only a small fraction of images need to have an ID variable. In a causal framework, the ID variable is added to the model to treat the domain as a latent variable. Samples with the same class and identifier are considered counterfactuals under different style interventions. Regularizing the network using a graph Laplacian improves performance in settings with changing domains. This approach links to interpretability, fairness, and transfer learning in deep neural networks. Deep neural networks have excelled in prediction tasks like visual object and speech recognition, but issues can arise when learned representations rely on dependencies that vanish in test distributions. Domain shifts caused by changing conditions can lead to degraded predictive performance, as seen in the \"Russian tank legend\" example where sampling biases in training data were not replicated in the real world. The accuracy of a machine learning system trained to distinguish between Russian and American tanks from photos was high, but only because images of Russian tanks were of poor quality. This hidden confounding factor highlights the need for large sample sizes in deep learning to account for indirect associations and ensure invariance to known factors like translation and rotation through data augmentation. The need for a large sample size in deep learning is emphasized to achieve invariance to factors like translation, rotation, and point of view through data augmentation. Adversarial examples, which are intentionally perturbed inputs misclassified by ML models, do not fool humans. The question is raised on mimicking human ability to learn invariances from a few instances of the same object and aligning DNN features with human cognition. Fairness and discrimination considerations also play a role in controlling input data characteristics in learned representations. Existing biases in datasets used for training ML algorithms tend to be replicated in the estimated models, leading to issues like Google's photo app tagging non-white people as \"gorillas\" due to biased training examples. To address this, counterfactual regularization (CORE) is proposed to control latent features extracted from input data, taking a causal view of the data generating process. Counterfactual regularization (CORE) controls latent features extracted from input data by categorizing them into 'conditionally invariant' (core) and 'orthogonal' (style) features. The goal is for a classifier to use only core features related to the target of interest in a stable manner, making the estimator robust to adversarial domain shifts. CORE exploits knowledge about grouping instances related to the same object, rather than pooling over all examples. Counterfactual regularization (CORE) utilizes knowledge about grouping instances related to the same object to control latent features. The manuscript discusses how CORE can improve predictive performance in small sample size settings and reduce the need for data augmentation. It introduces counterfactual regularization in logistic regression and evaluates its performance in various experiments using the CelebA dataset. The task involves classifying whether a person wears glasses, with the constraint that all images of the same person must yield the same prediction. Counterfactual regularization (CORE) uses grouping information to ensure consistent predictions for images of the same person. The approach involves including 10 identities in the training set, resulting in a total sample size of 321 images. Examples from the CelebA and MNIST datasets demonstrate the grouping-by-ID method. Counterfactual regularization (CORE) utilizes grouping information to improve predictions for images of the same person. By exploiting the group structure, test errors are significantly reduced, with a 32% decrease in test error compared to pooling over all samples. Additionally, in data augmentation, CORE makes the process more efficient by creating additional samples through modifications like rotation, translation, or flipping of images. In data augmentation, additional samples are created by modifying original inputs, such as rotating, translating, or flipping images. This process results in invariance of the estimator with respect to the transformations of interest. Using grouping information in CORE enforces invariance more strongly compared to normal data augmentation. Rotation is assessed on MNIST with a total sample size of 10100, including 100 augmented training examples for 10000 original samples. Rotation degrees are randomly sampled from [35, 70]. Using CORE for data augmentation on MNIST, rotation degrees are randomly sampled from [35, 70]. The average test error on rotated examples is reduced from 32.86% to 16.33%. This approach differs from Domain-Adversarial Neural Networks (DANN) by not requiring unlabeled data from the target task. The adversarial training procedure in this model maximizes domain classification loss while minimizing target prediction task loss simultaneously. Unlike other models, it assumes different realizations of the same object under different interventions, with a focus on adjusting variables to minimize distribution differences between domains. The key difference lies in using a different data basis and having a latent domain identifier. In contrast to other models, our approach utilizes an identifier variable to penalize the classifier for using latent features. Causal modeling aims to ensure valid predictions even under large interventions on predictor variables. However, transferring these results to adversarial domain changes in image classification faces challenges due to the anti-causal nature of the classification task and the lack of direct causality between images and object classes. The challenge in image classification lies in the anti-causal nature of the task, where the predictor is a descendant of the true class. Standard causal inference methods struggle to guard against domain shifts. Various approaches leverage causal motivations for deep learning, but do not consider anti-causal prediction or non-ancestral interventions on style variables. The Neural Causation Coefficient (NCC) is used to estimate causal relations between image features by distinguishing object features from context features. Generative neural networks are employed for cause-effect inference and identifying v-structures in graphs. Bahadori et al. (2017) introduce a regularizer combining a penalty with weights based on the probability of a feature being causal. Bahadori et al. (2017) propose a regularizer combining an 1 penalty with weights based on the estimated probability of features being causal. Besserve et al. (2017) connect GANs and causal generative models using a group theoretic framework. Kocaoglu et al. (2017) suggest causal implicit generative models for sampling from conditional and interventional distributions. BID29 use deep latent variable models to estimate individual treatment effects, while BID21 use causal reasoning to address fairness in machine learning by distinguishing between protected attributes and proxies. BID21 uses causal reasoning to address fairness in machine learning by deriving causal nondiscrimination criteria, requiring classifiers to be constant with proxy variables. Distinguishing between core and style features is akin to disentangling factors of variation, which has garnered interest in generative modeling. Matsuo et al. (2017) propose a \"Transform Invariant Autoencoder\" to reduce dependence of latent representation on specified object transforms in images. The goal is to learn a latent representation that excludes specified object transforms in images, such as location, image quality, posture, brightness, background, and contextual information. Matsuo et al. (2017) introduce a confounding situation where style features differ based on class, with the domain being unobserved but the ID variable used for grouping is observable. In a variational autoencoder framework, Bouchacourt et al. (2017) aim to separate style and content by exploiting grouped observations. They assume samples within a group share a common but unknown value for one factor of variation while the style can differ. This approach differs from solving a classification task directly without estimating latent factors explicitly in a generative framework. The standard notation for classification is described before developing a causal graph to compare adversarial domain shifts to transfer learning, domain adaptation, and adversarial examples. The text discusses parameter estimation in machine learning, focusing on minimizing expected loss through penalized empirical risk minimization. It involves choosing weights or parameters to minimize the empirical loss based on training data samples. The text discusses parameter estimation in machine learning, focusing on penalized empirical risk minimization to minimize the empirical loss. The model includes a latent domain variable D and an ID variable that can change based on class Y. The prediction is anti-causal, with predictors X being non-ancestral to Y. The prediction is anti-causal, with predictors X being non-ancestral to Y. The causal effect from the class label Y on the image X is mediated via core features X ci and style features X \u22a5. Interventions are possible on style features but not on core features. The style features X \u22a5 and Y are confounded by the latent domain D, while the core features X ci are conditionally invariant. The core features are conditionally invariant and mediate the causal effect from class label Y to image X. Interventions can be made on style features but not on core features. The style features and class label are confounded by the latent domain, while the style intervention variable influences both latent style and image. The prediction under style intervention is of interest in guarding against adversarial domain shifts. In this work, the causal graph is used to guard against adversarial domain shifts by minimizing adversarial loss in classification. Interventions on image X can cause misclassification due to imperceptible changes, leading to the goal of devising a classification that minimizes adversarial loss. The intervention variable influences both latent style and image, with the aim of estimating the conditional distribution of Y given the image under intervention. The text discusses minimizing adversarial loss in classification using a causal graph to guard against domain shifts. It focuses on interventions on image X that can lead to misclassification and aims to estimate the conditional distribution of Y given the image under intervention. The term \"adversarial\" is used for interventions on style features, distinct from domain adversarial neural networks. The text discusses protecting against shifts in test data distribution by distinguishing between core and style features. It highlights the challenge of causal inference in observing counterfactuals, where the treatment effect on health outcomes cannot be directly observed. Observing counterfactuals is generally impossible in causal inference, as we can only observe outcomes under treatment or no treatment. However, in image analysis, counterfactuals are conceivable as we can see the same object under different conditions. The style intervention \u2206 plays a role similar to treatment T in medical examples. In image analysis, we can observe the same object under different conditions using style interventions like treatment in medical examples. The intervention \u2206 helps classify images by ruling out certain variables while preserving class and identity. The pooled estimator treats all examples identically by summing over the loss with a ridge penalty. The adversarial loss of the pooled estimator may be infinite. The pooled estimator with a penalty parameter works well in terms of adversarial loss by ensuring certain conditions are met, such as the absence of certain edges in the model. The text discusses minimizing adversarial loss by ensuring the constancy of a function in an invariant parameter space. The challenge lies in inferring the invariant space from data to approximate the optimal parameter vector using empirical risk minimization. To approximate the optimal invariant parameter vector, empirical risk minimization is used. The unknown invariant parameters space is approximated by an empirically invariant space, with a regularization constant allowing for variations in estimated predictions. The true invariant space is a subset of the empirically invariant subspace, converging as n approaches infinity. The text discusses using the Lagrangian form of constrained optimization with a penalty parameter \u03bb instead of a constraint \u03c4. The matrix L ID is a graph Laplacian where samples with the same ID are connected, forming fully connected components. The regularization penalizes the sum of variances and is induced by the identifier variable ID in the sample space. This approach differs from prior methods that form graphs based on feature similarities. The outcome is shown to be robust to the choice of penalty \u03bb. The text discusses forming a graph using knowledge to connect similar features, showing robustness to penalty choice \u03bb. Adversarial loss is analyzed for pooled and CORE estimators in logistic regression, with the CORE estimator having infinite adversarial loss under certain assumptions. The text discusses using logistic regression to predict class labels, with the CORE estimator showing convergence to optimal adversarial loss. Various experiments are conducted to assess CORE's performance in handling confounded data and changing style features. Additional experiments involve classifying elephants and horses based on color and other attributes. TensorFlow BID0 implementation of CORE will be provided for reproducibility. The text discusses logistic regression for predicting class labels, with the CORE estimator showing optimal adversarial loss convergence. Experiments assess CORE's performance with confounded data and style features. TensorFlow BID0 implementation will be available for reproducibility. The tuning parameter \u03c4 or penalty \u03bb in Lagrangian form is an open question, with performance not highly sensitive to \u03bb choice. Data involves synthetically generated stickmen images, with Y \u2208 {adult, child} and X ci \u2261 height as a core feature. Age and X \u22a5 \u2261 movement have a dependence due to hidden common cause D \u2261 place of observation. The dependence between age and movement in the training dataset is due to a hidden common cause, the place of observation. The learned model may fail when presented with images that do not follow the expected patterns. Test sets 2 and 3 intervene on the relationship between movement and age, causing the dependence to vanish. Movement patterns in the test sets differ from those in the training set, with larger movements associated with both children and adults. The movements in test sets 2 and 3 are heavier compared to the training set, with larger movements associated with both children and adults. CORE achieves good predictive performance with as few as 50 counterfactual observations, while the pooled estimator fails. Including more counterfactual examples would not improve the performance of the pooled estimator due to bias. In the CelebA dataset, the image quality differs based on whether the person is wearing glasses, mimicking confounding factors. Counterfactual observations are generated by sampling new image quality from a Gaussian distribution. The strength of the intervention is controlled by the percentage of the original image quality. Examples from the training set are shown in FIG0. In the CelebA dataset, counterfactual observations for images with glasses are generated by sampling new image quality values. The misclassification rates for CORE and the pooled estimator are compared on different test sets, where the quality intervention varies. The pooled estimator outperforms CORE on test set 1, attributed to the distribution of image qualities. The pooled estimator performs better than CORE on test set 1 by exploiting predictive information in image quality. However, it does not perform well on test sets 2-4 as it uses image quality as a predictor. In contrast, CORE's predictive performance is hardly affected by changing image quality distributions. Experimental details and results for quality interventions are provided in \u00a7C.5. In \"Elmer the elephant,\" a colored elephant can still be recognized as an elephant, showing invariance with respect to color. The study aims to see if CORE can exclude color from its representation by including counterfactual examples of different colors using the AwA2 dataset. Counterfactual examples are added for elephants, with a total sample size of 1850. The experiment compares CORE and the pooled estimator's misclassification rates on different test sets. The study compares CORE and the pooled estimator's performance on different test sets with color variations. Test sets include original colored images, grayscale images, and images with color shifts. The pooled estimator struggles with color variations, while CORE's performance remains stable. The study compares CORE and the pooled estimator's performance on test sets with color variations. CORE's predictive accuracy on test set 1 is improved by using information, while the pooled estimator's performance is hardly affected by changing color distributions. The CORE estimator demands invariance of predictions for instances of the same elephant and can learn color invariance with a few added grayscale images. If \"color\" was a protected attribute, CORE would not include it in its learned representation, satisfying fairness. CORE aims to achieve fairness by not including \"color\" in its learned representation, unlike the pooled estimator which uses color for decisions. Counterfactual regularization (CORE) distinguishes core and style features in images to achieve robustness against interventions on style features. By demanding invariance of the classifier among instances of the same object, CORE ensures classification performance remains invariant to adversarial interventions on style features like image quality, fashion type, color, or body posture. The training is effective despite sampling biases in the data, with applications in various areas. The training of CORE aims to achieve fairness by not including \"color\" in its learned representation, unlike the pooled estimator. It distinguishes core and style features in images to achieve robustness against interventions on style features like image quality, fashion type, or body posture. The regularization of CORE penalizes features that vary strongly between different instances of the same object, even when style features are unknown. Using larger models like Inception or large ResNet architectures could be explored for improved performance. CORE aims for fairness by excluding \"color\" in its representation, distinguishing core and style features in images to resist interventions on implicit style features. Future directions could involve using video data for grouping and counterfactual regularization, potentially aiding in debiasing word embeddings. Linear structural equations for images and logistic regression for class prediction are considered, with interventions acting on style features. The image X is linear in style features X \u22a5, with logistic regression predicting class label Y. Interventions act additively on X \u22a5, which linearly affects X via matrix W. Core features X ci are conditionally invariant. Y, X, and ID are observed, while D, X ci, \u2206, X \u22a5, and noise variables are latent. Logistic regression is used for predicting Y from image data X with estimated parameters \u03b8. The logistic regression predicts Y from image data X, with interventions on style variables X \u22a5. The first loss is standard logistic loss without interventions, while the second loss allows for adversarial interventions on X \u22a5. The benchmarks are based on these losses. The assumptions for Theorem 1 include sampling \u2206 from a distribution with positive density on an -ball in 2-norm. Assuming \u2206 is sampled from a distribution in R q with positive density on an -ball in 2-norm around the origin, matrix W has full rank q, and c \u2265 q, where c = m - n, the sampling process involves collecting n independent samples and selecting c = m - n counterfactual examples. The sampling process involves collecting n independent samples and selecting c = m - n counterfactual examples. The pooled estimator has infinite adversarial loss with probability 1 with respect to the training data. An equivalent result can be derived for misclassification loss instead of logistic loss. The pooled estimator has infinite adversarial loss with probability 1, constrained to be orthogonal to the column space of W. By contradiction, if W t\u03b8pool = 0, then \u03b8 pool = \u03b8 * and the directional derivative of the training loss with respect to any \u03b4 in the column space of W should vanish at the solution \u03b8 *. The interventions only affect the column space of W in X, making the oracle estimator \u03b8 * identical under the true conditions. The interventions only affect the column space of W in X, making the oracle estimator \u03b8 * identical under the true conditions. The derivative g(\u03b4) in FORMULA24 can be written as DISPLAYFORM10. The eigenvalues of W t W are all positive, and r i (\u03b8 * ) is not affected by the interventions \u2206 i,j. The interventions \u2206 i,j are drawn from a continuous distribution, leading to the left hand side of the equation having a continuous distribution. The probability of it not being identically 0 is 1, completing the proof by contradiction. With probability 1, \u03b8 core = \u03b8 * as the invariant space is the linear subspace where W t \u03b8 = 0. This is shown by the number of counterfactual examples exceeding the rank of W, leading to \u03b8 core = \u03b8 * with certainty. The number of counterfactual examples exceeds the rank of W, leading to \u03b8 core = \u03b8 * with certainty. This shows that the estimator remains unchanged when using data without interventions as training data. Comparing FORMULA3 and FORMULA3, we have c = m - n samples redrawn from the empirical sample at random, completing the proof. The CelebA dataset is used to classify gender based on images, creating confounding by including mostly men wearing glasses. Counterfactuals are used to address this, with examples shown in test sets with different gender-glasses associations. In the CelebA dataset, gender classification is confounded by including mostly men wearing glasses. Test set 1 follows the training set distribution, while test set 2 flips the gender-glasses association. The study compares training a four-layer CNN end-to-end versus using Inception V3 features and retraining the softmax layer. Results show similar trends with increasing c, where the performance difference between CORE and the pooled estimator decreases. The pooled estimator performs worse on test set 2 as m increases, suggesting it exploits the confounding variable X more as m grows. In the CelebA dataset, the study analyzes a confounded setting where the hidden common cause of Y and X \u22a5 is whether the image was taken outdoors or indoors, affecting the brightness and glasses-wearing classification. Test set 2 shows the pooled estimator performing worse as m increases, indicating a greater exploitation of the confounding variable X. The study compares the performance of the pooled estimator and CORE on different test sets. The pooled estimator outperforms CORE on test set 1 by utilizing brightness information. However, it struggles on test sets 2 and 4 due to differences in brightness distribution. CORE shows more consistent predictive performance across test sets. The study compares the predictive performance of the pooled estimator and CORE on different test sets. The pooled estimator relies on brightness information as a predictor, but struggles when brightness distribution varies between training and test sets. In contrast, CORE's performance is more consistent across different brightness distributions. Different counterfactual settings are explored, with setting 1 showing the best results in explicitly controlling brightness. In the study, different counterfactual settings are compared to control brightness variations. Setting 1 works best for this purpose. The value of tuning parameters in the model is also discussed, showing that performance is not highly sensitive to the penalty parameter. In the experiment, varying the number of identities in the training data set shows that CORE improves predictive performance compared to pooling all images, especially with small sample sizes. As the number of identities and sample sizes increase, the performance of CORE and the pooled estimator become comparable. This indicates that larger sample sizes reduce the impact of confounding factors in the training data. In the experiment, varying the number of identities in the training data set shows that CORE improves predictive performance compared to pooling all images, especially with small sample sizes. Larger sample sizes reduce the impact of confounding factors in the training data. Results show that CORE makes data augmentation more efficient, with lower misclassification rates on rotated digits. In the experiment introduced in \u00a75.1, results for different numbers of counterfactual examples are shown. The CORE estimator's performance is not sensitive to the number of counterfactual examples once there are enough in the training set. The pooled estimator fails to achieve good predictive performance on test sets 2 and 3. Counterfactual setting 1 works best, with small differences between settings 2 and 3. There is a large performance difference between \u00b5 = 40. The pooled estimator shows a large performance difference between \u00b5 = 40 and \u00b5 = 50, with \u00b5 = 50 possibly not being predictive enough for the target. The models were implemented in TensorFlow and trained five times to assess variance. In experiments using the Adam optimizer BID22, models were trained five times to evaluate variance. Training data was shuffled in each epoch to ensure mini batches contain counterfactual observations. Mini batch size was set to 120, making optimization more challenging for small c."
}