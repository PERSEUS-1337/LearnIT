{
    "title": "rygFWAEFwS",
    "content": "We propose SWAP, an algorithm to speed up DNN training using large mini-batches and weight averaging. The resulting models generalize well on CIFAR10, CIFAR100, and ImageNet datasets. SGD is commonly used for DNN training, but increasing mini-batch size can accelerate training. Increasing the mini-batch size can accelerate DNN training by providing more precise gradient estimates, allowing for higher learning rates and larger reductions in training loss. In a distributed setting, multiple nodes can compute gradient estimates simultaneously on disjoint subsets of the mini-batch and produce a consensus estimate through averaging. Training with larger mini-batches requires fewer updates and synchronization events, leading to good overall scaling behavior. However, there is a maximum batch size where the model's generalization performance may worsen. Stochastic Weight Averaging (SWA) is a method that produces models with good generalization performance by averaging the weights of models sampled from the final stages of training. By generating multiple independent SGD sequences and averaging models from each, similar generalization performance can be achieved. Instead of sampling multiple models from a sequence generated by SGD, SWAP generates multiple independent SGD sequences and averages models from each, achieving similar generalization performance. SWAP accelerates DNN training by better utilizing compute resources, producing good results with minor tuning. It achieves generalization performance comparable to models trained with small-batches in a time similar to training with large-batches. SWAP accelerates DNN training by efficiently utilizing compute resources, achieving comparable generalization performance to small-batch models in a time similar to large-batch training. It has been shown to substantially reduce training times for efficient models and outperform the state of the art on CIFAR10, training in 68% of the time of the winning entry in the DAWNBench competition. The impact of training batch size on generalization performance is still not fully understood, with theories suggesting that larger mini-batches may lead to getting stuck in sharper global minima. The authors discuss the sensitivity of minima to data variations and the impact of batch size on accuracy in training models. They present examples where flat minimizers can be transformed into sharp ones without affecting model behavior, and predict that increasing batch size up to a critical point does not decrease accuracy. In large-batch training, the mini-batch gradient approximates the full gradient, and increasing batch size may not significantly improve signal to noise ratio. Larger batch sizes imply fewer model updates, affecting weight initialization distance and generalization performance. Training with large batches for longer can improve generalization, but takes more time than small-batch training. Batch size also impacts the optimization process for convex functions. In large-batch training, batch size affects optimization process. For convex functions, a critical batch size is shown to be important. Adaptive batch size methods exist but may require extensive tuning. Local SGD is a distributed optimization algorithm that balances gradient precision with communication costs. Post-local SGD is a variant of Local SGD that refines models after large-batch training, resulting in better generalization. While Post-local SGD lets models diverge for a few iterations, SWAP averages models after multiple epochs, leading to differences in model updates. In our experiments, we compare SWAP and Post-local SGD in DNN optimization. SWAP involves stochastic weight averaging from later stages of training, resulting in better generalization. This method has been effective in various domains and is adapted to accelerate DNN training. SWAP is described in three phases in Algorithm 1. In this work, SWAP is adapted to accelerate DNN training in three phases. Phase 1 involves all workers training a single model with large mini-batch updates and synchronization. Phase 2 has workers refining their models independently with smaller batch sizes and no synchronization. The final phase averages the weights of resulting models to produce the final output. Early termination in Phase 1 prevents optimization from getting stuck. During phase 2 of SWAP, small-batch training is conducted independently by each worker, resulting in different models. The optimal stopping accuracy is a hyper-parameter that needs tuning. The batch size is reduced, and training is done with different random order sampling. During the small-batch phase of SWAP, workers train independently, leading to diverging models. The averaged model outperforms individual models, showing the effectiveness of the approach. The SWAP model is computed by averaging independent models and evaluating the test loss. To visualize the mechanism, orthogonal vectors u, v are chosen to plot the loss values on a plane containing the algorithm's phases. The training and testing errors for the CIFAR10 dataset are shown in Figure 2, where 'LB' represents phase one, 'SGD' represents a single worker after phase two, and 'SWAP' is the final output. The level-sets of the training error form an almost convex basin in the plotted plane. After phase two, the 'SWAP' model in Figure 2a shows the training error level-sets forming a convex basin. The 'LB' and 'SGD' outputs are on the basin's edges, with 'SWAP' closer to the center. The topology variations cause 'LB' and 'SGD' to have higher errors, while 'SWAP' is less affected. In Figure 3, 'SGD1', 'SGD2', and 'SGD3' workers are plotted on different sides. In Figure 3, worker points 'SGD1', 'SGD2', and 'SGD3' are plotted in different regions of the topology, with 'SWAP' closer to the center. The change in the worker points' positions leads to higher testing errors compared to 'SWAP'. The authors suggest that in later stages of SGD, weight iterates behave like an Ornstein Uhlenbeck process, reaching a stationary distribution similar to a high-dimensional Gaussian. The authors argue that weight iterates in later stages of SGD behave like a high-dimensional Gaussian distribution, with mass concentrated near the ellipsoid's surface. Sampling weights from different SGD runs can lead to weights spread out on the ellipsoid's surface, closer to the center. This justifies sampling from different runs as long as they start in the same basin of attraction. The authors argue that sampling weights from different SGD runs can lead to faster progress towards the center of the basin, as shown by the cosine similarity decreasing as training progresses. This justifies the advantage of SWA and SWAP over SGD. In this section, the performance of SWAP for image classification tasks on CIFAR10, CIFAR100, and ImageNet datasets is evaluated. The experiments involved finding the best hyper-parameters through grid searches, training with mini-batch SGD using Nesterov momentum and weight decay, data augmentation with cutout, and using a custom ResNet 9 model. Experiments were conducted on a machine with 8 NVIDIA Tesla V100 GPUs using Horovod for distributed computation. Statistics were collected over 10 runs with specific settings for SWAP phase one. For image classification tasks on CIFAR10, CIFAR100, and ImageNet datasets, experiments were conducted using SWAP. Different settings were used for SWAP phase one and phase two, with varying batch sizes and number of GPUs. The experiments compared models trained with small-batch only, large-batch only, and SWAP, showing test accuracies and training times. Test Table 2 compares the best test accuracies and training times for models trained with small-batches, large-batches, and SWAP. SWAP shows significant improvement in test accuracies after averaging the models, achieving accuracies on par with small batch training in a comparable time to large-batch training. SWAP also achieves state-of-the-art training speeds for CIFAR10. The DAWNBench competition leader trains CIFAR10 to 94% accuracy in 37 seconds with 4 Tesla V100 GPUs. Using SWAP with 8 Tesla V100 GPUs, the same accuracy is achieved in 27 seconds. SWAP accelerates ImageNet training with modified schedules on 8 Tesla V100 GPUs for small-batch experiments and 16 Tesla V100 GPUs for large-batch experiments. Experiments on 16 Tesla V100 GPUs involve doubling batch size and learning rates, impacting test accuracies. SWAP phase 1 uses large-batch settings for 22 epochs, while phase 2 runs on 8 GPUs for 6 epochs with small-batch settings. Doubling batch size reduces accuracies but SWAP improves generalization performance with faster training times. Results in Table 3 show accelerations without additional tuning. SWAP transitions between large-batch and small-batch settings, doubling batch size, GPUs, and learning rate. SWAP is compared with SWA using CIFAR100 dataset, sampling models with the same number of epochs. SWA averages models with 10 epochs in-between, while SWAP uses 8 independent workers for 10 epochs each. The study explores if SWA can recover test accuracy of small-batch training on a large-batch. The study compares SWAP and SWA on the CIFAR100 dataset. SWAP uses 8 workers for 10 epochs each, while SWA averages models with 10 epochs in-between. SWA was unable to improve the large-batch training accuracy. SWA is then evaluated using small-batches after a large-batch training run. In phase 1 of the CIFAR100 experiment, SWA achieves the same accuracy as small-batch training but takes three times longer to compute the model. The learning rate schedule is illustrated in Figure 6b. SWA starts with the best model from small-batch training and uses a fixed cycle length and count. SWAP then uses the same peak learning rate settings. Phase two starts with the model from phase 1, resulting in better accuracy for small-batch SWA compared to SWAP. In the experiment on CIFAR100, SWA achieves the same accuracy as small-batch training but takes longer. SWAP improves accuracy by relaxing constraints and using larger mini-batches. SWAP achieves a test accuracy of 79.11% in 241 seconds, 3.5x faster than SWA. Our algorithm utilizes large mini-batches for quick approximate solutions, refining them by averaging weights of models trained with small-batches. The final model shows good generalization performance and is trained faster. This approach is novel and effective, as observed in CIFAR10, CIFAR100, and ImageNet datasets. Our method involves using large mini-batches for quick approximate solutions and refining them by averaging weights of models trained with small-batches. The final model demonstrates good generalization performance and is trained faster, as seen in CIFAR10, CIFAR100, and ImageNet datasets. The transition point between large-batch and small-batch is a key hyperparameter that we chose through grid search, with future work focusing on a more principled method for selecting this point. In future work, the focus will be on choosing the transition point for the method. The design of SWAP allows for the substitution of optimization schemes like LARS, mixed-precision training, post-local SGD, or NovoGrad. Experiment parameters were obtained through independent grid searches, with momentum and weight decay constants kept constant. Tables list the remaining hyperparameters used in the experiments."
}