{
    "title": "ryvxcPeAb",
    "content": "Deep neural networks excel in various applications but are susceptible to adversarial examples, which are small perturbations that can deceive different models, allowing attackers to exploit black-box systems. In this work, the adversarial perturbation is decomposed into model-specific and data-dependent components, with the latter contributing mainly to transferability. Adversarial examples are crafted using noise reduced gradient (NRG) to approximate the data-dependent component, enhancing transferability significantly across various ImageNet classification models. Low-capacity models outperform high-capacity models in attack capability with comparable test performance. These findings offer a principled approach to constructing successful adversarial examples and insights for developing defense strategies against black-box attacks in the era of neural networks. Adversarial examples can transfer across different models, fooling them with high probability. Understanding this phenomenon and effectively defending against adversarial examples are still open questions in the field. The transferability of adversarial examples allows them to fool different models, which can be exploited to attack black-box systems. Various methods have been proposed to generate adversarial examples, attributing their presence to the nonlinearity or linear nature of deep neural networks. Some methods, like the fast gradient sign method (FGSM) and DeepFool, have been shown to be effective for white-box attacks but not as successful for black-box attacks. BID11 introduced the DeepFool method for generating adversarial examples. BID8 analyzed adversarial example transferability and proposed ensemble-based approaches for black-box attacks. BID3 showed that high-confidence adversarial examples have strong transferability. Various defense mechanisms have been proposed, such as defensive distillation by BID12 and adversarial training by BID5. BID9 used image transformations to mitigate adversarial perturbations, while other works focused on detecting adversarial examples (BID7, BID4). In this work, the transferability of adversarial examples is explained, highlighting two components of adversarial perturbation: model-specific and data-dependent. The model-specific component is noisy and represents behavior off the data manifold, while the data-dependent component approximates the ground truth on the data manifold. The data-dependent part is argued to mainly contribute to the transferability of adversarial examples. The proposed noise-reduced gradient (NRG) method utilizes the data-dependent component of gradient for constructing adversarial examples, leading to increased success rates in black-box attacks on the ImageNet validation set. The success rate of these attacks is influenced by model-specific factors such as accuracy and capacity. The success rate of black-box attacks depends on model-specific factors like accuracy and capacity. Models with higher accuracy and lower capacity are better at attacking unseen models. This phenomenon can be explained by transferability and may offer guidance for attacking unseen models. The model function f : R^d \u2192 R^K is obtained by minimizing empirical risk over the training set. High dimensionality makes the model vulnerable to adversarial perturbations, where small imperceptible perturbations can be added to inputs. In the context of deep neural networks, adversarial examples are small perturbations that can cause misclassification. Non-targeted attacks aim to misclassify the input, while targeted attacks aim to produce a specific wrong label. In a black-box attack, the adversary has no knowledge of the target model and cannot query it. In a black-box attack, the adversary has no knowledge of the target model and cannot query it. The adversary constructs adversarial examples on a local model trained on a similar dataset and deploys them to fool the target model. Crafting adversarial perturbation involves optimizing a loss function to measure prediction discrepancy and a metric to quantify perturbation magnitude, with an implicit constraint for image data. The perturbation for image data has an implicit constraint of x adv \u2208 [0, 255] d. BID2 introduced a loss function that manipulates output logit directly. The best metric for distortion is human eyes, difficult to quantify. Ensemble-based approaches suggest using a large ensemble of source models to improve adversarial examples. The objective involves averaging predicted probabilities of each model with ensemble weights. In this paper, the ensemble-based method is discussed, which involves averaging predicted probabilities of each model with ensemble weights. Different optimizers are used to solve the problem, with a focus on the normalized-gradient based optimizer. Two methods, the Fast Gradient Based Method and the Iterative Gradient Method, are explored for solving non-targeted and targeted attacks. These methods are shown to be fast and have good transferability. The Iterative Gradient Method is a simple yet effective optimizer for solving non-targeted and targeted attacks. It performs normalized-gradient ascent for k steps using a projection operator and step size. The method is related to the fast gradient based method and has good transferability between models. The Iterative Gradient Method is an effective optimizer for non-targeted and targeted attacks, with good transferability between models. Transferable adversarial examples come from the similarity between decision boundaries of source and target models. Models A and B, with high performance on the same dataset, learn a similar function on the data manifold but may behave differently off the manifold due to architectural differences and random initializations. This suggests decomposing perturbations into on and off data factors. The perturbation can be decomposed into data-dependent and model-specific components, with the former contributing to transferability between models. The model-specific component has little impact on transferability due to different behaviors off the data manifold. This is illustrated in FIG0, showing how \u2207f A can mislead both model A and B in the inter-class area. The perturbation can be decomposed into data-dependent and model-specific components, with the former contributing to transferability between models. In the left panel of FIG0, \u2207f A can mislead both model A and B in the inter-class area. The data-dependent component \u2207f A attacks model B easily, while the model-specific component \u2207f A \u22a5 fools model A with little impact on transferability. The NRG method aims to enhance the data-dependent component for increased success rates of black-box adversarial attacks. The NRG method aims to enhance the data-dependent component for increased success rates of black-box adversarial attacks by reducing model-specific noise through local averaging. This noise-reduced gradient captures more data-dependent information than the ordinary gradient. The Noise-reduced Iterative Sign Gradient Method (nr-IGSM) enhances data-dependent component for black-box adversarial attacks by reducing model-specific noise through local averaging. It captures more data-dependent information than the ordinary gradient, leading to smoother gradients and better generalization. The Noise-reduced Iterative Sign Gradient Method (nr-IGSM) improves black-box adversarial attacks by reducing model-specific noise through local averaging. It enhances transferability by using classification models trained on ImageNet dataset. The ImageNet ILSVRC2012 validation set with 50,000 samples is used, selecting 5,000 images for each attack experiment that can be correctly recognized by all models. For targeted attacks, images are assigned random wrong labels. For targeted attack experiments, random wrong labels are assigned to images to create perturbations that models cannot classify correctly. Various pre-trained models from PyTorch are used, including resnet and vgg models. The performance of targeted attacks is evaluated based on Top-1 accuracy. The targeted attack performance is evaluated based on Top-1 success rate using pre-specified labels. The distortion is measured using \u221e norm and scaled 2 norm. The effectiveness of noise-reduced gradient technique is demonstrated with fast gradient based methods like FGSM and IGSM. The success rates of nr-FGSM are better than FGSM for blackbox attacks, even improving white-box attacks. Nr-IGSM generates more transferable adversarial examples compared to IGSM, with similar computational cost. Table 2 shows that adversarial examples generated by nr-IGSM transfer more easily than those by IGSM. Large models like resnet152 are more robust to adversarial transfer. IGSM generally generates stronger adversarial examples than FGSM, contradicting previous claims. The study found that adversarial examples generated by FGSM do not transfer as easily to alexnet compared to previous claims. This discrepancy may be due to inappropriate hyperparameters leading to underfitting. The architecture and test accuracy differences between alexnet and source models cause IGSM to overfit more, resulting in a lower fooling rate. This suggests caution in fully trusting the objective function used in the attack. Our noise reduced gradient technique aims to prevent overfitting in attacks like FGSM and IGSM by removing model-specific information from gradients. This allows for better cross-model generalization. We apply this method to ensemble-based approaches and evaluate on a smaller set of 1,000 images. Results show improved success rates for non-targeted attacks using our noise reduced versions of FGSM and IGSM. The Top-1 success rates of IGSM attacks are nearly saturated, so the corresponding Top-5 rates are reported to show improvements more clearly. Single-model based approaches are ineffective for generating targeted adversarial examples. Targeted attacks are sensitive to the step size used in optimization procedures, requiring a large step size for strong examples. Refer to Appendix A for detailed analysis. The Top-5 success rates of NRG methods for targeted and non-targeted attacks outperform normal methods by a large margin. Table 3 shows the comparison between the two approaches, with NRG methods showing significant improvements. Top-1 success rates can be found in Appendix C. In this section, the sensitivity of hyper parameters m and \u03c3 in NRG methods for black-box attacks is explored using the nr-FGSM approach. Larger m leads to higher fooling rates, while an optimal value of \u03c3 is crucial for best performance. Extremely large or small \u03c3 values can negatively impact the effectiveness of the noise reduction process. In this experiment, the optimal \u03c3 for adversarial perturbations varies between source models, being around 15 for resnet18 and 20 for densenet161. The study also investigates the robustness of adversarial examples to image transformations like rotation and blurring, which is crucial for real-world applications. Destruction rate is used to quantify the impact of transformations on adversarial examples. The study compares the robustness of adversarial images generated by NRG methods with vanilla methods using Densenet121 and Resnet34 models. Four image transformations are considered: rotation, Gaussian noise, Gaussian blur, and JPEG compression. Results show NRG-based methods are more robust. Decision boundaries of different models are analyzed, with Resnet34 as the source model and nine target models considered. The study compares the robustness of adversarial images generated by NRG methods with vanilla methods using Densenet121 and Resnet34 models. Different models are considered, including resnet18, resnet50, resnet101, resnet152, vgg11 bn, vgg16 bn, vgg19 bn, densenet121, alexnet. The \u2207f is estimated with m = 1000, \u03c3 = 15. Decision boundaries are analyzed, showing sensitivity differences between sign \u2207f and sign (\u2207f \u22a5 ) for different models. The study shows that removing \u2207f \u22a5 from gradients penalizes the optimizer along the model-specific direction, preventing overfitting to the source model. The minimal distance u for adversarial transfer varies among models, with larger distances for complex models like resnet152 compared to smaller models like resnet50. Adversarial examples crafted from alexnet generalize poorly across models, with only 19.3 percent success in transferring attacks to resnet152. Different models exhibit varying performances in attacking the same target model. Adversarial examples from densenet121 consistently transfer well to different models, such as 84.3% success rate to vgg19 bn. This observation guides the selection of a better local model for generating adversarial examples to attack remote black-box systems. VGG19 bn and resnet152 are chosen as target models for FGSM and IGSM attacks, with results summarized in FIG11 showing the relationship between Top-1 test error and model parameters. The results in FIG11 show that models with lower test error and fewer parameters have stronger attack capability, explained by transferability and model complexity. In this study, it was found that models with lower test error and fewer parameters exhibit stronger attack capability due to transferability and model complexity. Adversarial perturbations can be broken down into model-specific and data-dependent components, with the latter being the main contributor to the transferability of adversarial examples. The proposed noise-reduced gradient (NRG) based methods for crafting adversarial examples are more effective than previous methods. The noise-reduced gradient (NRG) based methods are proposed to craft more effective adversarial examples. Models with lower capacity and higher test accuracy are better at black-box attacks. Future research will focus on combining NRG-based methods with adversarial training for defense. Transferability in attacks is data-dependent and can potentially be defended against. White-box attacks are more difficult to defend due to high-dimensional space. Incorporating NRG strategy can lead to stable features beneficial for transfer learning. The research focuses on using NRG strategy to reduce model-specific noise for better transfer learning. It explores the impact of hyperparameters on generating adversarial examples using IGSM for black-box attacks. The success rates are evaluated on 1,000 randomly selected images using resnet152 and vgg16 bn as target models. The optimal step size for generating adversarial examples is crucial, with both too large and too small step sizes affecting attack performance. The study examines the impact of hyperparameters on generating adversarial examples using IGSM for black-box attacks. It notes that the step size is crucial, with both too large and too small step sizes affecting attack performance. Additionally, the influence of model redundancy on attack capability is confirmed through experiments on the MNIST dataset. The study explores the impact of hyperparameters on generating adversarial examples using IGSM for black-box attacks. Results show that low-capacity models have stronger attack capability than large-capacity models. The Top-1 success rates of cross-model attacks are reported, demonstrating the effectiveness of attacks. Architecture similarities are not considered in the analysis."
}