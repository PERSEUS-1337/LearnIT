{
    "title": "Bkgv71rtwr",
    "content": "Unsupervised domain adaptation has gained attention recently, focusing on closed-set scenarios where source and target domains share the same categories. However, in practice, target domains often have unseen classes. This paper introduces Self-Ensembling with Category-agnostic Clusters (SE-CC), enhancing domain adaptation by incorporating category-agnostic clusters in the target domain for improved alignment. SE-CC enhances domain adaptation by incorporating category-agnostic clusters in the target domain, improving generalization for closed-set and open-set scenarios. Clustering is used to reveal the target domain's data space structure, with a clustering branch ensuring the learned representation matches the cluster distribution. Mutual information maximization further enhances the representation. Extensive experiments on Office and VisDA datasets show superior results compared to existing methods. Convolutional Neural Networks (CNNs) have advanced vision technologies to new state-of-the-arts, relying on large annotated data for training. However, manual labeling is costly and labor-intensive. To address this, unsupervised domain adaptation leverages labeled source samples and unlabeled target samples to generalize a target model, mitigating the \"domain shift\" phenomenon. Unsupervised domain adaptation utilizes labeled source samples and unlabeled target samples to generalize a target model, overcoming the limitations of aligning data distributions between source and target domains. The challenge lies in distinguishing unknown target samples from known ones and learning a hybrid network for both closed-set and open-set domain adaptation. To address the challenge of distinguishing unknown target samples from known ones and learning a hybrid network for closed-set and open-set domain adaptation, clustering is proposed as a novel approach. This method aims to explicitly model the diverse semantics of both known and unknown samples, improving performance compared to binary classification methods. The novel approach proposed involves clustering unlabeled target samples to model diverse semantics of known and unknown classes in the target domain. By decomposing target samples into clusters and utilizing category-agnostic clusters, domain adaptation is enhanced to be domain-invariant for known classes and discriminative for unknown and known classes. Additionally, a clustering branch is integrated into Self-Ensembling French et al. (2018) to refine representations and preserve the structure of the target domain. The new approach involves clustering target samples into category-agnostic clusters to refine representations and preserve the target domain's structure. A clustering branch is integrated into Self-Ensembling to predict cluster assignment distribution and model the mismatch with inherent cluster distribution using KL-divergence. The SE-CC framework utilizes KL-divergence to model mismatch between estimated and inherent cluster distributions in target samples, preserving data structure. Mutual information is maximized to enhance feature representation. Unsupervised domain adaptation in CNNs involves minimizing domain discrepancy through MMD. Unsupervised domain adaptation in CNNs minimizes domain discrepancy through Maximum Mean Discrepancy (MMD). Different approaches include integrating MMD into CNNs, incorporating a residual transfer module, and using a domain discriminator to enforce domain confusion for domain invariance. Open-set domain adaptation extends traditional domain adaptation methods. Utilizing a gradient reversal algorithm for domain discriminator optimization, open-set domain adaptation addresses scenarios with new and unknown classes in the target domain. Early attempts by Panareda Busto & Gall (2017) and later work by Saito et al. (2018b) and Baktashmotlagh et al. (2019) focus on learning feature representations and factorizing data into shared and private subspaces for known classes. Baktashmotlagh et al. (2019) factorize source and target data into shared and private subspaces. The shared subspace models known classes, while the private subspace models unknown class samples. Conditional entropy and self-ensembling loss are used to align classification predictions between teacher and student models. Clustering is performed on unlabeled target samples to create category-agnostic clusters for closed-set and open-set scenarios. A clustering branch in the student model infers assignment distribution over all clusters for each target sample. SE-CC integrates an additional clustering branch into the student model to infer assignment distribution over clusters for each target sample. The feature representation is enforced to preserve data structure by aligning cluster assignment distribution with original clusters. Mutual information is maximized among feature map, classification, and cluster assignment distributions. SE-CC utilizes unlabeled target samples for learning task-specific classifiers in open-set scenarios, differentiating it from previous methods. SE-CC leverages category-agnostic clusters for representation learning in open-set domain adaptation. It aligns sample distributions within known and unknown classes, discriminates between them, and enhances representation by maximizing mutual information among input features, clusters, and class probabilities. This approach is novel and has not been fully explored before. In this paper, the SE-CC model integrates category-agnostic clusters into domain adaptation for open-set scenarios. The goal is to learn domain-invariant representations and classifiers for recognizing known classes in the target domain. Open-set domain adaptation aims to recognize known classes in the target domain while distinguishing unknown samples. Self-Ensembling method builds on Mean Teacher for consistent classification predictions between teacher and student models. The Self-Ensembling method aims to ensure consistent classification predictions between teacher and student models by penalizing differences in classification predictions. The teacher's weights are updated as an exponential moving average of the student's weights. Additionally, an unsupervised conditional entropy loss is adopted to drive decision boundaries away from high-density regions in the target domain. The overall training loss includes supervised cross entropy loss. The Self-Ensembling method includes supervised cross entropy loss on source data, unsupervised self-ensembling loss, and conditional entropy loss on unlabeled target data. It addresses open-set domain adaptation challenges by classifying both inliers and outliers into known and unknown classes. The typical approach of using a binary classifier oversimplifies the problem when unknown samples span multiple classes. To address the challenge of open-set domain adaptation, clustering is used to model diverse semantics in the target domain. This involves creating category-agnostic clusters integrated into Self-Ensembling for domain adaptation. A clustering branch is designed to align cluster assignment distribution with inherent cluster distribution, ensuring domain-invariant features for known classes and discriminative features for unknown and known classes in the target domain. In the target domain, clustering using k-means is utilized to group unlabeled data samples. The clusters obtained are category-agnostic but reveal the underlying structure tailored to the target domain. Target samples with similar semantics are closer within clusters. Each target sample is represented by output features of pre-trained CNNs for clustering. Periodic cluster refreshing did not significantly impact the results. The clustering branch in the student model predicts the distribution over all clusters based on the inherent structure of each target sample. The centroid of each cluster is defined as the average of all samples belonging to that cluster. Periodic refreshing of clusters did not have a significant impact on the results. The clustering branch in the student model predicts cluster assignments for target samples based on their features. It uses a modified softmax layer to infer cluster assignment distributions and is trained with supervision from the inherent cluster distribution of each sample. The clustering branch in the student model predicts cluster assignments for target samples using a KL-divergence loss to measure the mismatch between estimated and inherent cluster distributions. The loss is minimized to preserve the data structure and inter-cluster relationships, ensuring semantically similar clusters have similar assignment parameter matrices. The KL-divergence loss with inter-cluster relationships constraint is relaxed in the student model by leveraging Mutual Information Maximization (MIM) to enhance target feature learning in an unsupervised manner. The MIM module in the student simultaneously estimates and maximizes local and global mutual information between input features and output distributions. The MIM module in the student model aims to estimate and maximize local and global mutual information among input feature maps, output classification distribution, and cluster assignment distribution. Global mutual information is encoded from the output feature map of the last convolutional layer and concatenated with conditioning distributions for discrimination. The global Mutual Information discriminator is used to align the global feature vector with classification and cluster assignment distributions. It is implemented with a fully-connected network and nonlinear activation, estimating Mutual Information via Jensen-Shannon MI estimator. Additionally, local Mutual Information is exploited among local input features at every spatial location. The local Mutual Information is utilized to compare local input features with classification and cluster assignment distributions by spatially replicating and concatenating them with the input feature map before feeding into a discriminator. The final objective for the Mutual Information Maximization (MIM) module in SE-CC is a combination of local and global Mutual Information estimations, balanced with a tradeoff parameter \u03b1. The training objective of SE-CC integrates various losses on both the source and target data, including cross entropy loss, unsupervised self-ensembling loss, conditional entropy loss, KL-divergence loss, and local & global Mutual Information estimation. The SE-CC model integrates various losses on both the source and target data, including cross entropy loss, unsupervised self-ensembling loss, conditional entropy loss, KL-divergence loss, and local & global Mutual Information estimation. Empirical experiments on Office Saenko et al. VisDA dataset validate the effectiveness of SE-CC for synthetic-real image transfer tasks. For open-set adaptation, COCO images in the validation domain are used for evaluation with 12 known classes for both source and target domains, 33 background classes as unknown in source, and 69 COCO categories as unknown in target. The known-to-unknown ratio in the target domain is set at 1:10. Three evaluation metrics - Knwn, Mean, and Overall - are utilized. Knwn represents accuracy for known classes, Mean for known & unknown classes, and Overall for all target samples. Closed-set adaptation focuses on accuracy for all 12 classes. ResNet152 is used as the backbone for CNNs in both scenarios. In the closed-set setting, ResNet152 is used as the backbone for CNNs for clustering and adaptation. AODA and SE-CC \u2666 are compared in an open-set setting where unknown source samples are absent. SE-CC \u2666 learns a classifier without unknown source samples and performs better than other closed-set adaptation models. The SE-CC model outperforms other state-of-the-art closed-set and open-set adaptation models in most transfer directions, especially on harder transfers like D \u2192 A and W \u2192 A. It leverages category-agnostic clusters to create domain-invariant feature representations for known classes while effectively segregating target samples from known and unknown classes. SE-CC outperforms other adaptation models by injecting category-agnostic clusters as a constraint for feature learning and alignment, showing effectiveness in excluding unknown target samples during domain adaptation in open-set scenarios. RTN Long et al. (2016) conducted experiments on closed-set domain adaptation using SE-CC, showing better performance compared to other techniques on Office and VisDA datasets. The results highlight the advantage of utilizing category-agnostic clusters for domain adaptation, even in closed-set scenarios. Ablation study was also conducted to analyze the impact of each design in SE-CC on overall performance. The design in SE-CC influences performance by incorporating Conditional Entropy (CE), KL-divergence Loss (KL), and Mutual Information Maximization (MIM) to enhance classifier decision boundaries and feature refinement for target domain adaptation. CE is a general method to improve classifier performance in target domains, as shown in performance improvements on VisDA dataset. Incorporating Conditional Entropy (CE), KL-divergence Loss (KL), and Mutual Information Maximization (MIM) in SE-CC improves Mean accuracy by 4.2% in total. This approach effectively exploits category-agnostic clusters for open-set domain adaptation, separating unknown target samples and enhancing network integration. Incorporating Conditional Entropy, KL-divergence Loss, and Mutual Information Maximization in SE-CC improves Mean accuracy by 4.2%. The approach integrates category-agnostic clusters for open-set domain adaptation, separating unknown target samples and enhancing network integration. The study focuses on learning a hybrid network that integrates category-agnostic clusters into Self-Ensembling to separate unknown target samples from known ones. Clustering is initially performed to decompose target samples into category-agnostic clusters, with an additional clustering branch integrated into the student model to align cluster assignment distribution. Mutual information among input features, classification outputs, and clustering branches is utilized to enhance the learned features. Experiments on Office and VisDA datasets for open-set and closed-set adaptation tasks validate the proposal, showing performance improvements compared to state-of-the-art methods. The implementation of SE-CC is developed with PyTorch and optimized with SGD. Learning rate, mini-batch size, and training iterations are set for experiments on Office and VisDA datasets. Global feature dimension and cluster settings are detailed in Table 6 for open-set and closed-set adaptation tasks. The implementation of SE-CC in PyTorch is optimized with SGD. Parameters are tuned for open-set and closed-set adaptation tasks using Gap statistics method. Evaluation shows KL-divergence in SE-CC outperforms L1 and L2 distance in clustering branch design. In the evaluation of Mutual Information Maximization, different variants of MIM module in SE-CC are assessed by estimating mutual information between input features and various outputs. CLS, CLU, and CLS+CLU estimate local and global mutual information between input features and the output of classification and clustering branches. CLS and CLU slightly improve performance by exploiting mutual information between input features and branch outputs, while CLS+CLU shows a larger performance boost by combining outputs from both branches for mutual information estimation. The evaluation of Mutual Information Maximization in SE-CC involves assessing different variants of the MIM module by estimating mutual information between input features and outputs. CLS and CLU slightly improve performance by exploiting mutual information, while CLS+CLU shows a larger boost by combining outputs from both branches. The results highlight the importance of exploiting mutual information among input features and combined outputs for classification and clustering tasks. SE brings source and target distributions closer, but struggles to recognize unknown target samples. In contrast, SE-CC preserves the target data structure, separating unknown from known target samples while maintaining indistinguishability between known samples in different domains."
}