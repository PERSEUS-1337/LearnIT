{
    "title": "HyeuFOcMyX",
    "content": "Structural planning is crucial for generating long sentences in language models. This study introduces a planning phase in neural machine translation to control sentence structure. The model generates planner codes to guide the output words, improving translation performance. By manipulating the planner codes, translations with different structures can be achieved. Planning ahead is shown to enhance translation quality and ensure grammatical correctness in speech. In contrast to human speakers, neural machine translation models lack a planning phase when generating sentences, leading to uncertainty in word prediction. This research aims to address this issue by introducing a planning phase in NMT to improve translation quality and control sentence structure. In contrast to human speakers, neural machine translation models lack a planning phase when generating sentences, leading to uncertainty in word prediction. This research introduces a planning phase in NMT to improve translation quality and control sentence structure by inserting planner codes at the beginning of output sentences. The research introduces planner codes in NMT to improve translation quality and control sentence structure. Planner codes help disambiguate uncertain information about sentence structure, leading to better translation performance. Experiments show that manipulating planner codes can control the structure of output sentences effectively. The research introduces planner codes in NMT to improve translation quality and control sentence structure. By manipulating planner codes, the translation performance is enhanced, allowing for control over the structure of output sentences. Structural annotations are extracted to provide a \"big picture\" of the sentence, simplifying POS tags to reduce uncertainty in decoding. Beam search or the NMT model itself can efficiently solve uncertainty in local structures. The research introduces planner codes in NMT to improve translation quality and control sentence structure by simplifying POS tags. The code learning model computes discrete codes based on POS tags to remove uncertainty in sentence structure, enhancing translation performance. The code learning model computes discrete codes based on simplified POS tags using LSTM and Gumbel-Softmax trick. These codes are used to initialize a decoder LSTM for sequential prediction of tags. The model architecture includes a sequence auto-encoder with an extra context input X to the decoder, optimized with crossentropy. The code learning model, depicted in Fig. 2, is a sequence auto-encoder with an extra context input X to the decoder. Parameters are optimized with crossentropy loss. Planner codes C are obtained for target sentences in the training data. The training data consists of (X, Y) sentence pairs, which are transformed into (X, CY; Y) pairs with the planner codes. A regular NMT model is trained using the modified dataset, with beam search used during decoding. Planner codes are searched before emitting real words, and removed during evaluation. Recent methods aim to improve syntactic correctness in translations, with BID19 restricting the search space of the NMT decoder. Several methods have been proposed to enhance the syntactic correctness of translations. BID19 limits the NMT decoder's search space using a lattice from a Statistical Machine Translation system. BID2 employs a multi-task approach by allowing the NMT model to parse a dependency tree and combine parsing loss with the original loss. Other works incorporate target-side syntactic structures explicitly, such as interleaving CCG supertags with output words. Aharoni and Goldberg (2017) train a NMT model to generate linearized constituent parse trees instead of predicting words. BID20 introduces a model that generates words and parse actions simultaneously, conditioning word prediction on action prediction. However, none of these methods plan the structure before translation. The text discusses different methods for improving translation accuracy, including learning concept codes for word embeddings and breaking down word dependencies with shorter code sequences. Models are evaluated on various translation tasks using different tools for tokenization and encoding. The code learning model has hidden layers with 256 units and is trained using Nesterov's accelerated gradient for up to 50 epochs. Different settings for code length N are tested. The model has 256 hidden units and is trained using Nesterov's accelerated gradient for up to 50 epochs with a learning rate of 0.25. Different settings of code length N and number of code types K are tested, with the information capacity being N log K bits. A trade-off is observed between S Y accuracy and C Y accuracy, with the setting of N = 2, K = 4 showing a balanced trade-off. The NMT model uses 2 layers of bidirectional LSTM encoders and 2 layers of LSTM decoders with 256 units for the IWSLT De-En task. The NMT model utilizes 2 layers of bidirectional LSTM encoders and 2 layers of LSTM decoders with 256 units for the IWSLT De-En task. Key-Value Attention is applied in the first decoder layer, and a residual connection is used to combine hidden states in two decoder layers. Dropout with a rate of 0.2 is applied outside of the recurrent function. The NMT models are trained using the NAG optimizer with a learning rate of 0.25, annealed by a factor of 10 after 20K iterations if no improvement is observed. Conditioning word prediction on generated planner codes improves translation performance over a strong baseline. The NMT model with bidirectional LSTM encoders and LSTM decoders shows improved translation performance by conditioning word prediction on generated planner codes. However, applying greedy search on JaEn dataset results in lower BLEU scores compared to the baseline. Beam search followed by greedy search did not significantly change results. It is suggested that exploring multiple candidates with diverse structures simultaneously may enhance beam search performance. Manual selection of planner codes could also be considered. The NMT model with bidirectional LSTM encoders and LSTM decoders improves translation by using planner codes. Table 3 shows different translations based on planner codes. Manipulating codes results in translations with varied structures. The distribution of assigned planner codes for English sentences in the ASPEC Ja-En dataset is shown in Figure 3. The proposed method for English sentence translation in the ASPEC Ja-En dataset involves using planner codes to generate diverse paraphrased translations. The distribution of learned codes indicates room for improvement in code capacity utilization. Additionally, an alternative approach of predicting structural annotations directly may lead to performance degradation due to error propagation in word generation. In this paper, a planning phase is added to neural machine translation to generate planner codes for controlling the output sentence structure. An end-to-end neural network with a discretization bottleneck is designed to predict simplified POS tags of target sentences. Experimental results show improved translation performance and the ability to sample translations with different structures using planner codes. The planning phase helps remove uncertainty in sentence structure during decoding. This framework can be extended to plan other latent factors like sentiment or topic. The decoding algorithm removes uncertainty in sentence structure and can be extended to plan other latent factors like sentiment or topic."
}