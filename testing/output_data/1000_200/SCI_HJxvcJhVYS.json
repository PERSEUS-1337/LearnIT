{
    "title": "HJxvcJhVYS",
    "content": "Inverse problems are common in natural sciences, involving inferring complex posterior distributions over hidden parameters from observations. A proposed method uses Bayesian optimization to approximate these distributions through Stein variational gradient descent on Gaussian process estimates, showing promise in likelihood-free inference for reinforcement learning environments. The method uses Bayesian optimization and Stein variational gradient descent on Gaussian process estimates to approximate posterior distributions for inferring parameters in physical systems. Likelihood-free methods like ABC and conditional density estimation are employed when the likelihood function is unavailable. Challenges arise in robotics and reinforcement learning due to limited simulations, prompting the development of more efficient approaches. Recent methods address challenges in efficiency by constructing conditional density estimators from joint data or sequentially learning approximations to the likelihood function and then running MCMC. Gutmann and Corander derive an active learning approach using Bayesian optimization to propose parameters for simulations, reducing the number of simulator runs. This paper combines variational inference methods with Bayesian optimization using a Thompson sampling strategy. The approach combines variational inference methods with Bayesian optimization using a Thompson sampling strategy to refine variational approximations to a black-box posterior. Parameters for new simulations are proposed using Stein variational gradient descent over samples from a Gaussian process. The method also includes optimal subsampling of variational approximations for batch evaluations of simulator models. The goal is to estimate a distribution that approximates a posterior distribution over simulator parameters given observations from a target system. The text discusses Bayesian optimization for estimating a distribution over simulator parameters using a discrepancy measure between simulator outputs and observations. The approach involves a black-box method that does not require gradients of the target distribution, utilizing a GP model, Thompson sampling acquisition function, and kernel herding procedure. The text discusses using Stein variational gradient descent (SVGD) to learn parameters directly, bypassing the need for a Gaussian Process (GP) model. A synthetic likelihood function is used to model the discrepancy between simulations and observations, with background details on the Kernelized Stein Discrepancy (KSD) provided in the appendix. The text discusses using Thompson sampling with a Gaussian Process (GP) for selecting candidate distributions in Bayesian Optimization (BO) problems. Thompson sampling accounts for uncertainty by sampling functions from the GP posterior, allowing for efficient evaluation and differentiability in the BO loop. Thompson sampling with Gaussian Processes (GP) in Bayesian Optimization (BO) involves sampling functions from the GP posterior to address uncertainty. The approach uses sparse spectrum Gaussian processes (SSGPs) for sampling weights, defining an acquisition function based on the target posterior approximation. Stochastic Variational Gradient Descent (SVGD) optimizes a set of particles from the prior distribution through smooth perturbations using the SSGP kernel. Thompson sampling with Gaussian Processes in Bayesian Optimization involves using Sparse Spectrum Gaussian Processes (SSGPs) and Stochastic Variational Gradient Descent (SVGD) to optimize particles from the prior distribution through smooth perturbations with the SSGP kernel. The process guides particles to local maxima of the target posterior while encouraging diversification. Gradients of sample functions are available for SSGP models with differentiable mean functions. The use of a large number of particles in Algorithm 1 improves exploration of the approximate posterior surface in Bayesian Optimization. However, to avoid the high cost of simulations, an optimal subsampling method is used to select a subset of query parameters for running the simulator. Kernel herding is employed to construct a set of samples that minimizes error in empirical estimates under a given distribution. The error in the algorithm is bounded by the maximum mean discrepancy (MMD) between the kernel embedding of q and its subsampled version. Kernel herding is used to select informative samples for the model based on the GP posterior kernel. The distributional Bayesian optimization (DBO) algorithm is summarized in Algorithm 1, with experimental results presented for evaluating DBO in synthetic data. The distributional Bayesian optimisation (DBO) algorithm is summarized in Algorithm 1. Experimental results evaluating DBO in synthetic data scenarios are presented, comparing the method against mixture density networks (MDNs). The experiment is conducted on OpenAI Gym's 3 cart-pole environment with fixed physics parameters \u03b8 real. Details on the experimental setup can be found in Appendix B. The paper presents a Bayesian optimization approach for inverse problems on simulator parameters, showing that DBO outperforms MDN in recovering the target system's posterior. Results indicate DBO provides a more sample-efficient approach for inferring parameters in reinforcement learning environments. Future work includes scalability and theoretical analysis. The paper introduces a Bayesian optimization method, DBO, which outperforms MDN in recovering posterior parameters of a reinforcement learning environment. Future work involves scalability and theoretical analysis. The method allows for fast incremental updates of the GP posterior with time complexity O(M^2). To avoid recomputing A \u22121 N +1, one can keep track of its Cholesky factors for updating the GP posterior with time complexity O(M^2), which is constant with respect to the number of data points N."
}