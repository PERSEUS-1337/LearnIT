{
    "title": "HylzTiC5Km",
    "content": "The Subscale Pixel Network (SPN) is proposed as a conditional decoder architecture to generate high fidelity images. It addresses challenges in encoding vast context and learning distributions for global coherence and detail exactness. Multidimensional upscaling is used to grow images in size and depth efficiently. The Subscale Pixel Network (SPN) proposes using multidimensional upscaling to grow images in size and depth efficiently. It achieves state-of-the-art likelihood results in generating CelebAHQ and ImageNet images of various sizes. Autoregressive models have excelled in producing high-fidelity samples across various domains but struggle with long-range structure and semantic coherence in large-scale image generation. Autoregressive models excel in generating high-fidelity images but face challenges with long-range structure and semantic coherence. The relationship between MLE scores and sample fidelity is complex, as MLE forces models to support the entire distribution, leading to capacity allocation to irrelevant parts. Additionally, the high dimensionality of large images requires architectural connections to learn dependencies among positions. The text discusses the challenges faced by autoregressive models in generating high-fidelity images due to the need for architectural connections to learn dependencies among positions. It also mentions the complexity of the relationship between MLE scores and sample fidelity, as well as the allocation of capacity to irrelevant parts. The goal is to learn the full distribution over 8-bit RGB images of size up to 256 \u00d7 256 with high fidelity. The text discusses learning the full distribution over 8-bit RGB images up to 256 \u00d7 256 with high fidelity. It focuses on visually salient subsets of the distribution and uses Multidimensional Upscaling to map between them for image generation. The text discusses training three networks for upscaling images from 32 \u00d7 32 3-bit RGB to 128 \u00d7 128 8-bit RGB. A Subscale Pixel Network architecture is developed to address training difficulties. The process involves using decoders for small size, low depth images, size-upscaling, and depth-upscaling. The Subscale Pixel Network (SPN) divides an image into sub-images and uses two networks for conditioning and decoding. It can act as an independent image decoder with implicit size upscaling or as an explicit size upscaling network. The SPN is evaluated for performance in comparison to other upscaling methods. The Subscale Pixel Network (SPN) is evaluated for performance on image generation benchmarks CelebAHQ-256 and ImageNet up to 256 sizes. It achieves state-of-the-art results on CelebAHQ-256 and ImageNet-64, with strong benefits of multidimensional upscaling and SPN shown in sample fidelity. Samples produced at full 8-bit resolution are visually similar to those from GANs, with successful samples on unconditional image generation. The Subscale Pixel Network (SPN) demonstrates superior performance on image generation benchmarks CelebAHQ-256 and ImageNet up to 256 sizes, achieving state-of-the-art results. Samples at full 8-bit resolution show visual fidelity comparable to GANs, highlighting the impact of SPN and multidimensional upscaling on sample quality. The PixelCNN model generates color images using a deep neural network to parametrize conditional distributions. The Subscale Pixel Network (SPN) utilizes a unique ordering method for conditional distribution parametrization using a deep neural network. This alternative ordering divides large images into equally sized slices, enabling efficient encoding of long-range dependencies and inducing a spatial structure. The ordering allows for consistent application of the same decoder to all slices, facilitating the use of self-attention in the SPN. The Subscale Pixel Network (SPN) utilizes a unique two-dimensional ordering method for conditional distribution parametrization. A scaling factor is selected to obtain slices of the image, each specified by its row and column offset. The subscale ordering captures size upscaling implicitly and allows for consistent application of the same decoder to all slices. The Subscale Pixel Network (SPN) captures size upscaling implicitly and can act as a full-blown image model as well as a size upscaling model. The single-slice decoder can be trained on subimages to generate the first slice of a subscale ordering, with the main network generating the rest of the image according to the ordering. The SPN can also be used for Parallel Multi-Scale BID12 ordering for size upscaling. The Parallel Multi-Scale BID12 ordering for size upscaling involves doubling pixels in an image at every stage using distinct neural networks in parallel. Multidimensional upscaling expands not only the height and width but also the channel depth of an image in stages, with each network generating bits conditioned on the previous ones. The second stage of depth upscaling involves generating lower significance bits based on the first d1 bits of each channel. Unlike size upscaling, weights are not shared among networks at different stages. The goal is to focus on visually salient bits of an image, similar to Grayscale PixelCNN. Depth upscaling is related to generating context for each pixel dimension in AR approaches, requiring computation and memory. Existing AR approaches require significant computation and memory for generating context for each pixel dimension in colored images. The quadratic memory requirements of self-attention become limiting for images larger than 32 \u00d7 32, making it intractable for 256 \u00d7 256 images. Mitigating these requirements often sacrifices global context, such as cropping images or using local neighborhood self-attention. Model parallelism is not effective in learning global structure. To address these challenges, a new approach is devised. The Subscale Pixel Network (SPN) addresses challenges in learning global structure by using a scaling factor to obtain slices of the original image. The SPN architecture consists of an embedding part for slices at preceding metapositions to condition the decoder for the current slice. The SPN architecture includes an embedding part with a convolutional neural network for slices at preceding metapositions to condition the decoder for the current slice. Padding slices are used to preserve relative meta-positions and ensure the depth of the input slice tensor remains the same for all target slices. The embedding architecture in the SPN model includes padding slices to maintain input tensor depth and meta-position information. The decoder processes the target slice in raster-scan order using a hybrid architecture combining masked convolution and self-attention. The decoder in the SPN model processes the target slice in raster-scan order using a hybrid architecture combining masked convolution and self-attention. Initial 1D self-attention network gathers context in the slice, followed by masked 1D self-attention layers. The output is reshaped and given as conditioning input to a Gated PixelCNN for modeling the target slice with full masking over pixels and channel dimensions, resulting in significantly lower memory requirements. The SPN model's decoder processes the target slice using a hybrid architecture of masked convolution and self-attention. This results in significantly lower memory requirements due to the compact concatenation along the channel dimension of the input tensor. Maximum likelihood learning is performed through stochastic gradient descent on a Monte Carlo estimate, with all gradients computed by backpropagation. The SPN also serves as a size-upscaling network when initialized with an externally generated subimage. The SPN can upscale the depth of image channels by dividing the image into slices and concatenating them along the channel dimension. This allows for modeling with low bit depth data. The SPN is enhanced by adding a fixed additional input to model low bit depth data, resulting in high-quality samples at high resolution for CelebA-HQ and ImageNet images. The network operates on small images and can train large networks with multiple convolutional and self-attention layers. The context-embedding network has 5 convolutional layers and 6-8 self-attention layers. The masked decoder includes a PixelCNN with 15 layers. The 1D Transformer in the decoder has 8-10 layers. The hybrid decoder performs well on 32 \u00d7 32 and 64 \u00d7 64 Downsampled ImageNet datasets, achieving state-of-the-art results on the latter with a log-likelihood of 3.52 bits/dim. On 64 \u00d7 64 Downsampled ImageNet, state-of-the-art log-likelihood of 3.52 bits/dim was achieved. PixelSNAIL and SPN also showed promising results at this resolution. SPN improved log-likelihood on 128 \u00d7 128 ImageNet from 3.55 bits/dim to 3.08 bits/dim. The experiments used the standard ILSVRC Imagenet dataset resized with Tensorflow's function. The experiments on 128 \u00d7 128 ImageNet showed a log-likelihood improvement from 3.55 bits/dim to 3.08 bits/dim. Depth upscaling resulted in samples with significant semantic coherence, while multidimensional upscaling increased the overall success rate. High-fidelity samples of celebrity faces were produced at 256 \u00d7 256 resolution. The achieved MLE scores were significantly improved compared to previous reports. Samples of CelebAHQ-256 were compared favorably to other models like Glow and GANs. The SPN and Multidimensional Upscaling model achieves state-of-the-art MLE scores on large-scale images like CelebAHQ-256 and ImageNet-128. It can generate high fidelity 8-bit samples without altering the sampling process, showing unprecedented semantic coherence. The generated samples exhibit high semantic coherence and detail accuracy even at large scale sizes of 128 \u00d7 128 and 256 \u00d7 256 images. The entropy of the softmax output distributions can be adjusted using a \"temperature\" divisor on the predicted logits. Large batch sizes (up to 2048) are used to maintain consistency in pixel count during subscaling. Large batch sizes (up to 2048) are achieved by increasing data parallelism on Google Cloud TPU pods. For Imagenet 32, 64 tensorcores are used, while for ImageNet 64, 128, and 256, 128 tensorcores are used. When overfitting is an issue, batch size is decreased to 32 tensorcores. SPN architectures have between \u223c50M and \u223c250M parameters, with the number varying depending on the dataset. Depth-upscaling in CelebA-HQ 3bit samples from SPN doubles the parameters by using two separate networks with untied weights. Depth-upscaling in CelebA-HQ 3bit samples from SPN doubles the number of parameters by using two separate networks with untied weights. Sizeupscaling adds even more parameters for a separate decoder-only network, with the maximal parameter count reaching approximately 650M in the multidimensional upscaling setting for ImageNet 128."
}