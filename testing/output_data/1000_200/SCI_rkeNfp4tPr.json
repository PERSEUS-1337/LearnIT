{
    "title": "rkeNfp4tPr",
    "content": "Stochastic gradient descent with stochastic momentum is popular for training deep neural networks. Momentum adjustment has been shown to reduce convergence time in non-stochastic convex optimization, but its benefits in stochastic and non-convex settings are not well understood. Empirically, stochastic momentum improves convergence time in deep network training, leading to the development of popular update methods like ADAM and AMSGrad. Theoretical justification for stochastic momentum remains an open question. In this paper, the authors propose that stochastic momentum improves deep network training by helping SGD escape saddle points faster and find a second order stationary point more quickly. Theoretical analysis suggests that a large momentum parameter (close to 1) is ideal, which aligns with empirical findings. Experimental results further support these conclusions, highlighting the effectiveness of SGD with stochastic momentum in nonconvex optimization and deep learning. SGD with stochastic momentum is widely used in training machine learning models in various applications like computer vision, speech recognition, natural language processing, and reinforcement learning. It helps achieve faster convergence compared to standard SGD, making it a necessary tool for designing new optimization algorithms in deep learning. SGD with stochastic momentum is a crucial tool for designing optimization algorithms in deep learning. Popular methods like Adam and AMSGrad utilize momentum for faster convergence. Despite its widespread use, the rationale behind its empirical success and setting the momentum parameter remains unclear. This algorithm is the default in software packages like PyTorch and Tensorflow. SGD with stochastic momentum is essential for optimization in deep learning. It can help escape saddle points faster than standard SGD. Stochastic heavy ball momentum maintains a weighted average of stochastic gradients to update iterates. This approach aims to find second-order stationary points in non-convex optimization. In this paper, the focus is on finding a second-order stationary point for smooth non-convex optimization using SGD with stochastic heavy ball momentum. The goal is to reach an approximate second-order stationary point due to the difficulty of finding global or local minima in non-convex optimization. Most papers in this field target reaching this point with additional assumptions like Lipschitzness in the gradients. In nonconvex optimization, the focus is on reaching an approximate second-order stationary point with additional assumptions like Lipschitzness in the gradients. The use of momentum is aimed at showing the benefit of reaching a ( , )-second-order stationary point by ensuring updates have suitable correlation with negative curvature directions of the function. The stochastic momentum in SGD with heavy ball momentum amplifies the escape signal \u03b3, aiding in faster escape from saddle points. Under the Correlated Negative Curvature assumption, SGD with momentum properties like APAG, APCG, and GrACE takes T = O((1 \u2212 \u03b2) log(1/(1 \u2212 \u03b2) ) \u221210 ) iterations to reach a second-order stationary point. A larger momentum parameter \u03b2 can facilitate escaping saddle points. Theoretical results show that a larger momentum parameter \u03b2 helps in escaping saddle points faster in optimization and deep learning. This sheds light on why SGD with momentum enables faster training. The notation E t [\u00b7] represents conditional expectation fixing randomness up to but not including time t. A thought experiment illustrates the benefit of stochastic momentum in avoiding saddle points during iterative updates. In iterative updates, parameters can get stuck in saddle points where the gradient is small. Moving in the direction of the smallest eigenvector of the Hessian matrix can help escape quickly, but direct computation is costly. Researchers typically rely on gradient methods to avoid this issue. In the context of avoiding saddle points, researchers typically rely on gradient methods to escape quickly. Daneshmand et al. (2018) studied non-momentum SGD and made an assumption that drives updates out of the saddle point region. In this paper, stochastic momentum is studied, and the CNC property requires the update direction to be strongly non-orthogonal to the direction of large negative curvature. The analysis shows that updates begin to escape saddle points for similar reasons, especially when \u03b2 is close to 1. The text discusses how momentum in optimization algorithms can accelerate the escape process from saddle points. It explains that successive iterations with momentum update can speed up the escape process by a factor of 1 - \u03b2. However, it also mentions that the choice of \u03b2 is constrained and cannot be arbitrarily close to 1. The benefits of stochastic momentum are empirically established. The text demonstrates the benefits of stochastic momentum in accelerating the escape from saddle points in optimization tasks. Two stochastic optimization problems with significant saddle points are constructed, showing the effectiveness of momentum in escaping these points. The choice of momentum parameter \u03b2 is constrained and cannot be arbitrarily close to 1. In optimization tasks, stochastic momentum accelerates escape from saddle points. Two problems with significant saddle points show momentum's effectiveness. The momentum parameter \u03b2 has constraints and cannot be close to 1. The origin is near saddle points, and convergence in function value f(\u00b7) is plotted. Initialization is w0 = 0 with step size \u03b7 = 5 \u00d7 10 \u22125. Convergence in relative distance to the true model w* is also plotted. SGD and SGD with momentum start at the origin to escape saddle points before convergence. Phase retrieval problem with real applications in physical sciences involves finding an unknown w* with limited samples y i = (a i w * ) 2. In phase retrieval, larger choices of \u03b2 significantly accelerate convergence. Trajectories with large momentum escape saddle points quicker than those with smaller momentum, leading to a dramatic speed up in finding optimal solutions. The heavy ball method, proposed by Polyak in 1964, is also effective in this context. The heavy ball method, originally proposed by Polyak in 1964, does not provide a convergence speedup over standard gradient descent in most cases. However, in specialized algorithms designed to exploit negative curvature, like our work, it can help escape saddle points faster. Algorithms designed to exploit negative curvature can help escape saddle points faster. Pioneer works in this category include Ge et al. (2015) and Jin et al. (2017). Phase retrieval is nonconvex with strict saddle property, where each saddle exhibits negative curvature. Daneshmand et al. (2018) assume stochastic gradient has a component to escape, making an assumption of Correlated Negative Curvature (CNC) for stochastic gradient. Our work is motivated by Daneshmand et al. (2018) and assumes Correlated Negative Curvature (CNC) for stochastic momentum instead of stochastic gradient. We compare our results with related works in Appendix A. The gradient \u2207f is assumed to be L-Lipschitz and the Hessian \u22072f is \u03c1-Lipschitz. The stochastic gradient has bounded noise and the norm of stochastic momentum is bounded. Our analysis focuses on stochastic momentum. Our analysis of stochastic momentum focuses on three key properties: Almost Positively Aligned with Gradient (APAG), Almost Positively Correlated with Gradient (APCG), and Gradient Alignment or Curvature Exploitation (GrACE). These properties are essential for understanding the behavior of stochastic momentum in natural settings and empirical demonstrations. The SGD with momentum exhibits Gradient Alignment or Curvature Exploitation (GrACE) if the momentum term is not significantly misaligned with the gradient. This condition ensures progress in the algorithm when the gradient is large. Another related property is Almost Positively Correlated with Gradient (APCG), which measures the correlation between the momentum term and the gradient in the Mahalanobis norm induced by M t. The momentum term is positively correlated with the gradient in the Mahalanobis norm induced by Mt, measuring the local curvature of the function with respect to the trajectory of SGD with momentum. Empirical evidence supports this property on natural problems. APCG is relevant in saddle regions with significant iterations, reporting values when the gradient is large. The value is mostly nonnegative, especially in saddle points. The figures show that SGD with momentum exhibits APAG and APCG properties in experiments, with expected nonnegative values for the phase retrieval problem. GrACE measures alignment between stochastic momentum and gradient, as well as curvature exploitation. The text discusses the alignment between stochastic momentum and gradient, as well as curvature exploitation in optimization algorithms like SGD with momentum. The analysis follows a similar template to previous studies and is structured into three cases based on the gradient values. The proof follows a similar template to previous studies and is structured into three cases based on gradient values. The algorithm analyzed is Algorithm 2, with adjustments to step size parameters and momentum. The algorithm analyzed in Algorithm 2 makes progress in cases (a) and (b), and weakly hurts progress in case (c) when the goal is already met. It reaches a second order stationary point with high probability, satisfying certain properties throughout the iterations. The algorithm in Algorithm 2 reaches a second order stationary point with high probability, satisfying certain properties throughout the iterations. Stochastic momentum for SGD is advantageous, with higher \u03b2 leading to faster convergence to a second order stationary point. Constraints on \u03b2 are necessary to prevent it from being too close to 1. In Table 3, T thred =\u00d5 1 \u03b7 shows a dependency 1 \u2212 \u03b2, making it smaller than Daneshmand et al. (2018), indicating faster escape from saddle points with momentum. Algorithm 2 outperforms CNC-SGD in the high momentum regime, finding second order stationary points faster. Empirical results show c \u2248 0 and c h \u2248 0 in phase retrieval, satisfying conditions for a wide range of \u03b2. The analysis focuses on escaping saddle points with SGD momentum, entering regions with small gradients and large negative eigenvalues. The analysis focuses on escaping saddle points with SGD momentum, entering regions with small gradients and large negative eigenvalues. It takes at most T thred iterations to escape, with the function value decreasing by at least F thred on expectation. The technique used is proving by contradiction, showing that the lower bound is larger than the upper bound. The analysis focuses on escaping saddle points with SGD momentum, showing that the lower bound is larger than the upper bound. The function value must decrease by at least F thred in T thred iterations on expectation. The dependency on \u03b2 suggests that larger momentum helps in escaping saddle points faster. The analysis demonstrates the importance of momentum in escaping saddle points with SGD. The lower bound must exceed the upper bound, with the function value decreasing by at least F thred in T thred iterations. The key lies in the recursive dynamics of SGD with momentum, as shown in Lemmas 2 and 3. The analysis highlights the significance of momentum in SGD for faster convergence to a second-order stationary point. The lower bound must surpass the upper bound, with the function value decreasing by at least F thred in T thred iterations. This is achieved through the recursive dynamics of SGD with momentum, as demonstrated in Lemmas 2 and 3. In this paper, three properties are identified that guarantee faster convergence to a second-order stationary point with SGD momentum. A higher momentum leads to escaping strict saddle points faster. The conditions ensuring these properties are unclear, but understanding them could shed light on the success of SGD with momentum in non-convex optimization and deep learning. The heavy ball method, proposed by Polyak in 1964, has been observed to be effective. The heavy ball method, proposed by Polyak in 1964, has been observed to be effective in non-convex optimization and deep learning. Recent works have analyzed its performance for various optimization problems, showing that it converges at a rate of O(1/ \u221a t) for smooth non-convex objective functions. However, the convergence rate is not better than that of standard SGD. Variants of stochastic accelerated algorithms have also been proposed, but their effectiveness remains to be fully understood. The heavy ball method, proposed by Polyak in 1964, has been effective in non-convex optimization and deep learning. Recent works have analyzed its performance, showing convergence at a rate of O(1/ \u221a t) for smooth non-convex functions. However, the rate is not better than standard SGD. Some variants of stochastic accelerated algorithms have been proposed, but their effectiveness is not fully understood. Ghadimi & Lan (2016; 2013) proposed stochastic accelerated algorithms with first order stationary point guarantees, but they do not capture the stochastic heavy ball momentum used in practice. Kidambi et al. (2018) showed that for specific problems, SGD with heavy ball momentum fails to achieve the best convergence rate. There are works aiming at reaching a second order stationary point, classified into specialized algorithms and simple GD/SGD variants designed to exploit negative curvature explicitly. Specialized algorithms exploit negative curvature to escape saddle points faster than standard SGD. Fang et al. (2019) propose average-SGD with a suffix averaging scheme for updates. Iteration complexity results show differences between algorithms. Our analysis focuses on the effectiveness of stochastic heavy ball momentum compared to other algorithms. We build on previous work by Daneshmand et al. (2018) and suggest modifications to show the advantages of stochastic momentum in algorithms by Fang et al. (2019) and Jin et al. (2019). Lemma 7 highlights the impact of gradient norm under the APAG property on the performance of SGD with momentum. Lemma 6, 7, and 8 discuss the properties of SGD with momentum. Lemma 7 shows that under the APAG property, SGD with momentum decreases the function value by a constant when the gradient norm is large. Lemma 8 bounds the increase of function value of the next iterate using the GrACE property. Lemma 8 discusses the GrACE property of SGD with momentum, showing that the function value increase of the next iterate is bounded. Lemma 8 discusses the GrACE property of SGD with momentum, showing that the function value increase of the next iterate is bounded. The proof involves rewriting equations and bounding terms to establish the desired properties. Lemma 8 discusses the GrACE property of SGD with momentum, showing that the function value increase of the next iterate is bounded. The proof involves rewriting equations and bounding terms to establish the desired properties. In the proof, quadratic approximations are used at specific time points to derive the desired results. Lemma 5 discusses constraints on parameter \u03b2 in the context of SGD with momentum. The constraints ensure that \u03b2 is not too close to 1 for specific conditions to hold. These constraints are used in the proofs but are considered artifacts of the analysis. The constraints on parameter \u03b2 in SGD with momentum ensure it is not too close to 1. The dependence on L, \u03c3, and c are artificial and can be adjusted. Lemmas with parameter choices are used in proofs. The text discusses upper bounds derived from assumptions of Lipschitz gradients and Hessian properties in optimization algorithms. It also analyzes terms related to parameter choices in the context of stochastic gradient descent with momentum. The text discusses upper bounds derived from assumptions of Lipschitz gradients and Hessian properties in optimization algorithms, as well as terms related to parameter choices in stochastic gradient descent with momentum. The bounds are derived by utilizing various mathematical techniques and Lemmas to analyze the optimization process. The text presents Lemmas 12, 13, and 14 which provide lower bounds for different expectations in optimization algorithms under specific conditions. Lemma 12 discusses the lower bound for E t0 [2\u03b7 q v,t\u22121 , q \u03be,t\u22121], Lemma 13 for E t0 [2\u03b7 q v,t\u22121 , q m,t\u22121], and Lemma 14 for 2\u03b7E t0 [ q v,t\u22121 , q w,t\u22121]. These bounds are derived using mathematical techniques and properties of matrices in the optimization process. Lemma 14 provides a lower bound for 2\u03b7E t0 [ q v,t\u22121 , q w,t\u22121 ] in optimization algorithms under specific conditions. The proof involves leveraging properties of matrices and the APCG property to show that the function value must decrease at least F. The strategy involves showing that the lower bound is larger than the upper bound, leading to a contradiction and concluding that the function value must decrease at least F in T iterations. By using specific Lemmas, it is proven that certain conditions are met, ensuring the inequality holds. By choosing a large enough T, the inequality holds with high probability. Lemmas are used to define events and probabilities, ensuring the function value decreases in T iterations. If specific properties are met, SGD with momentum reaches a second order stationary point in O((1-\u03b2) log(Lcm\u03c3^2\u03c1cc h(1-\u03b2)\u03b4\u03b3)^-10) iterations. If specific properties are met, SGD with momentum reaches a second order stationary point in T = O((1 \u2212 \u03b2) log( Lcm\u03c3 2 \u03c1c c h (1\u2212\u03b2)\u03b4\u03b3 ) \u221210 ) iterations with high probability 1 \u2212 \u03b4. The proof uses Lemma 15 to show that sampling a w from a specific set gives a second order stationary point with high probability. The proof in Lemma 15 shows that by selecting a specific w, a second order stationary point can be reached in T iterations with high probability. The conditions for this to occur involve bounding the increase in function value and ensuring a limited step size. Lemma 15 demonstrates that selecting a specific w can lead to a second order stationary point in T iterations with high probability. The conditions involve limiting the increase in function value and step size. The proof also compares Algorithm 2 with a previous study, showing Algorithm 2's superiority due to its higher momentum. Algorithm 2 is superior to Daneshmand et al. (2018) as it can find a second order stationary point faster with higher momentum."
}