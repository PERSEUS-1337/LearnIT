{
    "title": "Byekm0VtwS",
    "content": "The uncertainty adaptation training scheme (UATS) helps neural networks achieve better performance on neuromorphic computing chips by incorporating uncertainty into the training process. This allows the neural networks to perform comparably on uncertain chips as they do on original platforms, such as CPUs/GPUs. The uncertainty adaptation training scheme (UATS) improves inference performances on neuromorphic computing chips compared to original platforms. Uncertainty reasoning is essential for human thinking and intelligence, with two types of uncertainties: fuzziness and stochasticity. Fuzziness helps the brain efficiently process real-world information by ignoring redundancies, while stochasticity enables creativity and adaptability. These characteristics are lacking in most existing artificial intelligence systems. The 32-bit or 64-bit floating numbers describe weights and activations in AI systems. Some researchers suggest using 8-bit integers for applications. Methods like network quantization and Bayesian networks address issues in AI systems. Neuromorphic computing chips supplement uncertainty in DNN. Nanotechnology devices and crossbar structures have advanced neuromorphic computing chips. Neuromorphic computing chips utilize nanotechnology devices and crossbar structures to enhance AI applications by implementing computing in memory architecture, relieving memory bottlenecks, and improving energy and area efficiency. Neuromorphic computing chips utilize nanotechnology devices and crossbar structures to enhance AI applications by implementing computing in memory architecture, relieving memory bottlenecks, and improving energy and area efficiency. The uncertainty in these chips comes from fuzziness caused by analog to digital converters (ADCs) and stochasticity induced by NVM devices. The VMM result is indicated as the summarization of currents, requiring the use of ADC to convert the analog output. The stochasticity of NVM devices, caused by random particle movement leading to varied conductance, is a key factor affecting neuromorphic computing chip performance. Utilizing this stochasticity in a training scheme can improve chip performance by leveraging non-ideal factors. In a training scheme for neuromorphic computing chips, the stochasticity of NVM devices like phase change memories, filamentary migrating oxide devices, and ferroelectric tunnel junction synapses is utilized to improve performance. The Gaussian distribution is used to model device stochasticity, with the mean representing the conductance value of the stable state and the variance corresponding to the mean. The conductance value of the stable state is represented by the mean of the Gaussian distribution. The variance is typically linked to the mean based on experimental findings. The standard deviation is assumed to be linearly and positively correlated to the mean. Conductances below a minimum value are cut off, with the model assuming a positive real conductance. The conductance of each device in a neuromorphic computing chip is determined by sampling from a Gaussian distribution with a mean and variance. Mapping processes scale weights into the device's conductance range, using the difference between two conductances to express a weight. Lower conductances are preferred for higher energy efficiency. To achieve higher energy efficiency in neuromorphic computing chips, weights are expressed using conductances G pos \u2212 G neg, with lower conductances preferred. However, accurate conductance writing is hindered by device stochasticity and circuit fuzziness, affecting measurement precision. A model is needed to describe this fuzziness in conductance values. The model uses Gaussian distribution to describe fuzziness in conductance values affected by device stochasticity and ADC precision. Uncertainty in neuromorphic computing chips can decrease DNN performance, but can be alleviated. The proposed uncertainty adaptation training scheme (UATS) helps alleviate the decrease in DNN performance caused by uncertainty. It guides neural networks to learn how to handle uncertainty by introducing a stochasticity model in the training process. The uncertainty adaptation training scheme (UATS) introduces a fuzziness model during training to handle uncertainty in neural networks. Weight updates are replaced by random variables, and the loss function is calculated based on the average output of multiple feedforward processes. This approach aims to improve network performance under uncertain conditions. The loss function is calculated based on the average output of multiple feedforward processes with the same input batch. The effect of uncertainty without UATS was evaluated on multiple models and datasets, including the MNIST dataset. Different models were used for training and testing with varying levels of uncertainty. The fuzziness and stochasticity models were used to test uncertainty levels on MLP and CNN models. Without UATS, uncertainty increased test errors, especially affecting the CNN model. The 'mlp2' model was more robust than 'mlp1'. UATS was then used to tune weights and retrain models. The 'mlp2' model proved more robust to uncertainty than 'mlp1'. UATS was used to tune weights and retrain models, showing significant accuracy improvements in both fine-tuning and retraining experiments. The results of retraining with UATS were better than fine-tuning. UATS also showed promising results on the CIFAR-10 dataset with a ResNet-44 DNN model. The UATS method was used with a ResNet-44 DNN model, showing lower error rates with proper hyper-parameters. It acts as a regularization method, especially effective with deeper neural networks. Uncertainty is crucial in intelligent systems, and while Bayesian networks are useful, they require controllable weight distributions. UATS, on the other hand, does not need additional circuitry and is more convenient for neuromorphic computing chips. The UATS method does not require additional circuitry and is more convenient for neuromorphic computing chips. Various distributions were tested to model device stochasticity, but the network performance remains similar with different distributions. The VMM transforms individual device distributions into random parameters for computation. The VMM transforms individual device distributions into random parameters for computation, which may require a large number of random numbers. Methods to reduce this requirement include sampling weights for every input or batch instead of every VMM, and using the uncertainty model of VMM results instead of weights, resulting in accelerated simulation speed with similar outcomes."
}