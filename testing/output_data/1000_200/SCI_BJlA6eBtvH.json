{
    "title": "BJlA6eBtvH",
    "content": "Continual learning involves sequentially acquiring new knowledge while preserving previously learned information. Catastrophic forgetting is a challenge for neural networks in dynamic data scenarios. A Differentiable Hebbian Consolidation model addresses this by combining rapid learning with fixed parameters, allowing retention of learned representations. Task-specific synaptic consolidation methods can be integrated to maintain important weights for each target task. Our method integrates task-specific synaptic consolidation methods to penalize changes in slow weights important for each target task. It outperforms comparable baselines on various benchmarks, including an imbalanced variant of Permuted MNIST. Human intelligence's ability to adapt in dynamic environments is challenging to embed in artificial intelligence. Recent ML advances show improvements by extensive training on large datasets, but most models struggle with forgetting. Continual learning in deep neural networks faces the challenge of catastrophic forgetting, where performance degrades when models are trained with new data after learning is complete. This phenomenon presents a crucial problem for lifelong learning models that aim to adapt to consecutive tasks without forgetting previously learned tasks. Continual learning aims to adapt to consecutive tasks without forgetting previous ones, facing challenges like concept drift and imbalanced class distributions. This leads to the \"stability-plasticity dilemma\" for neural networks, hindering scalability and efficiency over time. Continual learning involves balancing stability and plasticity in neural networks to integrate new knowledge while preserving existing knowledge. Synaptic plasticity is crucial for learning and memory in biological neural networks, with theories explaining how humans achieve continual learning by consolidating important synaptic parameters. The approach to continual learning involves consolidating and preserving important synaptic parameters through task-specific updates of synaptic weights in neural networks. The complementary learning system theory suggests that humans store high-level structural information in different brain areas while retaining episodic memories. Recent work on differentiable plasticity has shown that neural networks can be trained end-to-end to optimize both \"slow weights\" and the amount of plasticity in synaptic connections. Recent work on differentiable plasticity has shown that neural networks can optimize both \"slow weights\" and the plasticity in synaptic connections. Fast weights, representing short-term memory, can reactivate long-term memory traces. Networks with learned plasticity outperform those with uniform plasticity. Approaches have been proposed to overcome catastrophic forgetting by dynamically adjusting synaptic plasticity based on importance. Differentiable Hebbian Consolidation 1 model extends work on differentiable plasticity to task-incremental continual learning. It adapts quickly to changing environments by selectively adjusting synapse plasticity. The model augments slow weights in the final fully-connected layer with plastic weights using Differentiable Hebbian Plasticity. It demonstrates flexibility by combining with task-specific approaches. The Differentiable Hebbian Consolidation model combines plastic weights with Differentiable Hebbian Plasticity to adapt to new data and prevent catastrophic forgetting. It integrates concepts from Hebbian plasticity, synaptic consolidation, and CLS theory to enable rapid adaptation while leveraging compressed episodic memories. Tested on benchmark problems like Permuted MNIST, Split MNIST, and Vision Datasets Mixture. Plastic networks with task-specific synaptic consolidation methods outperform networks with uniform plasticity in benchmark problems like Permuted MNIST, Split MNIST, and Vision Datasets Mixture. Hebbian learning theory suggests that learning and memory are attributed to weight plasticity, where correlated activation of pre-and post-synaptic neurons strengthens the connection between them. Recent approaches in meta-learning literature have shown that incorporating fast weights into a neural network can enable one-shot and few-shot learning. Munkhdalai & Trischler (2018) proposed a model with fast weights implemented using Hebbian learning-based associative memory, while Rae et al. (2018) introduced a Hebbian Softmax layer for improved learning of rare classes. Rae et al. (2018) introduced a Hebbian Softmax layer to enhance learning of rare classes by combining Hebbian learning and SGD updates. Miconi et al. (2018) proposed differentiable plasticity, optimizing synaptic plasticity alongside fixed weights. These methods were effective for training RNNs in pattern memorization and maze exploration tasks but were not tested on the challenge of catastrophic forgetting. Our work addresses the challenge of catastrophic forgetting by using DHP to augment slow weights with fast weights in the FC layer. We update only the parameters of the softmax output layer to achieve fast learning and preserve knowledge over time. Strategies include Task-specific Synaptic Consolidation and CLS Theory, which leverage a dual memory system for retaining memories and rapid learning. The hippocampus performs rapid learning and individuated storage for memorizing new instances or experiences. Regularization strategies inspired by task-specific synaptic consolidation help overcome catastrophic forgetting in continual learning. These approaches estimate the importance of each parameter or synapse, updating network parameters either online or after learning a task. A regularizer is added to the loss function when learning a new task to dynamically adjust plasticity. Regularization strategies prevent forgetting in continual learning by adding a regularizer to the loss function when learning a new task. The importance of each parameter is estimated to adjust plasticity and prevent changes to important parameters of previously learned tasks. Elastic Weight Consolidation (EWC) computes the importance of each parameter using the diagonal of an approximated Fisher information matrix. An online variant of EWC was proposed to improve scalability. Our work draws inspiration from CLS theory, a computational framework for representing memories with a dual memory system. Various approaches based on CLS principles involve methods such as Synaptic Intelligence (SI) and Memory Aware Synapses (MAS) to measure parameter importance in continual learning tasks. Our work focuses on neuroplasticity techniques inspired by CLS theory to address catastrophic forgetting. Previous methods include pseudo-rehearsal, exact replay, and generative replay for memory representation. iCaRL utilizes external memory for rehearsal and regularization. Neuroplasticity techniques inspired by CLS theory aim to alleviate catastrophic forgetting. Previous research has shown how synaptic connections can have fixed slow weights for long-term knowledge and fast-changing weights for temporary memory. Recent studies have explored incorporating fast weights in RNNs, Hebbian Softmax layer, and differentiable plasticity methods. Comparisons with neuroplasticity-inspired CLS methods were not conducted as baselines in this work. The study compared various methods for rapid learning on simple tasks or meta-learning over a distribution of tasks. One method, the Hebbian Softmax layer, adjusts its parameters using a scheduling scheme to achieve fast binding for rarer classes but switches to SGD updates when many examples from the same class are observed frequently. The study focused on metalearning a local learning rule for fast weights in continual learning setups. The model includes slow weights and a Hebbian plastic component in the softmax layer, with a scaling parameter \u03b1 and Hebbian trace Hebb. The model includes fast weights of the same cardinality as the slow weights, with a plasticity coefficient \u03b1 and Hebbian trace Hebb. The \u03b1 scales the Hebb, which accumulates hidden activations for each target label in the mini-batch. The softmax function is applied to obtain predicted probabilities \u0177. The \u03b7 parameter dynamically learns how quickly to acquire new experiences into the plastic component, acting as a learning rate and decay term for the Hebb to prevent instability. The \u03b7 parameter acts as a learning rate and decay term for the Hebb to prevent instability in the plastic connections. The network parameters are optimized by gradient descent as the model is trained on different tasks. In our model, the Hebbian weight update is based on the activation levels of neurons and target classes. Our model utilizes Hebbian weight updates based on activation levels of neurons and target classes. During training, hidden activations are accumulated directly into the softmax output layer weights, leading to better initial representations and improved test accuracy. This approach allows for fast learning and retention of deep representations between tasks. The model utilizes Hebbian weight updates based on neuron activation levels and target classes. Fast learning is enabled by a highly plastic weight component, improving test accuracy. The plastic component decays between tasks to prevent interference, but selective consolidation into a stable component protects old memories. This approach allows the model to learn to remember by modeling plasticity over various timescales, forming a learned neural memory. DHP Softmax has an advantage over external memory as it is simple to implement and scales easily with increasing tasks. The model utilizes Hebbian weight updates for fast learning and improved test accuracy. The plastic component quickly stores memory traces for recent experiences without interference. Hidden activations for the same class are accumulated into one vector, forming a compressed episodic memory. This method enhances learning of rare classes and speeds up binding of class labels to deep representations without introducing any interference. The model utilizes Hebbian weight updates for fast learning and improved test accuracy. It enhances learning of rare classes and speeds up binding of class labels to deep representations without introducing interference. The synaptic consolidation approach regularizes the loss and updates synaptic importance parameters online. Task-specific consolidation approaches are adapted to the model. The model uses Hebbian weight updates for fast learning and better test accuracy, enhancing rare class learning and class label binding without interference. Task-specific consolidation approaches are adapted to the model, with a focus on regularizing loss and updating synaptic importance parameters online. The plastic component of the softmax layer helps prevent catastrophic forgetting by optimizing the plasticity of connections. Our approach uses gradient descent to optimize the plasticity of connections in neural networks for sequential task learning. We compare it with other methods like Online EWC, SI, and MAS, and add slow weights to the softmax layer to match model capacity. Testing on various benchmarks shows improved classification accuracy on previously learned tasks. The study evaluated model performance on sequential tasks, focusing on memory retention, flexibility, and forgetting. Backward transfer metric was used to measure the impact of new tasks on previous ones. Neural networks were trained with Online EWC, SI, and MAS for comparison. In a benchmark to compare task-specific consolidation methods, neural networks were trained with Online EWC, SI, and MAS sequentially on all tasks. The input distribution changes between tasks, leading to concept drift. The benchmarks used permuted MNIST and imbalanced permuted MNIST with MLP networks and a cross-entropy loss. The plastic component's \u03b7 was set to 0.001. The study utilized a multi-layered perceptron (MLP) network with two hidden layers and 400 ReLU nonlinearities, incorporating a cross-entropy loss. The plastic component's \u03b7 was set at 0.001 without extensive tuning. Comparisons were made between the network with DHP Softmax and a fine-tuned vanilla MLP network (Finetune), showing DHP Softmax's effectiveness in reducing catastrophic forgetting. Additionally, the performance with and without DHP Softmax using task-specific consolidation methods was evaluated, demonstrating improved test accuracy with DHP Softmax. The study compared the performance of a network with DHP Softmax to a vanilla MLP network, showing DHP Softmax's effectiveness in reducing catastrophic forgetting. Task-specific consolidation methods with DHP Softmax maintained higher test accuracy during sequential training of tasks. An ablation study examined the network's structural parameters and Hebb traces, showing how synaptic plasticity varied during training on the Permuted MNIST benchmark. The study introduced a method to reduce plasticity after the 3rd task to prevent interference between learned representations. The network leverages the structure in the plastic component, with gradient descent and backpropagation used for meta-learning. The Imbalanced Permuted MNIST problem was introduced, where each task has an imbalanced distribution with artificially removed training samples in each class. The Imbalanced Permuted MNIST problem involves imbalanced class distributions for each task, hindering predictive performance. DHP Softmax achieves 80.85% accuracy after learning 10 tasks sequentially, showing a 4.41% improvement over the standard neural network baseline. The compressed episodic memory mechanism in Hebbian traces allows rare classes to be remembered longer. The DHP Softmax with MAS achieves a 0.04 decrease in BWT, resulting in an average test accuracy of 88.80% and a 1.48% improvement over MAS alone. The MNIST dataset was split into 5 binary classification tasks with disjoint output spaces. The network used was an MLP with two hidden layers of 256 ReLU nonlinearities each. The DHP Softmax achieves a 7.80% improvement on test performance compared to a finetuned MLP network. Combining DHP Softmax with task-specific consolidation consistently decreases BWT and leads to higher average test accuracy across all tasks. Continual learning is performed on a sequence of 5 vision datasets: MNIST, notMNIST, FashionMNIST, SVHN, and CIFAR-10. The MNIST, notMNIST, and FashionMNIST datasets are zero-padded to 32\u00d732 size and replicated 3 times for grayscale images with 3 channels. A CNN architecture similar to previous studies is used with an initial \u03b7 parameter of 0.0001. Training is done with mini-batches of size 32 using plain SGD with a fixed learning rate of 0.01 for 50 epochs per task. Results show that DHP Softmax plus MAS improves average test accuracy by 2.14% over MAS alone. SI with DHP Softmax outperforms other methods with an average test performance of 81.75% and a BWT of -0.04 after learning all five tasks. The addition of compressed episodic memory in the softmax layer through DHP and task-specific updates on synaptic parameters help alleviate catastrophic forgetting in continual learning environments. This allows new information to be learned without interference, improving average test accuracy and generalization across experiences. The \u03b1 parameter in the plastic component of the neural network with DHP Softmax automatically scales the magnitude of plastic connections in Hebbian traces, allowing for generalization across experiences. DHP Softmax shows improvement compared to traditional softmax layers, without introducing extra hyperparameters. The model's flexibility was demonstrated by combining DHP Softmax with Hebbian Synaptic Consolidation methods like EWC, SI, and MAS to alleviate catastrophic forgetting. DHP Softmax with SI outperformed other methods on Split MNIST and 5-Vision Datasets, while combining DHP Softmax with MAS showed superior results on Permuted MNIST and Imbalanced Permuted MNIST benchmarks. The local variant of MAS computes synaptic importance parameters based on Hebb's rule, leading to lower negative BWT and higher test accuracy. Hebbian plasticity enables neural networks to learn continually and remember distant memories, reducing catastrophic forgetting in dynamic environments. Continual synaptic plasticity aids learning from limited labeled data and scaling at long timescales. Continual learning involves training a neural network on a sequence of tasks, each with its own task-specific loss. This approach allows the model to adapt and scale over long timescales, learning from limited labeled data. The goal is to enable gradient descent optimized Hebbian consolidation for learning and memory in deep neural networks. Continual learning involves training a neural network on a sequence of tasks with task-specific losses. The model learns an approximated mapping to the true underlying function and maps new inputs to target outputs for all tasks learned. Experiments were conducted on Nvidia Titan V or RTX 2080 Ti, training with mini-batches of size 64 using plain SGD with a learning rate of 0.01 for at least 10 epochs. Early stopping is performed based on validation. The model is trained on a sequence of tasks with mini-batches of size 64 using plain SGD with a learning rate of 0.01. Training continues for at least 10 epochs with early stopping based on validation error. Hyperparameters are set for different task-specific consolidation methods. In the study, different hyperparameter combinations were tested for various synaptic consolidation methods using a grid search approach. For Online EWC, values of \u03bb were tested in a range, for SI -\u03bb values were tested, and for MAS -\u03bb values were tested. The Imbalanced Permuted MNIST problem involved artificially removing training samples based on random probabilities for each class and task. The distribution of classes in each imbalanced dataset for tasks T n=1:10 is shown in Table 2, with the total number of samples being 30609 and 34515 for the respective tasks. For the Imbalanced Permuted MNIST experiments, regularization hyperparameters were set as \u03bb = 400 for Online EWC, \u03bb = 1.0 for SI, and \u03bb = 0.1 for MAS. Grid search was conducted to find the best hyperparameter combinations for each method. In experiments with different consolidation methods, hyperparameters were tested for EWC, SI, and MAS. For Split MNIST, \u03bb values were set at 400 for EWC, 1.0 for SI, and 1.5 for MAS. Grid search was performed to find optimal hyperparameter combinations for each method. For Online EWC, hyperparameters were tested with values of \u03bb, SI -\u03bb, and MAS -\u03bb. The network was trained on a sequence of 5 tasks with mini-batches of size 64 using plain SGD with a fixed learning rate. The Vision Datasets Mixture benchmark includes MNIST, notMNIST, FashionMNIST, SVHN, and CIFAR-10. The notMNIST dataset contains font glyphs for letters 'A' to 'J', with 60,000 training and 10,000 testing grayscale images. FashionMNIST has 10 clothing categories with 60,000 training and 10,000 testing grayscale images. The CNN architecture includes 2 convolutional layers with 20 and 50 channels, followed by LeakyReLU nonlinearities and max-pooling. An FC layer of size 500 precedes the final softmax output layer. FashionMNIST, SVHN, and CIFAR-10 datasets consist of grayscale or color images for training and testing. The CNN architecture includes 2 convolutional layers with 20 and 50 channels, followed by an FC layer of size 500 before the final softmax output layer. A multi-headed approach was used due to different class definitions between datasets. The model has a trainable \u03b7 value for each connection in the final output layer, improving stability of optimization. Using separate \u03b7 parameters for each connection allows for modulation of plasticity rates. The CNN architecture includes 2 convolutional layers with 20 and 50 channels, followed by an FC layer of size 500 before the final softmax output layer. A multi-headed approach was used due to different class definitions between datasets. The model has a trainable \u03b7 value for each connection in the final output layer, improving stability of optimization. Using separate \u03b7 parameters for each connection allows for modulation of plasticity rates. The regularization hyperparameters for task-specific consolidation methods were set as \u03bb = 100 for Online EWC, \u03bb = 0.1 for SI, and \u03bb = 1.0 for MAS. Sensitivity analysis was performed on the Hebb decay term \u03b7 to evaluate its impact on average test performance in continual learning setups. The analysis focused on the impact of the Hebb decay term \u03b7 on average test performance in continual learning setups. Different values of \u03b7 were tested, with lower values showing better performance in alleviating catastrophic forgetting. Sensitivity analysis was also conducted for the Split MNIST problem. The results are presented in Figure 4 and Table 4, showing the average test accuracy across multiple trials for MNIST-variant benchmarks. The curr_chunk discusses the initialization of learning rates and Hebbian traces for plastic connections in a neural network model. It also includes the initialization of weights and the learning rate parameter. The PyTorch implementation of the DHP Softmax model adds compressed episodic memory to a neural network's final output layer through plastic connections. The model outperforms Finetune on tasks involving the CIFAR-10 and CIFAR-100 datasets. The DHP Softmax model, with compressed episodic memory, outperforms Finetune on CIFAR-100 splits after learning the final task. DHP Softmax performs better than Finetune on each task in class-incremental learning and sometimes even outperforms training from scratch. Test accuracies were compared with Finetune, training from scratch, and SI."
}