{
    "title": "SkE6PjC9KX",
    "content": "Neural Processes (NPs) approach regression by learning to map a context set of observed input-output pairs to a distribution over regression functions. NPs efficiently fit observed data with linear complexity and can learn a wide family of conditional distributions. However, they suffer from underfitting, leading to inaccurate predictions at the inputs of the observed data. To address this issue, attention is incorporated into NPs, allowing each input location to attend to relevant context points for prediction, resulting in improved accuracy, faster training, and an expanded range of functions that can be learned. The approach of using attention in Neural Processes greatly improves prediction accuracy, speeds up training, and allows for modeling a wider range of functions in regression tasks. This method involves computing a distribution over functions that map inputs to outputs, enabling reasoning about multiple functions consistent with the data and capturing co-variability in outputs given inputs. Neural Processes (NPs) offer an efficient method for modeling a distribution over regression functions, with prediction complexity linear in the context set size. They can predict the distribution of an arbitrary target output conditioned on a set of context input-output pairs of any size, allowing them to model data generated from a stochastic process. NPs are trained on samples from multiple realizations of a stochastic process, while Gaussian Processes (GPs) are usually trained on observations from one realization. Neural Processes (NPs) are trained on multiple realizations of a stochastic process, unlike Gaussian Processes (GPs) which are trained on observations from a single realization. NPs can underfit the context set, leading to inaccurate predictions and overestimated variances. This is illustrated in examples such as 1D curve fitting and predicting the bottom half of a face image from its top half. The encoder in an NP aggregates the context set to a fixed-length latent summary, while the decoder maps the latent and target input to the target. The encoder in Neural Processes (NPs) aggregates context information to a fixed-length latent summary, which can lead to underfitting issues. This bottleneck occurs because the mean-aggregation step gives equal weight to all context points, making it challenging for the decoder to determine relevant information for target predictions. Increasing dimensionality alone may not solve this problem, so inspiration is drawn from Gaussian Processes (GPs) to address this issue. In Neural Processes (NPs), the kernel measures similarity between points in the input domain for regression. A similar mechanism to GPs is implemented in NPs using differentiable attention to attend to relevant contexts for a given target. Attentive Neural Processes (ANPs) greatly improve upon NPs in terms of context reconstruction and training speed. ANPs demonstrate enhanced expressiveness and can model a wider range of functions. The Neural Processes (NPs) model regression functions mapping inputs to outputs. It defines conditional distributions based on observed contexts and targets, with permutation invariance. The deterministic NP uses a function to aggregate context pairs into a finite representation. The Neural Processes (NPs) model uses a deterministic function to aggregate context pairs into a finite representation with permutation invariance. The likelihood of the target given context is modelled by a Gaussian factorised across targets, with a global latent variable z accounting for uncertainty in predictions. The Neural Processes (NPs) model uses a factorised Gaussian to model the latent variable z, which is parametrised by s. The encoder and decoder, q, r, s, are used to learn the parameters by maximizing the ELBO for a subset of contexts and targets. Incorporating both deterministic and latent paths allows for a more expressive model and facilitates attention mechanisms. The Neural Processes (NPs) model learns encoder and decoder parameters by maximizing ELBO for a subset of contexts and targets. The model reconstructs targets with a KL term regularization, assuming contexts and targets are from the same data-generating process. NPs offer scalability and flexibility in learning conditional distributions. Neural Processes (NPs) offer scalability and flexibility in learning conditional distributions, with linear scalability and permutation invariance. However, they lack consistency in contexts, as the distribution of targets may not match when generated in different orders. Maximum-likelihood learning minimizes the KL divergence between data-generating process distributions and NP conditional distributions. Neural Processes (NPs) approximate conditional distributions of data-generating processes by minimizing KL divergence. An attention mechanism computes weights for key-value pairs to form query values, with permutation invariance being key for NPs. Differentiable addressing mechanisms have been successfully applied in Deep Learning areas like handwriting generation and neural machine translation. Attention mechanisms have been successfully applied in Deep Learning, particularly in handwriting generation, neural machine translation, natural language processing, and image modeling. Different forms of attention, such as locality-based and dot-product attention, are used to weight key-value pairs based on similarity to queries. Multihead attention is a parametrised extension of dot-product attention, allowing for smoother query-values by attending to different keys for each head. Self-attention is applied to context points to compute representations of each (x, y) pair. In summary, self-attention is used to compute representations of context points, allowing the target input to predict the target output by attending to these representations. The self-attention mechanism helps model interactions between context points, capturing relations and higher-order interactions. The model uses self-attention to capture relations and interactions between context points, allowing each query to focus on relevant context points for prediction. The latent path preserves global dependencies in the stochastic process, while the deterministic path utilizes cross-attention for query-specific representations. The NP with attention combines global structure modeling with local fine-grained structure modeling. The decoder is modified to use query-specific representation. ANP is trained with the same loss function as NP but with increased computational complexity due to self-attention across contexts. The (A)NP has increased computational complexity due to self-attention across contexts, but most computation is done in parallel. Despite being slower at prediction time, ANPs learn significantly faster than NPs in terms of training iterations and wall-clock time. The (A)NP learns a stochastic process and should be trained on multiple functions that are realizations of the process. During training iterations, random points are selected from realisations for targets and contexts to optimize loss. The decoder architecture remains consistent, with 8 heads for multihead. Gaussian Process data is used for regression, with fixed or randomly varying kernel hyperparameters. The number of contexts and targets is randomly chosen at each iteration. During training iterations, random points are selected for targets and contexts to optimize loss. The decoder architecture remains consistent with 8 heads for multihead attention. Gaussian Process data is used for regression with fixed or randomly varying kernel hyperparameters. The number of contexts and targets is randomly chosen at each iteration, leading to a rapid decrease in reconstruction error and lower values at convergence for the ANP compared to the NP, especially for dot product and multihead attention. This difference is observed not only in training iteration but also in wall clock time, indicating fast learning despite the added computational cost. The computation times of Laplace and dot-product ANP are similar to the NP for the same value of d, while multihead ANP takes around twice the time. Raising the bottleneck size in NPs helps achieve better reconstructions, but there is a limit to improvement. Using ANPs has significant benefits over simply increasing the bottleneck size in NPs. In a comparison of attention mechanisms, the predictive means of NP and Laplace underfit the context, while dot-product attention accurately predicts most context points. Laplace attention is parameter-free, using x-coordinates as keys and queries, while dot-product attention uses parameterized representations of x-values for keys and queries. The dot-product similarities are computed in a learned manner. The dot-product attention mechanism, using parameterized representations of x-values for keys and queries, outperforms Laplace attention which computes similarities based on L1 distance in the x-coordinate domain. Multiple heads in multihead attention help smooth out interpolations and improve context reconstruction and target prediction, while maintaining increased predictive uncertainty away from contexts like a GP. The (A)NP trained on fixed GP kernel hyperparameters shows similar results but underfits to a lesser degree due to reduced variety. The (A)NP is more expressive than the NP, as evidenced by its ability to learn a wider range of functions. Trained (A)NPs are used in a toy Bayesian Optimization problem to find the minimum of test functions drawn from a GP prior, showcasing the utility of sampling entire functions and accurate context reconstructions. Regression on image data involves predicting pixel values by mapping pixel locations to pixel intensities. The process is trained on MNIST and CelebA datasets using self-attentional layers in the encoder. Three different models are compared: NP, Multihead ANP, and ANP with multihead attention and stacked self-attention layers. The study compares different models for regression on image data, including NP, Multihead ANP, and Stacked Multihead ANP. The use of attention in the models helps achieve accurate reconstructions of images with varying numbers of context pixels, enhancing the ability to model less smooth 2D functions. The use of attention in the Multihead ANP model improves context reconstruction error and NLL for target points compared to the NP model. The diversity in faces and digits obtained with different z values shows that z can model global image structure. The model can generalize to larger context sizes despite being trained on smaller context points. Multihead ANP improves context reconstruction error and NLL for target points compared to NP. Stacked self-attention shows gains in crispness and global coherence qualitatively. Each head of Multihead ANP in CelebA has different roles in attention focus. The Multihead ANP in CelebA has different roles in attention focus, with each head looking at specific regions of the image. Additionally, ANPs trained on images can map images from one resolution to another by predicting pixel intensities in a continuous space. This allows for mapping a given resolution to a higher resolution, although it may pose challenges for NPs. The Multihead ANP in CelebA can map images from one resolution to a higher resolution by predicting pixel intensities in a continuous space. This process may pose challenges for NPs, as reconstructions can be inaccurate, leading to different target resolutions from the original image. ANPs trained on images, however, may provide reliable mappings between different resolutions. The model can map images to higher resolutions, producing realistic high-resolution images with sharper edges and internal representation of facial features like eyes. The ANP is not meant to replace state-of-the-art algorithms for image inpainting or super-resolution. It showcases the flexibility of the ANP in modeling various conditional distributions. The related works on Gaussian Processes, Meta-Learning, conditional latent variable models, and Bayesian Learning have been extensively discussed in previous works, focusing on those relevant to ANPs. In the context of Gaussian Processes (GPs) and Bayesian Learning, attention in Neural Processes (NPs) is explored. The use of attention in NPs is related to Deep Kernel Learning, where a GP is applied to learned data representations. Training regimes of GPs and NPs differ, making direct comparison challenging. One approach is to learn the GP via the NPs training regime by updating kernel hyperparameters at each iteration. The comparison between Gaussian Processes (GPs) and Neural Processes (NPs) highlights differences in training regimes. GPs rely on kernel choices for predictive uncertainties, while NPs learn uncertainties directly from data. GPs offer consistency as stochastic processes with exact closed-form expressions for predictions, a feature lacking in NPs. Variational Implicit Processes (VIP) are also related to NPs, using a similar decoder setup with finite dimensional z. Processes (VIP) BID19 are related to NPs, using a GP to approximate the process and its posterior. Meta-Learning (A)NPs focus on few-shot learning, with works using attention for tasks in Meta-RL. Few-shot density estimation with attention has also been explored. Attention has been used for tasks in Meta-RL such as continuous control and visual navigation. Few-shot density estimation using attention has also been explored extensively in numerous works. The Neural Statistician and the Variational Homoencoder have a similar permutation invariant encoder but use local latents on top of a global latent. Multitask learning in the GP literature has been tackled by various works. Generative Query Networks are models for spatial prediction. In the GP literature, Generative Query Networks (GQN) are models for spatial prediction that render a scene given a viewpoint. ANPs augment NPs with attention to address underfitting issues, improving prediction accuracy and training speed. Future work for ANPs includes exploring different model architectures. Future work for Attentive Neural Processes (ANPs) involves incorporating cross-attention into the latent path, exploring model architectures like the Neural Statistician setup, and potentially training ANPs on text data. The Image Transformer (ImT) BID21 shows connections with ANPs, particularly in how both models attend to context information to predict target outputs. By replacing the MLP in the decoder of ANPs with self-attention across target pixels, a model similar to ImT can be achieved. Replacing the MLP in the decoder of the ANP with self-attention across target pixels creates a model resembling an Image Transformer (ImT) defined on arbitrary pixel orderings. The targets in this setup will influence each other's predictions, making target ordering and grouping important. Architectural details of the NP and Multihead ANP models used for regression experiments are shown in Figure 8. The decoder in the ANP is replaced with self-attention for regression experiments using multihead cross-attention. The model resembles an Image Transformer and does not use dropout to limit stochasticity to the latent z. Self-attention and cross-attention have similar architectures but differ in their input representations. The Stacked Multihead ANP uses self-attention with k i = v i , q = k j for each j \u2208 C to output |C| representations. Two layers of self-attention are stacked in 2D Image regression experiments. Different kernel hyperparameters are used for fixed and random cases, with a batch size of 16. In the setting, 16 curves are drawn from a GP with specific hyperparameters, while in the random hyperparameter setting, 16 random values are sampled. The Adam Optimiser BID14 with a fixed learning rate is used, along with Tensorflow defaults for other hyperparameters. One sample of q(z|s C ) is used to form a MC estimate of the loss. The trained (A)NP models are compared against the oracle GP, showing that the Multihead ANP is closer to the oracle GP but underestimates predictive variance. Variational inference used for learning the ANP typically leads to underestimates of predictive variance. Further investigation is needed to address this issue. The conditional distributions for fixed and random GP kernel hyperparameters show non-smooth behavior in dot-product attention. This behavior occurs when dot-product attention collapses to a local minimum, leading to good reconstructions but poor interpolations between context points. The KL term in NP loss differs between training on fixed and random kernel hyperparameter GP data. Further investigation is needed to address the issue of underestimating predictive variance in Variational Inference. The KL term in the (A)NP loss differs between training on fixed and random kernel hyperparameter GP data. In the fixed hyperparameter case, the model deems the deterministic path sufficient for accurate predictions, while in the random hyperparameter case, the attention gives a non-zero KL to model uncertainty in the stochastic process. The model uses latents to account for variations in explaining context points. The (A)NPs trained on 1D GP data are used to tackle the BO problem of finding the minimum of test functions drawn from a GP prior. The (A)NPs trained on 1D GP data use latents to model variation and tackle the BO problem of finding the minimum of test functions from a GP prior. ANPs with multihead attention show the smallest simple regret, approaching the oracle GP, in Thompson sampling for function selection. The smallest simple regret is observed for a NP with multihead attention, approaching the oracle GP. The cumulative regret decreases rapidly for multihead, indicating effective use of previous function evaluations for predicting the function minimum. The lower cumulative regret initially compared to the oracle GP is due to under-exploration, as uncertainties of ANP are smaller away from the context. Random pixels of an image are taken as targets during training, with a subset chosen as contexts. Batch size of 16 is used for both MNIST and CelebA datasets, with a learning rate of 5e-5 and 4e-5. The stacked self-attention architecture used for both MNIST and CelebA datasets does not include Dropout or positional embeddings of pixels. The NP overestimates predictive variance, as shown in the plot of standard deviation. The NP overestimates predictive variance, but the NP with attention reduces uncertainty significantly. Stacked Multihead ANP improves results over Multihead ANP, providing sharper images with better global coherence. Different heads play various roles in accurate predictions, even when the target is disjoint from the context. In the NP with attention, different heads play various roles in accurate predictions, even when the target is disjoint from the context."
}