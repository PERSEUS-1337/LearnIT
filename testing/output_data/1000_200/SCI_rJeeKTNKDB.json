{
    "title": "rJeeKTNKDB",
    "content": "Our work in this paper extends prior state-of-the-art on graph-to-graph translation methods for molecular optimization by creating coherent multi-resolution representations. The graph decoder is fully autoregressive and outperforms previous baselines in multiple molecular optimization tasks. Molecular optimization involves modifying compounds to enhance their biochemical properties through graph-to-graph translation. The task is challenging due to the vast candidate space and complex dependencies in molecular properties. Prior work utilized a junction tree encoder-decoder for generating molecular graphs. The architecture for generating molecular graphs proposed a junction tree encoder-decoder using valid chemical substructures. However, the approach had limitations such as separate tree and graph encoding, non-autoregressive attachment prediction leading to inconsistent substructure attachments. Our proposed multi-resolution, hierarchically coupled encoder-decoder for graph generation is non-autoregressive and interleaves the prediction of substructure components with their attachments. The encoder represents molecules at different resolutions, while the decoder predicts triplet sequences for expanding the graph, new substructure types, and attachments. This approach allows for modeling strong dependencies between successive attachments and substructure choices. The decoding process in our proposed method decomposes generation steps into smaller hierarchically coupled levels to avoid combinatorial explosion. This approach enables efficient handling of conditional translation criteria and prevents the generation of invalid junction trees by interleaving tree and graph decoding steps. Our proposed method interleaves tree and graph decoding steps to solve the issue of trees that cannot be assembled into molecules. We introduce an autoregressive decoder that predicts substructures with their attachments, outperforming previous methods in molecular optimization tasks. Our model shows significant improvements in discovering molecules with desired properties, running 6.3 times faster during decoding. Our model shows significant improvements in discovering molecules with desired properties, yielding 3.3% and 8.1% improvement on QED and DRD2 optimization tasks. It runs 6.3 times faster than previous substructure-based generation methods. Ablation studies validate the advantage of hierarchical decoding and multi-resolution encoding. Conditional translation can succeed even when trained on molecular pairs with only 1.6% having desired target property combination. Various approaches have been adopted for generating molecular graphs, with generative models outputting the adjacency. Various approaches have been adopted for generating molecular graphs, with generative models outputting the adjacency matrices and node labels of the graphs at once. Different methods have been proposed for molecule generation, including node-by-node approaches and hypergraph grammar based methods. Jin et al. (2018) utilized a two-stage procedure for realizing graphs, generating molecules based on substructures. Graph neural networks have been extensively studied for graph encoding, with various methods related to graph encoders for molecules. The second step in generating molecular graphs introduces local independence assumptions and lacks autoregressive decoding, unlike our method which jointly predicts substructures and attachments with an autoregressive decoder. Our method represents molecules as hierarchical graphs spanning from atom-level graphs to substructure-level trees, closely related to previous works that learn to represent graphs hierarchically. The encoder captures molecule information across three levels (atom layer, attachment layer, and substructure layer), while the decoder adds new substructures and predicts their attachment to the current graph. Our method focuses on graph generation by encoding molecules into multiple sets of vectors at different resolutions. These vectors are aggregated by decoder attention modules in each generation step. The goal is to learn a function that maps a molecule into another with improved chemical properties, using an encoder-decoder with neural attention. Our method utilizes an encoder-decoder with neural attention to generate molecules with improved chemical properties. The decoder adds new substructures and determines how they should be attached to the current graph. This hierarchical generation is supported by a matching encoder that represents molecules at multiple resolutions, using a hierarchical graph with substructure, attachment, and atom layers. The model utilizes an encoder-decoder with neural attention to generate molecules with improved chemical properties. The encoder represents molecules at multiple resolutions with substructure, attachment, and atom layers. Substructures are defined as subgraphs of molecules induced by atoms and bonds. The paper discusses extracting substructures from a molecule to form a substructure tree, connecting substructures based on shared atoms. A graph decoder then generates a molecule by expanding the substructure tree in a depth-first order. The graph decoder generates a molecule by incrementally expanding its substructure tree in a depth-first order. It predicts new substructures and how they should be attached to the graph, moving to the next substructure and repeating the process. The decoder first predicts whether a new substructure will be attached to the current node, using a MLP with attention over substructure vectors. If so, a new substructure is created. The model predicts the probability of creating a new substructure and its attachment to the current node in a molecule. It uses a MLP with attention over substructure vectors to make these predictions. If the probability is above 0.5, a new substructure is created and its type is predicted using another MLP. The attachment between the new and current substructures is determined by predicting atom pairs that are attached together. The model predicts the probability of creating a new substructure and its attachment to the current node in a molecule using a classification task. The predicted attaching points are used to find corresponding atoms in the substructure, and the probability of a candidate attachment is computed based on atom representations. These predictions give an autoregressive factorization of the distribution over the next substructure and its attachment, with each step depending on the outcome of the previous step. During training, teacher forcing is applied to the generation process. The model uses teacher forcing during training to predict attachments for substructures in a molecule. The encoder represents the molecule as a hierarchical graph with atom and attachment layers. The attachment enumeration is tractable due to small substructure sizes. The model utilizes teacher forcing to predict attachments for substructures in a molecule, represented as a hierarchical graph with atom, attachment, and substructure layers. The attachment layer provides information for attachment prediction, while the substructure layer is similar to the attachment layer. The substructure layer in the hierarchical graph provides essential information for substructure prediction in the decoding process. Edges connect atoms and substructures between layers to propagate information. The hierarchical graph is encoded by a hierarchical message passing network (MPN) with three layers encoded by MPNs. The hierarchical graph is encoded using a hierarchical message passing network (MPN) with three layers: Atom Layer MPN, Attachment Layer MPN, and MPN architecture from Jin et al. (2019). The Atom Layer MPN encodes atom representations by propagating message vectors between atoms. The Attachment Layer MPN encodes node and edge features based on embeddings and relative ordering between nodes. The hierarchical encoder uses message passing to compute substructure representations at different layers. The output is a set of vectors representing a molecule at multiple resolutions, which are then used in the decoder attention. During decoding, the hierarchical MPN architecture encodes the hierarchical graph at each step, providing substructure and atom vectors for future nodes. The hierarchical MPN architecture encodes the hierarchical graph at each step to generate diverse outputs for molecule translation. Training sets contain molecular pairs where each compound can have multiple outputs for property improvement. A variational translation model is used with an additional input z to indicate the intended mode of translation, sampled from a Gaussian prior during testing. The model is trained using variational inference. During training, the model uses variational inference and encodes structural changes from molecule X to Y to generate a latent code z for translation. The decoder reconstructs output Y using z and input representation c X. The model lacks knowledge of optimized properties during translation, posing a limitation during testing. During testing, users cannot change the behavior of a trained model, which may limit multi-property optimization. To address this, the method is extended to handle conditional translation by incorporating desired criteria as input. This involves computing \u00b5 X,Y and \u03c3 X,Y with an additional input g X,Y during variational inference. The latent code is augmented as [z, g X,Y] and passed to the decoder, allowing users to specify criteria to control the outcome during testing. The translation model is evaluated on single-property optimization tasks following an experimental design by Jin et al. (2019). The translation model, based on Jin et al. (2019), is evaluated on single-property optimization tasks. A novel conditional optimization task is constructed where desired criteria are inputted to prevent arbitrary compound translation. Molecular similarity between input X and output Y must meet a threshold at test time. The model is trained on four different tasks, including LogP Optimization, without g X,Y as input. The model is trained on LogP Optimization tasks with two similarity thresholds. It aims to improve properties of compounds during translation, such as drug-likeness and activity. Different criteria can be encoded as a vector g for conditional translation. The model is trained on LogP Optimization tasks with two similarity thresholds to improve compound properties during translation. Evaluation metrics include translation accuracy, diversity, and success rate based on similarity and property constraints. The HierG2G method is compared against baselines such as GCPN, MMPA, Seq2Seq, JTNN, and CG-VAE. A direct comparison is made with an atom-based translation model (AtomG2G). The evaluation metrics include translation accuracy, diversity, and success rate based on similarity and property constraints. Our model, HierG2G, achieves state-of-the-art results on four translation tasks, outperforming JTNN in translation accuracy. The AtomG2G baseline model predicts atom and bond types in each generation step using an atom-based approach. Our model, HierG2G, outperforms JTNN and AtomG2G in translation accuracy and output diversity on four translation tasks. It runs 6.3 times faster than JTNN during decoding and shows over 10% improvement on the DRD2 task. The hierarchical model achieves better results compared to other translation methods like Seq2Seq, JTNN, and AtomG2G. Our model, HierG2G, outperforms other models in translation accuracy and output diversity. Training on examples with strong constraints yields low success rates, but our conditional translation setup can transfer knowledge effectively. Ablation studies show the benefits of structure-based decoding in improving performance on tasks like QED and DRD2. The HierG2G model shows improved performance with structure-based decoding in tasks like QED and DRD2. Modifications to the decoder input, such as including atom and substructure vectors, result in performance decreases on the tasks. Removing hierarchies in the encoder and decoder affects translation accuracy, with the top substructure layer removal causing a slight drop and further removal leading to significant performance degradation. This is due to the loss of substructure information, requiring the model to infer substructures and their construction for each molecule. In this paper, a hierarchical graph-to-graph translation model is developed that generates molecular graphs using chemical substructures as building blocks. The model is fully autoregressive and learns coherent multi-resolution representations, outperforming previous models in various settings. The LSTM MPN architecture is used for both HierG2G and AtomG2G baseline, with experimental results showing superior performance. The message passing network MPN \u03c8 over graph H is defined as Algorithm 3 LSTM MPN with T message passing iterations for all edges in H simultaneously. Our attention layer is a bilinear attention function with parameter \u03b8. AtomG2G is an atom-based translation method directly comparable to HierG2G. AtomG2G is an atom-based translation method directly comparable to HierG2G, focusing on molecular graph representation. Training set sizes and substructure vocabulary are detailed in Table 3. Multi-property optimization combines QED and DRD2 datasets. Hyperparameters for HierG2G include hidden layer dimension of 270 and embedding layer dimension of 200. For HierG2G, the hidden layer dimension is set to 270, embedding layer dimension to 200, latent code dimension |z| = 8, and KL regularization weight \u03bb KL = 0.3. T = 20 iterations of message passing are run in each layer of the encoder. AtomG2G has hidden layer and embedding layer dimensions set to 400, \u03bb KL = 0.3, and T = 20 message passing iterations. Both models are trained with Adam optimizer. CG-VAE models are used for molecule generation and property prediction tasks. Three CG-VAE models are created for logP, QED, and DRD2 optimization tasks. At test time, compounds are translated using gradient ascent in the latent space to maximize property scores. A low KL regularization weight (\u03bb KL = 0.005) is found to be necessary for meaningful results. Ablation studies show that changing the decoder to AtomG2G's atom-based decoder affects the molecule generation process. In experiments, the decoder was changed to AtomG2G's atom-based decoder, modifying input attention and reducing hierarchies in the encoder and decoder MPN. Two-layer and one-layer models were tested, adjusting hidden layer dimensions to match the original model size."
}