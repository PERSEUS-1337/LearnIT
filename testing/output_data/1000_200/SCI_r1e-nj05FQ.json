{
    "title": "r1e-nj05FQ",
    "content": "Multi-agent cooperation is crucial in various organisms, despite individual incentives conflicting with the common good. The study focuses on intertemporal social dilemmas (ISDs) in multi-agent reinforcement learning (MARL) and evolutionary theory. By combining MARL with natural selection, a model-free way to learn cooperation biases is demonstrated using a modular architecture for deep reinforcement learning agents. The study introduces a modular architecture for deep reinforcement learning agents to support multi-level selection in achieving cooperation. It discusses the prevalence of cooperation in nature despite individual selfish interests, and explores various mechanisms such as kin selection, reciprocity, and group selection. The emergence of cooperation among self-interested agents is a key topic in multi-agent deep reinforcement learning. Social dilemmas involve a trade-off between collective welfare and individual utility, where self-interested agents tend to converge to defecting strategies. The goal is to find training regimes where cooperation emerges, with solutions falling into categories like opponent modeling. Evolution can be applied to remove hand-crafted intrinsic motivation in training regimes for resolving social dilemmas. This approach contrasts with end-to-end model-free learning algorithms, showing greater generalization ability. Evolution has been used in deep learning for optimizing hyperparameters, implementing black-box optimization, evolving neuroarchitectures, and modifying various functions. Evolutionary simulations have been used to optimize hyperparameters, implement black-box optimization, evolve neuroarchitectures, and modify functions. These principles are driven by single-agent search, competitive multi-agent tasks, and evolutionary simulations of predator-prey dynamics. The proposed system distinguishes between optimization processes unfolding over fast and slow time-scales in intertemporal social dilemmas. The curr_chunk discusses the implementation of an intrinsic reward function in evolutionary simulations to address intertemporal social dilemmas. The model uses a neural network to define the genotype for evolution and aims to bridge the gap between fast and slow time-scales. The goal is to structure evolutionary dynamics to achieve altruistic behavior in a population. To achieve altruistic behavior in a population, the implementation of a \"Greenbeard\" strategy is used, where agents choose partners based on signals of cooperativeness. Assortative matchmaking is introduced, but it has limitations in explaining cooperation across all taxa. To address this, a modular training scheme called shared reward network evolution is proposed, inspired by multi-level selection theory. Agents consist of two neural network modules in this approach. In shared reward network evolution, agents have policy and reward neural network modules that evolve separately. Policy networks are trained using modified rewards from the reward network on a fast timescale, while the reward network's fitness is based on the collective return of the group on a slow timescale. This approach aligns with multi-level selection theory, with policy networks as lower level units and reward networks as higher level units. The policy networks are lower level units of evolution, while reward networks are higher level units. Evolving them separately prevents overfitting and suggests a potential mechanism for social inductive biases. Various parameters were explored in different environments and reward network features within a MARL setting. In intertemporal social dilemmas, selfish actions benefit individuals in the short term but harm the group in the long term. Two dilemmas are studied on a 2D grid, including the Cleanup game where agents collect apples that spawn based on the cleanliness of an aquifer. The aquifer fills with waste over time, reducing apple respawn until none can spawn. In the Harvest game, agents face a dilemma between harvesting all apples quickly for short-term gain or preserving apples for long-term group yield. The apple spawn rate depends on nearby apples, falling to zero when none are left in a certain radius. This dilemma arises because agents must leave the apple field to clean, which offers no reward. If all agents refuse to clean, no rewards are received. In the model, rewards for agents consist of total, extrinsic, and intrinsic rewards. Extrinsic reward is obtained from the environment, while intrinsic reward is based on social preferences and calculated using a neural network with evolved parameters. The ReLU activation function is used in a 2-layer neural network with evolved parameters based on fitness. The parameters correspond to coefficients related to inequity aversion. Each agent has access to the same set of features, with their own feature demarcated specially. Features are based on received or expected future rewards, which may not be aligned in time in Markov games. In Markov games, rewards for players may not align in time, so social preference models should compare temporally averaged reward estimates. Two methods of aggregating rewards were considered. The architecture includes intrinsic and extrinsic value heads, a policy head, and evolution of the reward network. The retrospective method derives intrinsic reward from judging if other agents were rewarded recently. The architecture includes intrinsic and extrinsic value heads, a policy head, and evolution of the reward network. In the retrospective method, intrinsic reward is derived from judging if other agents were rewarded recently. The prospective variant derives intrinsic reward from agents expecting to be rewarded in the near future. The training framework used distributed asynchronous training in multi-agent environments with a population of 50 agents. The training framework involved population-based training (PBT) with 50 agents, where 5 players were sampled for each of 500 arenas running in parallel. Agents observed their last actions and rewards as input to the LSTM in their neural network for learning.Weights were updated using V-Trace, and the set of weights evolved included learning rate, entropy cost weight, and reward network weights. The training framework utilized population-based training with 50 agents observing their last actions and rewards as input to the LSTM in their neural network. The objective function consisted of the value function gradient, policy gradient, and entropy regularization, weighted by hyperparameters baseline cost and entropy cost. Evolution was based on a fitness measure calculated as a moving average of total episode return, with matches determined by random or assortative matchmaking methods. Evolution in the training framework involved random or assortative matchmaking methods based on agents' recent cooperativeness. Cooperative matchmaking grouped agents with similar rankings to ensure highly cooperative agents played with each other, while defecting agents played with other defectors. Cooperativeness was calculated differently for Cleanup and Harvest scenarios. Evolution in the training framework involved cooperative metric-based matchmaking based on agents' recent cooperativeness. This approach allowed for independent exploration of hyperparameters and the evolution of reward networks within their own population. This method enabled different modules of the agent to compete only with like components, leading to a wider exploration of the hyperparameter landscape compared to using a single pool. The training framework involved cooperative matchmaking based on agents' recent cooperativeness, allowing for independent exploration of hyperparameters and reward networks. Five policy networks were paired with a shared reward network in each episode, evolving based on individual agent return. This approach differs from previous work by evolving over social features rather than environmental events. Evolution of reward networks in a social setting is motivated by dealing with inherent tension in ISDs, resembling a form of communication for social cooperation. Shared reward networks mix group fitness on a long timescale with individual reward on a short timescale, contrasting with hand-crafted aggregation methods. Performance without intrinsic reward network is poor, as shown in FIG3, highlighting the importance of shared components in a social setting. The use of intrinsic reward networks performs poorly on both Cleanup and Harvest games, with total episode rewards asymptoting to 0 and 400, respectively. Comparing random and assortative matchmaking with PBT and reward networks using retrospective social features, it is found that individual reward network agents do not outperform PBT on Cleanup and only slightly on Harvest. Adding reward networks over social features offers little benefit if players have separate networks evolved selfishly. Assortative matchmaking experiments show that without a reward network, performance is the same as the PBT baseline, but with individual reward networks, performance is significantly higher, indicating the importance of conditioning internal rewards on social features and a preference for cooperative agents to play together. The use of shared reward networks in agents performs as well as assortative matchmaking and handcrafted inequity aversion intrinsic reward. Agents do not necessarily require immediate access to honest signals of cooperativeness to resolve the dilemma; having the same intrinsic reward function is sufficient. The prospective variant of reward network evolution results in worse performance and more instability compared to the retrospective variant. The prospective variant of reward network evolution generally results in worse performance and more instability compared to the retrospective variant. Social outcome metrics such as sustainability and equality are plotted to capture the complexities of agent behavior. Having no reward network leads to quick apple collection, while reward networks promote more sustainable behavior. The prospective version of reward networks leads to lower equality, while the retrospective variant has high equality. Tagging is higher with prospective or individual reward networks compared to a retrospective shared reward network. The weights of the final shared reward networks evolved differently for each game, suggesting varying social preferences are needed for resolution. In Cleanup, the final layer weights evolved differently compared to Harvest, indicating the need for varying social preferences for each game. Cleanup required a less complex reward network, while Harvest needed a more intricate one to prevent over-exploitation of resources. Random matchmaking led to arbitrary values in the first layer weights due to little evolutionary pressure for specialization. Organisms in real environments do not receive scalar reward signals to learn from. In real environments, organisms do not receive scalar reward signals to learn from. Evolutionary pressure for specialization in weights is low, leading to the need for varying social preferences in different scenarios. Implementing natural selection via genetic algorithms did not result in cooperation, but assortative matchmaking could generate cooperative behavior. A new evolutionary paradigm based on shared reward networks promotes cooperation in general situations by evolving intrinsic social preferences. Evolution promotes cooperation by improving credit assignment between selfish acts and negative group outcomes, exposing social signals correlated with selfishness, and enabling mechanisms like competitive altruism and inequity aversion. Humans cooperate more when communication is possible. The shared reward network evolution model is inspired by multi-level selection but differs in its approach. The shared reward network evolution model, inspired by multi-level selection, involves policy networks constantly swapping with a reward network. This modularity is seen in nature with examples like microorganisms forming multi-cellular structures and prokaryotes incorporating plasmids for cooperation. In humans, a reward network may represent a shared \"cultural norm\" based on accumulated cultural information. The spread of cultural norms in humans may be represented by a reward network, independent of individual success. Future research could explore alternative evolutionary mechanisms for cooperation, such as kin selection and reciprocity, to understand the evolutionary origins of social biases. Additionally, studying an emergent assortative matchmaking model and combining evolutionary approaches with multi-agent communication could provide further insights. The games Cleanup and Harvest have episodes lasting 1000 steps with a playable area size of 25\u00d718 and 36\u00d716 respectively. Agents can only observe a 15\u00d715 RGB window and have actions like moving, rotating, tagging, and cleaning waste. Training involved joint optimization of network parameters via SGD and evolution of hyperparameters/reward network parameters in a standard PBT setup. The training process involved joint optimization of network parameters through SGD and evolution of hyperparameters/reward network parameters in a standard PBT setup. Gradient updates were applied for every trajectory up to a maximum length of 100 steps, using a batch size of 32. Optimization was done via RMSProp with specific parameters, and the learning rates were allowed to evolve. PBT utilized genetic algorithms to search over hyperparameters, resulting in an adaptive schedule and joint optimization with network parameters learned through gradient descent. The training process involved joint optimization of network parameters through SGD and evolution of hyperparameters/reward network parameters in a standard PBT setup. A mutation rate of 0.1 was used when evolving hyperparameters, with perturbations for entropy cost, learning rate, and reward network parameters. A burn-in period of 4 \u00d7 10 6 agent steps was implemented before evolution to ensure accurate fitness assessment."
}