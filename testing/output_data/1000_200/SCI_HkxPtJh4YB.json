{
    "title": "HkxPtJh4YB",
    "content": "We introduce Sinkhorn variational marginal inference as a scalable alternative for exponential family inference over permutation matrices. This method is justified by the Sinkhorn approximation of the permanent, addressing the intractability issue as permutation size increases. The effectiveness of this approach is demonstrated in probabilistic neuron identification in C.elegans. The Frobenius matrix inner product, log L is a parameter matrix and Z L is the normalizing constant. Sinkhorn variational marginal inference is introduced to compute the matrix of expectations \u03c1 efficiently. The Sinkhorn operator applied to L approximates \u03c1, resulting in a doubly stochastic matrix. This approach is justified by the Sinkhorn approximation of the permanent, addressing intractability in probabilistic neuron identification in C.elegans. The Sinkhorn approximation is used for probabilistic inference of neural identity in C.elegans. It is based on the relation between marginal inference and the normalizing constant in exponential families. The dual function and matrix of marginals are key components in this approach. The Sinkhorn approximation links marginal inference and the normalizing constant in exponential families. The approximation replaces the intractable dual function with component-wise entropy, resulting in the Sinkhorn permanent. This method is used for probabilistic inference of neural identity in C.elegans. The Sinkhorn permanent is an approximation of the normalizing constant using component-wise entropy. Bounds for this approximation are provided, and it has been proposed independently by Powell and Smith. The Bethe variational inference method is a general rationale for obtaining variational approximations in graphical models. This method has been successfully applied to permutations in various studies. The Bethe approximation method has been successfully applied to permutations, providing better theoretical guarantees than the Sinkhorn approximation. Computational differences exist between the two methods, with the Bethe approximation requiring more complex message computations. In practice, the Bethe approximation produces better permanent approximations. In practice, the Bethe approximation produces better permanent approximations compared to the Sinkhorn approximation. The Sinkhorn approximation, however, often produces qualitatively better marginals by putting more mass on non-zero entries. Additionally, the Sinkhorn approximation scales better for moderate values of n, with faster iteration times compared to Bethe. The Sinkhorn approximation has faster iteration times compared to Bethe, with each iteration taking only 0.0027 seconds. Comparison of approximations using 1,000 submatrices of size n = 8 from the C.elegans dataset showed differences in log permanent and mean absolute errors of log marginals. Sampling-based methods can also be used for marginal inference, with sophisticated samplers proposed for polynomial approximability of the permanent. In section 3, an elementary MCMC sampler failed to produce sensible marginal inferences for the worm C.elegans due to technical challenges in identifying neuron labels. Recent advances in neurotechnology allow for whole brain imaging to study the relationship between worm brain activity and behavior. In the context of NeuroPAL, a methodology for probabilistic neural identification in C.elegans, the goal is to assign canonical labels to observed neurons represented as vectors in R6. Gaussian models are used for each canonical neuron, with parameters inferred from previously annotated worms. These probabilities provide uncertainty estimates for model predictions, offering a more complete picture than point estimates. In NeuroPAL, canonical neuron labels are assigned using gaussian models with parameters inferred from annotated worms. A posterior over permutations is induced, with a downstream task involving manual labeling of uncertain neurons based on probabilistic neural identifies. The probabilistic neural identifies uncertain neurons for manual labeling, leading to increased accuracy with human annotations. Sinkhorn, Bethe, MCMC, and random baselines are compared for approximation quality. The Sinkhorn and Bethe approximations outperform MCMC and other baselines in marginal inference accuracy, with Sinkhorn slightly better. MCMC shows no improvement over a naive baseline, indicating convergence issues. The Sinkhorn approximation for marginal inference is a faster and more accurate alternative to sampling compared to the Bethe approximation. Future work will analyze the relationship between permanent approximation quality and corresponding marginals. The (log) Sinkhorn approximation of the permanent of L can be obtained by evaluating S(L) in the problem it solves. The dataset used in the study consists of ten NeuroPAL worm heads with human labels, each summarized through a log-likelihood matrix. Sinkhorn and Bethe approximations were used with 200 iterations, leading to computation times described in Fig 1. MCMC sampler method from Diaconis (2009) was used with 100 chains of length 1000. The MCMC sampler method described in Diaconis (2009) was used with 100 chains of length 1000. An efficient log-space implementation of the message passing algorithm from Vontobel (2013) was utilized. Error bars were omitted due to their insignificance. Multiple submatrices of size n were randomly drawn from the log likelihood C.elegans matrices."
}