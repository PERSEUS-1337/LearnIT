{
    "title": "Bye5OiR5F7",
    "content": "A new method for training GANs is introduced using the Wasserstein-2 metric proximal on the generators. The approach is based on the gradient operator induced by optimal transport in implicit deep generative models, providing a regularizer for parameter updates. Experiments show improved speed and stability in training GANs in terms of wall-clock time and FID learning curves. The generator in implicit generative models aims to recreate the density distribution from the real source by minimizing a discrepancy measure like the Wasserstein distance. This approach has been used in defining loss functions for learning generative models, particularly in Wasserstein GANs. The Wasserstein GAN BID4 is of interest for deep generative models in GANs. The Wasserstein steepest descent flow is derived using the Wasserstein-2 metric function, providing a Riemannian structure and natural gradient advantages over the Euclidean gradient. In GANs, the Fisher-Rao natural gradient is problematic due to low dimensional support sets and difficulties with KL divergence. To address this, we propose using the gradient operator induced by the Wasserstein-2 metric. The proximal operator for GAN generators can be computed with the squared constrained Wasserstein-2 distance as regularization, approximated by a neural network. The constrained Wasserstein-2 metric simplifies computation in implicit generative models. We introduce a relaxed proximal operator for generators, simplifying parameter updates and making it easy to implement as a regularizer. The proximal operator involves simple parameter updates based on output differences, making it easy to implement as a regularizer for GAN generators. The paper introduces a Wasserstein proximal method and demonstrates its effectiveness in various GAN experiments. Additionally, optimal transport and its proximal operator on a parameter space are briefly discussed, applied to GAN optimization problems. The Wasserstein-2 distance, denoted as W, is a metric that transports probability densities \u03c10 to \u03c11 with minimal kinetic energy. This paper extends the classic theory to cover parameterized density models, where a parameterized probability \u03c1(\u03b8, x) is constrained within a parameter space \u0398 \u2282 Rd. The Wasserstein-2 metric function on parameter space \u0398 is defined by a constrained formulation involving Borel potential functions and continuous parameter paths. This metric differs from the full density set Wasserstein-2 distance and can be utilized for steepest descent optimization schemes based on Riemannian structures like the Fisher natural gradient. The constrained Wasserstein-2 metric allows for a Riemannian metric structure, leading to the Wasserstein natural gradient. The Wasserstein gradient operator for a loss function F on parameter space \u0398 is defined by a matrix G representing the Wasserstein Riemannian metric. The steepest descent flow is determined by this gradient operator. The steepest descent flow in Wasserstein metric is determined by the gradient operator \u2207 \u03b8 and matrix G. The gradient descent iteration uses the forward Euler method with step size h. Another numerical scheme, the backward Euler method or JKO scheme, involves the proximal operator. Approximating the distance update d W locally with a second order Taylor expansion is useful in the parameterized setting. The Semi-Backward Euler method for the gradient flow of loss function F in a parameterized setting is derived by approximating the distance update locally. It is easier to implement than the forward Euler method and simpler than the backward Euler method (JKO). The Semi-Backward Euler method is used in implicit generative models to compute dW for each parameter \u03b8 \u2208 Rd. The generator function g\u03b8 maps noise prior Z to output samples with density X. The constrained Wasserstein-2 metric simplifies the formulation and introduces a proximal operator algorithm for generators. The constrained Wasserstein-2 metric introduces a proximal operator algorithm for generators, involving a gradient constraint on the generator function with respect to the parameter space. The gradient constraint is challenging to fit, leading to the consideration of a relaxed Wasserstein metric on the parameter space for simpler computations. The relaxed Wasserstein proximal operator is approximated based on a new metric in parameter space, regularizing the generator by the expectation of squared difference in sample space. Algorithm 1 outlines the steps for the Relaxed Wasserstein Proximal, with parameters and optimizers specified for iterations and batch size. The effectiveness of Wasserstein proximal operator in GANs is illustrated using a toy example with a family of distributions. The proximal regularization for a loss function is defined, and various statistical distance functions are compared, including Wasserstein-2 distance and Euclidean distance. KL divergence and L2-distance are found inadequate for measuring differences in probability models. The Wasserstein-2 and Euclidean distances are effective when KL divergence and L2-distance fail to measure probability model differences. The Wasserstein-1 loss function with Wasserstein proximal outperforms Euclidean proximal in decreasing the objective function. Numerical experiments show the superiority of Wasserstein-2 proximal over Euclidean proximal for the Wasserstein-1 loss function. The Relaxed Wasserstein Proximal (RWP) algorithm improves speed and stability in training GANs by applying regularization on the generator, which is a novel approach compared to traditional methods focusing on the discriminator. It outperforms other methods in terms of wallclock speed and convergence, making it a valuable tool for GAN training. The Relaxed Wasserstein Proximal (RWP) algorithm enhances GAN training by updating the generator with a modified rule, introducing hyperparameters h and the number of iterations. It is tested on various GAN types using CIFAR-10 and CelebA datasets with DCGAN architecture. FID is used to evaluate sample quality and training convergence. The Relaxed Wasserstein Proximal (RWP) algorithm improves GAN training by updating the generator with hyperparameters h and the number of iterations. FID is used to evaluate sample quality and training convergence, with results showing improved speed and stability. The regularization in RWP improves convergence speed and lowers FID for all GAN types, with a 20% improvement in sample quality for DRAGAN. Results are consistent on the CelebA dataset as well. Multiple generator iterations may initially prevent Standard GANs from learning on CelebA, but this issue is easily detectable and can be rectified with a more stable loss function. The regularization in RWP improves convergence speed and lowers FID for all GAN types. Multiple generator updates in RWP show stability and lower FID compared to omitting regularization. RWP also improves speed and achieves a lower FID in comparison to other GAN types. Appendix F shows that RWP regularization improves speed and achieves a lower FID. Multiple generator iterations may cause initial learning to fail, but once successful, it remains stable. An experiment with 10 generator iterations per outer-iteration demonstrates the effectiveness of RWP in achieving convergence and lower FID compared to without RWP. Without RWP, training is highly variable with FID on a rising trend. The Semi-Backward Euler (SBE) method on the CIFAR-10 dataset shows comparable training to the standard WGAN-GP loss. The training of SBE involves approximating three functions: the discriminator, generator, and potential function \u03a6 p. The experiment was averaged over 5 runs, with RWP regularization leading to convergence and lower FID. The algorithm and hyperparameter settings are detailed in the appendix. Comparing SBE to WGAN-GP, both with the same generator iterations, shows similar performance. In the literature, optimal transport has been applied in machine learning and GANs, with many studies using the Wasserstein distance as the loss function. This distance is beneficial for comparing probability distributions on lower dimensional sets, introducing a statistical estimator based on the data's geometry. The Semi-Backward Euler method shows comparable performance to norm WGAN-GP, warranting further investigation. The Wasserstein distance is utilized in GANs for comparing probability distributions on lower dimensional sets. The loss function chosen is the Wasserstein-1 distance, requiring the discriminator to satisfy the 1-Lipschitz condition. The Wasserstein-2 metric forms a metric tensor structure in the probability space, creating an infinite dimensional Riemannian manifold known as the density manifold. Gradient flows in the density manifold are linked to transport-related partial differential equations. The density manifold BID18 is linked to transport-related partial differential equations, such as the Fokker-Planck equation. Two angles have been developed in learning communities: leveraging gradient flow structure in probability space and studying nonparametric models like the Stein gradient descent method BID24. An approximate inference method for computing Wasserstein gradient flow is also considered. Our approach applies the Wasserstein gradient to work on general implicit generative models, focusing on regularizing the generator rather than the discriminator. This involves applying the constrained Wasserstein gradient and its relaxations on implicit generative models, with a focus on Gaussian families and elliptical distributions. The proposed method focuses on regularizing the generator in Wasserstein GAN models, computing the Wasserstein-2 gradient flow for faster convergence. A variational formulation introduces a Riemannian structure in density space, considering smooth and strictly positive probability densities. The text discusses the tangent space of smooth and strictly positive probability densities, introducing a Riemannian metric tensor in density space for the Wasserstein gradient operator. It also mentions the Wasserstein-2 metric and gradient operator constrained on statistical models. The paper provides further analytical results on the Wasserstein-2 gradient flow, focusing on the Wasserstein-2 metric and gradient operator constrained on statistical models defined by a triplet (\u0398, R^n, \u03c1). The parameterization function \u03c1 is assumed to be locally injective, leading to the definition of a Riemannian metric on \u03c1(\u0398) by pulling back the Wasserstein-2 metric tensor. The associated metric tensor G(\u03b8) is smooth and positive definite, forming a smooth Riemannian manifold (\u0398, g_\u03b8) as studied in Theorem 2. Theorem 2 studies the constrained Wasserstein gradient operator in parameter space on a smooth and positive definite Riemannian manifold. The proof involves the distance function written in the action function on the Wasserstein statistical manifold, with the gradient operator defined accordingly. Additionally, the derivation of the proposed semi-backward method and proof of a claim regarding geodesic paths are presented. The proof of Proposition 4 involves the constrained Wasserstein gradient operator on a Riemannian manifold. The geodesic path is reparameterized to prove equations 6 and 7. The Semi-backward method is derived for consistent numerical methods in time. The probability density transition equation of g(\u03b8(t), z) satisfies the gradient constraint. The probability density transition equation of g(\u03b8(t), z) satisfies the constrained continuity equation and the push-forward relation. The proof of Proposition 5 allows for the explicit computation of the Wasserstein proximal operator. The proof of Proposition 5 enables the explicit computation of the Wasserstein proximal operator. Hyperparameter settings for the Relaxed Wasserstein Proximal experiments are provided, including batch size, optimizer details, latent space dimension, and generator iterations for different scenarios. The hyperparameter settings for the Relaxed Wasserstein Proximal experiments include latent space dimensions, optimizer details, and generator iterations for various scenarios such as CIFAR-10, CelebA, and Standard GANs. The algorithm for training GANs with non-saturating gradient includes optimizing parameters like Adam \u03c9 and Adam \u03b8, batch size, and generator iterations. The process involves updating the discriminator, performing Adam gradient descent, and repeating until a stopping condition is met. The Relaxed Wasserstein Proximal method introduces differences in terms and generator iterations. Samples generated from a Standard GAN with RWP regularization are shown in FIG4. In FIG4, samples from a Standard GAN with RWP regularization on the CelebA dataset have an FID of 17.105. FIG5 shows samples from WGAN-GP with RWP on CIFAR-10 dataset with an FID of 38.3. Latent space walk BID34 indicates no memorization in GANs with RWP. Specific hyperparameters for SBE on WGAN-GP include a batch size of 64, DCGAN architecture, MLP for \u03a6 p, layer normalization, and Adam optimizer with LR 0.0002, \u03b2 1 = 0.1, and \u03b2 2 = 0.999. In the training process, MLP was used for the potential \u03a6 p with layer normalization. The Adam optimizer with specific parameters was utilized for the generator, discriminator, and potential. The latent space dimension was set to 100, and updates were made to the discriminator, generator, and potential in each outer-iteration loop."
}