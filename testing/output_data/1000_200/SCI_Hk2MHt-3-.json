{
    "title": "Hk2MHt-3-",
    "content": "In this paper, the architecture of deep convolutional networks is explored. A reconfiguration of model parameters into parallel branches at the global network level is proposed, leading to a significant reduction in parameters while improving performance. This arrangement also introduces additional regularization. The branches are tightly coupled by averaging their log-probabilities, enhancing the learning of better representations. Termed \"coupled ensembles,\" this approach is applicable to various neural network architectures. The study introduces \"coupled ensembles,\" a branched architecture applicable to various neural network models. Results show improved performance with a parameter budget of 25M, achieving error rates of 2.92%, 15.68%, and 1.50% on CIFAR-10, CIFAR-100, and SVHN tasks. The design of early convolutional architectures involved hyper-parameter choices, but has since shifted towards a template with fixed filter size and feature maps. The design of neural network architectures has evolved towards a template with fixed filter size and feature maps, down-sampling using maxpool or strided convolutions, and doubling feature maps after each down-sampling. State-of-the-art models like ResNet and DenseNet have extended this template with skip-connections. A new element called \"coupled ensembling\" is proposed, decomposing the network into branches similar to complete CNNs. This approach achieves comparable performance to existing models with fewer parameters. In this paper, the authors propose a new approach called \"coupled ensembles\" to improve convolutional networks' performance on CIFAR and SVHN datasets with fewer parameters. They suggest splitting parameters among branches, combining activations with an arithmetic mean, and ensembling coupled ensembles for further improvement. The paper discusses related work, introduces coupled ensembles, evaluates the approach, and compares it with state-of-the-art models. In section 3, the concept of coupled ensembles is introduced with the motivation behind it. Section 4 evaluates the proposed approach and compares it with existing models. The network architecture proposed has similarities with Cire\u015fan's Neural Networks Committees and Multi-Column Deep Neural Network, but differs in training a single model composed of branches and having a fixed parameter budget for the entire model. Multi-branch architectures have been successful in vision applications, with recent modifications using grouped convolutions for spatial and depth-wise feature extraction. Template building blocks are stacked together to form the complete model. The approach combines activations of branches by their log-probabilities over target categories and uses the same input for all branches with different preprocessing blocks. In contrast to previous approaches using template building blocks, we propose a generic modification of the CNN structure at the global model level. Our method involves specifying an \"element block\" architecture and replicating it as parallel branches to create the final composite model. This approach differs from Shake-Shake regularization BID5, which relies on stochastic mixtures of branches and has longer convergence times and batch size dependencies. Our method, on the other hand, maintains the same hyper-parameters as the base CNN model. Our method involves a generic rearrangement of a given architecture's parameters, leading to efficient parameter usage compared to the base model. It does not require modification at a local level like BID26's parallel paths in a ResNet. Ensembling neural networks can improve performance by combining outputs from multiple trainings with different error distributions. Our proposed model architecture involves rearranging parameters to efficiently utilize them, similar to ResNet's parallel paths. Ensembling neural networks can improve performance by combining outputs from multiple trainings with different error distributions. This approach leads to better results in classification challenges despite increased training costs. The proposed model architecture involves using branches to increase performance. Ensembling models during training can lead to significant performance improvements, but may increase model size and prediction time. The approach aims to maintain model size while improving performance or achieving the same performance with a smaller model size. Each branch in the model takes a data point as input and produces a score vector. The proposed model architecture involves using branches to increase performance. Each branch takes a data point as input and produces a score vector corresponding to target classes. Different element blocks like DenseNet-BC and ResNet are used to form branches. The fuse layer combines branches by averaging their individual log probabilities. The model is designed for a classification task with samples associated to one class from a finite set. Neural network models for image classification tasks output a score vector for target classes, followed by a SoftMax layer for probability distribution. Different network architectures like BID14, BID20, and BID22 use a fully connected layer before the SoftMax layer. The internal setup before the last FC layer varies among architectures. The differences among neural network architectures like BID14, BID20, and BID22 lie in the setup before the last FC layer. Fusion in ensemble models involves computing individual predictions separately and averaging them, equivalent to a \"super-network\" with parallel branches. Supernetworks are not commonly implemented due to memory constraints, but the AVG layer operation can be implemented separately. In ensemble models like BID14, BID20, and BID22, the AVG layer operation can be implemented separately due to memory constraints. The model consists of parallel branches producing score vectors fused through a \"fuse layer\" for a single prediction. Three options for combining score vectors are explored: FC average, LSM average, and LL average. The proposed architecture in the study involves multiple branches with score vectors fused through a \"fuse layer\" for a single prediction. Three methods for combining score vectors are explored: FC average, LSM average, and LL average. The composite branched model shows improved performance with a lower parameter count in experiments on CIFAR-10 and CIFAR-100 datasets. The parameter vector W is the concatenation of parameter vectors from element blocks, with no parameters in the \"fuse layer\". The study evaluates the proposed architecture on CIFAR-10, CIFAR-100, and SVHN datasets. The images are normalized and standard data augmentation is used for CIFAR datasets, while no data augmentation is used for SVHN. All hyperparameters are set according to the original descriptions without alteration. During training on CIFAR datasets, standard data augmentation is used, while no data augmentation is used for SVHN. A dropout ratio of 0.2 is applied in the case of DenseNet when training on SVHN. Testing is done after normalizing the input in the same way as during training. Error rates are given in percentages and correspond to an average of the last 10 epochs. DenseNet-BC's PyTorch implementation is used, and all execution times were measured using a single NVIDIA 1080Ti GPU. Experiments are conducted on the CIFAR-100 dataset with DenseNet-BC, depth L = 100, growth rate k = 12. The curr_chunk discusses the comparison of a branched DenseNet-BC configuration with an ensemble of independent models. It shows that the jointly trained branched configuration outperforms the ensemble in terms of test error rate. Additionally, it compares the branched configuration with a single branch model having a similar number of parameters. The comparison between a branched DenseNet-BC configuration and an ensemble of independent models shows that the branched configuration outperforms in terms of test error rate. It also compares the branched configuration with a single branch model with a similar number of parameters, demonstrating the efficiency of arranging parameters into parallel branches. In Section 4.5, the performance of a branched model with different \"fuse layer\" choices is analyzed. Experiments evaluate training and prediction fusion combinations for a branched model with e = 4 under various conditions. Table 1 compares Coupled Ensembles of DenseNet-BCs (e = 4) with different \"fuse layer\" combinations against a single branch model, showing top-1 error rates on the CIFAR-100 test set. The columns indicate the architecture, number of branches, and type of \"fuse layer\" during training. Table 1 displays the performance of models with different \"fuse layer\" choices during inference, including a branched model with e = 4 and Avg. LSM. This model shows comparable performance to a DenseNet-BC (L = 250, k = 24). The branched model with e = 4 and Avg. LSM for the \"fuse layer\" performs similarly to a DenseNet-BC model with 5 times more parameters. Coupled ensembles with LSM fusion result in lower error rates for \"element blocks\" compared to training them separately, indicating better feature learning. Averaging log probabilities helps update all branches consistently, providing a stronger gradient signal for improved representations. The gradient back-propagated from the fuse layer is consistent across all branches, improving combined predictions. Ensemble combinations outperform single branch networks, with 4 branches reducing error rates. Avg. FC training shows poor performance, while Avg. FC prediction yields better results. The non-linearity of the SM layer distorts the FC average, but Avg. FC prediction works well. Avg. FC prediction performs better than Avg. SM prediction as FC values transmit more information. Experiments use Avg. LSM as the \"fuse layer\" in branched models to find the optimal number of branches for a given parameter budget. The optimal number of instances e depends on the network architecture, parameter budget, and dataset. The optimal number of instances e for branched models depends on network architecture, parameter budget, and dataset. Model configurations for DenseNet-BC are selected just below the target parameters for fair comparison. DenseNet-BC parameter counts are quantified based on depth L and growth rate k values. In the considered case of DenseNet-BC with CIFAR-100 and 800k parameters, using 2 to 4 branches shows a significant performance improvement over the single branch model. However, using 6 or 8 branches leads to worse performance. The model's performance remains robust to slight variations in parameters, indicating the effectiveness of the coupled ensemble approach and DenseNet-BC architecture. The coupled ensemble approach and DenseNet-BC architecture show robustness to variations in parameters, leading to improved performance. However, this comes with increased training and prediction times. Different models were evaluated, with DenseNet-BC being the current state of the art. The current state of the art models and performance of coupled ensembles are presented in a table. Coupled ensembles with ResNet pre-act show better performance than single branch models. Different network sizes for DenseNet-BC architecture were considered, with results for extreme cases reported. Trade-offs were observed in experiments. In experiments with DenseNet-BC architecture, trade-offs between depth L and growth rate k were found to be not critical for a given parameter budget. Different configurations were tested, including single-branch and multi-branch versions with varying numbers of branches. Error rates were higher than reported by BID11 for single branch DenseNet-BC, possibly due to differences in error rate measurement and statistical variations. The coupled ensemble of DenseNet-BC models outperforms other architectures, including DenseNet-BC and Shake-Shake S-S-I model BID5, on CIFAR 10, CIFAR 100, and SVHN datasets. The larger models achieve error rates of 2.92%, 15.68%, and 1.50% respectively, comparable to state-of-the-art implementations. Additionally, comparisons with meta-learning scenarios are detailed in the supplementary material. The coupled ensemble approach outperforms other architectures on various datasets. The limitation lies in the network size fitting into GPU memory and training time. Classical ensembling is used for larger models due to hardware constraints. Improvement in performance plateaus after a small number of models. Multiple trainings are costly, so ensembling is preferred for larger models. The coupled ensemble approach outperforms other architectures on various datasets, with significant gains seen from fusing two models. Ensembling four large models showed no further improvement. The error rates between single and multi-branch models are highlighted, with branched models outperforming single branch models with double the parameters. The proposed approach shows superior performance compared to state-of-the-art models. The proposed approach in CIFAR-100 involves replacing a single deep convolutional network with \"element blocks\" that resemble standalone CNN models. These blocks are coupled via a \"fuse layer\" to improve performance. This leads to the best performance for a given parameter budget, with a small increase in training and prediction times. The approach outperforms single branch models with double the parameters and is shown to be superior to state-of-the-art models. The proposed approach in CIFAR-100 involves using \"element blocks\" connected via a \"fuse layer\" to improve performance. It outperforms single branch models with double the parameters and is superior to state-of-the-art models. The increase in training and prediction times is due to sequential processing of branches, making data parallelism less efficient on GPUs. This can be addressed by extending data parallelism to branches or spreading branches over multiple GPUs. The proposed approach in CIFAR-100 involves using \"element blocks\" connected via a \"fuse layer\" to improve performance. Preliminary experiments on ImageNet show that coupled ensembles have lower errors compared to single branch models. The common structure of the test and train versions of networks used as element blocks is illustrated in Figure 3. In the test version, the averaging layer can be placed after the last FC layer and before the SM layer, while in the train version, it can be placed after the last FC layer and after the SM layer. In the train version, the averaging layer can be placed after the last FC layer, after the LSM layer, or after the LL layer. The e branches are defined by parameter vectors W e, while the global network is defined by a concatenation of all the W e parameter vectors. Dedicated scripts are used for splitting the W vector into the W e ones in coupled networks. In coupled networks, a dedicated script is used to split the W vector into W e ones for prediction. The global parameter vector W is used consistently for all train and test versions, with the same split and element block functions. The network architecture is determined by global hyper-parameters specifying train/test mode, number of branches, and placement of the AVG layer. Larger models may replicate one element block e times or use a list of e element blocks with their own hyper-parameters. For larger models, data batches are split into \"micro-batches\" to accommodate training with a batch size larger than 64. Gradient accumulation and averaging over micro-batches are used to approximate the equivalent gradient of processing data as a single batch, with BatchNorm layer adjustments for normalization. In practice, using micro-batch statistics instead of whole batch statistics does not significantly affect performance. Parameter updates are done with gradient for a batch, while forward passes are done with micro-batches for optimal throughput. Memory requirements depend on network depth and mini-batch size. Adjusting memory using micro-batches does not impact performance and can even improve it slightly. The memory needed for multi-branch versions only increases if the branches' width is reduced. The multi-branch version does not require more memory if the branches' width is kept constant. Memory is only needed if the branches' width is reduced. Hyper-parameter search experiments showed that reducing both width and depth was the best option. Training with micro-batch sizes of 16 for single-branch versions and 8 for multi-branch versions was done within 11GB memory. Splitting the network over two GPU boards allows for doubling the micro-batch sizes but does not significantly increase speed or improve performance. The multi-branch architecture does not require more memory if branch width is constant. Experiment results show significant performance gains with just two branches compared to a single-branch setup. Varying depth and growth rate does not greatly affect performance. The experiment results show that the (L = 70, k = 9, e = 3) combination is slightly better than the (L = 82, k = 8, e = 3) combination, but the difference may not be statistically significant. Comparing parameter usage and performance of branched coupled ensembles with model architectures recovered using meta learning techniques poses challenges in reproducibility and statistical significance. The performance measures in experiments show variations due to different factors such as framework used, random seed for network initialization, CuDNN non-determinism during training, fluctuations in batch normalization, and choice of model instance. During training, the choice between the model obtained after the last epoch or the best performing model can impact the evaluation measure due to variations in random initialization. Despite efforts to design and train neural networks properly, there is still a small but noticeable dispersion in performance among different local minima. The dispersion in performance among different local minima complicates comparisons between methods, making differences in measures lower than their dispersions likely nonsignificant. Experiments on a moderate scale model show the challenges of quantifying these effects, especially for larger models like DenseNet-BC with L = 100, k = 12 on CIFAR 100. The results of experiments on DenseNet-BC with L = 100, k = 12 on CIFAR 100 are presented in table 8, showing different configurations and performance measures. The study aimed to quantify the relative importance of various effects, considering factors like Torch7 vs PyTorch and using the same seed vs different seeds. The results include minimum, median, maximum, mean, and standard deviation over 10 measures from 10 identical runs. The study compared Torch7 and PyTorch implementations on DenseNet-BC with L = 100, k = 12 on CIFAR 100. Results show no significant difference between implementations or using the same seed vs different seeds. Reproducing exact results is challenging due to observed dispersion. Mean measures over single last epoch and last 10 epochs are similar. Standard deviation of measures from 10 runs shows slight variation. The standard deviation of measures computed on 10 runs is consistently smaller when computed on the last 10 epochs compared to the single last epoch. Averaging measures over the last 10 epochs reduces fluctuations. The mean of measures from 10 runs is significantly lower when taken at the best epoch compared to the single last epoch or last 10 epochs. The standard deviation of measures computed on 10 runs is consistently smaller when taken from the last 10 epochs compared to the single last epoch. Averaging measures over the last 10 epochs reduces fluctuations. Choosing the error rate at the last iteration or the 10 last iterations does not affect the mean but reduces the standard deviation. In experiments, using the error rate from the last 10 epochs is preferred for more robust results. Different values can be used without significant impact. For CIFAR experiments, averaging error rates from the last 10 epochs is recommended. For SVHN experiments, the last 4 iterations were used due to fewer and larger epochs. These observations highlight the importance of choosing the appropriate number of epochs for error rate estimation. In this study, comparisons between single-branch and multi-branch architectures were made at a constant parameter budget, showing an advantage for multi-branch networks. However, multi-branch networks currently have longer training times. The study investigates if multi-branch architectures can still outperform single-branch ones at a constant training time budget. Ways to reduce training time include better parallelization. Architectures can improve over single-branch ones at constant training time budget. Ways to reduce training time include reducing iterations, parameter count, or increasing width while reducing depth. Results for these options are shown for CIFAR 10 and 100. Multi-branch baseline has longer training time compared to single branch. Options include reducing training epochs or depth to decrease parameter count. Options are presented to reduce training time and parameter count in DenseNet-BC models. These options include reducing the number of training epochs, decreasing the depth of the model, and adjusting the parameter count to match a single-branch baseline. Despite slight performance differences, all options outperform the single-branch baseline. DenseNet-BC L = 88, k = 20, e = 4 performs better than the single-branch baseline with a significantly reduced parameter count and training time. In comparison to a single-branch baseline, DenseNet-BC L = 88, k = 20, e = 4 outperforms with a parameter count divided by 3.7 and training time divided by 1.5. Coupled ensembles show superior performance with a fixed parameter budget on datasets like STL-10 and CIFAR-100. The experiments compared single-branch and multi-branch coupled ensembles using images of size 256\u00d7256 due to constraints. Data augmentation included random flips and crops of size 224\u00d7224. DenseNet-169-k32-e1 was used as a baseline single-branch model, compared to DenseNet-121-k30-e2 coupled ensemble. Results in table 11 showed significant improvement with the coupled ensemble approach. Further experiments with full-sized images and increased data augmentation are planned. The results in table 11 demonstrate that the coupled ensemble approach with two branches significantly outperforms the baseline, even with a constant training time budget."
}