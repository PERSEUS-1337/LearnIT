{
    "title": "SyerXXt8IS",
    "content": "Auto-generate enhanced input features for ML methods with limited training data. Biological neural nets (BNNs) excel at fast learning, with the insect olfactory network rapidly learning new odors through competitive inhibition, sparse connectivity, and Hebbian updates. MothNet, a computational model of the moth olfactory network, generates new features for ML classifiers, outperforming other methods on MNIST and Omniglot datasets by reducing test set error averages by 20% to 55%. The MothNet feature generator outperforms other methods, reducing test set error averages by 20% to 55%. This highlights the potential value of BNN-inspired feature generators in the ML context, improving learning from limited data. The MothNet model, inspired by biological neural nets, rapidly learns new features from limited data, outperforming standard ML methods. It incorporates competitive inhibition, high-dimensional sparse layers, and a Hebbian update mechanism, showing superior performance in learning vectorized MNIST digits with minimal training samples per class. The MothNet model includes competitive inhibition in the AL, sparsity in the MB, and weight updates affecting only MB\u2192Readout connections. The MothNet model includes competitive inhibition in the AL, sparsity in the MB, and weight updates affecting only MB\u2192Readout connections. The architecture was tested as a front-end feature generator for an ML classifier, combining MothNet with a downstream ML module for automatic feature generation in a non-spatial task using vMNIST data. The MothNet model, incorporating competitive inhibition in the AL and sparsity in the MB, was tested as a feature generator for ML classifiers. It significantly improved ML accuracy compared to other methods like PCA, PLS, NNs, and transfer learning, indicating stronger feature generation capabilities. The MothNet model outperformed other methods in generating strong features for ML accuracy. vMNIST data set was used with baseline ML methods not achieving full accuracy at low N. MothNet instances were randomly generated with specific connectivity parameters. Experiments compared Cyborg vs baseline ML methods on vMNIST. Full network architecture details and Matlab code for the experiments can be found in references [11] and [12]. MothNet instances were randomly generated from templates with specific connectivity parameters. Experiments compared Cyborg vs baseline ML methods on vMNIST, where MothNet outperformed conventional ML methods. Trained ML accuracies of the baselines and cyborgs were compared to assess gains. To compare MothNet features with conventional ML methods, vMNIST experiments were conducted using feature generators such as PCA, PLS, and NN pre-trained on vMNIST data. CNNs were not used due to the lack of spatial content in vMNIST. Transfer learning was applied to NN with weights initialized by training on an 85-feature vectorized Omniglot dataset. MothNet features significantly improved ML accuracy on vMNIST data compared to conventional methods like PCA, PLS, and NN. Adding extra hidden layers did not enhance baseline performance, indicating MothNet features captured new class-relevant features effectively. Non-spatial tasks also showed similar gains with MothNet features. MothNet features improved ML accuracy on vMNIST data compared to conventional methods like PCA, PLS, and NN. The gains ranged from 10% to 88% depending on the method and N value. MothNet features increased accuracy across all ML models, with NN models seeing the greatest benefits of 40% to 55% relative reduction in test error. Even cases where ML baseline accuracy exceeded MothNet's ceiling of \u2248 75% saw improvements with MothNet features. The MothNet front-end improved ML accuracy, especially for NN models with N > 3 samples per class. Results showed significant gains in accuracy, with MothNet features outperforming PCA, PLS, and NN as feature generators. Pass-through AL also had a positive effect on accuracy. The MothNet architecture consists of a competitive inhibition layer (AL) and a high-dimensional, sparse layer (MB). Testing the MB alone with a pass-through AL layer still resulted in significant accuracy improvements over baseline ML methods. The gains with pass-through ALs were generally between 60% and 100% of the gains with normal ALs, indicating the importance of the trainable layer (MB). The AL layer added value in generating strong features, with NNs benefiting the most from it. The AL layer added value by generating strong features, benefiting NNs the most. An automated feature generator based on a simple BNN significantly improved learning abilities on vMNIST and vOmniglot datasets. MothNet's pre-processing made class-relevant information accessible, outperforming standard methods like PCA, PLS, NNs, and pre-training. The competitive inhibition layer enhances classification by creating attractor basins. The competitive inhibition layer in the insect MB enhances classification by creating attractor basins for inputs, increasing the effective distance between samples of different classes. The sparse connectivity from AL to MB provides computational and anti-noise benefits, resembling sparse autoencoders but with key differences such as no pre-training step and requiring very few samples. The Mushroom Body (MB) has a higher number of active neurons compared to the input dimension, with no pre-training step required. Unlike Reservoir Networks, MB neurons do not have recurrent connections. The Hebbian update mechanism in MB is different from backpropagation, with weight updates occurring on a local basis. The dissimilarity of optimizers between MothNet and machine learning may increase total encoded information."
}