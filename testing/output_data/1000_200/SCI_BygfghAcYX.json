{
    "title": "BygfghAcYX",
    "content": "In this work, a novel complexity measure based on unit-wise capacities is suggested for two layer ReLU networks, providing a tighter generalization bound. The capacity bound correlates with test error behavior with increasing network sizes and partly explains the improvement in generalization with over-parametrization. Additionally, a matching lower bound for the Rademacher complexity is presented, improving over previous capacity lower bounds for neural networks. Deep neural networks have been successful in various tasks, with models becoming over-parametrized but still achieving lower generalization error when trained with real labels. Increasing model size in neural networks does not necessarily lead to overfitting, contrary to traditional learning wisdom. Increasing model size in neural networks does not necessarily lead to overfitting, as observed in various empirical studies. Over-parametrization can actually improve generalization error, even without explicit regularization techniques. Existing works have proposed different measures of complexity for neural networks to explain the generalization behavior observed in practice. These measures include norm, margin, and sharpness based measures, which aim to capture the network's capacity. Even when a network is large enough to fit the training data perfectly, the test error continues to decrease for larger networks. This phenomenon has been observed in both fully connected feedforward networks and ResNet18 architecture. The text discusses the complexity measures for neural networks and their impact on generalization behavior. It highlights the observation that test error continues to decrease for larger networks, as seen in both fully connected feedforward networks and ResNet18 architecture. The importance of unit capacity and unit impact in determining network output is emphasized, with empirical evidence showing their decrease with the number of hidden units. The limitations of existing complexity measures in explaining the benefits of over-parametrization are also noted. The text discusses the limitations of existing complexity measures in explaining the benefits of over-parametrization in neural networks. To further analyze this phenomenon, the study simplifies the architecture by choosing two-layer ReLU networks, which exhibit similar behavior with increasing network size. In this study, the property of interest is preserved after simplifying the architecture with two-layer ReLU networks. A tighter generalization bound is proven for these networks, showing a correlation with test error and decreasing complexity with more hidden units. The key insight is characterizing complexity at a unit level, where measures shrink faster than 1/ \u221a h for each hidden unit as network size increases. The generalization bound depends on layer norms, specifically the Frobenius norm of the top layer and the difference of hidden layer weights with initialization. The generalization bound in deep networks is influenced by layer norms, with the Frobenius norm of the top layer and the difference in hidden layer weights from initialization playing a key role. As network size increases, this dependency decreases. In over-parametrized settings, training only the top layer can minimize training error due to the large number of hidden units representing all possible features. This suggests that as networks are over-parametrized, the optimization process simplifies to selecting the right features to minimize training loss. The optimization problem in deep networks involves selecting the right features to minimize training loss. Studies have shown the importance of initialization in network performance, with different metrics used to measure complexity and generalization behavior. Liang et al. (2017) proposed a Fisher-Rao metric for larger networks, while Nagarajan & Kolter (2017) focused on initialization-dependent generalization bounds for linear networks. In this paper, the authors empirically investigate the role of over-parametrization in generalization of neural networks on different datasets. They propose tighter generalization bounds for two layer ReLU networks and introduce a complexity measure that decreases with the increasing number of hidden units, potentially explaining the effect of over-parametrization on generalization. The study introduces tighter generalization bounds for two-layer ReLU networks with increasing hidden units, potentially explaining the impact of over-parametrization. It provides a new lower bound for the Rademacher complexity, surpassing previous bounds and considering network parameters like input and output dimensions. The study defines the margin operator for c-class classification tasks and introduces the ramp loss function. The expected margin loss is bounded between 0 and 1, with empirical estimates denoted as L\u03b3(f). The loss function is related to the expected risk and training error. The Rademacher complexity is a capacity measure that captures the ability of functions in a function class to fit random labels, which increases with the complexity of the class. It is used to bound the generalization error of neural networks. The Rademacher complexity of neural networks is used to bound the generalization error, depending on the function class chosen. Choosing the right function class is crucial to capture the real trained networks and explain the decrease in generalization error with increasing width. Experimental observations on network layers with increasing hidden units are discussed using the CIFAR-10 dataset. In Section A, similar observations on SVHN and MNIST datasets are discussed. The spectral and Frobenius norms of the learned layer initially decrease but eventually increase with h, with the Frobenius norm increasing faster. The distance Frobenius norm w.r.t. initialization decreases, indicating that the increase in Frobenius norm of weights in larger networks is due to random initialization. The distance to initialization per unit decreases with increasing h, and there is a shift in the distribution of angles between learned weights and initial weights from almost orthogonal in small networks to almost aligned in large networks. The unit capacity per distance to initialization decreases with increasing h. In the second layer, the Frobenius norm and distance to initialization both decrease with increasing h, suggesting a limited role of initialization for this layer. The norm of outgoing weights from a hidden unit decreases at a faster rate as the network size grows. The norm of outgoing weights from hidden units decreases faster than 1/ \u221a h as network size grows, impacting final decision classifiers. Unit impact \u03b1 i = v i 2 is defined as the magnitude of outgoing weights. Consider a class of neural networks dependent on hidden unit capacity and impact. The hypothesis class of neural networks represented using parameters in the set W has bounded unit capacity and unit impact. Studying the generalization behavior of this function class can provide a better understanding of these networks. A generalization bound for two layer ReLU networks is proven by bounding the Rademacher complexity of the class F W in terms of the sum over hidden units of the product of unit capacity and unit impact. This, combined with a specific equation, gives the generalization bound. The Rademacher complexity of two layer neural networks is bounded by decomposing the complexity across hidden units, leading to a tighter generalization bound. This new technique provides a more accurate estimate compared to previous methods that only consider layer complexity. The generalization bound for two layer neural networks is improved by considering the Rademacher complexity across hidden units. The bound holds for all networks by covering the space of possible values for \u03b1 and \u03b2. The generalization error is bounded with probability 1 \u2212 \u03b4 over the choice of the training set, and decreases with increasing width for networks learned in practice. An explicit lower bound for the Rademacher complexity is also provided. The Rademacher complexity for two-layer neural networks is improved by considering the generalization error with increasing network width. An explicit lower bound is provided, showing the tightness of the generalization bound. The additive factor in the bound is small in practice, resulting in a decrease in capacity with over-parametrization. The generalization bound is extended to p norms in Appendix Section B, presenting a finer tradeoff between terms. Comparisons with previous work show similar behavior in the key complexity terms. The Rademacher complexity for two-layer neural networks is improved by considering generalization error with increasing network width. The key complexity term in the bound is U \u2212 U 0 1,2 V 2, which increases with h. Experimental comparison on CIFAR-10 and SVHN datasets shows that even a network size of 128 is enough to achieve zero training error. In experiments on CIFAR-10 and SVHN datasets, network sizes beyond 128 improve generalization even without regularization. Unit capacity and unit impact decrease with increasing network size. The number of epochs needed to reach 0.01 cross-entropy loss decreases for larger networks. Generalization bounds scale with effective capacity C, as shown in FIG6. The effective capacity of function classes determines generalization bounds, with our bound being the only one decreasing with size and consistently lower than other bounds. Numerical values may be loose but offer insight into generalization behavior relative to complexity measures. Our capacity bound improves over VC-dimension for networks larger than 1024, as shown in experiments on CIFAR-10 and SVHN datasets. Our capacity bound decreases with network size and outperforms other bounds in generalization behavior. It correlates well with the distribution of margin normalized by our measure, showing its effectiveness in distinguishing networks trained on true and random labels. The lower bound for the Rademacher complexity of neural networks matches the dominant term in the upper bound theorem. It is proven on a smaller function class with an additional constraint on the spectral norm of the hidden layer, allowing for comparison with existing results and extending to a larger class. The parameter set is defined, and the proof is provided in the supplementary section. This complexity lower bound aligns with the upper bound theorem, up to a certain factor, derived from Lipschitz continuity. The complexity lower bound matches the upper bound in Theorem 1, showing tightness even with additional information on spectral norm bounds. Previous capacity lower bounds for neural networks with bounded spectral norms and scalar output correspond to the Lipschitz constant. The lower bound for spectral norm bounded neural networks with scalar output and element-wise activation functions improves over previous bounds by showing a gap between the Lipschitz constant and network capacity. It excludes networks with rank-1 weight matrices, revealing a capacity gap between ReLU and linear networks. This bound does not apply to linear networks and can be extended to more layers by setting intermediate layer weight matrices to the Identity matrix. The lower bound for spectral norm bounded neural networks with scalar output and element-wise activation functions improves over previous bounds by showing a gap between the Lipschitz constant and network capacity. It excludes networks with rank-1 weight matrices, revealing a capacity gap between ReLU and linear networks. This bound does not apply to linear networks and can be extended to more layers by setting intermediate layer weight matrices to the Identity matrix. Theorem 7 in Golowich et al. (2018) gives a \u2126(s1s2\u221ac) lower bound for the composition of 1-Lipschitz loss function and neural networks with bounded spectral norm, or \u221e-Schatten norm. Our result improves on this lower bound by a factor of \u221ah. The paper presents a new capacity bound for neural networks that decreases with the number of hidden units, potentially explaining the better generalization of larger networks. It focuses on the role of width in two-layer networks and suggests exploring depth and width interactions for future study. The paper also provides a lower bound for network capacity, improving on existing bounds. The absolute values of these bounds are still larger than the number of training samples, prompting the need for smaller value bounds. The paper does not address optimization convergence in this context. In this paper, a new capacity bound for neural networks is presented, decreasing with the number of hidden units. The focus is on the role of width in two-layer networks, with potential future exploration of depth and width interactions. The paper also provides a lower bound for network capacity, with values larger than the number of training samples, prompting the need for smaller value bounds. The experiment involved training a pre-activation ResNet18 architecture on the CIFAR-10 dataset. The architecture consists of 8 residual blocks with varying numbers of channels and strides. The kernel size used in all convolutional layers is 3. Training involves 11 architectures with different values of k. SGD is used with specific parameters and data augmentation techniques. No weight decay or dropout is used in the training process. The study trained fully connected feedforward networks on CIFAR-10, SVHN, and MNIST datasets without weight decay or dropout. Data augmentation was done by random horizontal flip and random crop followed by zero padding. Thirteen architectures were trained for each dataset, with increasing hidden units by a factor of 2. SGD with specific parameters was used for training, and experiments were stopped based on cross-entropy reaching 0.01 or after 1000 epochs. Generalization bounds were calculated, and the margin was set to the 5th percentile. The study trained fully connected feedforward networks on CIFAR-10, SVHN, and MNIST datasets without weight decay or dropout. Data augmentation was done by random horizontal flip and random crop followed by zero padding. Thirteen architectures were trained for each dataset, with increasing hidden units by a factor of 2. SGD with specific parameters was used for training, and experiments were stopped based on cross-entropy reaching 0.01 or after 1000 epochs. Generalization bounds were calculated, and the margin was set to the 5th percentile. For each generalization bound, exact calculations were done including logterms and constants. Bounds in BID2 and Neyshabur et al. (2015c) were adjusted for binary classification, and random initialization was used as the reference matrix for plotting distributions. Figures 6 and 7 show measures on networks trained on SVHN and MNIST datasets, with the left panel of FIG10 illustrating over-parametrization in the MNIST dataset. The study trained fully connected feedforward networks on CIFAR-10, SVHN, and MNIST datasets without weight decay or dropout. Data augmentation was done by random horizontal flip and random crop followed by zero padding. Thirteen architectures were trained for each dataset, with increasing hidden units by a factor of 2. SGD with specific parameters was used for training, and experiments were stopped based on cross-entropy reaching 0.01 or after 1000 epochs. Generalization bounds were calculated, and the margin was set to the 5th percentile. For each generalization bound, exact calculations were done including logterms and constants. Bounds in BID2 and Neyshabur et al. (2015c) were adjusted for binary classification, and random initialization was used as the reference matrix for plotting distributions. The left panel of FIG10 shows the over-parametrization phenomenon in the MNIST dataset, while the middle and right panels compare generalization bounds. Theorem 5 generalizes Theorem 2 to p norm, with Lemma 11 constructing a cover for the p ball with entry-wise dominance. The generalization error is bounded for any h, p \u2265 2, \u03b3 > 0, \u03b4 \u2208 (0, 1), and U 0 \u2208 R h\u00d7d. The bound improves on the additive term in Theorem 2 for p of order ln h, decreasing with h for larger values. The text discusses generalization bounds for fully connected feedforward networks trained on different datasets. The study includes data augmentation techniques and training details. Theorem 5 generalizes Theorem 2 to p norm, with Lemma 11 constructing a cover for the p ball with entry-wise dominance. The generalization error is bounded for any h, p \u2265 2, \u03b3 > 0, \u03b4 \u2208 (0, 1), and U 0 \u2208 R h\u00d7d, with a tighter bound decreasing with h for larger values. The Rademacher random variables are used to decompose the Rademacher complexity of networks. The complexity of the class of networks can be broken down to that of hidden units. Lemma 9 provides a bound on the Rademacher complexity of the class F W. The proof involves induction on t and the Lipschitzness of the ramp loss. Lemma 10 states the Ledoux-Talagrand contraction property, which is used in the proof of Theorem 1. The proof involves applying Lemma 9 and then Lemma 10 with specific parameters to derive the final result. The proof of Theorem 1 involves applying Lemmas 9 and 10 with specific parameters. Lemma 11 introduces a covering lemma for generalization bounds without knowledge of network parameter norms. Lemma 13 bounds the generalization error for specific parameters. Lemma 13 provides a bound on the generalization error with specific parameters, showing that the generalization error is bounded under certain conditions. Lemma 14 provides specific results for the case p = 2, giving a bound on the generalization error for a function f(x) under certain conditions. The proof directly follows from Lemma 14 and uses notation to hide constants. The proof of Theorem 2 follows from Lemma 14 and introduces a generalization bound for any p \u2265 2. Lemma 15 provides a bound on the generalization error for a function f(x) with specific conditions. For this choice of \u00b5 and p, the inequality (\u00b5 + 1) 2/p = e 2 holds. The proof of Theorem 5 follows from Lemma 15. The proof of Theorem 3 involves dividing the dataset into groups and defining specific matrices and functions. The text discusses the matrix U(\u03be) defined as Diag(\u03b2) \u00d7 F(\u03be), where F(\u03be) is orthonormal. The 2-norm of each row of F is bounded by 1, leading to U(\u03be) 2 \u2264 max i \u03b2 i."
}