{
    "title": "SygMXE2vAE",
    "content": "BERT and other Transformer-based models have achieved state-of-the-art results in Natural Language Processing tasks, but there is still a lack of understanding of their internal functioning. A layer-wise analysis of BERT's hidden states is presented to gain insights into how these models work. The focus is on models fine-tuned for Question Answering tasks, examining how token vectors are transformed to find the correct answer. Probing tasks are applied to reveal the information stored in each representation layer, with qualitative analysis of hidden state visualizations providing additional insights. The analysis of hidden state visualizations in BERT reveals insights into its reasoning process and ability to incorporate task-specific information. Fine-tuning has minimal impact on semantic abilities, and prediction errors can be identified in early layers. Transformer models have gained popularity in Natural Language Processing due to their advancements over RNNs in Machine Translation. Large models and extensive pre-training have led to improvements over RNNs in Machine Translation. Permission is granted for personal or classroom use, with copyright restrictions. The paper discusses BERT BID8, a popular Transformer model that has shown significant improvements in Natural Language Processing tasks. It addresses the problem of black box models in deep learning, highlighting the lack of transparency and reliability in these models. Transformers are considered moderately interpretable through inspection. This paper explores the interpretability of Transformer Networks by examining hidden states between encoder layers, addressing questions about how Transformers answer questions, the tasks of specific layers, the influence of fine-tuning, and evaluating network failures in predicting correct answers. The paper explores the interpretability of Transformer Networks by analyzing hidden states between encoder layers to understand why networks fail to predict correct answers in Question Answering tasks. The study includes fine-tuned models on standard QA datasets and proposes a layer-wise visualization of token representations to reveal internal information of Transformer networks. Preliminary tests on the BERT and small GPT-2 models show similar results. The study focuses on the interpretability of Transformer Networks, specifically BERT, by analyzing hidden states between encoder layers to understand prediction failures in Question Answering tasks. It includes fine-tuned models on QA datasets and visualizes token representations to reveal internal information. The analysis shows that BERT's transformations go through similar phases, with general language properties encoded in earlier layers and used to solve tasks in later layers. The study focuses on the interpretability of Transformer Networks, specifically BERT, by analyzing hidden states between encoder layers to understand prediction failures in Question Answering tasks. It includes fine-tuned models on QA datasets and visualizes token representations to reveal internal information. Additionally, the analysis mentions other Transformer models like GPT-2, Universal Transformer, and TransformerXL that aim to improve the Transformer architecture. Interpretability and explainability of neural models are highlighted as significant research areas. Recent research has focused on probing tasks and methodologies applied to trained neural models like ELMo, BERT, and GPT-1. Various probing tasks, including semantic and syntactic analysis, have been conducted on pre-trained models. BERT has been specifically analyzed in some studies, with a novel \"edge-probing\" framework proposed by Tenney et al. and additional probing tasks added by other researchers. Various studies have analyzed neural models like BERT through probing tasks and qualitative visual analysis. Researchers have explored attention values in different layers, performance metrics, and the importance of specific dimensions in word vectors. Some studies focus on pre-trained models, while others delve into tasks like phoneme recognition in DNNs. Li et al. BID17 analyze word vectors and their impact on sequence tagging and classification tasks. Their work is distinct from Liu et al. BID20, who focus on probing pre-trained models without considering fine-tuned models. Additionally, Jain and Wallace BID15's research questions the effectiveness of attention in solving explainability and interpretability issues. The study focuses on analyzing the transformations of input tokens in fine-tuned BERT models. Two approaches are used: qualitative analysis of token vectors in vector space and quantitative analysis through QA-related tasks. The architecture of BERT allows for tracking token transformations throughout the network, providing insights into changes in token representations at each layer. The study analyzes changes in token representations in BERT models by qualitatively examining token vectors in each layer. Dimensionality reduction is used to visualize token relations in a two-dimensional space. The study uses dimensionality reduction techniques like t-SNE, PCA, and ICA to visualize token relations in BERT models. K-means clustering is applied to verify the clusters in 2D space, which correspond with observations. The goal is to understand the model's abilities after each transformation through semantic probing. The study applies dimensionality reduction techniques to visualize token relations in BERT models. K-means clustering is used to verify clusters in 2D space, aligning with observations. Semantic probing tasks are then applied to analyze information stored within transformed tokens after each layer, focusing on language understanding and reasoning tasks. The study focuses on tasks like Entity Labeling, Coreference Resolution, and Relation Classification, essential for language understanding. Question Type Classification and Supporting Fact Identification are added for Question Answering. Entity Labeling involves predicting entity categories from token spans. Coreference Resolution predicts if two mentions refer to the same entity. Relation Classification predicts relation types between entities. Tasks are based on OntoNotes corpus and annotations. Relation Classification involves predicting relation types between known entities using samples from the SemEval 2010 Task 8 dataset. Question Type Classification identifies question types using the Question Classification dataset. Supporting Facts extraction is crucial for Question Answering tasks. BERT's token transformations are examined to understand how they distinguish important context parts from distracting ones in Question Answering tasks. A probing task is constructed to identify Supporting Facts, where the model predicts if a sentence contains relevant information for a specific question. HotpotQA and bAbI datasets provide sentencewise Supporting Facts, while SQuAD considers the answer phrase as the Supporting Fact. The probing task involves labeling sentences as supporting facts based on their relevance to specific questions. Input tokens are embedded using a fine-tuned BERT model across all layers, with only tokens of labeled edges considered for classification. This approach differs from previous methods and aims to recognize key information in the context. The curr_chunk discusses the process of classifying tokens of labeled edges within a sample for Relation Classification using a two-layer MLP classifier. The study compares the performance of pretrained BERT models without fine-tuning on Question Answering tasks like SQUAD, bAbI, and HotpotQA to understand the model's abilities. Coreference Resolution and Relation Modeling are used to find the correct answer in Question Answering datasets like SQUAD, bAbI, and HotpotQA. Detention is a common punishment in the UK, Ireland, and other countries, where students have to stay in school after hours or on non-school days. HotpotQA contains 112,000 question-answer pairs designed for Multihop QA tasks. The HotpotQA task involves 112,000 natural question-answer pairs designed for Multihop QA. The context includes supporting and distracting facts, with an average size of 900 words. To accommodate the BERT model's input size limit, distracting facts are reduced by a factor of 2.7. The bAbI tasks are artificial toy tasks aimed at understanding neural model abilities, focusing on reasoning over multiple sentences and including Positional Reasoning, Argument Relation Extraction, and Coreference Resolution. The tasks discussed involve Multihop QA and include Positional Reasoning, Argument Relation Extraction, and Coreference Resolution. Models analyzed are BERT BID8 and GPT-2 BID28, both based on Transformer architecture. The models integrate into a probing setup using Pytorch implementation of BERT. The decoder half of BERT is used in the probing setup, with training based on Pytorch implementation. Pre-trained models like bert-base-uncased and bert-large are studied, along with a small GPT-2 model. Models are fine-tuned on datasets with hyperparameters tuned through grid search. Input length is set to 384 tokens for bAbI and SQuAD tasks. The input length chosen is 384 tokens for the bAbI and SQuAD tasks, and a maximum of 512 tokens for the HotpotQA tasks. Evaluation includes models trained on single bAbI tasks and a multitask model trained on all 20 tasks. Two settings are considered: Span prediction and Sequence Classification. For HotpotQA, two tasks are distinguished: HotpotQA Support Only (SP) task and other tasks. The HotpotQA tasks include Support Only (SP) task with labeled Supporting Facts as context, and Distractor task with distracting sentences within 512 token limit. Evaluation results show high accuracy on SQuAD task, while HotpotQA tasks are more challenging, especially the distractor setting. GPT-2 performs worse on difficult tasks compared to bAbI tasks. BERT and GPT-2 easily solve the bAbI tasks, with GPT-2 outperforming BERT in tasks requiring positional or geometric reasoning. Qualitative analysis of vector transformations reveals recurring patterns, with results from probing tasks displayed for comparison. Results from probing tasks comparing different BERT models on various tasks are presented in FIG1 and FIG2. The PCA representations of tokens in different layers show the model going through multiple phases while answering questions, observed across different QA tasks. These phases are supported by the results of probing tasks, with the first phase being Semantic Clustering. In the middle layers of BERT-based models, entities are connected by their relation within a specific input context, forming task-specific clusters that filter question-relevant entities. This phase contrasts with the earlier layers that group tokens into topical clusters, resembling embedding spaces from Word2Vec but lacking task-specific information. Entities in BERT-based models are connected within task-specific clusters in the middle layers, filtering question-relevant entities. These clusters help solve specific questions by recognizing relevant entities and their relationships. The model's ability to identify entities is supported by probing results. The HotpotQA model shows clusters of coreferences, with improved entity recognition, coreference resolution, and relation recognition in higher network layers. BERT-based models also exhibit similar patterns, with the ability to match questions with supporting facts being crucial for Question Answering and Information Retrieval. BERT models in Question Answering and Information Retrieval transform tokens to match question tokens with relevant context tokens. Results from probing tasks show that the models excel in distinguishing relevant information from irrelevant information in higher layers. The performance for this task improves over successive layers for SQuAD and bAbI datasets. However, the fine-tuned HotpotQA model does not show a significant increase in accuracy compared to the model without fine-tuning. The fine-tuned HotpotQA model does not significantly improve accuracy compared to the model without fine-tuning. The BERT model struggles with identifying the correct Supporting Facts, affecting its performance on the dataset. Vector representations help in understanding which facts are considered important by the model, aiding in decision-making and transparency. In the final network layers, the model isolates correct answer tokens and possible candidates, forming distinct clusters. The vector representation at this stage is task-specific and learned during fine-tuning, leading to a performance drop in general NLP probing tasks. The vector representation in the last-layer of the large BERT model fine-tuned on HotpotQA shows a loss of information, impacting performance in general NLP tasks. The phases of answering questions can be likened to human reasoning, involving semantic clustering, building relations between context parts, separating important from irrelevant information, and grouping answer candidates. BERT and human reasoning share similarities in separating important from irrelevant information and grouping answer candidates. However, BERT can process all input at once, allowing for concurrent phases during the answering process. Contrasting with GPT-2, BERT's hidden states do not focus on the first token of a sequence. GPT-2, unlike BERT, pays particular attention to the first token of a sequence during dimensionality reduction, resulting in a separation of clusters. This issue is present in all layers of GPT-2 except for the Embedding Layer, the first Transformer block, and the last one. To address this, the first token is masked during dimensionality reduction. GPT-2, similar to BERT, separates relevant information in the vector space but also extracts additional sentences with similar meaning. Unlike BERT, GPT-2 does not distinctly separate the correct answer and instead leaves it as part of its sentence. The findings in GPT-2 suggest that the analysis extends beyond BERT and applies to other Transformer networks. Future work will involve more probing tasks to confirm this observation. Visualizations can reveal failure states and the difficulty of tasks based on hidden state representations. Correct predictions follow discussed phases, while wrong predictions show two possibilities when a confident candidate answer is present. When wrong predictions occur, there are two possibilities: if the network is confident in a candidate answer, the phases resemble a correct prediction but centered on the wrong answer. Inspecting early layers can reveal reasons for the wrong choice. However, if network confidence is low, transformations do not follow the usual phases, and extracted information may not be relevant to the prediction. The vector space is transformed in each layer, with tokens mostly kept in a single cluster. Even when the network's confidence is low, it maintains 'Semantic Clustering' similar to Word2Vec. Fine-tuning has little impact on the core NLP abilities of the model, as it already contains sufficient information for downstream tasks. Transfer Learning approach is successful as the model retains previously learned encoding when fitting specific tasks like QA. The positional embedding in Transformer networks is crucial for performance, maintaining its effects even in late layers. Visualizations on the SQuAD dataset show this behavior. Fine-tuning on SQuAD improves model performance from layer 5 onwards, highlighting the relevance of question type probing tasks. The model fine-tuned on SQuAD outperforms the base model from layer 5 onwards, showing the importance of resolving question types. In contrast, the model fine-tuned on bAbI tasks loses its ability to distinguish question types. Surprisingly, the model fine-tuned on HotpotQA does not outperform the model without fine-tuning. This suggests that BERT-large is pre-trained to recognize question types. These findings shed light on the inner workings of Transformer networks. Our work on Transformer networks reveals that BERT-large is pre-trained to recognize question types, with interpretable information stored in hidden states. Lower layers may be more useful for certain problems. Future work can explore methods to process this information further. Transfer Learning and Modularity in Transformer networks suggest that lower layers may be more suitable for specific tasks, indicating a potential modularity that can be leveraged in training. Further exploration of skip connections and layer depth customization based on tasks is recommended. This work aims to uncover internal processes within Transformer models. Our work focuses on directing research towards understanding state-of-the-art models and how they solve tasks, aiming to improve upon them by revealing internal processes within Transformer-based models."
}