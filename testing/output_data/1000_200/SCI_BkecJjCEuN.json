{
    "title": "BkecJjCEuN",
    "content": "The present work aims to enhance the label efficiency of large neural networks for audio data by combining multitask learning and self-supervised learning on unlabeled data. An end-to-end audio feature extractor based on WaveNet is trained, along with simple task-specific neural networks. Various self-supervised learning tasks are implemented on unlabeled audio data, resulting in up to 6% improvement in supervised classification tasks with limited labeled data. Data augmentation is also incorporated into the multitask setting. Incorporating self-supervised tasks and data augmentation in multitask learning improves classification performance by up to 6%. Deep neural networks are crucial for modeling auditory data but face challenges with limited labeled data. Effective unsupervised learning is explored to enhance generalization. Incorporating self-supervised auditory tasks during model training can significantly improve performance in deep neural networks for audio-related tasks. Using WaveNet as a feature extractor from raw waveform data enables the learning of multi-scale hierarchical representations, leading to efficient and robust adaptation to task variations. This approach is demonstrated to enhance performance in supervised classification tasks such as audio tagging, speaker identification, and speech command recognition. Incorporating self-supervised auditory tasks during model training can significantly improve performance in deep neural networks for audio-related tasks. The framework explores supervised classification tasks like audio tagging, speaker identification, and speech command recognition, showing the leverage of unlabeled data to enhance performance. Results suggest that self-supervised tasks can be used for pre-training and transfer learning, complementing common data augmentation techniques. The literature discusses sensory environments, utilizing Gabor filters for visual processing and gammatone filters for auditory processing. Multitask learning aims to uncover underlying structures for better single-task performance with less data. Self-supervised learning leverages unlabeled data to address label scarcity and has shown promising results in the visual domain. The text discusses self-supervised learning in the audio domain, utilizing an end-to-end audio processing network modeled after WaveNet architecture. The network finds a common embedding of the acoustic waveform within a \"trunk\" network and is trained jointly with task-specific \"head\" networks for each experiment. The WaveNet trunk consists of 3 blocks of 6 dilation stacks, each with a gate and filter module. It has an effective receptive field length of 190 samples. Tested on audio tagging, speaker identification, and speech command recognition tasks with up to three self-supervised tasks. The audio tagging task is trained on the FSDKaggle2018 dataset with 11,073 audio files. Each segment is cropped to 2 seconds and padded with zeros if needed. The WaveNet trunk produces embeddings that are averaged across time and fed into a fully-connected layer for classification. Training involves minimizing cross entropy with one-hot encoded labels. The speaker identification task is trained on the VoxCeleb-1 dataset with 336 hours of data from 1251 speakers. Each audio segment is cropped to 2 seconds and normalized before being fed into the network. The architecture includes a global average pooling layer, 2-layer perceptron with 1024 units per layer, batch normalization, and ReLU nonlinearity. The speech command recognition task utilizes a head architecture with a global average pooling layer, 2-layer perceptron with 1024 units per layer, batch normalization, ReLU nonlinearity, and a softmax layer. It is trained on the Speech Commands dataset containing 65,000 utterances of 30 short words. The head consists of three 1D convolutions with batch normalization, dropout, and ReLU nonlinearity between each layer. The convolution layers have widths of 100, 50, and 25 with respective strides of 16, 8, and 4. The output is evaluated using cross-entropy loss. The self-supervised tasks in the multitask framework for audio involve next-step prediction, noise reduction, and upsampling. These tasks are easily implemented and can be paired with the main supervised tasks. The auxiliary tasks were trained on both the main task's data and unlabeled data from the Librispeech dataset. All three auxiliary tasks share the same basic head architecture with convolutional layers and a regression-type loss function. The multitask framework for audio includes tasks like next-step prediction, noise reduction, and upsampling. These tasks are easily implemented and can be paired with main supervised tasks. The models used in the framework focus on waveform inputs rather than high-level feature representations like spectrograms to allow for a wider range of audio processing tasks. State-of-the-art baseline models for different tasks may vary in network architectures, limiting information gained from self-supervised tasks. In the context of audio multitask learning, emphasis is placed on models that make minimal assumptions about input representation. Multitask training with three self-supervised tasks has shown benefits for supervised tasks, improving performance without increasing training data. Future work aims to narrow the performance gap between models trained on spectral representations and waveforms. Incorporating larger versions of Librispeech into training regimen improved performance metrics with each increase in unlabeled dataset size. Speech command classification and speaker identification tasks also showed improvement with additional unlabeled data, with top-5 classification performance peaking at 75.22% for speaker identification. Multitask learning improved classification performance on the VoxCeleb dataset, reaching 75.22% from a baseline of 73.81%. Comparing this with data augmentation techniques like pitch shifting and additive noise, pitch-shift augmentation showed a significant increase in performance, while noise augmentation had a smaller impact. The benefits of noisy data augmentation were similar to those achieved through multitask learning. Training with both pitch-shift augmentation and additional self-supervised tasks led to the highest performance increase of .089, suggesting that these methods are complementary for improving label efficiency. Transfer learning from self-supervised tasks trained on unlabeled data to supervised tasks was considered, with a focus on pre-training self-supervised tasks before fine-tuning with labeled data. The study focused on pre-training self-supervised tasks on unlabeled data before fine-tuning with a smaller quantity of labeled data. Transfer learning experiments favored this approach over simultaneously training all tasks together. The research showed that jointly training supervised tasks with multiple self-supervised tasks using a WaveNet-based model on raw audio waveforms led to improved performance, especially with limited labeled data. The performance gains scaled with the quantity of unlabeled data and could enhance existing data augmentation methods. The study focused on pre-training self-supervised tasks on unlabeled data before fine-tuning with a smaller quantity of labeled data. Transfer learning experiments favored this approach over simultaneously training all tasks together. The research showed that jointly training supervised tasks with multiple self-supervised tasks using a WaveNet-based model on raw audio waveforms led to improved performance, especially with limited labeled data. The performance gains scaled with the quantity of unlabeled data and could enhance existing data augmentation methods. The methodology and results suggest potential for broader applications in supervised audio tasks. Our model, based on the WaveNet architecture, utilizes causal dilated convolutions to process high temporal resolution raw audio signals efficiently. This approach enables us to handle a variety of auditory tasks effectively. The WaveNet architecture utilizes causal dilated convolutions to process raw audio signals efficiently, with task-specific neural networks built on top of a task-agnostic trunk. The trunk consists of stacked dilated causal convolution layers with residual connections and nonlinearities. The WaveNet trunk utilizes residual atoms for computations, producing hidden state vectors and layer outputs. Each layer applies causal convolutions to raw audio waveforms to generate outputs. The effective receptive field of each block increases with each layer, resulting in a total effective receptive field for the trunk. The WaveNet trunk consists of 3 blocks with 6 layers each, resulting in a total receptive field of 190. Each task-specific head processes input data after passing through the shared trunk. Tasks have their own objective functions and optimizers. Supervised tasks are primary, while self-supervised tasks are auxiliary. In the experiments, supervised tasks are designated as primary, while self-supervised tasks are considered auxiliary. The head architectures are designed to be simple to allow the shared trunk to learn representations suitable for multiple audio tasks. The next-step prediction task involves predicting the next value in a sequence of audio waveform frames. The next-step prediction head in the audio waveform model uses a 2-layer stack of 1 \u00d7 1 convolutional layers to predict the next audio frame in the sequence. It treats the prediction as a regression problem, using mean squared error as the loss function. The audio waveform model uses actual values as a loss function for next-step prediction, treating it as a regression problem. In the noise reduction task, noise is treated as an additive random process on top of the true signal. The model is trained to predict the clean sample given a window of noisy data. The model is trained to predict the clean sample from noisy data by minimizing a smoothed L1 loss between the clean and noisy versions of the waveform inputs. The noise reduction head has a structure similar to the next-step head, with models well-adapted to both tasks. Smooth L1 loss is used for stability in convergence for denoising, and an unsupervised upsampling task can be easily created in a similar spirit. The unsupervised upsampling task involves downsampling the audio source and using the downsampled signal as input data. The network's job is to infer the high frequency information lost during the transform, similar to noise reduction and next-step prediction tasks. The original audio is first downsampled to 4 kHz and then repeated every time-point 4 times to mimic the original signal's 16 kHz sample rate. The upsampling task involves inferring high frequency information lost during the transform using a structure similar to noise reduction and next-step prediction tasks. The model is trained on raw audio waveform inputs from FSDKaggle2018 and Librispeech datasets, with inputs normalized to [-1, 1] and cropped to two seconds in duration. Samples shorter than 2 seconds are zero padded, and noise-reduction task requires noisy inputs obtained by adding noise to the samples. The noise-reduction task involved adding noise sampled from ChiME3 datasets at various SNR levels. A hyperparameter search was conducted for the network architecture, with variations in the number of blocks, layers per block, and learning rate. The network's performance was found to be largely unaffected by specific architecture specifications. The network's training characteristics were not significantly impacted by the architecture specifications. Hyperparameters were chosen based on performance on both the main and auxiliary tasks, with a uniform weighting strategy used during training. The \"Adam\" optimizer was utilized for optimization. In our experiments, we used a uniform weighting strategy and the \"Adam\" optimizer with specific parameters. The learning rate was decayed every 5 epochs for improved convergence. A batch size of 48 was used due to computational constraints. Noise reduction and upsampling tasks required separate forward propagations. Important model parameters can be found in TAB3."
}