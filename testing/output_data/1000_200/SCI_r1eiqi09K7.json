{
    "title": "r1eiqi09K7",
    "content": "Several first order stochastic optimization methods commonly used in the Euclidean domain have been adapted to certain Riemannian settings. However, popular optimization tools like Adam, Adagrad, and Amsgrad have not been generalized to Riemannian manifolds. The difficulty of generalizing adaptive schemes to Riemannian settings is discussed, and algorithms and convergence proofs are provided for geodesically convex objectives in the case of a product of Riemannian manifolds. The generalization allows adaptivity across manifolds in the cartesian product, with experimentally faster convergence shown. Developing powerful stochastic gradient-based optimization algorithms is crucial for various application domains, especially when dealing with a large number of parameters. Riemannian adaptive methods have shown faster convergence and lower train loss values compared to standard algorithms. This is particularly evident in tasks like embedding the WordNet taxonomy in the Poincare ball. Popular first order methods like ADAGRAD, ADADELTA, and ADAM have been successful in addressing this new need for efficient optimization in engineering and computational sciences. Recent developments in optimization algorithms have focused on parameters residing on a Riemannian manifold, allowing for non-Euclidean geometries. These algorithms have been applied in various areas such as solving Lyapunov equations, matrix factorization, and dictionary learning. Some first order stochastic methods have been adapted to this setting, with Riemannian stochastic gradient being a seminal example. In this work, the focus is on generalizing adaptive optimization algorithms to the Riemannian setting. The adaptivity of these algorithms assigns a learning rate per coordinate of the parameter vector, but on a Riemannian manifold, intrinsic coordinate systems are not given, making notions like sparsity or coordinate-wise updates meaningless. The authors explain the challenges in generalizing adaptive schemes to the Riemannian setting and propose new algorithms to address this issue. In this work, the authors propose generalizations of adaptive optimization algorithms in the Riemannian setting, specifically focusing on a product of manifolds. They empirically support their claims with hyperbolic taxonomy embedding tasks. The motivation behind developing Riemannian versions of ADAGRAD and ADAM was to learn symbolic embeddings in non-Euclidean spaces, such as the GloVe algorithm. The absence of Riemannian adaptive algorithms could hinder the development of competitive optimization-based Riemannian embedding methods, especially in hyperbolic spaces. A manifold is a space that can be approximated by a Euclidean space and is a generalization of the notion of surface. A Riemannian manifold is a space where a Riemannian metric defines the geometry locally. It consists of a pair (M, \u03c1) where M is the manifold and \u03c1 is the metric. The metric induces a distance function on M, allowing for the calculation of distances between points using geodesics. In a Riemannian manifold (M, \u03c1), Riemannian SGD is defined by updating along the shortest path in the relevant direction while staying in the manifold. This is done using the exponential map or a retraction map when the former is not known in closed-form. In practice, when exp x (v) is not known in closed-form, it is common to replace it by a retraction map R x (v), most often chosen as R x (v) = x + v. ADAGRAD rescales updates coordinate-wise based on past gradients, beneficial for sparse gradients or deep networks. ADAM's update rule includes momentum and adaptivity terms, with similarities to the method RMSPROP. DISPLAYFORM1 where m t = \u03b2 1 m t\u22121 + (1\u2212\u03b2 1 )g t is a momentum term, while DISPLAYFORM2 2 is an adaptivity term. When \u03b2 1 = 0, it resembles RMSPROP, with the difference being an exponential moving average instead of a sum. This helps in forgetting past gradients over time in the adaptivity term v t. The momentum term in ADAM for \u03b2 1 = 0 has shown significant empirical improvements. BID18 discovered a mistake in ADAM's convergence proof and proposed AMSGRAD as a fix, or using an increasing schedule for \u03b2 2 in ADAMNC. In AMSGRAD, an increasing schedule for \u03b2 2 is used, called ADAMNC. Updates on a Riemannian manifold require a coordinate system. Intrinsic quantities on the manifold are independent of the chart used. The Riemannian gradient of a smooth function on a manifold can be defined intrinsically, but its Hessian is only defined at critical points. The RSGD update is intrinsic as it involves objects intrinsic to the manifold. It is unclear if Eqs. (3,4,5) can be expressed in a coordinate-free manner. The RSGD update involves parallel-transporting gradients along the optimization trajectory on a Riemannian manifold. Curvature introduces a rotational component to parallel transport, affecting the sparsity of gradients and adaptivity. The coordinate system for representing gradients depends on the optimization path, losing the interpretation of optimizing different features at different speeds. The text discusses the challenges of designing adaptive schemes on a Riemannian manifold due to the absence of intrinsic coordinates. It proposes a solution by considering each component individually. Proposing a simple adaptation for adaptive schemes on a Riemannian manifold by considering each component as a \"coordinate\" and rescaling the gradient with Riemannian norms. Mentioned ADAGRAD, ADAM, and AMSGRAD, with ADAM described as a combination of ADAGRAD with momentum and AMSGRAD as a modification of ADAM for improved convergence. ADAMNC is ADAM with a non-constant schedule for parameters. ADAMNC is a variant of ADAM with a non-constant schedule for parameters, where the schedule proposed by BID18 for \u03b2 2 allows v t to recover the sum of squared-gradients of ADAGRAD. This modification results in ADAMNC without momentum yielding ADAGRAD. The assumptions and notations involve geodesically complete Riemannian manifolds with sectional curvature lower bounded by \u03ba i \u2264 0, and the projection operator \u03a0 Xi, parallel transport exp i, and logarithm log i are defined for feasible parameters on the product manifold. The Riemannian AMSGRAD algorithm is presented alongside the standard AMSGRAD algorithm in FIG1. RADAM and ADAM can be derived by removing the max operations from these algorithms. The convergence guarantee for RAMSGRAD is presented in Theorem 1, with the quantity \u03b6 defined. Convergence guarantees between RAMSGRAD and AMSGRAD coincide when (Mi, \u03c1i) = R for all i. The regret bound worsens at a speed of approximately 1 + D\u221e|\u03ba|/6 when the curvature is small but non-zero. Similar remarks apply to RADAMNC, with its convergence guarantee shown in Theorem 2. The convergence guarantees for RAMSGRAD and RADAMNC are presented in Theorems 1 and 2 respectively. The role of convexity is highlighted, with a comparison between convexity and geodesic convexity in differentiable functions. The notion of convexity in Theorem 5 was replaced by geodesic convexity in Theorem 1 for differentiable functions. Regret bounds for convex objectives involve bounding the difference between current and optimal values. In the Riemannian case, this involves \u03c1 xt (g t , \u2212 log xt (x * )). Using a cosine law, one can bound the term g t , x t \u2212 x * for an SGD update. In Riemannian manifolds, the bound for the second term requires a decreasing schedule for \u03b1. The lemma introduced by (reference) in Alexandrov spaces generalizes this step, applicable to geodesically convex subsets with lower bounded sectional curvature. The curvature dependent quantity \u03b6 from Eq. (10) is used to bound \u03c1 DISPLAYFORM11, showing the benefit of adaptivity. Improved bounds are seen for sparse gradients, especially when updating just a few words at a time on a manifold. The choice of \u03d5 i does not need to be specified for convergence theorems, indicating potential regret bounds. The proposed algorithms RADAM, RAMSGRAD, and RADAGRAD are empirically assessed compared to the non-adaptive RSGD method. The WordNet noun hierarchy is embedded in the Poincar\u00e9 model of hyperbolic geometry, known for its suitability in embedding tree-like graphs. Each word is embedded in a space of constant curvature, suggesting potential benefits for optimization tools in other algorithms. The Poincar\u00e9 model is used to embed the WordNet noun hierarchy in a space of constant curvature. The transitive closure of the WordNet taxonomy graph consists of 82,115 nouns and 743,241 hypernymy Is-A relations. The distance function, geodesics, exponential and logarithmic maps, and parallel transport are all defined in this model. The transitive closure of the WordNet taxonomy graph includes 82,115 nouns and 743,241 hypernymy Is-A relations. The embedding minimizes distances between connected words while maximizing distances otherwise. The loss function used is similar to log-likelihood, with negative word pair sampling. Metrics include loss value and mean average precision for link prediction and reconstruction tasks. The study focused on using hyperbolic spaces for link prediction and reconstruction tasks in the WordNet taxonomy graph. Training details included a burn-in phase with fixed learning rate and negative word sampling based on graph degree. RADAM optimization method showed slightly better results than RAMS-GRAD. The study compared RADAM and RAMS-GRAD optimization methods, with RADAM showing slightly better results. They observed convergence to lower loss values when using a first-order approximation of the exponential map. Retraction-based methods were reported separately as they are not directly comparable to fully Riemannian methods. Results for exponential and retraction-based methods were shown in FIG2 with different learning rates tested. RADAM consistently achieves the lowest training loss and outperforms all other methods on the MAP metric for both reconstruction and link prediction settings in the full Riemannian setting. In the \"retraction\" setting, RADAM reaches the lowest training loss value and performs on par with RSGD on the MAP evaluation for both reconstruction and link prediction settings. RAMSGRAD converges faster in terms of MAP. RAMSGRAD is faster to converge in terms of MAP for link prediction, suggesting better generalization capability. Various first-order Riemannian methods have emerged after Riemannian SGD, including Riemannian SVRG and Riemannian accelerated gradient descent. Stochastic gradient Langevin dynamics was also generalized for optimization on the probability simplex. Riemannian counterparts of SGD with momentum and RMSprop were proposed by BID20, with the idea of transporting the momentum term using parallel translation. The authors propose a generalized version of Riemannian ADAM for the Grassmann manifold G(1, n), focusing on adaptivity across manifolds and providing convergence analysis. This builds on previous work on Riemannian counterparts of SGD with momentum and RMSprop, emphasizing the importance of transporting the momentum term using parallel translation. The authors propose a generalized version of Riemannian ADAM for the Grassmann manifold G(1, n), focusing on adaptivity across manifolds and providing convergence analysis. They derive convergence rates similar to Euclidean models and show superior performance compared to non-adaptive methods like RSGD in hyperbolic word taxonomy embedding tasks. The proposed method extends popular adaptive optimization tools to Cartesian products of Riemannian manifolds in a principled and intrinsic manner. The text discusses the use of Cauchy-Schwarz' and Young's inequalities in proving convergence of gradient-based optimization algorithms for geodesically convex functions in Alexandrov spaces. It also introduces a user-friendly inequality known as the Cosine inequality in Alexandrov spaces. If the sides of a geodesic triangle in an Alexandrov space have curvature lower bounded by \u03ba, and A is the angle between sides b and c, then lemma 7 presents an analogue of Cauchy-Schwarz inequality. Lemma 8 states an inequality for non-negative real numbers y 1 , ..., y t. These lemmas are used in convergence proofs for optimization algorithms in Alexandrov spaces."
}