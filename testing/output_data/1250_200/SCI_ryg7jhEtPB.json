{
    "title": "ryg7jhEtPB",
    "content": "The importance weighted autoencoder (IWAE) is a variational-inference method that achieves a tighter evidence bound than standard variational autoencoders by optimizing a multi-sample objective. However, IWAE relies on reparametrizations and faces challenges with increasing Monte Carlo samples. Alternative approaches like IWAE-STL and IWAE-DREG have been proposed to address these issues. This work argues for directly optimizing the proposal distribution in importance sampling, as in the reweighted wake-sleep (RWS) algorithm, over IWAE-type multi-sample objectives. The importance weighted autoencoder (IWAE) achieves a tighter evidence bound than standard variational autoencoders by optimizing a multi-sample objective. An adaptive importance sampling framework called AISLE generalizes the RWS algorithm and includes IWAE-STL and IWAE-DREG as special cases. The goal is to learn the generative model and construct a variational approximation. In the context of optimizing a multi-sample objective, different algorithms like IWAE-DREG and RWS have been proposed for improving the generative model and variational approximation in neural-network applications. These algorithms aim to reduce errors by employing Monte Carlo samples and addressing problematic score-function terms. IWAE-DREG and RWS are algorithms for optimizing generative models and variational approximations in neural networks. IWAE-DREG removes problematic score-function terms from the IWAE gradient, while RWS optimizes separate objectives for \u03b8 and \u03c6 using self-normalized importance sampling. RWS improves its proposal distribution iteratively and does not degenerate as K \u2192 \u221e. IWAE is popular but suffers from \u03c6-gradient breakdown, while RWS has shown superior empirical performance. In this work, it is shown that directly optimizing the proposal distribution, as done by RWS, is preferable to optimizing the IWAE multi-sample objective. The IWAE approach relies on reparametrizations and can lead to gradient breakdown, while modifications like IWAE-STL and IWAE-DREG can be justified from an adaptive importance-sampling perspective. This conclusion aligns with previous findings that IWAE can be inferior to RWS, especially for discrete latent variables. The authors introduce a generic adaptive importance-sampling framework called AISLE, building on the principles of RWS for variational inference. The adaptive importance-sampling framework AISLE is introduced for variational inference, encompassing RWS, IWAE-DREG, and IWAE-STL gradients. Connections are established, showing how IWAE-STL can be derived from AISLE, providing a theoretical foundation for IWAE-STL and admitting IWAE-DREG as a special case. The AISLE framework introduces new gradient estimators for variational inference, including IWAE-DREG and IWAE-STL. It shows connections between IWAE-STL and AISLE, providing a theoretical foundation and proving that AISLE also admits IWAE-DREG as a special case. The learning rate scaling differs between IWAE \u03c6-gradient and AISLE, leading to a new family of gradient estimators for \u03b1-divergences. The focus is on providing insights into the impact of self-normalization bias on importance-sampling based gradient approximations. The AISLE framework introduces new gradient estimators for variational inference, including IWAE-DREG and IWAE-STL. It shows connections between IWAE-STL and AISLE, providing a theoretical foundation and proving that AISLE also admits IWAE-DREG as a special case. The learning rate scaling differs between IWAE \u03c6-gradient and AISLE, leading to a new family of gradient estimators for \u03b1-divergences. The focus is on the impact of self-normalization bias on importance-sampling based gradient approximations. Special cases of AISLE are preferable, with empirical comparisons available in related works. Notation and background on estimating expectations using importance weights are discussed. The self-normalised estimate \u03c0 \u03b8 \u03c6, z (f) is biased but its bias vanishes at rate O(K \u22121). The importance weighted autoencoder (IWAE) aims to maximize a lower bound L K \u03c8 on the log-marginal likelihood by optimizing the inference-network parameters \u03c6 and number of samples, K \u2265 1. For K > 1, IWAE extends VAE on an auxiliary-variable construction. The gradient of the IWAE objective tightens the evidence bound as K \u2192 \u221e. The IWAE objective from Andrieu & Roberts (2009) aims to maximize a lower bound on the log-marginal likelihood by using an auxiliary-variable construction. The gradient of the IWAE objective can be approximated using a reparametrisation trick to reduce high variance. The IWAE then uses a vanilla Monte Carlo estimate to tighten the evidence bound. The IWAE objective aims to maximize a lower bound on the log-marginal likelihood using an auxiliary-variable construction. The gradient of the IWAE objective can be approximated to reduce high variance, but it has drawbacks such as reliance on reparametrisations, vanishing signal-to-noise ratio, and inability to achieve zero variance. Various modifications have been proposed to address these issues. The IWAE objective aims to maximize a lower bound on the log-marginal likelihood using an auxiliary-variable construction. Modifications have been proposed to address issues such as high variance, vanishing signal-to-noise ratio, and inability to achieve zero variance. Two modifications, IWAE-STL and IWAE-DREG, have been proposed to remove score-function terms and reduce bias. The RWS algorithm and self-normalised importance sampling approximation are used to approximate intractable quantities. The self-normalised importance sampling approximation in RWS allows for simultaneous optimization of both \u03b8 and \u03c6, sharing particles and weights. The bias relative to traditional importance sampling is discussed, with the main drawback being the lack of a joint objective for \u03b8 and \u03c6. Optimizing \u03c6 can reduce error in \u03b8-gradient approximation, as Monte Carlo samples can be reused. Adapting the proposal distribution q \u03c6 in importance-sampling schemes is necessary in high-dimensional settings. Optimizing \u03c6 in importance-sampling schemes can reduce error in the \u03b8-gradient approximation. Various techniques exist for adapting the proposal distribution q \u03c6, such as minimizing the \u03c7 2 -divergence. The RWS-objective is slightly generalized to include minimizing \u0192-divergences, leading to the AISLE algorithm. This unified framework allows for the derivation of robust \u03c6-gradient estimators that remain stable as the number of samples increases. The unified framework of AISLE allows for the derivation of robust \u03c6-gradient estimators that do not degenerate with increasing samples. The \u03b8-gradient is the same for all algorithms discussed, interpreted differently by IWAE and AISLE. Integrals involving \u03c0 \u03b8 ([F \u2022 w \u03c8 ]\u2207 \u03c6 log q \u03c6 ) can be approximated using the vanilla Monte Carlo method with bias and standard deviation of order O(K \u22121 ) and O(K \u22121/2 ) respectively. Most \u0192-divergences used in variational inference have a function f with an exponent \u03ba and constant C(\u03b8) independent of \u03c6. Variational inference in intractable models involves optimizing the \u0192-divergence with a function f and exponent \u03ba. Self-importance sampling can approximate integrals with respect to \u03c0 \u03b8. Reparameterized estimators can be derived from AISLE, providing robust \u03c6-gradient estimators. Using reparametrisation, Equation (13) yields the gradient for IWAE-STL, derived in a principled manner from AISLE without the need for a multi-sample objective. Proposition 1 shows that IWAE-STL can achieve zero variance and provides a theoretical basis for it, previously seen as biased and heuristically justified. The empirical performance of IWAE-STL in Tucker et al. (2019) suggests that the breakdown in RWS may not be due to its lack of optimizing a joint objective. This is achieved by replacing the exact \u03c6-gradient with a self-normalized importance-sampling approximation and applying an identity from Lemma 1, resulting in variance reduction without changing the bias of the gradient estimator. Without relying on any reparametrisation, AISLE-\u03b1-NOREP yields a special case proportional to the 'score gradient' from Dieng et al. (2017, Appendix G). AISLE-\u03b1, using reparametrisation, results in a gradient derived from Equation (12). The special case of AISLE-\u03b1 is proportional to the 'score gradient' from Dieng et al. (2017, Appendix G). IWAE-DREG can be derived from AISLE without the need for a multi-sample objective. The learning rate needs to be scaled as O(K) for IWAE or IWAE-DREG \u03c6-gradients. The 'exclusive' KL-divergence can lead to faster convergence of \u03c6. The 'exclusive' KL-divergence proposed in Roeder et al. (2017, Equation 8) can lead to faster convergence of \u03c6. However, minimizing this divergence may negatively impact learning of \u03b8. The adaptive-importance sampling paradigm of reweighted wake-sleep (RWS) is preferred over the multi-sample objective paradigm of importance weighted autoencoders (IWAEs) as it achieves the same goals while avoiding drawbacks. In the self-normalised importance-sampling approximation, the number of particles, K, affects the accuracy of the approximation of \u03c0 \u03b8 (f). The number of particles, K, impacts the accuracy of the approximation of \u03c0 \u03b8 (f). As K increases, estimators become more accurate, while for K = 1, they reduce to vanilla Monte Carlo approximations. The small-K self-normalisation bias of AISLE \u03c6 gradients favors minimizing the exclusive KL-divergence. The small-K self-normalisation bias of AISLE \u03c6 gradients favors minimizing the exclusive KL-divergence, which can be controlled by ensuring q \u03c6 is close to \u03c0 \u03b8 in parts of the space Z with positive probability mass. The error of importance-sampling approximations is small when the 'inclusive' KL-divergence is small, indicating well-behaved importance weights. In Scenario 1, if the variational family Q is sufficiently flexible to contain a distribution close to \u03c0 \u03b8, minimizing the exclusive KL-divergence can yield well-behaved importance weights. However, in Scenario 2, if Q is not flexible enough and all its members are far from \u03c0 \u03b8, minimizing the exclusive KL-divergence could lead to poorly-behaved importance weights. In such cases, optimizing the KL-divergence in the reverse order may be more effective. In some scenarios, a gradient-descent algorithm may converge faster than an inclusive divergence minimization approach. In these cases, a smaller number of particles could be preferable for certain gradients due to self-normalization bias. However, setting the number of particles to 1 is not always optimal as increasing K can help reduce gradient approximation variance. It is important to utilize all particles and weights for gradient approximation to avoid poorly behaved importance-sampling approximations. In scenarios where gradient-descent converges faster, using fewer particles may be preferred due to self-normalization bias. However, increasing the number of particles can help reduce gradient approximation variance. It is crucial to utilize all particles and weights for accurate gradient approximation to avoid unreliable importance-sampling approximations. Different \u03c6-gradient estimators are compared, including AISLE-KL-NOREP, AISLE-KL, and AISLE-\u03c72-NOREP, each with their own characteristics and requirements. The gradient estimators compared include AISLE-\u03c72-NOREP, AISLE-\u03c72, IWAE, IWAE-DREG, and RWS-DREG, each with their own characteristics and requirements. The gradients for AISLE are based on the \u03c72-divergence, while IWAE employs the reparametrisation trick. IWAE-DREG and RWS-DREG are 'doubly-reparametrised' gradients proposed in previous studies. The 'doubly-reparametrised' IWAE gradient IWAE-DREG and RWS-DREG are discussed, with a focus on the generative model and proposal distributions used in the model. The parameters for optimization and reparametrisation trick are also highlighted. The proposal model in (20) aligns with the posterior mean in (19) when A = P and b. This model differs from previous benchmarks by allowing correlated latent vectors z in the generative model. Despite this, the variational approximation remains fully factored, potentially limiting its ability to capture uncertainty. The variance of the gradients in AISLE-KL/IWAE-STL and AISLE-\u03c7 2 /IWAE-DREG approaches zero as C \u2192 1 2 I, indicating minimal variance for the proposal mean parameters. The variance of the gradients in the model approaches zero as the parameters fall within a neighborhood of their optimal values. A comparison of algorithms is conducted with varying numbers of particles and model dimensions. The focus is on optimizing \u03c6 while fixing \u03b8. The focus is on optimizing \u03c6 while fixing \u03b8. Results for different model settings are shown, including scenarios where the generative model is specified via different covariance matrices. Gradient-ascent algorithms are used with different parameter values and normalizations. The total number of iterations is 10,000; learning-rate parameters at each step are i \u22121/2. The covariance matrix \u03a3 is not diagonal, with logarithmic scaling on the second axis."
}