{
    "title": "HyeuFOcMyX",
    "content": "Structural planning is crucial for generating long sentences, a component lacking in current language models. This study introduces a planning phase in neural machine translation to control sentence structure. The model generates planner codes to guide word prediction, improving translation performance. Evidence from linguists suggests that speakers plan ahead during speech, a process not present in NMT models. In neural machine translation, a planning phase is introduced to control sentence structure. Planner codes are generated to guide word prediction, improving translation performance. This approach aims to address the lack of structural planning in current language models. The planning phase in neural machine translation involves generating planner codes to guide word prediction and improve translation performance by controlling sentence structure. This approach addresses the lack of structural planning in current language models. The codes are learned through a discretization bottleneck in an end-to-end network that reconstructs the syntactic structure of the translation. Experiments show that incorporating structural planning generally enhances translation performance and allows for control over the output structure. In the planning phase of neural machine translation, planner codes are generated to improve translation performance by controlling sentence structure. The codes are learned through a discretization bottleneck in an end-to-end network. Experiments show that incorporating structural planning enhances translation performance and allows for control over the output structure. The process involves simplifying POS tags to extract coarse structural annotations for sentence generation. The process involves learning planner codes to control sentence structure in neural machine translation. The codes are generated through a discretization bottleneck in an end-to-end network, enhancing translation performance and allowing for output structure control. The method simplifies POS tags to extract coarse structural annotations for sentence generation. The architecture of the code learning model involves initializing a decoder LSTM to predict tags sequentially, optimizing parameters with crossentropy loss, and obtaining planner codes for target sentences. These codes are used in training data pairs with an extra context input, and a regular NMT model is trained using the modified dataset. Beam search is used during decoding, with planner codes searched before emitting real words. The code learning model involves using beam search during decoding sentences, with planner codes searched before emitting real words. Various methods have been proposed to improve syntactic correctness in translations, such as restricting the search space of the NMT decoder and incorporating target-side syntactic structures explicitly. Some models generate words and parse actions simultaneously, conditioning word prediction on action prediction. However, none of these methods plan the structure before translation. The code learning model involves compressing word embeddings into concept codes for faster decoding. Evaluation is done on different tasks using various tools like Kytea and moses toolkit. The model is trained using Nesterov's accelerated gradient for a set number of epochs. Different settings of code length and code types are tested, showing a trade-off between accuracy and information capacity. The code learning model involves compressing word embeddings into concept codes for faster decoding. There is a trade-off between S Y accuracy and C Y accuracy, with a balanced setting of N = 2, K = 4. The NMT model uses 2 layers of bidirectional LSTM encoders and 2 layers of LSTM decoders, with Key-Value Attention in the first decoder layer. Dropout is applied with a rate of 0.2, and the NAG optimizer is used for training. Best parameters are chosen based on a validation set. The code learning model involves compressing word embeddings into concept codes for faster decoding. Best parameters are chosen based on a validation set. By conditioning word prediction on generated planner codes, translation performance improves over a strong baseline. Greedy search on JaEn dataset results in lower BLEU score compared to baseline. Beam search followed by greedy search does not significantly change results. It is important to explore multiple candidates with different structures simultaneously on Ja-En task. Planning ahead allows for exploring more diverse candidates, improving beam search but not greedy search. The performance of beam search depends on candidate diversity. Manually choosing planner codes instead of letting beam search decide can also be effective. The model can produce different translations by manipulating planner codes, leading to diverse paraphrased translations. The distribution of learned codes for English sentences in the dataset shows variability in code assignment. This method can be useful for sampling diverse translations. The study explores the distribution of codes assigned to English sentences in a dataset, indicating potential for improvement. Instead of learning discrete codes, the model can predict structural annotations like POS tags, but this may lead to performance degradation. A planning phase is added to neural machine translation to generate planner codes for controlling sentence structure, improving translation performance. The method allows for sampling translations with diverse structures. The planning phase in neural machine translation generates planner codes to control sentence structure, improving translation performance. This method allows for sampling translations with diverse structures and can be extended to plan other latent factors like sentiment or topic."
}