{
    "title": "SJlmNI0ojQ",
    "content": "End-to-end acoustic-to-word speech recognition models have gained popularity for their ease of training, scalability to large datasets, and lack of a lexicon requirement. Contextual acoustic word embeddings are constructed directly from a supervised sequence-to-sequence model, showing competitive performance on standard sentence evaluation tasks and spoken language understanding. Learning fixed-size representations for variable length data like words or sentences is a key focus in this research. Learning fixed-size representations for variable length data like words or sentences is a key focus in research, with methods like word2vec, GLoVE, CoVe, and ELMo gaining popularity in natural language processing. In speech recognition, the challenge lies in dealing with short-term audio features instead of words or characters, requiring techniques like aligning speech and text or chunking input speech into fixed-length segments. Our work focuses on constructing individual acoustic word embeddings grounded in utterance-level acoustics, unlike techniques that ignore specific audio context. We present methods for obtaining these embeddings from an attention-based sequence-to-sequence model trained for direct Acoustic-to-Word speech recognition. By automatically segmenting and classifying input speech into individual words at the utterance level, we eliminate the need for pre-defined word boundaries. Our contextual acoustic word embeddings are evaluated in a spoken language understanding task, showing their utility in non-transcription downstream tasks. Our work demonstrates the usability of attention for constructing Contextual Acoustic Word Embeddings (CAWE) from a speech recognition model. These embeddings are competitive with text-based word2vec embeddings on standard sentence evaluation benchmarks. Additionally, we show the utility of CAWE in a speech-based downstream task of Spoken Language Understanding, highlighting the potential for transfer learning from pretrained speech models. Prior work in speech recognition models has shown the need for large amounts of training data and vocabulary restrictions. Solutions have focused on generating out-of-vocabulary words by using smaller units like characters or sub-words. Recent advancements include S2S models for pure-word large vocabulary recognition with a 300-hour corpus. Building upon this work, improvements have been made in training these models for large vocabulary tasks, showing the ability to automatically learn word boundaries without supervision. Additionally, the models have been expanded towards learning acoustic embeddings. In this work, the model expands towards learning acoustic embeddings. Various methods explore ways to learn acoustic word embeddings, with some using unsupervised learning methods. BID4 proposes an unsupervised method to learn speech embeddings using a fixed context of words in the past and future. Learning text-based word embeddings is also a rich area of research with established techniques. The curr_chunk discusses the use of contextual word embeddings in a speech recognition model. It mentions the structure of the model, including the encoder and decoder networks, as well as the use of a pyramidal multi-layer bi-directional LSTM network. The decoder network generates targets based on previous predictions from a word vocabulary. The curr_chunk describes the method of obtaining acoustic word embeddings from an end-to-end trained speech recognition system. It utilizes an attention mechanism to generate targets and constructs contextual embeddings using hidden representations from the encoder and attention weights from the decoder. The model follows a similar experimental setup as word-based models but learns 300-dimensional acoustic feature vectors instead of 320-dimensional ones. The method described involves constructing contextual acoustic word embeddings using an attention mechanism to generate targets. It utilizes hidden representations from the encoder and attention weights from the decoder to segment continuous speech into words and obtain word embeddings. The attention weights on acoustic frames reflect their importance in classifying a particular word, allowing for the construction of word representations based on the importance of the frames within a given acoustic context. The method involves using attention weights on acoustic frames to construct word representations based on their importance within a given acoustic context. The model obtains mappings of words to acoustic frames and describes different ways of obtaining acoustic word embeddings using attention. Contextual Acoustic Word Embeddings (CAWE) are created by using attention weights on acoustic frames to map to a specific word. This involves attention-weighted average and maximum attention techniques, which are contextual due to the use of attention scores. Two datasets are used for evaluation: the Switchboard corpus with telephonic conversations and the How2 dataset with instructional videos. The CAWE involves attention-weighted techniques to create word embeddings from acoustic frames. Evaluation is done on Switchboard and How2 datasets. The A2W achieves word error rates of 22.2% on Switchboard and 36.6% on CallHome set. Embeddings are evaluated on various tasks including Semantic Textual Similarity, classification, sentiment analysis, and paraphrase detection. Training details involve using the SentEval toolkit for downstream evaluations. The CAWE-M outperforms U-AVG and CAWE-W by 34% and 13% on Switchboard and How2 datasets in terms of average performance on STS tasks. CAWE-W usually performs worse than CAWE-M due to noisy estimation of word embeddings. The CAWE-M outperforms U-AVG and CAWE-W on STS tasks. Training details involve comparing embeddings from the speech recognition model's training set with word2vec embeddings. The A2W model has limitations in capturing vocabulary. The A2W speech recognition model has limitations in capturing vocabulary, recognizing only a small percentage of words. Despite this, CAWE performance is competitive with word2vec CBOW. Evaluations show that CAWE-M outperforms word2vec embeddings on various tasks, especially in Switchboard dataset. Additionally, CAWE is evaluated on the ATIS dataset for Spoken Language Understanding. CAWE is evaluated on the ATIS dataset for Spoken Language Understanding, which consists of spoken language queries for airline reservations. The model architecture includes an embedding layer, a single layer RNN variant, and a dense layer with softmax. Training involves 10 epochs with RMSProp. Direct speech-based word embeddings show comparable performance to text-based word embeddings in this speech-based downstream task. The study compares speech-based word embeddings to text-based word embeddings in a speech-based downstream task, showing the utility of speech-based embeddings. Contextual acoustic word embeddings are learned from a speech recognition model, with attention playing a key role. These embeddings outperform traditional methods by up to 34% in semantic textual similarity tasks and match the performance of text-based embeddings in spoken language understanding. The model can be used as a pre-trained model for other speech-based tasks, suggesting that contextual audio embeddings can improve downstream tasks similar to text embeddings. In the future, the model will be scaled to larger corpora and vocabularies to compare with non-contextual acoustic word embedding methods. This research was supported by the Center for Machine Learning and Health at Carnegie Mellon University and Facebook."
}