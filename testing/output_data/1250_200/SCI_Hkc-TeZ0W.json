{
    "title": "Hkc-TeZ0W",
    "content": "A hierarchical model is introduced for efficient placement of computational graphs on hardware devices in heterogeneous environments. The method learns to assign graph operations to groups and allocate them to available devices without human intervention. Experiments show that the algorithm can optimize placements for TensorFlow graphs with over 80,000 operations, outperforming human placements. Our method achieves significant runtime reductions in training steps for models like Neural Machine Translation by efficiently assigning graph operations to hardware devices without human intervention. This approach outperforms previous state-of-the-art methods based on deep reinforcement learning and addresses the increasing demand for computational resources in training neural networks. Automated device placement is essential for efficiently distributing computation in neural networks. Traditional graph partitioning methods, such as Scotch, are used as a baseline for this task. Various optimization techniques are employed, including k-way Fiduccia-Mattheyses, Multilevel methods, the Band Method, the Diffusion Method, and Dual Recursive Bipartitioning Mapping. The goal is to balance the computational load across available devices. In an effort to efficiently distribute computation in neural networks, various optimization techniques were explored, including Multilevel methods, the Band Method, the Diffusion Method, and Dual Recursive Bipartitioning Mapping. However, the approach yielded disappointing results due to the non-stationarity of the cost function. Recent work has proposed using deep networks and reinforcement learning for combinatorial optimization, with a focus on predicting the placement of operations in a computational graph for faster computation. In this paper, a new approach is proposed for optimizing device placement in neural networks with a large number of operations. The method involves a two-level hierarchical network - the Grouper groups operations and the Placer assigns them to devices. This approach eliminates the need for manual graph partitioning and can scale to larger graphs efficiently. The Placer, a sequence-to-sequence model, predicts device placement for groups in a two-level network trained using reinforcement learning. The method is end-to-end, automating device placement optimization for large graphs without manual grouping. Results show superior placements for various models compared to default TensorFlow placements, Scotch algorithm, and human expert placements. The approach learns the tradeoff between computation and communication in hardware. The proposed approach, known as the Hierarchical Planner, aims to optimize device placement for neural network training. It consists of a Grouper and a Placer model trained jointly to assign operations to groups and groups to target devices. The method achieves a 60.6% reduction in training time per iteration on a Neural Machine Translation model. Policy gradients are used to train the models, which learn the tradeoff between computation and communication in hardware. The Hierarchical Planner optimizes device placement for neural network training by using a Grouper and Placer model. The Grouper assigns operations to groups, and the Placer computes device placements for each group. The models are trained using policy gradients to learn the tradeoff between computation and communication in hardware. The Grouper is a feed forward model with a softmax layer, while the Placer is a sequence-to-sequence model with LSTM and attention mechanism for predicting placements. Operation embeddings are generated for input to the Grouper, consisting of vectors for operation type information. The Grouper and Placer models optimize device placement for neural network training. The Grouper assigns operations to groups, while the Placer computes device placements for each group. Operation embeddings are generated for input to the Grouper, including vectors for operation type information, output sizes, and adjacency information. The Placer's RNN encoder reads group embeddings to produce hidden states, with M as a hyperparameter. The decoder predicts one device per time step, using an attention mechanism to attend over encoder states. Each device has a trainable embedding fed to the next decoder step. The decoder uses an attention mechanism to attend over encoder states at each step. It samples a device per step from the Placer's softmax and applies a temperature and tanh constant to make activations less steep. A policy gradient method is used to train the Hierarchical Planner to improve its decisions over time. The planner aims to maximize the expectation of a reward for device placement, optimizing a cost function with parameters for the Grouper and Placer. The Hierarchical Planner optimizes for DISPLAYFORM2 with parameters \u03b8 g and \u03b8 d for the Grouper and Placer. Derivatives of the cost function are calculated using the REINFORCE rule. The Grouper assigns operations independently, while the Placer conditions placement on already placed groups. Distributed training is done with a shared parameter server among controllers. In distributed training, the policy is trained using a parameter server shared among controllers. Each controller communicates with k worker nodes, where k is determined by the number of GPUs needed for runtime measurements. The workers run placements in parallel, and the controller computes gradients using measured runtimes. Variance in runtime measurements is reduced by maintaining separate baselines for each controller. Despite the benefits of using many workers, it is possible to achieve comparable results with fewer resources. In Section 3, we demonstrate the effectiveness of training the policy with fewer resources using a Hierarchical Planner on machine learning models in computer vision and natural language processing. Our approach outperforms heuristic and RL-based graph optimization baselines, showing the ability to find optimal placements. Comparisons with no grouping and random grouping methods highlight the hierarchical architecture's capability to learn better placements. The evaluation is conducted on four popular deep neural networks, including Inception-V3, known for various computer vision tasks. The curr_chunk discusses the execution of different deep learning models with varying batch sizes and operations in TensorFlow. The models include ResNet for image classification, RNNLM for language modeling, and NMT for machine translation with attention mechanism. Each model has a different number of operations and batch size, impacting their computational complexity. The curr_chunk discusses optimizing the placement of LSTM layers, attention, and softmax layers on separate devices to reduce training time. It compares different versions of NMT models with varying operations and evaluates them on ResNet and more complex NMT models. The approach outperforms baselines like CPU-only execution for large models due to GPU memory limitations. The curr_chunk discusses different placement strategies for optimizing model execution, including CPU-only, GPU-only, Scotch static mapper, MinCut optimizer, and human expert placements for various models like Inception-V3, ResNet, RNNLM, and NMT. The curr_chunk discusses the ColocRL method using policy gradient to place colocation groups on devices for optimizing model execution. Reward is based on runtime for training steps of the TensorFlow model, with invalid placements receiving a large negative reward. Runtime is measured in seconds for one training step, with the reward calculated over 10 steps to reduce variance. The experiments are conducted on machines with specific hardware configurations and software versions. The policy network architecture includes a Grouper and Placer with specific hidden sizes and model configurations. The best results were obtained with a group size of 256 and a specific number of unrolled steps in the Placer. In our experiments, we found that a group size of 256 yielded the best results. The Placer's softmax output size is determined by the number of available hardware devices. To promote exploration, noise was added to the logits of the Grouper and Placer networks for the initial 500 policy training steps. The policy is updated only for valid placements after the first 500 steps to prevent convergence to rewards associated with invalid placements. Results show that the Hierarchical Planner outperforms Graph Partitioning Heuristics. The Hierarchical Planner outperforms Graph Partitioning Heuristics by learning efficient placements for different models on GPUs, achieving significant reductions in runtime. For example, it learns to use a single GPU for ResNet and RNNLM, distribute Inception-V3 across 2 GPUs for a 16.3% reduction, and allocate NMT models across multiple GPUs for up to 53.7% faster runtime than human-generated placements. Experiments with 2, 4, and 8 GPUs show performance improvements of 60.6% for NMT (2-layer) and 53.7% for NMT (4-layer). The automated method for finding placements for NMT models on GPUs outperforms human expert baselines, with significant improvements in runtime. Comparisons with other methods like Scotch and MinCut show worse results. However, direct comparisons with ColocRL are challenging due to different software and hardware configurations. The Hierarchical Planner achieves a 60.6% improvement over best heuristics for NMT (2-layer) and shows promising results for NMT (4-layer) and NMT (8-layer). The Hierarchical Planner outperforms ColocRL in finding placements for NMT models on GPUs. It achieves a 60.6% improvement over best heuristics for NMT (2-layer) and shows promising results for NMT (4-layer) and NMT (8-layer). The placements generated by the Hierarchical Planner are highly parallelized, distributing operations across multiple GPUs for faster runtime. Our method enables parallelized placement of LSTM, attention, and softmax layers across multiple GPUs, outperforming previous methods like ColocRL. The policy search space is large with 5 devices and 46,600 operations. The Grouper learns to group operations efficiently, converging to a small subset of groups. The device placement problem is treated as a sequential decision-making task. The device placement problem is approached as a sequential decision-making task, with the Placer using a bi-LSTM architecture to optimize placements for TensorFlow graphs. Experiments on NMT models showed less than 7% difference in placement speeds. The Hierarchical Planner trains a new policy for each model, taking up to three hours for the largest benchmark. The policy is a lightweight network trained on a single GPU. The policy is trained on a lightweight network using a single GPU to optimize model placement on multiple devices. Actual runtime measurements are used to measure reward, with worker nodes running the input model for predicted placements. Results show efficient training even with limited hardware settings, with policy loss reduction demonstrated in scenarios with 1 and 4 workers. The policy, hosted on a single K40 GPU, sends placements to worker(s) one at a time and applies a gradient step for each reward collected. The policy is trained on a single GPU to optimize model placement on multiple devices. With only one worker, it takes less than 2.5 hours for the policy to achieve a placement with a training step time of 1.94 seconds. This results in significant GPU-hour savings, even after considering the training time spent on the policy. The training of the Hierarchical Planner can be interleaved with the training of the target model for greater efficiency. The Simple Planner, a simpler model compared to the Hierarchical Planner, independently predicts placements for each operation in the input model. It performs well for some benchmarks but struggles with larger ones like NMT (2-layer). The Hierarchical Planner outperforms the Simple Planner in placement for NMT models with multiple layers. It breaks the problem into sub-tasks and can scale to larger models. End-to-end learning of grouping operations improves performance, as shown in experiments with randomized groupings. The paper presents a hierarchical method for efficiently placing operations of a computational graph onto devices. Using learned groupings instead of random ones significantly reduces runtime. The approach involves assigning operations to groups and then placing them onto devices using a policy gradient method. This method allows scaling to graphs with over 80,000 operations and outperforms previous state-of-the-art methods by up to 60.6% on various tasks."
}