{
    "title": "S1HlA-ZAZ",
    "content": "The memory system presented is inspired by Kanerva's sparse distributed memory and has a robust distributed reading and writing mechanism. It is analytically tractable and formulated as a hierarchical conditional generative model. The memory significantly improves generative models trained on Omniglot and CIFAR datasets, with greater capacity and ease of training compared to Differentiable Neural Computers (DNC). The memory systems in models like Differentiable Neural Computers collapse reading and writing into single slots, limiting information sharing. Matching Networks and Neural Episodic Controller store embeddings directly, requiring memory volume to increase with stored samples. In contrast, Neural Statistician averages embeddings for small \"statistics,\" potentially losing information. Associative memory architectures like the Hopfield Net store data in overlapping representations efficiently. The Hopfield Net and Boltzmann Machine are memory structures that store data efficiently. The Boltzmann Machine introduces latent variables but has slow reading and writing mechanisms. Kanerva's sparse distributed memory model resolves this issue by allowing fast reads and writes with independent memory size. A conditional generative memory model inspired by Kanerva's model is presented, with learnable addresses and reparametrised latent variables. The text discusses a memory model that optimally trades off preserving old content and storing new content by deriving a Bayesian memory update rule. This hierarchical generative model quickly adapts to new data, providing top-down knowledge in addition to bottom-up perception. The model enriches priors in VAE-like models through an adaptive memory system, offering effective online distributed writing for compression and storage of complex data. The memory architecture extends the variational autoencoder (VAE) by deriving the prior from an adaptive memory store. The text introduces a variational autoencoder (VAE) with a generative model specified by prior and conditional distributions, approximating the intractable posterior with an inference model. The objective is to maximize log-likelihood by optimizing parameters for a variational lower-bound, balancing reconstruction loss and regularization terms. The model utilizes multivariate Gaussian distributions and neural networks for parameterized distributions. The model introduces a memory-based generative model using an exchangeable episode concept. The objective is to maximize mutual information between memory and the episode to store, by writing X into the memory. This approach offers a principled way of formulating memory-based generative models. The generative model introduces a memory matrix M with a matrix variate Gaussian distribution. The distribution is factorized to show conditional independence of variables. The model assumes independence between columns of M, fixes V to the identity matrix, and allows full freedom for U. The covariance between rows of M is found to be useful for memory access coordination. The memory matrix M in the generative model is optimized through back-propagation with normalized rows to avoid degeneracy. The addressing variable yt computes weights for memory access, transformed by a learned projection bt. The weights wt across rows of M are computed using a multi-layer perception (MLP) projection f, generating samples xt through a memory-dependent prior for zt. The distribution for each time step t is tied together in a hierarchical model with memory-dependent priors. The global latent variable M captures episode statistics, while local variables y t and z t capture local data statistics. The posterior distribution refines the prior with evidence from the data x t. The parameterized posterior distribution q\u03c6(z_t|x_t,y_t,M) refines the prior distribution p\u03b8(z_t|y_t,M) with evidence from x_t. Memory updating involves balancing old and new information, which can be optimized using Bayes' rule. Memory writing is interpreted as inference, computing the posterior distribution of memory p(M|X). Batch and online inference methods are considered for updating memory. The approximated posterior distribution of memory is obtained by using one sample of y_t and x_t to approximate the integral. The posterior distributions of addressing variable q\u03c6(y_t|x_t) and code q\u03c6(z_t|x_t) are parameterized. The posterior distributions q\u03c6(y_t|x_t) and q\u03c6(z_t|x_t) are parameterized. Memory updating involves balancing old and new information using Bayes' rule. The posterior of memory p\u03b8(M|Y,Z) is analytically tractable, with parameters R and U updated accordingly. The prior parameters of p(M), R0 and U0, are trained through back-propagation to learn the general dataset structure. The update rule for memory involves balancing old and new information using Bayes' rule. The cost of the update rule mainly comes from inverting \u03a3z with a complexity of O(T^3). Online updating can reduce the per-step cost by performing the update rule using one sample at a time. Intermediate updates can be done using mini-batches with a size between 1 and T. The storage and multiplication of the memory's row-covariance matrix U is another major cost with a complexity of O(K^2). Although restricting this covariance to diagonal can reduce the cost to O(K), experiments suggest that the covariance is useful for coordinating memory accessing. The covariance of the memory is useful for coordinating memory accessing in the model. Training involves optimizing a variational lower-bound of the conditional likelihood using mean-field approximation for memory efficiency. Future work includes investigating low-rank approximation of U for better cost-performance balance. The first term in the bracket of the current text chunk is the VAE reconstruction error, while the second term penalizes deviation of the code from the memory-based prior. An iterative reading mechanism is present in Kanerva's sparse distributed memory, which is also available in the model discussed. This iterative process improves denoising and sampling, although convergence cannot be proven. Knowledge about memory aids in reading, suggesting the use of a parameterized model for addressing. The text discusses the use of a parameterized model for addressing memory in reading. Training with the whole matrix as input is costly, but efficient approximations can be made using loopy belief-propagation. Iterative reading is believed to work well in the model, with future research aiming to understand the process better. The model implementation details are provided in Appendix C, focusing on evaluating improvements with an adaptive memory. The same model architecture is used for experiments with different datasets, varying only in filter numbers, memory size, and code size. In experiments with Omniglot and CIFAR datasets, the model architecture remained the same, with variations in filter numbers, memory size, and code size. The Adam optimizer was used with minimal tuning. The Omniglot dataset, with 1623 classes and 20 examples each, posed challenges due to its complexity. A 64x100 memory and 64x50 address matrix were used. Episodes were formed by randomly sampling 32 images without class labels. A mini-batch size of 16 was used to optimize the variational lower-bound. The model was tested with the CIFAR dataset, using convolutional coders with 32 features at each layer, a code size of 200, and a 128x200 memory with a 128x50 address matrix. The training process was compared with a baseline VAE model, showing a modest increase in parameters for the Kanerva Machine. The Kanerva Machine outperformed the VAE model in terms of negative variational lower bound, reconstruction loss, and KL-divergence during learning. The model learned to use memory effectively, resulting in a more informative prior. This rich prior was achieved with a lower KL-divergence compared to the VAE model. The VAE model achieved a negative log-likelihood (NLL) of \u2264 112.7 at the end of training, worse than state-of-the-art unconditioned generation but comparable to IWAE training results. The Kanerva Machine, with the same encoder and decoders, achieved a conditional NLL of 68.3, showcasing the power of incorporating an adaptive memory. The model demonstrated a significant improvement in negative log-likelihood (NLL) by incorporating an adaptive memory into generative models. The weights were well distributed over the memory, showing patterns superimposed on others. The reconstruction process and denoising through iterative reading were illustrated, showcasing the ability to generate batches of images with various classes and samples. In this section, the model is tested using episodes with samples from 2, 4, or 12 classes. Samples from the VAE and Kanerva Machine are compared, showing improved sample quality in consecutive iterations. Conditional samples from CIFAR are also discussed, highlighting the limitations of VAEs in improving sample quality through iterative sampling. The 24 conditioning images are randomly sampled from the CIFAR dataset, showing a mix of classes. Samples from the Kanerva Machine exhibit clear local structures, unlike the blurred samples from the matched VAE. The model can recover original images from corrupted inputs through iterative reading, showcasing interpretability of internal representations in memory. Linear interpolations between address weights are meaningful, as demonstrated by interpolating weight vectors from random input images. Interpolations between address weights were found to be meaningful by computing weight vectors from random input images and linearly interpolating between them. The resulting vectors were used to read from memory and produce smoothly changing images. Additionally, a comparison was made between DNC and Kanerva Machine models, showing differences in training curves and test variational lower-bounds. The DNC and Kanerva Machine models were compared in a storage and retrieval task with Omniglot data. The DNC was sensitive to hyper-parameters, while the Kanerva Machine was robust and easier to train. The Kanerva Machine trained fastest with specific parameters and converged below 70 test loss with all configurations. This makes it significantly easier to train compared to the DNC. The Kanerva Machine, a novel memory model combining slow-learning neural networks and a fast-adapting linear Gaussian model, outperformed the DNC in a storage and retrieval task with Omniglot data. It generalizes well to larger episodes and maintains an advantage over the DNC in terms of variational lower-bound. The model is trained with episodes containing different numbers of classes, showing the ability to exploit redundancy for lower reconstruction losses. The model removes the assumption of uniform data distribution by training a generative model to learn observed data distribution. Memory is implemented as a generative model to retrieve unseen patterns through sampling, consistent with constructive memory neuroscience experiments. The model generalizes Kanerva's memory model to continuous, non-uniform data while integrating with deep neural networks. Other models have combined memory mechanisms with neural networks in a generative setting, such as using attention to retrieve information from a memory matrix. Our model differs from previous models by efficiently updating memory using statistical regularity in images, learned addresses, and Bayes' rule. Unlike other models that store raw pixels in memory, our model compresses information for fast adaptation without overwhelming storage costs. We employ an exact Bayes' update-rule for memory updating, enhancing the efficiency of our memory model. Our model efficiently updates memory using statistical regularity in images and Bayes' rule. It combines classical statistical models with neural networks for promising memory models in machine learning. Kanerva's sparse distributed memory is characterized by distributed reading and writing operations with fixed addresses pointing to modifiable memory. Kanerva's sparse distributed memory involves randomly sampled fixed addresses for input comparison through Hamming distance. The reading process sums memory contents pointed to by selected addresses, producing a readout. This process can be iterated multiple times, ensuring correct retrieval of stored vectors even after multiple overwrites. The operations are sparse and distributed, with a small portion of addresses always being selected when both K and D are large enough. The architecture of our model, compared to a standard VAE, includes a convolutional encoder converting input images into embedding vectors. The model uses 3 consecutive blocks of convolutional layers followed by a ResNet block. The assumption of uniform and binary data distribution in Kanerva's model limits its practical application due to real-world data typically lying on low-dimensional manifolds. The model architecture includes a convolutional encoder with 4x4 filters and stride 2, followed by ResNet blocks without bottleneck. The convolutional layers have 16 or 32 filters, and the output is linearly projected to a 2C dimensional vector. Adding noise to the input helps stabilize training. Different likelihood functions are used for different datasets. The differentiable neural computer (DNC) is wrapped with the same interface as Kanerva memory for fair comparison. The differentiable neural computer (DNC) is integrated with the Kanerva memory interface for comparison. The DNC receives addressing variable y t and input z t during writing. Separating reading and writing stages in experiments, the DNC's state is used as memory during writing and discarded during reading. A 2-layer MLP with ReLU nonlinearity is used as the controller instead of LSTM to prevent interference with DNC's external memory. Controllers bypassing memory output is avoided to prevent confusion in the auto-encoding setting. The DNC is modified to ensure it only reads from memory, focusing on memory performance. Models using full covariance matrices show faster test loss decrease. The model samples directly from memory for decoding, reconstructing solely using memory read-outs. The model reconstructs solely using memory read-outs, with a focus on memory performance. The training shows similar patterns to Omniglot training, with a small difference in KL-divergence affecting sample quality. The advantage of the Kanerva Machine over the VAE is increasing during training. The linear Gaussian model is defined by Eq. 6, with the posterior distribution derived using the conditional formula for the Gaussian. The update rule is given by eq. 9 to 11. The model described in the previous paragraphs focuses on memory performance and uses memory read-outs for reconstruction. The update rule in equations 9 to 11 is derived from properties of matrix variate Gaussian distribution. The model utilizes samples from q \u03c6 (z t |x t ) for writing to memory and mean-field approximation during reading. An alternative approach is presented that fully exploits the analytic tractability of the Gaussian distribution, using parameters \u03c8 = {R, U, V} for the memory."
}