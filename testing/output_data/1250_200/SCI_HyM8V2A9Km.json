{
    "title": "HyM8V2A9Km",
    "content": "Sparse reward is a challenging problem in reinforcement learning. Hindsight Experience Replay (HER) addresses this by converting failure experiences into successful ones. However, HER has limited applicability due to a lack of universal goal representation. Augmenting experienCe via TeacheR's adviCE (ACTRCE) extends HER using natural language as the goal representation. ACTRCE efficiently solves difficult reinforcement learning problems in 3D navigation tasks, where non-language goal representations fail. The use of language goal representations allows the agent to generalize to unseen instructions and lexicons. Hindsight advice is crucial for solving challenging tasks, but only a small amount is needed for learning to progress. The practical aspect of deep reinforcement learning applications relies on carefully-crafted reward functions. Designing a good reward function is challenging and can lead to biased learning. Sparse reward functions make learning difficult, but Hindsight Experience Replay (HER) addresses this by converting failed experiences into successful ones. Hindsight Experience Replay (HER) aims to convert failed experiences into successful ones by substituting them with a fake goal. Andrychowicz et al. assumed that for every state in the environment, there exists a goal achieved in that state. Representing goals using the state space is inefficient and redundant, leading to the need for a compact and informative goal representation. Language representation of goals is expressive, flexible, and abstract, satisfying the requirements of HER. Language representation of goals is abstract and flexible, allowing for the compression of redundant information in states. By combining the HER framework with natural language goal representation, a technique called ACTRCE is proposed to address reinforcement learning problems. This method involves a teacher providing advice in natural language to the agent after each episode, helping to alleviate the sparse reward problem. The benefits of language goal representation are highlighted in generalizing across goals. The agent can efficiently solve reinforcement learning problems in challenging environments by replacing the original goal with advice and a reward. Language goal representation, combined with hindsight advice, allows for generalization to unseen instructions and lexicons. Using hindsight advice is crucial for solving challenging tasks, with even a small amount being sufficient for learning to progress. This work is significant for language learning and grounding natural language understanding in a simulated physical world. Recent work has been using reinforcement learning techniques combined with rich language advice to address the limitations of hard coded rules in the physical world. The traditional reinforcement learning setting involves an agent interacting with a Markov Decision Process to maximize expected cumulative return. The action-value function is crucial in this process. The curr_chunk discusses the Q-learning algorithm, which is an off-policy, model-free reinforcement learning algorithm based on the Bellman equation. It uses semi-gradient descent to minimize the squared Temporal Difference error. Deep Q-Network builds on Q-learning by using a neural network to approximate the optimal Q-function. The goal-oriented reinforcement learning framework is also reviewed in the context of Markov Decision Processes. The curr_chunk discusses augmenting the Markov Decision Process with a goal space G, where a goal g is chosen for each episode. The agent's objective is to maximize the expected discounted cumulative return given the goal. A specific family of goal conditioned reward functions is considered, where the reward is either 0 or 1 based on the resulting state. The reward function is sparse, making it difficult for the agent to learn, but HER proposes a solution to this problem by collecting episode experiences. The curr_chunk discusses the challenges of sparse rewards in learning and proposes a solution through HER. It involves collecting experiences under a goal and using off-policy algorithms to learn from goal-transformed experiences. The authors suggest a representation map from state space to goal space for flexible relabeling, but constructing this mapping is complex. An example illustrates the difficulty in defining goals in a 3-D environment with multiple possible states. The curr_chunk discusses using natural language to represent goals in a 3-D environment for a goal-oriented MDP. This approach aims to reduce redundancy in representation compared to using raw pixel observations or subspace projections. Using natural language to represent goals in a 3-D environment for a goal-oriented MDP can reduce redundancy in representation. A teacher provides advice in natural language for each state, converting failure trajectories to successful ones by relabeling goals. Positive and negative experiences are both necessary for training. In addition to positive signals, negative experiences are essential for training. Different goal descriptions are provided by multiple teachers in a scenario with multiple achievable goals. Off-policy RL algorithms and replay buffers are utilized, with each goal corresponding to a different teacher. Training involves sampling goals and states, taking actions based on behavioral policy, and storing transitions in a replay buffer. The state described as \"Reach a blue torch\" can also be described as \"Reach the largest blue object\". In a scenario with multiple achievable goals, different descriptions are provided by various teachers. Each teacher corresponds to a different goal, and their advice is used to relabel the original goal and augment the replay buffer with new trajectories. The approach involves converting natural language goals into continuous vector representations for neural network training. The approach involves converting natural language goals into continuous vector representations for neural network training. This can be done by representing each word as a one-hot vector and using a recurrent neural network (RNN) to embed each word into its hidden state. Another method is to use a pre-trained language component to represent the given language instructions, allowing the agent to better understand unseen lexicons related to the language goals. The architecture consists of 3 modules: a language component that converts instructions into a continuous vector, an attention vector, and a component that processes the information. The architecture comprises 3 modules: a language component converting instructions into vectors, processing observations using convolutional neural networks, and fusing goal information. The fused representation is used to compute the output value. Experimental results demonstrate the method's effectiveness in two environments, KrazyGrid World and ViZDoom. A comparison of goal representations shows that as the number of instructions increases, the one hot approach does not scale well compared to GRU and pre-trained embeddings. The study shows that using GRU and pre-trained embeddings outperforms the one hot approach as the number of instructions increases. Additionally, significant improvement in sample efficiency is achieved by using teachers' advice, even with a limited amount. The method was tested in KrazyGrid World and ViZDoom environments. KrazyGrid World is a 2D grid environment with different types of tiles. ViZDoom is a 3D learning environment based on the game Doom. Both environments have specific goals and tasks for the agent to complete. In experiments with KrazyGrid World and ViZDoom, tasks are formed by combining singleton tasks A and B using \"and\" and \"or\" compositions. The completion criteria differ for \"A and B\" and \"A or B\". DQN algorithm was used for reinforcement learning in both environments. Multi-environment training involved sampling 16 environments, collecting data, updating agents, and repeating the process. The study compares language-based goal representations with non-language representations, showing that language representations are more effective as task difficulty increases. The study compares language-based goal representations with non-language representations, demonstrating that language representations are more effective as task difficulty increases. Language goal representations allow for generalization to unseen goals in training, while non-language representations cannot achieve this. Pre-trained sentence embeddings enhance the robustness of language goal representations. InferLite is a lightweight sentence encoder trained for natural language inference, similar to InferSent but without RNNs. One-Hot Representation is a non-language baseline for goal representation. It is compared with language-based representations in ViZDoom tasks, showing similar learning speeds in easier tasks but differences in more complex tasks. In more challenging tasks, one-hot goal representations have a 24% success rate compared to 97% with language representations like ACTRCE. Language representations allow for better generalization to unseen instructions, achieving an 83% success rate in zero-shot learning. Visualization analysis shows significant differences in learned embeddings for different goal instructions. The VizDoom benchmark demonstrated that there are significant differences in learned embeddings for different goal representations. Correlation matrices were calculated for each goal representation, showing that GRU and InferLite embeddings have similar structures, while one-hot goal embeddings have no correlation. Pre-trained embeddings were used to allow the model to generalize to unseen lexicons at test time. The study demonstrated the effectiveness of language goal representation and the importance of hindsight advice in learning. The agent achieved tasks above 66% of the time by understanding sentences with unseen lexicons and synonyms. This ability is beneficial when implementing methods with advice from humans, improving learning robustness in noisy settings. The comparison between the \"ACTRCE\" method and the DQN algorithm was also discussed. In this section, the study compared the \"ACTRCE\" method to the DQN algorithm, showing the importance of hindsight advice in learning. Results indicated that even a small amount of advice (1%) was sufficient for learning to progress effectively. The experiments involved tasks with different grids and goals, with the \"ACTRCE\" method outperforming the baseline DQN in both environments. Additionally, successful training was achieved in ViZDoom experiments with different configurations. In ViZDoom experiments, the agent trained with ACTRCE outperformed DQN in challenging environments with 7 objects. The training success rates were compared using ACTRCE versus DQN, showing better performance with ACTRCE. Additionally, in KGW experiments, ACTRCE efficiently learned tasks with 3 goals and 3 lavas, while DQN struggled to learn the same tasks. In ViZDoom, ACTRCE outperformed DQN in a compositional task with 5 objects in easy mode. The agent learned well with hindsight advice, even with minimal (1%) guidance, showcasing robustness in practical settings. Previous approaches have integrated natural language in reinforcement learning. Several approaches have utilized natural language in reinforcement learning. Maclin & Shavlik (1994) translated language advice into a neural network program to guide policy actions. BID17 and BID21 incorporated natural language feedback to assist RL agents in learning tasks. BID13 introduced an agent using English instructions to self-monitor progress in Atari games. BID0 proposed using language as a latent parameter space for few-shot learning and policy search. Additionally, there is research on using reinforcement learning to learn grounded language for task-oriented language grounding. In the context of utilizing natural language in reinforcement learning, various studies have focused on task-oriented language grounding. Misra et al. and Yu et al. mapped language instructions to actions in 2D environments. BID12 and Chaplot et al. developed agents that execute written instructions in 3D environments using reinforcement learning. BID20 introduced experience replay to accelerate credit assignment. The ACTRCE method uses natural language as a goal representation for hindsight advice in reinforcement learning. It shows that language goal representations can bring benefits when combined with hindsight advice, enabling efficient solving of challenging 3D navigation tasks. The agent can generalize to unseen instructions and lexicons with a pre-trained language component. In KrazyGrid World, the agent uses goal representations to generalize to unseen instructions and lexicons. The environment consists of 2D grid with different tiles and colors, where the agent's goal is to reach goals of different colors. The grid state is represented by functionality and color attributes in one hot vectors, with the agent's position also included. The experiments are conducted in a 9x9 grid with 3 distinct colored goals and varying number of lavas. In experiments, a 9x9 grid environment with 3 distinct colored goals and varying number of lavas is used. The episode terminates when the agent reaches a lava or a goal, or runs over a maximum time step. The goal space is enlarged by considering compositions of goals, with modifications made to the environment accordingly. An extra action called \"flag\" is added for the agent to terminate the episode when it thinks the given goal is accomplished. The ViZDoom BID14 BID4 3D learning environment is based on the game Doom, where the agent navigates a room with random objects using actions like turning and moving forward. The goal is to follow a natural language instruction to reach a specific object within 30 time steps, with a reward given for success. The environment has different difficulty modes based on object distribution and agent spawn location. In the ViZDoom BID14 BID4 3D learning environment, agents navigate rooms with random objects using natural language instructions. The environment has different difficulty modes based on object distribution and agent spawn location, with easy mode having fixed spawn locations and evenly spaced objects, while hard mode has random spawns. Compositional instructions consist of two single object instructions joined by \"and\", ensuring unambiguous directions. In the ViZDoom environment, agents receive instructions to navigate rooms with random objects. Compositional instructions are designed to be unambiguous by combining two single object instructions with \"and\". The HUD displays thumbnail images of reached objects, with the episode ending once two objects are reached. Positive and negative feedback is provided based on whether the agent reaches the correct object. In the ViZDoom environment, agents receive instructions to navigate rooms with random objects. For compositional tasks, instructions are combined to be unambiguous. Positive and negative feedback is given based on reaching the correct object. When the agent doesn't reach any objects, no positive feedback is provided. For reaching 2 objects, singleton instructions are generated for each object and merged with \"and\" for positive advice. For singleton instructions, different sets are generated for positive and negative advice. Convolution layers with ReLU activation are used for preprocessing grid observations. Language sentences are input as word-level one hot vectors to an LSTM with hidden size 128. The LSTM's output is passed through fully connected layers to predict action values. The architecture for processing grid observations involves convolution layers with ReLU activation functions and LSTM for language sentences. The observation is augmented with a history vector and processed further with additional convolution layers. The final output is passed through fully connected layers to predict action values. The architecture involves convolution layers with ReLU activation functions for processing grid observations and LSTM for language sentences. The state input is an RGB image, followed by convolution layers with different kernel sizes and strides. An embedding matrix is used for language input, processed by a GRU with 256 units. The attention vector is generated using a fully connected layer with sigmoid activation. The gated feature maps are flattened and passed through a fully connected layer with ReLU activation. Finally, the LSTM with 256 hidden units is used for further processing. For KrazyGrid World experiments, hyperparameters are tuned including learning rate, replay buffer size, and training frequency. Episodes are generated using an -greedy policy with Double DQN and Huber loss for stable gradients. In ViZDoom environment, training instructions are used for agent training and zero-shot evaluation. DQN is implemented on top of existing code with a cyclic buffer replay buffer for transitions. In KrazyGrid World experiments, hyperparameters are tuned for Double DQN with a cyclic buffer replay buffer. The replay buffer contains recent transitions for easy and hard modes. Episodes are generated using an -greedy policy with a linear decay. Double DQN is used to reduce Q-value overestimation, with an Adam optimizer and a learning rate of 0.0001. The network is updated every 4 frames on easy mode and 16 frames on hard mode for better performance. Sampling from the replay buffer involves selecting 32 consecutive frames from a random episode for more accurate estimates. In KrazyGrid World experiments, hyperparameters are tuned for Double DQN with a cyclic buffer replay buffer. Sampling involves selecting 32 consecutive frames for accurate estimates of the hidden state of the LSTM. Sequential mini-batch allows n-step Q learning. 16 parallel threads alleviate correlation between samples. Training thread model is synchronized with shared network for computing training loss and gradient. Target network is synchronized every 500 time steps. One additional thread evaluates multi-task success rate. Teacher types are described, including desirable and undesirable goals. Subset G d \u2286 G denotes all desired goals. Each episode samples a goal g \u2208 G d for the agent to explore. In this section, different types of teachers are explored in providing advice to the agent. Three types of teachers are considered: Optimistic, Knowledgeable, and Discouraging. Each teacher provides advice based on the agent's performance in achieving desired goals. The study compares the effectiveness of these teachers with the DQN algorithm. The method using only optimistic and discouraging teachers is denoted as \"ACTRCE \u2212\". In comparing different teacher types in providing advice, the method ACTRCE was evaluated in KrazyGrid World. Results showed that having knowledgeable teachers helped speed up learning, especially as the task became harder with increased lavas. ACTRCE with only optimistic and discouraging teachers initially performed well but struggled as the task difficulty increased. Having knowledgeable teachers accelerated learning in KrazyGrid World, even as the task became more challenging with increased lavas. ACTRCE showed consistent learning rates in difficult settings with knowledgeable teachers, outperforming ACTRCE without advice. Transfer learning experiments were designed to explore if learning easier tasks could aid in tackling more difficult tasks. In transfer learning experiments, a pessimistic teacher was used to pretrain agents, leading to faster learning compared to unpretrained agents. Even though pretraining goals did not overlap with actual training goals, the pretrained agents learned faster, especially in environments with multiple obstacles. This suggests that learning easier goals can provide signals for harder goals, especially when tasks require similar modules. In experiments on ViZDoom, DQN and ACTRCE were tested on tasks with 5 objects in easy and hard modes. DQN struggled with consistency on the hard task, while ACTRCE showed low variance. A3C had lower sample efficiency compared to DQN/ACTRCE on the easy task and failed to reproduce results on the hard task due to limited computational resources. A significant difference in sample efficiency was observed between A3C and DQN/ACTRCE on the hard task. During training in ViZDoom scenarios, the average episode length decreased with ACTRCE compared to baseline DQN for harder tasks. A cumulative success rate curve was constructed to evaluate model performance, showing higher success rates indicate better models. The Multi-task cumulative success rate for 3 ViZDoom environment tasks using GRU hidden state language encoding is shown in FIG0. In the 5 objects hard mode, ACTRCE outperformed baseline DQN after episode length of 20. In the 7 objects hard mode, ACTRCE maintained better performance while baseline DQN only succeeded in very short episodes. In the 5 objects composition task, ACTRCE had two groups of trajectories based on proximity of target objects. In the 5 objects composition task, ACTRCE had two groups of trajectories based on proximity of target objects, one requiring less than 10 time-steps and the other over 20 time-steps. The former group occurs when the two target objects are adjacent, making it easy to reach the second object quickly. The latter group occurs when the target objects are not adjacent, requiring the agent to carefully navigate and avoid obstacles."
}