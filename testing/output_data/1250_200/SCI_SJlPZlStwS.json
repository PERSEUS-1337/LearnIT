{
    "title": "SJlPZlStwS",
    "content": "Recent studies have shown that convolutional neural networks (CNNs) are vulnerable to various attacks. A new framework called EdgeGANRob has been proposed to improve CNN robustness by focusing on shape/structure features and using a generative adversarial network (GAN) to reconstruct images. Additionally, a robust edge detection approach called Robust Canny has been introduced to reduce sensitivity to adversarial perturbations. Comparisons with a simplified procedure called EdgeNetRob show that EdgeGANRob improves model accuracy without sacrificing robustness. EdgeNetRob boosts model robustness but reduces clean model accuracy. EdgeGANRob improves clean model accuracy without sacrificing robustness benefits from EdgeNetRob. Extensive experiments demonstrate EdgeGANRob's resilience across various learning tasks and settings. Recent studies have shown that Convolutional Neural Networks (CNNs) are vulnerable to adversarial examples, where imperceptible perturbations can manipulate predictions. Data poisoning or backdoor attacks manipulate training data to reduce model generalization accuracy. CNNs tend to learn surface statistical regularities instead of high-level abstractions, leading to a lack of generalization under distribution shifting. Improving the general robustness of Deep Neural Networks (DNNs) remains a challenge. Recent studies explore the vulnerability of CNNs to adversarial examples, attributing it to non-robust but highly-predictive features. Human recognition relies on global object shapes, while CNNs are biased towards local patterns. This bias contributes to vulnerability to adversarial examples and backdoor attacks. The paper proposes EdgeGANRob to enhance CNNs' robustness against adversarial attacks, distribution shifting, and backdoor attacks by focusing on global shape structures, specifically edges in images. This approach aims to improve predictions by incorporating texture information and reconstructing instances based on edge detection. The paper introduces EdgeGANRob to enhance CNNs' robustness by leveraging structural information in images, particularly edges. A simplified version, EdgeNetRob, detects edges and trains the classifier on them, forcing CNNs to rely on shape information rather than texture/color. The proposed robust edge detection algorithm, Robust Canny, significantly improves EdgeGANRob's robustness against attacks, outperforming adversarial retraining methods. The paper introduces EdgeGANRob to enhance CNNs' robustness by leveraging structural information in images, particularly edges. Robust Canny improves EdgeGANRob's robustness against attacks, outperforming adversarial retraining methods. EdgeNetRob improves CNNs' robustness but decreases clean accuracy due to missing texture/color information, motivating the development of EdgeGANRob. The proposed algorithm, Robust Canny, enhances edge detection by reducing sensitivity to adversarial perturbations. Evaluation on EdgeNetRob and EdgeGANRob shows significant improvements in adversarial attacks, distribution shifting, and backdoor attacks. Defense methods against adversarial examples are crucial, with a focus on robustness against adaptive attacks and customized white-box attacks. Defense methods should be evaluated against strong adaptive attacks. Distribution shifting is common in real-world applications, with CNNs having a tendency to learn superficial statistical cues. Wang et al. (2019a) proposes a method to robustify CNNs by penalizing the predictive power of local representations. Benchmark datasets are proposed by Hendrycks and Dietterich (2019) for evaluating model robustness under common perturbations. Backdoor attacks work by injecting a pattern into training data to predict a specific target class in test data. Tran et al. (2018) has proposed a procedure to detect poisoned training data using tools from robust statistics. In response to the need for defense against backdoor attacks, recent research has focused on protecting models using robust features. Studies have shown that CNNs rely more on textures than global shape structure for image recognition. To address this, a new classification pipeline called EdgeGANRob has been introduced, which utilizes edge features as a robust feature for improved model robustness. The EdgeGANRob classification pipeline utilizes edge features for improved model robustness. It includes EdgeNetRob, a backbone procedure that extracts edge maps from images and trains a classifier solely based on edges. This approach aims to reduce sensitivity to local textures in image recognition tasks. EdgeNetRob forces CNN decisions based on edges, reducing sensitivity to textures. EdgeGANRob fills edge maps with texture/colors for higher accuracy. Robust edge detection algorithm, Robust Canny, proposed due to vulnerabilities in existing detectors. A robust edge detection method, Robust Canny, is proposed to address vulnerabilities in existing detectors. Traditional methods like Canny are robust but can become noisy with adversarial perturbations. The proposed Robust Canny improves robustness by truncating noisy pixels in intermediate stages. It includes 6 stages: noise reduction, gradient computation using the Sobel operator, noise masking with thresholding, and non-maximum suppression. The Robust Canny edge detection method includes 6 stages: noise reduction, gradient computation, noise masking with thresholding, non-maximum suppression, double thresholding, and edge tracking by hysteresis. The method aims to reduce noise in the presence of adversarial perturbations by modifying the traditional Canny algorithm. The Robust Canny edge detection method aims to reduce noise in the presence of adversarial perturbations by adding a truncation operation and adjusting parameters like \u03c3 and thresholds \u03b8 l , \u03b8 h. Careful parameter selection is crucial for obtaining a robust edge detector. Additionally, training a Generative Adversarial Network (GAN) in EdgeGANRob involves following the image-to-image translation framework (pix2pix) and training the inpainting GAN in two stages. In EdgeGANRob, the inpainting GAN is trained in two stages following the pix2pix framework. The first stage involves training a conditional GAN with an objective function including adversarial and feature matching losses. In the second stage, the GAN is fine-tuned along with a classifier to improve accuracy on generated RGB images. This method enhances robustness against adversarial, distribution shifting, and backdoor attacks. EdgeGANRob improves robustness against adversarial attacks by focusing on edge features, helps with distribution shifting by preserving shape structure, and mitigates backdoor attacks by sanitizing data. The method's effectiveness is evaluated in this section. The proposed method EdgeNetRob enhances robustness against adversarial attacks and distribution shifting, while also addressing backdoor attacks. Experiments are conducted on Fashion MNIST and CelebA datasets, with a focus on gender classification. The choice of datasets is justified based on the limitations of MNIST and CIFAR-10 datasets. The study evaluates the proposed EdgeNetRob method's robustness against adversarial attacks using standard perturbation budgets on Fashion MNIST and CelebA datasets. The evaluation includes white-box attacks using the BPDA attack and the need for a robust edge detector to defend against adversarial attacks is highlighted. The study compares the robustness of three edge detection methods: RCF, Canny, and Robust Canny, by training a classifier on the extracted edge maps. Results show that RCF edges are not robust under strong adaptive attacks. The evaluation includes Fashion MNIST and CelebA datasets, comparing EdgeNetRob and EdgeGANRob with adversarial training for defense against white-box attacks. EdgeNetRob and EdgeGANRob show a slight drop in clean accuracy compared to the baseline model but outperform adversarial training with = 8. EdgeGANRob achieves higher clean accuracy than EdgeNetRob on CelebA dataset, highlighting the importance of using GANs on complex datasets. Both models exhibit robustness against strong adaptive attacks, with EdgeNetRob being more time-efficient due to not using adversarial training. Generalization ability is tested under distribution shifting using perturbed Fashion MNIST and CelebA datasets with different patterns. In 2019, models EdgeNetRob and EdgeGANRob are tested on perturbed Fashion MNIST and CelebA datasets with various patterns. They outperform the state-of-the-art method PAR and show improved accuracy on negative color, radial kernel, and random kernel patterns. The use of edge features helps with CNN generalization under distribution shifting and can serve as a defense against backdoor attacks. In 2019, EdgeNetRob and EdgeGANRob models are tested on Fashion MNIST and CelebA datasets with various patterns, outperforming the state-of-the-art method PAR. They show improved accuracy on negative color, radial kernel, and random kernel patterns. The use of edge features aids CNN generalization under distribution shifting and can defend against backdoor attacks. In a separate study, attacks are conducted by embedding invisible watermark patterns into images for Fashion MNIST and CelebA datasets. Results show successful attacks on the vanilla Net with high poisoning accuracy. Comparisons with the baseline method Spectral Signature reveal consistent low poisoning accuracy for EdgeNetRob and EdgeGANRob. The study introduces a new method using robust edge features to improve model robustness against invisible watermark patterns. EdgeNetRob and EdgeGANRob consistently show low poisoning accuracy compared to Spectral Signature. The edge detector effectively removes the invisible watermark pattern, with EdgeGANRob achieving better clean accuracy. By combining a robust edge feature extractor with a generative adversarial network, the method achieves competitive results in adversarial robustness and generalization under distribution shifting, improving model robustness against backdoor attacks. The importance of using shape information for enhancing model robustness is highlighted as a promising direction for future research. In improving model robustness, images are resized to 128 \u00d7 128 using bicubic interpolation. Data is normalized to [\u22121, 1]. Different models are used for Fashion-MNIST and CelebA datasets. Training is done using stochastic gradient descent with momentum, PGD, and CW attacks. Hyper-parameters for Robust Canny are chosen for evaluation. Fashion MNIST parameters include \u03c3 = 1, \u03b8 l = 0.1, \u03b8 h = 0.2, \u03b1 = 0.3. The hyper-parameters for Robust Canny are chosen using the validation set to balance robustness and accuracy. Differentiable approximations are used in a white-box attack scenario to construct adversarial samples, as non-differentiable transformations can be replaced with Backward Pass Differentiable Approximation (BPDA) technique. The Robust Canny algorithm is approximated in two stages: C1 for steps 1-3 and C2 for steps 4-6. The Robust Canny algorithm is approximated in two stages: C1 for steps 1-3 and C2 for steps 4-6. C2 is a non-differentiable operation where the output is a masked version of the input. To obtain a differentiable approximation for BPDA, the mask is assumed to be constant. Test accuracy changes are shown under radial and random mask transformations. Visualization results for CelebA under distribution shifting and qualitative results for backdoor attacks on Fashion are also presented. The visualization results for CelebA under distribution shifting and backdoor attacks on Fashion MNIST show that EdgeNetRob can slightly remove poisoning patterns, with each generated image having unique patterns."
}