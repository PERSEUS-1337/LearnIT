{
    "title": "BJgd7m0xRZ",
    "content": "Anomaly detection in unlabeled data identifies non-conforming data points, often targeted by malicious attacks. One-Class Support Vector Machines (OCSVMs) are effective but vulnerable to sophisticated adversaries compromising training data integrity. To enhance security, a defense mechanism based on data contraction is proposed, adding uncertainty to OCSVMs to thwart adversaries. The approach successfully identifies anomalies by contracting data in low dimensional spaces, as shown through theoretical analysis and empirical evidence on benchmark datasets. Anomaly detection using One-Class Support Vector Machines (OCSVM) is effective but vulnerable to adversarial attacks. A proposed defense mechanism involves contracting data in low dimensional spaces to improve OCSVM performance by 2-7%. This method successfully identifies anomalies that would not be detectable in the original dimensional space, enhancing security in various applications such as network intrusion detection and fraud detection. Adversarial attacks can compromise the integrity of learning algorithms used in anomaly detection systems. Adversaries may manipulate training data to distort the learning algorithm's decision-making capabilities, aiming to avoid detection or decrease system performance. Sophisticated adversaries can conduct attacks in various ways, such as forcing the algorithm to learn a distorted representation that benefits them. In this work, the focus is on making OCSVMs more resistant against adversarial attacks that target the integrity of training data through distortions. Adversaries can manipulate input data to force the learning algorithm to favor them, posing a threat to machine learning systems, especially in image recognition where imperceptible perturbations can lead to mis-classifications. In this study, the goal is to enhance the attack resistance of OCSVMs against adversarial attacks by using a nonlinear data projection algorithm. The use of selective nonlinear random projections can help increase the resistance of OCSVMs under adversarial conditions. This approach involves projecting a dataset using a carefully chosen projection matrix comprised of random elements, which can improve training and evaluation times without compromising accuracy. The study aims to improve OCSVMs' resistance to attacks by employing a nonlinear data projection method. By using a carefully selected random projection matrix, the original data distribution properties are preserved in the projected dataset with minimal changes. This additional layer of security makes it difficult for adversaries to guess the projection mechanism used by the learner. The study focuses on enhancing OCSVMs' defense against attacks through nonlinear data transformations. By introducing a random projection matrix, the original data distribution is maintained with minimal alterations, increasing security against adversaries. The proposed approach aims to increase attack resistance and provide unpredictability through randomized kernels, offering a significant advantage in security for anomaly detection. In this section, the use of randomized kernels for anomaly detection is discussed. BID13 introduced Random Kitchen Sinks (RKS) to approximate kernel functions by mapping data to a low-dimensional feature space. Rahimi and Recht explicitly mapped data to a low-dimensional space for kernel approximation. This method is more computationally efficient than basis expansion and allows for efficient evaluation of new data points. Subsequently, BID11 introduced a transformation method with lower time and space requirements. Randomized feature maps like RKS and transformation methods with lower time and space complexities have been introduced for efficient evaluation of new data points in anomaly detection. BID4 introduced R1SVM, an unsupervised anomaly detection technique using randomized, nonlinear features. Our work focuses on using random projections as a defense mechanism for OCSVMs under adversarial conditions, a novel approach not previously explored. The paper introduces a unique framework using OCSVMs and kernels for unsupervised learning in anomaly detection. It addresses the issue of adversarial attacks and defense mechanisms, contrasting with previous works using AD-SVM and DNNs. The focus is on the vulnerability of learning systems to adversarial data points, as demonstrated by recent research on altering road signs. This paper presents a framework combining adversarial learning, anomaly detection using OCSVMs, and randomized kernels to address the issue of adversarial attacks in anomaly detection. The adversary aims to disrupt the learning process by modifying the training data, making it challenging for the learner to detect anomalous data points. The training dataset is distorted by perturbations made by the adversary, and the learner cannot differentiate between the original data and the perturbations. The learner cannot distinguish between original data and perturbations made by the adversary, who can determine the perturbations based on limited knowledge. The data is projected to a lower dimensional space to minimize the effectiveness of attacks, using a nonlinear transformation with a randomly drawn vector. This transformation helps approximate nonlinear kernels like the Radial Basis Function, reducing computational overheads in anomaly detection. The anomaly detection problem is addressed using the OCSVM algorithm, which separates training data with a maximal margin in a transformed space. The dual form of the OCSVM algorithm is written in matrix notation, with \u03b1 as Lagrange multipliers, \u03bd as a parameter defining bounds on support vectors and outliers, and \u03c1 as the margin of the separating hyperplane. The adversary in an integrity attack uses perturbations to move the hyperplane away from the data. The adversary in an integrity attack aims to minimize the margin of separation by injecting malicious data into the training dataset. They focus on targeted attacks to smuggle specific anomalies across the separating margin, using a non-zero displacement vector to move data points without knowledge of the learner's projection mechanisms. This attack model is inspired by a restrained attack model, where the adversary selects a subset of anomalies to push towards the normal data cloud. The adversary in an integrity attack aims to minimize the margin of separation by injecting malicious data into the training dataset. They focus on targeted attacks to smuggle specific anomalies across the separating margin, using a non-zero displacement vector to move data points without knowledge of the learner's projection mechanisms. The attack model used is inspired by a restrained attack model described by BID16, where a random subset of anomalies is pushed towards the normal data cloud and injected into the training set. The severity of the attack is controlled by the parameter s attack \u2208 [0, 1], with distorted anomalies seen as normal data points by the learning algorithm. As the attack severity increases, the distorted digit tends to resemble its original form. The adversary optimally chooses a target point for distorting each data point, requiring computational effort and knowledge of data distribution. They use the centroid of normal data as the target for distorting anomaly data. By adding \u03ba ij to each attribute in the original feature space, the adversary can orchestrate different attacks by adjusting the percentage and severity of distortion. Pushing data away from the normal cloud risks detection of the attack. The adversary distorts anomaly data by pushing it away from the normal data cloud, increasing the risk of detection. Using random projections to reduce dimensionality introduces uncertainty, giving the learner a security advantage but also posing a caveat. Random projections can help reduce the dimensionality of data, providing a security advantage to the learner. However, the unpredictability of some projections can lead to data overlap between different classes. To increase attack resistance, a projection that conceals adversarial distortions is proposed. By selecting a projection that contracts the entire training set and separates data with the largest margin in the transformed space, a more resistant learning system can be achieved. This approach is motivated by a generalized version of Dunn's index BID2, proposing a compactness measure to identify suitable projections. The proposed approach aims to increase attack resistance by selecting a projection that contracts the entire training set and separates data with the largest margin in the transformed space. A compactness measure is used to identify suitable projection directions in a one-class problem, with the learner selecting the projection that gives the highest value. The objective is to minimize the effects of attacks by maximizing the radius of the minimum enclosing hypersphere in the transformed space. The learner aims to minimize the effects of attacks by selecting a projection that contracts the training set and maximizes the margin in the transformed space. Analyzing the adversary's perturbations on the margin of separation of the OCSVM, an upper bound on w2 is derived based on assumptions about small and positive distortions made by the adversary. The adversary distorts data in the original feature space, aligning datasets to force adversarial distortions towards normal data. Large distortions increase the risk of detection, so rational adversaries avoid significant perturbations. Theorem 1 states that the strength of attacks is reflected in the value of \u03c9 * p, which can be bounded by reducing the dimensionality of the data. The defender can tighten the upper bound of attacks by reducing dataset dimensionality. Empirical validation is in TAB2 and proof in Appendix A. Proposed defense mechanism effectiveness shown on MNIST, CIFAR-10, and SVHN datasets against directed attacks. Adversary aims to classify anomalies as normal data during evaluation. Test sets created with a normal to anomaly ratio of 5:1. The datasets for MNIST, CIFAR-10, and SVHN are divided into clean test sets (test C) and distorted test sets (test D) with anomalies pushed closer to normal data. Normalization is done by dividing all values by 255. Nonlinear projections are chosen based on local intrinsic dimensionality (LID) values. The learner performs 1,000 nonlinear transformations for each dimension and selects the projection with the highest compactness. The transformed training set is then used to train an OCSVM with a linear kernel. The learner uses Equation 5 to transform the training set and train an OCSVM with a linear kernel. The \u03bd parameter is kept fixed for all experiments to evaluate the impact of adversarial distortions. The OCSVM trained on nonlinearly transformed data shows 2-7% higher classification performance compared to the original feature space. The OCSVM trained on nonlinearly transformed data shows higher classification performance compared to the original feature space, with reduced computation time. Adversarial samples can be better identified by OCSVM trained on clean data, indicating vulnerability to integrity attacks. Reducing dimensions can increase f-scores, but further reduction leads to a decrease. Reducing dimensions can increase f-scores, but further reduction leads to a decrease. By projecting data to a lower dimensional space, adversarial samples can be better identified. There is a significant improvement in detecting adversarial samples under the proposed approach. However, reducing dimensions below a certain threshold can cause a decline in performance. The experiments demonstrate that reducing dimensionality can increase the robustness of OCSVMs to adversarial attacks on integrity. However, beyond a certain threshold, there is a loss of useful information leading to a decline in performance. The effectiveness of the bound derived in Theorem 1 is shown in TAB2, with the upper bound becoming tighter under dimension reduction. Overall, projecting a distorted dataset to a lower dimension can enhance model robustness, but excessive dimensionality reduction can negatively impact performance. This paper investigates unsupervised anomaly detection using OCSVMs and random projections for dimensionality reduction in the presence of an adversary. The results show that OCSVMs can be significantly affected by adversarial conditions, but random projections can enhance the robustness of OCSVMs. The approach adds a layer of uncertainty, making it difficult for an adversary to guess the learner's details, thus improving the security of the learning system. Our approach aims to enhance the security of learning systems by reducing the impact of adversarial perturbations and adding randomness to the search space. Future work will explore the compatibility of our approach with other learning algorithms and optimal selection of dimensions. Additionally, we plan to investigate game-theoretical formulations for anomaly detection and adversarial learning under dimensionality reduction techniques. Our approach aims to enhance the security of learning systems by reducing the impact of adversarial perturbations and adding randomness to the search space. Future work will explore game-theoretical formulations for anomaly detection and adversarial learning under dimensionality reduction techniques, including studying \"boiling frog\" type attacks where malicious data is gradually injected over time. The matrices and variables involved in the optimization problem for OCSVM with adversarial distortions are defined, with the optimal solution in the projected space being key to the solution. The optimal solution for OCSVM without distortion would give a value less than or equal to the value given by \u03b1. Define w * p as the primal solution optimization in the projected space."
}