{
    "title": "rJxVxiiDoX",
    "content": "We propose quantization-aware training to reduce computational cost of deep neural network based keyword spotting. Experimental results show that this approach can recover performance models quantized to lower bits representations. Combining quantization-aware training with weight matrix factorization significantly reduces model size and computation for small-footprint keyword spotting while maintaining performance. Quantization-aware training is used to optimize weights against quantization errors in building a small-footprint low-power keyword spotting system. Dynamic quantization approach is employed for DNN weight matrices, with inputs quantized row-wise on the fly. The accuracy loss due to quantization is incorporated via quantization-aware training, with experiments conducted using the keyword 'Alexa' on a 500 hrs far-field corpus. The study utilizes quantization-aware training to address loss due to quantization in a keyword spotting system. Experiments are conducted using the keyword 'Alexa' on a 500 hrs far-field corpus. Training involves a 3-stage process with GPU-based distributed DNN training. Results show improved performance with quantization-aware training, as depicted in DET curves and AUC improvement. The DET curves for 16 bit and 8 bit quantized-models are not shown as they are not significantly different from the full-precision model."
}