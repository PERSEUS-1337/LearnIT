{
    "title": "r1gRCiA5Ym",
    "content": "Dropout is a technique to improve generalization in deep neural networks. This paper discusses novel observations about dropout in DNNs with ReLU activations, leading to the proposed method \"Jumpout\" for better training on nearby data points. Jumpout is a technique that adaptively normalizes the dropout rate at each layer and training sample, improving performance on various datasets without significant additional costs. Dropout is a technique used to prevent overfitting in deep neural networks by randomly setting hidden neuron activations to 0. It can be applied to any layer without adding much computational overhead. However, tuning dropout rates for optimal performance can be challenging, as too high a rate can slow convergence and too low a rate may not improve generalization. In practice, a single dropout rate is often used for all layers throughout training. Dropout in deep neural networks helps prevent overfitting by randomly deactivating hidden neurons. However, using a fixed dropout rate may lead to too much or too little perturbation in different layers and samples. Additionally, dropout is not compatible with batch normalization, as rescaling undropped neurons can disrupt normalization parameters. The text discusses the drawbacks of using dropout with batch normalization in deep neural networks. It introduces \"jumpout,\" a modified version of dropout that aims to overcome these drawbacks by improving generalization performance for DNNs with rectified linear unit activations. The proposed modifications are motivated by observations about how dropout affects the polyhedral structure and linear models in DNNs with ReLU activations. Applying dropout to training samples changes ReLU activation patterns and linear models, improving generalization performance. However, fixed dropout rates may not achieve local smoothness. In \"jumpout,\" the dropout rate is a random variable sampled from a decreasing distribution to address this issue. In \"jumpout,\" the dropout rate is a random variable sampled from a decreasing distribution to achieve local smoothness and adaptively normalize the dropout rate for consistent neural deactivation rates. This addresses incompatibility issues between dropout and BN. In \"jumpout,\" the dropout rate is a random variable sampled from a decreasing distribution to achieve local smoothness and adaptively normalize the dropout rate for consistent neural deactivation rates. This addresses incompatibility issues between dropout and BN. The rate applied to activated neurons remains consistent across different layers and samples during training. Jumpout rescales outputs to maintain variance after neural deactivation, allowing for direct application of BN layers learned in training to the test phase. Jumpout, like dropout, randomly generates a 0/1 mask over hidden neurons for deactivation without extra training, showing improved performance on various tasks compared to dropout. Jumpout adjusts dropout rates based on ReLU activation patterns without relying on additional trained models. It introduces minimal computation and memory overhead and can easily be integrated into existing model architectures. This approach contrasts with other methods that require additional trained models to adaptively change dropout rates. Jumpout adjusts dropout rates based on ReLU activation patterns without additional trained models. It introduces minimal computation and memory overhead, easily integrated into existing model architectures. This contrasts with methods like Gaussian dropout and variational dropout, which optimize dropout directly or connect global uncertainty with dropout rates. Other variants like Swapout and Fraternal Dropout introduce different approaches to improve neural network architectures. Jumpout is a dropout variant that adjusts rates based on ReLU activation patterns without extra training costs. It can be integrated with existing model architectures and targets different dropout problems. The DNN formalization discussed can represent various architectures used in practice. The DNNs with bias terms at each layer can be represented in matrix multiplication. Convolution operator is a sparse weight matrix with tied parameters. Average-pooling is a linear operator, max-pooling is an activation function. Residual network block can be represented by appending an identity matrix. DNN with short-cut connections can be written in a specific form. Piecewise linear activation functions like ReLU result in a piecewise linear function for the DNN. The DNN with ReLU activation functions can be simplified into a piecewise linear function, where each layer's activation pattern modifies the weight matrix. This process eliminates the ReLU functions and produces a linear model. The gradient of the linear model is the weight vector, and it is associated with the activation patterns on all layers for a given input data point, defining a convex polyhedron. Dropout improves the generalization performance of deep neural networks by promoting independence among neurons and preventing co-adaptation. ReLU activations are cost-effective and widely used, simplifying DNNs into piecewise linear functions. This allows for easier analysis of local linear models and inspires modifications to dropout for better performance. Dropout improves DNN performance by promoting neuron independence and preventing co-adaptation. It trains smaller networks during training and acts as an ensemble during testing, reducing variance. Dropout also smooths local linear models in DNNs with ReLUs by dividing the input space into convex polyhedra, enhancing generalization. The number of convex polyhedra in DNNs can be exponential in the number of neurons, leading to dispersed training samples and distinct local linear models for each data point. Nearby polyhedra may result in significantly different linear models due to activation patterns. This lack of smoothness can make DNNs fragile and unstable on new data, weakening generalization ability. To address this, a dropout rate can be sampled from a truncated half-normal distribution. To address the instability of DNNs on new data and improve generalization ability, a dropout rate is proposed to be sampled from a truncated half-normal distribution. This method ensures a monotone decreasing probability of dropout rates, with smaller rates sampled more frequently. The standard deviation \u03c3 is used as a hyper-parameter to control generalization enforcement. Other distributions like the Beta distribution could also be explored in future work. The Gaussian-based dropout rate distribution encourages smoothness in generalization performance of local linear models. Tuning dropout rates for each layer separately is ideal but computationally expensive. Setting a single global dropout rate is suboptimal due to varying proportions of active neurons in different layers. To better control dropout behavior across different layers and training stages, the dropout rate is normalized by the fraction of active neurons in each layer. This allows for more consistent activation patterns and precise tuning of the dropout rate as a single hyper-parameter. In standard dropout, neurons are scaled by 1/p during training, but with this approach, the dropout rate is adjusted to achieve the desired level of smoothing encouragement. In standard dropout, neurons are scaled by 1/p during training to maintain consistency in mean values between training and test phases. However, the variance can differ significantly, leading to unpredictable behavior in DNNs when combined with batch normalization. One proposed solution is to combine dropout layers with BN layers, followed by a ReLU activation layer, to control dropout behavior and achieve precise tuning of the dropout rate as a single hyper-parameter. Dropout changes the scales of mean and variance of neurons during training, affecting the consistency of Batch Normalization (BN) parameters. To address this, rescaling the output y j by (1 - p j)^-1 can counteract dropout's impact on mean and variance scales, ensuring consistency between training and test phases. Dropout affects the scales of mean and variance of neurons during training. Rescaling the output y j by (1 - p j)^-1 can counteract dropout's impact on mean and variance scales. Rescaling factor should be (1 - p j)^-0.5 for recovering the scale of the variance if E(y j) is small. However, computing information about w j requires additional computation and memory cost. Simple scaling methods cannot resolve the shift in both mean and variance. Dropout affects the scales of mean and variance of neurons during training. Rescaling the output y j by (1 - p j)^-1 can counteract dropout's impact on mean and variance scales. The rescaling factor should be (1 - p j)^-0.5 for recovering the scale of the variance if E(y j) is small. In practice, a trade-off point of (1 - p j)^-0.75 is proposed to balance between (1 - p j)^-1 and (1 - p j)^-0.5. This approach makes both the mean and variance consistent for cases with and without dropout. The rescaling factor (1 - p j)^-0.75 is used to normalize the dropout rate in a convolutional network with and without Batch Normalization (BN). Using dropout with BN can improve performance, with larger dropout rates potentially resulting in more improvement. However, the original dropout with BN leads to decreased accuracy when the dropout rate exceeds 0.15. In contrast, using the rescaling factor maintains performance improvement with increasing dropout rates, up to 0.25, making it the best configuration among the four tested. Our proposed improved dropout, \"Jumpout,\" combines three modifications to overcome original dropout drawbacks. Jumpout samples from a decreasing distribution for a random dropout rate and normalizes it adaptively based on active neurons. It further scales outputs differently during training to balance mean and variance shifts. Jumpout requires a main hyper-parameter \u03c3 to control the standard deviation of the distribution. Jumpout requires three hyperparameters: \u03c3 for controlling the standard deviation, and two auxiliary truncation hyperparameters (p min , p max ) to bound samples from the distribution. Tuning \u03c3 alone yielded good performance. The input h j is considered as features of layer j for one data point. For a mini-batch, estimating q + j separately or averaging over data points is possible, with the latter being more efficient. Jumpout has similar memory cost as original dropout but requires counting active neurons for computation, which is minimal. Jumpout has a memory cost similar to original dropout, with minimal computation required for counting active neurons and sampling from the distribution. The study applies dropout and jumpout to various DNN architectures, comparing their performance on different benchmark datasets. The DNN architectures include small CNNs, WideResNet, ResNet, and others applied to datasets like CIFAR10, CIFAR100, Fashion-MNIST, SVHN, STL10, and ImageNet. Standard settings, data preprocessing, and hyperparameters are followed for experiments on CIFAR and Fashion-MNIST datasets, while starting from a pre-trained ResNet18 for ImageNet. The study applies dropout and jumpout to various DNN architectures on benchmark datasets like CIFAR10, CIFAR100, Fashion-MNIST, SVHN, STL10, and ImageNet. Starting from a pre-trained ResNet18 model for ImageNet, jumpout consistently outperforms dropout on all datasets tested, even improving test accuracy on Fashion-MNIST and CIFAR10. Additionally, jumpout achieves significant improvements on CIFAR100 and ImageNet compared to other DNNs and training methods. Jumpout demonstrates superior performance compared to dropout on various benchmark datasets, including CIFAR100 and ImageNet. A thorough ablation study confirms the effectiveness of each modification, with the combination of all three modifications (jumpout) achieving the best results. Learning curves show that jumpout with adaptive dropout rates per minibatch outperforms dropout in early learning stages and reaches higher accuracy faster. Further optimization of the learning rate schedule for jumpout may lead to even faster convergence to peak performance. In the study, jumpout outperforms dropout on benchmark datasets like CIFAR100 and ImageNet. The dropout rescaling factor (1 \u2212 p) \u22120.75 shows a good balance between mean and variance rescaling. The network used is \"CIFAR10(s)\" and the goal is to find a better learning rate schedule for jumpout to improve performance."
}