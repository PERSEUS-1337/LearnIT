{
    "title": "HJ3d2Ax0-",
    "content": "The success of modern Recurrent Neural Networks (RNNs) lies in their ability to model long-term temporal dependencies. However, a formal measure of RNNs' long-term memory capacity is lacking. This paper introduces the Start-End separation rank as a measure of information flow across time in deep recurrent networks. Deep recurrent networks have a higher Start-End separation rank compared to shallow counterparts, allowing them to correlate different parts of input sequences exponentially as the sequence length increases. Depth provides a significant advantage in modeling long-term dependencies, demonstrated by Recurrent Arithmetic Circuits (RACs). This highlights the superior ability of deep recurrent networks in capturing sequential data compared to shallow networks. Recurrent Neural Networks (RNNs) have excelled in modeling sequential data, showing success in language modeling, neural machine translation, speech recognition, and more. The structure of recurrent networks and their ability to integrate data over time is still not fully understood. Efforts to enhance their performance include using Long Short Term Memory (LSTM) networks and stacking layers to create deep recurrent networks. In this paper, the focus is on deep recurrent networks and their superiority over shallow ones in processing information hierarchically at every time-step. Experimental evidence suggests that deep networks model correlations corresponding to longer time-scales. The advantage of depth in complexity and temporal capacity of recurrent networks is addressed, with a discussion on the efficiency of deep networks in expressing functions that shallow networks would require a super-polynomial size for. The text discusses the efficiency of deep recurrent networks in modeling long-term dependencies. It introduces a recurrent arithmetic circuit (RAC) to explore the impact of depth on the network's ability to handle varying input sequence lengths. The study shows that depth provides an exponential boost to the network's capacity to capture long-term dependencies. The text introduces recurrent arithmetic circuits (RACs) as a modification to traditional RNNs, showing similarities to Multiplicative RNNs and Multiplicative Integration networks. The study connects RACs to Tensor Train decomposition, suggesting a relationship with Multiplicative RNNs. The analysis extends to common ConvNets, highlighting the depth efficiency of such networks. In section 3, the text introduces the Start-End separation rank as a measure of a recurrent network's ability to model long-term dependencies. This rank measures the distance from separability, indicating strong correlation between the beginning and end of an input sequence. The Start-End separation rank measures a recurrent network's ability to model long-term dependencies by showing strong correlation between the beginning and end of an input sequence. Deep networks have exponentially higher separation ranks than shallow networks, allowing them to model more elaborate input dependencies over longer periods of time. Additionally, the separation rank of deep networks grows exponentially with sequence length, while shallow networks remain independent of sequence length. This highlights the inadequacy of shallow networks in modeling correlations of long input sequences compared to deep networks. In this section, a quantitative conjecture is presented regarding the Start-End separation rank of recurrent networks, showing exponential growth with network depth. Recurrent Arithmetic Circuits (RACs) are introduced as a class of recurrent networks with similar architectural features to standard RNNs. The operation of RACs on sequential data is identical to RNNs, differing only in the type of non-linearity used in the calculation. The algebraic properties of RACs are utilized to prove their ability to model long-term dependencies of inputs. The basic framework of shallow recurrent networks is described, highlighting the differences between common RNNs and RACs. The basic framework of shallow recurrent networks is presented, focusing on sequence to sequence classification tasks. The network models a discrete-time dynamical system with temporal dependence denoted by t. The input is mapped to vectors in an input space X, and the output is a sequence of class scores vectors. The shallow recurrent network with hidden channels is depicted in fig. 1(a) and the output at time t is calculated using specific formulas. The shallow recurrent network with R hidden channels at time t is represented by ConvNet. The hidden state at time t is denoted by ht, with learned parameters \u0398 for input, hidden, and output weights matrices. The non-linearity for common RNNs is typically sigmoid or tanh, while for RACs, it is element-wise multiplication. This Multiplicative Integration approach is extended to deep recurrent networks following a common approach. The deep recurrent network follows a common approach where each layer acts as a recurrent network receiving the hidden state of the previous layer as input. The output is constructed using learned parameters and non-linear operations to determine the type of network. The newly presented class of RACs is considered a good surrogate for common RNNs. The newly presented class of Recurrent Arithmetic Circuits (RACs) is seen as a good substitute for traditional RNNs. There are structural similarities between RACs and RNNs, with both networks having a recurrent aspect in the same form. RACs, which include Multiplicative Integration, have shown superior performance compared to existing RNN models. The algebraic properties of RACs are used to highlight the benefits of depth in recurrent networks. In this section, we introduce the Start-End separation rank as a measure of information flow in recurrent networks for modeling long-term temporal dependencies. The rank is tied to grid tensors, allowing for an exponential boost in modeling capabilities with depth. The separation rank quantifies a function's distance from separability with respect to input subsets, showcasing the benefits of depth in recurrent networks. The Start-End separation rank measures information flow in recurrent networks for modeling long-term temporal dependencies. It quantifies a function's distance from separability with respect to input subsets, showcasing the benefits of depth in recurrent networks. The rank is connected to the L2 distance of the function from the set of separable functions. The separation rank in recurrent networks measures the distance from separability with respect to input subsets. It is linked to correlations modeled by deep convolutional networks and quantum entanglement measures. A separation rank of 1 indicates a function is separable and cannot model interactions between inputs at the beginning and end of a sequence. The higher the separation rank, the more the function models dependency between inputs. The separation rank in recurrent networks measures the distance from separability with respect to input subsets, indicating the ability to model dependencies between inputs. Deep RACs support exponentially larger Start-End separation ranks than shallow RACs, making them better suited for modeling long-term temporal dependencies. The upcoming analysis will use grid tensors to evaluate these separation ranks. Tensors are multi-dimensional arrays with modes representing the number of indexing entries. The tensor product is a fundamental operator in tensor analysis, denoted by \u2297, which combines two tensors to create a new tensor. Matricization of a tensor A w.r.t. a partition (S, E) arranges the tensor elements into a matrix. A shallow RAC with R hidden channels computes the score of a class c at time T using a recursive definition. The shallow RAC weights tensor, denoted as A T,1,\u0398 c, is a key component in computing the score of a class c at time T. It can be constructed using a Tensor Train (TT) decomposition method, which is analogous to a Matrix Product State (MPS) Tensor Network in quantum physics. The concept of grid tensors is introduced as a form of function discretization, where the function is evaluated on a large grid and the outcomes are stored in a tensor. This allows for the calculation of separation ranks and benefits of depth in recurrent networks. The grid tensors of functions realized by recurrent networks help in establishing conclusions about their structure and performance. The tensorial structure of a shallow RAC is tied to its Start-End separation rank through grid tensors. The network's initial mapping functions and g \u03bd functions must be measurable and squareintegrable. The Start-End separation rank is equal to the rank of the matrix obtained by the grid tensor matricization. The limitation to specific template vectors does not restrict the results, as grid tensors are used to bound the separation rank. The equality to the rank of the matrix obtained by matricizing the shallow RAC weights tensor is useful for proving main results. Deep RACs cannot be written in closed tensorial form like shallow RACs, leading to differences in function realization. Claim 2 provides a lower bound on the Start-End separation rank of functions realized by deep RACs. In the next section, we use Claim 2 to show that deep recurrent networks have exponentially higher Start-End separation rank than shallow networks. The main theoretical contributions of the paper include a result that separates the memory capacity of deep and shallow recurrent networks, along with a quantitative conjecture on the memory capacity of deep networks of varying depths. The formal proof of the conjecture is left for future work. Theorem 1 states that deep recurrent networks have exponentially higher Start-End separation rank than shallow networks, indicating enhanced ability to model long-term temporal dependencies in sequential input. This implies depth efficiency in recurrent networks. The ability of deep recurrent networks to model long-term temporal dependencies in sequential input is highlighted by Theorem 1, which shows depth efficiency. It states that deep networks require exponentially fewer parameters than shallow networks to implement the same function, due to the enhanced Start-End separation rank. This indicates that deep networks can capture more elaborate correlations over longer periods of time compared to shallow networks. Theorem 1 demonstrates the depth efficiency of deep recurrent networks in capturing long-term correlations compared to shallow networks. Deep networks require exponentially fewer parameters to model the same function, showcasing their ability to represent more elaborate correlations over longer periods of time. The proof sketch of theorem 1 establishes that shallow networks are more restricted in modeling long-term correlations, as their Start-End separation rank remains constant with sequence length T, while deep networks enjoy an exponentially increasing Start-End separation rank. The rank of the matrix obtained by matricizing any tensor according to a partition is equal to a min-cut separating S from E in the Tensor Network graph. The TT-decomposition implies that the min-cut in the appropriate Tensor Network graph is equal to R, and for a deep network, the Start-End separation rank is lower bounded by the rank of the matrix obtained by the corresponding grid tensor matricization. The rank of the matricized grid tensor is determined by the weight assignment, with a lemma stating that finding a single example where the rank exceeds the desired lower bound implies the desired inequality holds for most parameter values. The assignment results in a rank achieving the upper bound, as shown in Theorem 1 with a lower bound of DISPLAYFORM0. Theorem 1 establishes a lower bound on the Start-End separation rank of depth L = 2 recurrent networks, distinguishing deep from shallow networks. Conjecture 1 suggests a tighter lower bound for networks with depth L > 2, indicating exponential growth in memory capacity. Visualization of the computation in deep RACs through Tensor Networks supports this conjecture. The construction and motivation for the conjecture regarding deep RACs are detailed in appendix A. A Tensor Network is a graphical tool for algebraic operations resembling vector and matrix multiplications. An example in FIG3 illustrates a depth L = 3 RAC computation after T = 6 time-steps, showcasing the values of weight matrices. The inputs are integrated in a depth-dependent and time-advancing manner. To estimate a lower bound on the Start-End separation rank of depth L > 2 recurrent networks, a similar strategy to the L = 2 case is employed. To estimate a lower bound on the Start-End separation rank of a depth L > 2 recurrent network, a specific assignment of network weights is found, forming a basic unit connecting \"Start\" and \"End\" in the Tensor Network. This unit is raised to the power of its repetitions in the graph, with the rank upper bounded by a certain value. The challenge lies in proving that this upper bound is tight, as detailed in the proof presented in the appendix. In the deep RAC Tensor Network graph, the number of repetitions of the basic unit connecting \"Start\" and \"End\" is equal to DISPLAYFORM3 for any depth L. Conjecture 1 suggests a further exponential separation between recurrent networks of different depths, beyond the already proven advantages of deep networks over shallow ones. The proof of this result is left as an open problem to enhance the understanding of depth advantages in recurrent networks. In this paper, a measure called the Start-End separation rank is proposed to quantify the memory capacity of recurrent networks in modeling long-term temporal dependencies. This measure adjusts to the input sequence length and evaluates the network's ability to correlate sequential data over time. The analysis focuses on Recurrent Arithmetic Circuits, showing that deep RACs exhibit exponential separation rank growth with longer input sequences, while shallow RACs remain independent of input length. The study demonstrated that deep Recurrent Arithmetic Circuits (RACs) show exponential separation rank growth with longer input sequences, while shallow RACs remain unaffected by input length. The analysis utilized tools from various fields to highlight the advantage of depth in modeling long-term dependencies in recurrent networks. This approach can be extended to other architectural features in modern networks, providing new insights and potential practical applications. The experiments in Hermans and Schrauwen (2013) suggest that shallow layers of recurrent networks handle short time-scales, while deeper layers support longer time-scales. Levine et al. (2017) provide practical conclusions on choosing hidden channels in deep convolutional networks. The conjecture in this paper proposes that the separation rank of recurrent networks grows exponentially with depth, offering insights into enhancing memory capacity. These analyses, combined with experiments, aim to deepen our understanding of the role of deep layers in recurrent networks. The text discusses the importance of deep layers in recurrent networks for memory capacity. It introduces Tensor Networks (TNs) and their application in modeling temporal correlations. Sections cover TN construction for shallow and deep Recurrent Autoencoders (RACs), highlighting the enhanced ability of deep RACs to capture complex temporal dependencies. The text also presents a conjecture on the exponential growth of RACs' Start-End separation rank with depth. Tensor Networks (TNs) are weighted graphs where nodes correspond to tensors with edges representing different modes. Each edge has a bond dimension equal to the tensor mode's dimension. TNs represent tensors of various orders, with connectivity properties showing operations between tensors through contracted indices. Tensor Networks (TNs) represent tensors through contracted and open indices, with the entire TN calculated by summing over contracted indices. Contraction of indices in TNs is a generalization of matrix multiplication, as shown in an example with a vector and matrix. The computation of output in a shallow recurrent network can be written in terms of a TN, as depicted in FIG5. The computation in a shallow recurrent network can be represented by a Tensor Network (TN), which is essentially a temporal concatenation of a unit cell that performs a similar computation at every time-step. This unit cell consists of input and hidden weights matrices contracted with input and hidden state vectors, as well as a triangular tensor \u03b4. The recursive relation defined by the unit cell is given by a TN, as shown in FIG5. The restricted \u03b4 tensor in the Tensor Network (TN) yields element-wise multiplication property, allowing for the computation of a shallow Recurrent Autoencoder Circuit (RAC). The TN representing the shallow RAC weights tensor can be drawn in the form of a standard MPS TN, enabling the representation of an order T tensor. The TN representing the shallow RAC weights tensor can be drawn in the form of a standard MPS TN, allowing for the representation of an order T tensor with a linear amount of parameters. This enables the employment of min-cut analysis for quantifying information flow across time. The computation of a deep recurrent network in the language of TNs is more complex due to the reuse of information in deep networks. The computation of deep recurrent networks is more complex than shallow networks due to the reuse of information. Duplicating hidden states in each layer and sending them as inputs to the next layer is impossible to represent in Tensor Networks. The complexity of deep recurrent networks lies in the reuse of information, which is challenging to represent in Tensor Networks. To overcome this, a 'trick' is used to duplicate input data at each layer, allowing for the representation of the network's computation. The input is inserted into the TN multiple times to circumvent restrictions, yielding an elaborate TN construction for deep RACs. The TNs grow exponentially in size as the depth of the recurrent network increases, serving as a theoretical tool for analysis. The actual deep recurrent network is constructed differently, growing linearly in size despite the TN's exponential growth. This growth allows deep recurrent networks to model more intricate correlations over longer periods compared to shallower networks. The depth L = 2 recurrent network in FIG6 spreads out over T = 4 time-steps, focusing on the calculation of hidden state vector h 2,2. Input duplication technique inserts f (x 1 ) twice to achieve the same h 1,1 twice in the TN, leading to a fractal structure involving self similarities in deeper layers. The TN in FIG6 correctly holds the value of h 2,2. Deeper layers lead to a fractal structure with self similarities. The duplication of intermediate hidden states creates complexity in the RAC TN. Generalizing to different depths and time-steps results in increasing complexity. This construction allows for investigating the impact of network depth on modeling long-term dependencies. The formal motivation for the lower bound on the Start-End separation rank of deep recurrent networks is presented in conjecture 1. The conjecture relies on finding a specific instance of network parameters where the rank of the matrix obtained by grid tensor matricization is a lower bound on the Start-End separation rank. This is established by combining claim 2 and lemma 1, which show that the rank of the matricized grid tensor is greater for certain examples, thus providing the lower bound. The lower bound on the Start-End separation rank of deep recurrent networks is established by finding a specific instance of network parameters where the rank of the matricized grid tensor is greater than the desired lower bound. By choosing a weight assignment that separates the first layer from higher layers, the computation in deeper layers only contributes a constant factor. This simplifying assignment allows for the evaluation of the rank of the matrix, with graph segments involving only indices from the \"Start\" set not affecting the rank under mild conditions on the weights. The rank of the matrix in deep recurrent networks is evaluated by considering graph segments involving only \"Start\" set indices, which do not affect the rank under certain conditions on the weights. The effective TN after a certain number of time-steps is determined by the separation of \"Start\" and \"End\" indices, with the number of repetitions increasing exponentially with the depth of the network. The number of occurrences in layer L = 1 of the basic unit connecting \"Start\" and \"End\" indices increases exponentially with the depth of the RAC.\u03c6(T, L, R) represents the computation after T time-steps by an RAC with L layers and R hidden channels per layer. The tensor V d 1 ...d T/2 reflects the contribution of the \"Start\" set indices in layer L = 1. The computation after T time-steps by an RAC with L layers and R hidden channels per layer involves counting the number of occurrences of the basic unit connecting \"Start\" and \"End\" indices. This is achieved by considering a rank R matrix obtained by matricization of the TN basic unit. The lower bound presented in conjecture 1 is derived by defining functions of the respective decomposition to a sum of separable functions. The proof strategy outlined in section 4 shows an exponential advantage of deep recurrent networks over shallow ones in modeling long-term dependencies, as measured by the Start-End separation rank. The Tensor Network construction of the calculation by a shallow RAC is represented by a Matrix Product State Tensor Network. The shallow RAC weights tensor is represented by a Matrix Product State (MPS) Tensor Network with a bond dimension equal to R. The rank of the matrix obtained by matricizing any tensor according to a partition is equal to a min-cut separating S from E in the Tensor Network graph. The minimal cut in the MPS Tensor Network is equal to the bond dimension R, unless R > M T/2, in which case it contains the external legs instead. Claim 2 assures that the Start-End separation rank of a depth L = 2 RAC is lower bounded by the rank of the matrix obtained by the corresponding grid tensor matricization. The rank of a depth L = 2 RAC is lower bounded by the matrix rank obtained from the grid tensor matricization. By choosing specific template vectors and weight matrices, the rank can be achieved for all configurations of recurrent network weights except for a set of Lebesgue measure zero. The assignment involves setting matrices and hidden state values for a non-singular matrix F. The output after T time-steps is calculated based on specific template vectors and weight matrices. The grid tensor is evaluated using defined parameters, leading to a specific form of the tensor product. The assignment involves setting matrices and hidden state values for a non-singular matrix F. The output after T time-steps is calculated based on specific template vectors and weight matrices. The grid tensor is evaluated using defined parameters, leading to a specific form of the tensor product into two expressions. The left part contains only indices in the start set S, and the right part contains all external indices. Matricization w.r.t. the Start-End partition maps the left part to a vector a and the right part to a matrix B. The matrix B can be written as a sum of N rank-1 matrices, showing linear independence of vectors. In lemma 3, it is proven that there exists a value of \u2126 such that the maximal reward over all possible initial states is attained at a specific state q for all values of z except a finite set. This is crucial for establishing the theorem. Additionally, a claim regarding the prevalence of maximal matrix rank for matrices with polynomial entries is discussed, showing that the set of points where the rank is less than a certain value has zero measure. The text discusses the importance of finding a specific assignment of recurrent network weights to achieve a certain rank in order to prove a theorem. It also explains that for a matrix with polynomial entries, if a contributor to the determinant has the highest degree of x, the matrix is fully ranked for all values of x except a finite set. The lemma proves that a matrix is fully ranked if a contributor to its determinant has the highest degree of x, except for a finite set of values. This helps confirm the required grid tensor matricization rank for recurrent network weights. The vector rearrangement inequality is established using a theorem from Hardy et al. (1952). The rearrangement inequality theorem from Hardy et al. (1952) states that for a set of non-negative numbers, the inequality holds for all permutations, with equality only when certain conditions are met. This inequality applies to each component of the vectors separately. Additionally, it is proven that there exists a specific component for which the inequality is hard to satisfy. This result helps ensure that a matrix, denoted \u016a, meets certain conditions and is fully ranked."
}