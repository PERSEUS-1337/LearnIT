{
    "title": "HyGBdo0qFm",
    "content": "Alternatives to recurrent neural networks, such as architectures based on attention or convolutions, are gaining popularity for processing input sequences. The computational power of these alternatives, specifically the Transformer and Neural GPU models, has been explored. Both models are shown to be Turing complete based on their ability to compute and access internal data representations without needing external memory. This study highlights the importance of neural network architectures being Turing complete to learn algorithms from examples. The work by Siegelmann & Sontag (1995) established that recurrent neural networks (RNNs) are Turing complete even with a bounded number of resources. This is based on their ability to compute internal data representations and access them, releasing their full computational power without increasing model complexity. Many early neural architectures, including Neural Turing Machines, are extensions of RNNs and are Turing complete according to Siegelmann & Sontag's criteria. The Transformer and the Neural GPU are shown to be Turing complete without needing external memory, based on their ability to compute and access internal data representations. This holds for bounded architectures and is proven by simulating a Turing machine for the Transformer. Our study demonstrates the Turing completeness of Transformers and Neural GPUs by simulating a Turing machine and standard sequence-to-sequence RNNs, respectively. The comparison of their computational power is formalized in our paper, providing minimal sets of elements required for completeness. Background research connects neural networks to first-order logic and finite automata, with prior work establishing the Turing completeness of certain neural network architectures. The Turing completeness of various neural network architectures, such as Neural Turing Machines (NTMs), Stack-RNN, and Transformer, has been established. These architectures incorporate mechanisms like external memory and attention to enhance their ability to learn algorithms. The Transformer architecture, primarily based on attention mechanisms, has shown impressive results in language-processing tasks. Efforts have been made to enrich the Transformer's architecture with new features to improve its ability to learn general procedures. The original Transformer architecture struggles to generalize to input lengths not seen during training, but it is Turing complete. Different considerations are used compared to BID4, allowing for arbitrary internal precision during computation. Theoretical results can guide changes and improvements to the architecture, such as using hard attention instead of soft attention. The Neural GPU architecture combines convolutions and gated recurrences over tridimensional tensors, demonstrating power in learning decimal. The Neural GPU architecture combines convolutions and gated recurrences over tridimensional tensors, showing power in learning decimal multiplication. It is argued to be Turing complete, with the number of cells used proportional to the input sequence size. Padding with dummy symbols can address the need for more cells, but it is only a partial solution as memory requirements for Turing completeness cannot be determined a priori. The Neural GPU architecture combines convolutions and gated recurrences over tridimensional tensors to achieve Turing completeness without requiring additional cells for memory. The network operates on rational numbers with rational functions and uses a piecewise-linear sigmoidal activation function. It focuses on sequence-to-sequence neural network architectures that produce output sequences based on input sequences, with a seed vector and stopping criterion for output length determination. The Neural GPU architecture combines convolutions and gated recurrences over tridimensional tensors to achieve Turing completeness without requiring additional cells for memory. It focuses on sequence-to-sequence neural network architectures that produce output sequences based on input sequences, with a seed vector and stopping criterion for output length determination. In our formalization, a seq-to-seq network can produce a fixed number r \u2265 0 of output vectors, viewed as a language recognizer of strings. A seq-to-seq language recognizer is defined as a tuple A = (\u03a3, f, N, s, F), where \u03a3 is a finite alphabet, f is an embedding function, N is a seq-to-seq network, s is a seed vector, and F is a set of final vectors. The language accepted by A, denoted by L(A), is the set of all strings accepted by A. The function f : \u03a3 \u2192 Q d should be computed by a Turing machine in linear time. The set F should also be recognizable in linear time. A class N of seq-to-seq neural network architectures defines the class L N composed of all languages accepted by language recognizers using networks in N. An encoder-decoder RNN is defined by specific recursions. An encoder-decoder RNN is defined by specific recursions, and the Transformer architecture is presented in a formal way, abstracting away from specific choices of functions and parameters. The Transformer is heavily based on the attention mechanism. The Transformer architecture is heavily based on the attention mechanism, with scoring functions for query, keys, and values. Different scoring and normalization functions can be used, such as additive or multiplicative attention, combined with non-linear functions. Softmax or hardmax functions are typically used for normalization in proofs. The Transformer architecture utilizes the attention mechanism with scoring functions for query, keys, and values. The normalization function can be softmax or hardmax, with hardmax being used for hard attention. The single-layer encoder in the Transformer is defined by parametric functions Q(\u00b7), K(\u00b7), V(\u00b7), and O(\u00b7), typically involving matrix multiplications and a feed-forward network. Residual connections are added with + xi and + ai summands. The Transformer encoder is defined as the repeated application of single-layer encoders with residual connections. The L-layer Transformer encoder is defined by recursion, while a single-layer decoder includes attention to external key-value vectors. The decoder layer is parameterized by functions Q(\u00b7), K(\u00b7), V(\u00b7), and O(\u00b7). The Transformer decoder is a repeated application of single-layer decoders with a transformation function. It receives an input sequence X, a seed vector y0, and outputs a sequence Y. The Transformer is order-invariant, producing the same output for input sequences that are permutations of each other. The Transformer is order-invariant, meaning that input sequences that are permutations of each other produce the same output. This property has led to the use of positional encodings to include information about the order of the input sequence. The Transformer cannot recognize every regular language, as some regular languages are not order-invariant. The Transformer network is order-invariant, but it cannot recognize all regular languages. It satisfies a stronger property called proportion invariance. This means that not all regular languages that are order-invariant can be expressed by the Transformer. For example, the order-invariant regular language with an even number of 'a' symbols cannot be recognized by a Transformer network. On the other hand, Transformer networks can recognize languages that are not necessarily regular. The Transformer network can recognize non-regular languages, such as those with more 'a' symbols than 'b' symbols. The inclusion of positional encodings changes the computational power of Transformers, allowing them to express counting properties beyond regularity. The positional encoding must be computable by a Turing machine in linear time. The main result of this section is the completeness of Transformers with positional encodings, showing that a transformer can simulate the complete execution of a Turing machine. Strings are represented as sequences of one-hot vectors with positional encodings, and a transformer is constructed to produce sequences containing information about the state and symbol at each step. The construction and proof proceed by induction. The construction and proof of Transformers with positional encodings proceed by induction. The decoder computes the symbol at each step using M's transition function and residual connections. The symbol at each index is copied from the last symbol written by M at that index. This process is illustrated in a high-level diagram in FIG2. The decoder in the construction of Transformers with positional encodings copies symbols from the last symbol written by M at each index. The construction uses one encoder layer, three decoder layers, and vectors of dimension d = 2|Q| + 4|\u03a3| + 11. Different choices for functions and parameters are made compared to Vaswani et al. (2017), such as using hard attention to attend directly to specific positions. The construction of Transformers with positional encodings uses hard attention to attend directly to specific positions, unlike Vaswani et al. (2017) who use softmax and sin-cos functions. Future work could explore arbitrary functions with restrictions like finite precision. Arbitrary precision is crucial for internal representations in our Turing-complete proof, although practical implementations often rely on fixed precision hardware. Positional encodings can be seen as functions of the form pos : N \u2192 A in fixed precision settings. The Transformer with positional encodings and fixed precision is not Turing complete, but the computational power of fixed-precision Transformers can still be studied. The Neural GPU architecture combines convolutions and gated recurrences over tridimensional tensors, using functions U(\u00b7), R(\u00b7), and F(\u00b7) to produce sequences. Neural GPUs combine convolutions and gated recurrences over tridimensional tensors, with functions U(\u00b7) and R(\u00b7) acting as update and reset gates. The convolution operation is likened to a two-dimensional convolution with vector-matrix multiplication. This architecture shows similarities to cellular automata, where each cell is updated based on its neighbors according to a fixed rule. Neural GPUs combine convolutions and gated recurrences over tridimensional tensors, with functions U(\u00b7) and R(\u00b7) acting as update and reset gates. The architecture is similar to cellular automata, updating each cell based on neighbors according to a fixed rule. The computational power of Neural GPUs is studied by casting them as a standard seq-to-seq architecture. The output of the Neural GPU is a sequence of vectors Y, where each vector is derived from the tensor S. The bias tensor B in the equation grows with the input size, leading to the introduction of the notion of a uniform Neural GPU to address this issue. The uniform Neural GPU introduces a fixed architecture with a constant number of parameters, making it finitely specified. It is proven to be Turing complete by simulating a seq-to-seq RNN using kernel banks and uniform bias tensors. The uniform Neural GPU architecture is proven to be Turing complete by simulating a seq-to-seq RNN using kernel banks and uniform bias tensors. The computation involves updating vectors E and D sequentially, ensuring that D is always one iteration behind E. The construction requires 3d + 3 components to implement update and reset gates, with detailed proof in the appendix. The use of kernels of shape (2, 1, d, d) is essential for achieving Turing completeness. The proof of Turing completeness for the uniform Neural GPU architecture involves using kernels of shape (2, 1, d, d) and zero padding in convolution. Circular convolution instead of zero padding leads to loss of Turing completeness, as it cannot differentiate among periodic sequences of different lengths. This results in uniform Neural GPUs with circular convolutions not being Turing complete. Proposition 4.2 states that Uniform Neural GPUs with circular convolutions are not Turing complete. BID16 observed that Neural GPUs struggle with highly symmetric inputs, such as binary multiplication. BID5 simplified Neural GPUs by using piecewise linear activations and bidimensional input tensors, achieving better results in training time and generalization. Theoretical evidence suggests that these simplifications retain the full expressiveness of Neural GPUs while simplifying practical applicability. Future work includes exploring the implications of these theoretical results on practical observations. Theoretical evidence supports that simplifications in Neural GPUs maintain their full expressiveness while improving practical applicability. An analysis of Turing completeness was done on the Transformer and Neural GPU architectures, with plans for further refinement. Different features like residual connections and gating mechanisms were found to be crucial for achieving completeness. Abstract versions of both architectures were used in the analysis, with variations in functions and parameters from typical choices in practice. The use of hard attention and piecewise linear activation functions in Transformer and Neural GPU architectures is being studied for practical implications. Theoretical results suggest that undecidability in language modeling problems may require heuristic solutions due to the Turing completeness of RNNs. Further research will explore the implications of undecidability for Transformers and Neural GPUs in language modeling tasks. In future work, the study plans to explore the theoretical properties of architectures with finite precision. Siegelmann & Sontag demonstrate how a single RNN can simulate a two-stack machine, encoding strings as rational numbers for stack operations. Siegelmann & Sontag demonstrate how a single RNN can simulate a two-stack machine by encoding strings as rational numbers for stack operations. This representation allows for easy simulation of stack operations using affine transformations and \u03c3 activations. They construct two networks, N1 and N2, where N1 acts as the encoder and N2 as the decoder. The combined network N expects an input in a specific form to function properly, resembling a modern encoder-decoder RNN architecture. Siegelmann & Sontag demonstrate how a single RNN can simulate a two-stack machine by encoding strings as rational numbers for stack operations. They construct two networks, N1 and N2, where N1 is the encoder and N2 is the decoder. The construction allows for an RNN encoder-decoder N and a language recognizer A that uses N to simulate the two-stacks machine M. Important details include the use of specific formulas for N, the null matrix for R, and the embedding function for A. Additionally, modifications can be made to ensure specific neuron behavior in N2. The text discusses extending the PropInv function to sequences of vectors and introduces notation for simplifying the proof of Proposition 3.1. It highlights the relationship between embedding functions and PropInv, emphasizing the importance of proving Trans(f(w), s, r) = Trans(f(u), s, r) for every u in PropInv(w). The text discusses proving a property related to sequences of vectors and the encoder TEnc. It involves showing that for every pair of indices, if x_i = x_j, then a specific equation holds. This is done by utilizing normalization functions and a specific value \u03b3. The proof involves showing that if x_i = x_j, then k_i = k_j and v_i = v_j for every i and j. This is achieved by utilizing the encoder TEnc and normalization functions. The recursion Trans(X, y_0, r) is defined, and it is proven that Trans(X, y_0, r) = Trans(X', y_0, r) using an inductive argument. The proof involves showing that if x_i = x_j, then k_i = k_j and v_i = v_j for every i and j. This is achieved by utilizing the encoder TEnc and normalization functions. The recursion Trans(X, y_0, r) is defined, and it is proven that Trans(X, y_0, r) = Trans(X', y_0, r) using an inductive argument. TDec accesses (K, V) via attentions of the form Att(q, K, V), where q depends only on y_0. A language recognizer A using a Transformer network leads to a contradiction, demonstrating the corollary. A language recognizer A is constructed with a simple Transformer network to recognize strings with more 'a's than 'b's. The encoder and decoder parts of the transformer implement the identity function using null functions for self-attention and residual connections. External attention uses arbitrary scoring and normalization functions. Notation is introduced to prove that the network computes a sequence based on the input sequence and initial decoder value. The transformer network computes a sequence based on the input sequence and initial decoder value. The external attention produces the same score value for all positions, leading to specific calculations for each step in the sequence. The transformer network computes a sequence based on the input sequence and initial decoder value. After external attention and residual connection, the output is calculated. The proof involves showing that a certain condition holds true, leading to the acceptance of a string by a recognizer. A Turing machine is assumed to work in a specific manner when processing an input string. The transformer network Trans M simulates a Turing machine M on every input string by using an encoder to represent the input string and a decoder to process the output. The construction and proof are divided into three parts: strategy overview, encoder and decoder architecture details, and formal implementation proof. The encoder part of Trans M receives the input string and uses an embedding function to represent each character as a one-hot vector with positional encoding. The transformer network Trans M uses an encoder to represent input strings and a decoder to process outputs. The decoder simulates the execution of a Turing machine M over the input string by computing values for each time step using self-attention and attention over the encoder. This allows for the reconstruction of the complete history of the computation. The output vectors contain information about the state of the computation and input characters encoded as one-hot vectors. The construction and proof proceed by induction, starting with an initial vector representing the state before computation begins. The construction and proof of the transformer network Trans M involves encoding input strings and simulating a Turing machine's execution. The decoder computes values for each time step using self-attention and encoder attention, reconstructing the computation history. Output vectors contain information about the computation state and input characters as one-hot vectors. The construction proceeds by induction, starting with an initial vector representing the state before computation begins. The transition function \u03b4 is mimicked by a two-layer feed-forward network, producing values for q (i+1), v (i), and m (i). The construction of the transformer network Trans M involves encoding input strings and simulating a Turing machine's execution. The decoder computes values for each time step using self-attention and encoder attention, reconstructing the computation history. To compute the symbol under the head of machine M at the next time step, additional decoder layers are used. The process involves attending to different positions in the encoder to produce the symbol. The decoder in the transformer network Trans M computes, using self-attention and encoder attention, the index that machine M will point to at the next time step. This computation involves defining values c(i) and (i) to track the cell M is pointing to at each time step. The process shows that the computation of c(i) and c(i+1) can be done using one layer of self-attention. The decoder in the transformer network Trans M computes the index that machine M will point to at the next time step by tracking cell positions c(i) and values (i). The process involves self-attention and encoder attention, with an additional self-attention decoder layer for computing the desired s (r+1) value. The decoder in the transformer network Trans M computes the index for the next time step by tracking cell positions and values. It involves self-attention and encoder attention, with an additional self-attention decoder layer for computing the desired value. The architecture details of the encoder and decoder are explained, including the use of a non-linear function for the attention mechanism. The computation of hard attention is described, where the vector closest to 0 dot product with q is selected. The decoder in the transformer network Trans M computes the index for the next time step by tracking cell positions and values. It involves self-attention and encoder attention, with an additional self-attention decoder layer for computing the desired value. The architecture details of the encoder and decoder are explained, including the use of a non-linear function for the attention mechanism. When computing hard attention with the function score \u03d5 (\u00b7), the vector v j is selected such that the dot product q, k j is as close to 0 as possible. If multiple indexes minimize the dot product, attention behaves as an average of all value vectors. The vectors used in the Trans M layers are of dimension d = 2|Q|+4|\u03a3|+11, arranged in groups of values q i \u2208 Q |Q| , s i \u2208 Q |\u03a3| , and x i \u2208 Q. The transformer network Trans M uses an embedding function and positional encoding in its construction. The input sequence is represented by one-hot vectors, and positional encoding values are used to compute the input for Trans M. The positional encoding values 1/i and 1/i^2 are crucial for the correctness of the construction. The transformer network Trans M utilizes positional encoding values 1/i and 1/i^2 in its construction. The encoder part of Trans M is simple, using a single-layer encoder with self-attention and linear transformations. The decoder part of Trans M produces output sequences based on the state of M, the symbol under the head of M, and the direction of head movement. The decoder in the transformer network Trans M generates output sequences based on the state of M, the symbol under the head of M, and the direction of head movement. The construction involves using positional encodings and a proof by induction approach to create the sequence of vectors. The first self-attention produces the identity, leading to the generation of the sequence of vectors. The decoder in the transformer network Trans M generates output sequences based on the state of M, the symbol under the head of M, and the direction of head movement. The construction involves using positional encodings and a proof by induction approach to create the sequence of vectors. The first self-attention produces the identity, leading to the generation of the sequence of vectors. This is implemented with a trivial attention plus the residual connection, producing vectors and utilizing a two-layer feed-forward network for further transformations. The decoder layer in the transformer network generates output sequences based on the state of M, the symbol under the head of M, and the direction of head movement. Using positional encodings and a proof by induction approach, a sequence of vectors is created. The third decoder layer utilizes feed-forward networks to produce the desired output value.lemma B.4 shows that one can attend to position (i+1) and copy both values for every i. The decoder layer in the transformer network utilizes feed-forward networks to generate output sequences based on the state of M, the symbol under the head of M, and the direction of head movement. Lemma B.4 demonstrates that one can attend to position (i+1) and copy both values for every i, using previously computed values c(i) and c(i+1). The decoder layer in the transformer network uses feed-forward networks to generate output sequences based on the state of M, the symbol under the head of M, and the direction of head movement. It is shown that one can attend to position (i+1) and copy both values for every i, using previously computed values c(i) and c(i+1).\u03c0 1 : \u03a3 \u2192 {1, . . . , |\u03a3|} and \u03c0 2 : Q \u2192 {1, . . . , |Q|} are used to construct one-hot vectors for pairs in Q \u00d7 \u03a3.\u03b4(\u00b7, \u00b7) is the transition function of machine M. Three helping properties are proven, denoted as f 1, f \u03b4, and f 2. The decoder layer in the transformer network uses feed-forward networks to generate output sequences based on the state of M, the symbol under the head of M, and the direction of head movement. One-hot vectors for pairs in Q \u00d7 \u03a3 are constructed using functions \u03c0 1 and \u03c0 2. The transition function \u03b4(\u00b7, \u00b7) of machine M is used to construct matrices for transformations. The decoder layer in the transformer network uses feed-forward networks to generate output sequences based on the state of M and the symbol under the head of M. Matrices are constructed using the transition function \u03b4(\u00b7, \u00b7) of machine M. Functions \u03c0 1 and \u03c0 2 create one-hot vectors for pairs in Q \u00d7 \u03a3. The proof of the lemma involves constructing functions f 2 (\u00b7) and f 3 (\u00b7) as well as defining a function O 1. The decoder layer in the transformer network uses feed-forward networks to generate output sequences based on the state of M and the symbol under the head of M. Matrices are constructed using the transition function \u03b4(\u00b7, \u00b7) of machine M. Functions \u03c0 1 and \u03c0 2 create one-hot vectors for pairs in Q \u00d7 \u03a3. The proof of the lemma involves constructing functions f 2 (\u00b7) and f 3 (\u00b7) as well as defining a function O 1. In the current chunk, the attention mechanism is discussed, showing how the attention values are computed using linear transformations and feedforward networks. The process involves finding the value that maximizes a certain expression and ultimately obtaining the desired vector. The current chunk discusses the proof of a property related to the decoder layer in the transformer network. It involves mathematical calculations and inequalities to show that a specific vector is obtained. The proof concludes by establishing the argument that minimizes a certain expression. The current chunk provides a detailed proof involving mathematical calculations and inequalities to show that a specific vector is obtained in the context of the decoder layer in the transformer network. The proof concludes by establishing the argument that minimizes a certain expression. The current chunk describes the construction of a Neural GPU network that simulates a sequence N using input tensors and kernel banks. The proof shows that a bi-dimensional tensor is sufficient for simulating an RNN, and details the construction of kernel banks KU, KR, and KF. The construction of a Neural GPU network simulating a sequence N using input tensors and kernel banks is detailed. The proof shows that a bi-dimensional tensor is sufficient for simulating an RNN, and defines matrices in KF as block matrices. The properties for every t \u2265 0 are proven, showing how the construction simulates N by using different components for encoding, decoding, and communication. The proof is done by induction in t, demonstrating the computation of Ut and updating hidden states properly. The proof demonstrates that a uniform Neural GPU processing periodic input will produce periodic output. This is shown through induction, where the computation of Ut and updating hidden states are crucial components. The construction of the Neural GPU network simulating a sequence N using input tensors and kernel banks is detailed, showcasing the encoding, decoding, and communication aspects. The text chunk discusses the inability of uniform Neural GPUs to recognize the length of periodic inputs. It presents a scenario where a language recognizer defined by a uniform neural GPU fails to distinguish between strings of even and odd lengths, highlighting the limitations of such systems. The text explains how tensors S and T are defined to show that the outputs of a neural network for inputs X and X are the same, leading to a contradiction if A accepts w."
}