{
    "title": "HkxPtJh4YB",
    "content": "We introduce Sinkhorn variational marginal inference as a scalable alternative for marginal inference in exponential families defined over permutation matrices. This method is justified by the Sinkhorn approximation of the permanent, addressing the intractability of computing the matrix of expectations \u03c1 := E(P). Sinkhorn variational marginal inference is introduced as a scalable alternative for marginal inference in exponential families over permutation matrices. The method addresses the intractability of computing the matrix of expectations \u03c1 := E(P) by approximating it as S(L), the Sinkhorn operator applied to L. This approximation results in a doubly stochastic matrix, providing efficient computation and easy implementation. The Sinkhorn approximation is shown to produce the best results for probabilistic inference of neural identity in C.elegans. The Sinkhorn variational marginal inference method introduces a scalable approach for computing the matrix of expectations \u03c1 by approximating it as S(L), the Sinkhorn operator applied to L. This approximation results in a doubly stochastic matrix, facilitating efficient computation and implementation. The method is shown to be effective for probabilistic inference of neural identity in C.elegans. The Sinkhorn permanent approximation, perm S (L), provides bounds for the normalizing constant. The Bethe variational inference method offers a general rationale for variational approximations in graphical models. This method has been successfully applied to permutations, with better theoretical guarantees than the Sinkhorn approximation. The Bethe approximation, computed through belief propagation, offers better theoretical guarantees than the Sinkhorn approximation for the permanent. Computational differences exist, with the Sinkhorn algorithm requiring simpler iterations compared to the more complex message computations in the Bethe approximation. In practice, the Bethe approximation produces better permanent approximations, as shown in Fig 1 (b). However, the Sinkhorn approximation sometimes produces qualitatively better marginals by putting more mass on non-zero entries. The Sinkhorn approximation outperformed the Bethe approximation in producing better marginals by placing more mass on non-zero entries. It also showed better scalability for moderate n values. Comparison between the two approximations was done using 1,000 submatrices of size n = 8 from the C.elegans dataset. Additionally, sampling-based methods were considered for marginal inference. Recent advances in neurotechnology have enabled whole brain imaging of the C.elegans worm, whose nervous system remains stereotypical across individuals. The challenge now is to assign canonical labels to each neuron in volumetric images. A methodology for probabilistic neural identification was applied to the NeuroPAL transgene, facilitating this process. Our methodology focuses on probabilistic neural identification using NeuroPAL, a multicolor C.elegans transgene designed for neuron identification. We estimate probabilities for neuron identities, providing uncertainty estimates for model predictions. Gaussian models are used for canonical neurons, with parameters inferred from annotated worms. The likelihood of observing data is calculated, inducing a posterior over probabilities. The methodology focuses on probabilistic neural identification using NeuroPAL, a multicolor C.elegans transgene for neuron identification. Equation (3.1) induces a posterior over probabilities, with a downstream task involving the computation of approximate neural identities. Human annotation of uncertain neurons leads to model updates and increased identification accuracy. Results are compared to simple baselines in Fig 3, with details in the Appendix. Alternative methods considered include Sinkhorn approximation and Bethe. The methodology focuses on probabilistic neural identification using NeuroPAL, a multicolor C.elegans transgene for neuron identification. Results in Fig 3 compare alternative methods such as Sinkhorn and Bethe approximations. Sinkhorn is slightly better, providing more accurate estimates of low probability marginals. MCMC does not outperform the naive baseline, indicating lack of convergence. Sinkhorn approximation is introduced as a sensible alternative to sampling, offering faster and simpler marginal inference. The Sinkhorn approximation is introduced as a faster and more accurate alternative to sampling for marginal inference. It may provide better approximate marginals than the Bethe approximation, despite potentially leading to worse permanent approximations. The methodology involves using NeuroPAL for probabilistic neural identification, with results comparing different methods. The (log) Sinkhorn approximation of the permanent of L is obtained by evaluating S(L) in the problem it solves. The dataset used consists of NeuroPAL worm heads with available human labels and log-likelihood matrices computed with specific methods. The log-likelihood matrices of C.elegans worms with 180 to 195 neurons were computed using the Sinkhorn and Bethe approximation methods. The MCMC sampler was also utilized with specific parameters. Additionally, a log-space implementation of the message passing algorithm was described. Multiple submatrices of size n were randomly drawn from the log likelihood matrices for further analysis. The parameter eps is introduced for numerical stability. 1000 submatrices of size n were randomly drawn from the log likelihood C.elegans matrices. Error bars were too small to be noticed and thus omitted."
}